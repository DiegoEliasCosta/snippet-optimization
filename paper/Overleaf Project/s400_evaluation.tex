\subsection{Preprocessing SO Code Snippet.}

We are using Stackoverflow dump as our dataset. Cleaning of the stack overflow posts is needed to get clean code from it where each line should valid parsable code.
\begin{enumerate}        
	\item The code are present inside \texttt{<pre><code>} tag. We extracted all the code snippet present inside this tag for all the post.    
	\item Remove special characters from beginning of the line: There are post where the code was written starting with special characters like the following \texttt{>>> df.read\_csv(f)}. Except few special characters, python lines should not start with any special character. In this preprocessing step we removed any such special characters
          
	\item Removing Ipython terminal like codes: Several code fragments have the IPython tokens, IN and OUT, which has been removed. We only pick the code fragment between IN and OUT.
    \begin{lstlisting}
In [4]: df.groupby('A').sum()
Out[4]:
	1 5
	4 6
\end{lstlisting}
    
    \item Validating through Python AST: Finally the lines are passed through the python ast module to check for validity of the code.
    
\end{enumerate}

We have processed posts of stack overflow dump to clean the code in our preprocessing step.

\subsection{Evaluation Methods for Reusable Code Snippet}
     
To evaluate our approach we compare the code yielded by each method against our tagged dataset using two methods: Full solution comparison and statement granularity comparison.



\textbf{Full solution granularity.}
Compare the entire solution extracted by our approach against the manually crafted dataset.
\begin{itemize} 
	\item This is the most important evaluation to argument the usefulness of our methods.  
    \item We are only interested on precision, that is, a solution provided must be correct.
    \item However, we might weight differently incomplete solutions and suboptimal solution (solution with unessential statements).
\end{itemize}

\textbf{Statement granularity.}
Compare the each statement extracted by our methods against the statements of the solution.
\begin{itemize} 
	\item This evaluation allow us to better measure the extracted solution code in terms of accuracy, F1, precision and recall
    \item Recall is the most important aspect here, however recall is very sensitive to our high quality dataset as top questions often have short responses. 
    \item We have to think whether it does measure properly our main concern here: extraction of correct code template from Stack Overflow.
\end{itemize}

\subsection{Evaluation Methods for Cleaning Parallel Corpus for Machine Translation}
\neelesh{This section is written by Neelesh}\\
As reported in \cite{Rahman:2019_CleaningStackOverflowforMT} one of the limitations of MT is that there is no straightforward mechanism to evaluate quality of corpus except the actual training which takes a considerable amount of time. We used evaluation method \textit{Per-Word Alignment Entropy} proposed by the same work. They create simple maximum-likelihood machine translation models. Each English word is aligned to one of more code elements based on maximum-likelihood. The lower the alignment entropy for each English word the stronger the mapping to specific code elements and better the model.




% Our tagged dataset contains the solution code for the answer post. This tagged dataset will serve the purpose of ground truth. The stack overflow dump will be used for testing against this ground truth.

% The codes could be single line or multi-line. Right now, we are considering the \textit{Single line codes} for our evaluation.
% To evaluate we do the following steps:
% \begin{enumerate}
%   \item Iteratively pick answer posts from processed stack overflow dump and look for its presence in our tagged data set using the post ids.
%   \item If the post under consideration is present in the dataset Iterate through lines of code.
%   \item Our methods process takes one line at a time and predicts whether to label it as a solution or not.
%   \item This candidate line is then checked into the tagged dataset solution list and if it is present we mark it as actual solution. We use full text matching to check the presence of a line in solution set.\\
% \end{enumerate}

% For each line of each post of stack overflow dump, we calculate TP, TN, FP and FN as follows:\\\\
% \textbf{TP True Positive:} Method predicts the line as \textit{solution} and the line is also \textit{present} in solution list\\
% \textbf{TN True Negative:} Method predicts the line as \textit{not solution} and the line is \textit{not present} in solution list.\\
% \textbf{FP False Positive:} Method predicts the line as \textit{solution} and the line is \textit{not present} in solution list.\\
% \textbf{FN False Negative:} Method predicts the line as \textit{not solution} and the line is \textit{present} in solution list.\\\\
% We are using metrics Accuracy, Precision, Recall and F1 as define below:\\\\
% \[ Accuracy: \frac{TP+TN}{TP+TN+FP+FN} \]
% \[Precision: \frac{TP}{TP+FP} \]
% \[Recall: \frac{TP}{TP+FN} \]
% \[F1: \frac{2*Precision*Recall}{Precision+Recall} \]

