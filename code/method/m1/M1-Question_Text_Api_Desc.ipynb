{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Luigi migration. Task yet to be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'jupyter_notebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-124f44be4747>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjupyter_notebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'jupyter_notebook'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(\"../..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from jupyter_notebook import load_parameters \n",
    "\n",
    "pars = load_parameters()\n",
    "\n",
    "input_file = pars.get('input')\n",
    "output_file = pars.get('output')\n",
    "\n",
    "api_doc_file = pars.get('api_doc_file')\n",
    "tagged_dataset_file = pars.get('dataset')\n",
    "\n",
    "cosine_sim_th = pars.get('cosine_sim_th')\n",
    "\n",
    "debug = pars.get('debug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility to get similarity between two texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "\n",
    "'''remove punctuation, lowercase, stem'''\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    text1 = removeSpecialChars(text1)\n",
    "    text2 = removeSpecialChars(text2)\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "\n",
    "def removeSpecialChars(text):\n",
    "    return re.sub(\"[^a-zA-Z0-9]\", \" \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility to extract method name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class FunctionCallVisitor(ast.NodeVisitor):\n",
    "    def __init__(self):\n",
    "        self._name = deque()\n",
    "        self._pos = -1 \n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return '.'.join(self._name)\n",
    "    \n",
    "    @property\n",
    "    def lineno(self):\n",
    "        return self._pos\n",
    "    \n",
    "    @name.deleter\n",
    "    def name(self):\n",
    "        self._name.clear()\n",
    "    \n",
    "    def visit_Name(self, node):\n",
    "        self._pos = node.lineno # line number\n",
    "        self._name.appendleft(node.id)\n",
    "    \n",
    "    def visit_Attribute(self, node):\n",
    "        try:\n",
    "            self._pos = node.lineno # line number\n",
    "            self._name.appendleft(node.attr)\n",
    "            self._name.appendleft(node.value.id)\n",
    "        except AttributeError:\n",
    "            self.generic_visit(node)\n",
    "            \n",
    "def get_func_calls(tree):\n",
    "    func_calls = []\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Call):\n",
    "            callvisitor = FunctionCallVisitor()\n",
    "            callvisitor.visit(node.func)\n",
    "            func_calls.append((callvisitor.name, callvisitor.lineno))\n",
    "    return func_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load APIDoc and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_doc_file = '../../data-import/build_api_doc_base/api_doc.csv'\n",
    "tagged_dataset_file = '../../../data/stack-overflow/Dataset - Pandas.csv'\n",
    "input_file = '../../../data/stack-overflow/pandas-preprocessedcode-dataset-part3'\n",
    "\n",
    "api_df = pd.read_csv(api_doc_file, encoding='ISO-8859-1', error_bad_lines=False)\n",
    "tagged_dataset_df = pd.read_csv(tagged_dataset_file, encoding='ISO-8859-1', error_bad_lines=False)\n",
    "processed_stackoverflow_df = pd.read_pickle(input_file)\n",
    "\n",
    "cosine_sim_th = 0.3\n",
    "input_col = 'PreprocessedCode3'\n",
    "\n",
    "def buildAPIDictionary(api_df):\n",
    "    api_dict = dict()\n",
    "    try:\n",
    "        \n",
    "        for index, row in api_df.iterrows():\n",
    "            methodContext = row['Description']\n",
    "            tokens = row['FullyQualifiedName'].split('.')\n",
    "        \n",
    "            for token in tokens:\n",
    "                methodContext = str(methodContext)+' '+token\n",
    "            api_dict[row['MethodName']] = methodContext\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return api_dict\n",
    "        \n",
    "\n",
    "def buildAnswerIdQuestionTextDict(tagged_dataset_df):\n",
    "    dataset_answerId_QText_Dict = dict()\n",
    "    try:\n",
    "        for idx, row in tagged_dataset_df.iterrows():\n",
    "            answerId = row['AnswerId']\n",
    "            if answerId != 0:\n",
    "                dataset_answerId_QText_Dict[answerId] = row['QuestionText']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    return dataset_answerId_QText_Dict\n",
    "\n",
    "def buildStackOverflowDumpDict(processed_stackoverflow_df):\n",
    "    stackoverflow_dict = dict()\n",
    "    try:\n",
    "        for idx, row in processed_stackoverflow_df.iterrows():\n",
    "            postTypeId = row['PostTypeId']\n",
    "            if postTypeId == 2:\n",
    "                answerId = row['Id']\n",
    "                stackoverflow_dict[answerId] = row['PreprocessedCode']\n",
    "    except Exception as e:\n",
    "        e\n",
    "    return stackoverflow_dict\n",
    "    \n",
    "api_dict = buildAPIDictionary(api_df)\n",
    "# tagged_dataset_dict, total_solutions = buildTaggedDatasetDSForEvaluation(tagged_dataset_df)\n",
    "dataset_answerId_QText_Dict = buildAnswerIdQuestionTextDict(tagged_dataset_df)\n",
    "stackoverflow_dict = buildStackOverflowDumpDict(processed_stackoverflow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lookUpAPIDocForContext(method_name):\n",
    "    if method_name in api_dict.keys():\n",
    "        return api_dict[method_name]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def getSOContext(answerId):\n",
    "    return dataset_answerId_QText_Dict[int(answerId)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all the lines --> tp +tn + fp+fn\n",
    "tp-> identified as solution and the solution\n",
    "fp-> identified as solution but not the solution\n",
    "tn-> not identified as solution and also not the solution\n",
    "fn-> not identified as solution but the solution\n",
    "\n",
    "Accuracy: (tp+tn)/(tp+tn+fp+fn)\n",
    "Precision: tp/(tp+fp)\n",
    "Recall: tp/(tp+fn)\n",
    "F1: (2*P*R)/(P+R)\n",
    "\n",
    "TP-> Lines identified as solution are the solution\n",
    "TN -> Lines not identified as solution are not the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def applyM1(cosine_sim_thresould):\n",
    "    df_columns = ['ansId', 'actual','line','predicted']\n",
    "    result_df = pd.DataFrame(columns=df_columns)\n",
    "    df_row_id = 0\n",
    "    for key in tagged_dataset_dict.keys():\n",
    "        try:\n",
    "            solutionList = tagged_dataset_dict[key]\n",
    "            content = str(stackoverflow_dict[key])\n",
    "            lines = content.split(os.linesep)\n",
    "            for line in lines:\n",
    "                actualSolution = False\n",
    "                predictedSolution = False\n",
    "                for tup in solutionList:\n",
    "                    if tup[1].strip() == line.strip():\n",
    "                        actualSolution = True\n",
    "                \n",
    "                tree = ast.parse(line)\n",
    "                func_calls = get_func_calls(tree)\n",
    "                for func_call in func_calls:\n",
    "                    tokens = func_call.split('.')\n",
    "                    method_name = tokens[len(tokens)-1]\n",
    "                    API_Context = lookUpAPIDocForContext(method_name)\n",
    "                    if API_Context == \"\":\n",
    "                        cos_score = -1 # Reject the method\n",
    "                    else:\n",
    "                        SO_Context = getSOContext(key)\n",
    "                        cos_score = cosine_sim(API_Context, SO_Context)\n",
    "                    if cos_score > cosine_sim_thresould:\n",
    "                        predictedSolution = True\n",
    "                        break\n",
    "                \n",
    "                result_df.loc[df_row_id] = [key, line, actualSolution, predictedSolution]\n",
    "                df_row_id = df_row_id + 1\n",
    "        except Exception as e:\n",
    "            e\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(7837947, 'occurred at index 1190')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-2fe91cb4b400>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mprocessed_stackoverflow_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosine_sim_th\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;31m# processed_stackoverflow_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[0;32m   4260\u001b[0m                         \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4261\u001b[0m                         \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4262\u001b[1;33m                         ignore_failures=ignore_failures)\n\u001b[0m\u001b[0;32m   4263\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4264\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_apply_standard\u001b[1;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[0;32m   4356\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4357\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4358\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4359\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4360\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   4237\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0margs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mufunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4238\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4239\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4240\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4241\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-2fe91cb4b400>\u001b[0m in \u001b[0;36mM1\u001b[1;34m(df, cosine_sim_threshould)\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mcos_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;31m# Reject the method\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                 \u001b[0mSO_Context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetSOContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m                 \u001b[0mcos_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAPI_Context\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSO_Context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcos_score\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mcosine_sim_threshould\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-3f5def297763>\u001b[0m in \u001b[0;36mgetSOContext\u001b[1;34m(answerId)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetSOContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswerId\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdataset_answerId_QText_Dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswerId\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: (7837947, 'occurred at index 1190')"
     ]
    }
   ],
   "source": [
    "def M1(df, cosine_sim_threshould):\n",
    "    # Do not process questions\n",
    "    if df.PostTypeId == 1:\n",
    "        df['Solution'] = 'NA'\n",
    "            \n",
    "    else:\n",
    "#         try:\n",
    "        # Parse code and inspect the function\n",
    "        code_snippet = df[input_col]\n",
    "        tree = ast.parse(code_snippet)        \n",
    "        func_calls = get_func_calls(tree)\n",
    "\n",
    "        solution_lines = set()\n",
    "        for function, lineno in func_calls:   \n",
    "            # print(function)\n",
    "            tokens = function.split('.')\n",
    "            method_name = tokens[len(tokens)-1]\n",
    "\n",
    "            API_Context = lookUpAPIDocForContext(method_name)\n",
    "            if API_Context == \"\":\n",
    "                cos_score = -1 # Reject the method\n",
    "            else:\n",
    "                SO_Context = getSOContext(df['Id'])\n",
    "                cos_score = cosine_sim(API_Context, SO_Context)\n",
    "            if cos_score > cosine_sim_threshould:\n",
    "                solution_lines.add(lineno - 1)\n",
    "\n",
    "        solution = []\n",
    "        snippet_per_line = code_snippet.split(os.linesep)\n",
    "        solution_lines = sorted(solution_lines)\n",
    "        for i in solution_lines:\n",
    "            solution.append(snippet_per_line[i])\n",
    "\n",
    "        #print(solution)\n",
    "        df['Solution'] = solution\n",
    "            \n",
    "#         except KeyError:\n",
    "#             df['Solution'] = 'KeyError' # For now ignore the error\n",
    "            \n",
    "#         except LookupError:\n",
    "#             df['Solution'] = 'LookupError' # For now ignore the error\n",
    "        \n",
    "    return df\n",
    "\n",
    "processed_stackoverflow_df.apply(M1, args=(cosine_sim_th,), axis=1)\n",
    "# processed_stackoverflow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cosine_sim_th = 0.0\n",
    "result_df = applyM1(cosine_sim_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_file = '../../../data/results/result_df_m1'\n",
    "debug = True\n",
    "\n",
    "result_df.to_pickle(output_file)\n",
    "\n",
    "if debug:\n",
    "    result_df.to_csv(output_file + \".csv\", encoding='ISO-8859-1', sep=\",\", doublequote=True, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
