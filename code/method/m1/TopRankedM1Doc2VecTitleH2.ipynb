{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "\n",
    "'''remove punctuation, lowercase, stem'''\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    text1 = removeSpecialChars(text1)\n",
    "    text2 = removeSpecialChars(text2)\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "\n",
    "def removeSpecialChars(text):\n",
    "    return re.sub(\"[^a-zA-Z0-9]\", \" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Class to extract method as well as attribute calls. Each token after '.' is called attribute \n",
    "# be it function call or anything else\n",
    "\n",
    "class AttributeVisitor(ast.NodeVisitor):\n",
    "    def __init__(self):\n",
    "        self._name = deque()\n",
    "        self._pos = -1 \n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return '.'.join(self._name)\n",
    "    \n",
    "    @property\n",
    "    def lineno(self):\n",
    "        return self._pos\n",
    "    \n",
    "    @name.deleter\n",
    "    def name(self):\n",
    "        self._name.clear()\n",
    "    \n",
    "    def visit_Name(self, node):\n",
    "        self._pos = node.lineno # line number\n",
    "        self._name.appendleft(node.id)\n",
    "    \n",
    "    def visit_Attribute(self, node):\n",
    "        try:\n",
    "            self._pos = node.lineno # line number\n",
    "            self._name.appendleft(node.attr)\n",
    "            self._name.appendleft(node.value.id)\n",
    "        except AttributeError:\n",
    "            self.generic_visit(node)\n",
    "            \n",
    "def get_all_calls(tree):\n",
    "    all_calls = []\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Attribute):\n",
    "            callvisitor = AttributeVisitor()\n",
    "            callvisitor.visit(node)\n",
    "            all_calls.append((callvisitor.name, callvisitor.lineno))\n",
    "    return all_calls\n",
    "\n",
    "# Visitin method calls only\n",
    "class FunctionCallVisitor(ast.NodeVisitor):\n",
    "    def __init__(self):\n",
    "        self._name = deque()\n",
    "        self._pos = -1 \n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return '.'.join(self._name)\n",
    "    \n",
    "    @property\n",
    "    def lineno(self):\n",
    "        return self._pos\n",
    "    \n",
    "    @name.deleter\n",
    "    def name(self):\n",
    "        self._name.clear()\n",
    "    \n",
    "    def visit_Name(self, node):\n",
    "        self._pos = node.lineno # line number\n",
    "        self._name.appendleft(node.id)\n",
    "    \n",
    "    def visit_Attribute(self, node):\n",
    "        try:\n",
    "            self._pos = node.lineno # line number\n",
    "            self._name.appendleft(node.attr)\n",
    "            self._name.appendleft(node.value.id)\n",
    "        except AttributeError:\n",
    "            self.generic_visit(node)\n",
    "            \n",
    "def get_func_calls(tree):\n",
    "    func_calls = []\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Call):\n",
    "            callvisitor = FunctionCallVisitor()\n",
    "            callvisitor.visit(node.func)\n",
    "            func_calls.append((callvisitor.name, callvisitor.lineno))\n",
    "    return func_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_doc_file = '../../data-import/build_api_doc_base/api_doc.csv'\n",
    "so_dump_processed_file = '../../../data/stack-overflow/pandas-preprocessedcode-dataset-part3'\n",
    "code_snippet_col = 'PreprocessedCode3'\n",
    "id_col = 'Id'\n",
    "#cosine_sim_th = 0.0\n",
    "dataset = '../../../data/stack-overflow/Dataset - Pandas.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_df = pd.read_csv(api_doc_file, encoding='ISO-8859-1', error_bad_lines=False)\n",
    "dataset_df = pd.read_csv(dataset, encoding='ISO-8859-1', error_bad_lines=False)\n",
    "\n",
    "processed_stackoverflow_df = pd.read_pickle(so_dump_processed_file)\n",
    "\n",
    "\n",
    "## Get API description with fully qualified name for a method from API doc and build the context\n",
    "def buildAPIDictionary(api_df):\n",
    "    api_dict = dict()\n",
    "    try:\n",
    "        \n",
    "        for index, row in api_df.iterrows():\n",
    "            methodContext = row['Description']\n",
    "            tokens = row['FullyQualifiedName'].split('.')\n",
    "        \n",
    "            for token in tokens:\n",
    "                methodContext = str(methodContext)+' '+token\n",
    "            api_dict[row['MethodName']] = methodContext\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return api_dict\n",
    "\n",
    "def buildAPIDictionaryH2(api_df):\n",
    "    api_dict_h2 = dict()\n",
    "    try:\n",
    "        \n",
    "        for index, row in api_df.iterrows():\n",
    "            methodContext = row['SubCategory']\n",
    "            #tokens = row['FullyQualifiedName'].split('.')\n",
    "        \n",
    "            #for token in tokens:\n",
    "               #methodContext = str(methodContext)+' '+token\n",
    "            api_dict_h2[row['MethodName']] = methodContext\n",
    "    except Exception as e:\n",
    "        print('Error in method buildAPIDictionary',e)\n",
    "    return api_dict_h2\n",
    "        \n",
    "## Get AnswerId and Question Text combo from dataset to build the context\n",
    "def buildAnswerIdQuestionTextDict(dataset_df):\n",
    "    dataset_answerId_QText_Dict = dict()\n",
    "    try:\n",
    "        for idx, row in dataset_df.iterrows():\n",
    "            answerId = row['AnswerId']\n",
    "            if answerId != 0:\n",
    "                dataset_answerId_QText_Dict[answerId] = row['QuestionText']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    return dataset_answerId_QText_Dict\n",
    "\n",
    "api_dict = buildAPIDictionary(api_df)\n",
    "api_dict_h2 = buildAPIDictionaryH2(api_df)\n",
    "dataset_answerId_QText_Dict = buildAnswerIdQuestionTextDict(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookUpAPIDocForContext(method_name):\n",
    "    if method_name in api_dict.keys():\n",
    "        return api_dict[method_name]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def getSOContext(answerId):\n",
    "    return dataset_answerId_QText_Dict[int(answerId)]\n",
    "\n",
    "def lookUpAPIDocForContextH1H2(method_name):\n",
    "    try:\n",
    "        if method_name in api_dict_h2.keys():\n",
    "            if api_dict_h2[method_name] == \"Constructor\":\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print('Error in method lookUpAPIDocForContext', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10202789,df = pandas.DataFrame(np.random.randn(5,3),columns=['A','B','C'])\n",
      "10374456,DataFrame({'count' : df1.groupby( [ \"Name\", \"City\"] ).size()}).reset_index()\n",
      "10458386,In : data2 = data.set_index('a')\n",
      "11067072,df.reindex_axis(sorted(df.columns), axis=1)\n",
      "11138275,df_ora = pd.read_sql('select * from user_objects', con=ora_conn)    \n",
      "11287278,df1 = df.ix[0,0:2].copy() \n",
      "11346337,df = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\n",
      "11354850,df = df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'})\n",
      "11362056,paramdata.index\n",
      "11531402,df[df['A'].str.contains(\"hello\")]\n",
      "11711637,pd.set_option('display.height', 1000)\n",
      "12098586,df[df['A'].isin([3, 6])]\n",
      "12525836,df_norm.max() - df_norm.min()\n",
      "12555510,df1 = df1.assign(e=p.Series(np.random.randn(sLength)).values)\n",
      "12681217,                    for _, row in a.iterrows()]).reset_index()\n",
      "13148611,cols = df.columns.tolist()\n",
      "13295801,df.fillna(0)\n",
      "13434501,df = pd.DataFrame(np.random.randn(10,3))\n",
      "13682381,data['result'] = data['result'].map(lambda x: x.lstrip('+-').rstrip('aAbBcC'))\n",
      "13704307,dt = datetime.utcnow()\n",
      "13786327,df_ = pd.DataFrame(index=index, columns=columns)\n",
      "13842286,df.xs('C')['x']=10\n",
      "13851602,df[df['column name'].map(len) < 2]\n",
      "14508355,df.columns = df.columns.get_level_values(0)\n",
      "14661768,df.drop(df.index[[1,3]])\n",
      "14734627,gb.get_group('foo')\n",
      "14745484,df = pd.DataFrame(df.row.str.split(' ',1).tolist(),\n",
      "14900065,df4 = df3.drop_duplicates(subset='rownum', keep='last')\n",
      "14946246,grp = df.groupby('A')\n",
      "15252012,pd.read_csv(\"ragged.csv\", names=my_cols, engine='python')\n",
      "15362700,df = pd.DataFrame({'x' : [1, 2, 3, 4], 'y' : [4, 5, 6, 7]})\n",
      "15411596,table.groupby('YEARMONTH').CLIENTCODE.nunique()\n",
      "15705958,df.groupby(['Mt'], sort=False)['count'].max()\n",
      "15772263,df.resample(\"1d\").sum().fillna(0).rolling(window=3, min_periods=1).mean()\n",
      "15943975,df = pd.DataFrame(np.arange(9).reshape(3,3))\n",
      "16104482,df.iloc[[2]]\n",
      "16354730,df['Value'] = df.apply(lambda row: my_test(row['a'], row['c']), axis=1)\n",
      "16597375,df.append(data)\n",
      "16729808,sub_df.iloc[0]\n",
      "16735476,df.convert_objects(convert_numeric=True)\n",
      "16923367,df.to_csv(file_name, sep='\\t')\n",
      "17063653,df = xl.parse(\"Sheet1\")\n",
      "17071908,df = df.set_index(['B'])\n",
      "17095620,pd.DataFrame({'from': changed_from, 'to': changed_to}, index=changed.index)\n",
      "17098736,df = pd.read_pickle(file_name)\n",
      "17134750,df['col'] = pd.to_datetime(df['col'])\n",
      "17141755,df.sort_values(['a', 'b'], ascending=[True, False])\n",
      "17242374,df.index.values\n",
      "17531025,df = pd.read_csv('foo.csv', index_col=0)\n",
      "17619032,df.sort_values(['c1','c2'], ascending=[False,True])\n",
      "17682662,df[df.c > 0.5][['b', 'e']].values\n",
      "17682726,df.loc[df['c']>0.5,['a','d']].values\n",
      "17729985,d.sales = d.sales.replace(23, 24)\n",
      "17813222,df.plot(x='col_name_1', y='col_name_2', style='o')\n",
      "17841294,df.groupby('A').apply(lambda x: x.sum())\n",
      "17950531,df['A'].apply(str)\n",
      "18023468,df.index.name\n",
      "18062521,s1 = pd.Series([1, 2], index=['A', 'B'], name='s1')\n",
      "18129082,data = pd.read_csv('file1.csv', error_bad_lines=False)\n",
      "18327852,myseries[myseries == 7].index[0]\n",
      "18431417,df.fillna(-1)\n",
      "18695700,df.set_index('id').to_dict()\n",
      "18837389,s.reset_index()\n",
      "18942558,df['Col3'] = (df['Col2'] <= 1).astype(int)\n",
      "19237920,df[['Time', 'Product']].query('Product == p_id and Month < mn and Year == yr')\n",
      "19324591,idx = pd.date_range('09-01-2013', '09-30-2013')\n",
      "19378497,dataframe[\"period\"] = dataframe[\"Year\"].map(str) + dataframe[\"quarter\"]\n",
      "19385591,df[['col1', 'col2', 'col3', 'col4']].groupby(['col1', 'col2']).agg(['mean', 'count'])\n",
      "19483025,list(my_dataframe.columns.values)\n",
      "19851521,df1 = df.set_index('A')\n",
      "19960116,df.countries.isin(countries)\n",
      "19961557,df.pivot(index=0, columns=1, values=2)\n",
      "20084895,Series(df.values.ravel()).unique()\n",
      "20221655,data_filtered.to_excel(writer, \"Main\", cols=['Diff1', 'Diff2'])\n",
      "20461206,df.reset_index(level=0, inplace=True)\n",
      "20491748,df = df.reset_index(drop=True)\n",
      "20868446,Series.rename\n",
      "21232849,    df = pd.read_csv(file_,index_col=None, header=0)\n",
      "21291622,df.a = df.a.astype(float)\n",
      "21463854,data['amount'] = data[\"amount\"].fillna(data.groupby(\"num\")[\"amount\"].transform(\"mean\"))\n",
      "21800319,df[df['BoolCol'] == True].index.tolist()\n",
      "22006514,df.to_json()\n",
      "22341390,dfList = df['one'].tolist()\n",
      "22391554,df['a'].value_counts()\n",
      "22475141,df = pd.DataFrame([[1, 2.3456, 'c', 'd', 78]], columns=list(\"ABCDE\"))\n",
      "23307361,w['female'] = w['female'].map({'female': 1, 'male': 0})\n",
      "23749057,df['a'].values.tolist()\n",
      "24147363,df = pd.DataFrame(np.random.randn(100, 2))\n",
      "24793359,numpyMatrix = df.as_matrix()\n",
      "25254087,df = pd.DataFrame({'foo':list('ABC')}, index=[0,2,1])\n",
      "25376997,df.loc[len(df)]=['8/19/2014','Jun','Fly','98765'] \n",
      "25748826,df = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})\n",
      "25962187,for chunk in pd.read_csv(filename, chunksize=chunksize):\n",
      "26266451,s.isnull().sum()\n",
      "26301947,pd.set_option('display.max_colwidth', -1)\n",
      "27791362,df = pd.read_csv(StringIO(csv),\n",
      "28648923,df = pd.DataFrame(a, columns=['col1','col2','col3'])\n",
      "28902170,df2 = pd.DataFrame(data = {'col1' : [2, 3,4], 'col2' : [11, 12,13]})\n",
      "29530601,df.isnull().values.any()\n",
      "30512931,df_final = reduce(lambda left,right: pd.merge(left,right,on='name'), dfs)\n",
      "30691921,with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n",
      "32801170,groupby_object = df[['col1', 'col2', 'col3', 'col4']]\\\n",
      "34272155,df.drop_duplicates(subset=['A', 'C'], keep=False)\n",
      "34297689,df3 = df3[~df3.index.duplicated(keep='first')]\n",
      "36572039,df.read_csv(filename ,  index = False)  \n",
      "39358924,df['AB'].str.split(' ', 1, expand=True)\n",
      "44736467,df.loc[:, 'foo':'sat']\n"
     ]
    }
   ],
   "source": [
    "def M1(df):\n",
    "    \n",
    "    try:\n",
    "        # Do not process questions\n",
    "        if df.PostTypeId == 1:\n",
    "            df['Solution'] = 'NA'\n",
    "\n",
    "        else:\n",
    "            # Parse code and inspect the function\n",
    "            code_snippet = df[code_snippet_col]\n",
    "            Id = df[id_col]\n",
    "            tree = ast.parse(code_snippet)        \n",
    "            all_calls = get_all_calls(tree)\n",
    "\n",
    "            solution_lines = set()\n",
    "            max_score = -1\n",
    "            max_line = ''\n",
    "            snippet_per_line = code_snippet.split(os.linesep)\n",
    "            for call, lineno in all_calls:\n",
    "               \n",
    "                tokens = call.split('.')\n",
    "                method_name = tokens[len(tokens)-1]\n",
    "                \n",
    "                #if lookUpAPIDocForContextH1H2(method_name):\n",
    "                API_Context = lookUpAPIDocForContext(method_name)\n",
    "                \n",
    "                # If API context is not defined, i.e method is not present in the API, \n",
    "                # reject the method as it could not be solution as per our assumption by making cosine score negative\n",
    "                if API_Context == \"\":\n",
    "                            cos_score = -1 # Reject the method\n",
    "                \n",
    "                # If method is in API, use its description and match with question text (SO_context)\n",
    "                else:\n",
    "                    SO_Context = getSOContext(Id)\n",
    "                    cos_score = cosine_sim(API_Context, SO_Context)\n",
    "                if cos_score > max_score:\n",
    "                    max_score = cos_score\n",
    "                    max_line = snippet_per_line[lineno - 1]\n",
    "                    #print(API_Context, '\\n', SO_Context,  '\\n\\n')\n",
    "                    # Use a set to not add the same line twice\n",
    "                    solution_lines.add(lineno - 1)\n",
    "\n",
    "            if max_line:\n",
    "                print(str(Id)+','+max_line)\n",
    "            solution = []\n",
    "            solution_lines = sorted(solution_lines)\n",
    "            for i in solution_lines:\n",
    "                solution.append(snippet_per_line[i])\n",
    "\n",
    "\n",
    "            #print(solution)\n",
    "            df['Solution'] = solution\n",
    "            #df['Solution'] = os.linesep.join(solution)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return df\n",
    "\n",
    "\n",
    "solution_df = processed_stackoverflow_df.apply(M1, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
