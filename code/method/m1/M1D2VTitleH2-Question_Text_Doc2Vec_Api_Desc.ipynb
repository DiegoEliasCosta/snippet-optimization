{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, string\n",
    "import numpy as np\n",
    "import ast\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Luigi migration. Task yet to be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'jupyter_notebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-124f44be4747>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjupyter_notebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'jupyter_notebook'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(\"../../..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from jupyter_notebook import load_parameters \n",
    "\n",
    "pars = load_parameters()\n",
    "\n",
    "so_dump_processed_file = pars.get('input')\n",
    "output_file = pars.get('output')\n",
    "\n",
    "code_snippet_col = pars.get('code_snippet_col')\n",
    "id_col = pars.get('id_col')\n",
    "\n",
    "api_doc_file = pars.get('api_doc_file')\n",
    "dataset = pars.get('dataset')\n",
    "cosine_sim_th = pars.get('cosine_sim_th')\n",
    "\n",
    "debug = pars.get('debug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility to get similarity between two texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeSpecialChars(text):\n",
    "    return re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "ps =PorterStemmer()\n",
    "\n",
    "# Load Doc2Vec model\n",
    "doc2VecModel = Doc2Vec.load('doc2vec_title.model')\n",
    "\n",
    "def preprocessText(text):\n",
    "    text_without_spchar = removeSpecialChars(text)\n",
    "    word_tokens = word_tokenize(text_without_spchar) \n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "\n",
    "    Stem_words = []\n",
    "    for w in filtered_sentence:\n",
    "        rootWord=ps.stem(w)\n",
    "        Stem_words.append(rootWord)\n",
    "    return Stem_words\n",
    "\n",
    "def get_cosine_sim(text1, text2):\n",
    "    preprocessed_text1 = preprocessText(text1)\n",
    "    preprocessed_text2 = preprocessText(text2)\n",
    "    vector1 = doc2VecModel.infer_vector(preprocessed_text1)\n",
    "    vector2 = doc2VecModel.infer_vector(preprocessed_text2)\n",
    "    dot = np.dot(vector1, vector2)\n",
    "    norma = np.linalg.norm(vector1)\n",
    "    normb = np.linalg.norm(vector2)\n",
    "    cos = dot / (norma * normb)\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility to extract method name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Class to extract method as well as attribute calls. Each token after '.' is called attribute \n",
    "# be it function call or anything else\n",
    "\n",
    "class AttributeVisitor(ast.NodeVisitor):\n",
    "    def __init__(self):\n",
    "        self._name = deque()\n",
    "        self._pos = -1 \n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return '.'.join(self._name)\n",
    "    \n",
    "    @property\n",
    "    def lineno(self):\n",
    "        return self._pos\n",
    "    \n",
    "    @name.deleter\n",
    "    def name(self):\n",
    "        self._name.clear()\n",
    "    \n",
    "    def visit_Name(self, node):\n",
    "        self._pos = node.lineno # line number\n",
    "        self._name.appendleft(node.id)\n",
    "    \n",
    "    def visit_Attribute(self, node):\n",
    "        try:\n",
    "            self._pos = node.lineno # line number\n",
    "            self._name.appendleft(node.attr)\n",
    "            self._name.appendleft(node.value.id)\n",
    "        except AttributeError:\n",
    "            self.generic_visit(node)\n",
    "            \n",
    "def get_all_calls(tree):\n",
    "    all_calls = []\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Attribute):\n",
    "            callvisitor = AttributeVisitor()\n",
    "            callvisitor.visit(node)\n",
    "            all_calls.append((callvisitor.name, callvisitor.lineno))\n",
    "    return all_calls\n",
    "\n",
    "# Visitin method calls only\n",
    "class FunctionCallVisitor(ast.NodeVisitor):\n",
    "    def __init__(self):\n",
    "        self._name = deque()\n",
    "        self._pos = -1 \n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return '.'.join(self._name)\n",
    "    \n",
    "    @property\n",
    "    def lineno(self):\n",
    "        return self._pos\n",
    "    \n",
    "    @name.deleter\n",
    "    def name(self):\n",
    "        self._name.clear()\n",
    "    \n",
    "    def visit_Name(self, node):\n",
    "        self._pos = node.lineno # line number\n",
    "        self._name.appendleft(node.id)\n",
    "    \n",
    "    def visit_Attribute(self, node):\n",
    "        try:\n",
    "            self._pos = node.lineno # line number\n",
    "            self._name.appendleft(node.attr)\n",
    "            self._name.appendleft(node.value.id)\n",
    "        except AttributeError:\n",
    "            self.generic_visit(node)\n",
    "            \n",
    "def get_func_calls(tree):\n",
    "    func_calls = []\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Call):\n",
    "            callvisitor = FunctionCallVisitor()\n",
    "            callvisitor.visit(node.func)\n",
    "            func_calls.append((callvisitor.name, callvisitor.lineno))\n",
    "    return func_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load APIDoc and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#api_doc_file = '../../data-import/build_api_doc_base/api_doc.csv'\n",
    "#so_dump_processed_file = '../../../data/stack-overflow/pandas-preprocessedcode-dataset-part3'\n",
    "#output_file = '../../../data/stack-overflow/pandas-solutioncode-m1'\n",
    "#code_snippet_col = 'PreprocessedCode3'\n",
    "#id_col = 'Id'\n",
    "#cosine_sim_th = 0.0\n",
    "#dataset = '../../../data/stack-overflow/Dataset - Pandas.csv'\n",
    "\n",
    "api_df = pd.read_csv(api_doc_file, encoding='ISO-8859-1', error_bad_lines=False)\n",
    "dataset_df = pd.read_csv(dataset, encoding='ISO-8859-1', error_bad_lines=False)\n",
    "\n",
    "processed_stackoverflow_df = pd.read_pickle(so_dump_processed_file)\n",
    "\n",
    "\n",
    "## Get API description with fully qualified name for a method from API doc and build the context\n",
    "def buildAPIDictionary(api_df):\n",
    "    api_dict = dict()\n",
    "    try:\n",
    "        \n",
    "        for index, row in api_df.iterrows():\n",
    "            try:\n",
    "                r = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "                r.extract_keywords_from_text(row['Description'])\n",
    "                #methodContext[answerId] = ' '.join(r.get_ranked_phrases()[:1]])\n",
    "                methodContext = row['Description']\n",
    "            except:\n",
    "                methodContext = row['Description']\n",
    "            \n",
    "            tokens = row['FullyQualifiedName'].split('.')\n",
    "        \n",
    "            for token in tokens:\n",
    "                methodContext = str(methodContext)+' '+token\n",
    "            api_dict[row['MethodName']] = methodContext\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return api_dict\n",
    "\n",
    "def buildAPIDictionaryH2(api_df):\n",
    "    api_dict_h2 = dict()\n",
    "    try:\n",
    "        \n",
    "        for index, row in api_df.iterrows():\n",
    "            methodContext = row['SubCategory']\n",
    "            #tokens = row['FullyQualifiedName'].split('.')\n",
    "        \n",
    "            #for token in tokens:\n",
    "               #methodContext = str(methodContext)+' '+token\n",
    "            api_dict_h2[row['MethodName']] = methodContext\n",
    "    except Exception as e:\n",
    "        print('Error in method buildAPIDictionary',e)\n",
    "    return api_dict_h2\n",
    "        \n",
    "## Get AnswerId and Question Text combo from dataset to build the context\n",
    "def buildAnswerIdQuestionTextDict(dataset_df):\n",
    "    dataset_answerId_QText_Dict = dict()\n",
    "    try:\n",
    "        for idx, row in dataset_df.iterrows():\n",
    "            answerId = row['AnswerId']\n",
    "            if answerId != 0:\n",
    "                try:\n",
    "                    r = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "                    r.extract_keywords_from_text(row['QuestionText'])\n",
    "                    dataset_answerId_QText_Dict[answerId] = ' '.join(r.get_ranked_phrases()[:1])\n",
    "                except:\n",
    "                    dataset_answerId_QText_Dict[answerId] = row['QuestionText']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    return dataset_answerId_QText_Dict\n",
    "\n",
    "api_dict = buildAPIDictionary(api_df)\n",
    "api_dict_h2 = buildAPIDictionaryH2(api_df)\n",
    "dataset_answerId_QText_Dict = buildAnswerIdQuestionTextDict(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lookUpAPIDocForContext(method_name):\n",
    "    if method_name in api_dict.keys():\n",
    "        return api_dict[method_name]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def getSOContext(answerId):\n",
    "    return dataset_answerId_QText_Dict[int(answerId)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookUpAPIDocForContextH1H2(method_name):\n",
    "    try:\n",
    "        if method_name in api_dict_h2.keys():\n",
    "            if api_dict_h2[method_name] == \"Constructor\":\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print('Error in method lookUpAPIDocForContext', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def M1(df, cosine_sim_threshould):\n",
    "    \n",
    "    try:\n",
    "        # Do not process questions\n",
    "        if df.PostTypeId == 1:\n",
    "            df['Solution'] = 'NA'\n",
    "\n",
    "        else:\n",
    "            # Parse code and inspect the function\n",
    "            code_snippet = df[code_snippet_col]\n",
    "            Id = df[id_col]\n",
    "            tree = ast.parse(code_snippet)        \n",
    "            all_calls = get_all_calls(tree)\n",
    "\n",
    "            solution_lines = set()\n",
    "            for call, lineno in all_calls:   \n",
    "               \n",
    "                tokens = call.split('.')\n",
    "                method_name = tokens[len(tokens)-1]\n",
    "                if lookUpAPIDocForContextH1H2(method_name):\n",
    "                    API_Context = lookUpAPIDocForContext(method_name)\n",
    "                \n",
    "                    # If API context is not defined, i.e method is not present in the API, \n",
    "                    # reject the method as it could not be solution as per our assumption by making cosine score negative\n",
    "                    if API_Context == \"\":\n",
    "                                cos_score = -1 # Reject the method\n",
    "                \n",
    "                    # If method is in API, use its description and match with question text (SO_context)\n",
    "                    else:\n",
    "                        SO_Context = getSOContext(Id)\n",
    "                        cos_score = get_cosine_sim(API_Context, SO_Context)\n",
    "                    if cos_score > cosine_sim_threshould:\n",
    "                        #print(API_Context, '\\n', SO_Context,  '\\n\\n')\n",
    "                        # Use a set to not add the same line twice\n",
    "                        solution_lines.add(lineno - 1)\n",
    "\n",
    "            solution = []\n",
    "            snippet_per_line = code_snippet.split(os.linesep)\n",
    "            solution_lines = sorted(solution_lines)\n",
    "            for i in solution_lines:\n",
    "                solution.append(snippet_per_line[i])\n",
    "\n",
    "\n",
    "            #print(solution)\n",
    "            df['Solution'] = solution\n",
    "            #df['Solution'] = os.linesep.join(solution)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return df\n",
    "\n",
    "\n",
    "solution_df = processed_stackoverflow_df.apply(M1, args=(cosine_sim_th,), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debug = True\n",
    "\n",
    "solution_df.to_pickle(output_file)\n",
    "\n",
    "if debug:\n",
    "    solution_df.to_csv(output_file + \".csv\", sep=\",\", doublequote=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
