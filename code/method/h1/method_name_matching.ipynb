{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility to extract method name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class FunctionCallVisitor(ast.NodeVisitor):\n",
    "    def __init__(self):\n",
    "        self._name = deque()\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return '.'.join(self._name)\n",
    "    \n",
    "    @name.deleter\n",
    "    def name(self):\n",
    "        self._name.clear()\n",
    "    \n",
    "    def visit_Name(self, node):\n",
    "        self._name.appendleft(node.id)\n",
    "    \n",
    "    def visit_Attribute(self, node):\n",
    "        try:\n",
    "            self._name.appendleft(node.attr)\n",
    "            self._name.appendleft(node.value.id)\n",
    "        except AttributeError:\n",
    "            self.generic_visit(node)\n",
    "            \n",
    "def get_func_calls(tree):\n",
    "    func_calls = []\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Call):\n",
    "            callvisitor = FunctionCallVisitor()\n",
    "            callvisitor.visit(node.func)\n",
    "            func_calls.append(callvisitor.name)\n",
    "    return func_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load APIDoc and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot convert float NaN to integer\n"
     ]
    }
   ],
   "source": [
    "#API_DOC_FILE_PATH = 'api_doc.pkl'\n",
    "#DATASET_FILE_PATH = 'dataset_df.pkl'\n",
    "#api_df = pickle.load( open( API_DOC_FILE_PATH, \"rb\" ))\n",
    "#dataset_df = pickle.load( open( DATASET_FILE_PATH, \"rb\" ))\n",
    "\n",
    "API_DOC_OBJ_FILE_PATH = '../../data-import/build_api_doc_base/api_doc.csv'\n",
    "TAGGED_DATASET_FILE_PATH = '../../../data/stack-overflow/Dataset - Pandas.csv'\n",
    "STACK_OVERFLOW_PROCESSED_DUMP_FILE = '../../../data/stack-overflow/pandas-preprocessedcode-dataset-part3.csv'\n",
    "\n",
    "api_df = pd.read_csv(API_DOC_OBJ_FILE_PATH, encoding='ISO-8859-1', error_bad_lines=False)\n",
    "tagged_dataset_df = pd.read_csv(TAGGED_DATASET_FILE_PATH, encoding='ISO-8859-1', error_bad_lines=False)\n",
    "processed_stackoverflow_df = pd.read_csv(STACK_OVERFLOW_PROCESSED_DUMP_FILE, encoding='ISO-8859-1', error_bad_lines=False)\n",
    "\n",
    "\n",
    "\n",
    "def buildMethodNameSet(api_df):\n",
    "    method_set = set()\n",
    "    for index, row in api_df.iterrows():\n",
    "        method_set.add(row['MethodName'])\n",
    "    return method_set\n",
    "        \n",
    "\n",
    "def buildTaggedDatasetDSForEvaluation(tagged_dataset_df):\n",
    "    dataset_dict = dict()\n",
    "    total_solutions = 0\n",
    "    try:    \n",
    "        for idx, row in tagged_dataset_df.iterrows():\n",
    "            answerId = row['AnswerId']\n",
    "            if answerId != 0:\n",
    "                total_solutions = total_solutions +1\n",
    "                tup = (int(row['SolutionId']), row['Solution'])\n",
    "                if answerId in dataset_dict:\n",
    "                    ls = dataset_dict[answerId]\n",
    "                    ls.append(tup)\n",
    "                    dataset_dict[answerId] = ls\n",
    "                else:\n",
    "                    ls = list()\n",
    "                    ls.append(tup)\n",
    "                    dataset_dict[answerId] = ls\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    return dataset_dict, total_solutions\n",
    "\n",
    "def buildStackOverflowDumpDict(processed_stackoverflow_df):\n",
    "    stackoverflow_dict = dict()\n",
    "    try:\n",
    "        for idx, row in processed_stackoverflow_df.iterrows():\n",
    "            postTypeId = row['PostTypeId']\n",
    "            if postTypeId == 2:\n",
    "                answerId = row['Id']\n",
    "                stackoverflow_dict[answerId] = row['PreprocessedCode']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return stackoverflow_dict\n",
    "    \n",
    "method_set = buildMethodNameSet(api_df)\n",
    "tagged_dataset_dict, total_solutions = buildTaggedDatasetDSForEvaluation(tagged_dataset_df)\n",
    "stackoverflow_dict = buildStackOverflowDumpDict(processed_stackoverflow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookUpAPIDoc(method_set, method_name):\n",
    "    if method_name in method_set:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41386927\n",
      "39923012\n",
      "37787724\n",
      "46526249\n",
      "Accuracy = 90.5940594059406%\n",
      "Precision = 63.855421686746986%\n",
      "Recall = 86.88524590163935%\n",
      "F1 = 0.7361111111111112\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_CODE_OUTPUT_FOLDER = '../../data-preprocess/neelesh/processed_answer_codes'\n",
    "\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "'''for fil in os.listdir(PROCESSED_CODE_OUTPUT_FOLDER):\n",
    "    try:\n",
    "        solutionList = dataset_dict[int(fil)]\n",
    "        with open(os.path.join(PROCESSED_CODE_OUTPUT_FOLDER, fil), 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            #print f\n",
    "            for line in lines:\n",
    "                actualSolution = False\n",
    "                predictedSolution = False\n",
    "                for tup in solutionList:\n",
    "                    if tup[1].strip() == line.strip():\n",
    "                        actualSolution = True\n",
    "                tree = ast.parse(line)\n",
    "                func_calls = get_func_calls(tree)\n",
    "                for func_call in func_calls:\n",
    "                    tokens = func_call.split('.')\n",
    "                    method_name = tokens[len(tokens)-1]\n",
    "                    if lookUpAPIDoc(method_set, method_name):\n",
    "                        predictedSolution = True\n",
    "                        break\n",
    "                if actualSolution and predictedSolution:\n",
    "                    TP = TP + 1\n",
    "                if (not actualSolution) and predictedSolution:\n",
    "                    FP = FP + 1\n",
    "                if (not actualSolution) and (not predictedSolution):\n",
    "                    TN = TN + 1\n",
    "                if actualSolution and (not predictedSolution):\n",
    "                    FN = FN + 1\n",
    "    except Exception as e:\n",
    "        print e'''\n",
    "\n",
    "\n",
    "for key in tagged_dataset_dict.keys():\n",
    "    try:\n",
    "        solutionList = tagged_dataset_dict[key]\n",
    "        content = str(stackoverflow_dict[key])\n",
    "        lines = content.split(os.linesep)\n",
    "        for line in lines:\n",
    "            actualSolution = False\n",
    "            predictedSolution = False\n",
    "            for tup in solutionList:\n",
    "                    if tup[1].strip() == line.strip():\n",
    "                        actualSolution = True\n",
    "            tree = ast.parse(line)\n",
    "            func_calls = get_func_calls(tree)\n",
    "            for func_call in func_calls:\n",
    "                    tokens = func_call.split('.')\n",
    "                    method_name = tokens[len(tokens)-1]\n",
    "                    if lookUpAPIDoc(method_set, method_name):\n",
    "                        predictedSolution = True\n",
    "                        break\n",
    "            if actualSolution and predictedSolution:\n",
    "                TP = TP + 1\n",
    "            if (not actualSolution) and predictedSolution:\n",
    "                FP = FP + 1\n",
    "            if (not actualSolution) and (not predictedSolution):\n",
    "                TN = TN + 1\n",
    "            if actualSolution and (not predictedSolution):\n",
    "                FN = FN + 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "acc = (TP+TN)*100/(TP+TN+FP+FN)\n",
    "Precision = TP*100/(TP + FP)\n",
    "Recall = TP*100/(TP+FN)\n",
    "F1 = (2*Precision*Recall)/(Precision+Recall)\n",
    "F1 = float(F1)/100\n",
    "print('Accuracy = '+ str(acc) + '%')\n",
    "print('Precision = '+ str(Precision) + '%')\n",
    "print('Recall = '+ str(Recall) + '%')\n",
    "print('F1 = '+ str(F1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
