{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luigi Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(\"../..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from jupyter_notebook import load_parameters \n",
    "\n",
    "pars = load_parameters()\n",
    "\n",
    "input_file = pars.get('input')\n",
    "output_file = pars.get('output')\n",
    "\n",
    "# Dealing with multiple outputs\n",
    "\n",
    "dataset = pars.get('dataset')\n",
    "\n",
    "debug = pars.get('debug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input_file = '../../data/stack-overflow/pandas-solutioncode-m1'\n",
    "#output_file = '../../data/results/results-m1.txt'\n",
    "#dataset = '../../data/stack-overflow/Dataset - Pandas.csv'\n",
    "\n",
    "dataset = pd.read_csv(dataset, encoding='ISO-8859-1', error_bad_lines=False)\n",
    "\n",
    "result_df = pd.read_pickle(input_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### buildTaggedDatasetDSForEvaluation:\n",
    "\n",
    "This method is used to build marker for tagged dataset. \n",
    "A post can have mutiple solutions and a solution can have multiple lines.\n",
    "\n",
    "\\[Id, List of alternative solutions\\]\n",
    "List of alternative solutions --> List of \\[line, Marked_Flag\\] .. Defualt mark is False\n",
    "\n",
    "AnswerId: 41386927\t\n",
    "\n",
    "Sol 1= \tdf = pd.read_csv('filename.txt', sep=\";\", names=['Region Name'])\n",
    "\n",
    "Sol\t2=\t\"df.insert(0, 'State', df['Region Name'].str.extract('(.*)\\[edit\\]', expand=False).ffill())\n",
    "\n",
    "        df['Region Name'] = df['Region Name'].str.replace(r' \\(.+$', '')\"\n",
    "\n",
    "A dictionary element\n",
    "{41386927: [\n",
    "\n",
    "            List of alternative solutions\n",
    "            [   \n",
    "                List of lines of Sol 1 \n",
    "               [\n",
    "               A pair which has line and Marker Flag sol 1 line 1\n",
    "                   \n",
    "                   [df = pd.read_csv('filename.txt', sep=\";\", names=['Region Name']), False]           \n",
    "                ]                \n",
    "                 List of lines of Sol 2\n",
    "               [\n",
    "               A pair which has line and Marker Flag sol 2 line 1\n",
    "                   \n",
    "                   [df.insert(0, 'State', df['Region Name'].str.extract('(.*)\\[edit\\]', expand=False).ffill()), False] ,\n",
    "                   \n",
    "               A pair which has line and Marker Flag sol 2 line 2\n",
    "                   \n",
    "                   [df['Region Name'] = df['Region Name'].str.replace(r' \\(.+$', ''), False]\n",
    "                ]\n",
    "              ]\n",
    "             ]\n",
    "}\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### buildResultSetForEvaluation:\n",
    "\n",
    "This method is used to build marker for result which can have multiple lines.\n",
    "\n",
    "\\[Id, List of \\[line, Marked_Flag\\]\\] .. Defualt mark is False\n",
    "\n",
    "\n",
    "#### markTheLines\n",
    "\n",
    "This method iterate over the lines of result and if that line is present in the tagged solution dataset for corresponding answer, then mark the line in result and tagged dataset dictionary as True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildTaggedDatasetDSForEvaluation(tagged_dataset_df):\n",
    "    taggedDSEvalDict = dict()\n",
    "    \n",
    "    for index, row in tagged_dataset_df.iterrows():\n",
    "        try:\n",
    "            Id = row['AnswerId']\n",
    "            if Id != 0:\n",
    "\n",
    "                if Id in taggedDSEvalDict:\n",
    "                    solutionList = taggedDSEvalDict[Id]\n",
    "                else:\n",
    "                    solutionList = list()                    \n",
    "                lineList = list()\n",
    "                lines = row['Solution'].split('\\n')\n",
    "                for line in lines:\n",
    "                    if line.strip() != '':\n",
    "                        pair = [line, False]\n",
    "                        lineList.append(pair)\n",
    "                solutionList.append(lineList)\n",
    "                taggedDSEvalDict[Id]= solutionList\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return taggedDSEvalDict\n",
    "\n",
    "\n",
    "def buildResultSetForEvaluation(result_df):\n",
    "    resultSetDict = dict()\n",
    "    \n",
    "    for index, row in result_df.iterrows():\n",
    "        try:\n",
    "            ansId = row['Id']\n",
    "            solutionLinesTuple = list()                    \n",
    "            predictedLines = row['Solution']\n",
    "            for line in predictedLines:\n",
    "                if line.strip() != '':\n",
    "                    pair = [line, False]\n",
    "                    solutionLinesTuple.append(pair)\n",
    "            resultSetDict[ansId]= solutionLinesTuple\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(ansId, e)\n",
    "    return resultSetDict\n",
    "\n",
    "def markTheLines(taggedDSEvalDict, resultSetDict):\n",
    "    for Id, pairs in resultSetDict.items():\n",
    "        try:\n",
    "            solutionList = taggedDSEvalDict[Id] # Error\n",
    "            for respair in pairs:\n",
    "                for solution in solutionList:\n",
    "                    for tagpair in solution:\n",
    "                        if tagpair[0].strip() == respair[0].strip():\n",
    "                            tagpair[1] = True\n",
    "                            respair[1] = True\n",
    "        except KeyError:\n",
    "            pass # Ignore it for now\n",
    "    return taggedDSEvalDict, resultSetDict\n",
    "\n",
    "def outputResultAsCSV(taggedDSEvalDict, resultSetDict):\n",
    "    COLUMN_NAMES = ['Id', 'TP', 'FP', 'Solution', 'FN']\n",
    "    row_id = 0\n",
    "    output_df = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "    for Id, pairs in resultSetDict.items():\n",
    "        try:\n",
    "            mps = ''\n",
    "            umps = ''\n",
    "            for respair in pairs:\n",
    "                if respair[1]:\n",
    "                    mps = mps + respair[0].strip() +  '\\n'\n",
    "                else:\n",
    "                    umps = umps + respair[0].strip() +  '\\n'\n",
    "                    \n",
    "            solutionList = taggedDSEvalDict[Id] # Error\n",
    "            for solution in solutionList:\n",
    "                mas = ''\n",
    "                umas = ''\n",
    "                for tagpair in solution:\n",
    "                    if tagpair[1]:\n",
    "                        mas = mas + tagpair[0].strip() +  '\\n'\n",
    "                    else:\n",
    "                        umas = umas + tagpair[0].strip() +  '\\n'\n",
    "                output_df.loc[row_id] = [Id, mps, umps, mas, umas]\n",
    "                row_id = row_id + 1\n",
    "        except KeyError:\n",
    "            pass # Ignore it for now\n",
    "    return output_df\n",
    "\n",
    "\n",
    "taggedDSEvalDict =  buildTaggedDatasetDSForEvaluation(dataset)\n",
    "resultSetDict = buildResultSetForEvaluation(result_df)\n",
    "taggedDSEvalDict, resultSetDict = markTheLines(taggedDSEvalDict, resultSetDict)\n",
    "output_df = outputResultAsCSV(taggedDSEvalDict, resultSetDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation method\n",
    "\n",
    "]1. Check for all the lines in Predicted solution. If any of the line is marked False, means we are identifying few nonsolution lines as solution. Marking such cases as FP.\n",
    "\n",
    "For this defined a boolean variable MarkedPredicted (MP) which takes the AND of all lines of predicted solution. If all True then proceed to check the tagged dataset for actual solution, otherwise increase FP count for that Id.\n",
    "\n",
    "]2. If all the predicted lines are part of solution that means all marked as true, MP will be true.\n",
    "\n",
    "Check alternative solutions:\n",
    "\n",
    "For each solution for a particular answer, again defined MAS, MarkedActualSubsolution which is again 'AND' of all the lines of a specific solution. This means that all the lines in any alternative solution must be predicted by our system.\n",
    "\n",
    "Then we take 'OR'  of all the MAS as MarkedActual(MA) which says that if any of the actual solution is identified fully, we got a correct case and increase TP else again increase FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(result_df):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    \n",
    "    for Id, pairs in resultSetDict.items():\n",
    "        try:\n",
    "            MP = True\n",
    "            for pair in pairs:\n",
    "                MP = MP and pair[1]\n",
    "            if MP:\n",
    "                MA = False\n",
    "\n",
    "                solutions = taggedDSEvalDict[Id]\n",
    "                for solution in solutions:\n",
    "                    MAS = True\n",
    "                    for tagpair in solution:\n",
    "                        MAS = MAS and tagpair[1]\n",
    "                    MA = MA or MAS\n",
    "                if MA:\n",
    "                    TP = TP + 1\n",
    "                else:\n",
    "                    FP = FP + 1\n",
    "\n",
    "            else:\n",
    "                FP = FP + 1\n",
    "        except KeyError:\n",
    "            pass # Ignore it for now\n",
    "            \n",
    "    return {'precision': float(TP)/(TP+FP) }\n",
    "\n",
    "precision = evaluate(result_df)\n",
    "# precision = precision * 100\n",
    "# print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "with open(output_file, 'wb') as handle:\n",
    "    pickle.dump(precision, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# debug = True\n",
    "if debug:\n",
    "    with open(output_file + '.txt', 'w') as f:\n",
    "        f.write(json.dumps(precision))\n",
    "    output_df.to_csv(output_file + \".csv\", encoding='ISO-8859-1', sep=\",\", doublequote=True, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
