;AnswerCount;CommentCount;Id;ParentId;PostTypeId;Score;Tags;Title;Body;ViewCount;Code;AllCode
0;2.0;7;7776679;;1;25;<python><pandas>;append two data frame with pandas;"<p>When I try to merge two dataframes by rows doing:</p>

<pre><code>bigdata = data1.append(data2)
</code></pre>

<p>I get the following error:</p>

<blockquote>
<pre><code>Exception: Index cannot contain duplicate values!
</code></pre>
</blockquote>

<p>The index of the first data frame starts from 0 to 38 and the second one from 0 to 48. I didn't understand that I have to modify the index of one of the data frame before merging, but I don't know how to.</p>

<p>Thank you.</p>

<p>These are the two dataframes:</p>

<p><code>data1</code>:</p>

<pre><code>    meta  particle  ratio   area    type    
0   2     part10    1.348   0.8365  touching
1   2     part18    1.558   0.8244  single  
2   2     part2     1.893   0.894   single  
3   2     part37    0.6695  1.005   single  
....clip...
36  2     part23    1.051   0.8781  single  
37  2     part3     80.54   0.9714  nuclei  
38  2     part34    1.071   0.9337  single  
</code></pre>

<p><code>data2</code>:</p>

<pre><code>    meta  particle  ratio    area    type    
0   3     part10    0.4756   1.025   single  
1   3     part18    0.04387  1.232   dusts   
2   3     part2     1.132    0.8927  single  
...clip...
46  3     part46    13.71    1.001   nuclei  
47  3     part3     0.7439   0.9038  single  
48  3     part34    0.4349   0.9956  single 
</code></pre>

<p>the first column is the index</p>
";25479.0;['bigdata = data1.append(data2)\n', 'Exception: Index cannot contain duplicate values!\n', '    meta  particle  ratio   area    type    \n0   2     part10    1.348   0.8365  touching\n1   2     part18    1.558   0.8244  single  \n2   2     part2     1.893   0.894   single  \n3   2     part37    0.6695  1.005   single  \n....clip...\n36  2     part23    1.051   0.8781  single  \n37  2     part3     80.54   0.9714  nuclei  \n38  2     part34    1.071   0.9337  single  \n', '    meta  particle  ratio    area    type    \n0   3     part10    0.4756   1.025   single  \n1   3     part18    0.04387  1.232   dusts   \n2   3     part2     1.132    0.8927  single  \n...clip...\n46  3     part46    13.71    1.001   nuclei  \n47  3     part3     0.7439   0.9038  single  \n48  3     part34    0.4349   0.9956  single \n'];['bigdata = data1.append(data2)\n', 'Exception: Index cannot contain duplicate values!\n', 'data1', '    meta  particle  ratio   area    type    \n0   2     part10    1.348   0.8365  touching\n1   2     part18    1.558   0.8244  single  \n2   2     part2     1.893   0.894   single  \n3   2     part37    0.6695  1.005   single  \n....clip...\n36  2     part23    1.051   0.8781  single  \n37  2     part3     80.54   0.9714  nuclei  \n38  2     part34    1.071   0.9337  single  \n', 'data2', '    meta  particle  ratio    area    type    \n0   3     part10    0.4756   1.025   single  \n1   3     part18    0.04387  1.232   dusts   \n2   3     part2     1.132    0.8927  single  \n...clip...\n46  3     part46    13.71    1.001   nuclei  \n47  3     part3     0.7439   0.9038  single  \n48  3     part34    0.4349   0.9956  single \n']
1;7.0;3;7837722;;1;187;<python><performance><for-loop><pandas>;What is the most efficient way to loop through dataframes with pandas?;"<p>I want to perform my own complex operations on financial data in dataframes in a sequential manner.</p>

<p>For example I am using the following MSFT CSV file taken from <a href=""http://finance.yahoo.com/q/hp?s=MSFT"">Yahoo Finance</a>:</p>

<pre><code>Date,Open,High,Low,Close,Volume,Adj Close
2011-10-19,27.37,27.47,27.01,27.13,42880000,27.13
2011-10-18,26.94,27.40,26.80,27.31,52487900,27.31
2011-10-17,27.11,27.42,26.85,26.98,39433400,26.98
2011-10-14,27.31,27.50,27.02,27.27,50947700,27.27

....
</code></pre>

<p>I then do the following:</p>

<pre><code>#!/usr/bin/env python
from pandas import *

df = read_csv('table.csv')

for i, row in enumerate(df.values):
    date = df.index[i]
    open, high, low, close, adjclose = row
    #now perform analysis on open/close based on date, etc..
</code></pre>

<p>Is that the most efficient way? Given the focus on speed in pandas, I would assume there must be some special function to iterate through the  values in a manner that one also retrieves the index (possibly through a generator to be memory efficient)? <code>df.iteritems</code> unfortunately only iterates column by column.</p>
";195364.0;"['Date,Open,High,Low,Close,Volume,Adj Close\n2011-10-19,27.37,27.47,27.01,27.13,42880000,27.13\n2011-10-18,26.94,27.40,26.80,27.31,52487900,27.31\n2011-10-17,27.11,27.42,26.85,26.98,39433400,26.98\n2011-10-14,27.31,27.50,27.02,27.27,50947700,27.27\n\n....\n', ""#!/usr/bin/env python\nfrom pandas import *\n\ndf = read_csv('table.csv')\n\nfor i, row in enumerate(df.values):\n    date = df.index[i]\n    open, high, low, close, adjclose = row\n    #now perform analysis on open/close based on date, etc..\n""]";"['Date,Open,High,Low,Close,Volume,Adj Close\n2011-10-19,27.37,27.47,27.01,27.13,42880000,27.13\n2011-10-18,26.94,27.40,26.80,27.31,52487900,27.31\n2011-10-17,27.11,27.42,26.85,26.98,39433400,26.98\n2011-10-14,27.31,27.50,27.02,27.27,50947700,27.27\n\n....\n', ""#!/usr/bin/env python\nfrom pandas import *\n\ndf = read_csv('table.csv')\n\nfor i, row in enumerate(df.values):\n    date = df.index[i]\n    open, high, low, close, adjclose = row\n    #now perform analysis on open/close based on date, etc..\n"", 'df.iteritems']"
2;2.0;0;8916302;;1;26;<python><csv><numpy><tab-delimited><pandas>;selecting across multiple columns with python pandas?;"<p>I have a dataframe <code>df</code> in pandas that was built using <code>pandas.read_table</code> from a csv file. The dataframe has several columns and it is indexed by one of the columns (which is unique, in that each row has a unique value for that column used for indexing.) </p>

<p>How can I select rows of my dataframe based on a ""complex"" filter applied to multiple columns? I can easily select out the slice of the dataframe where column <code>colA</code> is greater than 10 for example:</p>

<pre><code>df_greater_than10 = df[df[""colA""] &gt; 10]
</code></pre>

<p>But what if I wanted a filter like: select the slice of <code>df</code> where <em>any</em> of the columns are greater than 10? </p>

<p>Or where the value for <code>colA</code> is greater than 10 but the value for <code>colB</code> is less than 5?</p>

<p>How are these implemented in pandas?
Thanks.</p>
";19805.0;"['df_greater_than10 = df[df[""colA""] > 10]\n']";"['df', 'pandas.read_table', 'colA', 'df_greater_than10 = df[df[""colA""] > 10]\n', 'df', 'colA', 'colB']"
3;3.0;16;8991709;;1;134;<python><r><join><data.table><pandas>;Why are pandas merges in python faster than data.table merges in R?;"<p>I recently came across the <a href=""http://pandas.sourceforge.net/"" rel=""noreferrer"">pandas</a> library for python, which according to <a href=""http://wesmckinney.com/blog/some-pandas-database-join-merge-benchmarks-vs-r-basemerge/"" rel=""noreferrer"">this benchmark</a> performs very fast in-memory merges.  It's even faster than the <a href=""http://cran.r-project.org/web/packages/data.table/index.html"" rel=""noreferrer"">data.table</a> package in R (my language of choice for analysis).</p>

<p>Why is <code>pandas</code> so much faster than <code>data.table</code>?  Is it because of an inherent speed advantage python has over R, or is there some tradeoff I'm not aware of?  Is there a way to perform inner and outer joins in <code>data.table</code> without resorting to <code>merge(X, Y, all=FALSE)</code> and <code>merge(X, Y, all=TRUE)</code>?</p>

<p><img src=""https://i.stack.imgur.com/0pCvh.png"" alt=""Comparison""></p>

<p>Here's the <a href=""https://github.com/wesm/pandas/blob/master/bench/bench_merge.R"" rel=""noreferrer"">R code</a> and the <a href=""https://github.com/wesm/pandas/blob/master/bench/bench_merge.py"" rel=""noreferrer"">Python code</a> used to benchmark the various packages.</p>
";16173.0;[];['pandas', 'data.table', 'data.table', 'merge(X, Y, all=FALSE)', 'merge(X, Y, all=TRUE)']
4;2.0;2;9588331;;1;21;<python><pandas>;Simple cross-tabulation in pandas;"<p>I stumbled across <a href=""http://pandas.pydata.org/"" rel=""noreferrer"">pandas</a> and it looks ideal for simple calculations that I'd like to do. I have a SAS background and was thinking it'd replace proc freq -- it looks like it'll scale to what I may want to do in the future. However, I just can't seem to get my head around a simple task (I'm not sure if I'm supposed to look at <code>pivot/crosstab/indexing</code> - whether I should have a <code>Panel</code> or <code>DataFrames</code> etc...). Could someone give me some pointers on how to do the following:</p>

<p>I have two CSV files (one for year 2010, one for year 2011 - simple transactional data) - The columns are category and amount</p>

<p>2010:</p>

<pre><code>AB,100.00
AB,200.00
AC,150.00
AD,500.00
</code></pre>

<p>2011:</p>

<pre><code>AB,500.00
AC,250.00
AX,900.00
</code></pre>

<p>These are loaded into separate DataFrame objects.</p>

<p>What I'd like to do is get the category, the sum of the category, and the frequency of the category, eg:</p>

<p>2010:</p>

<pre><code>AB,300.00,2
AC,150.00,1
AD,500.00,1
</code></pre>

<p>2011:</p>

<pre><code>AB,500.00,1
AC,250.00,1
AX,900.00,1
</code></pre>

<p>I can't work out whether I should be using <code>pivot/crosstab/groupby/an index</code>
etc... I can get either the sum or the frequency - I can't seem to get both... It gets a bit more complex because I would like to do it on a month by month basis, but I think if someone would be so kind to point me to the right technique/direction I'll be able to go from there.</p>
";9493.0;['AB,100.00\nAB,200.00\nAC,150.00\nAD,500.00\n', 'AB,500.00\nAC,250.00\nAX,900.00\n', 'AB,300.00,2\nAC,150.00,1\nAD,500.00,1\n', 'AB,500.00,1\nAC,250.00,1\nAX,900.00,1\n'];['pivot/crosstab/indexing', 'Panel', 'DataFrames', 'AB,100.00\nAB,200.00\nAC,150.00\nAD,500.00\n', 'AB,500.00\nAC,250.00\nAX,900.00\n', 'AB,300.00,2\nAC,150.00,1\nAD,500.00,1\n', 'AB,500.00,1\nAC,250.00,1\nAX,900.00,1\n', 'pivot/crosstab/groupby/an index']
5;3.0;0;9652832;;1;38;<python><pandas><tsv>;How to I load a tsv file into a Pandas DataFrame?;"<p>I'm new to python and pandas.  I'm trying to get a <code>tsv</code> file loaded into a pandas <code>DataFrame</code>.  </p>

<p>This is what I'm trying and the error I'm getting:</p>

<pre><code>&gt;&gt;&gt; df1 = DataFrame(csv.reader(open('c:/~/trainSetRel3.txt'), delimiter='\t'))

Traceback (most recent call last):
  File ""&lt;pyshell#28&gt;"", line 1, in &lt;module&gt;
    df1 = DataFrame(csv.reader(open('c:/~/trainSetRel3.txt'), delimiter='\t'))
  File ""C:\Python27\lib\site-packages\pandas\core\frame.py"", line 318, in __init__
    raise PandasError('DataFrame constructor not properly called!')
PandasError: DataFrame constructor not properly called!
</code></pre>
";32040.0;"['>>> df1 = DataFrame(csv.reader(open(\'c:/~/trainSetRel3.txt\'), delimiter=\'\\t\'))\n\nTraceback (most recent call last):\n  File ""<pyshell#28>"", line 1, in <module>\n    df1 = DataFrame(csv.reader(open(\'c:/~/trainSetRel3.txt\'), delimiter=\'\\t\'))\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 318, in __init__\n    raise PandasError(\'DataFrame constructor not properly called!\')\nPandasError: DataFrame constructor not properly called!\n']";"['tsv', 'DataFrame', '>>> df1 = DataFrame(csv.reader(open(\'c:/~/trainSetRel3.txt\'), delimiter=\'\\t\'))\n\nTraceback (most recent call last):\n  File ""<pyshell#28>"", line 1, in <module>\n    df1 = DataFrame(csv.reader(open(\'c:/~/trainSetRel3.txt\'), delimiter=\'\\t\'))\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 318, in __init__\n    raise PandasError(\'DataFrame constructor not properly called!\')\nPandasError: DataFrame constructor not properly called!\n']"
6;6.0;0;9758450;;1;34;<python><pandas>;Pandas convert dataframe to array of tuples;"<p>I have manipulated some data using pandas and now I want to carry out a batch save back to the database. This requires me to convert the dataframe into an array of tuples, with each tuple corresponding to a ""row"" of the dataframe.</p>

<p>My DataFrame looks something like:</p>

<pre><code>In [182]: data_set
Out[182]: 
  index data_date   data_1  data_2
0  14303 2012-02-17  24.75   25.03 
1  12009 2012-02-16  25.00   25.07 
2  11830 2012-02-15  24.99   25.15 
3  6274  2012-02-14  24.68   25.05 
4  2302  2012-02-13  24.62   24.77 
5  14085 2012-02-10  24.38   24.61 
</code></pre>

<p>I want to convert it to an array of tuples like:</p>

<pre><code>[(datetime.date(2012,2,17),24.75,25.03),
(datetime.date(2012,2,16),25.00,25.07),
...etc. ]
</code></pre>

<p>Any suggestion on how I can efficiently do this?</p>
";36433.0;['In [182]: data_set\nOut[182]: \n  index data_date   data_1  data_2\n0  14303 2012-02-17  24.75   25.03 \n1  12009 2012-02-16  25.00   25.07 \n2  11830 2012-02-15  24.99   25.15 \n3  6274  2012-02-14  24.68   25.05 \n4  2302  2012-02-13  24.62   24.77 \n5  14085 2012-02-10  24.38   24.61 \n', '[(datetime.date(2012,2,17),24.75,25.03),\n(datetime.date(2012,2,16),25.00,25.07),\n...etc. ]\n'];['In [182]: data_set\nOut[182]: \n  index data_date   data_1  data_2\n0  14303 2012-02-17  24.75   25.03 \n1  12009 2012-02-16  25.00   25.07 \n2  11830 2012-02-15  24.99   25.15 \n3  6274  2012-02-14  24.68   25.05 \n4  2302  2012-02-13  24.62   24.77 \n5  14085 2012-02-10  24.38   24.61 \n', '[(datetime.date(2012,2,17),24.75,25.03),\n(datetime.date(2012,2,16),25.00,25.07),\n...etc. ]\n']
7;11.0;4;10065051;;1;77;<python><pandas>;python-pandas and databases like mysql;"<p>The documentation for Pandas has numerous examples of best practices for working with data stored in various formats.</p>

<p>However, I am unable to find any good examples for working with databases like MySQL for example.</p>

<p>Can anyone point me to links or give some code snippets of how to convert query results using <strong>mysql-python</strong> to data frames in Panda efficiently ?</p>
";72432.0;[];[]
8;4.0;2;10202570;;1;84;<python><pandas>;Pandas DataFrame - Find row where values for column is maximal;"<p>How can I find the row for which the value of a specific column is <strong>maximal</strong>?</p>

<p><code>df.max()</code> will give me the maximal value for each column, I don't know how to get the corresponding row.</p>
";75652.0;[];['df.max()']
9;6.0;0;10373660;;1;173;<python><pandas><dataframe><group-by><multi-index>;Converting a Pandas GroupBy object to DataFrame;"<p>I'm starting with input data like this</p>

<pre><code>df1 = pandas.DataFrame( { 
    ""Name"" : [""Alice"", ""Bob"", ""Mallory"", ""Mallory"", ""Bob"" , ""Mallory""] , 
    ""City"" : [""Seattle"", ""Seattle"", ""Portland"", ""Seattle"", ""Seattle"", ""Portland""] } )
</code></pre>

<p>Which when printed appears as this:</p>

<pre><code>   City     Name
0   Seattle    Alice
1   Seattle      Bob
2  Portland  Mallory
3   Seattle  Mallory
4   Seattle      Bob
5  Portland  Mallory
</code></pre>

<p>Grouping is simple enough:</p>

<pre><code>g1 = df1.groupby( [ ""Name"", ""City""] ).count()
</code></pre>

<p>and printing yields a <code>GroupBy</code> object:</p>

<pre><code>                  City  Name
Name    City
Alice   Seattle      1     1
Bob     Seattle      2     2
Mallory Portland     2     2
        Seattle      1     1
</code></pre>

<p>But what I want eventually is another DataFrame object that contains all the rows in the GroupBy object. In other words I want to get the following result:</p>

<pre><code>                  City  Name
Name    City
Alice   Seattle      1     1
Bob     Seattle      2     2
Mallory Portland     2     2
Mallory Seattle      1     1
</code></pre>

<p>I can't quite see how to accomplish this in the pandas documentation. Any hints would be welcome.</p>
";167415.0;"['df1 = pandas.DataFrame( { \n    ""Name"" : [""Alice"", ""Bob"", ""Mallory"", ""Mallory"", ""Bob"" , ""Mallory""] , \n    ""City"" : [""Seattle"", ""Seattle"", ""Portland"", ""Seattle"", ""Seattle"", ""Portland""] } )\n', '   City     Name\n0   Seattle    Alice\n1   Seattle      Bob\n2  Portland  Mallory\n3   Seattle  Mallory\n4   Seattle      Bob\n5  Portland  Mallory\n', 'g1 = df1.groupby( [ ""Name"", ""City""] ).count()\n', '                  City  Name\nName    City\nAlice   Seattle      1     1\nBob     Seattle      2     2\nMallory Portland     2     2\n        Seattle      1     1\n', '                  City  Name\nName    City\nAlice   Seattle      1     1\nBob     Seattle      2     2\nMallory Portland     2     2\nMallory Seattle      1     1\n']";"['df1 = pandas.DataFrame( { \n    ""Name"" : [""Alice"", ""Bob"", ""Mallory"", ""Mallory"", ""Bob"" , ""Mallory""] , \n    ""City"" : [""Seattle"", ""Seattle"", ""Portland"", ""Seattle"", ""Seattle"", ""Portland""] } )\n', '   City     Name\n0   Seattle    Alice\n1   Seattle      Bob\n2  Portland  Mallory\n3   Seattle  Mallory\n4   Seattle      Bob\n5  Portland  Mallory\n', 'g1 = df1.groupby( [ ""Name"", ""City""] ).count()\n', 'GroupBy', '                  City  Name\nName    City\nAlice   Seattle      1     1\nBob     Seattle      2     2\nMallory Portland     2     2\n        Seattle      1     1\n', '                  City  Name\nName    City\nAlice   Seattle      1     1\nBob     Seattle      2     2\nMallory Portland     2     2\nMallory Seattle      1     1\n']"
10;2.0;1;10457584;;1;68;<python><pandas>;Redefining the Index in a Pandas DataFrame object;"<p>I am trying to re-index a pandas <code>DataFrame</code> object, like so,</p>

<pre><code>From:
            a   b   c
        0   1   2   3
        1  10  11  12
        2  20  21  22

To :
           b   c
       1   2   3
      10  11  12
      20  21  22
</code></pre>

<p>I am going about this as shown below and am getting the wrong answer. Any clues on how to do this?</p>

<pre><code>&gt;&gt;&gt; col = ['a','b','c']
&gt;&gt;&gt; data = DataFrame([[1,2,3],[10,11,12],[20,21,22]],columns=col)
&gt;&gt;&gt; data
    a   b   c
0   1   2   3
1  10  11  12
2  20  21  22
&gt;&gt;&gt; idx2 = data.a.values
&gt;&gt;&gt; idx2
array([ 1, 10, 20], dtype=int64)
&gt;&gt;&gt; data2 = DataFrame(data,index=idx2,columns=col[1:])
&gt;&gt;&gt; data2
     b   c
1   11  12
10 NaN NaN
20 NaN NaN
</code></pre>

<p>Any idea why this is happening?</p>
";107691.0;"['From:\n            a   b   c\n        0   1   2   3\n        1  10  11  12\n        2  20  21  22\n\nTo :\n           b   c\n       1   2   3\n      10  11  12\n      20  21  22\n', "">>> col = ['a','b','c']\n>>> data = DataFrame([[1,2,3],[10,11,12],[20,21,22]],columns=col)\n>>> data\n    a   b   c\n0   1   2   3\n1  10  11  12\n2  20  21  22\n>>> idx2 = data.a.values\n>>> idx2\narray([ 1, 10, 20], dtype=int64)\n>>> data2 = DataFrame(data,index=idx2,columns=col[1:])\n>>> data2\n     b   c\n1   11  12\n10 NaN NaN\n20 NaN NaN\n""]";"['DataFrame', 'From:\n            a   b   c\n        0   1   2   3\n        1  10  11  12\n        2  20  21  22\n\nTo :\n           b   c\n       1   2   3\n      10  11  12\n      20  21  22\n', "">>> col = ['a','b','c']\n>>> data = DataFrame([[1,2,3],[10,11,12],[20,21,22]],columns=col)\n>>> data\n    a   b   c\n0   1   2   3\n1  10  11  12\n2  20  21  22\n>>> idx2 = data.a.values\n>>> idx2\narray([ 1, 10, 20], dtype=int64)\n>>> data2 = DataFrame(data,index=idx2,columns=col[1:])\n>>> data2\n     b   c\n1   11  12\n10 NaN NaN\n20 NaN NaN\n""]"
11;2.0;0;10464738;;1;23;<python><pandas>;Interpolation on DataFrame in pandas;"<p>I have a DataFrame, say a volatility surface with index as time and column as strike. How do I do two dimensional interpolation? I can <code>reindex</code> but how do i deal with <code>NaN</code>? I know we can <code>fillna(method='pad')</code> but it is not even linear interpolation. Is there a way we can plug in our own method to do interpolation?</p>
";15524.0;[];"['reindex', 'NaN', ""fillna(method='pad')""]"
12;6.0;3;10511024;;1;64;<python><ipython><pandas>;in Ipython notebook, Pandas is not displying the graph I try to plot;"<p>I am trying to plot some data using pandas in Ipython Notebook, and while it gives me the object, it doesn't actually plot the graph itself. So it looks like this:</p>

<pre><code>In [7]:

pledge.Amount.plot()

Out[7]:

&lt;matplotlib.axes.AxesSubplot at 0x9397c6c&gt;
</code></pre>

<p>The graph should follow after that, but it simply doesn't appear. I have imported matplotlib, so that's not the problem. Is there any other module I need to import ?</p>
";44369.0;['In [7]:\n\npledge.Amount.plot()\n\nOut[7]:\n\n<matplotlib.axes.AxesSubplot at 0x9397c6c>\n'];['In [7]:\n\npledge.Amount.plot()\n\nOut[7]:\n\n<matplotlib.axes.AxesSubplot at 0x9397c6c>\n']
13;2.0;2;10591000;;1;25;<python><pandas>;Specifying data type in Pandas csv reader;"<p>I am just getting started with Pandas and I am reading in a csv file using the <code>read_csv()</code> method. The difficulty I am having is preventing pandas from converting my telephone numbers to large numbers, instead of keeping them as strings. I defined a converter which just left the numbers alone, but then they still converted to numbers. When I changed my converter to prepend a 'z' to the phone numbers, then they stayed strings. Is there some way to keep them strings without modifying the values of the fields?</p>
";17261.0;[];['read_csv()']
14;14.0;5;10636024;;1;33;<python><user-interface><pandas><dataframe>;Python / Pandas - GUI for viewing a DataFrame or Matrix;"<p>I'm using the Pandas package and it creates a DataFrame object, which is basically a labeled matrix. Often I have columns that have long string fields, or dataframes with many columns, so the simple print command doesn't work well. I've written some text output functions, but they aren't great.</p>

<p>What I'd really love is a simple GUI that lets me interact with a dataframe / matrix / table. Just like you would find in a SQL tool. Basically a window that has a read-only spreadsheet like view into the data. I can expand columns, page up and down through long tables, etc.</p>

<p>I would suspect something like this exists, but I must be Googling with the wrong terms. It would be great if it is pandas specific, but I would guess I could use any matrix-accepting tool. (BTW - I'm on Windows.)</p>

<p>Any pointers?</p>

<p>Or, conversely, if someone knows this space well and knows this probably doesn't exist, any suggestions on if there is a simple GUI framework / widget I could use to roll my own? (But since my needs are limited, I'm reluctant to have to learn a big GUI framework and do a bunch of coding for this one piece.)</p>
";26216.0;[];[]
15;7.0;3;10665889;;1;126;<python><numpy><pandas><slice>;How to take column-slices of dataframe in pandas;"<p>I load a some machine learning data from a csv file. The first 2 columns are observations and the remaining columns are features.</p>

<p>Currently, I do the following :</p>

<pre><code>data = pandas.read_csv('mydata.csv')
</code></pre>

<p>which gives something like:</p>

<pre><code>data = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))
</code></pre>

<p>I'd like to slice this dataframe in two dataframes: one containing the columns <code>a</code> and <code>b</code> and one containing the columns <code>c</code>, <code>d</code> and <code>e</code>.</p>

<p>It is not possible to write something like </p>

<pre><code>observations = data[:'c']
features = data['c':]
</code></pre>

<p>I'm not sure what the best method is. Do I need a <code>pd.Panel</code>?</p>

<p>By the way, I find dataframe indexing pretty inconsistent: <code>data['a']</code> is permitted, but <code>data[0]</code> is not. On the other side, <code>data['a':]</code> is not permitted but <code>data[0:]</code> is.
Is there a practical reason for this? This is really confusing if columns are indexed by Int, given that <code>data[0] != data[0:1]</code></p>
";154109.0;"[""data = pandas.read_csv('mydata.csv')\n"", ""data = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))\n"", ""observations = data[:'c']\nfeatures = data['c':]\n""]";"[""data = pandas.read_csv('mydata.csv')\n"", ""data = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))\n"", 'a', 'b', 'c', 'd', 'e', ""observations = data[:'c']\nfeatures = data['c':]\n"", 'pd.Panel', ""data['a']"", 'data[0]', ""data['a':]"", 'data[0:]', 'data[0] != data[0:1]']"
16;13.0;3;10715965;;1;312;<python><pandas>;add one row in a pandas.DataFrame;"<p>I understand that pandas is designed to load fully populated <code>DataFrame</code> but I need to <strong>create an empty DataFrame then add rows, one by one</strong>.
What is the best way to do this ?</p>

<p>I successfully created an empty DataFrame with :</p>

<pre><code>res = DataFrame(columns=('lib', 'qty1', 'qty2'))
</code></pre>

<p>Then I can add a new row and fill a field with :</p>

<pre><code>res = res.set_value(len(res), 'qty1', 10.0)
</code></pre>

<p>It works but seems very odd :-/ (it fails for adding string value)</p>

<p>How can I add a new row to my DataFrame (with different columns type) ?</p>
";349935.0;"[""res = DataFrame(columns=('lib', 'qty1', 'qty2'))\n"", ""res = res.set_value(len(res), 'qty1', 10.0)\n""]";"['DataFrame', ""res = DataFrame(columns=('lib', 'qty1', 'qty2'))\n"", ""res = res.set_value(len(res), 'qty1', 10.0)\n""]"
17;1.0;2;10729210;;1;122;<python><pandas>;iterating row by row through a pandas dataframe;"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/7837722/what-is-the-most-efficient-way-to-loop-through-dataframes-with-pandas"">What is the most efficient way to loop through dataframes with pandas?</a>  </p>
</blockquote>



<p>I'm looking to iterate row by row through a pandas <code>DataFrame</code>.  The way I'm doing it so far is as follows:</p>

<pre><code>for i in df.index:
    do_something(df.ix[i])
</code></pre>

<p>Is there a more performant and/or more idiomatic way to do this?  I know about apply, but sometimes it's more convenient to use a for loop.  Thanks in advance.</p>
";174898.0;['for i in df.index:\n    do_something(df.ix[i])\n'];['DataFrame', 'for i in df.index:\n    do_something(df.ix[i])\n']
18;1.0;2;10751127;;1;41;<python><pandas>;Returning multiple values from pandas apply on a DataFrame;"<p>I'm using a Pandas <code>DataFrame</code> to do a row-wise t-test as per this example:</p>

<pre><code>import numpy
import pandas

df = pandas.DataFrame(numpy.log2(numpy.randn(1000, 4), 
                      columns=[""a"", ""b"", ""c"", ""d""])

df = df.dropna()
</code></pre>

<p>Now, supposing I have ""a"" and ""b"" as one group, and ""c"" and ""d"" at the other, I'm performing the t-test row-wise. This is fairly trivial with pandas, using <code>apply</code> with axis=1. However, I can either return a DataFrame of the same shape if my function doesn't aggregate, or a Series if it aggregates.</p>

<p>Normally I would just output the p-value (so, aggregation) but I would like to generate an additional value based on other calculations (in other words, return two values). I can of course do two runs, aggregating the p-values first, then doing the other work, but I was wondering if there is a more efficient way to do so as the data is reasonably large.</p>

<p>As an example of the calculation, a hypotethical function would be:</p>

<pre><code>from scipy.stats import ttest_ind

def t_test_and_mean(series, first, second):
    first_group = series[first]
    second_group = series[second]
    _, pvalue = ttest_ind(first_group, second_group)

    mean_ratio = second_group.mean() / first_group.mean()

    return (pvalue, mean_ratio)
</code></pre>

<p>Then invoked with </p>

<pre><code>df.apply(t_test_and_mean, first=[""a"", ""b""], second=[""c"", ""d""], axis=1)
</code></pre>

<p>Of course in this case it returns a single Series with the two tuples as value.</p>

<p>Instead, ny expected output would be a DataFrame with two columns, one for the first result, and one for the second. Is this possible or I have to do two runs for the two calculations, then merge them together?</p>
";19415.0;"['import numpy\nimport pandas\n\ndf = pandas.DataFrame(numpy.log2(numpy.randn(1000, 4), \n                      columns=[""a"", ""b"", ""c"", ""d""])\n\ndf = df.dropna()\n', 'from scipy.stats import ttest_ind\n\ndef t_test_and_mean(series, first, second):\n    first_group = series[first]\n    second_group = series[second]\n    _, pvalue = ttest_ind(first_group, second_group)\n\n    mean_ratio = second_group.mean() / first_group.mean()\n\n    return (pvalue, mean_ratio)\n', 'df.apply(t_test_and_mean, first=[""a"", ""b""], second=[""c"", ""d""], axis=1)\n']";"['DataFrame', 'import numpy\nimport pandas\n\ndf = pandas.DataFrame(numpy.log2(numpy.randn(1000, 4), \n                      columns=[""a"", ""b"", ""c"", ""d""])\n\ndf = df.dropna()\n', 'apply', 'from scipy.stats import ttest_ind\n\ndef t_test_and_mean(series, first, second):\n    first_group = series[first]\n    second_group = series[second]\n    _, pvalue = ttest_ind(first_group, second_group)\n\n    mean_ratio = second_group.mean() / first_group.mean()\n\n    return (pvalue, mean_ratio)\n', 'df.apply(t_test_and_mean, first=[""a"", ""b""], second=[""c"", ""d""], axis=1)\n']"
19;1.0;1;10857924;;1;39;<python><pandas>;Remove NULL columns in a dataframe Pandas?;"<p>I have a <code>dataFrame</code> in pandas and several of the columns have all null values. Is there a built in function which will let me remove those columns?</p>

<p>Thank you!</p>
";33392.0;[];['dataFrame']
20;3.0;0;10867028;;1;23;<python><csv><pandas>;Get pandas.read_csv to read empty values as empty string instead of nan;"<p>I'm using the pandas library to read in some CSV data.  In my data, certain columns contain strings.  The string <code>""nan""</code> is a possible value, as is an empty string.  I managed to get pandas to read ""nan"" as a string, but I can't figure out how to get it not to read an empty value as NaN.  Here's sample data and output</p>

<pre><code>One,Two,Three
a,1,one
b,2,two
,3,three
d,4,nan
e,5,five
nan,6,
g,7,seven

&gt;&gt;&gt; pandas.read_csv('test.csv', na_values={'One': [], ""Three"": []})
    One  Two  Three
0    a    1    one
1    b    2    two
2  NaN    3  three
3    d    4    nan
4    e    5   five
5  nan    6    NaN
6    g    7  seven
</code></pre>

<p>It correctly reads ""nan"" as the string ""nan', but still reads the empty cells as NaN.  I tried passing in <code>str</code> in the <code>converters</code> argument to read_csv (with <code>converters={'One': str})</code>), but it still reads the empty cells as NaN.</p>

<p>I realize I can fill the values after reading, with fillna, but is there really no way to tell pandas that an empty cell in a particular CSV column should be read as an empty string instead of NaN?</p>
";21259.0;"['One,Two,Three\na,1,one\nb,2,two\n,3,three\nd,4,nan\ne,5,five\nnan,6,\ng,7,seven\n\n>>> pandas.read_csv(\'test.csv\', na_values={\'One\': [], ""Three"": []})\n    One  Two  Three\n0    a    1    one\n1    b    2    two\n2  NaN    3  three\n3    d    4    nan\n4    e    5   five\n5  nan    6    NaN\n6    g    7  seven\n']";"['""nan""', 'One,Two,Three\na,1,one\nb,2,two\n,3,three\nd,4,nan\ne,5,five\nnan,6,\ng,7,seven\n\n>>> pandas.read_csv(\'test.csv\', na_values={\'One\': [], ""Three"": []})\n    One  Two  Three\n0    a    1    one\n1    b    2    two\n2  NaN    3  three\n3    d    4    nan\n4    e    5   five\n5  nan    6    NaN\n6    g    7  seven\n', 'str', 'converters', ""converters={'One': str})""]"
21;5.0;0;10951341;;1;45;<python><pandas>;Pandas DataFrame aggregate function using multiple columns;"<p>Is there a way to write an aggregation function as is used in <code>DataFrame.agg</code> method, that would have access to more than one column of the data that is being aggregated? Typical use cases would be weighted average, weighted standard deviation funcs.</p>

<p>I would like to be able to write something like</p>

<pre><code>def wAvg(c, w):
    return ((c * w).sum() / w.sum())

df = DataFrame(....) # df has columns c and w, i want weighted average
                     # of c using w as weight.
df.aggregate ({""c"": wAvg}) # and somehow tell it to use w column as weights ...
</code></pre>
";17889.0;"['def wAvg(c, w):\n    return ((c * w).sum() / w.sum())\n\ndf = DataFrame(....) # df has columns c and w, i want weighted average\n                     # of c using w as weight.\ndf.aggregate ({""c"": wAvg}) # and somehow tell it to use w column as weights ...\n']";"['DataFrame.agg', 'def wAvg(c, w):\n    return ((c * w).sum() / w.sum())\n\ndf = DataFrame(....) # df has columns c and w, i want weighted average\n                     # of c using w as weight.\ndf.aggregate ({""c"": wAvg}) # and somehow tell it to use w column as weights ...\n']"
22;3.0;1;10982089;;1;38;<python><pandas><dataframe>;How to shift a column in Pandas DataFrame;"<p>I would like to shift a column in a Pandas <code>DataFrame</code>, but I haven't been able to find a method to do it from the documentation without rewriting the whole DF. Does anyone know how to do it? 
DataFrame:</p>

<pre><code>##    x1   x2
##0  206  214
##1  226  234
##2  245  253
##3  265  272
##4  283  291
</code></pre>

<p>Desired output:</p>

<pre><code>##    x1   x2
##0  206  nan
##1  226  214
##2  245  234
##3  265  253
##4  283  272
##5  nan  291
</code></pre>
";31697.0;['##    x1   x2\n##0  206  214\n##1  226  234\n##2  245  253\n##3  265  272\n##4  283  291\n', '##    x1   x2\n##0  206  nan\n##1  226  214\n##2  245  234\n##3  265  253\n##4  283  272\n##5  nan  291\n'];['DataFrame', '##    x1   x2\n##0  206  214\n##1  226  234\n##2  245  253\n##3  265  272\n##4  283  291\n', '##    x1   x2\n##0  206  nan\n##1  226  214\n##2  245  234\n##3  265  253\n##4  283  272\n##5  nan  291\n']
23;11.0;1;11067027;;1;102;<python><pandas><order>;Python Pandas - Re-ordering columns in a dataframe based on column name;"<p>I have a <code>dataframe</code> with over 200 columns (don't ask why). The issue is as they were generated the order is</p>

<pre><code>['Q1.3','Q6.1','Q1.2','Q1.1',......]
</code></pre>

<p>I need to re-order the columns as follows:</p>

<pre><code>['Q1.1','Q1.2','Q1.3',.....'Q6.1',......]
</code></pre>

<p>Is there some way for me to do this within python?</p>
";74414.0;"[""['Q1.3','Q6.1','Q1.2','Q1.1',......]\n"", ""['Q1.1','Q1.2','Q1.3',.....'Q6.1',......]\n""]";"['dataframe', ""['Q1.3','Q6.1','Q1.2','Q1.1',......]\n"", ""['Q1.1','Q1.2','Q1.3',.....'Q6.1',......]\n""]"
24;1.0;0;11073609;;1;28;<python><pandas>;How to group DataFrame by a period of time?;"<p>I have some data from log files and would like to group entries by a minute:</p>

<pre><code> def gen(date, count=10):
     while count &gt; 0:
         yield date, ""event{}"".format(randint(1,9)), ""source{}"".format(randint(1,3))
         count -= 1
         date += DateOffset(seconds=randint(40))

 df = DataFrame.from_records(list(gen(datetime(2012,1,1,12, 30))), index='Time', columns=['Time', 'Event', 'Source'])
</code></pre>

<p>df:</p>

<pre><code> Event  Source
 2012-01-01 12:30:00     event3  source1
 2012-01-01 12:30:12     event2  source2
 2012-01-01 12:30:12     event2  source2
 2012-01-01 12:30:29     event6  source1
 2012-01-01 12:30:38     event1  source1
 2012-01-01 12:31:05     event4  source2
 2012-01-01 12:31:38     event4  source1
 2012-01-01 12:31:44     event5  source1
 2012-01-01 12:31:48     event5  source2
 2012-01-01 12:32:23     event6  source1
</code></pre>

<p>I tried these options:</p>

<ol>
<li><code>df.resample('Min')</code> is too high level and wants to aggregate.</li>
<li><code>df.groupby(date_range(datetime(2012,1,1,12, 30), freq='Min',
periods=4))</code> fails with exception.</li>
<li><p><code>df.groupby(TimeGrouper(freq='Min'))</code> works fine and returns a <code>DataFrameGroupBy</code> object for further processing, e.g.:</p>

<pre><code>grouped = df.groupby(TimeGrouper(freq='Min'))
grouped.Source.value_counts()
2012-01-01 12:30:00  source1    1
2012-01-01 12:31:00  source2    2
                     source1    2
2012-01-01 12:32:00  source2    2
                     source1    2
2012-01-01 12:33:00  source1    1
</code></pre></li>
</ol>

<p><em>However</em>, the <code>TimeGrouper</code> class is not documented.</p>

<p>What is the correct way to group by a period of time? How can I group the data by a minute AND by the Source column, e.g. <code>groupby([TimeGrouper(freq='Min'), df.Source])</code>?</p>
";20945.0;"[' def gen(date, count=10):\n     while count > 0:\n         yield date, ""event{}"".format(randint(1,9)), ""source{}"".format(randint(1,3))\n         count -= 1\n         date += DateOffset(seconds=randint(40))\n\n df = DataFrame.from_records(list(gen(datetime(2012,1,1,12, 30))), index=\'Time\', columns=[\'Time\', \'Event\', \'Source\'])\n', ' Event  Source\n 2012-01-01 12:30:00     event3  source1\n 2012-01-01 12:30:12     event2  source2\n 2012-01-01 12:30:12     event2  source2\n 2012-01-01 12:30:29     event6  source1\n 2012-01-01 12:30:38     event1  source1\n 2012-01-01 12:31:05     event4  source2\n 2012-01-01 12:31:38     event4  source1\n 2012-01-01 12:31:44     event5  source1\n 2012-01-01 12:31:48     event5  source2\n 2012-01-01 12:32:23     event6  source1\n', ""grouped = df.groupby(TimeGrouper(freq='Min'))\ngrouped.Source.value_counts()\n2012-01-01 12:30:00  source1    1\n2012-01-01 12:31:00  source2    2\n                     source1    2\n2012-01-01 12:32:00  source2    2\n                     source1    2\n2012-01-01 12:33:00  source1    1\n""]";"[' def gen(date, count=10):\n     while count > 0:\n         yield date, ""event{}"".format(randint(1,9)), ""source{}"".format(randint(1,3))\n         count -= 1\n         date += DateOffset(seconds=randint(40))\n\n df = DataFrame.from_records(list(gen(datetime(2012,1,1,12, 30))), index=\'Time\', columns=[\'Time\', \'Event\', \'Source\'])\n', ' Event  Source\n 2012-01-01 12:30:00     event3  source1\n 2012-01-01 12:30:12     event2  source2\n 2012-01-01 12:30:12     event2  source2\n 2012-01-01 12:30:29     event6  source1\n 2012-01-01 12:30:38     event1  source1\n 2012-01-01 12:31:05     event4  source2\n 2012-01-01 12:31:38     event4  source1\n 2012-01-01 12:31:44     event5  source1\n 2012-01-01 12:31:48     event5  source2\n 2012-01-01 12:32:23     event6  source1\n', ""df.resample('Min')"", ""df.groupby(date_range(datetime(2012,1,1,12, 30), freq='Min',\nperiods=4))"", ""df.groupby(TimeGrouper(freq='Min'))"", 'DataFrameGroupBy', ""grouped = df.groupby(TimeGrouper(freq='Min'))\ngrouped.Source.value_counts()\n2012-01-01 12:30:00  source1    1\n2012-01-01 12:31:00  source2    2\n                     source1    2\n2012-01-01 12:32:00  source2    2\n                     source1    2\n2012-01-01 12:33:00  source1    1\n"", 'TimeGrouper', ""groupby([TimeGrouper(freq='Min'), df.Source])""]"
25;3.0;3;11077023;;1;106;<python><numpy><scipy><pandas>;What are the differences between Pandas and NumPy+SciPy in Python?;"<p>They both seem <em>exceedingly</em> similar and I'm curious as to which package would be more beneficial for financial data analysis. </p>
";54531.0;[];[]
26;4.0;1;11106823;;1;26;<python><pandas>;Adding two pandas dataframes;"<p>I have two <code>dataframes</code>, both indexed by <code>timeseries</code>.  I need to add the elements together to form a new <code>dataframe</code>, but only if the index and column are the same.  If the item does not exist in one of the <code>dataframe</code>s then it should be treated as a zero.</p>

<p>I've tried using <code>.add</code> but this sums regardless of index and column.  Also tried a simple <code>combined_data = dataframe1 + dataframe2</code> but this give a <code>NaN</code> if both dataframes don't have the element.</p>

<p>Any suggestions?</p>
";19395.0;[];['dataframes', 'timeseries', 'dataframe', 'dataframe', '.add', 'combined_data = dataframe1 + dataframe2', 'NaN']
27;4.0;0;11232275;;1;28;<python><pandas>;Pandas pivot warning about repeated entries on index;"<p>On Pandas documentation of the <code>pivot</code> method, we have:</p>

<pre><code>Examples
--------
&gt;&gt;&gt; df
    foo   bar  baz
0   one   A    1.
1   one   B    2.
2   one   C    3.
3   two   A    4.
4   two   B    5.
5   two   C    6.

&gt;&gt;&gt; df.pivot('foo', 'bar', 'baz')
     A   B   C
one  1   2   3
two  4   5   6
</code></pre>

<p>My <code>DataFrame</code> is structured like this:</p>

<pre><code>   name   id     x
----------------------
0  john   1      0
1  john   2      0
2  mike   1      1
3  mike   2      0
</code></pre>

<p>And I want something like this:</p>

<pre><code>      1    2   # (this is the id as columns)
----------------------
mike  0    0   # (and this is the 'x' as values)
john  1    0
</code></pre>

<p>But when I run the <code>pivot</code> method, it is saying:</p>

<pre><code>*** ReshapeError: Index contains duplicate entries, cannot reshape
</code></pre>

<p>Which doesn't makes sense, even in example there are repeated entries on the <code>foo</code> column. I'm using the <code>name</code> column as the index of the pivot, the first argument of the <code>pivot</code> method call.</p>
";12221.0;"[""Examples\n--------\n>>> df\n    foo   bar  baz\n0   one   A    1.\n1   one   B    2.\n2   one   C    3.\n3   two   A    4.\n4   two   B    5.\n5   two   C    6.\n\n>>> df.pivot('foo', 'bar', 'baz')\n     A   B   C\none  1   2   3\ntwo  4   5   6\n"", '   name   id     x\n----------------------\n0  john   1      0\n1  john   2      0\n2  mike   1      1\n3  mike   2      0\n', ""      1    2   # (this is the id as columns)\n----------------------\nmike  0    0   # (and this is the 'x' as values)\njohn  1    0\n"", '*** ReshapeError: Index contains duplicate entries, cannot reshape\n']";"['pivot', ""Examples\n--------\n>>> df\n    foo   bar  baz\n0   one   A    1.\n1   one   B    2.\n2   one   C    3.\n3   two   A    4.\n4   two   B    5.\n5   two   C    6.\n\n>>> df.pivot('foo', 'bar', 'baz')\n     A   B   C\none  1   2   3\ntwo  4   5   6\n"", 'DataFrame', '   name   id     x\n----------------------\n0  john   1      0\n1  john   2      0\n2  mike   1      1\n3  mike   2      0\n', ""      1    2   # (this is the id as columns)\n----------------------\nmike  0    0   # (and this is the 'x' as values)\njohn  1    0\n"", 'pivot', '*** ReshapeError: Index contains duplicate entries, cannot reshape\n', 'foo', 'name', 'pivot']"
28;7.0;4;11285613;;1;280;<python><pandas>;Selecting columns;"<p>I have data in different columns but I don't know how to extract it to save it in another variable.</p>

<pre><code>index  a   b   c
1      2   3   4
2      3   4   5
</code></pre>

<p>How do I select <code>'b'</code>, <code>'c'</code> and save it in to df1?</p>

<p>I tried </p>

<pre><code>df1 = df['a':'b']
df1 = df.ix[:, 'a':'b']
</code></pre>

<p>None seem to work. Any ideas would help thanks.</p>
";413954.0;"['index  a   b   c\n1      2   3   4\n2      3   4   5\n', ""df1 = df['a':'b']\ndf1 = df.ix[:, 'a':'b']\n""]";"['index  a   b   c\n1      2   3   4\n2      3   4   5\n', ""'b'"", ""'c'"", ""df1 = df['a':'b']\ndf1 = df.ix[:, 'a':'b']\n""]"
29;16.0;0;11346283;;1;750;<python><pandas><replace><dataframe><rename>;Renaming columns in pandas;"<p>I have a data table using pandas and column labels that I need to edit to replace the original column labels. </p>

<p>I'd like to change the column names in a data table <code>A</code> where the original column names are:</p>

<pre><code>['$a', '$b', '$c', '$d', '$e'] 
</code></pre>

<p>to </p>

<pre><code>['a', 'b', 'c', 'd', 'e'].
</code></pre>

<p>I have the edited column names stored it in a list, but I don't know how to replace the column names.</p>
";767316.0;"[""['$a', '$b', '$c', '$d', '$e'] \n"", ""['a', 'b', 'c', 'd', 'e'].\n""]";"['A', ""['$a', '$b', '$c', '$d', '$e'] \n"", ""['a', 'b', 'c', 'd', 'e'].\n""]"
30;6.0;0;11350770;;1;132;<python><pandas>;pandas + dataframe - select by partial string;"<p>I have a <code>DataFrame</code> with 4 columns of which 2 contain string values. I was wondering if there was a way to select rows based on a partial string match against a particular column?</p>

<p>In other words, a function or lambda function that would do something like </p>

<pre><code>re.search(pattern, cell_in_question) 
</code></pre>

<p>returning a boolean. I am familiar with the syntax of <code>df[df['A'] == ""hello world""]</code> but can't seem to find a way to do the same with a partial string match say <code>'hello'</code>.</p>

<p>Would someone be able to point me in the right direction?</p>
";130134.0;['re.search(pattern, cell_in_question) \n'];"['DataFrame', 're.search(pattern, cell_in_question) \n', 'df[df[\'A\'] == ""hello world""]', ""'hello'""]"
31;6.0;1;11361985;;1;59;<python><numpy><pandas>;Output data from all columns in a dataframe in pandas;"<p>I have a csv file with the name <code>params.csv</code>. I opened up <code>ipython qtconsole</code> and created a pandas <code>dataframe</code> using:</p>

<pre><code>import pandas
paramdata = pandas.read_csv('params.csv', names=paramnames)
</code></pre>

<p>where, <code>paramnames</code> is a python list of string objects. Example of <code>paramnames</code> (the length of actual list is 22):</p>

<pre><code>paramnames = [""id"",
""fc"",
""mc"",
""markup"",
""asplevel"",
""aspreview"",
""reviewpd""]
</code></pre>

<p>At the ipython prompt if I type <code>paramdata</code> and press enter then I do not get the dataframe with columns and values as shown in examples on <a href=""http://pandas.sourceforge.net/indexing.html#additional-column-access"" rel=""noreferrer"">Pandas website</a>. Instead, I get information about the dataframe. I get:</p>

<pre><code>In[35]: paramdata
Out[35]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 59 entries, 0 to 58
Data columns:
id                    59  non-null values
fc                    59  non-null values
mc                    59  non-null values
markup                59  non-null values
asplevel              59  non-null values
aspreview             59  non-null values
reviewpd              59  non-null values
</code></pre>

<p>If I type <code>paramdata['mc']</code> then I do get the values as expected for the <code>mc</code> column. I have two questions:</p>

<p>(1) In the examples on the pandas website (see, for example, the output of <code>df</code> here: <a href=""http://pandas.sourceforge.net/indexing.html#additional-column-access"" rel=""noreferrer"">http://pandas.sourceforge.net/indexing.html#additional-column-access</a>) typing the name of the dataframe gives the actual data. Why am I getting information about the dataframe as shown above instead of the actual data? Do I need to set some output options somewhere?</p>

<p>(2) How do I output all columns in the dataframe to the screen without having to type their names, i.e., without having to type something like <code>paramdata[['id','fc','mc']]</code>. </p>

<p>I am using pandas version 0.8. </p>

<p>Thank you.</p>
";106412.0;"[""import pandas\nparamdata = pandas.read_csv('params.csv', names=paramnames)\n"", 'paramnames = [""id"",\n""fc"",\n""mc"",\n""markup"",\n""asplevel"",\n""aspreview"",\n""reviewpd""]\n', ""In[35]: paramdata\nOut[35]: \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 59 entries, 0 to 58\nData columns:\nid                    59  non-null values\nfc                    59  non-null values\nmc                    59  non-null values\nmarkup                59  non-null values\nasplevel              59  non-null values\naspreview             59  non-null values\nreviewpd              59  non-null values\n""]";"['params.csv', 'ipython qtconsole', 'dataframe', ""import pandas\nparamdata = pandas.read_csv('params.csv', names=paramnames)\n"", 'paramnames', 'paramnames', 'paramnames = [""id"",\n""fc"",\n""mc"",\n""markup"",\n""asplevel"",\n""aspreview"",\n""reviewpd""]\n', 'paramdata', ""In[35]: paramdata\nOut[35]: \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 59 entries, 0 to 58\nData columns:\nid                    59  non-null values\nfc                    59  non-null values\nmc                    59  non-null values\nmarkup                59  non-null values\nasplevel              59  non-null values\naspreview             59  non-null values\nreviewpd              59  non-null values\n"", ""paramdata['mc']"", 'mc', 'df', ""paramdata[['id','fc','mc']]""]"
32;3.0;0;11391969;;1;39;<python><pandas>;How to group pandas DataFrame entries by date in a non-unique column;"<p>A Pandas <code>DataFrame</code> contains column named <code>""date""</code> that contains non-unique <code>datetime</code> values. 
I can group the lines in this frame using:</p>

<pre><code>data.groupby(data['date'])
</code></pre>

<p>However, this splits the data by the <code>datetime</code> values. I would like to group these data by the year stored in the ""date"" column. <a href=""http://wesmckinney.com/blog/?p=125"" rel=""noreferrer"">This page</a> shows how to group by year in cases where the time stamp is used as an index, which is not true in my case.</p>

<p>How do I achieve this grouping?</p>
";24050.0;"[""data.groupby(data['date'])\n""]";"['DataFrame', '""date""', 'datetime', ""data.groupby(data['date'])\n"", 'datetime']"
33;3.0;0;11418192;;1;36;<pandas>;pandas: complex filter on rows of DataFrame;"<p>I would like to filter rows by a function of each row, e.g.</p>

<pre><code>def f(row):
  return sin(row['velocity'])/np.prod(['masses']) &gt; 5

df = pandas.DataFrame(...)
filtered = df[apply_to_all_rows(df, f)]
</code></pre>

<p>Or for another more complex, contrived example,</p>

<pre><code>def g(row):
  if row['col1'].method1() == 1:
    val = row['col1'].method2() / row['col1'].method3(row['col3'], row['col4'])
  else:
    val = row['col2'].method5(row['col6'])
  return np.sin(val)

df = pandas.DataFrame(...)
filtered = df[apply_to_all_rows(df, g)]
</code></pre>

<p><em>How can I do so?</em></p>
";20153.0;"[""def f(row):\n  return sin(row['velocity'])/np.prod(['masses']) > 5\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, f)]\n"", ""def g(row):\n  if row['col1'].method1() == 1:\n    val = row['col1'].method2() / row['col1'].method3(row['col3'], row['col4'])\n  else:\n    val = row['col2'].method5(row['col6'])\n  return np.sin(val)\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, g)]\n""]";"[""def f(row):\n  return sin(row['velocity'])/np.prod(['masses']) > 5\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, f)]\n"", ""def g(row):\n  if row['col1'].method1() == 1:\n    val = row['col1'].method2() / row['col1'].method3(row['col3'], row['col4'])\n  else:\n    val = row['col2'].method5(row['col6'])\n  return np.sin(val)\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, g)]\n""]"
34;2.0;0;11495051;;1;21;<python><r><pandas><rpy2><statsmodels>;Difference in Python statsmodels OLS and R's lm;"<p>I'm not sure why I'm getting slightly different results for a simple OLS, depending on whether I go through <a href=""http://pandas.pydata.org/pandas-docs/dev/r_interface.html"" rel=""nofollow noreferrer"">panda's experimental rpy interface</a> to do the regression in <code>R</code> or whether I use <a href=""http://statsmodels.sourceforge.net/devel/index.html"" rel=""nofollow noreferrer"">statsmodels</a> in Python.</p>

<pre><code>import pandas
from rpy2.robjects import r

from functools import partial

loadcsv = partial(pandas.DataFrame.from_csv,
                  index_col=""seqn"", parse_dates=False)

demoq = loadcsv(""csv/DEMO.csv"")
rxq = loadcsv(""csv/quest/RXQ_RX.csv"")

num_rx = {}
for seqn, num in rxq.rxd295.iteritems():
    try:
        val = int(num)
    except ValueError:
        val = 0
    num_rx[seqn] = val

series = pandas.Series(num_rx, name=""num_rx"")
demoq = demoq.join(series)

import pandas.rpy.common as com
df = com.convert_to_r_dataframe(demoq)
r.assign(""demoq"", df)
r('lmout &lt;- lm(demoq$num_rx ~ demoq$ridageyr)')  # run the regression
r('print(summary(lmout))')  # print from R
</code></pre>

<p>From <code>R</code>, I get the following summary:</p>

<pre><code>Call:
lm(formula = demoq$num_rx ~ demoq$ridageyr)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.9086 -0.6908 -0.2940  0.1358 15.7003 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    -0.1358216  0.0241399  -5.626 1.89e-08 ***
demoq$ridageyr  0.0358161  0.0006232  57.469  &lt; 2e-16 ***
---
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1 

Residual standard error: 1.545 on 9963 degrees of freedom
Multiple R-squared: 0.249,  Adjusted R-squared: 0.2489 
F-statistic:  3303 on 1 and 9963 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Using <code>statsmodels.api</code> to do the OLS:</p>

<pre><code>import statsmodels.api as sm
results = sm.OLS(demoq.num_rx, demoq.ridageyr).fit()
results.summary()
</code></pre>

<p>The results are similar to R's output but not the same:</p>

<pre><code>OLS Regression Results
Adj. R-squared:  0.247
Log-Likelihood:  -18488.
No. Observations:    9965    AIC:   3.698e+04
Df Residuals:    9964    BIC:   3.698e+04
             coef   std err  t     P&gt;|t|    [95.0% Conf. Int.]
ridageyr     0.0331  0.000   82.787    0.000        0.032 0.034
</code></pre>

<p>The install process is a a bit cumbersome. But, there is an <em>ipython notebook</em> <a href=""https://github.com/skyl/NHANES-opensource/blob/master/1999-2000/RX-Explore.ipynb"" rel=""nofollow noreferrer"">here</a>, that can reproduce the inconsistency.</p>
";4994.0;"['import pandas\nfrom rpy2.robjects import r\n\nfrom functools import partial\n\nloadcsv = partial(pandas.DataFrame.from_csv,\n                  index_col=""seqn"", parse_dates=False)\n\ndemoq = loadcsv(""csv/DEMO.csv"")\nrxq = loadcsv(""csv/quest/RXQ_RX.csv"")\n\nnum_rx = {}\nfor seqn, num in rxq.rxd295.iteritems():\n    try:\n        val = int(num)\n    except ValueError:\n        val = 0\n    num_rx[seqn] = val\n\nseries = pandas.Series(num_rx, name=""num_rx"")\ndemoq = demoq.join(series)\n\nimport pandas.rpy.common as com\ndf = com.convert_to_r_dataframe(demoq)\nr.assign(""demoq"", df)\nr(\'lmout <- lm(demoq$num_rx ~ demoq$ridageyr)\')  # run the regression\nr(\'print(summary(lmout))\')  # print from R\n', 'Call:\nlm(formula = demoq$num_rx ~ demoq$ridageyr)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9086 -0.6908 -0.2940  0.1358 15.7003 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -0.1358216  0.0241399  -5.626 1.89e-08 ***\ndemoq$ridageyr  0.0358161  0.0006232  57.469  < 2e-16 ***\n---\nSignif. codes:  0 \x91***\x92 0.001 \x91**\x92 0.01 \x91*\x92 0.05 \x91.\x92 0.1 \x91 \x92 1 \n\nResidual standard error: 1.545 on 9963 degrees of freedom\nMultiple R-squared: 0.249,  Adjusted R-squared: 0.2489 \nF-statistic:  3303 on 1 and 9963 DF,  p-value: < 2.2e-16\n', 'import statsmodels.api as sm\nresults = sm.OLS(demoq.num_rx, demoq.ridageyr).fit()\nresults.summary()\n', 'OLS Regression Results\nAdj. R-squared:  0.247\nLog-Likelihood:  -18488.\nNo. Observations:    9965    AIC:   3.698e+04\nDf Residuals:    9964    BIC:   3.698e+04\n             coef   std err  t     P>|t|    [95.0% Conf. Int.]\nridageyr     0.0331  0.000   82.787    0.000        0.032 0.034\n']";"['R', 'import pandas\nfrom rpy2.robjects import r\n\nfrom functools import partial\n\nloadcsv = partial(pandas.DataFrame.from_csv,\n                  index_col=""seqn"", parse_dates=False)\n\ndemoq = loadcsv(""csv/DEMO.csv"")\nrxq = loadcsv(""csv/quest/RXQ_RX.csv"")\n\nnum_rx = {}\nfor seqn, num in rxq.rxd295.iteritems():\n    try:\n        val = int(num)\n    except ValueError:\n        val = 0\n    num_rx[seqn] = val\n\nseries = pandas.Series(num_rx, name=""num_rx"")\ndemoq = demoq.join(series)\n\nimport pandas.rpy.common as com\ndf = com.convert_to_r_dataframe(demoq)\nr.assign(""demoq"", df)\nr(\'lmout <- lm(demoq$num_rx ~ demoq$ridageyr)\')  # run the regression\nr(\'print(summary(lmout))\')  # print from R\n', 'R', 'Call:\nlm(formula = demoq$num_rx ~ demoq$ridageyr)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9086 -0.6908 -0.2940  0.1358 15.7003 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -0.1358216  0.0241399  -5.626 1.89e-08 ***\ndemoq$ridageyr  0.0358161  0.0006232  57.469  < 2e-16 ***\n---\nSignif. codes:  0 \x91***\x92 0.001 \x91**\x92 0.01 \x91*\x92 0.05 \x91.\x92 0.1 \x91 \x92 1 \n\nResidual standard error: 1.545 on 9963 degrees of freedom\nMultiple R-squared: 0.249,  Adjusted R-squared: 0.2489 \nF-statistic:  3303 on 1 and 9963 DF,  p-value: < 2.2e-16\n', 'statsmodels.api', 'import statsmodels.api as sm\nresults = sm.OLS(demoq.num_rx, demoq.ridageyr).fit()\nresults.summary()\n', 'OLS Regression Results\nAdj. R-squared:  0.247\nLog-Likelihood:  -18488.\nNo. Observations:    9965    AIC:   3.698e+04\nDf Residuals:    9964    BIC:   3.698e+04\n             coef   std err  t     P>|t|    [95.0% Conf. Int.]\nridageyr     0.0331  0.000   82.787    0.000        0.032 0.034\n']"
35;2.0;3;11548005;;1;54;<python><numpy><int><pandas><data-type-conversion>;NumPy or Pandas: Keeping array type as integer while having a NaN value;"<p>Is there a preferred way to keep the data type of a <code>numpy</code> array fixed as <code>int</code> (or <code>int64</code> or whatever), while still having an element inside listed as <code>numpy.NaN</code>?</p>

<p>In particular, I am converting an in-house data structure to a Pandas DataFrame. In our structure, we have integer-type columns that still have NaN's (but the dtype of the column is int). It seems to recast everything as a float if we make this a DataFrame, but we'd really like to be <code>int</code>.</p>

<p>Thoughts?</p>

<p><strong>Things tried:</strong></p>

<p>I tried using the <code>from_records()</code> function under pandas.DataFrame, with <code>coerce_float=False</code> and this did not help. I also tried using NumPy masked arrays, with NaN fill_value, which also did not work. All of these caused the column data type to become a float.</p>
";16797.0;[];['numpy', 'int', 'int64', 'numpy.NaN', 'int', 'from_records()', 'coerce_float=False']
36;2.0;0;11615504;;1;21;<python><pandas>;Parse dates when YYYYMMDD and HH are in separate columns using pandas in Python;"<p>I have a simple question related with csv files and parsing datetime.</p>

<p>I have a csv file that look like this:</p>

<pre><code>YYYYMMDD, HH,    X
20110101,  1,   10
20110101,  2,   20
20110101,  3,   30
</code></pre>

<p>I would like to read it using pandas (read_csv) and have it in a dataframe indexed by the datetime. So far I've tried to implement the following:</p>

<pre><code>import pandas as pnd
pnd.read_csv(""..\\file.csv"",  parse_dates = True, index_col = [0,1])
</code></pre>

<p>and the result I get is:</p>

<pre><code>                         X
YYYYMMDD    HH            
2011-01-01 2012-07-01   10
           2012-07-02   20
           2012-07-03   30
</code></pre>

<p>As you see the parse_dates in converting the HH into a different date.</p>

<p>Is there a simple and efficient way to combine properly the column ""YYYYMMDD"" with the column ""HH"" in order to have something like this? :</p>

<pre><code>                      X
Datetime              
2011-01-01 01:00:00  10
2011-01-01 02:00:00  20
2011-01-01 03:00:00  30
</code></pre>

<p>Thanks in advance for the help.</p>
";15326.0;"['YYYYMMDD, HH,    X\n20110101,  1,   10\n20110101,  2,   20\n20110101,  3,   30\n', 'import pandas as pnd\npnd.read_csv(""..\\\\file.csv"",  parse_dates = True, index_col = [0,1])\n', '                         X\nYYYYMMDD    HH            \n2011-01-01 2012-07-01   10\n           2012-07-02   20\n           2012-07-03   30\n', '                      X\nDatetime              \n2011-01-01 01:00:00  10\n2011-01-01 02:00:00  20\n2011-01-01 03:00:00  30\n']";"['YYYYMMDD, HH,    X\n20110101,  1,   10\n20110101,  2,   20\n20110101,  3,   30\n', 'import pandas as pnd\npnd.read_csv(""..\\\\file.csv"",  parse_dates = True, index_col = [0,1])\n', '                         X\nYYYYMMDD    HH            \n2011-01-01 2012-07-01   10\n           2012-07-02   20\n           2012-07-03   30\n', '                      X\nDatetime              \n2011-01-01 01:00:00  10\n2011-01-01 02:00:00  20\n2011-01-01 03:00:00  30\n']"
37;6.0;1;11622652;;1;74;<python><pandas><sas>;Large, persistent DataFrame in pandas;"<p>I am exploring switching to python and pandas as a long-time SAS user.  </p>

<p>However, when running some tests today, I was surprised that python ran out of memory when trying to <code>pandas.read_csv()</code> a 128mb csv file.  It had about 200,000 rows and 200 columns of mostly numeric data.</p>

<p>With SAS, I can import a csv file into a SAS dataset and it can be as large as my hard drive. </p>

<p>Is there something analogous in <code>pandas</code>? </p>

<p>I regularly work with large files and do not have access to a distributed computing network.</p>
";45446.0;[];['pandas.read_csv()', 'pandas']
38;1.0;0;11640243;;1;33;<python><pandas>;PANDAS plot multiple Y axes;"<p>I know pandas supports a secondary Y axis, but Im curious if anyone knows a way to put a tertiary Y axis on plots... currently I am achieving this with numpy+pyplot ... but it is slow with large data sets.</p>

<p>this is to plot different measurements with distinct units on the same graph for easy comparison (eg Relative Humidity/Temperature/ and Electrical Conductivity)</p>

<p>so really just curious if anyone knows if this is possible in <code>pandas</code> without too much work.</p>

<p>[Edit] I doubt that there is a way to do this(without too much overhead) however I hope to be proven wrong , this may be a limitation of matplotlib...</p>
";15502.0;[];['pandas']
39;4.0;6;11697887;;1;43;<python><django><pandas>;Converting Django QuerySet to pandas DataFrame;"<p>I am going to convert a Django QuerySet to a pandas <code>DataFrame</code> as follows:</p>

<pre><code>qs = SomeModel.objects.select_related().filter(date__year=2012)
q = qs.values('date', 'OtherField')
df = pd.DataFrame.from_records(q)
</code></pre>

<p>It works, but is there a more efficient way?</p>
";10165.0;"[""qs = SomeModel.objects.select_related().filter(date__year=2012)\nq = qs.values('date', 'OtherField')\ndf = pd.DataFrame.from_records(q)\n""]";"['DataFrame', ""qs = SomeModel.objects.select_related().filter(date__year=2012)\nq = qs.values('date', 'OtherField')\ndf = pd.DataFrame.from_records(q)\n""]"
40;8.0;0;11707586;;1;157;<python><pandas><options><display><column-width>;Python pandas, how to widen output display to see more columns?;"<p>Is there a way to widen the display of output in either interactive or script-execution mode?</p>

<p>Specifically, I am using the describe() function on a Pandas <code>dataframe</code>.  When the <code>dataframe</code> is 5 columns (labels) wide, I get the descriptive statistics that I want.  However, if the <code>dataframe</code> has any more columns, the statistics are suppressed and something like this is returned:</p>

<pre><code>&gt;Index: 8 entries, count to max  
&gt;Data columns:  
&gt;x1          8  non-null values  
&gt;x2          8  non-null values  
&gt;x3          8  non-null values  
&gt;x4          8  non-null values  
&gt;x5          8  non-null values  
&gt;x6          8  non-null values  
&gt;x7          8  non-null values  
</code></pre>

<p>The ""8"" value is given whether there are 6 or 7 columns.  What does the ""8"" refer to?</p>

<p>I have already tried dragging the IDLE window larger, as well as increasing the ""Configure IDLE"" width options, to no avail.</p>

<p>My purpose in using Pandas and describe() is to avoid using a second program like STATA to do basic data manipulation and investigation.</p>

<p>Thanks.</p>

<p>Python/IDLE 2.7.3<br>
Pandas 0.8.1<br>
Notepad++ 6.1.4 (UNICODE)<br>
Windows Vista SP2  </p>
";96153.0;['>Index: 8 entries, count to max  \n>Data columns:  \n>x1          8  non-null values  \n>x2          8  non-null values  \n>x3          8  non-null values  \n>x4          8  non-null values  \n>x5          8  non-null values  \n>x6          8  non-null values  \n>x7          8  non-null values  \n'];['dataframe', 'dataframe', 'dataframe', '>Index: 8 entries, count to max  \n>Data columns:  \n>x1          8  non-null values  \n>x2          8  non-null values  \n>x3          8  non-null values  \n>x4          8  non-null values  \n>x5          8  non-null values  \n>x6          8  non-null values  \n>x7          8  non-null values  \n']
41;1.0;7;11728836;;1;87;<python><pandas><multiprocessing><shared-memory>;Efficiently applying a function to a grouped pandas DataFrame in parallel;"<p>I often need to apply a function to the groups of a very large <code>DataFrame</code> (of mixed data types) and would like to take advantage of multiple cores.</p>

<p>I can create an iterator from the groups and use the multiprocessing module, but it is not efficient because every group and the results of the function must be pickled for messaging between processes.</p>

<p>Is there any way to avoid the pickling or even avoid the copying of the <code>DataFrame</code> completely? It looks like the shared memory functions of the multiprocessing modules are limited to <code>numpy</code> arrays. Are there any other options?</p>
";5986.0;[];['DataFrame', 'DataFrame', 'numpy']
42;4.0;0;11858472;;1;41;<python><numpy><dataframe><pandas>;Pandas: Combine string and int columns;"<p>I have a following <code>DataFrame</code>:</p>

<pre><code>from pandas import *
df = DataFrame({'foo':['a','b','c'], 'bar':[1, 2, 3]})
</code></pre>

<p>It looks like this:</p>

<pre><code>    bar foo
0    1   a
1    2   b
2    3   c
</code></pre>

<p>Now I want to have something like:</p>

<pre><code>     bar
0    1 is a
1    2 is b
2    3 is c
</code></pre>

<p>How can I achieve this?
I tried the following:</p>

<pre><code>df['foo'] = '%s is %s' % (df['bar'], df['foo'])
</code></pre>

<p>but it gives me a wrong result:</p>

<pre><code>&gt;&gt;&gt;print df.ix[0]

bar                                                    a
foo    0    a
1    b
2    c
Name: bar is 0    1
1    2
2
Name: 0
</code></pre>

<p>Sorry for a dumb question, but this one <a href=""https://stackoverflow.com/questions/10972410/pandas-combine-two-columns-in-a-dataframe"">pandas: combine two columns in a DataFrame</a> wasn't helpful for me.</p>
";30814.0;"[""from pandas import *\ndf = DataFrame({'foo':['a','b','c'], 'bar':[1, 2, 3]})\n"", '    bar foo\n0    1   a\n1    2   b\n2    3   c\n', '     bar\n0    1 is a\n1    2 is b\n2    3 is c\n', ""df['foo'] = '%s is %s' % (df['bar'], df['foo'])\n"", '>>>print df.ix[0]\n\nbar                                                    a\nfoo    0    a\n1    b\n2    c\nName: bar is 0    1\n1    2\n2\nName: 0\n']";"['DataFrame', ""from pandas import *\ndf = DataFrame({'foo':['a','b','c'], 'bar':[1, 2, 3]})\n"", '    bar foo\n0    1   a\n1    2   b\n2    3   c\n', '     bar\n0    1 is a\n1    2 is b\n2    3 is c\n', ""df['foo'] = '%s is %s' % (df['bar'], df['foo'])\n"", '>>>print df.ix[0]\n\nbar                                                    a\nfoo    0    a\n1    b\n2    c\nName: bar is 0    1\n1    2\n2\nName: 0\n']"
43;8.0;0;11869910;;1;141;<pandas>;pandas: filter rows of DataFrame with operator chaining;"<p>Most operations in <code>pandas</code> can be accomplished with operator chaining (<code>groupby</code>, <code>aggregate</code>, <code>apply</code>, etc), but the only way I've found to filter rows is via normal bracket indexing</p>

<pre><code>df_filtered = df[df['column'] == value]
</code></pre>

<p>This is unappealing as it requires I assign <code>df</code> to a variable before being able to filter on its values.  Is there something more like the following?</p>

<pre><code>df_filtered = df.mask(lambda x: x['column'] == value)
</code></pre>
";187196.0;"[""df_filtered = df[df['column'] == value]\n"", ""df_filtered = df.mask(lambda x: x['column'] == value)\n""]";"['pandas', 'groupby', 'aggregate', 'apply', ""df_filtered = df[df['column'] == value]\n"", 'df', ""df_filtered = df.mask(lambda x: x['column'] == value)\n""]"
44;2.0;0;11927715;;1;38;<python><matplotlib><pandas>;How to give a pandas/matplotlib bar graph custom colors;"<p>I just started using pandas/matplotlib as a replacement for Excel to generate stacked bar charts.  I am running into an issue  </p>

<p>(1) there are only 5 colors in the default colormap, so if I have more than 5 categories then the colors repeat.  How can I specify more colors?  Ideally, a gradient with a start color and an end color, and a way to dynamically generate n colors in between?</p>

<p>(2) the colors are not very visually pleasing.  How do I specify a custom set of n colors?  Or, a gradient would also work.</p>

<p>An example which illustrates both of the above points is below:</p>

<pre><code>  4 from matplotlib import pyplot
  5 from pandas import *
  6 import random
  7 
  8 x = [{i:random.randint(1,5)} for i in range(10)]
  9 df = DataFrame(x)
 10 
 11 df.plot(kind='bar', stacked=True)
</code></pre>

<p>And the output is this:</p>

<p><img src=""https://i.stack.imgur.com/SC7g4.png"" alt=""enter image description here""></p>
";29388.0;"[""  4 from matplotlib import pyplot\n  5 from pandas import *\n  6 import random\n  7 \n  8 x = [{i:random.randint(1,5)} for i in range(10)]\n  9 df = DataFrame(x)\n 10 \n 11 df.plot(kind='bar', stacked=True)\n""]";"[""  4 from matplotlib import pyplot\n  5 from pandas import *\n  6 import random\n  7 \n  8 x = [{i:random.randint(1,5)} for i in range(10)]\n  9 df = DataFrame(x)\n 10 \n 11 df.plot(kind='bar', stacked=True)\n""]"
45;3.0;0;11941492;;1;30;<python><ipython><pandas>;Selecting rows from a Pandas dataframe with a compound (hierarchical) index;"<p>I'm suspicious that this is trivial, but I yet to discover the incantation that will let me select rows from a Pandas <code>dataframe</code> based on the values of a hierarchical key. So, for example, imagine we have the following <code>dataframe</code>:</p>

<pre><code>import pandas
df = pandas.DataFrame({'group1': ['a','a','a','b','b','b'],
                       'group2': ['c','c','d','d','d','e'],
                       'value1': [1.1,2,3,4,5,6],
                       'value2': [7.1,8,9,10,11,12]
})
df = df.set_index(['group1', 'group2'])
</code></pre>

<p>df looks as we would expect: </p>

<p><img src=""https://i.stack.imgur.com/VGwOg.png"" alt=""enter image description here""></p>

<p>If df were not indexed on group1 I could do the following:</p>

<pre><code>df['group1' == 'a']
</code></pre>

<p>But that fails on this dataframe with an index. So maybe I should think of this like a Pandas series with a hierarchical index:</p>

<pre><code>df['a','c']
</code></pre>

<p>Nope. That fails as well. </p>

<p>So how do I select out all the rows where:</p>

<ol>
<li>group1 == 'a'</li>
<li>group1 == 'a' &amp; group2 == 'c'</li>
<li>group2 == 'c'</li>
<li>group1 in ['a','b','c']</li>
</ol>
";25839.0;"[""import pandas\ndf = pandas.DataFrame({'group1': ['a','a','a','b','b','b'],\n                       'group2': ['c','c','d','d','d','e'],\n                       'value1': [1.1,2,3,4,5,6],\n                       'value2': [7.1,8,9,10,11,12]\n})\ndf = df.set_index(['group1', 'group2'])\n"", ""df['group1' == 'a']\n"", ""df['a','c']\n""]";"['dataframe', 'dataframe', ""import pandas\ndf = pandas.DataFrame({'group1': ['a','a','a','b','b','b'],\n                       'group2': ['c','c','d','d','d','e'],\n                       'value1': [1.1,2,3,4,5,6],\n                       'value2': [7.1,8,9,10,11,12]\n})\ndf = df.set_index(['group1', 'group2'])\n"", ""df['group1' == 'a']\n"", ""df['a','c']\n""]"
46;1.0;1;11976503;;1;38;<python><pandas>;How to keep index when using pandas merge;"<p>I would like to merge two <code>DataFrames</code>, and keep the index from the first frame as the index on the merged dataset.  However, when I do the merge, the resulting DataFrame has integer index.  How can I specify that I want to keep the index from the left data frame?</p>

<pre><code>In [441]: a=DataFrame(data={""col1"": [1,2,3], 'to_merge_on' : [1,3,4]}, index=[""a"",""b"",""c""])

In [442]: b=DataFrame(data={""col2"": [1,2,3], 'to_merge_on' : [1,3,5]})
In [443]: a
Out[443]: 
   col1  to_merge_on
a     1            1
b     2            3
c     3            4

In [444]: b
Out[444]: 
   col2  to_merge_on
0     1            1
1     2            3
2     3            5


In [445]: a.merge(b, how=""left"")
Out[445]: 
   col1  to_merge_on  col2
0     1            1     1
1     2            3     2
2     3            4   NaN

In [446]: _.index
Out[447]: Int64Index([0, 1, 2])
</code></pre>

<p>EDIT: Switched to example code that can be easily reproduced</p>
";12679.0;"['In [441]: a=DataFrame(data={""col1"": [1,2,3], \'to_merge_on\' : [1,3,4]}, index=[""a"",""b"",""c""])\n\nIn [442]: b=DataFrame(data={""col2"": [1,2,3], \'to_merge_on\' : [1,3,5]})\nIn [443]: a\nOut[443]: \n   col1  to_merge_on\na     1            1\nb     2            3\nc     3            4\n\nIn [444]: b\nOut[444]: \n   col2  to_merge_on\n0     1            1\n1     2            3\n2     3            5\n\n\nIn [445]: a.merge(b, how=""left"")\nOut[445]: \n   col1  to_merge_on  col2\n0     1            1     1\n1     2            3     2\n2     3            4   NaN\n\nIn [446]: _.index\nOut[447]: Int64Index([0, 1, 2])\n']";"['DataFrames', 'In [441]: a=DataFrame(data={""col1"": [1,2,3], \'to_merge_on\' : [1,3,4]}, index=[""a"",""b"",""c""])\n\nIn [442]: b=DataFrame(data={""col2"": [1,2,3], \'to_merge_on\' : [1,3,5]})\nIn [443]: a\nOut[443]: \n   col1  to_merge_on\na     1            1\nb     2            3\nc     3            4\n\nIn [444]: b\nOut[444]: \n   col2  to_merge_on\n0     1            1\n1     2            3\n2     3            5\n\n\nIn [445]: a.merge(b, how=""left"")\nOut[445]: \n   col1  to_merge_on  col2\n0     1            1     1\n1     2            3     2\n2     3            4   NaN\n\nIn [446]: _.index\nOut[447]: Int64Index([0, 1, 2])\n']"
47;3.0;7;12021730;;1;21;<python><pandas>;Can pandas handle variable-length whitespace as column delimiters;"<p>I have a textfile where columns are separated by variable amounts of whitespace. Is it possible to load this file directly as a pandas dataframe without pre-processing the file? In the <a href=""http://pandas-docs.github.io/pandas-docs-travis/io.html#csv-text-files"">pandas documentation the delimiter section</a> says that I can use a <code>'s*'</code> construct but I couldn't get this to work. </p>

<pre><code>## sample data
head sample.txt

#                                                                            --- full sequence --- -------------- this domain -------------   hmm coord   ali coord   env coord
# target name        accession   tlen query name           accession   qlen   E-value  score  bias   #  of  c-Evalue  i-Evalue  score  bias  from    to  from    to  from    to  acc description of target
#------------------- ---------- ----- -------------------- ---------- ----- --------- ------ ----- --- --- --------- --------- ------ ----- ----- ----- ----- ----- ----- ----- ---- ---------------------
ABC_membrane         PF00664.18   275 AAF67494.2_AF170880  -            615     8e-29  100.7  11.4   1   1     3e-32     1e-28  100.4   7.9     3   273    42   313    40   315 0.95 ABC transporter transmembrane region
ABC_tran             PF00005.22   118 AAF67494.2_AF170880  -            615   2.6e-20   72.8   0.0   1   1   1.9e-23   6.4e-20   71.5   0.0     1   118   402   527   402   527 0.93 ABC transporter
SMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   1   2    0.0036        12    4.9   0.0    27    40   391   404   383   408 0.86 RecF/RecN/SMC N terminal domain
SMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   2   2   1.8e-09   6.1e-06   25.4   0.0   116   210   461   568   428   575 0.85 RecF/RecN/SMC N terminal domain
AAA_16               PF13191.1    166 AAF67494.2_AF170880  -            615   3.1e-06   27.5   0.3   1   1     2e-09     7e-06   26.4   0.2    20   158   386   544   376   556 0.72 AAA ATPase domain
YceG                 PF02618.11   297 AAF67495.1_AF170880  -            284   3.4e-64  216.6   0.0   1   1   2.9e-68     4e-64  216.3   0.0    68   296    53   274    29   275 0.85 YceG-like family
Pyr_redox_3          PF13738.1    203 AAF67496.2_AF170880  -            352   2.9e-28   99.1   0.0   1   2   2.8e-30   4.8e-27   95.2   0.0     1   201     4   198     4   200 0.85 Pyridine nucleotide-disulphide oxidoreductase

#load data
from pandas import *
data = read_table('sample.txt', skiprows=3, header=None, sep="" "")

ValueError: Expecting 83 columns, got 91 in row 4

#load data part 2
data = read_table('sample.txt', skiprows=3, header=None, sep=""'s*' "")
#this mushes some of the columns into the first column and drops the rest.
    X.1
1    ABC_tran PF00005.22 118 AAF67494.2_
2    SMC_N PF02463.14 220 AAF67494.2_
3    SMC_N PF02463.14 220 AAF67494.2_
4    AAA_16 PF13191.1 166 AAF67494.2_
5    YceG PF02618.11 297 AAF67495.1_
6    Pyr_redox_3 PF13738.1 203 AAF67496.2_
7    Pyr_redox_3 PF13738.1 203 AAF67496.2_
8    FMO-like PF00743.14 532 AAF67496.2_
9    FMO-like PF00743.14 532 AAF67496.2_
</code></pre>

<p>While I can preprocess the files to change the whitespace to commas/tabs it would be nice to load them directly.</p>

<p>(FYI this is the *.hmmdomtblout output from the <a href=""http://hmmer.janelia.org/"">hmmscan program</a>)</p>
";8587.0;"['## sample data\nhead sample.txt\n\n#                                                                            --- full sequence --- -------------- this domain -------------   hmm coord   ali coord   env coord\n# target name        accession   tlen query name           accession   qlen   E-value  score  bias   #  of  c-Evalue  i-Evalue  score  bias  from    to  from    to  from    to  acc description of target\n#------------------- ---------- ----- -------------------- ---------- ----- --------- ------ ----- --- --- --------- --------- ------ ----- ----- ----- ----- ----- ----- ----- ---- ---------------------\nABC_membrane         PF00664.18   275 AAF67494.2_AF170880  -            615     8e-29  100.7  11.4   1   1     3e-32     1e-28  100.4   7.9     3   273    42   313    40   315 0.95 ABC transporter transmembrane region\nABC_tran             PF00005.22   118 AAF67494.2_AF170880  -            615   2.6e-20   72.8   0.0   1   1   1.9e-23   6.4e-20   71.5   0.0     1   118   402   527   402   527 0.93 ABC transporter\nSMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   1   2    0.0036        12    4.9   0.0    27    40   391   404   383   408 0.86 RecF/RecN/SMC N terminal domain\nSMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   2   2   1.8e-09   6.1e-06   25.4   0.0   116   210   461   568   428   575 0.85 RecF/RecN/SMC N terminal domain\nAAA_16               PF13191.1    166 AAF67494.2_AF170880  -            615   3.1e-06   27.5   0.3   1   1     2e-09     7e-06   26.4   0.2    20   158   386   544   376   556 0.72 AAA ATPase domain\nYceG                 PF02618.11   297 AAF67495.1_AF170880  -            284   3.4e-64  216.6   0.0   1   1   2.9e-68     4e-64  216.3   0.0    68   296    53   274    29   275 0.85 YceG-like family\nPyr_redox_3          PF13738.1    203 AAF67496.2_AF170880  -            352   2.9e-28   99.1   0.0   1   2   2.8e-30   4.8e-27   95.2   0.0     1   201     4   198     4   200 0.85 Pyridine nucleotide-disulphide oxidoreductase\n\n#load data\nfrom pandas import *\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep="" "")\n\nValueError: Expecting 83 columns, got 91 in row 4\n\n#load data part 2\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep=""\'s*\' "")\n#this mushes some of the columns into the first column and drops the rest.\n    X.1\n1    ABC_tran PF00005.22 118 AAF67494.2_\n2    SMC_N PF02463.14 220 AAF67494.2_\n3    SMC_N PF02463.14 220 AAF67494.2_\n4    AAA_16 PF13191.1 166 AAF67494.2_\n5    YceG PF02618.11 297 AAF67495.1_\n6    Pyr_redox_3 PF13738.1 203 AAF67496.2_\n7    Pyr_redox_3 PF13738.1 203 AAF67496.2_\n8    FMO-like PF00743.14 532 AAF67496.2_\n9    FMO-like PF00743.14 532 AAF67496.2_\n']";"[""'s*'"", '## sample data\nhead sample.txt\n\n#                                                                            --- full sequence --- -------------- this domain -------------   hmm coord   ali coord   env coord\n# target name        accession   tlen query name           accession   qlen   E-value  score  bias   #  of  c-Evalue  i-Evalue  score  bias  from    to  from    to  from    to  acc description of target\n#------------------- ---------- ----- -------------------- ---------- ----- --------- ------ ----- --- --- --------- --------- ------ ----- ----- ----- ----- ----- ----- ----- ---- ---------------------\nABC_membrane         PF00664.18   275 AAF67494.2_AF170880  -            615     8e-29  100.7  11.4   1   1     3e-32     1e-28  100.4   7.9     3   273    42   313    40   315 0.95 ABC transporter transmembrane region\nABC_tran             PF00005.22   118 AAF67494.2_AF170880  -            615   2.6e-20   72.8   0.0   1   1   1.9e-23   6.4e-20   71.5   0.0     1   118   402   527   402   527 0.93 ABC transporter\nSMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   1   2    0.0036        12    4.9   0.0    27    40   391   404   383   408 0.86 RecF/RecN/SMC N terminal domain\nSMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   2   2   1.8e-09   6.1e-06   25.4   0.0   116   210   461   568   428   575 0.85 RecF/RecN/SMC N terminal domain\nAAA_16               PF13191.1    166 AAF67494.2_AF170880  -            615   3.1e-06   27.5   0.3   1   1     2e-09     7e-06   26.4   0.2    20   158   386   544   376   556 0.72 AAA ATPase domain\nYceG                 PF02618.11   297 AAF67495.1_AF170880  -            284   3.4e-64  216.6   0.0   1   1   2.9e-68     4e-64  216.3   0.0    68   296    53   274    29   275 0.85 YceG-like family\nPyr_redox_3          PF13738.1    203 AAF67496.2_AF170880  -            352   2.9e-28   99.1   0.0   1   2   2.8e-30   4.8e-27   95.2   0.0     1   201     4   198     4   200 0.85 Pyridine nucleotide-disulphide oxidoreductase\n\n#load data\nfrom pandas import *\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep="" "")\n\nValueError: Expecting 83 columns, got 91 in row 4\n\n#load data part 2\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep=""\'s*\' "")\n#this mushes some of the columns into the first column and drops the rest.\n    X.1\n1    ABC_tran PF00005.22 118 AAF67494.2_\n2    SMC_N PF02463.14 220 AAF67494.2_\n3    SMC_N PF02463.14 220 AAF67494.2_\n4    AAA_16 PF13191.1 166 AAF67494.2_\n5    YceG PF02618.11 297 AAF67495.1_\n6    Pyr_redox_3 PF13738.1 203 AAF67496.2_\n7    Pyr_redox_3 PF13738.1 203 AAF67496.2_\n8    FMO-like PF00743.14 532 AAF67496.2_\n9    FMO-like PF00743.14 532 AAF67496.2_\n']"
48;12.0;3;12047193;;1;43;<python><mysql><data-structures><pandas>;How to convert SQL Query result to PANDAS Data Structure?;"<p>Any help on this problem will be greatly appreciated. So basically I want to run a query to my SQL database and store the returned data as Pandas data structure. I have attached code for query. I am reading the documentation on Pandas, but I have problem to identify the return type of my query. I tried to print the query result, but it doesn't give any useful information. 
    Thanks!!!! </p>

<pre><code>from sqlalchemy import create_engine


engine2 = create_engine('mysql://THE DATABASE I AM ACCESSING')
connection2 = engine2.connect()
dataid = 1022
resoverall = connection2.execute(""SELECT sum(BLABLA) AS BLA, sum(BLABLABLA2) AS BLABLABLA2, sum(SOME_INT) AS SOME_INT, sum(SOME_INT2) AS SOME_INT2, 100*sum(SOME_INT2)/sum(SOME_INT) AS ctr, sum(SOME_INT2)/sum(SOME_INT) AS cpc FROM daily_report_cooked WHERE campaign_id = '%s'""%dataid)
</code></pre>

<p>So I sort of want to understand what's the format/datatype of my variable ""resoverall"" and how to put it with PANDAS data structure.</p>
";69160.0;"['from sqlalchemy import create_engine\n\n\nengine2 = create_engine(\'mysql://THE DATABASE I AM ACCESSING\')\nconnection2 = engine2.connect()\ndataid = 1022\nresoverall = connection2.execute(""SELECT sum(BLABLA) AS BLA, sum(BLABLABLA2) AS BLABLABLA2, sum(SOME_INT) AS SOME_INT, sum(SOME_INT2) AS SOME_INT2, 100*sum(SOME_INT2)/sum(SOME_INT) AS ctr, sum(SOME_INT2)/sum(SOME_INT) AS cpc FROM daily_report_cooked WHERE campaign_id = \'%s\'""%dataid)\n']";"['from sqlalchemy import create_engine\n\n\nengine2 = create_engine(\'mysql://THE DATABASE I AM ACCESSING\')\nconnection2 = engine2.connect()\ndataid = 1022\nresoverall = connection2.execute(""SELECT sum(BLABLA) AS BLA, sum(BLABLABLA2) AS BLABLABLA2, sum(SOME_INT) AS SOME_INT, sum(SOME_INT2) AS SOME_INT2, 100*sum(SOME_INT2)/sum(SOME_INT) AS ctr, sum(SOME_INT2)/sum(SOME_INT) AS cpc FROM daily_report_cooked WHERE campaign_id = \'%s\'""%dataid)\n']"
49;6.0;0;12065885;;1;223;<python><pandas><dataframe>;Filter dataframe rows if value in column is in a set list of values;"<p>I have a Python pandas DataFrame <code>rpt</code>:</p>

<pre><code>rpt
&lt;class 'pandas.core.frame.DataFrame'&gt;
MultiIndex: 47518 entries, ('000002', '20120331') to ('603366', '20091231')
Data columns:
STK_ID                    47518  non-null values
STK_Name                  47518  non-null values
RPT_Date                  47518  non-null values
sales                     47518  non-null values
</code></pre>

<p>I can filter the rows whose stock id is <code>'600809'</code> like this: <code>rpt[rpt['STK_ID'] == '600809']</code></p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
MultiIndex: 25 entries, ('600809', '20120331') to ('600809', '20060331')
Data columns:
STK_ID                    25  non-null values
STK_Name                  25  non-null values
RPT_Date                  25  non-null values
sales                     25  non-null values
</code></pre>

<p>and I want to get all the rows of some stocks together, such as <code>['600809','600141','600329']</code>. That means I want a syntax like this: </p>

<pre><code>stk_list = ['600809','600141','600329']

rst = rpt[rpt['STK_ID'] in stk_list] # this does not works in pandas 
</code></pre>

<p>Since pandas not accept above command, how to achieve the target? </p>
";129843.0;"[""rpt\n<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 47518 entries, ('000002', '20120331') to ('603366', '20091231')\nData columns:\nSTK_ID                    47518  non-null values\nSTK_Name                  47518  non-null values\nRPT_Date                  47518  non-null values\nsales                     47518  non-null values\n"", ""<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 25 entries, ('600809', '20120331') to ('600809', '20060331')\nData columns:\nSTK_ID                    25  non-null values\nSTK_Name                  25  non-null values\nRPT_Date                  25  non-null values\nsales                     25  non-null values\n"", ""stk_list = ['600809','600141','600329']\n\nrst = rpt[rpt['STK_ID'] in stk_list] # this does not works in pandas \n""]";"['rpt', ""rpt\n<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 47518 entries, ('000002', '20120331') to ('603366', '20091231')\nData columns:\nSTK_ID                    47518  non-null values\nSTK_Name                  47518  non-null values\nRPT_Date                  47518  non-null values\nsales                     47518  non-null values\n"", ""'600809'"", ""rpt[rpt['STK_ID'] == '600809']"", ""<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 25 entries, ('600809', '20120331') to ('600809', '20060331')\nData columns:\nSTK_ID                    25  non-null values\nSTK_Name                  25  non-null values\nRPT_Date                  25  non-null values\nsales                     25  non-null values\n"", ""['600809','600141','600329']"", ""stk_list = ['600809','600141','600329']\n\nrst = rpt[rpt['STK_ID'] in stk_list] # this does not works in pandas \n""]"
50;1.0;1;12096252;;1;241;<python><pandas>;use a list of values to select rows from a pandas dataframe;"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/12065885/how-to-filter-the-dataframe-rows-of-pandas-by-within-in"">how to filter the dataframe rows of pandas by &ldquo;within&rdquo;/&ldquo;in&rdquo;?</a>  </p>
</blockquote>



<p>Lets say I have the following pandas dataframe:</p>

<pre><code>df = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})
df

     A   B
0    5   1
1    6   2
2    3   3
3    4   5
</code></pre>

<p>I can subset based on a specific value:</p>

<pre><code>x = df[df['A'] == 3]
x

     A   B
2    3   3
</code></pre>

<p>But how can I subset based on a list of values? - something like this:</p>

<pre><code>list_of_values = [3,6]

y = df[df['A'] in list_of_values]
</code></pre>
";178009.0;"[""df = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})\ndf\n\n     A   B\n0    5   1\n1    6   2\n2    3   3\n3    4   5\n"", ""x = df[df['A'] == 3]\nx\n\n     A   B\n2    3   3\n"", ""list_of_values = [3,6]\n\ny = df[df['A'] in list_of_values]\n""]";"[""df = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})\ndf\n\n     A   B\n0    5   1\n1    6   2\n2    3   3\n3    4   5\n"", ""x = df[df['A'] == 3]\nx\n\n     A   B\n2    3   3\n"", ""list_of_values = [3,6]\n\ny = df[df['A'] in list_of_values]\n""]"
51;3.0;1;12182744;;1;49;<python><pandas><apply>;python pandas: apply a function with arguments to a series;"<p>I want to apply a function with arguments to a series in python pandas:</p>

<pre><code>x = my_series.apply(my_function, more_arguments_1)
y = my_series.apply(my_function, more_arguments_2)
...
</code></pre>

<p>The <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html"" rel=""nofollow noreferrer"">documentation</a> describes support for an apply method, but it doesn't accept any arguments.  Is there a different method that accepts arguments?  Alternatively, am I missing a simple workaround?</p>
";45364.0;['x = my_series.apply(my_function, more_arguments_1)\ny = my_series.apply(my_function, more_arguments_2)\n...\n'];['x = my_series.apply(my_function, more_arguments_1)\ny = my_series.apply(my_function, more_arguments_2)\n...\n']
52;5.0;1;12190874;;1;59;<python><partitioning><pandas>;Pandas: Sampling a DataFrame;"<p>I'm trying to read a fairly large CSV file with Pandas and split it up into two random chunks, one of which being 10% of the data and the other being 90%.</p>

<p>Here's my current attempt:</p>

<pre><code>rows = data.index
row_count = len(rows)
random.shuffle(list(rows))

data.reindex(rows)

training_data = data[row_count // 10:]
testing_data = data[:row_count // 10]
</code></pre>

<p>For some reason, <code>sklearn</code> throws this error when I try to use one of these resulting DataFrame objects inside of a SVM classifier:</p>

<pre><code>IndexError: each subindex must be either a slice, an integer, Ellipsis, or newaxis
</code></pre>

<p>I think I'm doing it wrong. Is there a better way to do this?</p>
";57789.0;['rows = data.index\nrow_count = len(rows)\nrandom.shuffle(list(rows))\n\ndata.reindex(rows)\n\ntraining_data = data[row_count // 10:]\ntesting_data = data[:row_count // 10]\n', 'IndexError: each subindex must be either a slice, an integer, Ellipsis, or newaxis\n'];['rows = data.index\nrow_count = len(rows)\nrandom.shuffle(list(rows))\n\ndata.reindex(rows)\n\ntraining_data = data[row_count // 10:]\ntesting_data = data[:row_count // 10]\n', 'sklearn', 'IndexError: each subindex must be either a slice, an integer, Ellipsis, or newaxis\n']
53;5.0;3;12200693;;1;34;<python><group-by><dataframe><pandas>;Python Pandas How to assign groupby operation results back to columns in parent dataframe?;"<p>I have the following data frame in IPython, where each row is a single stock:</p>

<pre><code>In [261]: bdata
Out[261]:
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 21210 entries, 0 to 21209
Data columns:
BloombergTicker      21206  non-null values
Company              21210  non-null values
Country              21210  non-null values
MarketCap            21210  non-null values
PriceReturn          21210  non-null values
SEDOL                21210  non-null values
yearmonth            21210  non-null values
dtypes: float64(2), int64(1), object(4)
</code></pre>

<p>I want to apply a groupby operation that computes cap-weighted average return across everything, per each date in the ""yearmonth"" column.</p>

<p>This works as expected:</p>

<pre><code>In [262]: bdata.groupby(""yearmonth"").apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())
Out[262]:
yearmonth
201204      -0.109444
201205      -0.290546
</code></pre>

<p>But then I want to sort of ""broadcast"" these values back to the indices in the original data frame, and save them as constant columns where the dates match.</p>

<pre><code>In [263]: dateGrps = bdata.groupby(""yearmonth"")

In [264]: dateGrps[""MarketReturn""] = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/mnt/bos-devrnd04/usr6/home/espears/ws/Research/Projects/python-util/src/util/&lt;ipython-input-264-4a68c8782426&gt; in &lt;module&gt;()
----&gt; 1 dateGrps[""MarketReturn""] = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())

TypeError: 'DataFrameGroupBy' object does not support item assignment
</code></pre>

<p>I realize this naive assignment should not work. But what is the ""right"" Pandas idiom for assigning the result of a groupby operation into a new column on the parent dataframe?</p>

<p>In the end, I want a column called ""MarketReturn"" than will be a repeated constant value for all indices that have matching date with the output of the groupby operation.</p>

<p>One hack to achieve this would be the following:</p>

<pre><code>marketRetsByDate  = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())

bdata[""MarketReturn""] = np.repeat(np.NaN, len(bdata))

for elem in marketRetsByDate.index.values:
    bdata[""MarketReturn""][bdata[""yearmonth""]==elem] = marketRetsByDate.ix[elem]
</code></pre>

<p>But this is slow, bad, and unPythonic.</p>
";28220.0;"[""In [261]: bdata\nOut[261]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 21210 entries, 0 to 21209\nData columns:\nBloombergTicker      21206  non-null values\nCompany              21210  non-null values\nCountry              21210  non-null values\nMarketCap            21210  non-null values\nPriceReturn          21210  non-null values\nSEDOL                21210  non-null values\nyearmonth            21210  non-null values\ndtypes: float64(2), int64(1), object(4)\n"", 'In [262]: bdata.groupby(""yearmonth"").apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\nOut[262]:\nyearmonth\n201204      -0.109444\n201205      -0.290546\n', 'In [263]: dateGrps = bdata.groupby(""yearmonth"")\n\nIn [264]: dateGrps[""MarketReturn""] = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/mnt/bos-devrnd04/usr6/home/espears/ws/Research/Projects/python-util/src/util/<ipython-input-264-4a68c8782426> in <module>()\n----> 1 dateGrps[""MarketReturn""] = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n\nTypeError: \'DataFrameGroupBy\' object does not support item assignment\n', 'marketRetsByDate  = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n\nbdata[""MarketReturn""] = np.repeat(np.NaN, len(bdata))\n\nfor elem in marketRetsByDate.index.values:\n    bdata[""MarketReturn""][bdata[""yearmonth""]==elem] = marketRetsByDate.ix[elem]\n']";"[""In [261]: bdata\nOut[261]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 21210 entries, 0 to 21209\nData columns:\nBloombergTicker      21206  non-null values\nCompany              21210  non-null values\nCountry              21210  non-null values\nMarketCap            21210  non-null values\nPriceReturn          21210  non-null values\nSEDOL                21210  non-null values\nyearmonth            21210  non-null values\ndtypes: float64(2), int64(1), object(4)\n"", 'In [262]: bdata.groupby(""yearmonth"").apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\nOut[262]:\nyearmonth\n201204      -0.109444\n201205      -0.290546\n', 'In [263]: dateGrps = bdata.groupby(""yearmonth"")\n\nIn [264]: dateGrps[""MarketReturn""] = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/mnt/bos-devrnd04/usr6/home/espears/ws/Research/Projects/python-util/src/util/<ipython-input-264-4a68c8782426> in <module>()\n----> 1 dateGrps[""MarketReturn""] = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n\nTypeError: \'DataFrameGroupBy\' object does not support item assignment\n', 'marketRetsByDate  = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n\nbdata[""MarketReturn""] = np.repeat(np.NaN, len(bdata))\n\nfor elem in marketRetsByDate.index.values:\n    bdata[""MarketReturn""][bdata[""yearmonth""]==elem] = marketRetsByDate.ix[elem]\n']"
54;2.0;0;12207326;;1;60;<python><statistics><pandas><frequency>;Frequency table for a single variable;"<p>One last newbie pandas question for the day:  How do I generate a table for a single Series?</p>

<p>For example:</p>

<pre><code>my_series = pandas.Series([1,2,2,3,3,3])
pandas.magical_frequency_function( my_series )

&gt;&gt; {
     1 : 1,
     2 : 2, 
     3 : 3
   }
</code></pre>

<p>Lots of googling has led me to Series.describe() and pandas.crosstabs, but neither of these does quite what I need: one variable, counts by categories.  Oh, and it'd be nice if it worked for different data types: strings, ints, etc.</p>
";54196.0;['my_series = pandas.Series([1,2,2,3,3,3])\npandas.magical_frequency_function( my_series )\n\n>> {\n     1 : 1,\n     2 : 2, \n     3 : 3\n   }\n'];['my_series = pandas.Series([1,2,2,3,3,3])\npandas.magical_frequency_function( my_series )\n\n>> {\n     1 : 1,\n     2 : 2, \n     3 : 3\n   }\n']
55;3.0;1;12286607;;1;44;<python><pandas><heatmap>;python Making heatmap from DataFrame;"<p>I have a dataframe generated from Python's Pandas package. How can I generate  heatmap using DataFrame from pandas package. </p>

<pre><code>import numpy as np 
from pandas import *

Index= ['aaa','bbb','ccc','ddd','eee']
Cols = ['A', 'B', 'C','D']
df = DataFrame(abs(np.random.randn(5, 4)), index= Index, columns=Cols)

&gt;&gt;&gt; df
          A         B         C         D
aaa  2.431645  1.248688  0.267648  0.613826
bbb  0.809296  1.671020  1.564420  0.347662
ccc  1.501939  1.126518  0.702019  1.596048
ddd  0.137160  0.147368  1.504663  0.202822
eee  0.134540  3.708104  0.309097  1.641090
&gt;&gt;&gt; 
</code></pre>
";34778.0;"[""import numpy as np \nfrom pandas import *\n\nIndex= ['aaa','bbb','ccc','ddd','eee']\nCols = ['A', 'B', 'C','D']\ndf = DataFrame(abs(np.random.randn(5, 4)), index= Index, columns=Cols)\n\n>>> df\n          A         B         C         D\naaa  2.431645  1.248688  0.267648  0.613826\nbbb  0.809296  1.671020  1.564420  0.347662\nccc  1.501939  1.126518  0.702019  1.596048\nddd  0.137160  0.147368  1.504663  0.202822\neee  0.134540  3.708104  0.309097  1.641090\n>>> \n""]";"[""import numpy as np \nfrom pandas import *\n\nIndex= ['aaa','bbb','ccc','ddd','eee']\nCols = ['A', 'B', 'C','D']\ndf = DataFrame(abs(np.random.randn(5, 4)), index= Index, columns=Cols)\n\n>>> df\n          A         B         C         D\naaa  2.431645  1.248688  0.267648  0.613826\nbbb  0.809296  1.671020  1.564420  0.347662\nccc  1.501939  1.126518  0.702019  1.596048\nddd  0.137160  0.147368  1.504663  0.202822\neee  0.134540  3.708104  0.309097  1.641090\n>>> \n""]"
56;3.0;0;12307099;;1;82;<python><pandas>;Modifying a subset of rows in a pandas dataframe;"<p>Assume I have a pandas DataFrame with two columns, A and B. I'd like to modify this DataFrame (or create a copy) so that B is always NaN whenever A is 0. How would I achieve that?</p>

<p>I tried the following</p>

<pre><code>df['A'==0]['B'] = np.nan
</code></pre>

<p>and</p>

<pre><code>df['A'==0]['B'].values.fill(np.nan)
</code></pre>

<p>without success.</p>
";48481.0;"[""df['A'==0]['B'] = np.nan\n"", ""df['A'==0]['B'].values.fill(np.nan)\n""]";"[""df['A'==0]['B'] = np.nan\n"", ""df['A'==0]['B'].values.fill(np.nan)\n""]"
57;2.0;0;12322779;;1;22;<pandas>;Pandas: unique dataframe;"<p>I have a DataFrame that has duplicated rows. I'd like to get a DataFrame with a unique index and no duplicates. It's ok to discard the duplicated values. Is this possible? Would it be a done by <code>groupby</code>?</p>
";26767.0;[];['groupby']
58;1.0;0;12356501;;1;68;<python><pandas>;Pandas: create two new columns in a dataframe with values calculated from a pre-existing column;"<p>I am working with the <a href=""http://pandas.pydata.org/"">pandas</a> library and I want to add two new columns to a dataframe <code>df</code> with n columns (n > 0).<br>
These new columns result from the application of a function to one of the columns in the dataframe.</p>

<p>The function to apply is like:</p>

<pre><code>def calculate(x):
    ...operate...
    return z, y
</code></pre>

<p>One method for creating a new column for a function returning only a value is:</p>

<pre><code>df['new_col']) = df['column_A'].map(a_function)
</code></pre>

<p>So, what I want, and tried unsuccesfully (*), is something like:</p>

<pre><code>(df['new_col_zetas'], df['new_col_ys']) = df['column_A'].map(calculate)
</code></pre>

<p>What the best way to accomplish this could be ? I scanned the <a href=""http://pandas.pydata.org/pandas-docs/stable/"">documentation</a> with no clue. </p>

<p>*<em><code>df['column_A'].map(calculate)</code> returns a panda Series each item consisting of a tuple z, y. And trying to assign this to two dataframe columns produces a ValueError.</em> </p>
";45846.0;"['def calculate(x):\n    ...operate...\n    return z, y\n', ""df['new_col']) = df['column_A'].map(a_function)\n"", ""(df['new_col_zetas'], df['new_col_ys']) = df['column_A'].map(calculate)\n""]";"['df', 'def calculate(x):\n    ...operate...\n    return z, y\n', ""df['new_col']) = df['column_A'].map(a_function)\n"", ""(df['new_col_zetas'], df['new_col_ys']) = df['column_A'].map(calculate)\n"", ""df['column_A'].map(calculate)""]"
59;4.0;0;12376863;;1;51;<python><pandas>;Adding calculated column(s) to a dataframe in pandas;"<p>I have an OHLC price data set, that I have parsed from CSV into a Pandas dataframe and resampled to 15 min bars:</p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 500047 entries, 1998-05-04 04:45:00 to 2012-08-07 00:15:00
Freq: 15T
Data columns:
Close    363152  non-null values
High     363152  non-null values
Low      363152  non-null values
Open     363152  non-null values
dtypes: float64(4)
</code></pre>

<p>I would like to add various calculated columns, starting with simple ones such as period Range (H-L) and then booleans to indicate the occurrence of price patterns that I will define - e.g. a hammer candle pattern, for which a sample definition:</p>

<pre><code>def closed_in_top_half_of_range(h,l,c):
    return c &gt; l + (h-1)/2

def lower_wick(o,l,c):
    return min(o,c)-l

def real_body(o,c):
    return abs(c-o)

def lower_wick_at_least_twice_real_body(o,l,c):
    return lower_wick(o,l,c) &gt;= 2 * real_body(o,c)

def is_hammer(row):
    return lower_wick_at_least_twice_real_body(row[""Open""],row[""Low""],row[""Close""]) \
    and closed_in_top_half_of_range(row[""High""],row[""Low""],row[""Close""])
</code></pre>

<p>Basic problem: how do I map the function to the column, specifically where I would like to reference more than one other column or the whole row or whatever? </p>

<p><a href=""https://stackoverflow.com/questions/12356501/pandas-create-two-new-columns-in-a-dataframe-with-values-calculated-from-a-pre"">This post</a> deals with adding two calculated columns off of a single source column, which is close, but not quite it.</p>

<p>And slightly more advanced: for price patterns that are determined with reference to more than a single bar (T), how can I reference different rows (e.g. T-1, T-2 etc.) from within the function definition?</p>

<p>Many thanks in advance.</p>
";70320.0;"[""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 500047 entries, 1998-05-04 04:45:00 to 2012-08-07 00:15:00\nFreq: 15T\nData columns:\nClose    363152  non-null values\nHigh     363152  non-null values\nLow      363152  non-null values\nOpen     363152  non-null values\ndtypes: float64(4)\n"", 'def closed_in_top_half_of_range(h,l,c):\n    return c > l + (h-1)/2\n\ndef lower_wick(o,l,c):\n    return min(o,c)-l\n\ndef real_body(o,c):\n    return abs(c-o)\n\ndef lower_wick_at_least_twice_real_body(o,l,c):\n    return lower_wick(o,l,c) >= 2 * real_body(o,c)\n\ndef is_hammer(row):\n    return lower_wick_at_least_twice_real_body(row[""Open""],row[""Low""],row[""Close""]) \\\n    and closed_in_top_half_of_range(row[""High""],row[""Low""],row[""Close""])\n']";"[""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 500047 entries, 1998-05-04 04:45:00 to 2012-08-07 00:15:00\nFreq: 15T\nData columns:\nClose    363152  non-null values\nHigh     363152  non-null values\nLow      363152  non-null values\nOpen     363152  non-null values\ndtypes: float64(4)\n"", 'def closed_in_top_half_of_range(h,l,c):\n    return c > l + (h-1)/2\n\ndef lower_wick(o,l,c):\n    return min(o,c)-l\n\ndef real_body(o,c):\n    return abs(c-o)\n\ndef lower_wick_at_least_twice_real_body(o,l,c):\n    return lower_wick(o,l,c) >= 2 * real_body(o,c)\n\ndef is_hammer(row):\n    return lower_wick_at_least_twice_real_body(row[""Open""],row[""Low""],row[""Close""]) \\\n    and closed_in_top_half_of_range(row[""High""],row[""Low""],row[""Close""])\n']"
60;1.0;3;12389898;;1;22;<python><group-by><transform><dataframe><pandas>;Python Pandas: how to add a totally new column to a data frame inside of a groupby/transform operation;"<p>I want to mark some quantiles in my data, and for each row of the DataFrame, I would like the entry in a new column called e.g. ""xtile"" to hold this value.</p>

<p>For example, suppose I create a data frame like this:</p>

<pre><code>import pandas, numpy as np
dfrm = pandas.DataFrame({'A':np.random.rand(100), 
                         'B':(50+np.random.randn(100)), 
                         'C':np.random.randint(low=0, high=3, size=(100,))})
</code></pre>

<p>And let's say I write my own function to compute the quintile of each element in an array. I have my own function for this, but for example just refer to scipy.stats.mstats.mquantile.</p>

<pre><code>import scipy.stats as st
def mark_quintiles(x, breakpoints):
    # Assume this is filled in, using st.mstats.mquantiles.
    # This returns an array the same shape as x, with an integer for which
    # breakpoint-bucket that entry of x falls into.
</code></pre>

<p>Now, the real question is how to use <code>transform</code> to add a new column to the data. Something like this:</p>

<pre><code>def transformXtiles(dataFrame, inputColumnName, newColumnName, breaks):
    dataFrame[newColumnName] = mark_quintiles(dataFrame[inputColumnName].values, 
                                              breaks)
    return dataFrame
</code></pre>

<p>And then:</p>

<pre><code>dfrm.groupby(""C"").transform(lambda x: transformXtiles(x, ""A"", ""A_xtile"", [0.2, 0.4, 0.6, 0.8, 1.0]))
</code></pre>

<p>The problem is that the above code will not add the new column ""A_xtile"". It just returns my data frame unchanged. If I first add a column full of dummy values, like NaN, called ""A_xtile"", then it <em>does</em> successfully over-write this column to include the correct quintile markings.</p>

<p>But it is extremely inconvenient to have to first write in the column for anything like this that I may want to add on the fly.</p>

<p>Note that a simple <code>apply</code> will not work here, since it won't know how to make sense of the possibly differently-sized result arrays for each group.</p>
";18845.0;"[""import pandas, numpy as np\ndfrm = pandas.DataFrame({'A':np.random.rand(100), \n                         'B':(50+np.random.randn(100)), \n                         'C':np.random.randint(low=0, high=3, size=(100,))})\n"", 'import scipy.stats as st\ndef mark_quintiles(x, breakpoints):\n    # Assume this is filled in, using st.mstats.mquantiles.\n    # This returns an array the same shape as x, with an integer for which\n    # breakpoint-bucket that entry of x falls into.\n', 'def transformXtiles(dataFrame, inputColumnName, newColumnName, breaks):\n    dataFrame[newColumnName] = mark_quintiles(dataFrame[inputColumnName].values, \n                                              breaks)\n    return dataFrame\n', 'dfrm.groupby(""C"").transform(lambda x: transformXtiles(x, ""A"", ""A_xtile"", [0.2, 0.4, 0.6, 0.8, 1.0]))\n']";"[""import pandas, numpy as np\ndfrm = pandas.DataFrame({'A':np.random.rand(100), \n                         'B':(50+np.random.randn(100)), \n                         'C':np.random.randint(low=0, high=3, size=(100,))})\n"", 'import scipy.stats as st\ndef mark_quintiles(x, breakpoints):\n    # Assume this is filled in, using st.mstats.mquantiles.\n    # This returns an array the same shape as x, with an integer for which\n    # breakpoint-bucket that entry of x falls into.\n', 'transform', 'def transformXtiles(dataFrame, inputColumnName, newColumnName, breaks):\n    dataFrame[newColumnName] = mark_quintiles(dataFrame[inputColumnName].values, \n                                              breaks)\n    return dataFrame\n', 'dfrm.groupby(""C"").transform(lambda x: transformXtiles(x, ""A"", ""A_xtile"", [0.2, 0.4, 0.6, 0.8, 1.0]))\n', 'apply']"
61;4.0;1;12433076;;1;34;<pandas><finance><yahoo-finance><google-finance><stockquotes>;Download history stock prices automatically from yahoo finance in python;"<p>Is there a way to automatically download historical prices of stocks from yahoo finance or google finance (csv format)? Preferably in Python.</p>
";81673.0;[];[]
62;6.0;1;12497402;;1;43;<python><duplicates><pandas>;python pandas: Remove duplicates by columns A, keeping the row with the highest value in column B;"<p>I have a dataframe with repeat values in column A.  I want to drop duplicates, keeping the row with the highest value in column B.</p>

<p>So this:</p>

<pre><code>A B
1 10
1 20
2 30
2 40
3 10
</code></pre>

<p>Should turn into this:</p>

<pre><code>A B
1 20
2 40
3 10
</code></pre>

<p>Wes has added some nice functionality to drop duplicates: <a href=""http://wesmckinney.com/blog/?p=340"" rel=""noreferrer"">http://wesmckinney.com/blog/?p=340</a>.  But AFAICT, it's designed for exact duplicates, so there's no mention of criteria for selecting which rows get kept.</p>

<p>I'm guessing there's probably an easy way to do this---maybe as easy as sorting the dataframe before dropping duplicates---but I don't know groupby's internal logic well enough to figure it out.  Any suggestions?</p>
";38793.0;['A B\n1 10\n1 20\n2 30\n2 40\n3 10\n', 'A B\n1 20\n2 40\n3 10\n'];['A B\n1 10\n1 20\n2 30\n2 40\n3 10\n', 'A B\n1 20\n2 40\n3 10\n']
63;3.0;0;12504976;;1;26;<python><string><pandas><split>;"Get last ""column"" after .str.split() operation on column in pandas DataFrame";"<p>I have a column in a pandas DataFrame that I would like to split on a single space. The splitting is simple enough with <code>DataFrame.str.split(' ')</code>, but I can't make a new column from the last entry. When I <code>.str.split()</code> the column I get a list of arrays and I don't know how to manipulate this to get a new column for my DataFrame.</p>

<p>Here is an example. Each entry in the column contains 'symbol data price' and I would like to split off the price (and eventually remove the ""p""... or ""c"" in half the cases).</p>

<pre><code>import pandas as pd
temp = pd.DataFrame({'ticker' : ['spx 5/25/2001 p500', 'spx 5/25/2001 p600', 'spx 5/25/2001 p700']})
temp2 = temp.ticker.str.split(' ')
</code></pre>

<p>which yields</p>

<pre><code>0    ['spx', '5/25/2001', 'p500']
1    ['spx', '5/25/2001', 'p600']
2    ['spx', '5/25/2001', 'p700']
</code></pre>

<p>But <code>temp2[0]</code> just gives one list entry's array and <code>temp2[:][-1]</code> fails. How can I convert the last entry in each array to a new column? Thanks!</p>
";14984.0;"[""import pandas as pd\ntemp = pd.DataFrame({'ticker' : ['spx 5/25/2001 p500', 'spx 5/25/2001 p600', 'spx 5/25/2001 p700']})\ntemp2 = temp.ticker.str.split(' ')\n"", ""0    ['spx', '5/25/2001', 'p500']\n1    ['spx', '5/25/2001', 'p600']\n2    ['spx', '5/25/2001', 'p700']\n""]";"[""DataFrame.str.split(' ')"", '.str.split()', ""import pandas as pd\ntemp = pd.DataFrame({'ticker' : ['spx 5/25/2001 p500', 'spx 5/25/2001 p600', 'spx 5/25/2001 p700']})\ntemp2 = temp.ticker.str.split(' ')\n"", ""0    ['spx', '5/25/2001', 'p500']\n1    ['spx', '5/25/2001', 'p600']\n2    ['spx', '5/25/2001', 'p700']\n"", 'temp2[0]', 'temp2[:][-1]']"
64;4.0;0;12525722;;1;64;<python><pandas><numpy>;Normalize data in pandas;"<p>Suppose I have a pandas data frame df: </p>

<p>I want to calculate the column wise mean of a data frame, </p>

<p>This is easy: </p>

<pre><code>df.apply(average) 
</code></pre>

<p>then the column wise range max(col) - min (col). this is easy again: </p>

<pre><code>df.apply(max) - df.apply(min)
</code></pre>

<p>Now for each element I want to subtract its columns mean and divide by its columns range. 
I am not sure how to do that</p>

<p>Any help/pointers much appreciated. </p>
";84877.0;['df.apply(average) \n', 'df.apply(max) - df.apply(min)\n'];['df.apply(average) \n', 'df.apply(max) - df.apply(min)\n']
65;18.0;0;12555323;;1;350;<python><pandas><dataframe>;Adding new column to existing DataFrame in Python pandas;"<p>I have the following indexed DataFrame with named columns and rows not- continuous numbers:</p>

<pre><code>          a         b         c         d
2  0.671399  0.101208 -0.181532  0.241273
3  0.446172 -0.243316  0.051767  1.577318
5  0.614758  0.075793 -0.451460 -0.012493
</code></pre>

<p>I would like to add a new column, <code>'e'</code>, to the existing data frame and do not want to change anything in the data frame (i.e., the new column always has the same length as the DataFrame). </p>

<pre><code>0   -0.335485
1   -1.166658
2   -0.385571
dtype: float64
</code></pre>

<p>I tried different versions of <code>join</code>, <code>append</code>, <code>merge</code>, but I did not get the result I wanted, only errors at most. How can I add column <code>e</code> to the above example? </p>
";578606.0;['          a         b         c         d\n2  0.671399  0.101208 -0.181532  0.241273\n3  0.446172 -0.243316  0.051767  1.577318\n5  0.614758  0.075793 -0.451460 -0.012493\n', '0   -0.335485\n1   -1.166658\n2   -0.385571\ndtype: float64\n'];"['          a         b         c         d\n2  0.671399  0.101208 -0.181532  0.241273\n3  0.446172 -0.243316  0.051767  1.577318\n5  0.614758  0.075793 -0.451460 -0.012493\n', ""'e'"", '0   -0.335485\n1   -1.166658\n2   -0.385571\ndtype: float64\n', 'join', 'append', 'merge', 'e']"
66;2.0;4;12569730;;1;31;<python><wildcard><pandas>;Sum all columns with a wildcard name search using Python Pandas;"<p>I have a dataframe in python pandas with several columns taken from a CSV file.</p>

<p>For instance, data =:</p>

<pre><code>Day P1S1 P1S2 P1S3 P2S1 P2S2 P2S3
1   1    2    2    3    1    2
2   2    2    3    5    4    2
</code></pre>

<p>And what I need is to get the sum of all columns which name starts with P1... something like P1* with a wildcard.</p>

<p>Something like the following which gives an error:</p>

<blockquote>
  <p>P1Sum = data[""P1*""]</p>
</blockquote>

<p>Is there any why to do this with pandas?</p>
";6662.0;['Day P1S1 P1S2 P1S3 P2S1 P2S2 P2S3\n1   1    2    2    3    1    2\n2   2    2    3    5    4    2\n'];['Day P1S1 P1S2 P1S3 P2S1 P2S2 P2S3\n1   1    2    2    3    1    2\n2   2    2    3    5    4    2\n']
67;2.0;0;12589481;;1;30;<python><aggregate><pandas>;Python Pandas: Multiple aggregations of the same column;"<p>Given the following (totally overkill) data frame example</p>

<pre><code>df = pandas.DataFrame({
                       ""date"":[datetime.date(2012,x,1) for x in range(1,11)], 
                       ""returns"":0.05*np.random.randn(10), 
                       ""dummy"":np.repeat(1,10) 
                      })
</code></pre>

<p>is there an existing built-in way to apply two different aggregating functions to the same column, without having to call <code>agg</code> multiple times? </p>

<p>The syntactically wrong, but intuitively right, way to do it would be:</p>

<pre><code># Assume `function1` and `function2` are defined for aggregating.
df.groupby(""dummy"").agg({""returns"":function1, ""returns"":function2})
</code></pre>

<p>Obviously, Python doesn't allow duplicate keys. Is there any other manner for expressing the input to <code>agg</code>? Perhaps a list of tuples <code>[(column, function)]</code> would work better, to allow multiple functions applied to the same column? But it seems like it only accepts a dictionary.</p>

<p>Is there a workaround for this besides defining an auxiliary function that just applies both of the functions inside of it? (How would this work with aggregation anyway?)</p>
";11477.0;"['df = pandas.DataFrame({\n                       ""date"":[datetime.date(2012,x,1) for x in range(1,11)], \n                       ""returns"":0.05*np.random.randn(10), \n                       ""dummy"":np.repeat(1,10) \n                      })\n', '# Assume `function1` and `function2` are defined for aggregating.\ndf.groupby(""dummy"").agg({""returns"":function1, ""returns"":function2})\n']";"['df = pandas.DataFrame({\n                       ""date"":[datetime.date(2012,x,1) for x in range(1,11)], \n                       ""returns"":0.05*np.random.randn(10), \n                       ""dummy"":np.repeat(1,10) \n                      })\n', 'agg', '# Assume `function1` and `function2` are defined for aggregating.\ndf.groupby(""dummy"").agg({""returns"":function1, ""returns"":function2})\n', 'agg', '[(column, function)]']"
68;1.0;2;12604909;;1;28;<python><database><pandas>;Pandas: how to change all the values of a column?;"<p>I have a data frame with a column called <code>""Date""</code> and want all the values from this column to have the same value (the year only). Example:</p>

<pre><code>City     Date
Paris    01/04/2004
Lisbon   01/09/2004
Madrid   2004
Pekin    31/2004
</code></pre>

<p>What I want is:</p>

<pre><code>City     Date
Paris    2004
Lisbon   2004
Madrid   2004
Pekin    2004
</code></pre>

<p>Here is my code:</p>

<pre><code>fr61_70xls = pd.ExcelFile('AMADEUS FRANCE 1961-1970.xlsx')

#Here we import the individual sheets and clean the sheets    
years=(['1961','1962','1963','1964','1965','1966','1967','1968','1969','1970'])

fr={}

header=(['City','Country','NACE','Cons','Last_year','Op_Rev_EUR_Last_avail_yr','BvD_Indep_Indic','GUO_Name','Legal_status','Date_of_incorporation','Legal_status_date'])

for year in years:
    # save every sheet in variable fr['1961'], fr['1962'] and so on
    fr[year]=fr61_70xls.parse(year,header=0,parse_cols=10)
    fr[year].columns=header
    # drop the entire Legal status date column
    fr[year]=fr[year].drop(['Legal_status_date','Date_of_incorporation'],axis=1)
    # drop every row where GUO Name is empty
    fr[year]=fr[year].dropna(axis=0,how='all',subset=[['GUO_Name']])
    fr[year]=fr[year].set_index(['GUO_Name','Date_of_incorporation'])
</code></pre>

<p><em>It happens that in my DataFrames, called for example <code>fr['1961']</code> the values of <code>Date_of_incorporation</code> can be anything (strings, integer, and so on), so maybe it would be best to completely erase this column and then attach another column with only the year to the DataFrames?</em></p>
";53708.0;"['City     Date\nParis    01/04/2004\nLisbon   01/09/2004\nMadrid   2004\nPekin    31/2004\n', 'City     Date\nParis    2004\nLisbon   2004\nMadrid   2004\nPekin    2004\n', ""fr61_70xls = pd.ExcelFile('AMADEUS FRANCE 1961-1970.xlsx')\n\n#Here we import the individual sheets and clean the sheets    \nyears=(['1961','1962','1963','1964','1965','1966','1967','1968','1969','1970'])\n\nfr={}\n\nheader=(['City','Country','NACE','Cons','Last_year','Op_Rev_EUR_Last_avail_yr','BvD_Indep_Indic','GUO_Name','Legal_status','Date_of_incorporation','Legal_status_date'])\n\nfor year in years:\n    # save every sheet in variable fr['1961'], fr['1962'] and so on\n    fr[year]=fr61_70xls.parse(year,header=0,parse_cols=10)\n    fr[year].columns=header\n    # drop the entire Legal status date column\n    fr[year]=fr[year].drop(['Legal_status_date','Date_of_incorporation'],axis=1)\n    # drop every row where GUO Name is empty\n    fr[year]=fr[year].dropna(axis=0,how='all',subset=[['GUO_Name']])\n    fr[year]=fr[year].set_index(['GUO_Name','Date_of_incorporation'])\n""]";"['""Date""', 'City     Date\nParis    01/04/2004\nLisbon   01/09/2004\nMadrid   2004\nPekin    31/2004\n', 'City     Date\nParis    2004\nLisbon   2004\nMadrid   2004\nPekin    2004\n', ""fr61_70xls = pd.ExcelFile('AMADEUS FRANCE 1961-1970.xlsx')\n\n#Here we import the individual sheets and clean the sheets    \nyears=(['1961','1962','1963','1964','1965','1966','1967','1968','1969','1970'])\n\nfr={}\n\nheader=(['City','Country','NACE','Cons','Last_year','Op_Rev_EUR_Last_avail_yr','BvD_Indep_Indic','GUO_Name','Legal_status','Date_of_incorporation','Legal_status_date'])\n\nfor year in years:\n    # save every sheet in variable fr['1961'], fr['1962'] and so on\n    fr[year]=fr61_70xls.parse(year,header=0,parse_cols=10)\n    fr[year].columns=header\n    # drop the entire Legal status date column\n    fr[year]=fr[year].drop(['Legal_status_date','Date_of_incorporation'],axis=1)\n    # drop every row where GUO Name is empty\n    fr[year]=fr[year].dropna(axis=0,how='all',subset=[['GUO_Name']])\n    fr[year]=fr[year].set_index(['GUO_Name','Date_of_incorporation'])\n"", ""fr['1961']"", 'Date_of_incorporation']"
69;9.0;0;12680754;;1;38;<python><pandas><numpy><dataframe>;Split pandas dataframe string entry to separate rows;"<p>I have a <code>pandas dataframe</code> in which one column of text strings contains comma-separated values. I want to split each CSV field and create a new row per entry (assume that CSV are clean and need only be split on ','). For example, <code>a</code> should become <code>b</code>:</p>

<pre><code>In [7]: a
Out[7]: 
    var1  var2
0  a,b,c     1
1  d,e,f     2

In [8]: b
Out[8]: 
  var1  var2
0    a     1
1    b     1
2    c     1
3    d     2
4    e     2
5    f     2
</code></pre>

<p>So far, I have tried various simple functions, but the <code>.apply</code> method seems to only accept one row as return value when it is used on an axis, and I can't get <code>.transform</code> to work. Any suggestions would be much appreciated!</p>

<p>Example data: </p>

<pre><code>from pandas import DataFrame
import numpy as np
a = DataFrame([{'var1': 'a,b,c', 'var2': 1},
               {'var1': 'd,e,f', 'var2': 2}])
b = DataFrame([{'var1': 'a', 'var2': 1},
               {'var1': 'b', 'var2': 1},
               {'var1': 'c', 'var2': 1},
               {'var1': 'd', 'var2': 2},
               {'var1': 'e', 'var2': 2},
               {'var1': 'f', 'var2': 2}])
</code></pre>

<p>I know this won't work because we lose DataFrame meta-data by going through numpy, but it should give you a sense of what I tried to do: </p>

<pre><code>def fun(row):
    letters = row['var1']
    letters = letters.split(',')
    out = np.array([row] * len(letters))
    out['var1'] = letters
a['idx'] = range(a.shape[0])
z = a.groupby('idx')
z.transform(fun)
</code></pre>
";23089.0;"['In [7]: a\nOut[7]: \n    var1  var2\n0  a,b,c     1\n1  d,e,f     2\n\nIn [8]: b\nOut[8]: \n  var1  var2\n0    a     1\n1    b     1\n2    c     1\n3    d     2\n4    e     2\n5    f     2\n', ""from pandas import DataFrame\nimport numpy as np\na = DataFrame([{'var1': 'a,b,c', 'var2': 1},\n               {'var1': 'd,e,f', 'var2': 2}])\nb = DataFrame([{'var1': 'a', 'var2': 1},\n               {'var1': 'b', 'var2': 1},\n               {'var1': 'c', 'var2': 1},\n               {'var1': 'd', 'var2': 2},\n               {'var1': 'e', 'var2': 2},\n               {'var1': 'f', 'var2': 2}])\n"", ""def fun(row):\n    letters = row['var1']\n    letters = letters.split(',')\n    out = np.array([row] * len(letters))\n    out['var1'] = letters\na['idx'] = range(a.shape[0])\nz = a.groupby('idx')\nz.transform(fun)\n""]";"['pandas dataframe', 'a', 'b', 'In [7]: a\nOut[7]: \n    var1  var2\n0  a,b,c     1\n1  d,e,f     2\n\nIn [8]: b\nOut[8]: \n  var1  var2\n0    a     1\n1    b     1\n2    c     1\n3    d     2\n4    e     2\n5    f     2\n', '.apply', '.transform', ""from pandas import DataFrame\nimport numpy as np\na = DataFrame([{'var1': 'a,b,c', 'var2': 1},\n               {'var1': 'd,e,f', 'var2': 2}])\nb = DataFrame([{'var1': 'a', 'var2': 1},\n               {'var1': 'b', 'var2': 1},\n               {'var1': 'c', 'var2': 1},\n               {'var1': 'd', 'var2': 2},\n               {'var1': 'e', 'var2': 2},\n               {'var1': 'f', 'var2': 2}])\n"", ""def fun(row):\n    letters = row['var1']\n    letters = letters.split(',')\n    out = np.array([row] * len(letters))\n    out['var1'] = letters\na['idx'] = range(a.shape[0])\nz = a.groupby('idx')\nz.transform(fun)\n""]"
70;2.0;2;12725417;;1;21;<python><pandas>;Drop non-numeric columns from a pandas DataFrame;"<p>In my application I load text files that are structured as follows:</p>

<ul>
<li>First non numeric column (ID)</li>
<li>A number of non-numeric columns (strings)</li>
<li>A number of numeric columns (floats)</li>
</ul>

<p>The number of the non-numeric columns is variable. Currently I load the data into a DataFrame like this:</p>

<pre><code>source = pandas.read_table(inputfile, index_col=0)
</code></pre>

<p>I would like to drop all non-numeric columns in one fell swoop, without knowing their names or indices, since this could be doable reading their dtype. Is this possible with pandas or do I have to cook up something on my own?</p>
";9057.0;['source = pandas.read_table(inputfile, index_col=0)\n'];['source = pandas.read_table(inputfile, index_col=0)\n']
71;1.0;3;12741092;;1;22;<python><dataframe><pandas>;Pandas DataFrame: apply function to all columns;"<p>I can use <code>.map(func)</code> on any column in a df, like:</p>

<pre><code>df=DataFrame({'a':[1,2,3,4,5,6],'b':[2,3,4,5,6,7]})

df['a']=df['a'].map(lambda x: x &gt; 1)
</code></pre>

<p>I could also:</p>

<pre><code>df['a'],df['b']=df['a'].map(lambda x: x &gt; 1),df['b'].map(lambda x: x &gt; 1)
</code></pre>

<p>Is there a more pythonic way to apply a function to all columns or the entire frame (without a loop)?</p>
";13636.0;"[""df=DataFrame({'a':[1,2,3,4,5,6],'b':[2,3,4,5,6,7]})\n\ndf['a']=df['a'].map(lambda x: x > 1)\n"", ""df['a'],df['b']=df['a'].map(lambda x: x > 1),df['b'].map(lambda x: x > 1)\n""]";"['.map(func)', ""df=DataFrame({'a':[1,2,3,4,5,6],'b':[2,3,4,5,6,7]})\n\ndf['a']=df['a'].map(lambda x: x > 1)\n"", ""df['a'],df['b']=df['a'].map(lambda x: x > 1),df['b'].map(lambda x: x > 1)\n""]"
72;3.0;1;12860421;;1;24;<python><pandas><pivot-table>;Python Pandas : pivot table with aggfunc = count unique distinct;"<pre><code>df2 = pd.DataFrame({'X' : ['X1', 'X1', 'X1', 'X1'], 'Y' : ['Y2','Y1','Y1','Y1'], 'Z' : ['Z3','Z1','Z1','Z2']})

    X   Y   Z
0  X1  Y2  Z3
1  X1  Y1  Z1
2  X1  Y1  Z1
3  X1  Y1  Z2

g=df2.groupby('X')

pd.pivot_table(g, values='X', rows='Y', cols='Z', margins=False, aggfunc='count')
</code></pre>

<blockquote>
  <p>Traceback (most recent call last): ... AttributeError: 'Index' object
  has no attribute 'index'</p>
</blockquote>

<p>How do I get a Pivot Table with <strong>counts of unique values</strong> of one DataFrame column for two other columns?<br>
Is there <code>aggfunc</code> for count unique? Should I be using <code>np.bincount()</code>?</p>

<p>NB. I am aware of 'Series' <code>values_counts()</code> however I need a pivot table.</p>

<hr>

<p>EDIT: The output should be:</p>

<pre><code>Z   Z1  Z2  Z3
Y             
Y1   1   1 NaN
Y2 NaN NaN   1
</code></pre>
";31611.0;"[""df2 = pd.DataFrame({'X' : ['X1', 'X1', 'X1', 'X1'], 'Y' : ['Y2','Y1','Y1','Y1'], 'Z' : ['Z3','Z1','Z1','Z2']})\n\n    X   Y   Z\n0  X1  Y2  Z3\n1  X1  Y1  Z1\n2  X1  Y1  Z1\n3  X1  Y1  Z2\n\ng=df2.groupby('X')\n\npd.pivot_table(g, values='X', rows='Y', cols='Z', margins=False, aggfunc='count')\n"", 'Z   Z1  Z2  Z3\nY             \nY1   1   1 NaN\nY2 NaN NaN   1\n']";"[""df2 = pd.DataFrame({'X' : ['X1', 'X1', 'X1', 'X1'], 'Y' : ['Y2','Y1','Y1','Y1'], 'Z' : ['Z3','Z1','Z1','Z2']})\n\n    X   Y   Z\n0  X1  Y2  Z3\n1  X1  Y1  Z1\n2  X1  Y1  Z1\n3  X1  Y1  Z2\n\ng=df2.groupby('X')\n\npd.pivot_table(g, values='X', rows='Y', cols='Z', margins=False, aggfunc='count')\n"", 'aggfunc', 'np.bincount()', 'values_counts()', 'Z   Z1  Z2  Z3\nY             \nY1   1   1 NaN\nY2 NaN NaN   1\n']"
73;5.0;4;12867178;;1;26;<python><pandas>;pandas: count things;"<p>In the following, male_trips is a big pandas data frame and stations is a small pandas data frame. For each station id I'd like to know how many male trips took place. The following does the job, but takes a long time:</p>

<pre><code>mc = [ sum( male_trips['start_station_id'] == id ) for id in stations['id'] ]
</code></pre>

<p>how should I go about this instead?</p>

<hr>

<p>Update! So there were two main approaches: <code>groupby()</code> followed by <code>size()</code>, and the simpler <code>.value_counts()</code>. I did a quick <code>timeit</code>, and the <code>groupby</code> approach wins by quite a large margin! Here is the code:</p>

<pre><code>from timeit import Timer
setup = ""import pandas; male_trips=pandas.load('maletrips')""
a  = ""male_trips.start_station_id.value_counts()""
b = ""male_trips.groupby('start_station_id').size()""
Timer(a,setup).timeit(100)
Timer(b,setup).timeit(100)
</code></pre>

<p>and here is the result:</p>

<pre><code>In [4]: Timer(a,setup).timeit(100) # &lt;- this is value_counts
Out[4]: 9.709594964981079

In [5]: Timer(b,setup).timeit(100) # &lt;- this is groupby / size
Out[5]: 1.5574288368225098
</code></pre>

<p>Note that, at this speed, for exploring data <em>typing</em> value_counts is marginally quicker and less remembering!</p>
";42829.0;"[""mc = [ sum( male_trips['start_station_id'] == id ) for id in stations['id'] ]\n"", 'from timeit import Timer\nsetup = ""import pandas; male_trips=pandas.load(\'maletrips\')""\na  = ""male_trips.start_station_id.value_counts()""\nb = ""male_trips.groupby(\'start_station_id\').size()""\nTimer(a,setup).timeit(100)\nTimer(b,setup).timeit(100)\n', 'In [4]: Timer(a,setup).timeit(100) # <- this is value_counts\nOut[4]: 9.709594964981079\n\nIn [5]: Timer(b,setup).timeit(100) # <- this is groupby / size\nOut[5]: 1.5574288368225098\n']";"[""mc = [ sum( male_trips['start_station_id'] == id ) for id in stations['id'] ]\n"", 'groupby()', 'size()', '.value_counts()', 'timeit', 'groupby', 'from timeit import Timer\nsetup = ""import pandas; male_trips=pandas.load(\'maletrips\')""\na  = ""male_trips.start_station_id.value_counts()""\nb = ""male_trips.groupby(\'start_station_id\').size()""\nTimer(a,setup).timeit(100)\nTimer(b,setup).timeit(100)\n', 'In [4]: Timer(a,setup).timeit(100) # <- this is value_counts\nOut[4]: 9.709594964981079\n\nIn [5]: Timer(b,setup).timeit(100) # <- this is groupby / size\nOut[5]: 1.5574288368225098\n']"
74;2.0;3;12877189;;1;21;<python><numpy><pandas>;float64 with pandas to_csv;"<p>I'm reading a CSV with float numbers like this:</p>

<pre><code>Bob,0.085
Alice,0.005
</code></pre>

<p>And import into a dataframe, and write this dataframe to a new place</p>

<pre><code>df = pd.read_csv(orig)
df.to_csv(pandasfile)
</code></pre>

<p>Now this <code>pandasfile</code> has:</p>

<pre><code>Bob,0.085000000000000006
Alice,0.0050000000000000001
</code></pre>

<p>What happen? maybe I have to cast to a different type like float32 or something? </p>

<p>Im using <strong>pandas 0.9.0</strong> and <strong>numpy 1.6.2</strong>.</p>
";18798.0;['Bob,0.085\nAlice,0.005\n', 'df = pd.read_csv(orig)\ndf.to_csv(pandasfile)\n', 'Bob,0.085000000000000006\nAlice,0.0050000000000000001\n'];['Bob,0.085\nAlice,0.005\n', 'df = pd.read_csv(orig)\ndf.to_csv(pandasfile)\n', 'pandasfile', 'Bob,0.085000000000000006\nAlice,0.0050000000000000001\n']
75;1.0;3;12945971;;1;70;<python><matplotlib><pandas>;Pandas timeseries plot setting x-axis major and minor ticks and labels;"<p>I want to be able to set the major and minor xticks and their labels for a time series graph plotted from a Pandas time series object.  </p>

<p>The Pandas 0.9 ""what's new"" page says: </p>

<blockquote>
  <p>""you can either use to_pydatetime or register a converter for the
  Timestamp type""</p>
</blockquote>

<p>but I can't work out how to do that so that I can use the matplotlib <code>ax.xaxis.set_major_locator</code> and <code>ax.xaxis.set_major_formatter</code> (and minor) commands.</p>

<p>If I use them without converting the pandas times, the x-axis ticks and labels end up wrong.</p>

<p>By using the 'xticks' parameter I can pass the major ticks to pandas.plot, and then set the major tick labels. I can't work out how to do the minor ticks using this approach. (I can set the labels on the default minor ticks set by pandas.plot)</p>

<p>Here is my test code:</p>

<pre class=""lang-py prettyprint-override""><code>import pandas
print 'pandas.__version__ is ', pandas.__version__
print 'matplotlib.__version__ is ', matplotlib.__version__    

dStart = datetime.datetime(2011,5,1) # 1 May
dEnd = datetime.datetime(2011,7,1) # 1 July    

dateIndex = pandas.date_range(start=dStart, end=dEnd, freq='D')
print ""1 May to 1 July 2011"", dateIndex      

testSeries = pandas.Series(data=np.random.randn(len(dateIndex)),
                           index=dateIndex)    

ax = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)
testSeries.plot(ax=ax, style='v-', label='first line')    

# using MatPlotLib date time locators and formatters doesn't work with new
# pandas datetime index
ax.xaxis.set_minor_locator(matplotlib.dates.WeekdayLocator(byweekday=(1),
                                                           interval=1))
ax.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter('%d\n%a'))
ax.xaxis.grid(True, which=""minor"")
ax.xaxis.grid(False, which=""major"")
ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('\n\n\n%b%Y'))
plt.show()    

# set the major xticks and labels through pandas
ax2 = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)
xticks = pandas.date_range(start=dStart, end=dEnd, freq='W-Tue')
print ""xticks: "", xticks
testSeries.plot(ax=ax2, style='-v', label='second line',
                xticks=xticks.to_pydatetime())
ax2.set_xticklabels([x.strftime('%a\n%d\n%h\n%Y') for x in xticks]);
# set the text of the first few minor ticks created by pandas.plot
#    ax2.set_xticklabels(['a','b','c','d','e'], minor=True)
# remove the minor xtick labels set by pandas.plot 
ax2.set_xticklabels([], minor=True)
# turn the minor ticks created by pandas.plot off 
# plt.minorticks_off()
plt.show()
print testSeries['6/4/2011':'6/7/2011']
</code></pre>

<p>and its output:</p>

<pre><code>pandas.__version__ is  0.9.1.dev-3de54ae
matplotlib.__version__ is  1.1.1
1 May to 1 July 2011 &lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2011-05-01 00:00:00, ..., 2011-07-01 00:00:00]
Length: 62, Freq: D, Timezone: None
</code></pre>

<p><img src=""https://i.stack.imgur.com/8ndjM.png"" alt=""Graph with strange dates on xaxis""></p>

<pre><code>xticks:  &lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2011-05-03 00:00:00, ..., 2011-06-28 00:00:00]
Length: 9, Freq: W-TUE, Timezone: None
</code></pre>

<p><img src=""https://i.stack.imgur.com/hQy8v.png"" alt=""Graph with correct dates""></p>

<pre><code>2011-06-04   -0.199393
2011-06-05   -0.043118
2011-06-06    0.477771
2011-06-07   -0.033207
Freq: D
</code></pre>

<p><strong>Update:</strong> I've been able to get closer to the layout I wanted by using a loop to build the major xtick labels:</p>

<pre><code># only show month for first label in month
month = dStart.month - 1
xticklabels = []
for x in xticks:
    if  month != x.month :
        xticklabels.append(x.strftime('%d\n%a\n%h'))
        month = x.month
    else:
        xticklabels.append(x.strftime('%d\n%a'))
</code></pre>

<p>However, this is a bit like doing the x-axis using <code>ax.annotate</code>: possible but not ideal.</p>
";63942.0;"['import pandas\nprint \'pandas.__version__ is \', pandas.__version__\nprint \'matplotlib.__version__ is \', matplotlib.__version__    \n\ndStart = datetime.datetime(2011,5,1) # 1 May\ndEnd = datetime.datetime(2011,7,1) # 1 July    \n\ndateIndex = pandas.date_range(start=dStart, end=dEnd, freq=\'D\')\nprint ""1 May to 1 July 2011"", dateIndex      \n\ntestSeries = pandas.Series(data=np.random.randn(len(dateIndex)),\n                           index=dateIndex)    \n\nax = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\ntestSeries.plot(ax=ax, style=\'v-\', label=\'first line\')    \n\n# using MatPlotLib date time locators and formatters doesn\'t work with new\n# pandas datetime index\nax.xaxis.set_minor_locator(matplotlib.dates.WeekdayLocator(byweekday=(1),\n                                                           interval=1))\nax.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter(\'%d\\n%a\'))\nax.xaxis.grid(True, which=""minor"")\nax.xaxis.grid(False, which=""major"")\nax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\'\\n\\n\\n%b%Y\'))\nplt.show()    \n\n# set the major xticks and labels through pandas\nax2 = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\nxticks = pandas.date_range(start=dStart, end=dEnd, freq=\'W-Tue\')\nprint ""xticks: "", xticks\ntestSeries.plot(ax=ax2, style=\'-v\', label=\'second line\',\n                xticks=xticks.to_pydatetime())\nax2.set_xticklabels([x.strftime(\'%a\\n%d\\n%h\\n%Y\') for x in xticks]);\n# set the text of the first few minor ticks created by pandas.plot\n#    ax2.set_xticklabels([\'a\',\'b\',\'c\',\'d\',\'e\'], minor=True)\n# remove the minor xtick labels set by pandas.plot \nax2.set_xticklabels([], minor=True)\n# turn the minor ticks created by pandas.plot off \n# plt.minorticks_off()\nplt.show()\nprint testSeries[\'6/4/2011\':\'6/7/2011\']\n', ""pandas.__version__ is  0.9.1.dev-3de54ae\nmatplotlib.__version__ is  1.1.1\n1 May to 1 July 2011 <class 'pandas.tseries.index.DatetimeIndex'>\n[2011-05-01 00:00:00, ..., 2011-07-01 00:00:00]\nLength: 62, Freq: D, Timezone: None\n"", ""xticks:  <class 'pandas.tseries.index.DatetimeIndex'>\n[2011-05-03 00:00:00, ..., 2011-06-28 00:00:00]\nLength: 9, Freq: W-TUE, Timezone: None\n"", '2011-06-04   -0.199393\n2011-06-05   -0.043118\n2011-06-06    0.477771\n2011-06-07   -0.033207\nFreq: D\n', ""# only show month for first label in month\nmonth = dStart.month - 1\nxticklabels = []\nfor x in xticks:\n    if  month != x.month :\n        xticklabels.append(x.strftime('%d\\n%a\\n%h'))\n        month = x.month\n    else:\n        xticklabels.append(x.strftime('%d\\n%a'))\n""]";"['ax.xaxis.set_major_locator', 'ax.xaxis.set_major_formatter', 'import pandas\nprint \'pandas.__version__ is \', pandas.__version__\nprint \'matplotlib.__version__ is \', matplotlib.__version__    \n\ndStart = datetime.datetime(2011,5,1) # 1 May\ndEnd = datetime.datetime(2011,7,1) # 1 July    \n\ndateIndex = pandas.date_range(start=dStart, end=dEnd, freq=\'D\')\nprint ""1 May to 1 July 2011"", dateIndex      \n\ntestSeries = pandas.Series(data=np.random.randn(len(dateIndex)),\n                           index=dateIndex)    \n\nax = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\ntestSeries.plot(ax=ax, style=\'v-\', label=\'first line\')    \n\n# using MatPlotLib date time locators and formatters doesn\'t work with new\n# pandas datetime index\nax.xaxis.set_minor_locator(matplotlib.dates.WeekdayLocator(byweekday=(1),\n                                                           interval=1))\nax.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter(\'%d\\n%a\'))\nax.xaxis.grid(True, which=""minor"")\nax.xaxis.grid(False, which=""major"")\nax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\'\\n\\n\\n%b%Y\'))\nplt.show()    \n\n# set the major xticks and labels through pandas\nax2 = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\nxticks = pandas.date_range(start=dStart, end=dEnd, freq=\'W-Tue\')\nprint ""xticks: "", xticks\ntestSeries.plot(ax=ax2, style=\'-v\', label=\'second line\',\n                xticks=xticks.to_pydatetime())\nax2.set_xticklabels([x.strftime(\'%a\\n%d\\n%h\\n%Y\') for x in xticks]);\n# set the text of the first few minor ticks created by pandas.plot\n#    ax2.set_xticklabels([\'a\',\'b\',\'c\',\'d\',\'e\'], minor=True)\n# remove the minor xtick labels set by pandas.plot \nax2.set_xticklabels([], minor=True)\n# turn the minor ticks created by pandas.plot off \n# plt.minorticks_off()\nplt.show()\nprint testSeries[\'6/4/2011\':\'6/7/2011\']\n', ""pandas.__version__ is  0.9.1.dev-3de54ae\nmatplotlib.__version__ is  1.1.1\n1 May to 1 July 2011 <class 'pandas.tseries.index.DatetimeIndex'>\n[2011-05-01 00:00:00, ..., 2011-07-01 00:00:00]\nLength: 62, Freq: D, Timezone: None\n"", ""xticks:  <class 'pandas.tseries.index.DatetimeIndex'>\n[2011-05-03 00:00:00, ..., 2011-06-28 00:00:00]\nLength: 9, Freq: W-TUE, Timezone: None\n"", '2011-06-04   -0.199393\n2011-06-05   -0.043118\n2011-06-06    0.477771\n2011-06-07   -0.033207\nFreq: D\n', ""# only show month for first label in month\nmonth = dStart.month - 1\nxticklabels = []\nfor x in xticks:\n    if  month != x.month :\n        xticklabels.append(x.strftime('%d\\n%a\\n%h'))\n        month = x.month\n    else:\n        xticklabels.append(x.strftime('%d\\n%a'))\n"", 'ax.annotate']"
76;6.0;0;13003051;;1;21;<pandas>;How do I exclude a few columns from a DataFrame plot?;"<p>I have a DataFrame with about 25 columns, several of which hold data unsuitable for plotting.  DataFrame.hist() throws errors on those.  How can I specify that those columns should be excluded from the plotting?</p>
";15239.0;[];[]
77;3.0;0;13021654;;1;54;<python><dataframe><pandas>;Retrieving column index from column name in python pandas;"<p>In R when you need to retrieve a column index based on the name of the column you could do</p>

<pre><code>idx &lt;- which(names(my_data)==my_colum_name)
</code></pre>

<p>Is there a way to do the same with pandas dataframes?</p>
";40205.0;['idx <- which(names(my_data)==my_colum_name)\n'];['idx <- which(names(my_data)==my_colum_name)\n']
78;4.0;0;13035764;;1;90;<python><pandas>;Remove rows with duplicate indices (Pandas DataFrame and TimeSeries);"<p>I'm reading some automated weather data from the web. The observations occur every 5 minutes and are compiled into monthly files for each weather station. Once I'm done parsing a file, the DataFrame looks something like this:</p>

<pre><code>                      Sta  Precip1hr  Precip5min  Temp  DewPnt  WindSpd  WindDir  AtmPress
Date                                                                                      
2001-01-01 00:00:00  KPDX          0           0     4       3        0        0     30.31
2001-01-01 00:05:00  KPDX          0           0     4       3        0        0     30.30
2001-01-01 00:10:00  KPDX          0           0     4       3        4       80     30.30
2001-01-01 00:15:00  KPDX          0           0     3       2        5       90     30.30
2001-01-01 00:20:00  KPDX          0           0     3       2       10      110     30.28
</code></pre>

<p>The problem I'm having is that sometimes a scientist goes back and corrects observations -- not by editing the erroneous rows, but by appending a duplicate row to the end of a file. Simple example of such a case is illustrated below:</p>

<pre><code>import pandas 
import datetime
startdate = datetime.datetime(2001, 1, 1, 0, 0)
enddate = datetime.datetime(2001, 1, 1, 5, 0)
index = pandas.DatetimeIndex(start=startdate, end=enddate, freq='H')
data = {'A' : range(6), 'B' : range(6)}
data1 = {'A' : [20, -30, 40], 'B' : [-50, 60, -70]}
df1 = pandas.DataFrame(data=data, index=index)
df2 = pandas.DataFrame(data=data1, index=index[:3])
df3 = df1.append(df2)
df3
                       A   B
2001-01-01 00:00:00   20 -50
2001-01-01 01:00:00  -30  60
2001-01-01 02:00:00   40 -70
2001-01-01 03:00:00    3   3
2001-01-01 04:00:00    4   4
2001-01-01 05:00:00    5   5
2001-01-01 00:00:00    0   0
2001-01-01 01:00:00    1   1
2001-01-01 02:00:00    2   2
</code></pre>

<p>And so I need <code>df3</code> to evenutally become:</p>

<pre><code>                       A   B
2001-01-01 00:00:00    0   0
2001-01-01 01:00:00    1   1
2001-01-01 02:00:00    2   2
2001-01-01 03:00:00    3   3
2001-01-01 04:00:00    4   4
2001-01-01 05:00:00    5   5
</code></pre>

<p>I thought that adding a column of row numbers (<code>df3['rownum'] = range(df3.shape[0])</code>) would help me select out the bottom-most row for any value of the <code>DatetimeIndex</code>, but I am stuck on figuring out the <code>group_by</code> or <code>pivot</code> (or ???) statements to make that work.</p>
";67548.0;"['                      Sta  Precip1hr  Precip5min  Temp  DewPnt  WindSpd  WindDir  AtmPress\nDate                                                                                      \n2001-01-01 00:00:00  KPDX          0           0     4       3        0        0     30.31\n2001-01-01 00:05:00  KPDX          0           0     4       3        0        0     30.30\n2001-01-01 00:10:00  KPDX          0           0     4       3        4       80     30.30\n2001-01-01 00:15:00  KPDX          0           0     3       2        5       90     30.30\n2001-01-01 00:20:00  KPDX          0           0     3       2       10      110     30.28\n', ""import pandas \nimport datetime\nstartdate = datetime.datetime(2001, 1, 1, 0, 0)\nenddate = datetime.datetime(2001, 1, 1, 5, 0)\nindex = pandas.DatetimeIndex(start=startdate, end=enddate, freq='H')\ndata = {'A' : range(6), 'B' : range(6)}\ndata1 = {'A' : [20, -30, 40], 'B' : [-50, 60, -70]}\ndf1 = pandas.DataFrame(data=data, index=index)\ndf2 = pandas.DataFrame(data=data1, index=index[:3])\ndf3 = df1.append(df2)\ndf3\n                       A   B\n2001-01-01 00:00:00   20 -50\n2001-01-01 01:00:00  -30  60\n2001-01-01 02:00:00   40 -70\n2001-01-01 03:00:00    3   3\n2001-01-01 04:00:00    4   4\n2001-01-01 05:00:00    5   5\n2001-01-01 00:00:00    0   0\n2001-01-01 01:00:00    1   1\n2001-01-01 02:00:00    2   2\n"", '                       A   B\n2001-01-01 00:00:00    0   0\n2001-01-01 01:00:00    1   1\n2001-01-01 02:00:00    2   2\n2001-01-01 03:00:00    3   3\n2001-01-01 04:00:00    4   4\n2001-01-01 05:00:00    5   5\n']";"['                      Sta  Precip1hr  Precip5min  Temp  DewPnt  WindSpd  WindDir  AtmPress\nDate                                                                                      \n2001-01-01 00:00:00  KPDX          0           0     4       3        0        0     30.31\n2001-01-01 00:05:00  KPDX          0           0     4       3        0        0     30.30\n2001-01-01 00:10:00  KPDX          0           0     4       3        4       80     30.30\n2001-01-01 00:15:00  KPDX          0           0     3       2        5       90     30.30\n2001-01-01 00:20:00  KPDX          0           0     3       2       10      110     30.28\n', ""import pandas \nimport datetime\nstartdate = datetime.datetime(2001, 1, 1, 0, 0)\nenddate = datetime.datetime(2001, 1, 1, 5, 0)\nindex = pandas.DatetimeIndex(start=startdate, end=enddate, freq='H')\ndata = {'A' : range(6), 'B' : range(6)}\ndata1 = {'A' : [20, -30, 40], 'B' : [-50, 60, -70]}\ndf1 = pandas.DataFrame(data=data, index=index)\ndf2 = pandas.DataFrame(data=data1, index=index[:3])\ndf3 = df1.append(df2)\ndf3\n                       A   B\n2001-01-01 00:00:00   20 -50\n2001-01-01 01:00:00  -30  60\n2001-01-01 02:00:00   40 -70\n2001-01-01 03:00:00    3   3\n2001-01-01 04:00:00    4   4\n2001-01-01 05:00:00    5   5\n2001-01-01 00:00:00    0   0\n2001-01-01 01:00:00    1   1\n2001-01-01 02:00:00    2   2\n"", 'df3', '                       A   B\n2001-01-01 00:00:00    0   0\n2001-01-01 01:00:00    1   1\n2001-01-01 02:00:00    2   2\n2001-01-01 03:00:00    3   3\n2001-01-01 04:00:00    4   4\n2001-01-01 05:00:00    5   5\n', ""df3['rownum'] = range(df3.shape[0])"", 'DatetimeIndex', 'group_by', 'pivot']"
79;2.0;4;13114512;;1;24;<python><pandas>;Calculating difference between two rows in Python / Pandas;"<p>In python, how can I reference previous row and calculate something against it?  Specifically, I am working with <code>dataframes</code> in <code>pandas</code> - I have a data frame full of stock price information that looks like this:</p>

<pre><code>           Date   Close  Adj Close
251  2011-01-03  147.48     143.25
250  2011-01-04  147.64     143.41
249  2011-01-05  147.05     142.83
248  2011-01-06  148.66     144.40
247  2011-01-07  147.93     143.69
</code></pre>

<p>Here is how I created this dataframe:</p>

<pre><code>import pandas

url = 'http://ichart.finance.yahoo.com/table.csv?s=IBM&amp;a=00&amp;b=1&amp;c=2011&amp;d=11&amp;e=31&amp;f=2011&amp;g=d&amp;ignore=.csv'
data = data = pandas.read_csv(url)

## now I sorted the data frame ascending by date 
data = data.sort(columns='Date')
</code></pre>

<p>Starting with row number 2, or in this case, I guess it's 250 (PS - is that the index?), I want to calculate the difference between 2011-01-03 and 2011-01-04, for every entry in this dataframe.  I believe the appropriate way is to write a function that takes the current row, then figures out the previous row, and calculates the difference between them, the use the <code>pandas</code> <code>apply</code> function to update the dataframe with the value.  </p>

<p>Is that the right approach?  If so, should I be using the index to determine the difference?  (note - I'm still in python beginner mode, so index may not be the right term, nor even the correct way to implement this)</p>
";31121.0;"['           Date   Close  Adj Close\n251  2011-01-03  147.48     143.25\n250  2011-01-04  147.64     143.41\n249  2011-01-05  147.05     142.83\n248  2011-01-06  148.66     144.40\n247  2011-01-07  147.93     143.69\n', ""import pandas\n\nurl = 'http://ichart.finance.yahoo.com/table.csv?s=IBM&a=00&b=1&c=2011&d=11&e=31&f=2011&g=d&ignore=.csv'\ndata = data = pandas.read_csv(url)\n\n## now I sorted the data frame ascending by date \ndata = data.sort(columns='Date')\n""]";"['dataframes', 'pandas', '           Date   Close  Adj Close\n251  2011-01-03  147.48     143.25\n250  2011-01-04  147.64     143.41\n249  2011-01-05  147.05     142.83\n248  2011-01-06  148.66     144.40\n247  2011-01-07  147.93     143.69\n', ""import pandas\n\nurl = 'http://ichart.finance.yahoo.com/table.csv?s=IBM&a=00&b=1&c=2011&d=11&e=31&f=2011&g=d&ignore=.csv'\ndata = data = pandas.read_csv(url)\n\n## now I sorted the data frame ascending by date \ndata = data.sort(columns='Date')\n"", 'pandas', 'apply']"
80;3.0;1;13129618;;1;33;<python><pandas><numpy><matplotlib>;Histogram values of a Pandas Series;"<p>I have some values in a Python Pandas Series (type: pandas.core.series.Series)</p>

<pre><code>In [1]: series = pd.Series([0.0,950.0,-70.0,812.0,0.0,-90.0,0.0,0.0,-90.0,0.0,-64.0,208.0,0.0,-90.0,0.0,-80.0,0.0,0.0,-80.0,-48.0,840.0,-100.0,190.0,130.0,-100.0,-100.0,0.0,-50.0,0.0,-100.0,-100.0,0.0,-90.0,0.0,-90.0,-90.0,63.0,-90.0,0.0,0.0,-90.0,-80.0,0.0,])

In [2]: series.min()
Out[2]: -100.0

In [3]: series.max()
Out[3]: 950.0
</code></pre>

<p>I would like to get values of histogram (not necessary plotting histogram)... I just need to get the frequency for each interval.</p>

<p>Let's say that my intervals are going from [-200; -150] to [950; 1000]</p>

<p>so lower bounds are</p>

<pre><code>lwb = range(-200,1000,50)
</code></pre>

<p>and upper bounds are</p>

<pre><code>upb = range(-150,1050,50)
</code></pre>

<p>I don't know how to get frequency (the number of values that are inside each interval) now...
I'm sure that defining lwb and upb is not necessary... but I don't know what
function I should use to perform this!
(after diving in Pandas doc, I think <code>cut</code> function can help me because it's a discretization problem... but I'm don't understand how to use it)</p>

<p>After being able to do this, I will have a look at the way to display histogram (but that's an other problem)</p>
";23683.0;['In [1]: series = pd.Series([0.0,950.0,-70.0,812.0,0.0,-90.0,0.0,0.0,-90.0,0.0,-64.0,208.0,0.0,-90.0,0.0,-80.0,0.0,0.0,-80.0,-48.0,840.0,-100.0,190.0,130.0,-100.0,-100.0,0.0,-50.0,0.0,-100.0,-100.0,0.0,-90.0,0.0,-90.0,-90.0,63.0,-90.0,0.0,0.0,-90.0,-80.0,0.0,])\n\nIn [2]: series.min()\nOut[2]: -100.0\n\nIn [3]: series.max()\nOut[3]: 950.0\n', 'lwb = range(-200,1000,50)\n', 'upb = range(-150,1050,50)\n'];['In [1]: series = pd.Series([0.0,950.0,-70.0,812.0,0.0,-90.0,0.0,0.0,-90.0,0.0,-64.0,208.0,0.0,-90.0,0.0,-80.0,0.0,0.0,-80.0,-48.0,840.0,-100.0,190.0,130.0,-100.0,-100.0,0.0,-50.0,0.0,-100.0,-100.0,0.0,-90.0,0.0,-90.0,-90.0,63.0,-90.0,0.0,0.0,-90.0,-80.0,0.0,])\n\nIn [2]: series.min()\nOut[2]: -100.0\n\nIn [3]: series.max()\nOut[3]: 950.0\n', 'lwb = range(-200,1000,50)\n', 'upb = range(-150,1050,50)\n', 'cut']
81;18.0;2;13148429;;1;286;<python><pandas>;How to change the order of DataFrame columns?;"<p>I have the following <code>DataFrame</code> (<code>df</code>):</p>

<pre><code>import numpy as np
import pandas as pd

df = pd.DataFrame(np.random.rand(10, 5))
</code></pre>

<p>I add more column(s) by assignment:</p>

<pre><code>df['mean'] = df.mean(1)
</code></pre>

<p>How can I move the column <code>mean</code> to the front, i.e. set it as first column leaving the order of the other columns untouched?</p>
";187524.0;"['import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.rand(10, 5))\n', ""df['mean'] = df.mean(1)\n""]";"['DataFrame', 'df', 'import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.rand(10, 5))\n', ""df['mean'] = df.mean(1)\n"", 'mean']"
82;2.0;5;13167391;;1;25;<python><pandas>;filtering grouped df in pandas;"<p>I am creating a <code>groupby</code> object from a Pandas <code>DataFrame</code> and want to select out all the groups with > 1 size.</p>

<p>The following doesn't seem to work:</p>

<pre><code>grouped[grouped.size &gt; 1 ]
</code></pre>

<p>Also, how can one filter out certain values from a grouped <code>DataFrame</code>? For example, how could I remove all the rows from <code>grouped</code> where the column <code>'name'</code> has a value <code>'foo'</code> or <code>'bar'</code>?</p>

<p><strong>Contrived Example:</strong></p>

<pre><code>df = pandas.DataFrame({'A': ['foo','bar','foo','foo'],
                       'B': range(4)})
grouped = df.groupby('A')
</code></pre>

<p>I need the <code>groupby</code> object after removing the groups that have a group size &lt;= 1.</p>

<p>I tried the following, which didn't work:</p>

<pre><code>grouped[grouped.size() &gt; 1]
</code></pre>

<p>I expected:</p>

<pre><code>A
foo 0
    2
    3
</code></pre>

<p>I am not sure how indexing/slicing works for the <code>grouped</code> object.</p>
";14412.0;"['grouped[grouped.size > 1 ]\n', ""df = pandas.DataFrame({'A': ['foo','bar','foo','foo'],\n                       'B': range(4)})\ngrouped = df.groupby('A')\n"", 'grouped[grouped.size() > 1]\n', 'A\nfoo 0\n    2\n    3\n']";"['groupby', 'DataFrame', 'grouped[grouped.size > 1 ]\n', 'DataFrame', 'grouped', ""'name'"", ""'foo'"", ""'bar'"", ""df = pandas.DataFrame({'A': ['foo','bar','foo','foo'],\n                       'B': range(4)})\ngrouped = df.groupby('A')\n"", 'groupby', 'grouped[grouped.size() > 1]\n', 'A\nfoo 0\n    2\n    3\n', 'grouped']"
83;7.0;0;13187778;;1;92;<python><arrays><numpy><pandas><type-conversion>;Convert pandas dataframe to numpy array, preserving index;"<p>I am interested in knowing how to convert a pandas dataframe into a numpy array, including the index, and set the dtypes.</p>

<p>dataframe:</p>

<pre><code>label   A    B    C
ID                                 
1   NaN  0.2  NaN
2   NaN  NaN  0.5
3   NaN  0.2  0.5
4   0.1  0.2  NaN
5   0.1  0.2  0.5
6   0.1  NaN  0.5
7   0.1  NaN  NaN
</code></pre>

<p>convert df to array returns:</p>

<pre><code>array([[ nan,  0.2,  nan],
       [ nan,  nan,  0.5],
       [ nan,  0.2,  0.5],
       [ 0.1,  0.2,  nan],
       [ 0.1,  0.2,  0.5],
       [ 0.1,  nan,  0.5],
       [ 0.1,  nan,  nan]])
</code></pre>

<p>However, I would like:</p>

<pre><code>array([[ 1, nan,  0.2,  nan],
       [ 2, nan,  nan,  0.5],
       [ 3, nan,  0.2,  0.5],
       [ 4, 0.1,  0.2,  nan],
       [ 5, 0.1,  0.2,  0.5],
       [ 6, 0.1,  nan,  0.5],
       [ 7, 0.1,  nan,  nan]],
     dtype=[('ID', '&lt;i4'), ('A', '&lt;f8'), ('B', '&lt;f8'), ('B', '&lt;f8')])
</code></pre>

<p>(or similar)</p>

<p>Any suggestions on how to accomplish this? (I don't know if I need 1D or 2D array at this point.) I've seen a few posts that touch on this, but nothing dealing specifically with the dataframe.index.</p>

<p>I am writing the dataframe disk using to_csv (and reading it back in to create array) as a workaround, but would prefer something more eloquent than my new-to-pandas kludging. </p>
";161037.0;"['label   A    B    C\nID                                 \n1   NaN  0.2  NaN\n2   NaN  NaN  0.5\n3   NaN  0.2  0.5\n4   0.1  0.2  NaN\n5   0.1  0.2  0.5\n6   0.1  NaN  0.5\n7   0.1  NaN  NaN\n', 'array([[ nan,  0.2,  nan],\n       [ nan,  nan,  0.5],\n       [ nan,  0.2,  0.5],\n       [ 0.1,  0.2,  nan],\n       [ 0.1,  0.2,  0.5],\n       [ 0.1,  nan,  0.5],\n       [ 0.1,  nan,  nan]])\n', ""array([[ 1, nan,  0.2,  nan],\n       [ 2, nan,  nan,  0.5],\n       [ 3, nan,  0.2,  0.5],\n       [ 4, 0.1,  0.2,  nan],\n       [ 5, 0.1,  0.2,  0.5],\n       [ 6, 0.1,  nan,  0.5],\n       [ 7, 0.1,  nan,  nan]],\n     dtype=[('ID', '<i4'), ('A', '<f8'), ('B', '<f8'), ('B', '<f8')])\n""]";"['label   A    B    C\nID                                 \n1   NaN  0.2  NaN\n2   NaN  NaN  0.5\n3   NaN  0.2  0.5\n4   0.1  0.2  NaN\n5   0.1  0.2  0.5\n6   0.1  NaN  0.5\n7   0.1  NaN  NaN\n', 'array([[ nan,  0.2,  nan],\n       [ nan,  nan,  0.5],\n       [ nan,  0.2,  0.5],\n       [ 0.1,  0.2,  nan],\n       [ 0.1,  0.2,  0.5],\n       [ 0.1,  nan,  0.5],\n       [ 0.1,  nan,  nan]])\n', ""array([[ 1, nan,  0.2,  nan],\n       [ 2, nan,  nan,  0.5],\n       [ 3, nan,  0.2,  0.5],\n       [ 4, 0.1,  0.2,  nan],\n       [ 5, 0.1,  0.2,  0.5],\n       [ 6, 0.1,  nan,  0.5],\n       [ 7, 0.1,  nan,  nan]],\n     dtype=[('ID', '<i4'), ('A', '<f8'), ('B', '<f8'), ('B', '<f8')])\n""]"
84;1.0;0;13226029;;1;29;<python><pandas><multi-index>;Benefits of panda's multiindex?;"<p>So I learned that I can use DataFrame.groupby without having a MultiIndex to do subsampling/cross-sections.</p>

<p>On the other hand, when I have a MultiIndex on a DataFrame, I still need to use DataFrame.groupby to do sub-sampling/cross-sections.</p>

<p>So what is a MultiIndex good for apart from the quite helpful and pretty display of the hierarchies when printing?</p>
";15935.0;[];[]
85;4.0;3;13269890;;1;30;<python><pandas>;cartesian product in pandas;"<p>I have two pandas dataframes:</p>

<pre><code>from pandas import DataFrame
df1 = DataFrame({'col1':[1,2],'col2':[3,4]})
df2 = DataFrame({'col3':[5,6]})     
</code></pre>

<p>What is the best practice to get their cartesian product (of course without writing it explicitly like me)?</p>

<pre><code>#df1, df2 cartesian product
df_cartesian = DataFrame({'col1':[1,2,1,2],'col2':[3,4,3,4],'col3':[5,5,6,6]})
</code></pre>
";18300.0;"[""from pandas import DataFrame\ndf1 = DataFrame({'col1':[1,2],'col2':[3,4]})\ndf2 = DataFrame({'col3':[5,6]})     \n"", ""#df1, df2 cartesian product\ndf_cartesian = DataFrame({'col1':[1,2,1,2],'col2':[3,4,3,4],'col3':[5,5,6,6]})\n""]";"[""from pandas import DataFrame\ndf1 = DataFrame({'col1':[1,2],'col2':[3,4]})\ndf2 = DataFrame({'col3':[5,6]})     \n"", ""#df1, df2 cartesian product\ndf_cartesian = DataFrame({'col1':[1,2,1,2],'col2':[3,4,3,4],'col3':[5,5,6,6]})\n""]"
86;2.0;2;13293810;;1;32;<pandas>;Import pandas dataframe column as string not int;"<p>I would like to import the following csv as strings not as int64. Pandas read_csv automatically converts it to int64, but I need this column as string.</p>

<pre><code>ID
00013007854817840016671868
00013007854817840016749251
00013007854817840016754630
00013007854817840016781876
00013007854817840017028824
00013007854817840017963235
00013007854817840018860166


df = read_csv('sample.csv')

df.ID
&gt;&gt;

0   -9223372036854775808
1   -9223372036854775808
2   -9223372036854775808
3   -9223372036854775808
4   -9223372036854775808
5   -9223372036854775808
6   -9223372036854775808
Name: ID
</code></pre>

<p>Unfortunately using converters gives the same result. </p>

<pre><code>df = read_csv('sample.csv', converters={'ID': str})
df.ID
&gt;&gt;

0   -9223372036854775808
1   -9223372036854775808
2   -9223372036854775808
3   -9223372036854775808
4   -9223372036854775808
5   -9223372036854775808
6   -9223372036854775808
Name: ID
</code></pre>
";28863.0;"[""ID\n00013007854817840016671868\n00013007854817840016749251\n00013007854817840016754630\n00013007854817840016781876\n00013007854817840017028824\n00013007854817840017963235\n00013007854817840018860166\n\n\ndf = read_csv('sample.csv')\n\ndf.ID\n>>\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\nName: ID\n"", ""df = read_csv('sample.csv', converters={'ID': str})\ndf.ID\n>>\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\nName: ID\n""]";"[""ID\n00013007854817840016671868\n00013007854817840016749251\n00013007854817840016754630\n00013007854817840016781876\n00013007854817840017028824\n00013007854817840017963235\n00013007854817840018860166\n\n\ndf = read_csv('sample.csv')\n\ndf.ID\n>>\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\nName: ID\n"", ""df = read_csv('sample.csv', converters={'ID': str})\ndf.ID\n>>\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\nName: ID\n""]"
87;7.0;1;13295735;;1;145;<python><pandas>;How can I replace all the NaN values with Zero's in a column of a pandas dataframe;"<p>I have a dataframe as below</p>

<pre><code>      itm Date                  Amount 
67    420 2012-09-30 00:00:00   65211
68    421 2012-09-09 00:00:00   29424
69    421 2012-09-16 00:00:00   29877
70    421 2012-09-23 00:00:00   30990
71    421 2012-09-30 00:00:00   61303
72    485 2012-09-09 00:00:00   71781
73    485 2012-09-16 00:00:00     NaN
74    485 2012-09-23 00:00:00   11072
75    485 2012-09-30 00:00:00  113702
76    489 2012-09-09 00:00:00   64731
77    489 2012-09-16 00:00:00     NaN
</code></pre>

<p>when I try to .apply a function to the Amount column I get the following error.</p>

<pre><code>ValueError: cannot convert float NaN to integer
</code></pre>

<p>I have tried applying a function using .isnan from the Math Module
I have tried the pandas .replace attribute
I tried the .sparse data attribute from pandas 0.9
I have also tried if NaN == NaN statement in a function.
I have also looked at this article <a href=""https://stackoverflow.com/questions/8161836/how-do-i-replace-na-values-with-zeros-in-r"">How do I replace NA values with zeros in an R dataframe?</a> whilst looking at some other articles. 
All the methods I have tried have not worked or do not recognise NaN.
Any Hints or solutions would be appreciated. </p>
";174458.0;['      itm Date                  Amount \n67    420 2012-09-30 00:00:00   65211\n68    421 2012-09-09 00:00:00   29424\n69    421 2012-09-16 00:00:00   29877\n70    421 2012-09-23 00:00:00   30990\n71    421 2012-09-30 00:00:00   61303\n72    485 2012-09-09 00:00:00   71781\n73    485 2012-09-16 00:00:00     NaN\n74    485 2012-09-23 00:00:00   11072\n75    485 2012-09-30 00:00:00  113702\n76    489 2012-09-09 00:00:00   64731\n77    489 2012-09-16 00:00:00     NaN\n', 'ValueError: cannot convert float NaN to integer\n'];['      itm Date                  Amount \n67    420 2012-09-30 00:00:00   65211\n68    421 2012-09-09 00:00:00   29424\n69    421 2012-09-16 00:00:00   29877\n70    421 2012-09-23 00:00:00   30990\n71    421 2012-09-30 00:00:00   61303\n72    485 2012-09-09 00:00:00   71781\n73    485 2012-09-16 00:00:00     NaN\n74    485 2012-09-23 00:00:00   11072\n75    485 2012-09-30 00:00:00  113702\n76    489 2012-09-09 00:00:00   64731\n77    489 2012-09-16 00:00:00     NaN\n', 'ValueError: cannot convert float NaN to integer\n']
88;5.0;0;13331518;;1;26;<python><pandas>;How to add a single item to a Pandas Series;"<p>How Do I add a single item to a serialized panda series. I know it's not the most efficient way memory wise, but i still need to do that.</p>

<p>Something along:</p>

<pre><code>&gt;&gt; x = Series()
&gt;&gt; N = 4
&gt;&gt; for i in xrange(N):
&gt;&gt;     x.some_appending_function(i**2)    
&gt;&gt; print x

0 | 0
1 | 1
2 | 4
3 | 9
</code></pre>

<p>also, how can i add a single row to a pandas DataFrame? </p>
";38577.0;['>> x = Series()\n>> N = 4\n>> for i in xrange(N):\n>>     x.some_appending_function(i**2)    \n>> print x\n\n0 | 0\n1 | 1\n2 | 4\n3 | 9\n'];['>> x = Series()\n>> N = 4\n>> for i in xrange(N):\n>>     x.some_appending_function(i**2)    \n>> print x\n\n0 | 0\n1 | 1\n2 | 4\n3 | 9\n']
89;8.0;4;13331698;;1;137;<python><pandas>;How to apply a function to two columns of Pandas dataframe;"<p>Suppose I have a <code>df</code> which has columns of <code>'ID', 'col_1', 'col_2'</code>. And I define a function :</p>

<p><code>f = lambda x, y : my_function_expression</code>.</p>

<p>Now I want to apply the <code>f</code> to <code>df</code>'s two columns <code>'col_1', 'col_2'</code> to element-wise calculate a new column <code>'col_3'</code> , somewhat like :</p>

<pre><code>df['col_3'] = df[['col_1','col_2']].apply(f)  
# Pandas gives : TypeError: ('&lt;lambda&gt;() takes exactly 2 arguments (1 given)'
</code></pre>

<p>How to do ?</p>

<p><em><strong></em>**<em></strong> Add detail sample as below <strong></em>***</strong></p>

<pre><code>import pandas as pd

df = pd.DataFrame({'ID':['1','2','3'], 'col_1': [0,2,3], 'col_2':[1,4,5]})
mylist = ['a','b','c','d','e','f']

def get_sublist(sta,end):
    return mylist[sta:end+1]

#df['col_3'] = df[['col_1','col_2']].apply(get_sublist,axis=1)
# expect above to output df as below 

  ID  col_1  col_2            col_3
0  1      0      1       ['a', 'b']
1  2      2      4  ['c', 'd', 'e']
2  3      3      5  ['d', 'e', 'f']
</code></pre>
";154207.0;"[""df['col_3'] = df[['col_1','col_2']].apply(f)  \n# Pandas gives : TypeError: ('<lambda>() takes exactly 2 arguments (1 given)'\n"", ""import pandas as pd\n\ndf = pd.DataFrame({'ID':['1','2','3'], 'col_1': [0,2,3], 'col_2':[1,4,5]})\nmylist = ['a','b','c','d','e','f']\n\ndef get_sublist(sta,end):\n    return mylist[sta:end+1]\n\n#df['col_3'] = df[['col_1','col_2']].apply(get_sublist,axis=1)\n# expect above to output df as below \n\n  ID  col_1  col_2            col_3\n0  1      0      1       ['a', 'b']\n1  2      2      4  ['c', 'd', 'e']\n2  3      3      5  ['d', 'e', 'f']\n""]";"['df', ""'ID', 'col_1', 'col_2'"", 'f = lambda x, y : my_function_expression', 'f', 'df', ""'col_1', 'col_2'"", ""'col_3'"", ""df['col_3'] = df[['col_1','col_2']].apply(f)  \n# Pandas gives : TypeError: ('<lambda>() takes exactly 2 arguments (1 given)'\n"", ""import pandas as pd\n\ndf = pd.DataFrame({'ID':['1','2','3'], 'col_1': [0,2,3], 'col_2':[1,4,5]})\nmylist = ['a','b','c','d','e','f']\n\ndef get_sublist(sta,end):\n    return mylist[sta:end+1]\n\n#df['col_3'] = df[['col_1','col_2']].apply(get_sublist,axis=1)\n# expect above to output df as below \n\n  ID  col_1  col_2            col_3\n0  1      0      1       ['a', 'b']\n1  2      2      4  ['c', 'd', 'e']\n2  3      3      5  ['d', 'e', 'f']\n""]"
90;6.0;0;13385860;;1;28;<python><parsing><pandas>;How can I remove extra whitespace from strings when parsing a csv file in Pandas?;"<p>I have the following file named 'data.csv':</p>

<pre><code>    1997,Ford,E350
    1997, Ford , E350
    1997,Ford,E350,""Super, luxurious truck""
    1997,Ford,E350,""Super """"luxurious"""" truck""
    1997,Ford,E350,"" Super luxurious truck ""
    ""1997"",Ford,E350
    1997,Ford,E350
    2000,Mercury,Cougar
</code></pre>

<p>And I would like to parse it into a pandas DataFrame so that the DataFrame looks as follows:</p>

<pre><code>       Year     Make   Model              Description
    0  1997     Ford    E350                     None
    1  1997     Ford    E350                     None
    2  1997     Ford    E350   Super, luxurious truck
    3  1997     Ford    E350  Super ""luxurious"" truck
    4  1997     Ford    E350    Super luxurious truck
    5  1997     Ford    E350                     None
    6  1997     Ford    E350                     None
    7  2000  Mercury  Cougar                     None
</code></pre>

<p>The best I could do was:</p>

<pre><code>    pd.read_table(""data.csv"", sep=r',', names=[""Year"", ""Make"", ""Model"", ""Description""])
</code></pre>

<p>Which gets me:</p>

<pre><code>    Year     Make   Model              Description
 0  1997     Ford    E350                     None
 1  1997    Ford     E350                     None
 2  1997     Ford    E350   Super, luxurious truck
 3  1997     Ford    E350  Super ""luxurious"" truck
 4  1997     Ford    E350   Super luxurious truck 
 5  1997     Ford    E350                     None
 6  1997     Ford    E350                     None
 7  2000  Mercury  Cougar                     None
</code></pre>

<p>How can I get the DataFrame without those whitespaces?</p>
";28532.0;"['    1997,Ford,E350\n    1997, Ford , E350\n    1997,Ford,E350,""Super, luxurious truck""\n    1997,Ford,E350,""Super """"luxurious"""" truck""\n    1997,Ford,E350,"" Super luxurious truck ""\n    ""1997"",Ford,E350\n    1997,Ford,E350\n    2000,Mercury,Cougar\n', '       Year     Make   Model              Description\n    0  1997     Ford    E350                     None\n    1  1997     Ford    E350                     None\n    2  1997     Ford    E350   Super, luxurious truck\n    3  1997     Ford    E350  Super ""luxurious"" truck\n    4  1997     Ford    E350    Super luxurious truck\n    5  1997     Ford    E350                     None\n    6  1997     Ford    E350                     None\n    7  2000  Mercury  Cougar                     None\n', '    pd.read_table(""data.csv"", sep=r\',\', names=[""Year"", ""Make"", ""Model"", ""Description""])\n', '    Year     Make   Model              Description\n 0  1997     Ford    E350                     None\n 1  1997    Ford     E350                     None\n 2  1997     Ford    E350   Super, luxurious truck\n 3  1997     Ford    E350  Super ""luxurious"" truck\n 4  1997     Ford    E350   Super luxurious truck \n 5  1997     Ford    E350                     None\n 6  1997     Ford    E350                     None\n 7  2000  Mercury  Cougar                     None\n']";"['    1997,Ford,E350\n    1997, Ford , E350\n    1997,Ford,E350,""Super, luxurious truck""\n    1997,Ford,E350,""Super """"luxurious"""" truck""\n    1997,Ford,E350,"" Super luxurious truck ""\n    ""1997"",Ford,E350\n    1997,Ford,E350\n    2000,Mercury,Cougar\n', '       Year     Make   Model              Description\n    0  1997     Ford    E350                     None\n    1  1997     Ford    E350                     None\n    2  1997     Ford    E350   Super, luxurious truck\n    3  1997     Ford    E350  Super ""luxurious"" truck\n    4  1997     Ford    E350    Super luxurious truck\n    5  1997     Ford    E350                     None\n    6  1997     Ford    E350                     None\n    7  2000  Mercury  Cougar                     None\n', '    pd.read_table(""data.csv"", sep=r\',\', names=[""Year"", ""Make"", ""Model"", ""Description""])\n', '    Year     Make   Model              Description\n 0  1997     Ford    E350                     None\n 1  1997    Ford     E350                     None\n 2  1997     Ford    E350   Super, luxurious truck\n 3  1997     Ford    E350  Super ""luxurious"" truck\n 4  1997     Ford    E350   Super luxurious truck \n 5  1997     Ford    E350                     None\n 6  1997     Ford    E350                     None\n 7  2000  Mercury  Cougar                     None\n']"
91;2.0;0;13404468;;1;32;<python><pandas><scipy><statistics><hypothesis-test>;T-test in Pandas (Python);"<p>If I want to calculate the mean of two categories in Pandas, I can do it like this:</p>

<pre><code>data = {'Category': ['cat2','cat1','cat2','cat1','cat2','cat1','cat2','cat1','cat1','cat1','cat2'],
        'values': [1,2,3,1,2,3,1,2,3,5,1]}
my_data = DataFrame(data)
my_data.groupby('Category').mean()

Category:     values:   
cat1     2.666667
cat2     1.600000
</code></pre>

<p>I have a lot of data formatted this way, and now I need to do a <em>T</em>-test to see if the mean of <em>cat1</em> and <em>cat2</em> are statistically different. How can I do that?</p>
";27308.0;"[""data = {'Category': ['cat2','cat1','cat2','cat1','cat2','cat1','cat2','cat1','cat1','cat1','cat2'],\n        'values': [1,2,3,1,2,3,1,2,3,5,1]}\nmy_data = DataFrame(data)\nmy_data.groupby('Category').mean()\n\nCategory:     values:   \ncat1     2.666667\ncat2     1.600000\n""]";"[""data = {'Category': ['cat2','cat1','cat2','cat1','cat2','cat1','cat2','cat1','cat1','cat1','cat2'],\n        'values': [1,2,3,1,2,3,1,2,3,5,1]}\nmy_data = DataFrame(data)\nmy_data.groupby('Category').mean()\n\nCategory:     values:   \ncat1     2.666667\ncat2     1.600000\n""]"
92;11.0;0;13411544;;1;643;<python><pandas><design><dataframe><magic-methods>;Delete column from pandas DataFrame;"<p>When deleting a column in a DataFrame I use:</p>

<pre><code>del df['column_name']
</code></pre>

<p>and this works great. Why can't I use:</p>

<pre><code>del df.column_name
</code></pre>

<p><em>As you can access the column/Series as <code>df.column_name</code>, I expect this to work.</em></p>
";684200.0;"[""del df['column_name']\n"", 'del df.column_name\n']";"[""del df['column_name']\n"", 'del df.column_name\n', 'df.column_name']"
93;8.0;2;13413590;;1;272;<python><pandas><dataframe>;How to drop rows of Pandas DataFrame whose value in certain columns is NaN;"<p>I have a <code>DataFrame</code>:</p>

<pre><code>&gt;&gt;&gt; df
                 STK_ID  EPS  cash
STK_ID RPT_Date                   
601166 20111231  601166  NaN   NaN
600036 20111231  600036  NaN    12
600016 20111231  600016  4.3   NaN
601009 20111231  601009  NaN   NaN
601939 20111231  601939  2.5   NaN
000001 20111231  000001  NaN   NaN
</code></pre>

<p>Then I just want the records whose <code>EPS</code> is not <code>NaN</code>, that is, <code>df.drop(....)</code> will return the dataframe as below:</p>

<pre><code>                  STK_ID  EPS  cash
STK_ID RPT_Date                   
600016 20111231  600016  4.3   NaN
601939 20111231  601939  2.5   NaN
</code></pre>

<p>How do I do that?</p>
";280310.0;['>>> df\n                 STK_ID  EPS  cash\nSTK_ID RPT_Date                   \n601166 20111231  601166  NaN   NaN\n600036 20111231  600036  NaN    12\n600016 20111231  600016  4.3   NaN\n601009 20111231  601009  NaN   NaN\n601939 20111231  601939  2.5   NaN\n000001 20111231  000001  NaN   NaN\n', '                  STK_ID  EPS  cash\nSTK_ID RPT_Date                   \n600016 20111231  600016  4.3   NaN\n601939 20111231  601939  2.5   NaN\n'];['DataFrame', '>>> df\n                 STK_ID  EPS  cash\nSTK_ID RPT_Date                   \n601166 20111231  601166  NaN   NaN\n600036 20111231  600036  NaN    12\n600016 20111231  600016  4.3   NaN\n601009 20111231  601009  NaN   NaN\n601939 20111231  601939  2.5   NaN\n000001 20111231  000001  NaN   NaN\n', 'EPS', 'NaN', 'df.drop(....)', '                  STK_ID  EPS  cash\nSTK_ID RPT_Date                   \n600016 20111231  600016  4.3   NaN\n601939 20111231  601939  2.5   NaN\n']
94;3.0;2;13445241;;1;57;<python><pandas>;Replacing blank values (white space) with NaN in pandas;"<p>I want to find all values in a Pandas dataframe that contain whitespace (any arbitrary amount) and replace those values with NaNs.</p>

<p>Any ideas how this can be improved?</p>

<p>Basically I want to turn this:</p>

<pre><code>                   A    B    C
2000-01-01 -0.532681  foo    0
2000-01-02  1.490752  bar    1
2000-01-03 -1.387326  foo    2
2000-01-04  0.814772  baz     
2000-01-05 -0.222552         4
2000-01-06 -1.176781  qux     
</code></pre>

<p>Into this:</p>

<pre><code>                   A     B     C
2000-01-01 -0.532681   foo     0
2000-01-02  1.490752   bar     1
2000-01-03 -1.387326   foo     2
2000-01-04  0.814772   baz   NaN
2000-01-05 -0.222552   NaN     4
2000-01-06 -1.176781   qux   NaN
</code></pre>

<p>I've managed to do it with the code below, but man is it ugly. It's not Pythonic and I'm sure it's not the most efficient use of pandas either. I loop through each column and do boolean replacement against a column mask generated by applying a function that does a regex search of each value, matching on whitespace.</p>

<pre><code>for i in df.columns:
    df[i][df[i].apply(lambda i: True if re.search('^\s*$', str(i)) else False)]=None
</code></pre>

<p>It could be optimized a bit by only iterating through fields that could contain empty strings:</p>

<pre><code>if df[i].dtype == np.dtype('object')
</code></pre>

<p>But that's not much of an improvement</p>

<p>And finally, this code sets the target strings to None, which works with Pandas' functions like fillna(), but it would be nice for completeness if I could actually insert a NaN directly instead of None.</p>

<p>Help!</p>
";50114.0;"['                   A    B    C\n2000-01-01 -0.532681  foo    0\n2000-01-02  1.490752  bar    1\n2000-01-03 -1.387326  foo    2\n2000-01-04  0.814772  baz     \n2000-01-05 -0.222552         4\n2000-01-06 -1.176781  qux     \n', '                   A     B     C\n2000-01-01 -0.532681   foo     0\n2000-01-02  1.490752   bar     1\n2000-01-03 -1.387326   foo     2\n2000-01-04  0.814772   baz   NaN\n2000-01-05 -0.222552   NaN     4\n2000-01-06 -1.176781   qux   NaN\n', ""for i in df.columns:\n    df[i][df[i].apply(lambda i: True if re.search('^\\s*$', str(i)) else False)]=None\n"", ""if df[i].dtype == np.dtype('object')\n""]";"['                   A    B    C\n2000-01-01 -0.532681  foo    0\n2000-01-02  1.490752  bar    1\n2000-01-03 -1.387326  foo    2\n2000-01-04  0.814772  baz     \n2000-01-05 -0.222552         4\n2000-01-06 -1.176781  qux     \n', '                   A     B     C\n2000-01-01 -0.532681   foo     0\n2000-01-02  1.490752   bar     1\n2000-01-03 -1.387326   foo     2\n2000-01-04  0.814772   baz   NaN\n2000-01-05 -0.222552   NaN     4\n2000-01-06 -1.176781   qux   NaN\n', ""for i in df.columns:\n    df[i][df[i].apply(lambda i: True if re.search('^\\s*$', str(i)) else False)]=None\n"", ""if df[i].dtype == np.dtype('object')\n""]"
95;2.0;0;13575090;;1;29;<python><dataframe><pandas>;Construct pandas DataFrame from items in nested dictionary;"<p>Suppose I have a nested dictionary 'user_dict' with structure:</p>

<p><strong>Level 1:</strong> UserId (Long Integer)</p>

<p><strong>Level 2:</strong> Category (String)</p>

<p><strong>Level 3:</strong> Assorted Attributes (floats, ints, etc..)</p>

<p>For example, an entry of this dictionary would be:</p>

<pre><code>user_dict[12] = {
    ""Category 1"": {""att_1"": 1, 
                   ""att_2"": ""whatever""},
    ""Category 2"": {""att_1"": 23, 
                   ""att_2"": ""another""}}
</code></pre>

<p>each item in ""user_dict"" has the same structure and ""user_dict"" contains a large number of items which I want to feed to a pandas DataFrame, constructing the series from the attributes. In this case a hierarchical index would be useful for the purpose.</p>

<p>Specifically, my question is whether there exists a way to to help the DataFrame constructor understand that the series should be built from the values of the ""level 3"" in the dictionary?</p>

<p>If I try something like:</p>

<pre><code>df = pandas.DataFrame(users_summary)
</code></pre>

<p>The items in ""level 1"" (the user id's) are taken as columns, which is the opposite of what I want to achieve (have user id's as index). </p>

<p>I know I could construct the series after iterating over the dictionary entries, but if there is a more direct way this would be very useful. A similar question would be asking whether it is possible to construct a pandas DataFrame from json objects listed in a file. </p>
";27582.0;"['user_dict[12] = {\n    ""Category 1"": {""att_1"": 1, \n                   ""att_2"": ""whatever""},\n    ""Category 2"": {""att_1"": 23, \n                   ""att_2"": ""another""}}\n', 'df = pandas.DataFrame(users_summary)\n']";"['user_dict[12] = {\n    ""Category 1"": {""att_1"": 1, \n                   ""att_2"": ""whatever""},\n    ""Category 2"": {""att_1"": 23, \n                   ""att_2"": ""another""}}\n', 'df = pandas.DataFrame(users_summary)\n']"
96;1.0;0;13582449;;1;21;<python><pandas>;Convert DataFrameGroupBy object to DataFrame pandas;"<p>I had a dataframe and did a groupby in FIPS and summed the groups that worked fine.</p>

<pre><code>kl = ks.groupby('FIPS')

kl.aggregate(np.sum)
</code></pre>

<p>I just want a normal Dataframe back but I have a <code>pandas.core.groupby.DataFrameGroupBy</code> object. </p>

<p>There is a question that sounds like this one but it is not the same. </p>
";15555.0;"[""kl = ks.groupby('FIPS')\n\nkl.aggregate(np.sum)\n""]";"[""kl = ks.groupby('FIPS')\n\nkl.aggregate(np.sum)\n"", 'pandas.core.groupby.DataFrameGroupBy']"
97;3.0;1;13611065;;1;46;<python><algorithm><pandas>;Efficient way to apply multiple filters to pandas DataFrame or Series;"<p>I have a scenario where a user wants to apply several filters to a Pandas DataFrame or Series object.  Essentially, I want to efficiently chain a bunch of filtering (comparison operations) together that are specified at run-time by the user.</p>

<p>The filters should be additive (aka each one applied should narrow results).</p>

<p>I'm currently using <code>reindex()</code> but this creates a new object each time and copies the underlying data (if I understand the documentation correctly).  So, this could be really inefficient when filtering a big Series or DataFrame.</p>

<p>I'm thinking that using <code>apply()</code>, <code>map()</code>, or something similar might be better.  I'm pretty new to Pandas though so still trying to wrap my head around everything.</p>

<h2>TL;DR</h2>

<p>I want to take a dictionary of the following form and apply each operation to a given Series object and return a 'filtered' Series object.</p>

<pre><code>relops = {'&gt;=': [1], '&lt;=': [1]}
</code></pre>

<h2>Long Example</h2>

<p>I'll start with an example of what I have currently and just filtering a single Series object.  Below is the function I'm currently using:</p>

<pre><code>   def apply_relops(series, relops):
        """"""
        Pass dictionary of relational operators to perform on given series object
        """"""
        for op, vals in relops.iteritems():
            op_func = ops[op]
            for val in vals:
                filtered = op_func(series, val)
                series = series.reindex(series[filtered])
        return series
</code></pre>

<p>The user provides a dictionary with the operations they want to perform:</p>

<pre><code>&gt;&gt;&gt; df = pandas.DataFrame({'col1': [0, 1, 2], 'col2': [10, 11, 12]})
&gt;&gt;&gt; print df
&gt;&gt;&gt; print df
   col1  col2
0     0    10
1     1    11
2     2    12

&gt;&gt;&gt; from operator import le, ge
&gt;&gt;&gt; ops ={'&gt;=': ge, '&lt;=': le}
&gt;&gt;&gt; apply_relops(df['col1'], {'&gt;=': [1]})
col1
1       1
2       2
Name: col1
&gt;&gt;&gt; apply_relops(df['col1'], relops = {'&gt;=': [1], '&lt;=': [1]})
col1
1       1
Name: col1
</code></pre>

<p>Again, the 'problem' with my above approach is that I think there is a lot of possibly unnecessary copying of the data for the in-between steps.</p>

<p>Also, I would like to expand this so that the dictionary passed in can include the columns to operator on and filter an entire DataFrame based on the input dictionary.  However, I'm assuming whatever works for the Series can be easily expanded to a DataFrame.</p>
";52152.0;"[""relops = {'>=': [1], '<=': [1]}\n"", '   def apply_relops(series, relops):\n        """"""\n        Pass dictionary of relational operators to perform on given series object\n        """"""\n        for op, vals in relops.iteritems():\n            op_func = ops[op]\n            for val in vals:\n                filtered = op_func(series, val)\n                series = series.reindex(series[filtered])\n        return series\n', "">>> df = pandas.DataFrame({'col1': [0, 1, 2], 'col2': [10, 11, 12]})\n>>> print df\n>>> print df\n   col1  col2\n0     0    10\n1     1    11\n2     2    12\n\n>>> from operator import le, ge\n>>> ops ={'>=': ge, '<=': le}\n>>> apply_relops(df['col1'], {'>=': [1]})\ncol1\n1       1\n2       2\nName: col1\n>>> apply_relops(df['col1'], relops = {'>=': [1], '<=': [1]})\ncol1\n1       1\nName: col1\n""]";"['reindex()', 'apply()', 'map()', ""relops = {'>=': [1], '<=': [1]}\n"", '   def apply_relops(series, relops):\n        """"""\n        Pass dictionary of relational operators to perform on given series object\n        """"""\n        for op, vals in relops.iteritems():\n            op_func = ops[op]\n            for val in vals:\n                filtered = op_func(series, val)\n                series = series.reindex(series[filtered])\n        return series\n', "">>> df = pandas.DataFrame({'col1': [0, 1, 2], 'col2': [10, 11, 12]})\n>>> print df\n>>> print df\n   col1  col2\n0     0    10\n1     1    11\n2     2    12\n\n>>> from operator import le, ge\n>>> ops ={'>=': ge, '<=': le}\n>>> apply_relops(df['col1'], {'>=': [1]})\ncol1\n1       1\n2       2\nName: col1\n>>> apply_relops(df['col1'], relops = {'>=': [1], '<=': [1]})\ncol1\n1       1\nName: col1\n""]"
98;5.0;0;13636592;;1;23;<python><pandas>;How to sort a Pandas DataFrame according to multiple criteria?;"<p>I have the following DataFrame containing song names, their peak chart positions and the number of weeks they spent at position no 1:</p>

<pre><code>                                          Song            Peak            Weeks
76                            Paperback Writer               1               16
117                               Lady Madonna               1                9
118                                   Hey Jude               1               27
22                           Can't Buy Me Love               1               17
29                          A Hard Day's Night               1               14
48                              Ticket To Ride               1               14
56                                       Help!               1               17
109                       All You Need Is Love               1               16
173                The Ballad Of John And Yoko               1               13
85                               Eleanor Rigby               1               14
87                            Yellow Submarine               1               14
20                    I Want To Hold Your Hand               1               24
45                                 I Feel Fine               1               15
60                                 Day Tripper               1               12
61                          We Can Work It Out               1               12
10                               She Loves You               1               36
155                                   Get Back               1                6
8                               From Me To You               1                7
115                              Hello Goodbye               1                7
2                             Please Please Me               2               20
92                   Strawberry Fields Forever               2               12
93                                  Penny Lane               2               13
107                       Magical Mystery Tour               2               16
176                                  Let It Be               2               14
0                                   Love Me Do               4               26
157                                  Something               4                9
166                              Come Together               4               10
58                                   Yesterday               8               21
135                       Back In The U.S.S.R.              19                3
164                         Here Comes The Sun              58               19
96       Sgt. Pepper's Lonely Hearts Club Band              63               12
105         With A Little Help From My Friends              63                7
</code></pre>

<p>I'd like to rank these songs in order of popularity, so I'd like to sort them according to the following criteria: songs that reached the highest position come first, but if there is a tie, the songs that remained in the charts for the longest come first.</p>

<p>I can't seem to figure out how to do this in Pandas.</p>
";50220.0;"[""                                          Song            Peak            Weeks\n76                            Paperback Writer               1               16\n117                               Lady Madonna               1                9\n118                                   Hey Jude               1               27\n22                           Can't Buy Me Love               1               17\n29                          A Hard Day's Night               1               14\n48                              Ticket To Ride               1               14\n56                                       Help!               1               17\n109                       All You Need Is Love               1               16\n173                The Ballad Of John And Yoko               1               13\n85                               Eleanor Rigby               1               14\n87                            Yellow Submarine               1               14\n20                    I Want To Hold Your Hand               1               24\n45                                 I Feel Fine               1               15\n60                                 Day Tripper               1               12\n61                          We Can Work It Out               1               12\n10                               She Loves You               1               36\n155                                   Get Back               1                6\n8                               From Me To You               1                7\n115                              Hello Goodbye               1                7\n2                             Please Please Me               2               20\n92                   Strawberry Fields Forever               2               12\n93                                  Penny Lane               2               13\n107                       Magical Mystery Tour               2               16\n176                                  Let It Be               2               14\n0                                   Love Me Do               4               26\n157                                  Something               4                9\n166                              Come Together               4               10\n58                                   Yesterday               8               21\n135                       Back In The U.S.S.R.              19                3\n164                         Here Comes The Sun              58               19\n96       Sgt. Pepper's Lonely Hearts Club Band              63               12\n105         With A Little Help From My Friends              63                7\n""]";"[""                                          Song            Peak            Weeks\n76                            Paperback Writer               1               16\n117                               Lady Madonna               1                9\n118                                   Hey Jude               1               27\n22                           Can't Buy Me Love               1               17\n29                          A Hard Day's Night               1               14\n48                              Ticket To Ride               1               14\n56                                       Help!               1               17\n109                       All You Need Is Love               1               16\n173                The Ballad Of John And Yoko               1               13\n85                               Eleanor Rigby               1               14\n87                            Yellow Submarine               1               14\n20                    I Want To Hold Your Hand               1               24\n45                                 I Feel Fine               1               15\n60                                 Day Tripper               1               12\n61                          We Can Work It Out               1               12\n10                               She Loves You               1               36\n155                                   Get Back               1                6\n8                               From Me To You               1                7\n115                              Hello Goodbye               1                7\n2                             Please Please Me               2               20\n92                   Strawberry Fields Forever               2               12\n93                                  Penny Lane               2               13\n107                       Magical Mystery Tour               2               16\n176                                  Let It Be               2               14\n0                                   Love Me Do               4               26\n157                                  Something               4                9\n166                              Come Together               4               10\n58                                   Yesterday               8               21\n135                       Back In The U.S.S.R.              19                3\n164                         Here Comes The Sun              58               19\n96       Sgt. Pepper's Lonely Hearts Club Band              63               12\n105         With A Little Help From My Friends              63                7\n""]"
99;4.0;2;13636848;;1;24;<python><pandas>;is it possible to do fuzzy match merge with python pandas?;"<p>I have two DataFrames which I want to merge based on a column. However, due to alternate spellings, different number of spaces, absence/presence of diacritical marks, I would like to be able to merge as long as they are similar to one another.</p>

<p>Any similarity algorithm will do (soundex, Levenshtein, difflib's). </p>

<p>Say one DataFrame has the following data:</p>

<pre><code>df1 = DataFrame([[1],[2],[3],[4],[5]], index=['one','two','three','four','five'], columns=['number'])

       number
one         1
two         2
three       3
four        4
five        5

df2 = DataFrame([['a'],['b'],['c'],['d'],['e']], index=['one','too','three','fours','five'], columns=['letter'])

      letter
one        a
too        b
three      c
fours      d
five       e
</code></pre>

<p>Then I want to get the resulting DataFrame</p>

<pre><code>       number letter
one         1      a
two         2      b
three       3      c
four        4      d
five        5      e
</code></pre>
";10322.0;"[""df1 = DataFrame([[1],[2],[3],[4],[5]], index=['one','two','three','four','five'], columns=['number'])\n\n       number\none         1\ntwo         2\nthree       3\nfour        4\nfive        5\n\ndf2 = DataFrame([['a'],['b'],['c'],['d'],['e']], index=['one','too','three','fours','five'], columns=['letter'])\n\n      letter\none        a\ntoo        b\nthree      c\nfours      d\nfive       e\n"", '       number letter\none         1      a\ntwo         2      b\nthree       3      c\nfour        4      d\nfive        5      e\n']";"[""df1 = DataFrame([[1],[2],[3],[4],[5]], index=['one','two','three','four','five'], columns=['number'])\n\n       number\none         1\ntwo         2\nthree       3\nfour        4\nfive        5\n\ndf2 = DataFrame([['a'],['b'],['c'],['d'],['e']], index=['one','too','three','fours','five'], columns=['letter'])\n\n      letter\none        a\ntoo        b\nthree      c\nfours      d\nfive       e\n"", '       number letter\none         1      a\ntwo         2      b\nthree       3      c\nfour        4      d\nfive        5      e\n']"
100;2.0;0;13651117;;1;39;<pandas>;pandas: filter lines on load in read_csv;"<p>How can I filter which lines of a CSV to be loaded into memory using pandas?  This seems like an option that one should find in <code>read_csv</code>.  Am I missing something?</p>

<p>Example: we've a CSV with a timestamp column and we'd like to load just the lines that with a timestamp greater than a given constant.</p>
";17401.0;[];['read_csv']
101;1.0;0;13654699;;1;27;<python><datetime><python-2.7><pandas>;Reindexing pandas timeseries from object dtype to datetime dtype;"<p>I have a time-series that is not recognized as a DatetimeIndex despite being indexed by standard YYYY-MM-DD strings with valid dates. Coercing them to a valid DatetimeIndex seems to be inelegant enough to make me think I'm doing something wrong.</p>

<p>I read in (someone else's lazily formatted) data that contains invalid datetime values and remove these invalid observations.</p>

<pre><code>In [1]: df = pd.read_csv('data.csv',index_col=0)
In [2]: print df['2008-02-27':'2008-03-02']
Out[2]: 
             count
2008-02-27  20
2008-02-28   0
2008-02-29  27
2008-02-30   0
2008-02-31   0
2008-03-01   0
2008-03-02  17

In [3]: def clean_timestamps(df):
    # remove invalid dates like '2008-02-30' and '2009-04-31'
    to_drop = list()
    for d in df.index:
        try:
            datetime.date(int(d[0:4]),int(d[5:7]),int(d[8:10]))
        except ValueError:
            to_drop.append(d)
    df2 = df.drop(to_drop,axis=0)
    return df2

In [4]: df2 = clean_timestamps(df)
In [5] :print df2['2008-02-27':'2008-03-02']
Out[5]:
             count
2008-02-27  20
2008-02-28   0
2008-02-29  27
2008-03-01   0
2008-03-02  17
</code></pre>

<p>This new index is still only recognized as a 'object' dtype rather than a DatetimeIndex. </p>

<pre><code>In [6]: df2.index
Out[6]: Index([2008-01-01, 2008-01-02, 2008-01-03, ..., 2012-11-27, 2012-11-28,
   2012-11-29], dtype=object)
</code></pre>

<p>Reindexing produces NaNs because they're different dtypes.</p>

<pre><code>In [7]: i = pd.date_range(start=min(df2.index),end=max(df2.index))
In [8]: df3 = df2.reindex(index=i,columns=['count'])
In [9]: df3['2008-02-27':'2008-03-02']
Out[9]: 
            count
2008-02-27 NaN
2008-02-28 NaN
2008-02-29 NaN
2008-03-01 NaN
2008-03-02 NaN
</code></pre>

<p>I create a fresh dataframe with the appropriate index, drop the data to a dictionary, then populate the new dataframe based on the dictionary values (skipping missing values).</p>

<pre><code>In [10]: df3 = pd.DataFrame(columns=['count'],index=i)
In [11]: values = dict(df2['count'])
In [12]: for d in i:
    try:
        df3.set_value(index=d,col='count',value=values[d.isoformat()[0:10]])
    except KeyError:
        pass
In [13]: print df3['2008-02-27':'2008-03-02']
Out[13]: 

             count
2008-02-27  20
2008-02-28   0
2008-02-29  27
2008-03-01   0
2008-03-02  17

In [14]: df3.index
Out[14];
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2008-01-01 00:00:00, ..., 2012-11-29 00:00:00]
Length: 1795, Freq: D, Timezone: None
</code></pre>

<p>This last part of setting values based on lookups to a dictionary keyed by strings seems especially hacky and makes me think I've missed something important.</p>
";23824.0;"[""In [1]: df = pd.read_csv('data.csv',index_col=0)\nIn [2]: print df['2008-02-27':'2008-03-02']\nOut[2]: \n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-02-30   0\n2008-02-31   0\n2008-03-01   0\n2008-03-02  17\n\nIn [3]: def clean_timestamps(df):\n    # remove invalid dates like '2008-02-30' and '2009-04-31'\n    to_drop = list()\n    for d in df.index:\n        try:\n            datetime.date(int(d[0:4]),int(d[5:7]),int(d[8:10]))\n        except ValueError:\n            to_drop.append(d)\n    df2 = df.drop(to_drop,axis=0)\n    return df2\n\nIn [4]: df2 = clean_timestamps(df)\nIn [5] :print df2['2008-02-27':'2008-03-02']\nOut[5]:\n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-03-01   0\n2008-03-02  17\n"", 'In [6]: df2.index\nOut[6]: Index([2008-01-01, 2008-01-02, 2008-01-03, ..., 2012-11-27, 2012-11-28,\n   2012-11-29], dtype=object)\n', ""In [7]: i = pd.date_range(start=min(df2.index),end=max(df2.index))\nIn [8]: df3 = df2.reindex(index=i,columns=['count'])\nIn [9]: df3['2008-02-27':'2008-03-02']\nOut[9]: \n            count\n2008-02-27 NaN\n2008-02-28 NaN\n2008-02-29 NaN\n2008-03-01 NaN\n2008-03-02 NaN\n"", ""In [10]: df3 = pd.DataFrame(columns=['count'],index=i)\nIn [11]: values = dict(df2['count'])\nIn [12]: for d in i:\n    try:\n        df3.set_value(index=d,col='count',value=values[d.isoformat()[0:10]])\n    except KeyError:\n        pass\nIn [13]: print df3['2008-02-27':'2008-03-02']\nOut[13]: \n\n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-03-01   0\n2008-03-02  17\n\nIn [14]: df3.index\nOut[14];\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2008-01-01 00:00:00, ..., 2012-11-29 00:00:00]\nLength: 1795, Freq: D, Timezone: None\n""]";"[""In [1]: df = pd.read_csv('data.csv',index_col=0)\nIn [2]: print df['2008-02-27':'2008-03-02']\nOut[2]: \n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-02-30   0\n2008-02-31   0\n2008-03-01   0\n2008-03-02  17\n\nIn [3]: def clean_timestamps(df):\n    # remove invalid dates like '2008-02-30' and '2009-04-31'\n    to_drop = list()\n    for d in df.index:\n        try:\n            datetime.date(int(d[0:4]),int(d[5:7]),int(d[8:10]))\n        except ValueError:\n            to_drop.append(d)\n    df2 = df.drop(to_drop,axis=0)\n    return df2\n\nIn [4]: df2 = clean_timestamps(df)\nIn [5] :print df2['2008-02-27':'2008-03-02']\nOut[5]:\n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-03-01   0\n2008-03-02  17\n"", 'In [6]: df2.index\nOut[6]: Index([2008-01-01, 2008-01-02, 2008-01-03, ..., 2012-11-27, 2012-11-28,\n   2012-11-29], dtype=object)\n', ""In [7]: i = pd.date_range(start=min(df2.index),end=max(df2.index))\nIn [8]: df3 = df2.reindex(index=i,columns=['count'])\nIn [9]: df3['2008-02-27':'2008-03-02']\nOut[9]: \n            count\n2008-02-27 NaN\n2008-02-28 NaN\n2008-02-29 NaN\n2008-03-01 NaN\n2008-03-02 NaN\n"", ""In [10]: df3 = pd.DataFrame(columns=['count'],index=i)\nIn [11]: values = dict(df2['count'])\nIn [12]: for d in i:\n    try:\n        df3.set_value(index=d,col='count',value=values[d.isoformat()[0:10]])\n    except KeyError:\n        pass\nIn [13]: print df3['2008-02-27':'2008-03-02']\nOut[13]: \n\n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-03-01   0\n2008-03-02  17\n\nIn [14]: df3.index\nOut[14];\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2008-01-01 00:00:00, ..., 2012-11-29 00:00:00]\nLength: 1795, Freq: D, Timezone: None\n""]"
102;1.0;0;13659881;;1;23;<pandas>;Count by unique pair of columns in pandas;"<p>I'm trying to figure out how to count by number of rows per unique pair of columns (ip, useragent), e.g.</p>

<pre><code>d = pd.DataFrame({'ip': ['192.168.0.1', '192.168.0.1', '192.168.0.1', '192.168.0.2'], 'useragent': ['a', 'a', 'b', 'b']})

     ip              useragent
0    192.168.0.1     a
1    192.168.0.1     a
2    192.168.0.1     b
3    192.168.0.2     b
</code></pre>

<p>To produce:</p>

<pre><code>ip           useragent  
192.168.0.1  a           2
192.168.0.1  b           1
192.168.0.2  b           1
</code></pre>

<p>Ideas?</p>
";12957.0;"[""d = pd.DataFrame({'ip': ['192.168.0.1', '192.168.0.1', '192.168.0.1', '192.168.0.2'], 'useragent': ['a', 'a', 'b', 'b']})\n\n     ip              useragent\n0    192.168.0.1     a\n1    192.168.0.1     a\n2    192.168.0.1     b\n3    192.168.0.2     b\n"", 'ip           useragent  \n192.168.0.1  a           2\n192.168.0.1  b           1\n192.168.0.2  b           1\n']";"[""d = pd.DataFrame({'ip': ['192.168.0.1', '192.168.0.1', '192.168.0.1', '192.168.0.2'], 'useragent': ['a', 'a', 'b', 'b']})\n\n     ip              useragent\n0    192.168.0.1     a\n1    192.168.0.1     a\n2    192.168.0.1     b\n3    192.168.0.2     b\n"", 'ip           useragent  \n192.168.0.1  a           2\n192.168.0.1  b           1\n192.168.0.2  b           1\n']"
103;6.0;0;13682044;;1;48;<python><dataframe><pandas>;Pandas DataFrame: remove unwanted parts from strings in a column;"<p>I am looking for an efficient way to remove unwanted parts from strings in a DataFrame column.</p>

<p>Data looks like:</p>

<pre><code>    time    result
1    09:00   +52A
2    10:00   +62B
3    11:00   +44a
4    12:00   +30b
5    13:00   -110a
</code></pre>

<p>I need to trim these data to:</p>

<pre><code>    time    result
1    09:00   52
2    10:00   62
3    11:00   44
4    12:00   30
5    13:00   110
</code></pre>

<p>I tried <code>.str.lstrip('+-')</code> and .<code>str.rstrip('aAbBcC')</code>, but got an error:  </p>

<pre><code>TypeError: wrapper() takes exactly 1 argument (2 given)
</code></pre>

<p>Any pointers would be greatly appreciated!</p>
";66744.0;['    time    result\n1    09:00   +52A\n2    10:00   +62B\n3    11:00   +44a\n4    12:00   +30b\n5    13:00   -110a\n', '    time    result\n1    09:00   52\n2    10:00   62\n3    11:00   44\n4    12:00   30\n5    13:00   110\n', 'TypeError: wrapper() takes exactly 1 argument (2 given)\n'];"['    time    result\n1    09:00   +52A\n2    10:00   +62B\n3    11:00   +44a\n4    12:00   +30b\n5    13:00   -110a\n', '    time    result\n1    09:00   52\n2    10:00   62\n3    11:00   44\n4    12:00   30\n5    13:00   110\n', "".str.lstrip('+-')"", ""str.rstrip('aAbBcC')"", 'TypeError: wrapper() takes exactly 1 argument (2 given)\n']"
104;10.0;3;13703720;;1;140;<python><datetime><numpy><pandas>;Converting between datetime, Timestamp and datetime64;"<p>How do I convert a <code>numpy.datetime64</code> object to a <code>datetime.datetime</code> (or <code>Timestamp</code>)?</p>

<p>In the following code, I create a datetime, timestamp and datetime64 objects.</p>

<pre><code>import datetime
import numpy as np
import pandas as pd
dt = datetime.datetime(2012, 5, 1)
# A strange way to extract a Timestamp object, there's surely a better way?
ts = pd.DatetimeIndex([dt])[0]
dt64 = np.datetime64(dt)

In [7]: dt
Out[7]: datetime.datetime(2012, 5, 1, 0, 0)

In [8]: ts
Out[8]: &lt;Timestamp: 2012-05-01 00:00:00&gt;

In [9]: dt64
Out[9]: numpy.datetime64('2012-05-01T01:00:00.000000+0100')
</code></pre>

<p><em>Note: it's easy to get the datetime from the Timestamp:</em></p>

<pre><code>In [10]: ts.to_datetime()
Out[10]: datetime.datetime(2012, 5, 1, 0, 0)
</code></pre>

<p>But how do we extract the <code>datetime</code> or <code>Timestamp</code> from a <code>numpy.datetime64</code> (<code>dt64</code>)?</p>

<p>.</p>

<p>Update: a somewhat nasty example in my dataset (perhaps the motivating example) seems to be:</p>

<pre><code>dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100')
</code></pre>

<p>which should be <code>datetime.datetime(2002, 6, 28, 1, 0)</code>, and not a long (!) (<code>1025222400000000000L</code>)...</p>
";163489.0;"[""import datetime\nimport numpy as np\nimport pandas as pd\ndt = datetime.datetime(2012, 5, 1)\n# A strange way to extract a Timestamp object, there's surely a better way?\nts = pd.DatetimeIndex([dt])[0]\ndt64 = np.datetime64(dt)\n\nIn [7]: dt\nOut[7]: datetime.datetime(2012, 5, 1, 0, 0)\n\nIn [8]: ts\nOut[8]: <Timestamp: 2012-05-01 00:00:00>\n\nIn [9]: dt64\nOut[9]: numpy.datetime64('2012-05-01T01:00:00.000000+0100')\n"", 'In [10]: ts.to_datetime()\nOut[10]: datetime.datetime(2012, 5, 1, 0, 0)\n', ""dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100')\n""]";"['numpy.datetime64', 'datetime.datetime', 'Timestamp', ""import datetime\nimport numpy as np\nimport pandas as pd\ndt = datetime.datetime(2012, 5, 1)\n# A strange way to extract a Timestamp object, there's surely a better way?\nts = pd.DatetimeIndex([dt])[0]\ndt64 = np.datetime64(dt)\n\nIn [7]: dt\nOut[7]: datetime.datetime(2012, 5, 1, 0, 0)\n\nIn [8]: ts\nOut[8]: <Timestamp: 2012-05-01 00:00:00>\n\nIn [9]: dt64\nOut[9]: numpy.datetime64('2012-05-01T01:00:00.000000+0100')\n"", 'In [10]: ts.to_datetime()\nOut[10]: datetime.datetime(2012, 5, 1, 0, 0)\n', 'datetime', 'Timestamp', 'numpy.datetime64', 'dt64', ""dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100')\n"", 'datetime.datetime(2002, 6, 28, 1, 0)', '1025222400000000000L']"
105;2.0;0;13784192;;1;131;<python><dataframe><pandas>;Creating an empty Pandas DataFrame, then filling it?;"<p>I'm starting from the pandas Data Frame docs here: <a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/dsintro.html</a></p>

<p>I'd like to iteratively fill the Data Frame with values in a time series kind of calculation.
So basically, I'd like to initialize, data frame with columns A,B and timestamp rows, all 0 or all NaN.</p>

<p>I'd then add initial values and go over this data calculating the new row from the row before, say row[A][t] = row[A][t-1]+1 or so.</p>

<p>I'm currently using the code as below, but I feel it's kind of ugly and there must be a  way to do this with a data frame directly or just a better way in general.
Note: I'm using Python 2.7.</p>

<pre><code>import datetime as dt
import pandas as pd
import scipy as s

if __name__ == '__main__':
    base = dt.datetime.today().date()
    dates = [ base - dt.timedelta(days=x) for x in range(0,10) ]
    dates.sort()

    valdict = {}
    symbols = ['A','B', 'C']
    for symb in symbols:
        valdict[symb] = pd.Series( s.zeros( len(dates)), dates )

    for thedate in dates:
        if thedate &gt; dates[0]:
            for symb in valdict:
                valdict[symb][thedate] = 1+valdict[symb][thedate - dt.timedelta(days=1)]

    print valdict
</code></pre>
";326681.0;"[""import datetime as dt\nimport pandas as pd\nimport scipy as s\n\nif __name__ == '__main__':\n    base = dt.datetime.today().date()\n    dates = [ base - dt.timedelta(days=x) for x in range(0,10) ]\n    dates.sort()\n\n    valdict = {}\n    symbols = ['A','B', 'C']\n    for symb in symbols:\n        valdict[symb] = pd.Series( s.zeros( len(dates)), dates )\n\n    for thedate in dates:\n        if thedate > dates[0]:\n            for symb in valdict:\n                valdict[symb][thedate] = 1+valdict[symb][thedate - dt.timedelta(days=1)]\n\n    print valdict\n""]";"[""import datetime as dt\nimport pandas as pd\nimport scipy as s\n\nif __name__ == '__main__':\n    base = dt.datetime.today().date()\n    dates = [ base - dt.timedelta(days=x) for x in range(0,10) ]\n    dates.sort()\n\n    valdict = {}\n    symbols = ['A','B', 'C']\n    for symb in symbols:\n        valdict[symb] = pd.Series( s.zeros( len(dates)), dates )\n\n    for thedate in dates:\n        if thedate > dates[0]:\n            for symb in valdict:\n                valdict[symb][thedate] = 1+valdict[symb][thedate - dt.timedelta(days=1)]\n\n    print valdict\n""]"
106;3.0;1;13838405;;1;25;<python><pandas>;Custom sorting in pandas dataframe;"<p>I have python pandas dataframe, in which a column contains month name.</p>

<p>How can I  do a custom sort using a dictionary, for example:</p>

<pre><code>custom_dict = {'March':0, 'April':1, 'Dec':3}  
</code></pre>
";17015.0;"[""custom_dict = {'March':0, 'April':1, 'Dec':3}  \n""]";"[""custom_dict = {'March':0, 'April':1, 'Dec':3}  \n""]"
107;8.0;3;13842088;;1;136;<python><pandas>;Set value for particular cell in pandas DataFrame;"<p><br>
I've created a pandas DataFrame</p>

<pre><code>df=DataFrame(index=['A','B','C'], columns=['x','y'])
</code></pre>

<p>and got this</p>

<pre>
    x    y
A  NaN  NaN
B  NaN  NaN
C  NaN  NaN
</pre>

<p><br>
Then I want to assign value to particular cell, for example for row 'C' and column 'x'.
I've expected to get such result:</p>

<pre>
    x    y
A  NaN  NaN
B  NaN  NaN
C  10  NaN
</pre>

<p>with this code:</p>

<pre><code>df.xs('C')['x']=10
</code></pre>

<p>but contents of <b>df</b> haven't changed. It's again only Nan's in dataframe. </p>

<p>Any suggestions?</p>
";182280.0;"[""df=DataFrame(index=['A','B','C'], columns=['x','y'])\n"", '\n    x    y\nA  NaN  NaN\nB  NaN  NaN\nC  NaN  NaN\n', '\n    x    y\nA  NaN  NaN\nB  NaN  NaN\nC  10  NaN\n', ""df.xs('C')['x']=10\n""]";"[""df=DataFrame(index=['A','B','C'], columns=['x','y'])\n"", ""df.xs('C')['x']=10\n""]"
108;3.0;0;13851535;;1;48;<python><pandas>;How to delete rows from a pandas DataFrame based on a conditional expression;"<p>I have a pandas DataFrame and I want to delete rows from it where the length of the string in a particular column is greater than 2. I know I can use <code>df.dropna()</code> to get rid of rows that contain any <code>NaN</code>, but I'm not seeing how to remove rows based on a conditional expression. </p>

<p>The answer for <a href=""https://stackoverflow.com/questions/11881165/slice-pandas-dataframe-by-row"">this question</a> seems very close to what I want -- it seems like I should be able to do something like this:</p>

<pre><code>df[(len(df['column name']) &lt; 2)]
</code></pre>

<p>but I just get the error:</p>

<pre><code>KeyError: u'no item named False'
</code></pre>

<p>Can anyone tell me what I'm doing wrong?</p>
";79345.0;"[""df[(len(df['column name']) < 2)]\n"", ""KeyError: u'no item named False'\n""]";"['df.dropna()', 'NaN', ""df[(len(df['column name']) < 2)]\n"", ""KeyError: u'no item named False'\n""]"
109;3.0;0;13872533;;1;31;<python><matplotlib><pandas>;Plot different DataFrames in the same figure;"<p>I have a temperature file with many years temperature records, in a format as below:</p>

<pre><code>2012-04-12,16:13:09,20.6
2012-04-12,17:13:09,20.9
2012-04-12,18:13:09,20.6
2007-05-12,19:13:09,5.4
2007-05-12,20:13:09,20.6
2007-05-12,20:13:09,20.6
2005-08-11,11:13:09,20.6
2005-08-11,11:13:09,17.5
2005-08-13,07:13:09,20.6
2006-04-13,01:13:09,20.6
</code></pre>

<p>Every year has different numbers, time of the records, so the pandas datetimeindices are all different.</p>

<p>I want to plot the different year's data in the same figure for comparing . The X-axis is Jan to Dec, the Y-axis is temperature. How should I go about doing this? </p>
";31002.0;['2012-04-12,16:13:09,20.6\n2012-04-12,17:13:09,20.9\n2012-04-12,18:13:09,20.6\n2007-05-12,19:13:09,5.4\n2007-05-12,20:13:09,20.6\n2007-05-12,20:13:09,20.6\n2005-08-11,11:13:09,20.6\n2005-08-11,11:13:09,17.5\n2005-08-13,07:13:09,20.6\n2006-04-13,01:13:09,20.6\n'];['2012-04-12,16:13:09,20.6\n2012-04-12,17:13:09,20.9\n2012-04-12,18:13:09,20.6\n2007-05-12,19:13:09,5.4\n2007-05-12,20:13:09,20.6\n2007-05-12,20:13:09,20.6\n2005-08-11,11:13:09,20.6\n2005-08-11,11:13:09,17.5\n2005-08-13,07:13:09,20.6\n2006-04-13,01:13:09,20.6\n']
110;2.0;3;13888468;;1;30;<pandas>;Get unique values from index column in MultiIndex;"<p>I know that I can get the unique values of a <code>DataFrame</code> by resetting the index but is there a way to avoid this step and get the unique values directly?</p>

<p>Given I have:</p>

<pre><code>        C
 A B     
 0 one  3
 1 one  2
 2 two  1
</code></pre>

<p>I can do:</p>

<pre><code>df = df.reset_index()
uniq_b = df.B.unique()
df = df.set_index(['A','B'])
</code></pre>

<p>Is there a way built in pandas to do this?</p>
";21242.0;"['        C\n A B     \n 0 one  3\n 1 one  2\n 2 two  1\n', ""df = df.reset_index()\nuniq_b = df.B.unique()\ndf = df.set_index(['A','B'])\n""]";"['DataFrame', '        C\n A B     \n 0 one  3\n 1 one  2\n 2 two  1\n', ""df = df.reset_index()\nuniq_b = df.B.unique()\ndf = df.set_index(['A','B'])\n""]"
111;2.0;0;13921647;;1;43;<python><r><pandas>;Python - Dimension of Data Frame;"<p>New to Python.</p>

<p>In R, you can get the dimension of a matrix using dim(...).   What is the corresponding function in Python Pandas for their data frame?</p>
";25371.0;[];[]
112;2.0;0;13999850;;1;28;<python><pandas>;How to specify date format when using pandas.to_csv?;"<p>The default output format of <code>to_csv()</code> is:</p>

<pre><code>12/14/2012  12:00:00 AM
</code></pre>

<p>I cannot figure out how to output only the date part with specific format:</p>

<pre><code>20121214
</code></pre>

<p>or date and time in two separate columns in the csv file:</p>

<pre><code>20121214,  084530
</code></pre>

<p>The documentation is too brief to give me any clue as to how to do these. Can anyone help?</p>
";23394.0;['12/14/2012  12:00:00 AM\n', '20121214\n', '20121214,  084530\n'];['to_csv()', '12/14/2012  12:00:00 AM\n', '20121214\n', '20121214,  084530\n']
113;2.0;2;14016247;;1;49;<python><pandas>;Python - find integer index of rows with NaN in pandas;"<p>I have a pandas DataFrame like this:</p>

<pre><code>                    a         b
2011-01-01 00:00:00 1.883381  -0.416629
2011-01-01 01:00:00 0.149948  -1.782170
2011-01-01 02:00:00 -0.407604 0.314168
2011-01-01 03:00:00 1.452354  NaN
2011-01-01 04:00:00 -1.224869 -0.947457
2011-01-01 05:00:00 0.498326  0.070416
2011-01-01 06:00:00 0.401665  NaN
2011-01-01 07:00:00 -0.019766 0.533641
2011-01-01 08:00:00 -1.101303 -1.408561
2011-01-01 09:00:00 1.671795  -0.764629
</code></pre>

<p>Is there an efficient way to find the ""integer"" index of rows with NaNs? In this case the desired output should be <code>[3, 6]</code>.</p>
";59715.0;['                    a         b\n2011-01-01 00:00:00 1.883381  -0.416629\n2011-01-01 01:00:00 0.149948  -1.782170\n2011-01-01 02:00:00 -0.407604 0.314168\n2011-01-01 03:00:00 1.452354  NaN\n2011-01-01 04:00:00 -1.224869 -0.947457\n2011-01-01 05:00:00 0.498326  0.070416\n2011-01-01 06:00:00 0.401665  NaN\n2011-01-01 07:00:00 -0.019766 0.533641\n2011-01-01 08:00:00 -1.101303 -1.408561\n2011-01-01 09:00:00 1.671795  -0.764629\n'];['                    a         b\n2011-01-01 00:00:00 1.883381  -0.416629\n2011-01-01 01:00:00 0.149948  -1.782170\n2011-01-01 02:00:00 -0.407604 0.314168\n2011-01-01 03:00:00 1.452354  NaN\n2011-01-01 04:00:00 -1.224869 -0.947457\n2011-01-01 05:00:00 0.498326  0.070416\n2011-01-01 06:00:00 0.401665  NaN\n2011-01-01 07:00:00 -0.019766 0.533641\n2011-01-01 08:00:00 -1.101303 -1.408561\n2011-01-01 09:00:00 1.671795  -0.764629\n', '[3, 6]']
114;4.0;0;14057007;;1;21;<python><filtering><pandas>;Remove rows not .isin('X');"<p>Sorry just getting into Pandas, this seems like it should be a very straight forward question. How can I use the <code>isin('X')</code> to remove rows that <strong>are in</strong> the list <code>X</code>? In R I would write <code>!which(a %in% b)</code>.</p>
";20970.0;[];"[""isin('X')"", 'X', '!which(a %in% b)']"
115;5.0;0;14059094;;1;24;<python><python-2.7><pandas>;I want to multiply two columns in a pandas DataFrame and add the result into a new column;"<p>I'm trying to multiply two existing columns in a pandas Dataframe (orders_df) - Prices (stock close price) and Amount (stock quantities) and add the calculation to a new column called 'Value'. For some reason when I run this code, all the rows under the 'Value' column are positive numbers, while some of the rows should be negative. Under the Action column in the DataFrame there are seven rows with the 'Sell' string and seven with the 'Buy' string.</p>

<pre><code>for i in orders_df.Action:
 if i  == 'Sell':
  orders_df['Value'] = orders_df.Prices*orders_df.Amount
 elif i == 'Buy':
  orders_df['Value'] = -orders_df.Prices*orders_df.Amount)
</code></pre>

<p>Please let me know what i'm doing wrong !</p>
";39480.0;"[""for i in orders_df.Action:\n if i  == 'Sell':\n  orders_df['Value'] = orders_df.Prices*orders_df.Amount\n elif i == 'Buy':\n  orders_df['Value'] = -orders_df.Prices*orders_df.Amount)\n""]";"[""for i in orders_df.Action:\n if i  == 'Sell':\n  orders_df['Value'] = orders_df.Prices*orders_df.Amount\n elif i == 'Buy':\n  orders_df['Value'] = -orders_df.Prices*orders_df.Amount)\n""]"
116;4.0;1;14162723;;1;36;<numpy><pandas><mysql-python>;Replacing Pandas or Numpy Nan with a None to use with MysqlDB;"<p>I am trying to write a Pandas dataframe (or can use a numpy array) to a mysql database using MysqlDB . MysqlDB doesn't seem understand 'nan' and my database throws out an error saying nan is not in the field list. I need to find a way to convert the 'nan' into a NoneType.</p>

<p>Any ideas? </p>
";22191.0;[];[]
117;1.0;0;14225676;;1;43;<python><pandas><openpyxl>;Save list of DataFrames to multisheet Excel spreadsheet;"<p>How can I export a list of DataFrames into one Excel spreadsheet?<br>
The docs for <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_excel.html""><code>to_excel</code></a> state:</p>

<blockquote>
  <p>Notes<br>
  If passing an existing ExcelWriter object, then the sheet will be added
  to the existing workbook.  This can be used to save different
  DataFrames to one workbook</p>
  
  <p><code>writer = ExcelWriter('output.xlsx')</code><br>
  <code>df1.to_excel(writer, 'sheet1')</code><br>
  <code>df2.to_excel(writer, 'sheet2')</code><br>
  <code>writer.save()</code></p>
</blockquote>

<p>Following this, I thought I could write a function which saves a list of DataFrames to one spreadsheet as follows:</p>

<pre><code>from openpyxl.writer.excel import ExcelWriter
def save_xls(list_dfs, xls_path):
    writer = ExcelWriter(xls_path)
    for n, df in enumerate(list_dfs):
        df.to_excel(writer,'sheet%s' % n)
    writer.save()
</code></pre>

<p>However (with a list of two small DataFrames, each of which can save <code>to_excel</code> individually), an exception is raised <em>(Edit: traceback removed)</em>:</p>

<pre><code>AttributeError: 'str' object has no attribute 'worksheets'
</code></pre>

<p>Presumably I am not calling <a href=""http://www.nullege.com/codes/search?cq=openpyxl.writer.excel.ExcelWriter""><code>ExcelWriter</code></a> correctly, how should I be in order to do this?</p>
";23952.0;"[""from openpyxl.writer.excel import ExcelWriter\ndef save_xls(list_dfs, xls_path):\n    writer = ExcelWriter(xls_path)\n    for n, df in enumerate(list_dfs):\n        df.to_excel(writer,'sheet%s' % n)\n    writer.save()\n"", ""AttributeError: 'str' object has no attribute 'worksheets'\n""]";"['to_excel', ""writer = ExcelWriter('output.xlsx')"", ""df1.to_excel(writer, 'sheet1')"", ""df2.to_excel(writer, 'sheet2')"", 'writer.save()', ""from openpyxl.writer.excel import ExcelWriter\ndef save_xls(list_dfs, xls_path):\n    writer = ExcelWriter(xls_path)\n    for n, df in enumerate(list_dfs):\n        df.to_excel(writer,'sheet%s' % n)\n    writer.save()\n"", 'to_excel', ""AttributeError: 'str' object has no attribute 'worksheets'\n"", 'ExcelWriter']"
118;2.0;0;14247586;;1;60;<python><pandas><null><nan>;Python Pandas How to select rows with one or more nulls from a DataFrame without listing columns explicitly?;"<p>I have a dataframe with ~300K rows and ~40 columns.
I want to find out if any rows contain null values - and put these 'null'-rows into a separate dataframe so that I could explore them easily.</p>

<p>I can create a mask explicitly:</p>

<pre><code>mask=False
for col in df.columns: mask = mask | df[col].isnull()
dfnulls = df[mask]
</code></pre>

<p>Or I can do something like:</p>

<pre><code>df.ix[df.index[(df.T == np.nan).sum() &gt; 1]]
</code></pre>

<p>Is there a more elegant way of doing it (locating rows with nulls in them)?</p>
";66974.0;['mask=False\nfor col in df.columns: mask = mask | df[col].isnull()\ndfnulls = df[mask]\n', 'df.ix[df.index[(df.T == np.nan).sum() > 1]]\n'];['mask=False\nfor col in df.columns: mask = mask | df[col].isnull()\ndfnulls = df[mask]\n', 'df.ix[df.index[(df.T == np.nan).sum() > 1]]\n']
119;11.0;4;14262433;;1;577;<python><mongodb><pandas><large-data><hdf5>;"""Large data"" work flows using pandas";"<p>I have tried to puzzle out an answer to this question for many months while learning pandas.  I use SAS for my day-to-day work and it is great for it's out-of-core support.  However, SAS is horrible as a piece of software for numerous other reasons.</p>

<p>One day I hope to replace my use of SAS with python and pandas, but I currently lack an out-of-core workflow for large datasets.  I'm not talking about ""big data"" that requires a distributed network, but rather files too large to fit in memory but small enough to fit on a hard-drive.</p>

<p>My first thought is to use <code>HDFStore</code> to hold large datasets on disk and pull only the pieces I need into dataframes for analysis.  Others have mentioned MongoDB as an easier to use alternative.  My question is this:</p>

<p>What are some best-practice workflows for accomplishing the following:</p>

<ol>
<li>Loading flat files into a permanent, on-disk database structure</li>
<li>Querying that database to retrieve data to feed into a pandas data structure</li>
<li>Updating the database after manipulating pieces in pandas</li>
</ol>

<p>Real-world examples would be much appreciated, especially from anyone who uses pandas on ""large data"".</p>

<p>Edit -- an example of how I would like this to work:</p>

<ol>
<li>Iteratively import a large flat-file and store it in a permanent, on-disk database structure.  These files are typically too large to fit in memory.</li>
<li>In order to use Pandas, I would like to read subsets of this data (usually just a few columns at a time) that can fit in memory.</li>
<li>I would create new columns by performing various operations on the selected columns.</li>
<li>I would then have to append these new columns into the database structure.</li>
</ol>

<p>I am trying to find a best-practice way of performing these steps. Reading links about pandas and pytables it seems that appending a new column could be a problem.</p>

<p>Edit -- Responding to Jeff's questions specifically:</p>

<ol>
<li>I am building consumer credit risk models. The kinds of data include phone, SSN and address characteristics; property values; derogatory information like criminal records, bankruptcies, etc... The datasets I use every day have nearly 1,000 to 2,000 fields on average of mixed data types: continuous, nominal and ordinal variables of both numeric and character data.  I rarely append rows, but I do perform many operations that create new columns.</li>
<li>Typical operations involve combining several columns using conditional logic into a new, compound column. For example, <code>if var1 &gt; 2 then newvar = 'A' elif var2 = 4 then newvar = 'B'</code>.  The result of these operations is a new column for every record in my dataset.</li>
<li>Finally, I would like to append these new columns into the on-disk data structure.  I would repeat step 2, exploring the data with crosstabs and descriptive statistics trying to find interesting, intuitive relationships to model.</li>
<li>A typical project file is usually about 1GB.  Files are organized into such a manner where a row consists of a record of consumer data.  Each row has the same number of columns for every record.  This will always be the case.</li>
<li>It's pretty rare that I would subset by rows when creating a new column.  However, it's pretty common for me to subset on rows when creating reports or generating descriptive statistics.  For example, I might want to create a simple frequency for a specific line of business, say Retail credit cards.  To do this, I would select only those records where the line of business = retail in addition to whichever columns I want to report on.  When creating new columns, however, I would pull all rows of data and only the columns I need for the operations.</li>
<li>The modeling process requires that I analyze every column, look for interesting relationships with some outcome variable, and create new compound columns that describe those relationships.  The columns that I explore are usually done in small sets.  For example, I will focus on a set of say 20 columns just dealing with property values and observe how they relate to defaulting on a loan.  Once those are explored and new columns are created, I then move on to another group of columns, say college education, and repeat the process.  What I'm doing is creating candidate variables that explain the relationship between my data and some outcome.  At the very end of this process, I apply some learning techniques that create an equation out of those compound columns.</li>
</ol>

<p>It is rare that I would ever add rows to the dataset.  I will nearly always be creating new columns (variables or features in statistics/machine learning parlance).</p>
";161470.0;[];"['HDFStore', ""if var1 > 2 then newvar = 'A' elif var2 = 4 then newvar = 'B'""]"
120;1.0;2;14300137;;1;52;<python><matplotlib><plot><dataframe><pandas>;making matplotlib scatter plots from dataframes in Python's pandas;"<p>What is the best way to make a series of scatter plots using <code>matplotlib</code> from a <code>pandas</code> dataframe in Python? </p>

<p>For example, if I have a dataframe <code>df</code> that has some columns of interest, I find myself typically converting everything to arrays:</p>

<pre><code>import matplotlib.pylab as plt
# df is a DataFrame: fetch col1 and col2 
# and drop na rows if any of the columns are NA
mydata = df[[""col1"", ""col2""]].dropna(how=""any"")
# Now plot with matplotlib
vals = mydata.values
plt.scatter(vals[:, 0], vals[:, 1])
</code></pre>

<p>The problem with converting everything to array before plotting is that it forces you to break out of dataframes.</p>

<p>Consider these two use cases where having the full dataframe is essential to plotting:</p>

<ol>
<li><p>For example, what if you wanted to now look at all the values of <code>col3</code> for the corresponding values that you plotted in the call to <code>scatter</code>, and color each point (or size) it by that value? You'd have to go back, pull out the non-na values of <code>col1,col2</code> and check what their corresponding values.</p>

<p>Is there a way to plot while preserving the dataframe? For example:</p>

<pre><code>mydata = df.dropna(how=""any"", subset=[""col1"", ""col2""])
# plot a scatter of col1 by col2, with sizes according to col3
scatter(mydata([""col1"", ""col2""]), s=mydata[""col3""])
</code></pre></li>
<li><p>Similarly, imagine that you wanted to filter or color each point differently depending on the values of some of its columns. E.g. what if you wanted to automatically plot the labels of the points that meet a certain cutoff on <code>col1, col2</code> alongside them (where the labels are stored in another column of the df), or color these points differently, like people do with dataframes in R.  For example:</p>

<pre><code>mydata = df.dropna(how=""any"", subset=[""col1"", ""col2""]) 
myscatter = scatter(mydata[[""col1"", ""col2""]], s=1)
# Plot in red, with smaller size, all the points that 
# have a col2 value greater than 0.5
myscatter.replot(mydata[""col2""] &gt; 0.5, color=""red"", s=0.5)
</code></pre></li>
</ol>

<p>How can this be done?</p>

<p><strong>EDIT</strong> Reply to crewbum:</p>

<p>You say that the best way is to plot each condition (like <code>subset_a</code>, <code>subset_b</code>) separately. What if you have many conditions, e.g. you want to split up the scatters into 4 types of points or even more, plotting each in different shape/color. How can you elegantly apply condition a, b, c, etc. and make sure you then plot ""the rest"" (things not in any of these conditions) as the last step? </p>

<p>Similarly in your example where you plot <code>col1,col2</code> differently based on <code>col3</code>, what if there are NA values that break the association between <code>col1,col2,col3</code>? For example if you want to plot all <code>col2</code> values based on their <code>col3</code> values, but some rows have an NA value in either <code>col1</code> or <code>col3</code>, forcing you to use <code>dropna</code> first. So you would do:</p>

<pre><code>mydata = df.dropna(how=""any"", subset=[""col1"", ""col2"", ""col3"")
</code></pre>

<p>then you can plot using <code>mydata</code> like you show -- plotting the scatter between <code>col1,col2</code> using the values of <code>col3</code>. But <code>mydata</code> will be missing some points that have values for <code>col1,col2</code> but are NA for <code>col3</code>, and those still have to be plotted... so how would you basically plot ""the rest"" of the data, i.e. the points that are <em>not</em> in the filtered set <code>mydata</code>?</p>
";43938.0;"['import matplotlib.pylab as plt\n# df is a DataFrame: fetch col1 and col2 \n# and drop na rows if any of the columns are NA\nmydata = df[[""col1"", ""col2""]].dropna(how=""any"")\n# Now plot with matplotlib\nvals = mydata.values\nplt.scatter(vals[:, 0], vals[:, 1])\n', 'mydata = df.dropna(how=""any"", subset=[""col1"", ""col2""])\n# plot a scatter of col1 by col2, with sizes according to col3\nscatter(mydata([""col1"", ""col2""]), s=mydata[""col3""])\n', 'mydata = df.dropna(how=""any"", subset=[""col1"", ""col2""]) \nmyscatter = scatter(mydata[[""col1"", ""col2""]], s=1)\n# Plot in red, with smaller size, all the points that \n# have a col2 value greater than 0.5\nmyscatter.replot(mydata[""col2""] > 0.5, color=""red"", s=0.5)\n', 'mydata = df.dropna(how=""any"", subset=[""col1"", ""col2"", ""col3"")\n']";"['matplotlib', 'pandas', 'df', 'import matplotlib.pylab as plt\n# df is a DataFrame: fetch col1 and col2 \n# and drop na rows if any of the columns are NA\nmydata = df[[""col1"", ""col2""]].dropna(how=""any"")\n# Now plot with matplotlib\nvals = mydata.values\nplt.scatter(vals[:, 0], vals[:, 1])\n', 'col3', 'scatter', 'col1,col2', 'mydata = df.dropna(how=""any"", subset=[""col1"", ""col2""])\n# plot a scatter of col1 by col2, with sizes according to col3\nscatter(mydata([""col1"", ""col2""]), s=mydata[""col3""])\n', 'col1, col2', 'mydata = df.dropna(how=""any"", subset=[""col1"", ""col2""]) \nmyscatter = scatter(mydata[[""col1"", ""col2""]], s=1)\n# Plot in red, with smaller size, all the points that \n# have a col2 value greater than 0.5\nmyscatter.replot(mydata[""col2""] > 0.5, color=""red"", s=0.5)\n', 'subset_a', 'subset_b', 'col1,col2', 'col3', 'col1,col2,col3', 'col2', 'col3', 'col1', 'col3', 'dropna', 'mydata = df.dropna(how=""any"", subset=[""col1"", ""col2"", ""col3"")\n', 'mydata', 'col1,col2', 'col3', 'mydata', 'col1,col2', 'col3', 'mydata']"
121;7.0;5;14349055;;1;53;<python><r><matplotlib><plot><pandas>;making matplotlib graphs look like R by default?;"<p>Is there a way to make <code>matplotlib</code> behave identically to R, or almost like R, in terms of plotting defaults? For example R treats its axes pretty differently from <code>matplotlib</code>. The following histogram
<img src=""https://i.stack.imgur.com/PC9A7.png"" alt=""enter image description here""></p>

<p>has ""floating axes"" with outward ticks, such that there are no inner ticks (unlike <code>matplotlib</code>) and the axes do not cross ""near"" the origin. Also, the histogram can ""spillover"" to values that are not marked by the tick - e.g. the x-axis ends at 3 but the histograms extends slightly beyond it. How can this be achieved automatically for all histograms in <code>matplotlib</code>?</p>

<p>Related question: scatter plots and line plots have different default axes settings in R, for example:
<img src=""https://i.stack.imgur.com/Ln7QH.png"" alt=""enter image description here""></p>

<p>There no inner ticks again and the ticks face outward. Also, the ticks start slightly after the origin point (where the y and x axes cross at the bottom left of the axes) and the ticks end slightly before the axes end. This way the labels of the lowest x-axis tick and lowest y-axis tick can't really cross, because there's a space between them and this gives the plots a very elegant clean look.  Note that there's also considerably more space between the axes ticklabels and the ticks themselves.</p>

<p>Also, by default there are no ticks on the non-labeled x or y axes, meaning the y-axis on the left that is parallel to the labeled y-axis on the right has no ticks, and same for the x-axis, again removing clutter from the plots.</p>

<p>Is there a way to make matplotlib look like this? And in general to look by default as much as default R plots?  I like <code>matplotlib</code> a lot but I think the R defaults / out-of-the-box plotting behavior really have gotten things right and its default settings rarely lead to overlapping tick labels, clutter or squished data, so I  would like the defaults to be as much like that as possible.</p>
";21800.0;[];['matplotlib', 'matplotlib', 'matplotlib', 'matplotlib', 'matplotlib']
122;4.0;3;14365542;;1;35;<python><r><csv><pandas>;read csv file and return data.frame in Python;"<p>I have a CSV file, <code>""value.txt""</code> with the following content: 
the first few rows of the file are :</p>

<pre><code>Date,""price"",""factor_1"",""factor_2""
2012-06-11,1600.20,1.255,1.548
2012-06-12,1610.02,1.258,1.554
2012-06-13,1618.07,1.249,1.552
2012-06-14,1624.40,1.253,1.556
2012-06-15,1626.15,1.258,1.552
2012-06-16,1626.15,1.263,1.558
2012-06-17,1626.15,1.264,1.572
</code></pre>

<p>In R we can read this file in using </p>

<pre><code>price &lt;- read.csv(""value.txt"")  
</code></pre>

<p>and that will return a data.frame which I can use for statistical operations:</p>

<pre><code>&gt; price &lt;- read.csv(""value.txt"")
&gt; price
     Date   price factor_1 factor_2
1  2012-06-11 1600.20    1.255    1.548
2  2012-06-12 1610.02    1.258    1.554
3  2012-06-13 1618.07    1.249    1.552
4  2012-06-14 1624.40    1.253    1.556
5  2012-06-15 1626.15    1.258    1.552
6  2012-06-16 1626.15    1.263    1.558
7  2012-06-17 1626.15    1.264    1.572
</code></pre>

<p>Is there a Pythonic way to get the same functionality?</p>
";64795.0;"['Date,""price"",""factor_1"",""factor_2""\n2012-06-11,1600.20,1.255,1.548\n2012-06-12,1610.02,1.258,1.554\n2012-06-13,1618.07,1.249,1.552\n2012-06-14,1624.40,1.253,1.556\n2012-06-15,1626.15,1.258,1.552\n2012-06-16,1626.15,1.263,1.558\n2012-06-17,1626.15,1.264,1.572\n', 'price <- read.csv(""value.txt"")  \n', '> price <- read.csv(""value.txt"")\n> price\n     Date   price factor_1 factor_2\n1  2012-06-11 1600.20    1.255    1.548\n2  2012-06-12 1610.02    1.258    1.554\n3  2012-06-13 1618.07    1.249    1.552\n4  2012-06-14 1624.40    1.253    1.556\n5  2012-06-15 1626.15    1.258    1.552\n6  2012-06-16 1626.15    1.263    1.558\n7  2012-06-17 1626.15    1.264    1.572\n']";"['""value.txt""', 'Date,""price"",""factor_1"",""factor_2""\n2012-06-11,1600.20,1.255,1.548\n2012-06-12,1610.02,1.258,1.554\n2012-06-13,1618.07,1.249,1.552\n2012-06-14,1624.40,1.253,1.556\n2012-06-15,1626.15,1.258,1.552\n2012-06-16,1626.15,1.263,1.558\n2012-06-17,1626.15,1.264,1.572\n', 'price <- read.csv(""value.txt"")  \n', '> price <- read.csv(""value.txt"")\n> price\n     Date   price factor_1 factor_2\n1  2012-06-11 1600.20    1.255    1.548\n2  2012-06-12 1610.02    1.258    1.554\n3  2012-06-13 1618.07    1.249    1.552\n4  2012-06-14 1624.40    1.253    1.556\n5  2012-06-15 1626.15    1.258    1.552\n6  2012-06-16 1626.15    1.263    1.558\n7  2012-06-17 1626.15    1.264    1.572\n']"
123;2.0;1;14380371;;1;30;<python><latex><dataframe><pandas>;Export a LaTeX table from pandas DataFrame;"<p>Is there an easy way to export a data frame (or even a part of it) to LaTeX?  </p>

<p><em>I searched in google and was only able to find solutions using asciitables.</em></p>
";10522.0;[];[]
124;10.0;2;14507794;;1;89;<python><pandas>;Python Pandas - How to flatten a hierarchical index in columns;"<p>I have a data frame with a hierarchical index in axis 1 (columns) (from a groupby.agg operation):</p>

<pre><code>     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf       
                                     sum   sum   sum    sum   amax   amin
0  702730  26451  1993      1    1     1     0    12     13  30.92  24.98
1  702730  26451  1993      1    2     0     0    13     13  32.00  24.98
2  702730  26451  1993      1    3     1    10     2     13  23.00   6.98
3  702730  26451  1993      1    4     1     0    12     13  10.04   3.92
4  702730  26451  1993      1    5     3     0    10     13  19.94  10.94
</code></pre>

<p>I want to flatten it, so that it looks like this (names aren't critical - I could rename):</p>

<pre><code>     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf_amax  tmpf_amin   
0  702730  26451  1993      1    1     1     0    12     13  30.92          24.98
1  702730  26451  1993      1    2     0     0    13     13  32.00          24.98
2  702730  26451  1993      1    3     1    10     2     13  23.00          6.98
3  702730  26451  1993      1    4     1     0    12     13  10.04          3.92
4  702730  26451  1993      1    5     3     0    10     13  19.94          10.94
</code></pre>

<p>How do I do this? (I've tried a lot, to no avail.) </p>

<p>Per a suggestion, here is the head in dict form</p>

<pre><code>{('USAF', ''): {0: '702730',
  1: '702730',
  2: '702730',
  3: '702730',
  4: '702730'},
 ('WBAN', ''): {0: '26451', 1: '26451', 2: '26451', 3: '26451', 4: '26451'},
 ('day', ''): {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},
 ('month', ''): {0: 1, 1: 1, 2: 1, 3: 1, 4: 1},
 ('s_CD', 'sum'): {0: 12.0, 1: 13.0, 2: 2.0, 3: 12.0, 4: 10.0},
 ('s_CL', 'sum'): {0: 0.0, 1: 0.0, 2: 10.0, 3: 0.0, 4: 0.0},
 ('s_CNT', 'sum'): {0: 13.0, 1: 13.0, 2: 13.0, 3: 13.0, 4: 13.0},
 ('s_PC', 'sum'): {0: 1.0, 1: 0.0, 2: 1.0, 3: 1.0, 4: 3.0},
 ('tempf', 'amax'): {0: 30.920000000000002,
  1: 32.0,
  2: 23.0,
  3: 10.039999999999999,
  4: 19.939999999999998},
 ('tempf', 'amin'): {0: 24.98,
  1: 24.98,
  2: 6.9799999999999969,
  3: 3.9199999999999982,
  4: 10.940000000000001},
 ('year', ''): {0: 1993, 1: 1993, 2: 1993, 3: 1993, 4: 1993}}
</code></pre>
";47981.0;"['     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf       \n                                     sum   sum   sum    sum   amax   amin\n0  702730  26451  1993      1    1     1     0    12     13  30.92  24.98\n1  702730  26451  1993      1    2     0     0    13     13  32.00  24.98\n2  702730  26451  1993      1    3     1    10     2     13  23.00   6.98\n3  702730  26451  1993      1    4     1     0    12     13  10.04   3.92\n4  702730  26451  1993      1    5     3     0    10     13  19.94  10.94\n', '     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf_amax  tmpf_amin   \n0  702730  26451  1993      1    1     1     0    12     13  30.92          24.98\n1  702730  26451  1993      1    2     0     0    13     13  32.00          24.98\n2  702730  26451  1993      1    3     1    10     2     13  23.00          6.98\n3  702730  26451  1993      1    4     1     0    12     13  10.04          3.92\n4  702730  26451  1993      1    5     3     0    10     13  19.94          10.94\n', ""{('USAF', ''): {0: '702730',\n  1: '702730',\n  2: '702730',\n  3: '702730',\n  4: '702730'},\n ('WBAN', ''): {0: '26451', 1: '26451', 2: '26451', 3: '26451', 4: '26451'},\n ('day', ''): {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n ('month', ''): {0: 1, 1: 1, 2: 1, 3: 1, 4: 1},\n ('s_CD', 'sum'): {0: 12.0, 1: 13.0, 2: 2.0, 3: 12.0, 4: 10.0},\n ('s_CL', 'sum'): {0: 0.0, 1: 0.0, 2: 10.0, 3: 0.0, 4: 0.0},\n ('s_CNT', 'sum'): {0: 13.0, 1: 13.0, 2: 13.0, 3: 13.0, 4: 13.0},\n ('s_PC', 'sum'): {0: 1.0, 1: 0.0, 2: 1.0, 3: 1.0, 4: 3.0},\n ('tempf', 'amax'): {0: 30.920000000000002,\n  1: 32.0,\n  2: 23.0,\n  3: 10.039999999999999,\n  4: 19.939999999999998},\n ('tempf', 'amin'): {0: 24.98,\n  1: 24.98,\n  2: 6.9799999999999969,\n  3: 3.9199999999999982,\n  4: 10.940000000000001},\n ('year', ''): {0: 1993, 1: 1993, 2: 1993, 3: 1993, 4: 1993}}\n""]";"['     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf       \n                                     sum   sum   sum    sum   amax   amin\n0  702730  26451  1993      1    1     1     0    12     13  30.92  24.98\n1  702730  26451  1993      1    2     0     0    13     13  32.00  24.98\n2  702730  26451  1993      1    3     1    10     2     13  23.00   6.98\n3  702730  26451  1993      1    4     1     0    12     13  10.04   3.92\n4  702730  26451  1993      1    5     3     0    10     13  19.94  10.94\n', '     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf_amax  tmpf_amin   \n0  702730  26451  1993      1    1     1     0    12     13  30.92          24.98\n1  702730  26451  1993      1    2     0     0    13     13  32.00          24.98\n2  702730  26451  1993      1    3     1    10     2     13  23.00          6.98\n3  702730  26451  1993      1    4     1     0    12     13  10.04          3.92\n4  702730  26451  1993      1    5     3     0    10     13  19.94          10.94\n', ""{('USAF', ''): {0: '702730',\n  1: '702730',\n  2: '702730',\n  3: '702730',\n  4: '702730'},\n ('WBAN', ''): {0: '26451', 1: '26451', 2: '26451', 3: '26451', 4: '26451'},\n ('day', ''): {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n ('month', ''): {0: 1, 1: 1, 2: 1, 3: 1, 4: 1},\n ('s_CD', 'sum'): {0: 12.0, 1: 13.0, 2: 2.0, 3: 12.0, 4: 10.0},\n ('s_CL', 'sum'): {0: 0.0, 1: 0.0, 2: 10.0, 3: 0.0, 4: 0.0},\n ('s_CNT', 'sum'): {0: 13.0, 1: 13.0, 2: 13.0, 3: 13.0, 4: 13.0},\n ('s_PC', 'sum'): {0: 1.0, 1: 0.0, 2: 1.0, 3: 1.0, 4: 3.0},\n ('tempf', 'amax'): {0: 30.920000000000002,\n  1: 32.0,\n  2: 23.0,\n  3: 10.039999999999999,\n  4: 19.939999999999998},\n ('tempf', 'amin'): {0: 24.98,\n  1: 24.98,\n  2: 6.9799999999999969,\n  3: 3.9199999999999982,\n  4: 10.940000000000001},\n ('year', ''): {0: 1993, 1: 1993, 2: 1993, 3: 1993, 4: 1993}}\n""]"
125;1.0;0;14529838;;1;59;<python><group-by><aggregate-functions><pandas>;Apply multiple functions to multiple groupby columns;"<p>The <a href=""http://pandas.pydata.org/pandas-docs/dev/groupby.html#applying-multiple-functions-at-once"">docs</a> show how to apply multiple functions on a groupby object at a time using a dict with the output column names as the keys:</p>

<pre><code>In [563]: grouped['D'].agg({'result1' : np.sum,
   .....:                   'result2' : np.mean})
   .....:
Out[563]: 
      result2   result1
A                      
bar -0.579846 -1.739537
foo -0.280588 -1.402938
</code></pre>

<p>However, this only works on a Series groupby object. And when a dict is similarly passed to a groupby DataFrame, it expects the keys to be the column names that the function will be applied to.</p>

<p>What I want to do is apply multiple functions to several columns (but certain columns will be operated on multiple times). Also, <em>some functions will depend on other columns in the groupby object</em> (like sumif functions). My current solution is to go column by column, and doing something like the code above, using lambdas for functions that depend on other rows. But this is taking a long time, (I think it takes a long time to iterate through a groupby object). I'll have to change it so that I iterate through the whole groupby object in a single run, but I'm wondering if there's a built in way in pandas to do this somewhat cleanly.</p>

<p>For example, I've tried something like </p>

<pre><code>grouped.agg({'C_sum' : lambda x: x['C'].sum(),
             'C_std': lambda x: x['C'].std(),
             'D_sum' : lambda x: x['D'].sum()},
             'D_sumifC3': lambda x: x['D'][x['C'] == 3].sum(), ...)
</code></pre>

<p>but as expected I get a KeyError (since the keys have to be a column if <code>agg</code> is called from a DataFrame).</p>

<p>Is there any built in way to do what I'd like to do, or a possibility that this functionality may be added, or will I just need to iterate through the groupby manually?</p>

<p>Thanks</p>
";34437.0;"[""In [563]: grouped['D'].agg({'result1' : np.sum,\n   .....:                   'result2' : np.mean})\n   .....:\nOut[563]: \n      result2   result1\nA                      \nbar -0.579846 -1.739537\nfoo -0.280588 -1.402938\n"", ""grouped.agg({'C_sum' : lambda x: x['C'].sum(),\n             'C_std': lambda x: x['C'].std(),\n             'D_sum' : lambda x: x['D'].sum()},\n             'D_sumifC3': lambda x: x['D'][x['C'] == 3].sum(), ...)\n""]";"[""In [563]: grouped['D'].agg({'result1' : np.sum,\n   .....:                   'result2' : np.mean})\n   .....:\nOut[563]: \n      result2   result1\nA                      \nbar -0.579846 -1.739537\nfoo -0.280588 -1.402938\n"", ""grouped.agg({'C_sum' : lambda x: x['C'].sum(),\n             'C_std': lambda x: x['C'].std(),\n             'D_sum' : lambda x: x['D'].sum()},\n             'D_sumifC3': lambda x: x['D'][x['C'] == 3].sum(), ...)\n"", 'agg']"
126;1.0;1;14627380;;1;21;<python><html><css><dataframe><pandas>;pandas: HTML output with conditional formatting;"<p>I am trying to format a table, such that data in each column are formatted in a style depending on their values (similar to conditional formatting in spreadsheet programs). How can I achieve that in pandas using the HTML formatter?</p>

<p>A typical use case is highlighting significant values in a table. For example:</p>

<pre><code>    correlation  p-value
0   0.5          0.1
1   0.1          0.8
2   0.9          *0.01*
</code></pre>

<p>pandas allows to define custom formatters for HTML output - to obtain above output one could use:</p>

<pre><code>import pandas as pd
from pandas.core import format
from StringIO import StringIO
buf = StringIO()
df = pd.DataFrame({'correlation':[0.5, 0.1,0.9], 'p_value':[0.1,0.8,0.01]})
fmt = format.DataFrameFormatter(df, 
          formatters={'p_value':lambda x: ""*%f*"" % x if x&lt;0.05 else str(x)})
format.HTMLFormatter(fmt).write_result(buf)
</code></pre>

<p>However, I would like to change the style for significant values (for example, by using bold font).</p>

<p>A possible solution would be to attach a CSS class to <code>&lt;td&gt;</code> tags in the HTML output, which could be then formatted using CSS stylesheet. The above would then become:</p>

<pre><code>&lt;table border=""1"" class=""dataframe""&gt;
  &lt;thead&gt;
    &lt;tr style=""text-align: right;""&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;correlation&lt;/th&gt;
      &lt;th&gt;p_value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt; 0.5&lt;/td&gt;
      &lt;td&gt; 0.10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt; 0.1&lt;/td&gt;
      &lt;td&gt; 0.80&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt; 0.9&lt;/td&gt;
      &lt;td class='significant'&gt; 0.01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</code></pre>

<p><strong>Edit</strong>: As suggested by @Andy-Hayden I can add formatting by simply replacing stars with <code>&lt;span class=""signifcant""&gt;...&lt;/span&gt;</code> in my example:</p>

<pre><code>import pandas as pd
from StringIO import StringIO
buf = StringIO()
significant = lambda x: '&lt;span class=""significant""&gt;%f&lt;/span&gt;' % x if x&lt;0.05 else str(x)
df = pd.DataFrame({'correlation':[0.5, 0.1,0.9], 'p_value':[0.1,0.8,0.01]})
df.to_html(buf, formatters={'p_value': significant})
</code></pre>

<p>Newer versions of pandas escape the tags. To avoid it replace last line with:</p>

<pre><code>df.to_html(buf, formatters={'p_value': significant}, escape=False)
</code></pre>
";9977.0;"['    correlation  p-value\n0   0.5          0.1\n1   0.1          0.8\n2   0.9          *0.01*\n', 'import pandas as pd\nfrom pandas.core import format\nfrom StringIO import StringIO\nbuf = StringIO()\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\nfmt = format.DataFrameFormatter(df, \n          formatters={\'p_value\':lambda x: ""*%f*"" % x if x<0.05 else str(x)})\nformat.HTMLFormatter(fmt).write_result(buf)\n', '<table border=""1"" class=""dataframe"">\n  <thead>\n    <tr style=""text-align: right;"">\n      <th></th>\n      <th>correlation</th>\n      <th>p_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td> 0.5</td>\n      <td> 0.10</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td> 0.1</td>\n      <td> 0.80</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td> 0.9</td>\n      <td class=\'significant\'> 0.01</td>\n    </tr>\n  </tbody>\n</table>\n', 'import pandas as pd\nfrom StringIO import StringIO\nbuf = StringIO()\nsignificant = lambda x: \'<span class=""significant"">%f</span>\' % x if x<0.05 else str(x)\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\ndf.to_html(buf, formatters={\'p_value\': significant})\n', ""df.to_html(buf, formatters={'p_value': significant}, escape=False)\n""]";"['    correlation  p-value\n0   0.5          0.1\n1   0.1          0.8\n2   0.9          *0.01*\n', 'import pandas as pd\nfrom pandas.core import format\nfrom StringIO import StringIO\nbuf = StringIO()\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\nfmt = format.DataFrameFormatter(df, \n          formatters={\'p_value\':lambda x: ""*%f*"" % x if x<0.05 else str(x)})\nformat.HTMLFormatter(fmt).write_result(buf)\n', '<td>', '<table border=""1"" class=""dataframe"">\n  <thead>\n    <tr style=""text-align: right;"">\n      <th></th>\n      <th>correlation</th>\n      <th>p_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td> 0.5</td>\n      <td> 0.10</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td> 0.1</td>\n      <td> 0.80</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td> 0.9</td>\n      <td class=\'significant\'> 0.01</td>\n    </tr>\n  </tbody>\n</table>\n', '<span class=""signifcant"">...</span>', 'import pandas as pd\nfrom StringIO import StringIO\nbuf = StringIO()\nsignificant = lambda x: \'<span class=""significant"">%f</span>\' % x if x<0.05 else str(x)\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\ndf.to_html(buf, formatters={\'p_value\': significant})\n', ""df.to_html(buf, formatters={'p_value': significant}, escape=False)\n""]"
127;5.0;0;14661701;;1;133;<python><pandas>;How to drop a list of rows from Pandas dataframe?;"<p>I have a dataframe df :</p>

<pre><code>&gt;&gt;&gt; df
                  sales  discount  net_sales    cogs
STK_ID RPT_Date                                     
600141 20060331   2.709       NaN      2.709   2.245
       20060630   6.590       NaN      6.590   5.291
       20060930  10.103       NaN     10.103   7.981
       20061231  15.915       NaN     15.915  12.686
       20070331   3.196       NaN      3.196   2.710
       20070630   7.907       NaN      7.907   6.459
</code></pre>

<p>Then I want to drop rows with certain sequence numbers which indicated in a list, suppose here is <code>[1,2,4],</code> then left:</p>

<pre><code>                  sales  discount  net_sales    cogs
STK_ID RPT_Date                                     
600141 20060331   2.709       NaN      2.709   2.245
       20061231  15.915       NaN     15.915  12.686
       20070630   7.907       NaN      7.907   6.459
</code></pre>

<p>How or what function can do that ?</p>
";202455.0;['>>> df\n                  sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                     \n600141 20060331   2.709       NaN      2.709   2.245\n       20060630   6.590       NaN      6.590   5.291\n       20060930  10.103       NaN     10.103   7.981\n       20061231  15.915       NaN     15.915  12.686\n       20070331   3.196       NaN      3.196   2.710\n       20070630   7.907       NaN      7.907   6.459\n', '                  sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                     \n600141 20060331   2.709       NaN      2.709   2.245\n       20061231  15.915       NaN     15.915  12.686\n       20070630   7.907       NaN      7.907   6.459\n'];['>>> df\n                  sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                     \n600141 20060331   2.709       NaN      2.709   2.245\n       20060630   6.590       NaN      6.590   5.291\n       20060930  10.103       NaN     10.103   7.981\n       20061231  15.915       NaN     15.915  12.686\n       20070331   3.196       NaN      3.196   2.710\n       20070630   7.907       NaN      7.907   6.459\n', '[1,2,4],', '                  sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                     \n600141 20060331   2.709       NaN      2.709   2.245\n       20061231  15.915       NaN     15.915  12.686\n       20070630   7.907       NaN      7.907   6.459\n']
128;2.0;4;14663004;;1;53;<pandas>;How to get the last n row of pandas dataframe?;"<p>I have pandas dataframe <code>df1</code> and <code>df2</code> (df1 is vanila dataframe, df2 is indexed by 'STK_ID' &amp; 'RPT_Date') :</p>

<pre><code>&gt;&gt;&gt; df1
    STK_ID  RPT_Date  TClose   sales  discount
0   000568  20060331    3.69   5.975       NaN
1   000568  20060630    9.14  10.143       NaN
2   000568  20060930    9.49  13.854       NaN
3   000568  20061231   15.84  19.262       NaN
4   000568  20070331   17.00   6.803       NaN
5   000568  20070630   26.31  12.940       NaN
6   000568  20070930   39.12  19.977       NaN
7   000568  20071231   45.94  29.269       NaN
8   000568  20080331   38.75  12.668       NaN
9   000568  20080630   30.09  21.102       NaN
10  000568  20080930   26.00  30.769       NaN

&gt;&gt;&gt; df2
                 TClose   sales  discount  net_sales    cogs
STK_ID RPT_Date                                             
000568 20060331    3.69   5.975       NaN      5.975   2.591
       20060630    9.14  10.143       NaN     10.143   4.363
       20060930    9.49  13.854       NaN     13.854   5.901
       20061231   15.84  19.262       NaN     19.262   8.407
       20070331   17.00   6.803       NaN      6.803   2.815
       20070630   26.31  12.940       NaN     12.940   5.418
       20070930   39.12  19.977       NaN     19.977   8.452
       20071231   45.94  29.269       NaN     29.269  12.606
       20080331   38.75  12.668       NaN     12.668   3.958
       20080630   30.09  21.102       NaN     21.102   7.431
</code></pre>

<p>I can get the last 3 rows of df2 by:</p>

<pre><code>&gt;&gt;&gt; df2.ix[-3:]
                 TClose   sales  discount  net_sales    cogs
STK_ID RPT_Date                                             
000568 20071231   45.94  29.269       NaN     29.269  12.606
       20080331   38.75  12.668       NaN     12.668   3.958
       20080630   30.09  21.102       NaN     21.102   7.431
</code></pre>

<p>while <code>df1.ix[-3:]</code> give all the rows: </p>

<pre><code>&gt;&gt;&gt; df1.ix[-3:]
    STK_ID  RPT_Date  TClose   sales  discount
0   000568  20060331    3.69   5.975       NaN
1   000568  20060630    9.14  10.143       NaN
2   000568  20060930    9.49  13.854       NaN
3   000568  20061231   15.84  19.262       NaN
4   000568  20070331   17.00   6.803       NaN
5   000568  20070630   26.31  12.940       NaN
6   000568  20070930   39.12  19.977       NaN
7   000568  20071231   45.94  29.269       NaN
8   000568  20080331   38.75  12.668       NaN
9   000568  20080630   30.09  21.102       NaN
10  000568  20080930   26.00  30.769       NaN
</code></pre>

<p>Why ? How to get the last 3 rows of <code>df1</code> (dataframe without index) ?
Pandas 0.10.1</p>
";73792.0;['>>> df1\n    STK_ID  RPT_Date  TClose   sales  discount\n0   000568  20060331    3.69   5.975       NaN\n1   000568  20060630    9.14  10.143       NaN\n2   000568  20060930    9.49  13.854       NaN\n3   000568  20061231   15.84  19.262       NaN\n4   000568  20070331   17.00   6.803       NaN\n5   000568  20070630   26.31  12.940       NaN\n6   000568  20070930   39.12  19.977       NaN\n7   000568  20071231   45.94  29.269       NaN\n8   000568  20080331   38.75  12.668       NaN\n9   000568  20080630   30.09  21.102       NaN\n10  000568  20080930   26.00  30.769       NaN\n\n>>> df2\n                 TClose   sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                             \n000568 20060331    3.69   5.975       NaN      5.975   2.591\n       20060630    9.14  10.143       NaN     10.143   4.363\n       20060930    9.49  13.854       NaN     13.854   5.901\n       20061231   15.84  19.262       NaN     19.262   8.407\n       20070331   17.00   6.803       NaN      6.803   2.815\n       20070630   26.31  12.940       NaN     12.940   5.418\n       20070930   39.12  19.977       NaN     19.977   8.452\n       20071231   45.94  29.269       NaN     29.269  12.606\n       20080331   38.75  12.668       NaN     12.668   3.958\n       20080630   30.09  21.102       NaN     21.102   7.431\n', '>>> df2.ix[-3:]\n                 TClose   sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                             \n000568 20071231   45.94  29.269       NaN     29.269  12.606\n       20080331   38.75  12.668       NaN     12.668   3.958\n       20080630   30.09  21.102       NaN     21.102   7.431\n', '>>> df1.ix[-3:]\n    STK_ID  RPT_Date  TClose   sales  discount\n0   000568  20060331    3.69   5.975       NaN\n1   000568  20060630    9.14  10.143       NaN\n2   000568  20060930    9.49  13.854       NaN\n3   000568  20061231   15.84  19.262       NaN\n4   000568  20070331   17.00   6.803       NaN\n5   000568  20070630   26.31  12.940       NaN\n6   000568  20070930   39.12  19.977       NaN\n7   000568  20071231   45.94  29.269       NaN\n8   000568  20080331   38.75  12.668       NaN\n9   000568  20080630   30.09  21.102       NaN\n10  000568  20080930   26.00  30.769       NaN\n'];['df1', 'df2', '>>> df1\n    STK_ID  RPT_Date  TClose   sales  discount\n0   000568  20060331    3.69   5.975       NaN\n1   000568  20060630    9.14  10.143       NaN\n2   000568  20060930    9.49  13.854       NaN\n3   000568  20061231   15.84  19.262       NaN\n4   000568  20070331   17.00   6.803       NaN\n5   000568  20070630   26.31  12.940       NaN\n6   000568  20070930   39.12  19.977       NaN\n7   000568  20071231   45.94  29.269       NaN\n8   000568  20080331   38.75  12.668       NaN\n9   000568  20080630   30.09  21.102       NaN\n10  000568  20080930   26.00  30.769       NaN\n\n>>> df2\n                 TClose   sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                             \n000568 20060331    3.69   5.975       NaN      5.975   2.591\n       20060630    9.14  10.143       NaN     10.143   4.363\n       20060930    9.49  13.854       NaN     13.854   5.901\n       20061231   15.84  19.262       NaN     19.262   8.407\n       20070331   17.00   6.803       NaN      6.803   2.815\n       20070630   26.31  12.940       NaN     12.940   5.418\n       20070930   39.12  19.977       NaN     19.977   8.452\n       20071231   45.94  29.269       NaN     29.269  12.606\n       20080331   38.75  12.668       NaN     12.668   3.958\n       20080630   30.09  21.102       NaN     21.102   7.431\n', '>>> df2.ix[-3:]\n                 TClose   sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                             \n000568 20071231   45.94  29.269       NaN     29.269  12.606\n       20080331   38.75  12.668       NaN     12.668   3.958\n       20080630   30.09  21.102       NaN     21.102   7.431\n', 'df1.ix[-3:]', '>>> df1.ix[-3:]\n    STK_ID  RPT_Date  TClose   sales  discount\n0   000568  20060331    3.69   5.975       NaN\n1   000568  20060630    9.14  10.143       NaN\n2   000568  20060930    9.49  13.854       NaN\n3   000568  20061231   15.84  19.262       NaN\n4   000568  20070331   17.00   6.803       NaN\n5   000568  20070630   26.31  12.940       NaN\n6   000568  20070930   39.12  19.977       NaN\n7   000568  20071231   45.94  29.269       NaN\n8   000568  20080331   38.75  12.668       NaN\n9   000568  20080630   30.09  21.102       NaN\n10  000568  20080930   26.00  30.769       NaN\n', 'df1']
129;5.0;0;14688306;;1;35;<python><pandas>;Adding meta-information/metadata to pandas DataFrame;"<p>Is it possible to add some meta-information/metadata to a pandas DataFrame?</p>

<p>For example, the instrument's name used to measure the data, the instrument responsible, etc.</p>

<p><em>One workaround would be to create a column with that information, but it seems wasteful to store a single piece of information in every row!</em></p>
";9816.0;[];[]
130;1.0;0;14733871;;1;42;<python><sorting><pandas><multi-index>;Multi Index Sorting in Pandas;"<p>I have a dataset with multi-index columns in a pandas df that I would like to sort by values in a specific column.  I have tried using sortindex and sortlevel but haven't been able get the results I am looking for.  My dataset looks like:</p>

<pre><code>    Group1    Group2
    A B C     A B C
1   1 0 3     2 5 7
2   5 6 9     1 0 0
3   7 0 2     0 3 5 
</code></pre>

<p>I want to sort all data and the index by column C in Group 1 in descending order so my results look like:</p>

<pre><code>    Group1    Group2
    A B C     A B C
 2  5 6 9     1 0 0
 1  1 0 3     2 5 7
 3  7 0 2     0 3 5 
</code></pre>

<p>Is it possible to do this sort with the structure that my data is in, or should I be swapping Group1 to the index side?</p>
";17418.0;['    Group1    Group2\n    A B C     A B C\n1   1 0 3     2 5 7\n2   5 6 9     1 0 0\n3   7 0 2     0 3 5 \n', '    Group1    Group2\n    A B C     A B C\n 2  5 6 9     1 0 0\n 1  1 0 3     2 5 7\n 3  7 0 2     0 3 5 \n'];['    Group1    Group2\n    A B C     A B C\n1   1 0 3     2 5 7\n2   5 6 9     1 0 0\n3   7 0 2     0 3 5 \n', '    Group1    Group2\n    A B C     A B C\n 2  5 6 9     1 0 0\n 1  1 0 3     2 5 7\n 3  7 0 2     0 3 5 \n']
131;5.0;0;14734533;;1;71;<python><group-by><dataframe><pandas>;How to access pandas groupby dataframe by key;"<p>How do I access the corresponding groupby dataframe in a groupby object by the key? With the following groupby:</p>

<pre><code>rand = np.random.RandomState(1)
df = pd.DataFrame({'A': ['foo', 'bar'] * 3,
                   'B': rand.randn(6),
                   'C': rand.randint(0, 20, 6)})
gb = df.groupby(['A'])
</code></pre>

<p>I can iterate through it to get the keys and groups:</p>

<pre><code>In [11]: for k, gp in gb:
             print 'key=' + str(k)
             print gp
key=bar
     A         B   C
1  bar -0.611756  18
3  bar -1.072969  10
5  bar -2.301539  18
key=foo
     A         B   C
0  foo  1.624345   5
2  foo -0.528172  11
4  foo  0.865408  14
</code></pre>

<p>I would like to be able to do something like</p>

<pre><code>In [12]: gb['foo']
Out[12]:  
     A         B   C
0  foo  1.624345   5
2  foo -0.528172  11
4  foo  0.865408  14
</code></pre>

<p>But when I do that (well, actually I have to do <code>gb[('foo',)]</code>), I get this weird <code>pandas.core.groupby.DataFrameGroupBy</code> thing which doesn't seem to have any methods that correspond to the DataFrame I want.</p>

<p>The best I can think of is</p>

<pre><code>In [13]: def gb_df_key(gb, key, orig_df):
             ix = gb.indices[key]
             return orig_df.ix[ix]

         gb_df_key(gb, 'foo', df)
Out[13]:
     A         B   C
0  foo  1.624345   5
2  foo -0.528172  11
4  foo  0.865408  14  
</code></pre>

<p>but this is kind of nasty, considering how nice pandas usually is at these things.<br>
What's the built-in way of doing this?</p>
";77177.0;"[""rand = np.random.RandomState(1)\ndf = pd.DataFrame({'A': ['foo', 'bar'] * 3,\n                   'B': rand.randn(6),\n                   'C': rand.randint(0, 20, 6)})\ngb = df.groupby(['A'])\n"", ""In [11]: for k, gp in gb:\n             print 'key=' + str(k)\n             print gp\nkey=bar\n     A         B   C\n1  bar -0.611756  18\n3  bar -1.072969  10\n5  bar -2.301539  18\nkey=foo\n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n"", ""In [12]: gb['foo']\nOut[12]:  \n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n"", ""In [13]: def gb_df_key(gb, key, orig_df):\n             ix = gb.indices[key]\n             return orig_df.ix[ix]\n\n         gb_df_key(gb, 'foo', df)\nOut[13]:\n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14  \n""]";"[""rand = np.random.RandomState(1)\ndf = pd.DataFrame({'A': ['foo', 'bar'] * 3,\n                   'B': rand.randn(6),\n                   'C': rand.randint(0, 20, 6)})\ngb = df.groupby(['A'])\n"", ""In [11]: for k, gp in gb:\n             print 'key=' + str(k)\n             print gp\nkey=bar\n     A         B   C\n1  bar -0.611756  18\n3  bar -1.072969  10\n5  bar -2.301539  18\nkey=foo\n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n"", ""In [12]: gb['foo']\nOut[12]:  \n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n"", ""gb[('foo',)]"", 'pandas.core.groupby.DataFrameGroupBy', ""In [13]: def gb_df_key(gb, key, orig_df):\n             ix = gb.indices[key]\n             return orig_df.ix[ix]\n\n         gb_df_key(gb, 'foo', df)\nOut[13]:\n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14  \n""]"
132;2.0;0;14744068;;1;22;<python><pandas>;Prepend a level to a pandas MultiIndex;"<p>I have a DataFrame with a MultiIndex created after some grouping:</p>

<pre><code>import numpy as np
import pandas as p
from numpy.random import randn

df = p.DataFrame({
    'A' : ['a1', 'a1', 'a2', 'a3']
  , 'B' : ['b1', 'b2', 'b3', 'b4']
  , 'Vals' : randn(4)
}).groupby(['A', 'B']).sum()

df

Output&gt;            Vals
Output&gt; A  B           
Output&gt; a1 b1 -1.632460
Output&gt;    b2  0.596027
Output&gt; a2 b3 -0.619130
Output&gt; a3 b4 -0.002009
</code></pre>

<p>How do I prepend a level to the MultiIndex so that I turn it into something like:</p>

<pre><code>Output&gt;                       Vals
Output&gt; FirstLevel A  B           
Output&gt; Foo        a1 b1 -1.632460
Output&gt;               b2  0.596027
Output&gt;            a2 b3 -0.619130
Output&gt;            a3 b4 -0.002009
</code></pre>
";16301.0;"[""import numpy as np\nimport pandas as p\nfrom numpy.random import randn\n\ndf = p.DataFrame({\n    'A' : ['a1', 'a1', 'a2', 'a3']\n  , 'B' : ['b1', 'b2', 'b3', 'b4']\n  , 'Vals' : randn(4)\n}).groupby(['A', 'B']).sum()\n\ndf\n\nOutput>            Vals\nOutput> A  B           \nOutput> a1 b1 -1.632460\nOutput>    b2  0.596027\nOutput> a2 b3 -0.619130\nOutput> a3 b4 -0.002009\n"", 'Output>                       Vals\nOutput> FirstLevel A  B           \nOutput> Foo        a1 b1 -1.632460\nOutput>               b2  0.596027\nOutput>            a2 b3 -0.619130\nOutput>            a3 b4 -0.002009\n']";"[""import numpy as np\nimport pandas as p\nfrom numpy.random import randn\n\ndf = p.DataFrame({\n    'A' : ['a1', 'a1', 'a2', 'a3']\n  , 'B' : ['b1', 'b2', 'b3', 'b4']\n  , 'Vals' : randn(4)\n}).groupby(['A', 'B']).sum()\n\ndf\n\nOutput>            Vals\nOutput> A  B           \nOutput> a1 b1 -1.632460\nOutput>    b2  0.596027\nOutput> a2 b3 -0.619130\nOutput> a3 b4 -0.002009\n"", 'Output>                       Vals\nOutput> FirstLevel A  B           \nOutput> Foo        a1 b1 -1.632460\nOutput>               b2  0.596027\nOutput>            a2 b3 -0.619130\nOutput>            a3 b4 -0.002009\n']"
133;4.0;1;14745022;;1;42;<python><dataframe><pandas>;Pandas DataFrame, how do i split a column into two;"<p>I have a data frame with one column and i'd like to split it into two columns, with one column header as '<code>fips'</code> and the other <code>'row'</code></p>

<p>My datFrame df looks like this currently</p>

<pre><code>          row
0    00000 UNITED STATES
1    01000 ALABAMA
2    01001 Autauga County, AL
3    01003 Baldwin County, AL
4    01005 Barbour County, AL
</code></pre>

<p>I am new to python and do not understand how using <code>df.row.str[:]</code> would help me achieve my goal of splitting the row cell.  I can use <code>df['fips'] = hello</code> to add a new column and populate it with hello. Any ideas? </p>

<pre><code>         fips       row
0    00000 UNITED STATES
1    01000 ALABAMA 
2    01001 Autauga County, AL
3    01003 Baldwin County, AL
4    01005 Barbour County, AL
</code></pre>
";56622.0;['          row\n0    00000 UNITED STATES\n1    01000 ALABAMA\n2    01001 Autauga County, AL\n3    01003 Baldwin County, AL\n4    01005 Barbour County, AL\n', '         fips       row\n0    00000 UNITED STATES\n1    01000 ALABAMA \n2    01001 Autauga County, AL\n3    01003 Baldwin County, AL\n4    01005 Barbour County, AL\n'];"[""fips'"", ""'row'"", '          row\n0    00000 UNITED STATES\n1    01000 ALABAMA\n2    01001 Autauga County, AL\n3    01003 Baldwin County, AL\n4    01005 Barbour County, AL\n', 'df.row.str[:]', ""df['fips'] = hello"", '         fips       row\n0    00000 UNITED STATES\n1    01000 ALABAMA \n2    01001 Autauga County, AL\n3    01003 Baldwin County, AL\n4    01005 Barbour County, AL\n']"
134;2.0;1;14808945;;1;34;<python><pandas>;check if variable is dataframe;"<p>when my function f is called with a variable I want to check if var is a pandas dataframe:</p>

<pre><code>def f(var):
if var == pd.DataFrame():
    print ""do stuff""
</code></pre>

<p>I guess the solution might be quite simple but even with </p>

<pre><code>def f(var):
if var.values != None:
    print ""do stuff""
</code></pre>

<p>I can't get it to work like expected. </p>
";21988.0;"['def f(var):\nif var == pd.DataFrame():\n    print ""do stuff""\n', 'def f(var):\nif var.values != None:\n    print ""do stuff""\n']";"['def f(var):\nif var == pd.DataFrame():\n    print ""do stuff""\n', 'def f(var):\nif var.values != None:\n    print ""do stuff""\n']"
135;6.0;1;14940743;;1;84;<python><pandas>;Selecting/Excluding sets of columns in Pandas;"<p>I would like to create views or dataframes from an existing dataframe based on column selections.</p>

<p>For example, I would like to create a dataframe df2 from a dataframe df1 that holds all columns from it except two of them. I tried doing the following, but it didn't work:</p>

<pre><code>import numpy as np
import pandas as pd

# Create a dataframe with columns A,B,C and D
df = pd.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))

# Try to create a second dataframe df2 from df with all columns except 'B' and D
my_cols = set(df.columns)
my_cols.remove('B').remove('D')

# This returns an error (""unhashable type: set"")
df2 = df[my_cols]
</code></pre>

<p>What am I doing wrong? Perhaps more generally, what mechanisms does Panda have to support the picking and <strong>exclusions</strong> of arbitrary sets of columns from a dataframe?</p>
";77656.0;"['import numpy as np\nimport pandas as pd\n\n# Create a dataframe with columns A,B,C and D\ndf = pd.DataFrame(np.random.randn(100, 4), columns=list(\'ABCD\'))\n\n# Try to create a second dataframe df2 from df with all columns except \'B\' and D\nmy_cols = set(df.columns)\nmy_cols.remove(\'B\').remove(\'D\')\n\n# This returns an error (""unhashable type: set"")\ndf2 = df[my_cols]\n']";"['import numpy as np\nimport pandas as pd\n\n# Create a dataframe with columns A,B,C and D\ndf = pd.DataFrame(np.random.randn(100, 4), columns=list(\'ABCD\'))\n\n# Try to create a second dataframe df2 from df with all columns except \'B\' and D\nmy_cols = set(df.columns)\nmy_cols.remove(\'B\').remove(\'D\')\n\n# This returns an error (""unhashable type: set"")\ndf2 = df[my_cols]\n']"
136;3.0;0;14941366;;1;43;<python><sorting><group-by><dataframe><pandas>;Pandas sort by group aggregate and column;"<p>Given the following dataframe</p>

<pre><code>In [31]: rand = np.random.RandomState(1)
         df = pd.DataFrame({'A': ['foo', 'bar', 'baz'] * 2,
                            'B': rand.randn(6),
                            'C': rand.rand(6) &gt; .5})

In [32]: df
Out[32]:      A         B      C
         0  foo  1.624345  False
         1  bar -0.611756   True
         2  baz -0.528172  False
         3  foo -1.072969   True
         4  bar  0.865408  False
         5  baz -2.301539   True 
</code></pre>

<p>I would like to sort it in groups (<code>A</code>) by the aggregated sum of <code>B</code>, and then by the value in <code>C</code> (not aggregated). So basically get the order of the <code>A</code> groups with</p>

<pre><code>In [28]: df.groupby('A').sum().sort('B')
Out[28]:             B  C
         A               
         baz -2.829710  1
         bar  0.253651  1
         foo  0.551377  1
</code></pre>

<p>And then by True/False, so that it ultimately looks like this:</p>

<pre><code>In [30]: df.ix[[5, 2, 1, 4, 3, 0]]
Out[30]: A         B      C
    5  baz -2.301539   True
    2  baz -0.528172  False
    1  bar -0.611756   True
    4  bar  0.865408  False
    3  foo -1.072969   True
    0  foo  1.624345  False
</code></pre>

<p>How can this be done?                         </p>
";70881.0;"[""In [31]: rand = np.random.RandomState(1)\n         df = pd.DataFrame({'A': ['foo', 'bar', 'baz'] * 2,\n                            'B': rand.randn(6),\n                            'C': rand.rand(6) > .5})\n\nIn [32]: df\nOut[32]:      A         B      C\n         0  foo  1.624345  False\n         1  bar -0.611756   True\n         2  baz -0.528172  False\n         3  foo -1.072969   True\n         4  bar  0.865408  False\n         5  baz -2.301539   True \n"", ""In [28]: df.groupby('A').sum().sort('B')\nOut[28]:             B  C\n         A               \n         baz -2.829710  1\n         bar  0.253651  1\n         foo  0.551377  1\n"", 'In [30]: df.ix[[5, 2, 1, 4, 3, 0]]\nOut[30]: A         B      C\n    5  baz -2.301539   True\n    2  baz -0.528172  False\n    1  bar -0.611756   True\n    4  bar  0.865408  False\n    3  foo -1.072969   True\n    0  foo  1.624345  False\n']";"[""In [31]: rand = np.random.RandomState(1)\n         df = pd.DataFrame({'A': ['foo', 'bar', 'baz'] * 2,\n                            'B': rand.randn(6),\n                            'C': rand.rand(6) > .5})\n\nIn [32]: df\nOut[32]:      A         B      C\n         0  foo  1.624345  False\n         1  bar -0.611756   True\n         2  baz -0.528172  False\n         3  foo -1.072969   True\n         4  bar  0.865408  False\n         5  baz -2.301539   True \n"", 'A', 'B', 'C', 'A', ""In [28]: df.groupby('A').sum().sort('B')\nOut[28]:             B  C\n         A               \n         baz -2.829710  1\n         bar  0.253651  1\n         foo  0.551377  1\n"", 'In [30]: df.ix[[5, 2, 1, 4, 3, 0]]\nOut[30]: A         B      C\n    5  baz -2.301539   True\n    2  baz -0.528172  False\n    1  bar -0.611756   True\n    4  bar  0.865408  False\n    3  foo -1.072969   True\n    0  foo  1.624345  False\n']"
137;2.0;0;14964493;;1;21;<pandas>;MultiIndex-based indexing in pandas;"<p>If I define a hierarchically-indexed dataframe like this:</p>

<pre><code>import itertools
import pandas as pd
import numpy as np
a = ('A', 'B')
i = (0, 1, 2)
b = (True, False)
idx = pd.MultiIndex.from_tuples(list(itertools.product(a, i, b)),
                                names=('Alpha', 'Int', 'Bool'))
df = pd.DataFrame(np.random.randn(len(idx), 7), index=idx,
                  columns=('I', 'II', 'III', 'IV', 'V', 'VI', 'VII'))
</code></pre>

<p>the contents look like this:</p>

<pre><code>In [19]: df
Out[19]: 
                        I        II       III        IV         V        VI       VII
Alpha Int Bool                                                                       
A     0   True  -0.462924  1.210442  0.306737  0.325116 -1.320084 -0.831699  0.892865
          False -0.850570 -0.949779  0.022074 -0.205575 -0.684794 -0.214307 -1.133833
      1   True   0.603602  1.387020 -0.830780 -1.242000 -0.321938  0.484271  0.171738
          False -1.591730  1.282136  0.095159 -1.239882  0.760880 -0.606444 -0.485957
      2   True  -1.346883  1.650247 -1.476443  2.092067  1.344689  0.177083  0.100844
          False  0.001407 -1.127299 -0.417828  0.143595 -0.277838 -0.478262 -0.350906
B     0   True   0.722781 -1.093182  0.237536  0.457614 -2.500885  0.338257  0.009128
          False  0.321022  0.419357  1.161140 -1.371035  1.093696  0.250517 -1.125612
      1   True   0.237441  1.739933  0.029653  0.327823 -0.384647  1.523628 -0.009053
          False -0.459148 -0.598577 -0.593486 -0.607447  1.478399  0.504028 -0.329555
      2   True  -0.583052 -0.986493 -0.057788 -0.639798  1.400311  0.076471 -0.212513
          False  0.896755  2.583520  1.520151  2.367336 -1.084994 -1.233548 -2.414215
</code></pre>

<p>I know how to extract the data corresponding to a given column.  E.g. for column <code>'VII'</code>:</p>

<pre><code>In [20]: df['VII']
Out[20]: 
Alpha  Int  Bool 
A      0    True     0.892865
            False   -1.133833
       1    True     0.171738
            False   -0.485957
       2    True     0.100844
            False   -0.350906
B      0    True     0.009128
            False   -1.125612
       1    True    -0.009053
            False   -0.329555
       2    True    -0.212513
            False   -2.414215
Name: VII
</code></pre>

<p>How do I extract the data matching the following sets of criteria:</p>

<ol>
<li><code>Alpha=='B'</code></li>
<li><code>Alpha=='B'</code>, <code>Bool==False</code></li>
<li><code>Alpha=='B'</code>, <code>Bool==False</code>, column <code>'I'</code></li>
<li><code>Alpha=='B'</code>, <code>Bool==False</code>, columns <code>'I'</code> and <code>'III'</code></li>
<li><code>Alpha=='B'</code>, <code>Bool==False</code>, columns <code>'I'</code>, <code>'III'</code>, and all columns from <code>'V'</code> onwards</li>
<li><code>Int</code> is even</li>
</ol>

<p>(BTW, I did rtfm, more than once even, but I really find it incomprehensible.)</p>
";10792.0;"[""import itertools\nimport pandas as pd\nimport numpy as np\na = ('A', 'B')\ni = (0, 1, 2)\nb = (True, False)\nidx = pd.MultiIndex.from_tuples(list(itertools.product(a, i, b)),\n                                names=('Alpha', 'Int', 'Bool'))\ndf = pd.DataFrame(np.random.randn(len(idx), 7), index=idx,\n                  columns=('I', 'II', 'III', 'IV', 'V', 'VI', 'VII'))\n"", 'In [19]: df\nOut[19]: \n                        I        II       III        IV         V        VI       VII\nAlpha Int Bool                                                                       \nA     0   True  -0.462924  1.210442  0.306737  0.325116 -1.320084 -0.831699  0.892865\n          False -0.850570 -0.949779  0.022074 -0.205575 -0.684794 -0.214307 -1.133833\n      1   True   0.603602  1.387020 -0.830780 -1.242000 -0.321938  0.484271  0.171738\n          False -1.591730  1.282136  0.095159 -1.239882  0.760880 -0.606444 -0.485957\n      2   True  -1.346883  1.650247 -1.476443  2.092067  1.344689  0.177083  0.100844\n          False  0.001407 -1.127299 -0.417828  0.143595 -0.277838 -0.478262 -0.350906\nB     0   True   0.722781 -1.093182  0.237536  0.457614 -2.500885  0.338257  0.009128\n          False  0.321022  0.419357  1.161140 -1.371035  1.093696  0.250517 -1.125612\n      1   True   0.237441  1.739933  0.029653  0.327823 -0.384647  1.523628 -0.009053\n          False -0.459148 -0.598577 -0.593486 -0.607447  1.478399  0.504028 -0.329555\n      2   True  -0.583052 -0.986493 -0.057788 -0.639798  1.400311  0.076471 -0.212513\n          False  0.896755  2.583520  1.520151  2.367336 -1.084994 -1.233548 -2.414215\n', ""In [20]: df['VII']\nOut[20]: \nAlpha  Int  Bool \nA      0    True     0.892865\n            False   -1.133833\n       1    True     0.171738\n            False   -0.485957\n       2    True     0.100844\n            False   -0.350906\nB      0    True     0.009128\n            False   -1.125612\n       1    True    -0.009053\n            False   -0.329555\n       2    True    -0.212513\n            False   -2.414215\nName: VII\n""]";"[""import itertools\nimport pandas as pd\nimport numpy as np\na = ('A', 'B')\ni = (0, 1, 2)\nb = (True, False)\nidx = pd.MultiIndex.from_tuples(list(itertools.product(a, i, b)),\n                                names=('Alpha', 'Int', 'Bool'))\ndf = pd.DataFrame(np.random.randn(len(idx), 7), index=idx,\n                  columns=('I', 'II', 'III', 'IV', 'V', 'VI', 'VII'))\n"", 'In [19]: df\nOut[19]: \n                        I        II       III        IV         V        VI       VII\nAlpha Int Bool                                                                       \nA     0   True  -0.462924  1.210442  0.306737  0.325116 -1.320084 -0.831699  0.892865\n          False -0.850570 -0.949779  0.022074 -0.205575 -0.684794 -0.214307 -1.133833\n      1   True   0.603602  1.387020 -0.830780 -1.242000 -0.321938  0.484271  0.171738\n          False -1.591730  1.282136  0.095159 -1.239882  0.760880 -0.606444 -0.485957\n      2   True  -1.346883  1.650247 -1.476443  2.092067  1.344689  0.177083  0.100844\n          False  0.001407 -1.127299 -0.417828  0.143595 -0.277838 -0.478262 -0.350906\nB     0   True   0.722781 -1.093182  0.237536  0.457614 -2.500885  0.338257  0.009128\n          False  0.321022  0.419357  1.161140 -1.371035  1.093696  0.250517 -1.125612\n      1   True   0.237441  1.739933  0.029653  0.327823 -0.384647  1.523628 -0.009053\n          False -0.459148 -0.598577 -0.593486 -0.607447  1.478399  0.504028 -0.329555\n      2   True  -0.583052 -0.986493 -0.057788 -0.639798  1.400311  0.076471 -0.212513\n          False  0.896755  2.583520  1.520151  2.367336 -1.084994 -1.233548 -2.414215\n', ""'VII'"", ""In [20]: df['VII']\nOut[20]: \nAlpha  Int  Bool \nA      0    True     0.892865\n            False   -1.133833\n       1    True     0.171738\n            False   -0.485957\n       2    True     0.100844\n            False   -0.350906\nB      0    True     0.009128\n            False   -1.125612\n       1    True    -0.009053\n            False   -0.329555\n       2    True    -0.212513\n            False   -2.414215\nName: VII\n"", ""Alpha=='B'"", ""Alpha=='B'"", 'Bool==False', ""Alpha=='B'"", 'Bool==False', ""'I'"", ""Alpha=='B'"", 'Bool==False', ""'I'"", ""'III'"", ""Alpha=='B'"", 'Bool==False', ""'I'"", ""'III'"", ""'V'"", 'Int']"
138;4.0;3;14984119;;1;24;<python><pandas>;python pandas remove duplicate columns;"<p>What is the easiest way to remove duplicate columns from a dataframe?</p>

<p>I am reading a text file that has duplicate columns via:</p>

<pre><code>import pandas as pd

df=pd.read_table(fname)
</code></pre>

<p>The column names are:</p>

<pre><code>Time, Time Relative, N2, Time, Time Relative, H2, etc...
</code></pre>

<p>All the Time and Time Relative columns contain the same data. I want:</p>

<pre><code>Time, Time Relative, N2, H2
</code></pre>

<p>All my attempts at dropping, deleting, etc  such as:</p>

<pre><code>df=df.T.drop_duplicates().T
</code></pre>

<p>Result in uniquely valued index errors:</p>

<pre><code>Reindexing only valid with uniquely valued index objects
</code></pre>

<p>Sorry for being a Pandas noob. Any Suggestions would be appreciated.</p>

<hr>

<p><strong>Additional Details</strong></p>

<p>Pandas version: 0.9.0<br>
Python Version: 2.7.3<br>
Windows 7<br>
(installed via Pythonxy 2.7.3.0)</p>

<p>data file (note: in the real file, columns are separated by tabs, here they are separated by 4 spaces):</p>

<pre><code>Time    Time Relative [s]    N2[%]    Time    Time Relative [s]    H2[ppm]
2/12/2013 9:20:55 AM    6.177    9.99268e+001    2/12/2013 9:20:55 AM    6.177    3.216293e-005    
2/12/2013 9:21:06 AM    17.689    9.99296e+001    2/12/2013 9:21:06 AM    17.689    3.841667e-005    
2/12/2013 9:21:18 AM    29.186    9.992954e+001    2/12/2013 9:21:18 AM    29.186    3.880365e-005    
... etc ...
2/12/2013 2:12:44 PM    17515.269    9.991756+001    2/12/2013 2:12:44 PM    17515.269    2.800279e-005    
2/12/2013 2:12:55 PM    17526.769    9.991754e+001    2/12/2013 2:12:55 PM    17526.769    2.880386e-005
2/12/2013 2:13:07 PM    17538.273    9.991797e+001    2/12/2013 2:13:07 PM    17538.273    3.131447e-005
</code></pre>
";23542.0;['import pandas as pd\n\ndf=pd.read_table(fname)\n', 'Time, Time Relative, N2, Time, Time Relative, H2, etc...\n', 'Time, Time Relative, N2, H2\n', 'df=df.T.drop_duplicates().T\n', 'Reindexing only valid with uniquely valued index objects\n', 'Time    Time Relative [s]    N2[%]    Time    Time Relative [s]    H2[ppm]\n2/12/2013 9:20:55 AM    6.177    9.99268e+001    2/12/2013 9:20:55 AM    6.177    3.216293e-005    \n2/12/2013 9:21:06 AM    17.689    9.99296e+001    2/12/2013 9:21:06 AM    17.689    3.841667e-005    \n2/12/2013 9:21:18 AM    29.186    9.992954e+001    2/12/2013 9:21:18 AM    29.186    3.880365e-005    \n... etc ...\n2/12/2013 2:12:44 PM    17515.269    9.991756+001    2/12/2013 2:12:44 PM    17515.269    2.800279e-005    \n2/12/2013 2:12:55 PM    17526.769    9.991754e+001    2/12/2013 2:12:55 PM    17526.769    2.880386e-005\n2/12/2013 2:13:07 PM    17538.273    9.991797e+001    2/12/2013 2:13:07 PM    17538.273    3.131447e-005\n'];['import pandas as pd\n\ndf=pd.read_table(fname)\n', 'Time, Time Relative, N2, Time, Time Relative, H2, etc...\n', 'Time, Time Relative, N2, H2\n', 'df=df.T.drop_duplicates().T\n', 'Reindexing only valid with uniquely valued index objects\n', 'Time    Time Relative [s]    N2[%]    Time    Time Relative [s]    H2[ppm]\n2/12/2013 9:20:55 AM    6.177    9.99268e+001    2/12/2013 9:20:55 AM    6.177    3.216293e-005    \n2/12/2013 9:21:06 AM    17.689    9.99296e+001    2/12/2013 9:21:06 AM    17.689    3.841667e-005    \n2/12/2013 9:21:18 AM    29.186    9.992954e+001    2/12/2013 9:21:18 AM    29.186    3.880365e-005    \n... etc ...\n2/12/2013 2:12:44 PM    17515.269    9.991756+001    2/12/2013 2:12:44 PM    17515.269    2.800279e-005    \n2/12/2013 2:12:55 PM    17526.769    9.991754e+001    2/12/2013 2:12:55 PM    17526.769    2.880386e-005\n2/12/2013 2:13:07 PM    17538.273    9.991797e+001    2/12/2013 2:13:07 PM    17538.273    3.131447e-005\n']
139;2.0;2;14988480;;1;26;<python><r><dataframe><pandas>;Pandas version of rbind;"<p>In R, you can combine two dataframes by sticking the columns of one onto the bottom of the columns of the other using rbind. In pandas, how do you accomplish the same thing? It seems bizarrely difficult. </p>

<p>Using append results in a horrible mess including NaNs and things for reasons I don't understand. I'm just trying to ""rbind"" two identical frames that look like this:</p>

<p>EDIT: I was creating the DataFrames in a stupid way, which was causing issues. Append=rbind to all intents and purposes. See answer below.</p>

<pre><code>        0         1       2        3          4          5        6                    7
0   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42
1   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42
2   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43
3  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43
4   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44
5  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44
6   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45
</code></pre>

<p>But I'm getting something horrible a la this: </p>

<pre><code>        0         1        2        3          4         5        6                    7       0         1       2        3          4          5        6                    7
0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42
1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42
2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43
3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43
4     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44
5     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44
6     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45
0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42
1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42
2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43
3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  
</code></pre>

<p>And I don't understand why. I'm starting to miss R :(</p>
";17614.0;['        0         1       2        3          4          5        6                    7\n0   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43\n4   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44\n5  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44\n6   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45\n', '        0         1        2        3          4         5        6                    7       0         1       2        3          4          5        6                    7\n0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43\n4     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44\n5     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44\n6     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45\n0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  \n'];['        0         1       2        3          4          5        6                    7\n0   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43\n4   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44\n5  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44\n6   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45\n', '        0         1        2        3          4         5        6                    7       0         1       2        3          4          5        6                    7\n0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43\n4     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44\n5     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44\n6     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45\n0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  \n']
140;1.0;0;14991195;;1;23;<python><pandas>;How to remove rows with null values from kth column onward in python;"<p>I need to remove all rows in which elements from column 3 onwards are all NaN</p>

<pre><code>df = DataFrame(np.random.randn(6, 5), index=['a', 'c', 'e', 'f', 'g','h'], columns=['one', 'two', 'three', 'four', 'five'])

df2 = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])
df2.ix[1][0] = 111
df2.ix[1][1] = 222
</code></pre>

<p>In the example above, my final data frame would not be having rows 'b' and 'c'. </p>

<p>How to use <code>df.dropna()</code> in this case?</p>
";33921.0;"[""df = DataFrame(np.random.randn(6, 5), index=['a', 'c', 'e', 'f', 'g','h'], columns=['one', 'two', 'three', 'four', 'five'])\n\ndf2 = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\ndf2.ix[1][0] = 111\ndf2.ix[1][1] = 222\n""]";"[""df = DataFrame(np.random.randn(6, 5), index=['a', 'c', 'e', 'f', 'g','h'], columns=['one', 'two', 'three', 'four', 'five'])\n\ndf2 = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\ndf2.ix[1][0] = 111\ndf2.ix[1][1] = 222\n"", 'df.dropna()']"
141;7.0;4;15006298;;1;23;<python><pandas><dataframe><ipython><ipython-notebook>;How to preview a part of a large pandas DataFrame?;"<p>I am just getting started with pandas in the IPython Notebook and encountering the following problem: When a <code>DataFrame</code> read from a CSV file is small, the IPython Notebook displays it in a nice table view. When the <code>DataFrame</code> is large, something like this is ouput:</p>

<pre><code>In [27]:

evaluation = readCSV(""evaluation_MO_without_VNS_quality.csv"").filter([""solver"", ""instance"", ""runtime"", ""objective""])

In [37]:

evaluation

Out[37]:

&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 333 entries, 0 to 332
Data columns:
solver       333  non-null values
instance     333  non-null values
runtime      333  non-null values
objective    333  non-null values
dtypes: int64(1), object(3)
</code></pre>

<p>I would like to see a small portion of the data frame as a table just to make sure it is in the right format. What options do I have?</p>
";40523.0;"['In [27]:\n\nevaluation = readCSV(""evaluation_MO_without_VNS_quality.csv"").filter([""solver"", ""instance"", ""runtime"", ""objective""])\n\nIn [37]:\n\nevaluation\n\nOut[37]:\n\n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 333 entries, 0 to 332\nData columns:\nsolver       333  non-null values\ninstance     333  non-null values\nruntime      333  non-null values\nobjective    333  non-null values\ndtypes: int64(1), object(3)\n']";"['DataFrame', 'DataFrame', 'In [27]:\n\nevaluation = readCSV(""evaluation_MO_without_VNS_quality.csv"").filter([""solver"", ""instance"", ""runtime"", ""objective""])\n\nIn [37]:\n\nevaluation\n\nOut[37]:\n\n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 333 entries, 0 to 332\nData columns:\nsolver       333  non-null values\ninstance     333  non-null values\nruntime      333  non-null values\nobjective    333  non-null values\ndtypes: int64(1), object(3)\n']"
142;2.0;1;15008970;;1;24;<csv><dataframe><pandas>;Way to read first few lines for pandas dataframe;"<p>Is there a built-in way to use <code>read_csv</code> to read only the first <code>n</code> lines of a file without knowing the length of the lines ahead of time? I have a large file that takes a long time to read, and occasionally only want to use the first, say, 20 lines to get a sample of it (and prefer not to load the full thing and take the head of it).</p>

<p>If I knew the total number of lines I could do something like <code>footer_lines = total_lines - n</code> and pass this to the <code>skipfooter</code> keyword arg. My current solution is to manually grab the first <code>n</code> lines with python and StringIO it to pandas:</p>

<pre><code>import pandas as pd
from StringIO import StringIO

n = 20
with open('big_file.csv', 'r') as f:
    head = ''.join(f.readlines(n))

df = pd.read_csv(StringIO(head))
</code></pre>

<p>It's not that bad, but is there a more concise, 'pandasic' (?) way to do it with keywords or something?</p>
";24174.0;"[""import pandas as pd\nfrom StringIO import StringIO\n\nn = 20\nwith open('big_file.csv', 'r') as f:\n    head = ''.join(f.readlines(n))\n\ndf = pd.read_csv(StringIO(head))\n""]";"['read_csv', 'n', 'footer_lines = total_lines - n', 'skipfooter', 'n', ""import pandas as pd\nfrom StringIO import StringIO\n\nn = 20\nwith open('big_file.csv', 'r') as f:\n    head = ''.join(f.readlines(n))\n\ndf = pd.read_csv(StringIO(head))\n""]"
143;4.0;4;15017072;;1;37;<python><pandas>;pandas read_csv and filter columns with usecols;"<p>I have a csv file which isn't coming in correctly with <code>pandas.read_csv</code> when I  filter the columns with <code>usecols</code> and use multiple indexes.<br>
</p>

<pre><code>import pandas as pd
csv = r""""""dummy,date,loc,x
bar,20090101,a,1
bar,20090102,a,3
bar,20090103,a,5
bar,20090101,b,1
bar,20090102,b,3
bar,20090103,b,5""""""
f = open('foo.csv', 'w')
f.write(csv)
f.close()

df1 = pd.read_csv('foo.csv', 
        index_col=[""date"", ""loc""], 
        usecols=[""dummy"", ""date"", ""loc"", ""x""],
        parse_dates=[""date""],
        header=0,
        names=[""dummy"", ""date"", ""loc"", ""x""])
print df1

# Ignore the dummy columns
df2 = pd.read_csv('foo.csv', 
        index_col=[""date"", ""loc""], 
        usecols=[""date"", ""loc"", ""x""], # &lt;----------- Changed
        parse_dates=[""date""],
        header=0,
        names=[""dummy"", ""date"", ""loc"", ""x""])
print df2
</code></pre>

<p>I expect that df1 and df2 should be the same except for the missing dummy column, but the columns come in mislabeled.  Also the date is getting parsed as a date.  </p>

<pre><code>In [118]: %run test.py
               dummy  x
date       loc
2009-01-01 a     bar  1
2009-01-02 a     bar  3
2009-01-03 a     bar  5
2009-01-01 b     bar  1
2009-01-02 b     bar  3
2009-01-03 b     bar  5
              date
date loc
a    1    20090101
     3    20090102
     5    20090103
b    1    20090101
     3    20090102
     5    20090103
</code></pre>

<p>Using column numbers instead of names give me the same problem.  I can workaround the issue by dropping the dummy column after the read_csv step, but I'm trying to understand what is going wrong.  I'm using pandas 0.10.1.</p>

<p>edit: fixed bad header usage.</p>
";74330.0;"['import pandas as pd\ncsv = r""""""dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nbar,20090103,b,5""""""\nf = open(\'foo.csv\', \'w\')\nf.write(csv)\nf.close()\n\ndf1 = pd.read_csv(\'foo.csv\', \n        index_col=[""date"", ""loc""], \n        usecols=[""dummy"", ""date"", ""loc"", ""x""],\n        parse_dates=[""date""],\n        header=0,\n        names=[""dummy"", ""date"", ""loc"", ""x""])\nprint df1\n\n# Ignore the dummy columns\ndf2 = pd.read_csv(\'foo.csv\', \n        index_col=[""date"", ""loc""], \n        usecols=[""date"", ""loc"", ""x""], # <----------- Changed\n        parse_dates=[""date""],\n        header=0,\n        names=[""dummy"", ""date"", ""loc"", ""x""])\nprint df2\n', 'In [118]: %run test.py\n               dummy  x\ndate       loc\n2009-01-01 a     bar  1\n2009-01-02 a     bar  3\n2009-01-03 a     bar  5\n2009-01-01 b     bar  1\n2009-01-02 b     bar  3\n2009-01-03 b     bar  5\n              date\ndate loc\na    1    20090101\n     3    20090102\n     5    20090103\nb    1    20090101\n     3    20090102\n     5    20090103\n']";"['pandas.read_csv', 'usecols', 'import pandas as pd\ncsv = r""""""dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nbar,20090103,b,5""""""\nf = open(\'foo.csv\', \'w\')\nf.write(csv)\nf.close()\n\ndf1 = pd.read_csv(\'foo.csv\', \n        index_col=[""date"", ""loc""], \n        usecols=[""dummy"", ""date"", ""loc"", ""x""],\n        parse_dates=[""date""],\n        header=0,\n        names=[""dummy"", ""date"", ""loc"", ""x""])\nprint df1\n\n# Ignore the dummy columns\ndf2 = pd.read_csv(\'foo.csv\', \n        index_col=[""date"", ""loc""], \n        usecols=[""date"", ""loc"", ""x""], # <----------- Changed\n        parse_dates=[""date""],\n        header=0,\n        names=[""dummy"", ""date"", ""loc"", ""x""])\nprint df2\n', 'In [118]: %run test.py\n               dummy  x\ndate       loc\n2009-01-01 a     bar  1\n2009-01-02 a     bar  3\n2009-01-03 a     bar  5\n2009-01-01 b     bar  1\n2009-01-02 b     bar  3\n2009-01-03 b     bar  5\n              date\ndate loc\na    1    20090101\n     3    20090102\n     5    20090103\nb    1    20090101\n     3    20090102\n     5    20090103\n']"
144;2.0;0;15026698;;1;30;<python><csv><pandas><dataframe><whitespace>;How to make separator in read_csv more flexible wrt whitespace?;"<p>I need to created a data frame using data stored in a file. For that I want to use <code>read_csv</code> method. However, the separator is not very regular. Some columns are separated by tabs (<code>\t</code>), other are separated by spaces. Moreover, some columns can be separated by 2 or 3 or more spaces or even by a combination of spaces and tabs (for example 3 spaces, two tabs and then 1 space).</p>

<p>Is there a way to tell pandas to treat these files properly?</p>

<p>By the way, I do not have this problem if I use Python. I use:</p>

<pre><code>for line in file(file_name):
   fld = line.split()
</code></pre>

<p>And it works perfect. It does not care if there are 2 or 3 spaces between the fields. Even combinations of spaces and tabs do not cause any problem. Can pandas do the same?</p>
";25215.0;['for line in file(file_name):\n   fld = line.split()\n'];['read_csv', '\\t', 'for line in file(file_name):\n   fld = line.split()\n']
145;4.0;0;15118111;;1;24;<python><pandas>;Apply function to each row of pandas dataframe to create two new columns;"<p>I have a pandas DataFrame, <code>st</code> containing multiple columns:</p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 53732 entries, 1993-01-07 12:23:58 to 2012-12-02 20:06:23
Data columns:
Date(dd-mm-yy)_Time(hh-mm-ss)       53732  non-null values
Julian_Day                          53732  non-null values
AOT_1020                            53716  non-null values
AOT_870                             53732  non-null values
AOT_675                             53188  non-null values
AOT_500                             51687  non-null values
AOT_440                             53727  non-null values
AOT_380                             51864  non-null values
AOT_340                             52852  non-null values
Water(cm)                           51687  non-null values
%TripletVar_1020                    53710  non-null values
%TripletVar_870                     53726  non-null values
%TripletVar_675                     53182  non-null values
%TripletVar_500                     51683  non-null values
%TripletVar_440                     53721  non-null values
%TripletVar_380                     51860  non-null values
%TripletVar_340                     52846  non-null values
440-870Angstrom                     53732  non-null values
380-500Angstrom                     52253  non-null values
440-675Angstrom                     53732  non-null values
500-870Angstrom                     53732  non-null values
340-440Angstrom                     53277  non-null values
Last_Processing_Date(dd/mm/yyyy)    53732  non-null values
Solar_Zenith_Angle                  53732  non-null values
dtypes: datetime64[ns](1), float64(22), object(1)
</code></pre>

<p>I want to create two new columns for this dataframe based on applying a function to each row of the dataframe. I don't want to have to call the function multiple times (eg. by doing two separate <code>apply</code> calls) as it is rather computationally intensive. I have tried doing this in two ways, and neither of them work:</p>

<hr>

<p><strong>Using <code>apply</code>:</strong></p>

<p>I have written a function which takes a <code>Series</code> and returns a tuple of the values I want:</p>

<pre><code>def calculate(s):
    a = s['path'] + 2*s['row'] # Simple calc for example
    b = s['path'] * 0.153
    return (a, b)
</code></pre>

<p>Trying to apply this to the DataFrame gives an error:</p>

<pre><code>st.apply(calculate, axis=1)
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
&lt;ipython-input-248-acb7a44054a7&gt; in &lt;module&gt;()
----&gt; 1 st.apply(calculate, axis=1)

C:\Python27\lib\site-packages\pandas\core\frame.pyc in apply(self, func, axis, broadcast, raw, args, **kwds)
   4191                     return self._apply_raw(f, axis)
   4192                 else:
-&gt; 4193                     return self._apply_standard(f, axis)
   4194             else:
   4195                 return self._apply_broadcast(f, axis)

C:\Python27\lib\site-packages\pandas\core\frame.pyc in _apply_standard(self, func, axis, ignore_failures)
   4274                 index = None
   4275 
-&gt; 4276             result = self._constructor(data=results, index=index)
   4277             result.rename(columns=dict(zip(range(len(res_index)), res_index)),
   4278                           inplace=True)

C:\Python27\lib\site-packages\pandas\core\frame.pyc in __init__(self, data, index, columns, dtype, copy)
    390             mgr = self._init_mgr(data, index, columns, dtype=dtype, copy=copy)
    391         elif isinstance(data, dict):
--&gt; 392             mgr = self._init_dict(data, index, columns, dtype=dtype)
    393         elif isinstance(data, ma.MaskedArray):
    394             mask = ma.getmaskarray(data)

C:\Python27\lib\site-packages\pandas\core\frame.pyc in _init_dict(self, data, index, columns, dtype)
    521 
    522         return _arrays_to_mgr(arrays, data_names, index, columns,
--&gt; 523                               dtype=dtype)
    524 
    525     def _init_ndarray(self, values, index, columns, dtype=None,

C:\Python27\lib\site-packages\pandas\core\frame.pyc in _arrays_to_mgr(arrays, arr_names, index, columns, dtype)
   5411 
   5412     # consolidate for now
-&gt; 5413     mgr = BlockManager(blocks, axes)
   5414     return mgr.consolidate()
   5415 

C:\Python27\lib\site-packages\pandas\core\internals.pyc in __init__(self, blocks, axes, do_integrity_check)
    802 
    803         if do_integrity_check:
--&gt; 804             self._verify_integrity()
    805 
    806         self._consolidate_check()

C:\Python27\lib\site-packages\pandas\core\internals.pyc in _verify_integrity(self)
    892                                      ""items"")
    893             if block.values.shape[1:] != mgr_shape[1:]:
--&gt; 894                 raise AssertionError('Block shape incompatible with manager')
    895         tot_items = sum(len(x.items) for x in self.blocks)
    896         if len(self.items) != tot_items:

AssertionError: Block shape incompatible with manager
</code></pre>

<p>I was then going to assign the values returned from <code>apply</code> to two new columns using the method shown in <a href=""https://stackoverflow.com/questions/12356501/pandas-create-two-new-columns-in-a-dataframe-with-values-calculated-from-a-pre"">this question</a>. However, I can't even get to this point! This all works fine if I just return one value.</p>

<hr>

<p><strong>Using a loop:</strong></p>

<p>I first created two new columns of the dataframe and set them to <code>None</code>:</p>

<pre><code>st['a'] = None
st['b'] = None
</code></pre>

<p>Then looped over all of the indices and tried to modify these <code>None</code> values that I'd got in there, but the modifications I did didn't seem to work. That is, no error was generated, but the DataFrame didn't seem to be modified.</p>

<pre><code>for i in st.index:
    # do calc here
    st.ix[i]['a'] = a
    st.ix[i]['b'] = b
</code></pre>

<hr>

<p>I thought that both of these methods would work, but neither of them did. So, what am I doing wrong here? And what is the best, most 'pythonic' and 'pandaonic' way to do this?</p>
";61486.0;"[""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 53732 entries, 1993-01-07 12:23:58 to 2012-12-02 20:06:23\nData columns:\nDate(dd-mm-yy)_Time(hh-mm-ss)       53732  non-null values\nJulian_Day                          53732  non-null values\nAOT_1020                            53716  non-null values\nAOT_870                             53732  non-null values\nAOT_675                             53188  non-null values\nAOT_500                             51687  non-null values\nAOT_440                             53727  non-null values\nAOT_380                             51864  non-null values\nAOT_340                             52852  non-null values\nWater(cm)                           51687  non-null values\n%TripletVar_1020                    53710  non-null values\n%TripletVar_870                     53726  non-null values\n%TripletVar_675                     53182  non-null values\n%TripletVar_500                     51683  non-null values\n%TripletVar_440                     53721  non-null values\n%TripletVar_380                     51860  non-null values\n%TripletVar_340                     52846  non-null values\n440-870Angstrom                     53732  non-null values\n380-500Angstrom                     52253  non-null values\n440-675Angstrom                     53732  non-null values\n500-870Angstrom                     53732  non-null values\n340-440Angstrom                     53277  non-null values\nLast_Processing_Date(dd/mm/yyyy)    53732  non-null values\nSolar_Zenith_Angle                  53732  non-null values\ndtypes: datetime64[ns](1), float64(22), object(1)\n"", ""def calculate(s):\n    a = s['path'] + 2*s['row'] # Simple calc for example\n    b = s['path'] * 0.153\n    return (a, b)\n"", 'st.apply(calculate, axis=1)\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n<ipython-input-248-acb7a44054a7> in <module>()\n----> 1 st.apply(calculate, axis=1)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in apply(self, func, axis, broadcast, raw, args, **kwds)\n   4191                     return self._apply_raw(f, axis)\n   4192                 else:\n-> 4193                     return self._apply_standard(f, axis)\n   4194             else:\n   4195                 return self._apply_broadcast(f, axis)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _apply_standard(self, func, axis, ignore_failures)\n   4274                 index = None\n   4275 \n-> 4276             result = self._constructor(data=results, index=index)\n   4277             result.rename(columns=dict(zip(range(len(res_index)), res_index)),\n   4278                           inplace=True)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in __init__(self, data, index, columns, dtype, copy)\n    390             mgr = self._init_mgr(data, index, columns, dtype=dtype, copy=copy)\n    391         elif isinstance(data, dict):\n--> 392             mgr = self._init_dict(data, index, columns, dtype=dtype)\n    393         elif isinstance(data, ma.MaskedArray):\n    394             mask = ma.getmaskarray(data)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _init_dict(self, data, index, columns, dtype)\n    521 \n    522         return _arrays_to_mgr(arrays, data_names, index, columns,\n--> 523                               dtype=dtype)\n    524 \n    525     def _init_ndarray(self, values, index, columns, dtype=None,\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _arrays_to_mgr(arrays, arr_names, index, columns, dtype)\n   5411 \n   5412     # consolidate for now\n-> 5413     mgr = BlockManager(blocks, axes)\n   5414     return mgr.consolidate()\n   5415 \n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in __init__(self, blocks, axes, do_integrity_check)\n    802 \n    803         if do_integrity_check:\n--> 804             self._verify_integrity()\n    805 \n    806         self._consolidate_check()\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in _verify_integrity(self)\n    892                                      ""items"")\n    893             if block.values.shape[1:] != mgr_shape[1:]:\n--> 894                 raise AssertionError(\'Block shape incompatible with manager\')\n    895         tot_items = sum(len(x.items) for x in self.blocks)\n    896         if len(self.items) != tot_items:\n\nAssertionError: Block shape incompatible with manager\n', ""st['a'] = None\nst['b'] = None\n"", ""for i in st.index:\n    # do calc here\n    st.ix[i]['a'] = a\n    st.ix[i]['b'] = b\n""]";"['st', ""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 53732 entries, 1993-01-07 12:23:58 to 2012-12-02 20:06:23\nData columns:\nDate(dd-mm-yy)_Time(hh-mm-ss)       53732  non-null values\nJulian_Day                          53732  non-null values\nAOT_1020                            53716  non-null values\nAOT_870                             53732  non-null values\nAOT_675                             53188  non-null values\nAOT_500                             51687  non-null values\nAOT_440                             53727  non-null values\nAOT_380                             51864  non-null values\nAOT_340                             52852  non-null values\nWater(cm)                           51687  non-null values\n%TripletVar_1020                    53710  non-null values\n%TripletVar_870                     53726  non-null values\n%TripletVar_675                     53182  non-null values\n%TripletVar_500                     51683  non-null values\n%TripletVar_440                     53721  non-null values\n%TripletVar_380                     51860  non-null values\n%TripletVar_340                     52846  non-null values\n440-870Angstrom                     53732  non-null values\n380-500Angstrom                     52253  non-null values\n440-675Angstrom                     53732  non-null values\n500-870Angstrom                     53732  non-null values\n340-440Angstrom                     53277  non-null values\nLast_Processing_Date(dd/mm/yyyy)    53732  non-null values\nSolar_Zenith_Angle                  53732  non-null values\ndtypes: datetime64[ns](1), float64(22), object(1)\n"", 'apply', 'apply', 'Series', ""def calculate(s):\n    a = s['path'] + 2*s['row'] # Simple calc for example\n    b = s['path'] * 0.153\n    return (a, b)\n"", 'st.apply(calculate, axis=1)\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n<ipython-input-248-acb7a44054a7> in <module>()\n----> 1 st.apply(calculate, axis=1)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in apply(self, func, axis, broadcast, raw, args, **kwds)\n   4191                     return self._apply_raw(f, axis)\n   4192                 else:\n-> 4193                     return self._apply_standard(f, axis)\n   4194             else:\n   4195                 return self._apply_broadcast(f, axis)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _apply_standard(self, func, axis, ignore_failures)\n   4274                 index = None\n   4275 \n-> 4276             result = self._constructor(data=results, index=index)\n   4277             result.rename(columns=dict(zip(range(len(res_index)), res_index)),\n   4278                           inplace=True)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in __init__(self, data, index, columns, dtype, copy)\n    390             mgr = self._init_mgr(data, index, columns, dtype=dtype, copy=copy)\n    391         elif isinstance(data, dict):\n--> 392             mgr = self._init_dict(data, index, columns, dtype=dtype)\n    393         elif isinstance(data, ma.MaskedArray):\n    394             mask = ma.getmaskarray(data)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _init_dict(self, data, index, columns, dtype)\n    521 \n    522         return _arrays_to_mgr(arrays, data_names, index, columns,\n--> 523                               dtype=dtype)\n    524 \n    525     def _init_ndarray(self, values, index, columns, dtype=None,\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _arrays_to_mgr(arrays, arr_names, index, columns, dtype)\n   5411 \n   5412     # consolidate for now\n-> 5413     mgr = BlockManager(blocks, axes)\n   5414     return mgr.consolidate()\n   5415 \n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in __init__(self, blocks, axes, do_integrity_check)\n    802 \n    803         if do_integrity_check:\n--> 804             self._verify_integrity()\n    805 \n    806         self._consolidate_check()\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in _verify_integrity(self)\n    892                                      ""items"")\n    893             if block.values.shape[1:] != mgr_shape[1:]:\n--> 894                 raise AssertionError(\'Block shape incompatible with manager\')\n    895         tot_items = sum(len(x.items) for x in self.blocks)\n    896         if len(self.items) != tot_items:\n\nAssertionError: Block shape incompatible with manager\n', 'apply', 'None', ""st['a'] = None\nst['b'] = None\n"", 'None', ""for i in st.index:\n    # do calc here\n    st.ix[i]['a'] = a\n    st.ix[i]['b'] = b\n""]"
146;2.0;0;15203623;;1;30;<python><pandas>;Convert pandas DateTimeIndex to Unix Time?;"<p>What is the idiomatic way of converting a pandas DateTimeIndex to (an iterable of) Unix Time? 
This is probably not the way to go:</p>

<pre><code>[time.mktime(t.timetuple()) for t in my_data_frame.index.to_pydatetime()]
</code></pre>
";20077.0;['[time.mktime(t.timetuple()) for t in my_data_frame.index.to_pydatetime()]\n'];['[time.mktime(t.timetuple()) for t in my_data_frame.index.to_pydatetime()]\n']
147;2.0;2;15210962;;1;34;<numpy><pandas>;Specifying dtype float32 with pandas.read_csv on pandas 0.10.1;"<p>I'm attempting to read a simple space-separated file with pandas <code>read_csv</code> method.  However, pandas doesn't seem to be obeying my <code>dtype</code> argument.  Maybe I'm incorrectly specifying it?</p>

<p>I've distilled down my somewhat complicated call to <code>read_csv</code> to this simple test case.  I'm actually using the <code>converters</code> argument in my 'real' scenario but I removed this for simplicity.</p>

<p>Below is my ipython session:</p>

<pre><code>&gt;&gt;&gt; cat test.out
a b
0.76398 0.81394
0.32136 0.91063
&gt;&gt;&gt; import pandas
&gt;&gt;&gt; import numpy
&gt;&gt;&gt; x = pandas.read_csv('test.out', dtype={'a': numpy.float32}, delim_whitespace=True)
&gt;&gt;&gt; x
         a        b
0  0.76398  0.81394
1  0.32136  0.91063
&gt;&gt;&gt; x.a.dtype
dtype('float64')
</code></pre>

<p>I've also tried this using this with a <code>dtype</code> of <code>numpy.int32</code> or <code>numpy.int64</code>.  These choices result in an exception:</p>

<pre><code>AttributeError: 'NoneType' object has no attribute 'dtype'
</code></pre>

<p>I'm assuming the <code>AttributeError</code> is because pandas will not automatically try to convert/truncate the float values into an integer?</p>

<p>I'm running on a 32-bit machine with a 32-bit version of Python.</p>

<pre><code>&gt;&gt;&gt; !uname -a
Linux ubuntu 3.0.0-13-generic #22-Ubuntu SMP Wed Nov 2 13:25:36 UTC 2011 i686 i686 i386 GNU/Linux
&gt;&gt;&gt; import platform
&gt;&gt;&gt; platform.architecture()
('32bit', 'ELF')
&gt;&gt;&gt; pandas.__version__
'0.10.1'
</code></pre>
";52865.0;"["">>> cat test.out\na b\n0.76398 0.81394\n0.32136 0.91063\n>>> import pandas\n>>> import numpy\n>>> x = pandas.read_csv('test.out', dtype={'a': numpy.float32}, delim_whitespace=True)\n>>> x\n         a        b\n0  0.76398  0.81394\n1  0.32136  0.91063\n>>> x.a.dtype\ndtype('float64')\n"", ""AttributeError: 'NoneType' object has no attribute 'dtype'\n"", "">>> !uname -a\nLinux ubuntu 3.0.0-13-generic #22-Ubuntu SMP Wed Nov 2 13:25:36 UTC 2011 i686 i686 i386 GNU/Linux\n>>> import platform\n>>> platform.architecture()\n('32bit', 'ELF')\n>>> pandas.__version__\n'0.10.1'\n""]";"['read_csv', 'dtype', 'read_csv', 'converters', "">>> cat test.out\na b\n0.76398 0.81394\n0.32136 0.91063\n>>> import pandas\n>>> import numpy\n>>> x = pandas.read_csv('test.out', dtype={'a': numpy.float32}, delim_whitespace=True)\n>>> x\n         a        b\n0  0.76398  0.81394\n1  0.32136  0.91063\n>>> x.a.dtype\ndtype('float64')\n"", 'dtype', 'numpy.int32', 'numpy.int64', ""AttributeError: 'NoneType' object has no attribute 'dtype'\n"", 'AttributeError', "">>> !uname -a\nLinux ubuntu 3.0.0-13-generic #22-Ubuntu SMP Wed Nov 2 13:25:36 UTC 2011 i686 i686 i386 GNU/Linux\n>>> import platform\n>>> platform.architecture()\n('32bit', 'ELF')\n>>> pandas.__version__\n'0.10.1'\n""]"
148;3.0;0;15222754;;1;23;<python><pandas>;Group by pandas dataframe and select most common string factor;"<p>I have a data frame with three string columns. I know that the only one value in the 3rd column is valid for every combination of the first two. To clean the data I have to group by data frame by first two columns and select most common value of the third column for each combination.</p>

<p>My code:</p>

<pre><code>import pandas as pd
from scipy import stats

source = pd.DataFrame({'Country' : ['USA', 'USA', 'Russia','USA'], 
                  'City' : ['New-York', 'New-York', 'Sankt-Petersburg', 'New-York'],
                  'Short name' : ['NY','New','Spb','NY']})

print source.groupby(['Country','City']).agg(lambda x: stats.mode(x['Short name'])[0])
</code></pre>

<p>Last line of code doesn't work, it says ""Key error 'Short name'"" and if I try to group only by City, then I got an AssertionError. What can I do fix it?</p>
";15902.0;"[""import pandas as pd\nfrom scipy import stats\n\nsource = pd.DataFrame({'Country' : ['USA', 'USA', 'Russia','USA'], \n                  'City' : ['New-York', 'New-York', 'Sankt-Petersburg', 'New-York'],\n                  'Short name' : ['NY','New','Spb','NY']})\n\nprint source.groupby(['Country','City']).agg(lambda x: stats.mode(x['Short name'])[0])\n""]";"[""import pandas as pd\nfrom scipy import stats\n\nsource = pd.DataFrame({'Country' : ['USA', 'USA', 'Russia','USA'], \n                  'City' : ['New-York', 'New-York', 'Sankt-Petersburg', 'New-York'],\n                  'Short name' : ['NY','New','Spb','NY']})\n\nprint source.groupby(['Country','City']).agg(lambda x: stats.mode(x['Short name'])[0])\n""]"
149;3.0;0;15242746;;1;38;<python><pandas>;Handling Variable Number of Columns with Pandas - Python;"<p>I have a data set that looks like this (at most 5 columns - but can be less)</p>

<pre><code>1,2,3
1,2,3,4
1,2,3,4,5
1,2
1,2,3,4
....
</code></pre>

<p>I am trying to use pandas read_table to read this into a 5 column data frame. I would like to read this in without additional massaging. </p>

<p>If I try</p>

<pre><code>import pandas as pd
my_cols=['A','B','C','D','E']
my_df=pd.read_table(path,sep=',',header=None,names=my_cols)
</code></pre>

<p>I get an error - ""column names have 5 fields, data has 3 fields"". </p>

<p><strong>Is there any way to make pandas fill in NaN for the missing columns while reading the data?</strong></p>
";15050.0;"['1,2,3\n1,2,3,4\n1,2,3,4,5\n1,2\n1,2,3,4\n....\n', ""import pandas as pd\nmy_cols=['A','B','C','D','E']\nmy_df=pd.read_table(path,sep=',',header=None,names=my_cols)\n""]";"['1,2,3\n1,2,3,4\n1,2,3,4,5\n1,2\n1,2,3,4\n....\n', ""import pandas as pd\nmy_cols=['A','B','C','D','E']\nmy_df=pd.read_table(path,sep=',',header=None,names=my_cols)\n""]"
150;1.0;0;15315452;;1;77;<python><pandas>;Selecting with complex criteria from pandas.DataFrame;"<p>For example I have simple DF:</p>

<pre><code>import pandas as pd
from random import randint

df = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],
                   'B': [randint(1, 9)*10 for x in xrange(10)],
                   'C': [randint(1, 9)*100 for x in xrange(10)]})
</code></pre>

<p>Can I select values from 'A' for which corresponding values for 'B' will be greater than 50, and for 'C' - not equal 900, using methods and idioms of Pandas?</p>
";153283.0;"[""import pandas as pd\nfrom random import randint\n\ndf = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],\n                   'B': [randint(1, 9)*10 for x in xrange(10)],\n                   'C': [randint(1, 9)*100 for x in xrange(10)]})\n""]";"[""import pandas as pd\nfrom random import randint\n\ndf = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],\n                   'B': [randint(1, 9)*10 for x in xrange(10)],\n                   'C': [randint(1, 9)*100 for x in xrange(10)]})\n""]"
151;2.0;0;15322632;;1;37;<python><group-by><pandas>;python pandas, DF.groupby().agg(), column reference in agg();"<p>On a concrete problem, say I have a DataFrame DF</p>

<pre><code>     word  tag count
0    a     S    30
1    the   S    20
2    a     T    60
3    an    T    5
4    the   T    10 
</code></pre>

<p>I want to find, <strong>for every ""word"", the ""tag"" that has the most ""count""</strong>. So the return would be something like</p>

<pre><code>     word  tag count
1    the   S    20
2    a     T    60
3    an    T    5
</code></pre>

<p>I don't care about the count column or if the order/Index is original or messed up. Returning a dictionary {<strong>'the' : 'S'</strong>, ...} is just fine.</p>

<p>I hope I can do</p>

<pre><code>DF.groupby(['word']).agg(lambda x: x['tag'][ x['count'].argmax() ] )
</code></pre>

<p>but it doesn't work. I can't access column information.</p>

<p>More abstractly, <strong>what does the <em>function</em> in agg(<em>function</em>) see as its argument</strong>?</p>

<p>btw, is .agg() the same as .aggregate() ?</p>

<p>Many thanks.</p>
";68108.0;"['     word  tag count\n0    a     S    30\n1    the   S    20\n2    a     T    60\n3    an    T    5\n4    the   T    10 \n', '     word  tag count\n1    the   S    20\n2    a     T    60\n3    an    T    5\n', ""DF.groupby(['word']).agg(lambda x: x['tag'][ x['count'].argmax() ] )\n""]";"['     word  tag count\n0    a     S    30\n1    the   S    20\n2    a     T    60\n3    an    T    5\n4    the   T    10 \n', '     word  tag count\n1    the   S    20\n2    a     T    60\n3    an    T    5\n', ""DF.groupby(['word']).agg(lambda x: x['tag'][ x['count'].argmax() ] )\n""]"
152;4.0;2;15325182;;1;56;<python><regex><pandas>;How to filter rows in pandas by regex;"<p>I would like to cleanly filter a dataframe using regex on one of the columns.</p>

<p>For a contrived example:</p>

<pre><code>In [210]: foo = pd.DataFrame({'a' : [1,2,3,4], 'b' : ['hi', 'foo', 'fat', 'cat']})
In [211]: foo
Out[211]: 
   a    b
0  1   hi
1  2  foo
2  3  fat
3  4  cat
</code></pre>

<p>I want to filter the rows to those that start with <code>f</code> using a regex. First go:</p>

<pre><code>In [213]: foo.b.str.match('f.*')
Out[213]: 
0    []
1    ()
2    ()
3    []
</code></pre>

<p>That's not too terribly useful. However this will get me my boolean index:</p>

<pre><code>In [226]: foo.b.str.match('(f.*)').str.len() &gt; 0
Out[226]: 
0    False
1     True
2     True
3    False
Name: b
</code></pre>

<p>So I could then do my restriction by:</p>

<pre><code>In [229]: foo[foo.b.str.match('(f.*)').str.len() &gt; 0]
Out[229]: 
   a    b
1  2  foo
2  3  fat
</code></pre>

<p>That makes me artificially put a group into the regex though, and seems like maybe not the clean way to go. Is there a better way to do this?</p>
";38880.0;"[""In [210]: foo = pd.DataFrame({'a' : [1,2,3,4], 'b' : ['hi', 'foo', 'fat', 'cat']})\nIn [211]: foo\nOut[211]: \n   a    b\n0  1   hi\n1  2  foo\n2  3  fat\n3  4  cat\n"", ""In [213]: foo.b.str.match('f.*')\nOut[213]: \n0    []\n1    ()\n2    ()\n3    []\n"", ""In [226]: foo.b.str.match('(f.*)').str.len() > 0\nOut[226]: \n0    False\n1     True\n2     True\n3    False\nName: b\n"", ""In [229]: foo[foo.b.str.match('(f.*)').str.len() > 0]\nOut[229]: \n   a    b\n1  2  foo\n2  3  fat\n""]";"[""In [210]: foo = pd.DataFrame({'a' : [1,2,3,4], 'b' : ['hi', 'foo', 'fat', 'cat']})\nIn [211]: foo\nOut[211]: \n   a    b\n0  1   hi\n1  2  foo\n2  3  fat\n3  4  cat\n"", 'f', ""In [213]: foo.b.str.match('f.*')\nOut[213]: \n0    []\n1    ()\n2    ()\n3    []\n"", ""In [226]: foo.b.str.match('(f.*)').str.len() > 0\nOut[226]: \n0    False\n1     True\n2     True\n3    False\nName: b\n"", ""In [229]: foo[foo.b.str.match('(f.*)').str.len() > 0]\nOut[229]: \n   a    b\n1  2  foo\n2  3  fat\n""]"
153;4.0;0;15360925;;1;55;<python><dataframe><pandas><series>;How to get the first column of a pandas DataFrame as a Series?;"<p>I tried:</p>

<pre><code>x=pandas.DataFrame(...)
s = x.take([0], axis=1)
</code></pre>

<p>And <code>s</code> gets a DataFrame, not a Series.</p>
";81214.0;['x=pandas.DataFrame(...)\ns = x.take([0], axis=1)\n'];['x=pandas.DataFrame(...)\ns = x.take([0], axis=1)\n', 's']
154;4.0;2;15411158;;1;87;<python><pandas><count><group-by><distinct>;Pandas count(distinct) equivalent;"<p>I am using pandas as a db substitute as I have multiple databases (oracle, mssql, etc) and I am unable to make a sequence of commands to a SQL equivalent.</p>

<p>I have a table loaded in a DataFrame with some columns:</p>

<pre><code>YEARMONTH, CLIENTCODE, SIZE, .... etc etc
</code></pre>

<p>In SQL, to count the amount of different clients per year would be:</p>

<pre><code>SELECT count(distinct CLIENTCODE) FROM table GROUP BY YEARMONTH;
</code></pre>

<p>And the result would be </p>

<pre><code>201301    5000
201302    13245
</code></pre>

<p>How can I do that in pandas? </p>
";100056.0;"['YEARMONTH, CLIENTCODE, SIZE, .... etc etc\n', 'SELECT count(distinct CLIENTCODE) FROM table GROUP BY YEARMONTH;\n', '201301    5000\n201302    13245\n']";"['YEARMONTH, CLIENTCODE, SIZE, .... etc etc\n', 'SELECT count(distinct CLIENTCODE) FROM table GROUP BY YEARMONTH;\n', '201301    5000\n201302    13245\n']"
155;1.0;1;15465645;;1;35;<python><matplotlib><group-by><pandas><data-analysis>;Plotting results of Pandas GroupBy;"<p>I'm starting to learn Pandas and am trying to find the most Pythonic (or panda-thonic?) ways to do certain tasks.</p>

<p>Suppose we have a DataFrame with columns A, B, and C.</p>

<ul>
<li>Column A contains boolean values: each row's A value is either true or false.</li>
<li>Column B has some important values we want to plot.</li>
</ul>

<p>What we want to discover is the subtle distinctions between B values for rows that have A set to false, vs. B values for rows that have A is true.</p>

<p>In other words, <strong>how can I group by the value of column A (either true or false), then plot the values of column B for both groups on the same graph?</strong> The two datasets should be colored differently to be able to distinguish the points.</p>

<hr>

<p>Next, let's add another feature to this program: before graphing, we want to compute another value for each row and store it in column D. This value is the mean of all data stored in B for the entire five minutes before a record - but we only include rows that have the same boolean value stored in A.</p>

<p>In other words, <strong>if I have a row where <code>A=True</code> and <code>time=t</code>, I want to compute a value for column D that is the mean of B for all records from time <code>t-5</code> to <code>t</code> that have the same <code>A=True</code>.</strong></p>

<p>In this case, how can we execute the groupby on values of A, then apply this computation to each individual group, and finally plot the D values for the two groups?</p>
";31249.0;[];['A=True', 'time=t', 't-5', 't', 'A=True']
156;4.0;0;15570099;;1;26;<python><pandas><pivot-table>;Pandas Pivot tables row subtotals;"<p>I'm using Pandas 0.10.1</p>

<p>Considering this Dataframe:</p>

<pre><code>Date       State   City    SalesToday  SalesMTD  SalesYTD
20130320     stA    ctA            20       400      1000
20130320     stA    ctB            30       500      1100
20130320     stB    ctC            10       500       900
20130320     stB    ctD            40       200      1300
20130320     stC    ctF            30       300       800
</code></pre>

<p>How can i group subtotals per state?</p>

<pre><code>State   City  SalesToday  SalesMTD  SalesYTD
  stA    ALL          50       900      2100
  stA    ctA          20       400      1000
  stA    ctB          30       500      1100
</code></pre>

<p>I tried with a pivot table but i only can have subtotals in columns</p>

<pre><code>table = pivot_table(df, values=['SalesToday', 'SalesMTD','SalesYTD'],\
                     rows=['State','City'], aggfunc=np.sum, margins=True)
</code></pre>

<p>I can achieve this on excel, with a pivot table.</p>
";17499.0;"['Date       State   City    SalesToday  SalesMTD  SalesYTD\n20130320     stA    ctA            20       400      1000\n20130320     stA    ctB            30       500      1100\n20130320     stB    ctC            10       500       900\n20130320     stB    ctD            40       200      1300\n20130320     stC    ctF            30       300       800\n', 'State   City  SalesToday  SalesMTD  SalesYTD\n  stA    ALL          50       900      2100\n  stA    ctA          20       400      1000\n  stA    ctB          30       500      1100\n', ""table = pivot_table(df, values=['SalesToday', 'SalesMTD','SalesYTD'],\\\n                     rows=['State','City'], aggfunc=np.sum, margins=True)\n""]";"['Date       State   City    SalesToday  SalesMTD  SalesYTD\n20130320     stA    ctA            20       400      1000\n20130320     stA    ctB            30       500      1100\n20130320     stB    ctC            10       500       900\n20130320     stB    ctD            40       200      1300\n20130320     stC    ctF            30       300       800\n', 'State   City  SalesToday  SalesMTD  SalesYTD\n  stA    ALL          50       900      2100\n  stA    ctA          20       400      1000\n  stA    ctB          30       500      1100\n', ""table = pivot_table(df, values=['SalesToday', 'SalesMTD','SalesYTD'],\\\n                     rows=['State','City'], aggfunc=np.sum, margins=True)\n""]"
157;5.0;5;15705630;;1;40;<python><pandas>;Python : Getting the Row which has the max value in groups using groupby;"<p>I hope I can find help for my question. I am searching for a solution for the following problem:</p>

<p>I have a dataFrame like:</p>

<pre><code> Sp  Mt Value  count
0  MM1  S1   a      **3**
1  MM1  S1   n      2
2  MM1  S3   cb     5
3  MM2  S3   mk      **8**
4  MM2  S4   bg     **10**
5  MM2  S4   dgd      1
6  MM4  S2  rd     2
7  MM4  S2   cb      2
8  MM4  S2   uyi      **7**
</code></pre>

<p>My objective is to get the result rows whose count is max between the groups, like :</p>

<pre><code>0  MM1  S1   a      **3**
1 3  MM2  S3   mk      **8**
4  MM2  S4   bg     **10** 
8  MM4  S2   uyi      **7**
</code></pre>

<p>Somebody knows how can I do it in pandas or in python?</p>

<p><strong>UPDATE</strong></p>

<p>I didn't give more details for my question. For my problem, I want to group by ['Sp','Mt'].  Let take a second example like this :</p>

<pre><code>   Sp   Mt   Value  count
4  MM2  S4   bg     10
5  MM2  S4   dgd    1
6  MM4  S2   rd     2
7  MM4  S2   cb     8
8  MM4  S2   uyi    8
</code></pre>

<p>For the above example, I want to get ALL the rows where count equals max in each group e.g :</p>

<pre><code>MM2  S4   bg     10
MM4  S2   cb     8
MM4  S2   uyi    8
</code></pre>
";46906.0;[' Sp  Mt Value  count\n0  MM1  S1   a      **3**\n1  MM1  S1   n      2\n2  MM1  S3   cb     5\n3  MM2  S3   mk      **8**\n4  MM2  S4   bg     **10**\n5  MM2  S4   dgd      1\n6  MM4  S2  rd     2\n7  MM4  S2   cb      2\n8  MM4  S2   uyi      **7**\n', '0  MM1  S1   a      **3**\n1 3  MM2  S3   mk      **8**\n4  MM2  S4   bg     **10** \n8  MM4  S2   uyi      **7**\n', '   Sp   Mt   Value  count\n4  MM2  S4   bg     10\n5  MM2  S4   dgd    1\n6  MM4  S2   rd     2\n7  MM4  S2   cb     8\n8  MM4  S2   uyi    8\n', 'MM2  S4   bg     10\nMM4  S2   cb     8\nMM4  S2   uyi    8\n'];[' Sp  Mt Value  count\n0  MM1  S1   a      **3**\n1  MM1  S1   n      2\n2  MM1  S3   cb     5\n3  MM2  S3   mk      **8**\n4  MM2  S4   bg     **10**\n5  MM2  S4   dgd      1\n6  MM4  S2  rd     2\n7  MM4  S2   cb      2\n8  MM4  S2   uyi      **7**\n', '0  MM1  S1   a      **3**\n1 3  MM2  S3   mk      **8**\n4  MM2  S4   bg     **10** \n8  MM4  S2   uyi      **7**\n', '   Sp   Mt   Value  count\n4  MM2  S4   bg     10\n5  MM2  S4   dgd    1\n6  MM4  S2   rd     2\n7  MM4  S2   cb     8\n8  MM4  S2   uyi    8\n', 'MM2  S4   bg     10\nMM4  S2   cb     8\nMM4  S2   uyi    8\n']
158;3.0;0;15723628;;1;30;<python><pandas>;Pandas - make a column dtype object or Factor;"<p>In pandas, how can I convert a column of a DataFrame into dtype object?
Or better yet, into a factor? (For those who speak R, in Python, how do I <code>as.factor()</code>?)</p>

<p>Also, what's the difference between <code>pandas.Factor</code> and <code>pandas.Categorical</code>?</p>
";21102.0;[];['as.factor()', 'pandas.Factor', 'pandas.Categorical']
159;3.0;0;15741759;;1;26;<python><pandas>;Find maximum value of a column and return the corresponding row values using Pandas;"<p><img src=""https://i.stack.imgur.com/a34it.png"" alt=""Structure of data;""></p>

<p>Using Python Pandas I am trying to find the 'Country' &amp; 'Place' with the maximum value.</p>

<p>This returns the maximum value:</p>

<pre><code>data.groupby(['Country','Place'])['Value'].max()
</code></pre>

<p>But how do I get the corresponding 'Country' and 'Place' name?</p>
";51057.0;"[""data.groupby(['Country','Place'])['Value'].max()\n""]";"[""data.groupby(['Country','Place'])['Value'].max()\n""]"
160;7.0;3;15771472;;1;41;<python><pandas><time-series>;Pandas: rolling mean by time interval;"<p>I'm new to Pandas.... I've got a bunch of polling data; I want to compute a rolling mean to get an estimate for each day based on a three-day window. As I understand from <a href=""https://stackoverflow.com/questions/9762193/pandas-rolling-median-for-duplicate-time-series-data"">this question</a>, the rolling_* functions compute the window based on a specified number of values, and not a specific datetime range. </p>

<p>Is there a different function that implements this functionality? Or am I stuck writing my own?</p>

<p>EDIT: </p>

<p>Sample input data: </p>

<pre><code>polls_subset.tail(20)
Out[185]: 
            favorable  unfavorable  other

enddate                                  
2012-10-25       0.48         0.49   0.03
2012-10-25       0.51         0.48   0.02
2012-10-27       0.51         0.47   0.02
2012-10-26       0.56         0.40   0.04
2012-10-28       0.48         0.49   0.04
2012-10-28       0.46         0.46   0.09
2012-10-28       0.48         0.49   0.03
2012-10-28       0.49         0.48   0.03
2012-10-30       0.53         0.45   0.02
2012-11-01       0.49         0.49   0.03
2012-11-01       0.47         0.47   0.05
2012-11-01       0.51         0.45   0.04
2012-11-03       0.49         0.45   0.06
2012-11-04       0.53         0.39   0.00
2012-11-04       0.47         0.44   0.08
2012-11-04       0.49         0.48   0.03
2012-11-04       0.52         0.46   0.01
2012-11-04       0.50         0.47   0.03
2012-11-05       0.51         0.46   0.02
2012-11-07       0.51         0.41   0.00
</code></pre>

<p>Output would have only one row for each date. </p>

<p>EDIT x2: fixed typo </p>
";47293.0;['polls_subset.tail(20)\nOut[185]: \n            favorable  unfavorable  other\n\nenddate                                  \n2012-10-25       0.48         0.49   0.03\n2012-10-25       0.51         0.48   0.02\n2012-10-27       0.51         0.47   0.02\n2012-10-26       0.56         0.40   0.04\n2012-10-28       0.48         0.49   0.04\n2012-10-28       0.46         0.46   0.09\n2012-10-28       0.48         0.49   0.03\n2012-10-28       0.49         0.48   0.03\n2012-10-30       0.53         0.45   0.02\n2012-11-01       0.49         0.49   0.03\n2012-11-01       0.47         0.47   0.05\n2012-11-01       0.51         0.45   0.04\n2012-11-03       0.49         0.45   0.06\n2012-11-04       0.53         0.39   0.00\n2012-11-04       0.47         0.44   0.08\n2012-11-04       0.49         0.48   0.03\n2012-11-04       0.52         0.46   0.01\n2012-11-04       0.50         0.47   0.03\n2012-11-05       0.51         0.46   0.02\n2012-11-07       0.51         0.41   0.00\n'];['polls_subset.tail(20)\nOut[185]: \n            favorable  unfavorable  other\n\nenddate                                  \n2012-10-25       0.48         0.49   0.03\n2012-10-25       0.51         0.48   0.02\n2012-10-27       0.51         0.47   0.02\n2012-10-26       0.56         0.40   0.04\n2012-10-28       0.48         0.49   0.04\n2012-10-28       0.46         0.46   0.09\n2012-10-28       0.48         0.49   0.03\n2012-10-28       0.49         0.48   0.03\n2012-10-30       0.53         0.45   0.02\n2012-11-01       0.49         0.49   0.03\n2012-11-01       0.47         0.47   0.05\n2012-11-01       0.51         0.45   0.04\n2012-11-03       0.49         0.45   0.06\n2012-11-04       0.53         0.39   0.00\n2012-11-04       0.47         0.44   0.08\n2012-11-04       0.49         0.48   0.03\n2012-11-04       0.52         0.46   0.01\n2012-11-04       0.50         0.47   0.03\n2012-11-05       0.51         0.46   0.02\n2012-11-07       0.51         0.41   0.00\n']
161;9.0;0;15772009;;1;45;<python><numpy><pandas>;shuffling/permutating a DataFrame in pandas;"<p>What's a simple and efficient way to shuffle a dataframe in pandas, by rows or by columns? I.e. how to write a function <code>shuffle(df, n, axis=0)</code> that takes a dataframe, a number of shuffles <code>n</code>, and an axis (<code>axis=0</code> is rows, <code>axis=1</code> is columns) and returns a copy of the dataframe that has been shuffled <code>n</code> times. </p>

<p><strong>Edit</strong>: key is to do this without destroying the row/column labels of the dataframe. If you just shuffle <code>df.index</code> that loses all that information. I want the resulting <code>df</code> to be the same as the original except with the order of rows or order of columns different.</p>

<p><strong>Edit2</strong>: My question was unclear. When I say shuffle the rows, I mean shuffle each row independently. So if you have two columns <code>a</code> and <code>b</code>, I want each row shuffled on its own, so that you don't have the same associations between <code>a</code> and <code>b</code> as you do if you just re-order each row as a whole. Something like: </p>

<pre><code>for 1...n:
  for each col in df: shuffle column
return new_df
</code></pre>

<p>But hopefully more efficient than naive looping. This does not work for me:</p>

<pre><code>def shuffle(df, n, axis=0):
        shuffled_df = df.copy()
        for k in range(n):
            shuffled_df.apply(np.random.shuffle(shuffled_df.values),axis=axis)
        return shuffled_df

df = pandas.DataFrame({'A':range(10), 'B':range(10)})
shuffle(df, 5)
</code></pre>
";40329.0;"['for 1...n:\n  for each col in df: shuffle column\nreturn new_df\n', ""def shuffle(df, n, axis=0):\n        shuffled_df = df.copy()\n        for k in range(n):\n            shuffled_df.apply(np.random.shuffle(shuffled_df.values),axis=axis)\n        return shuffled_df\n\ndf = pandas.DataFrame({'A':range(10), 'B':range(10)})\nshuffle(df, 5)\n""]";"['shuffle(df, n, axis=0)', 'n', 'axis=0', 'axis=1', 'n', 'df.index', 'df', 'a', 'b', 'a', 'b', 'for 1...n:\n  for each col in df: shuffle column\nreturn new_df\n', ""def shuffle(df, n, axis=0):\n        shuffled_df = df.copy()\n        for k in range(n):\n            shuffled_df.apply(np.random.shuffle(shuffled_df.values),axis=axis)\n        return shuffled_df\n\ndf = pandas.DataFrame({'A':range(10), 'B':range(10)})\nshuffle(df, 5)\n""]"
162;1.0;0;15777951;;1;25;<pandas><suppress-warnings>;How to suppress Pandas Future warning ?;"<p>When I run the program, Pandas gives 'Future warning' like below every time.</p>

<pre><code>D:\Python\lib\site-packages\pandas\core\frame.py:3581: FutureWarning: rename with inplace=True  will return None from pandas 0.11 onward
  "" from pandas 0.11 onward"", FutureWarning) 
</code></pre>

<p>I got the msg, but I just want to stop Pandas showing such msg again and again, is there any buildin parameter that I can set to let Pandas not pop up the 'Future warning' ?</p>
";7622.0;"['D:\\Python\\lib\\site-packages\\pandas\\core\\frame.py:3581: FutureWarning: rename with inplace=True  will return None from pandas 0.11 onward\n  "" from pandas 0.11 onward"", FutureWarning) \n']";"['D:\\Python\\lib\\site-packages\\pandas\\core\\frame.py:3581: FutureWarning: rename with inplace=True  will return None from pandas 0.11 onward\n  "" from pandas 0.11 onward"", FutureWarning) \n']"
163;1.0;0;15819050;;1;22;<python><pandas>;Pandas DataFrame concat vs append;"<p>I have a list of 4 pandas dataframes containing a day of tick data that I want to merge into a single data frame. I cannot understand the behavior of concat on my timestamps. See details below:</p>

<pre><code>data

[&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 35228 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-03-28 18:59:20.357000+02:00
Data columns:
Price       4040  non-null values
Volume      4040  non-null values
BidQty      35228  non-null values
BidPrice    35228  non-null values
AskPrice    35228  non-null values
AskQty      35228  non-null values
dtypes: float64(6),
&lt;class 'pandas.core.frame.DataFrame'&gt;

DatetimeIndex: 33088 entries, 2013-04-01 00:03:17.047000+02:00 to 2013-04-01 18:59:58.175000+02:00
Data columns:
Price       3969  non-null values
Volume      3969  non-null values
BidQty      33088  non-null values
BidPrice    33088  non-null values
AskPrice    33088  non-null values
AskQty      33088  non-null values
dtypes: float64(6),
&lt;class 'pandas.core.frame.DataFrame'&gt;

DatetimeIndex: 50740 entries, 2013-04-02 00:03:27.470000+02:00 to 2013-04-02 18:59:58.172000+02:00
Data columns:
Price       7326  non-null values
Volume      7326  non-null values
BidQty      50740  non-null values
BidPrice    50740  non-null values
AskPrice    50740  non-null values
AskQty      50740  non-null values
dtypes: float64(6),
&lt;class 'pandas.core.frame.DataFrame'&gt;

DatetimeIndex: 60799 entries, 2013-04-03 00:03:06.994000+02:00 to 2013-04-03 18:59:58.180000+02:00
Data columns:
Price       8258  non-null values
Volume      8258  non-null values
BidQty      60799  non-null values
BidPrice    60799  non-null values
AskPrice    60799  non-null values
AskQty      60799  non-null values
dtypes: float64(6)]
</code></pre>

<p>Using <code>append</code> I get:</p>

<pre><code>pd.DataFrame().append(data)

&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 179855 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-04-03 18:59:58.180000+02:00
Data columns:
AskPrice    179855  non-null values
AskQty      179855  non-null values
BidPrice    179855  non-null values
BidQty      179855  non-null values
Price       23593  non-null values
Volume      23593  non-null values
dtypes: float64(6)
</code></pre>

<p>Using <code>concat</code> I get:</p>

<pre><code>pd.concat(data)

&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 179855 entries, 2013-03-27 22:00:07.089000+02:00 to 2013-04-03 16:59:58.180000+02:00
Data columns:
Price       23593  non-null values
Volume      23593  non-null values
BidQty      179855  non-null values
BidPrice    179855  non-null values
AskPrice    179855  non-null values
AskQty      179855  non-null values
dtypes: float64(6)
</code></pre>

<p>Notice how the index changes when using <code>concat</code>. Why is that happening and how would I go about using <code>concat</code> to reproduce the results obtained using <code>append</code>? (Since <code>concat</code> seems so much faster; 24.6 ms per loop vs 3.02 s per loop)</p>
";33712.0;"[""data\n\n[<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 35228 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-03-28 18:59:20.357000+02:00\nData columns:\nPrice       4040  non-null values\nVolume      4040  non-null values\nBidQty      35228  non-null values\nBidPrice    35228  non-null values\nAskPrice    35228  non-null values\nAskQty      35228  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 33088 entries, 2013-04-01 00:03:17.047000+02:00 to 2013-04-01 18:59:58.175000+02:00\nData columns:\nPrice       3969  non-null values\nVolume      3969  non-null values\nBidQty      33088  non-null values\nBidPrice    33088  non-null values\nAskPrice    33088  non-null values\nAskQty      33088  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 50740 entries, 2013-04-02 00:03:27.470000+02:00 to 2013-04-02 18:59:58.172000+02:00\nData columns:\nPrice       7326  non-null values\nVolume      7326  non-null values\nBidQty      50740  non-null values\nBidPrice    50740  non-null values\nAskPrice    50740  non-null values\nAskQty      50740  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 60799 entries, 2013-04-03 00:03:06.994000+02:00 to 2013-04-03 18:59:58.180000+02:00\nData columns:\nPrice       8258  non-null values\nVolume      8258  non-null values\nBidQty      60799  non-null values\nBidPrice    60799  non-null values\nAskPrice    60799  non-null values\nAskQty      60799  non-null values\ndtypes: float64(6)]\n"", ""pd.DataFrame().append(data)\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 179855 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-04-03 18:59:58.180000+02:00\nData columns:\nAskPrice    179855  non-null values\nAskQty      179855  non-null values\nBidPrice    179855  non-null values\nBidQty      179855  non-null values\nPrice       23593  non-null values\nVolume      23593  non-null values\ndtypes: float64(6)\n"", ""pd.concat(data)\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 179855 entries, 2013-03-27 22:00:07.089000+02:00 to 2013-04-03 16:59:58.180000+02:00\nData columns:\nPrice       23593  non-null values\nVolume      23593  non-null values\nBidQty      179855  non-null values\nBidPrice    179855  non-null values\nAskPrice    179855  non-null values\nAskQty      179855  non-null values\ndtypes: float64(6)\n""]";"[""data\n\n[<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 35228 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-03-28 18:59:20.357000+02:00\nData columns:\nPrice       4040  non-null values\nVolume      4040  non-null values\nBidQty      35228  non-null values\nBidPrice    35228  non-null values\nAskPrice    35228  non-null values\nAskQty      35228  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 33088 entries, 2013-04-01 00:03:17.047000+02:00 to 2013-04-01 18:59:58.175000+02:00\nData columns:\nPrice       3969  non-null values\nVolume      3969  non-null values\nBidQty      33088  non-null values\nBidPrice    33088  non-null values\nAskPrice    33088  non-null values\nAskQty      33088  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 50740 entries, 2013-04-02 00:03:27.470000+02:00 to 2013-04-02 18:59:58.172000+02:00\nData columns:\nPrice       7326  non-null values\nVolume      7326  non-null values\nBidQty      50740  non-null values\nBidPrice    50740  non-null values\nAskPrice    50740  non-null values\nAskQty      50740  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 60799 entries, 2013-04-03 00:03:06.994000+02:00 to 2013-04-03 18:59:58.180000+02:00\nData columns:\nPrice       8258  non-null values\nVolume      8258  non-null values\nBidQty      60799  non-null values\nBidPrice    60799  non-null values\nAskPrice    60799  non-null values\nAskQty      60799  non-null values\ndtypes: float64(6)]\n"", 'append', ""pd.DataFrame().append(data)\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 179855 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-04-03 18:59:58.180000+02:00\nData columns:\nAskPrice    179855  non-null values\nAskQty      179855  non-null values\nBidPrice    179855  non-null values\nBidQty      179855  non-null values\nPrice       23593  non-null values\nVolume      23593  non-null values\ndtypes: float64(6)\n"", 'concat', ""pd.concat(data)\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 179855 entries, 2013-03-27 22:00:07.089000+02:00 to 2013-04-03 16:59:58.180000+02:00\nData columns:\nPrice       23593  non-null values\nVolume      23593  non-null values\nBidQty      179855  non-null values\nBidPrice    179855  non-null values\nAskPrice    179855  non-null values\nAskQty      179855  non-null values\ndtypes: float64(6)\n"", 'concat', 'concat', 'append', 'concat']"
164;1.0;0;15854878;;1;28;<python><pandas>;Correlation between columns in DataFrame;"<p>I'm pretty new to pandas, so I guess I'm doing something wrong - </p>

<p>I have a DataFrame:</p>

<pre><code>     a     b
0  0.5  0.75
1  0.5  0.75
2  0.5  0.75
3  0.5  0.75
4  0.5  0.75
</code></pre>

<p><code>df.corr()</code> gives me: </p>

<pre><code>    a   b
a NaN NaN
b NaN NaN
</code></pre>

<p>but <code>np.correlate(df[""a""], df[""b""])</code> gives: <code>1.875</code></p>

<p>Why is that? 
I want to have the correlation matrix for my DataFrame and thought that <code>corr()</code> does that (at least according to the documentation). Why does it return <code>NaN</code>?</p>

<p>What's the correct way to calculate?</p>

<p>Many thanks!</p>
";45152.0;['     a     b\n0  0.5  0.75\n1  0.5  0.75\n2  0.5  0.75\n3  0.5  0.75\n4  0.5  0.75\n', '    a   b\na NaN NaN\nb NaN NaN\n'];"['     a     b\n0  0.5  0.75\n1  0.5  0.75\n2  0.5  0.75\n3  0.5  0.75\n4  0.5  0.75\n', 'df.corr()', '    a   b\na NaN NaN\nb NaN NaN\n', 'np.correlate(df[""a""], df[""b""])', '1.875', 'corr()', 'NaN']"
165;2.0;0;15862034;;1;21;<python><pandas>;Access index of last element in data frame;"<p>I've looking around for this but I can't seem to find it (though it must be extremely trivial).</p>

<p>The problem that I have is that I would like to retrieve the value of a column for the first and last entries of a data frame. But if I do:</p>

<pre><code>df.ix[0]['date']
</code></pre>

<p>I get:</p>

<pre><code>datetime.datetime(2011, 1, 10, 16, 0)
</code></pre>

<p>but if I do:</p>

<pre><code>df[-1:]['date']
</code></pre>

<p>I get:</p>

<pre><code>myIndex
13         2011-12-20 16:00:00
Name: mydate
</code></pre>

<p>with a different format. Ideally, I would like to be able to access the value of the last index of the data frame, but I can't find how.</p>

<p>I even tried to create a column (IndexCopy) with the values of the index and try:</p>

<pre><code>df.ix[df.tail(1)['IndexCopy']]['mydate']
</code></pre>

<p>but this also yields a different format (since df.tail(1)['IndexCopy'] does not output a simple integer). </p>

<p>Any ideas? </p>
";31919.0;"[""df.ix[0]['date']\n"", 'datetime.datetime(2011, 1, 10, 16, 0)\n', ""df[-1:]['date']\n"", 'myIndex\n13         2011-12-20 16:00:00\nName: mydate\n', ""df.ix[df.tail(1)['IndexCopy']]['mydate']\n""]";"[""df.ix[0]['date']\n"", 'datetime.datetime(2011, 1, 10, 16, 0)\n', ""df[-1:]['date']\n"", 'myIndex\n13         2011-12-20 16:00:00\nName: mydate\n', ""df.ix[df.tail(1)['IndexCopy']]['mydate']\n""]"
166;4.0;5;15891038;;1;257;<python><pandas><dataframe><types><casting>;Pandas: change data type of columns;"<p>I want to convert a table, represented as a list of lists, into a Pandas DataFrame. As an extremely simplified example:</p>

<pre><code>a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]
df = pd.DataFrame(a)
</code></pre>

<p>What is the best way to convert the columns to the appropriate types, in this case columns 2 and 3 into floats? Is there a way to specify the types while converting to DataFrame? Or is it better to create the DataFrame first and then loop through the columns to change the type for each column? Ideally I would like to do this in a dynamic way because there can be hundreds of columns and I don't want to specify exactly which columns are of which type. All I can guarantee is that each columns contains values of the same type.</p>
";406291.0;"[""a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\ndf = pd.DataFrame(a)\n""]";"[""a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\ndf = pd.DataFrame(a)\n""]"
167;5.0;0;15923826;;1;33;<python><pandas>;Random row selection in Pandas dataframe;"<p>Is there a way to select random rows from a DataFrame in Pandas.</p>

<p>In R, using the car package, there is a useful function <code>some(x, n)</code> which is similar to head but selects, in this example, 10 rows at random from x.</p>

<p>I have also looked at the slicing documentation and there seems to be nothing equivalent.</p>

<h1>Update</h1>

<p>Now using version 20. There is a sample method.</p>

<p><code>df.sample(n)</code></p>
";26646.0;[];['some(x, n)', 'df.sample(n)']
168;7.0;3;15943769;;1;260;<python><pandas><dataframe>;How do I get the row count of a Pandas dataframe?;"<p>I'm trying to get the number of rows of dataframe df with Pandas, and here is my code.</p>

<h3>Method 1:</h3>

<pre><code>total_rows = df.count
print total_rows +1
</code></pre>

<h3>Method 2:</h3>

<pre><code>total_rows = df['First_columnn_label'].count
print total_rows +1
</code></pre>

<p>Both the code snippets give me this error:</p>

<blockquote>
  <p>TypeError: unsupported operand type(s) for +: 'instancemethod' and 'int'</p>
</blockquote>

<p>What am I doing wrong?</p>

<p>According to <a href=""https://stackoverflow.com/a/15943975/4230591"">the answer</a> given by <a href=""https://stackoverflow.com/users/1199589/root"">@root</a> the best (the fastest) way to check df length is to call:</p>

<pre><code>len(df.index)
</code></pre>
";373069.0;"['total_rows = df.count\nprint total_rows +1\n', ""total_rows = df['First_columnn_label'].count\nprint total_rows +1\n"", 'len(df.index)\n']";"['total_rows = df.count\nprint total_rows +1\n', ""total_rows = df['First_columnn_label'].count\nprint total_rows +1\n"", 'len(df.index)\n']"
169;4.0;0;15998188;;1;80;<python><pandas><boolean-logic>;How can I obtain the element-wise logical NOT of a pandas Series?;"<p>I have a relatively straightforward question, today.  I have a pandas <code>Series</code> object containing boolean values. How can I get a series containing the logical <code>NOT</code> of each value?</p>

<p>For example, consider a series containing:</p>

<pre><code>True
True
True
False
</code></pre>

<p>The series I'd like to get would contain:</p>

<pre><code>False
False
False
True
</code></pre>

<p>This seems like it should be reasonably simple, but apparently I've misplaced my mojo today =(</p>

<p>Thanks!</p>
";30530.0;['True\nTrue\nTrue\nFalse\n', 'False\nFalse\nFalse\nTrue\n'];['Series', 'NOT', 'True\nTrue\nTrue\nFalse\n', 'False\nFalse\nFalse\nTrue\n']
170;2.0;0;16031056;;1;38;<python><dataframe><pandas><tuples>;How to form tuple column from two columns in Pandas;"<p>I've got a Pandas DataFrame and I want to combine the 'lat' and 'long' columns to form a tuple.</p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 205482 entries, 0 to 209018
Data columns:
Month           205482  non-null values
Reported by     205482  non-null values
Falls within    205482  non-null values
Easting         205482  non-null values
Northing        205482  non-null values
Location        205482  non-null values
Crime type      205482  non-null values
long            205482  non-null values
lat             205482  non-null values
dtypes: float64(4), object(5)
</code></pre>

<p>The code I tried to use was:</p>

<pre><code>def merge_two_cols(series): 
    return (series['lat'], series['long'])

sample['lat_long'] = sample.apply(merge_two_cols, axis=1)
</code></pre>

<p>However, this returned the following error:</p>

<pre><code>---------------------------------------------------------------------------
 AssertionError                            Traceback (most recent call last)
&lt;ipython-input-261-e752e52a96e6&gt; in &lt;module&gt;()
      2     return (series['lat'], series['long'])
      3 
----&gt; 4 sample['lat_long'] = sample.apply(merge_two_cols, axis=1)
      5
</code></pre>

<p>...</p>

<pre><code>AssertionError: Block shape incompatible with manager 
</code></pre>

<p>How can I solve this problem?</p>
";23199.0;"[""<class 'pandas.core.frame.DataFrame'>\nInt64Index: 205482 entries, 0 to 209018\nData columns:\nMonth           205482  non-null values\nReported by     205482  non-null values\nFalls within    205482  non-null values\nEasting         205482  non-null values\nNorthing        205482  non-null values\nLocation        205482  non-null values\nCrime type      205482  non-null values\nlong            205482  non-null values\nlat             205482  non-null values\ndtypes: float64(4), object(5)\n"", ""def merge_two_cols(series): \n    return (series['lat'], series['long'])\n\nsample['lat_long'] = sample.apply(merge_two_cols, axis=1)\n"", ""---------------------------------------------------------------------------\n AssertionError                            Traceback (most recent call last)\n<ipython-input-261-e752e52a96e6> in <module>()\n      2     return (series['lat'], series['long'])\n      3 \n----> 4 sample['lat_long'] = sample.apply(merge_two_cols, axis=1)\n      5\n"", 'AssertionError: Block shape incompatible with manager \n']";"[""<class 'pandas.core.frame.DataFrame'>\nInt64Index: 205482 entries, 0 to 209018\nData columns:\nMonth           205482  non-null values\nReported by     205482  non-null values\nFalls within    205482  non-null values\nEasting         205482  non-null values\nNorthing        205482  non-null values\nLocation        205482  non-null values\nCrime type      205482  non-null values\nlong            205482  non-null values\nlat             205482  non-null values\ndtypes: float64(4), object(5)\n"", ""def merge_two_cols(series): \n    return (series['lat'], series['long'])\n\nsample['lat_long'] = sample.apply(merge_two_cols, axis=1)\n"", ""---------------------------------------------------------------------------\n AssertionError                            Traceback (most recent call last)\n<ipython-input-261-e752e52a96e6> in <module>()\n      2     return (series['lat'], series['long'])\n      3 \n----> 4 sample['lat_long'] = sample.apply(merge_two_cols, axis=1)\n      5\n"", 'AssertionError: Block shape incompatible with manager \n']"
171;3.0;1;16074392;;1;38;<python><matplotlib><pandas>;Getting vertical gridlines to appear in line plot in matplotlib;"<p>I want to get both horizontal and vertical grid lines on my plot but only the horizontal grid lines are appearing by default. I am using a <code>pandas.DataFrame</code> from an sql query in python to generate a line plot with dates on the x-axis. I'm not sure why they do not appear on the dates and I have tried to search for an answer to this but couldn't find one.</p>

<p>All I have used to plot the graph is the simple code below. </p>

<pre><code>data.plot()
grid('on')
</code></pre>

<p>data is the DataFrame which contains the dates and the data from the sql query. </p>

<p>I have also tried adding the code below but I still get the same output with no vertical grid lines.</p>

<pre><code>ax = plt.axes()        
ax.yaxis.grid() # horizontal lines
ax.xaxis.grid() # vertical lines
</code></pre>

<p>Any suggestions?</p>

<p><img src=""https://i.stack.imgur.com/nsdzO.png"" alt=""enter image description here""></p>
";50935.0;"[""data.plot()\ngrid('on')\n"", 'ax = plt.axes()        \nax.yaxis.grid() # horizontal lines\nax.xaxis.grid() # vertical lines\n']";"['pandas.DataFrame', ""data.plot()\ngrid('on')\n"", 'ax = plt.axes()        \nax.yaxis.grid() # horizontal lines\nax.xaxis.grid() # vertical lines\n']"
172;2.0;0;16088741;;1;22;<pandas><multi-index>;Pandas: add a column to a multiindex column dataframe;"<p>I would like to add a column to the second level of a multiindex column dataframe.</p>

<pre><code>In [151]: df
Out[151]: 
first        bar                 baz           
second       one       two       one       two 
A       0.487880 -0.487661 -1.030176  0.100813 
B       0.267913  1.918923  0.132791  0.178503
C       1.550526 -0.312235 -1.177689 -0.081596 
</code></pre>

<p>The usual trick of direct assignment does not work:</p>

<pre><code>In [152]: df['bar']['three'] = [0, 1, 2]

In [153]: df
Out[153]: 
first        bar                 baz           
second       one       two       one       two 
A       0.487880 -0.487661 -1.030176  0.100813
B       0.267913  1.918923  0.132791  0.178503
C       1.550526 -0.312235 -1.177689 -0.081596
</code></pre>

<p>How can I add the third row to under ""bar""?</p>
";10377.0;"['In [151]: df\nOut[151]: \nfirst        bar                 baz           \nsecond       one       two       one       two \nA       0.487880 -0.487661 -1.030176  0.100813 \nB       0.267913  1.918923  0.132791  0.178503\nC       1.550526 -0.312235 -1.177689 -0.081596 \n', ""In [152]: df['bar']['three'] = [0, 1, 2]\n\nIn [153]: df\nOut[153]: \nfirst        bar                 baz           \nsecond       one       two       one       two \nA       0.487880 -0.487661 -1.030176  0.100813\nB       0.267913  1.918923  0.132791  0.178503\nC       1.550526 -0.312235 -1.177689 -0.081596\n""]";"['In [151]: df\nOut[151]: \nfirst        bar                 baz           \nsecond       one       two       one       two \nA       0.487880 -0.487661 -1.030176  0.100813 \nB       0.267913  1.918923  0.132791  0.178503\nC       1.550526 -0.312235 -1.177689 -0.081596 \n', ""In [152]: df['bar']['three'] = [0, 1, 2]\n\nIn [153]: df\nOut[153]: \nfirst        bar                 baz           \nsecond       one       two       one       two \nA       0.487880 -0.487661 -1.030176  0.100813\nB       0.267913  1.918923  0.132791  0.178503\nC       1.550526 -0.312235 -1.177689 -0.081596\n""]"
173;5.0;1;16096627;;1;166;<pandas>;Pandas select row of data frame by integer index;"<p>I am curious as to why <code>df[2]</code> is not supported, while <code>df.ix[2]</code> and <code>df[2:3]</code> both work. </p>

<pre><code>In [26]: df.ix[2]
Out[26]: 
A    1.027680
B    1.514210
C   -1.466963
D   -0.162339
Name: 2000-01-03 00:00:00

In [27]: df[2:3]
Out[27]: 
                  A        B         C         D
2000-01-03  1.02768  1.51421 -1.466963 -0.162339
</code></pre>

<p>I would expect <code>df[2]</code> to work the same way as <code>df[2:3]</code> to be consistent with Python indexing convention. Is there a design reason for not supporting indexing row by single integer?</p>
";227892.0;['In [26]: df.ix[2]\nOut[26]: \nA    1.027680\nB    1.514210\nC   -1.466963\nD   -0.162339\nName: 2000-01-03 00:00:00\n\nIn [27]: df[2:3]\nOut[27]: \n                  A        B         C         D\n2000-01-03  1.02768  1.51421 -1.466963 -0.162339\n'];['df[2]', 'df.ix[2]', 'df[2:3]', 'In [26]: df.ix[2]\nOut[26]: \nA    1.027680\nB    1.514210\nC   -1.466963\nD   -0.162339\nName: 2000-01-03 00:00:00\n\nIn [27]: df[2:3]\nOut[27]: \n                  A        B         C         D\n2000-01-03  1.02768  1.51421 -1.466963 -0.162339\n', 'df[2]', 'df[2:3]']
174;3.0;0;16175874;;1;29;<python><dataframe><pandas>;python pandas dataframe slicing by date conditions;"<p>I am able to read and slice pandas dataframe using python datetime objects, however I am forced to use only <em>existing dates</em> in index.  For example, this works:</p>

<pre><code>&gt;&gt;&gt; data
&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 252 entries, 2010-12-31 00:00:00 to 2010-04-01 00:00:00
Data columns:
Adj Close    252  non-null values
dtypes: float64(1)

&gt;&gt;&gt; st = datetime.datetime(2010, 12, 31, 0, 0)
&gt;&gt;&gt; en = datetime.datetime(2010, 12, 28, 0, 0)

&gt;&gt;&gt; data[st:en]
            Adj Close
Date                 
2010-12-31     593.97
2010-12-30     598.86
2010-12-29     601.00
2010-12-28     598.92
</code></pre>

<p>However if I use a start or end date that is not present in the DF, I get python KeyError.</p>

<p>My Question : How do I query the dataframe object for a date range; even when the start and end dates are not present in the DataFrame.  Does pandas allow for range based slicing?</p>

<p>I am using pandas version 0.10.1</p>
";43773.0;"["">>> data\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 252 entries, 2010-12-31 00:00:00 to 2010-04-01 00:00:00\nData columns:\nAdj Close    252  non-null values\ndtypes: float64(1)\n\n>>> st = datetime.datetime(2010, 12, 31, 0, 0)\n>>> en = datetime.datetime(2010, 12, 28, 0, 0)\n\n>>> data[st:en]\n            Adj Close\nDate                 \n2010-12-31     593.97\n2010-12-30     598.86\n2010-12-29     601.00\n2010-12-28     598.92\n""]";"["">>> data\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 252 entries, 2010-12-31 00:00:00 to 2010-04-01 00:00:00\nData columns:\nAdj Close    252  non-null values\ndtypes: float64(1)\n\n>>> st = datetime.datetime(2010, 12, 31, 0, 0)\n>>> en = datetime.datetime(2010, 12, 28, 0, 0)\n\n>>> data[st:en]\n            Adj Close\nDate                 \n2010-12-31     593.97\n2010-12-30     598.86\n2010-12-29     601.00\n2010-12-28     598.92\n""]"
175;4.0;6;16176996;;1;33;<python><datetime><pandas>;Keep only date part when using pandas.to_datetime;"<p>I use <code>pandas.to_datetime</code> to parse the dates in my data. Pandas by default represents the dates with <code>datetime64[ns]</code> even though the dates are all daily only.
I wonder whether there is an elegant/clever way to convert the dates to <code>datetime.date</code> or <code>datetime64[D]</code> so that, when I write the data to CSV, the dates are not appended with <code>00:00:00</code>. I know I can convert the type manually element-by-element:</p>

<pre><code>[dt.to_datetime().date() for dt in df.dates]
</code></pre>

<p>But this is really slow since I have many rows and it sort of defeats the purpose of using <code>pandas.to_datetime</code>. Is there a way to convert the <code>dtype</code> of the entire column at once? Or alternatively, does <code>pandas.to_datetime</code> support a precision specification so that I can get rid of the time part while working with daily data?</p>
";31231.0;['[dt.to_datetime().date() for dt in df.dates]\n'];['pandas.to_datetime', 'datetime64[ns]', 'datetime.date', 'datetime64[D]', '00:00:00', '[dt.to_datetime().date() for dt in df.dates]\n', 'pandas.to_datetime', 'dtype', 'pandas.to_datetime']
176;5.0;4;16236684;;1;75;<merge><pandas><multiple-columns><return-type>;Apply pandas function to column to create multiple new columns?;"<p>How to do this in pandas:</p>

<p>I have a function <code>extract_text_features</code> on a single text column, returning multiple output columns. Specifically, the function returns 6 values.</p>

<p>The function works, however there doesn't seem to be any proper return type (pandas DataFrame/ numpy array/ Python list) such that the output can get correctly assigned <code>df.ix[: ,10:16] = df.textcol.map(extract_text_features)</code></p>

<p>So I think I need to drop back to iterating with <code>df.iterrows()</code>, as per <a href=""https://stackoverflow.com/questions/7837722/what-is-the-most-efficient-way-to-loop-through-dataframes-with-pandas"">this</a>?</p>

<p>UPDATE: 
Iterating with <code>df.iterrows()</code> is at least 20x slower, so I surrendered and split out the function into six distinct <code>.map(lambda ...)</code> calls.</p>
";33681.0;[];['extract_text_features', 'df.ix[: ,10:16] = df.textcol.map(extract_text_features)', 'df.iterrows()', 'df.iterrows()', '.map(lambda ...)']
177;8.0;1;16249736;;1;38;<python><mongodb><pandas><pymongo>;How to import data from mongodb to pandas?;"<p>I have a large amount of data in a collection in mongodb which I need to analyze. How do i import that data to pandas?</p>

<p>I am new to pandas and numpy.</p>

<p>EDIT:
The mongodb collection contains sensor values tagged with date and time. The sensor values are of float datatype. </p>

<p>Sample Data:</p>

<pre><code>{
""_cls"" : ""SensorReport"",
""_id"" : ObjectId(""515a963b78f6a035d9fa531b""),
""_types"" : [
    ""SensorReport""
],
""Readings"" : [
    {
        ""a"" : 0.958069536790466,
        ""_types"" : [
            ""Reading""
        ],
        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:26:35.297Z""),
        ""b"" : 6.296118156595,
        ""_cls"" : ""Reading""
    },
    {
        ""a"" : 0.95574014778624,
        ""_types"" : [
            ""Reading""
        ],
        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:27:09.963Z""),
        ""b"" : 6.29651468650064,
        ""_cls"" : ""Reading""
    },
    {
        ""a"" : 0.953648289182713,
        ""_types"" : [
            ""Reading""
        ],
        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:27:37.545Z""),
        ""b"" : 7.29679823731148,
        ""_cls"" : ""Reading""
    },
    {
        ""a"" : 0.955931884300997,
        ""_types"" : [
            ""Reading""
        ],
        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:28:21.369Z""),
        ""b"" : 6.29642922525632,
        ""_cls"" : ""Reading""
    },
    {
        ""a"" : 0.95821381,
        ""_types"" : [
            ""Reading""
        ],
        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:41:20.801Z""),
        ""b"" : 7.28956613,
        ""_cls"" : ""Reading""
    },
    {
        ""a"" : 4.95821335,
        ""_types"" : [
            ""Reading""
        ],
        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:41:36.931Z""),
        ""b"" : 6.28956574,
        ""_cls"" : ""Reading""
    },
    {
        ""a"" : 9.95821341,
        ""_types"" : [
            ""Reading""
        ],
        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:42:09.971Z""),
        ""b"" : 0.28956488,
        ""_cls"" : ""Reading""
    },
    {
        ""a"" : 1.95667927,
        ""_types"" : [
            ""Reading""
        ],
        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:43:55.463Z""),
        ""b"" : 0.29115237,
        ""_cls"" : ""Reading""
    }
],
""latestReportTime"" : ISODate(""2013-04-02T08:43:55.463Z""),
""sensorName"" : ""56847890-0"",
""reportCount"" : 8
}
</code></pre>
";24496.0;"['{\n""_cls"" : ""SensorReport"",\n""_id"" : ObjectId(""515a963b78f6a035d9fa531b""),\n""_types"" : [\n    ""SensorReport""\n],\n""Readings"" : [\n    {\n        ""a"" : 0.958069536790466,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:26:35.297Z""),\n        ""b"" : 6.296118156595,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.95574014778624,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:27:09.963Z""),\n        ""b"" : 6.29651468650064,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.953648289182713,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:27:37.545Z""),\n        ""b"" : 7.29679823731148,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.955931884300997,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:28:21.369Z""),\n        ""b"" : 6.29642922525632,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.95821381,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:41:20.801Z""),\n        ""b"" : 7.28956613,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 4.95821335,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:41:36.931Z""),\n        ""b"" : 6.28956574,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 9.95821341,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:42:09.971Z""),\n        ""b"" : 0.28956488,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 1.95667927,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:43:55.463Z""),\n        ""b"" : 0.29115237,\n        ""_cls"" : ""Reading""\n    }\n],\n""latestReportTime"" : ISODate(""2013-04-02T08:43:55.463Z""),\n""sensorName"" : ""56847890-0"",\n""reportCount"" : 8\n}\n']";"['{\n""_cls"" : ""SensorReport"",\n""_id"" : ObjectId(""515a963b78f6a035d9fa531b""),\n""_types"" : [\n    ""SensorReport""\n],\n""Readings"" : [\n    {\n        ""a"" : 0.958069536790466,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:26:35.297Z""),\n        ""b"" : 6.296118156595,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.95574014778624,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:27:09.963Z""),\n        ""b"" : 6.29651468650064,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.953648289182713,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:27:37.545Z""),\n        ""b"" : 7.29679823731148,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.955931884300997,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:28:21.369Z""),\n        ""b"" : 6.29642922525632,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.95821381,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:41:20.801Z""),\n        ""b"" : 7.28956613,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 4.95821335,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:41:36.931Z""),\n        ""b"" : 6.28956574,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 9.95821341,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:42:09.971Z""),\n        ""b"" : 0.28956488,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 1.95667927,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:43:55.463Z""),\n        ""b"" : 0.29115237,\n        ""_cls"" : ""Reading""\n    }\n],\n""latestReportTime"" : ISODate(""2013-04-02T08:43:55.463Z""),\n""sensorName"" : ""56847890-0"",\n""reportCount"" : 8\n}\n']"
178;3.0;2;16266019;;1;24;<python><date><pandas>;Python Pandas: Group datetime column into hour and minute aggregations;"<p>This seems like it would be fairly straight forward but after nearly an entire day I have not found the solution. I've loaded my dataframe with read_csv and easily parsed, combined and indexed a date and a time column into one column but now I want to be able to just reshape and perform calculations based on hour and minute groupings similar to what you can do in excel pivot. </p>

<p>I know how to resample to hour or minute but it maintains the date portion associated with each hour/minute whereas I want to aggregate the data set ONLY to hour and minute similar to grouping in excel pivots and selecting ""hour"" and ""minute"" but not selecting anything else. </p>

<p>Any help would be greatly appreciated. </p>
";20761.0;[];[]
179;5.0;1;16327055;;1;73;<python><pandas>;How to add an empty column to a dataframe?;"<p>What's the easiest way to add an empty column to a pandas <code>DataFrame</code> object?  The best I've stumbled upon is something like</p>

<pre><code>df['foo'] = df.apply(lambda _: '', axis=1)
</code></pre>

<p>Is there a less perverse method?</p>
";65645.0;"[""df['foo'] = df.apply(lambda _: '', axis=1)\n""]";"['DataFrame', ""df['foo'] = df.apply(lambda _: '', axis=1)\n""]"
180;2.0;0;16353729;;1;84;<python><python-2.7><pandas><dataframe><apply>;Pandas: How to use apply function to multiple columns;"<p>I have some problems with the Pandas apply function, when using multiple columns with the following dataframe</p>

<pre><code>df = DataFrame ({'a' : np.random.randn(6),
             'b' : ['foo', 'bar'] * 3,
             'c' : np.random.randn(6)})
</code></pre>

<p>and the following function</p>

<pre><code>def my_test(a, b):
    return a % b
</code></pre>

<p>When I try to apply this function with :</p>

<pre><code>df['Value'] = df.apply(lambda row: my_test(row[a], row[c]), axis=1)
</code></pre>

<p>I get the error message:</p>

<pre><code>NameError: (""global name 'a' is not defined"", u'occurred at index 0')
</code></pre>

<p>I do not understand this message, I defined the name properly. </p>

<p>I would highly appreciate any help on this issue</p>

<p>Update</p>

<p>Thanks for your help. I made indeed some syntax mistakes with the code, the index should be put ''. However I have still the same issue using a more complex function such as:</p>

<pre><code>def my_test(a):
    cum_diff = 0
    for ix in df.index():
        cum_diff = cum_diff + (a - df['a'][ix])
    return cum_diff 
</code></pre>

<p>Thank you</p>
";136478.0;"[""df = DataFrame ({'a' : np.random.randn(6),\n             'b' : ['foo', 'bar'] * 3,\n             'c' : np.random.randn(6)})\n"", 'def my_test(a, b):\n    return a % b\n', ""df['Value'] = df.apply(lambda row: my_test(row[a], row[c]), axis=1)\n"", 'NameError: (""global name \'a\' is not defined"", u\'occurred at index 0\')\n', ""def my_test(a):\n    cum_diff = 0\n    for ix in df.index():\n        cum_diff = cum_diff + (a - df['a'][ix])\n    return cum_diff \n""]";"[""df = DataFrame ({'a' : np.random.randn(6),\n             'b' : ['foo', 'bar'] * 3,\n             'c' : np.random.randn(6)})\n"", 'def my_test(a, b):\n    return a % b\n', ""df['Value'] = df.apply(lambda row: my_test(row[a], row[c]), axis=1)\n"", 'NameError: (""global name \'a\' is not defined"", u\'occurred at index 0\')\n', ""def my_test(a):\n    cum_diff = 0\n    for ix in df.index():\n        cum_diff = cum_diff + (a - df['a'][ix])\n    return cum_diff \n""]"
181;4.0;0;16392921;;1;47;<python><pandas><ipython><ipython-notebook>;Make more than one chart in same IPython Notebook cell;"<p>I have started my IPython Notebook with </p>

<pre><code>ipython notebook --pylab inline
</code></pre>

<p>This is my code in one cell</p>

<pre><code>df['korisnika'].plot()
df['osiguranika'].plot()
</code></pre>

<p>This is working fine, it will draw two lines, but on the same chart.</p>

<p>I would like to draw each line on a separate chart.
And it would be great if the charts would be next to each other, not one after the other.</p>

<p>I know that I can put the second line in the next cell, and then I would get two charts. But I would like the charts close to each other, because they represent the same logical unit.</p>
";38376.0;"['ipython notebook --pylab inline\n', ""df['korisnika'].plot()\ndf['osiguranika'].plot()\n""]";"['ipython notebook --pylab inline\n', ""df['korisnika'].plot()\ndf['osiguranika'].plot()\n""]"
182;5.0;0;16396903;;1;58;<pandas>;Delete the first three rows of a dataframe in pandas;"<p>I need to delete the first three rows of a dataframe in pandas.</p>

<p>I know <code>df.ix[:-1]</code> would remove the last row, but I can't figure out how to remove first n rows.</p>
";65041.0;[];['df.ix[:-1]']
183;3.0;8;16424493;;1;44;<python><formatting><pandas><ipython-notebook>;Pandas: Setting no. of max rows;"<p>I have a problem viewing the following <code>DataFrame</code>: </p>

<pre><code>n = 100
foo = DataFrame(index=range(n))
foo['floats'] = np.random.randn(n)
foo
</code></pre>

<p>The problem is that it does not print all rows per default in ipython notebook, but I have to slice to view the resulting rows. Even the following option does not change the output:</p>

<pre><code>pd.set_option('display.max_rows', 500)
</code></pre>

<p>Does anyone know how to display the whole array?</p>
";28777.0;"[""n = 100\nfoo = DataFrame(index=range(n))\nfoo['floats'] = np.random.randn(n)\nfoo\n"", ""pd.set_option('display.max_rows', 500)\n""]";"['DataFrame', ""n = 100\nfoo = DataFrame(index=range(n))\nfoo['floats'] = np.random.randn(n)\nfoo\n"", ""pd.set_option('display.max_rows', 500)\n""]"
184;8.0;2;16476924;;1;379;<python><pandas><rows><dataframe>;How to iterate over rows in a DataFrame in Pandas?;"<p>I have a DataFrames from pandas:</p>

<pre><code>import pandas as pd
inp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]
df = pd.DataFrame(inp)
print df
</code></pre>

<p>Output:</p>

<pre><code>   c1   c2
0  10  100
1  11  110
2  12  120
</code></pre>

<p>Now I want to iterate over the rows of the above frame. For every row I want to be able to access its elements (values in cells) by the name of the columns. So, for example, I would like to have something like that:</p>

<pre><code>for row in df.rows:
   print row['c1'], row['c2']
</code></pre>

<p>Is it possible to do that in pandas?</p>

<p>I found <a href=""https://stackoverflow.com/questions/7837722/what-is-the-most-efficient-way-to-loop-through-dataframes-with-pandas"">similar question</a>. But it does not give me the answer I need. For example, it is suggested there to use:</p>

<pre><code>for date, row in df.T.iteritems():
</code></pre>

<p>or</p>

<pre><code>for row in df.iterrows():
</code></pre>

<p>But I do not understand what the <code>row</code> object is and how I can work with it.</p>
";384371.0;"[""import pandas as pd\ninp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]\ndf = pd.DataFrame(inp)\nprint df\n"", '   c1   c2\n0  10  100\n1  11  110\n2  12  120\n', ""for row in df.rows:\n   print row['c1'], row['c2']\n"", 'for date, row in df.T.iteritems():\n', 'for row in df.iterrows():\n']";"[""import pandas as pd\ninp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]\ndf = pd.DataFrame(inp)\nprint df\n"", '   c1   c2\n0  10  100\n1  11  110\n2  12  120\n', ""for row in df.rows:\n   print row['c1'], row['c2']\n"", 'for date, row in df.T.iteritems():\n', 'for row in df.iterrows():\n', 'row']"
185;2.0;2;16522380;;1;52;<python><matplotlib><pandas>;Matplotlib plot is a no-show;"<p>When I run this code</p>

<pre><code>import pandas as pd
import numpy as np
def add_prop(group):
    births = group.births.astype(float)
    group['prop'] = births/births.sum()
    return group

pieces = []
columns = ['name', 'sex', 'births']

for year in range(1880, 2012):
    path = 'yob%d.txt' % year
    frame = pd.read_csv(path, names = columns)
    frame['year'] = year
    pieces.append(frame)
    names = pd.concat(pieces, ignore_index = True)

total_births = names.pivot_table('births', rows = 'year', cols = 'sex', aggfunc = sum)
total_births.plot(title = 'Total Births by sex and year')
</code></pre>

<p>I get no plot.  This is from Wes McKinney's book on using Python for data analysis.
Can anyone point me in the right direction?</p>
";30493.0;"[""import pandas as pd\nimport numpy as np\ndef add_prop(group):\n    births = group.births.astype(float)\n    group['prop'] = births/births.sum()\n    return group\n\npieces = []\ncolumns = ['name', 'sex', 'births']\n\nfor year in range(1880, 2012):\n    path = 'yob%d.txt' % year\n    frame = pd.read_csv(path, names = columns)\n    frame['year'] = year\n    pieces.append(frame)\n    names = pd.concat(pieces, ignore_index = True)\n\ntotal_births = names.pivot_table('births', rows = 'year', cols = 'sex', aggfunc = sum)\ntotal_births.plot(title = 'Total Births by sex and year')\n""]";"[""import pandas as pd\nimport numpy as np\ndef add_prop(group):\n    births = group.births.astype(float)\n    group['prop'] = births/births.sum()\n    return group\n\npieces = []\ncolumns = ['name', 'sex', 'births']\n\nfor year in range(1880, 2012):\n    path = 'yob%d.txt' % year\n    frame = pd.read_csv(path, names = columns)\n    frame['year'] = year\n    pieces.append(frame)\n    names = pd.concat(pieces, ignore_index = True)\n\ntotal_births = names.pivot_table('births', rows = 'year', cols = 'sex', aggfunc = sum)\ntotal_births.plot(title = 'Total Births by sex and year')\n""]"
186;3.0;1;16597265;;1;84;<python><pandas>;Appending to an empty data frame in Pandas?;"<p>Is it possible to append to an empty data frame that doesn't contain any indices or columns?</p>

<p>I have tried to do this, but keep getting an empty dataframe at the end.</p>

<p>e.g.</p>

<pre><code>df = pd.DataFrame()
data = ['some kind of data here' --&gt; I have checked the type already, and it is a dataframe]
df.append(data)
</code></pre>

<p>The result looks like this:</p>

<pre><code>Empty DataFrame
Columns: []
Index: []
</code></pre>
";97800.0;"[""df = pd.DataFrame()\ndata = ['some kind of data here' --> I have checked the type already, and it is a dataframe]\ndf.append(data)\n"", 'Empty DataFrame\nColumns: []\nIndex: []\n']";"[""df = pd.DataFrame()\ndata = ['some kind of data here' --> I have checked the type already, and it is a dataframe]\ndf.append(data)\n"", 'Empty DataFrame\nColumns: []\nIndex: []\n']"
187;2.0;2;16628329;;1;39;<python><sqlite><pandas><hdf5>;HDF5 - concurrency, compression & I/O performance;"<p>I have the following questions about HDF5 performance and concurrency:</p>

<ol>
<li>Does HDF5 support concurrent write access? </li>
<li>Concurrency considerations aside, how is HDF5 performance in terms of <strong>I/O performance</strong> (does <strong>compression rates</strong> affect the performance)?</li>
<li>Since I use HDF5 with Python I wonder wow does it's performance compare to Sqlite.</li>
</ol>

<p>References:</p>

<ul>
<li><a href=""http://www.sqlite.org/faq.html#q5"" rel=""nofollow noreferrer"">http://www.sqlite.org/faq.html#q5</a></li>
<li><a href=""https://stackoverflow.com/questions/9907429/locking-sqlite-file-on-nfs-filesystem-possible"">Locking sqlite file on NFS filesystem possible?</a></li>
<li><a href=""http://pandas.pydata.org/"" rel=""nofollow noreferrer"">http://pandas.pydata.org/</a></li>
</ul>
";17032.0;[];[]
188;4.0;4;16628819;;1;22;<python><pandas>;Convert pandas timezone-aware DateTimeIndex to naive timestamp, but in certain timezone;"<p>You can use the function <code>tz_localize</code> to make a Timestamp or DateTimeIndex timezone aware, but how can you do the opposite: how can you convert a timezone aware Timestamp to a naive one, while preserving its timezone?</p>

<p>An example:</p>

<pre><code>In [82]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=10, freq='s', tz=""Europe/Brussels"")

In [83]: t
Out[83]: 
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]
Length: 10, Freq: S, Timezone: Europe/Brussels
</code></pre>

<p>I could remove the timezone by setting it to None, but then the result is converted to UTC (12 o'clock became 10):</p>

<pre><code>In [86]: t.tz = None

In [87]: t
Out[87]: 
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2013-05-18 10:00:00, ..., 2013-05-18 10:00:09]
Length: 10, Freq: S, Timezone: None
</code></pre>

<p>Is there another way I can convert a DateTimeIndex to timezone naive, but while preserving the timezone it was set in?</p>

<hr>

<p>Some <strong>context</strong> on the reason I am asking this: I want to work with timezone naive timeseries (to avoid the extra hassle with timezones, and I do not need them for the case I am working on).<br>
But for some reason, I have to deal with a timezone-aware timeseries in my local timezone (Europe/Brussels). As all my other data are timezone naive (but represented in my local timezone), I want to convert this timeseries to naive to further work with it, but it also has to be represented in my local timezone (so just remove the timezone info, without converting the <em>user-visible</em> time to UTC).  </p>

<p>I know the time is actually internal stored as UTC and only converted to another timezone when you represent it, so there has to be some kind of conversion when I want to ""delocalize"" it. For example, with the python datetime module you can ""remove"" the timezone like this:</p>

<pre><code>In [119]: d = pd.Timestamp(""2013-05-18 12:00:00"", tz=""Europe/Brussels"")

In [120]: d
Out[120]: &lt;Timestamp: 2013-05-18 12:00:00+0200 CEST, tz=Europe/Brussels&gt;

In [121]: d.replace(tzinfo=None)
Out[121]: &lt;Timestamp: 2013-05-18 12:00:00&gt; 
</code></pre>

<p>So, based on this, I could do the following, but I suppose this will not be very efficient when working with a larger timeseries:</p>

<pre><code>In [124]: t
Out[124]: 
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]
Length: 10, Freq: S, Timezone: Europe/Brussels

In [125]: pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])
Out[125]: 
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]
Length: 10, Freq: None, Timezone: None
</code></pre>
";16527.0;"['In [82]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=10, freq=\'s\', tz=""Europe/Brussels"")\n\nIn [83]: t\nOut[83]: \n<class \'pandas.tseries.index.DatetimeIndex\'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: S, Timezone: Europe/Brussels\n', ""In [86]: t.tz = None\n\nIn [87]: t\nOut[87]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-05-18 10:00:00, ..., 2013-05-18 10:00:09]\nLength: 10, Freq: S, Timezone: None\n"", 'In [119]: d = pd.Timestamp(""2013-05-18 12:00:00"", tz=""Europe/Brussels"")\n\nIn [120]: d\nOut[120]: <Timestamp: 2013-05-18 12:00:00+0200 CEST, tz=Europe/Brussels>\n\nIn [121]: d.replace(tzinfo=None)\nOut[121]: <Timestamp: 2013-05-18 12:00:00> \n', ""In [124]: t\nOut[124]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: S, Timezone: Europe/Brussels\n\nIn [125]: pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])\nOut[125]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: None, Timezone: None\n""]";"['tz_localize', 'In [82]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=10, freq=\'s\', tz=""Europe/Brussels"")\n\nIn [83]: t\nOut[83]: \n<class \'pandas.tseries.index.DatetimeIndex\'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: S, Timezone: Europe/Brussels\n', ""In [86]: t.tz = None\n\nIn [87]: t\nOut[87]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-05-18 10:00:00, ..., 2013-05-18 10:00:09]\nLength: 10, Freq: S, Timezone: None\n"", 'In [119]: d = pd.Timestamp(""2013-05-18 12:00:00"", tz=""Europe/Brussels"")\n\nIn [120]: d\nOut[120]: <Timestamp: 2013-05-18 12:00:00+0200 CEST, tz=Europe/Brussels>\n\nIn [121]: d.replace(tzinfo=None)\nOut[121]: <Timestamp: 2013-05-18 12:00:00> \n', ""In [124]: t\nOut[124]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: S, Timezone: Europe/Brussels\n\nIn [125]: pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])\nOut[125]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: None, Timezone: None\n""]"
189;1.0;2;16689514;;1;21;<python><pandas><dataframe>;how to get the average of dataframe column values;"<pre><code>                    A        B
DATE                 
2013-05-01        473077    71333
2013-05-02         35131    62441
2013-05-03           727    27381
2013-05-04           481     1206
2013-05-05           226     1733
2013-05-06           NaN     4064
2013-05-07           NaN    41151
2013-05-08           NaN     8144
2013-05-09           NaN       23
2013-05-10           NaN       10
</code></pre>

<p>say i have the dataframe above.  what is the easiest way to get a series with the same index which is the average of the columns A and B?  the average needs to ignore NaN values. the twist is that this solution needs to be flexible to the addition of new columns to the dataframe.</p>

<p>the closest i have come was </p>

<pre><code>df.sum(axis=1) / len(df.columns)
</code></pre>

<p>however, this does not seem to ignore the NaN values</p>

<p>(note:  i am still a bit new to the pandas library, so i'm guessing there's an obvious way to do this that my limited brain is simply not seeing)</p>
";52385.0;['                    A        B\nDATE                 \n2013-05-01        473077    71333\n2013-05-02         35131    62441\n2013-05-03           727    27381\n2013-05-04           481     1206\n2013-05-05           226     1733\n2013-05-06           NaN     4064\n2013-05-07           NaN    41151\n2013-05-08           NaN     8144\n2013-05-09           NaN       23\n2013-05-10           NaN       10\n', 'df.sum(axis=1) / len(df.columns)\n'];['                    A        B\nDATE                 \n2013-05-01        473077    71333\n2013-05-02         35131    62441\n2013-05-03           727    27381\n2013-05-04           481     1206\n2013-05-05           226     1733\n2013-05-06           NaN     4064\n2013-05-07           NaN    41151\n2013-05-08           NaN     8144\n2013-05-09           NaN       23\n2013-05-10           NaN       10\n', 'df.sum(axis=1) / len(df.columns)\n']
190;4.0;0;16729483;;1;39;<python><pandas>;Converting strings to floats in a DataFrame;"<p>How to covert a DataFrame column containing strings and <code>NaN</code> values to floats. And there is another column whose values are strings and floats; how to convert this entire column to floats. </p>
";88771.0;[];['NaN']
191;5.0;0;16729574;;1;86;<python><pandas><dataframe>;How to get a value from a cell of a data frame?;"<p>I have constructed a condition that extract exactly one row from my data frame:</p>

<pre><code>d2 = df[(df['l_ext']==l_ext) &amp; (df['item']==item) &amp; (df['wn']==wn) &amp; (df['wd']==1)]
</code></pre>

<p>Now I would like to take a value from a particular column:</p>

<pre><code>val = d2['col_name']
</code></pre>

<p>But as a result I get a data frame that contains one row and one column (<em>i.e.</em> one cell). It is not what I need. I need one value (one float number). How can I do it in pandas?</p>
";161920.0;"[""d2 = df[(df['l_ext']==l_ext) & (df['item']==item) & (df['wn']==wn) & (df['wd']==1)]\n"", ""val = d2['col_name']\n""]";"[""d2 = df[(df['l_ext']==l_ext) & (df['item']==item) & (df['wn']==wn) & (df['wd']==1)]\n"", ""val = d2['col_name']\n""]"
192;2.0;6;16740887;;1;46;<python><pandas>;How to handle incoming real time data with python pandas;"<p>Which is the most recommended/pythonic way of handling live incoming data with pandas?</p>

<p>Every few seconds I'm receiving a data point in the format below:</p>

<pre><code>{'time' :'2013-01-01 00:00:00', 'stock' : 'BLAH',
 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}
</code></pre>

<p>I would like to append it to an existing DataFrame and then run some analysis on it.</p>

<p>The problem is, just appending rows with DataFrame.append can lead to performance issues with all that copying.</p>

<h3>Things I've tried:</h3>

<p>A few people suggested preallocating a big DataFrame and updating it as data comes in:</p>

<pre><code>In [1]: index = pd.DatetimeIndex(start='2013-01-01 00:00:00', freq='S', periods=5)

In [2]: columns = ['high', 'low', 'open', 'close']

In [3]: df = pd.DataFrame(index=t, columns=columns)

In [4]: df
Out[4]: 
                    high  low open close
2013-01-01 00:00:00  NaN  NaN  NaN   NaN
2013-01-01 00:00:01  NaN  NaN  NaN   NaN
2013-01-01 00:00:02  NaN  NaN  NaN   NaN
2013-01-01 00:00:03  NaN  NaN  NaN   NaN
2013-01-01 00:00:04  NaN  NaN  NaN   NaN

In [5]: data = {'time' :'2013-01-01 00:00:02', 'stock' : 'BLAH', 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}

In [6]: data_ = pd.Series(data)

In [7]: df.loc[data['time']] = data_

In [8]: df
Out[8]: 
                    high  low open close
2013-01-01 00:00:00  NaN  NaN  NaN   NaN
2013-01-01 00:00:01  NaN  NaN  NaN   NaN
2013-01-01 00:00:02    4    3    2     1
2013-01-01 00:00:03  NaN  NaN  NaN   NaN
2013-01-01 00:00:04  NaN  NaN  NaN   NaN
</code></pre>

<p>The other alternative is building a list of dicts. Simply appending the incoming data to a list and slicing it into smaller DataFrames to do the work.</p>

<pre><code>In [9]: ls = []

In [10]: for n in range(5):
   .....:     # Naive stuff ahead =)
   .....:     time = '2013-01-01 00:00:0' + str(n)
   .....:     d = {'time' : time, 'stock' : 'BLAH', 'high' : np.random.rand()*10, 'low' : np.random.rand()*10, 'open' : np.random.rand()*10, 'close' : np.random.rand()*10}
   .....:     ls.append(d)

In [11]: df = pd.DataFrame(ls[1:3]).set_index('time')

In [12]: df
Out[12]: 
                        close      high       low      open stock
time                                                             
2013-01-01 00:00:01  3.270078  1.008289  7.486118  2.180683  BLAH
2013-01-01 00:00:02  3.883586  2.215645  0.051799  2.310823  BLAH
</code></pre>

<p>or something like that, maybe processing the input a little bit more.</p>
";7890.0;"[""{'time' :'2013-01-01 00:00:00', 'stock' : 'BLAH',\n 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}\n"", ""In [1]: index = pd.DatetimeIndex(start='2013-01-01 00:00:00', freq='S', periods=5)\n\nIn [2]: columns = ['high', 'low', 'open', 'close']\n\nIn [3]: df = pd.DataFrame(index=t, columns=columns)\n\nIn [4]: df\nOut[4]: \n                    high  low open close\n2013-01-01 00:00:00  NaN  NaN  NaN   NaN\n2013-01-01 00:00:01  NaN  NaN  NaN   NaN\n2013-01-01 00:00:02  NaN  NaN  NaN   NaN\n2013-01-01 00:00:03  NaN  NaN  NaN   NaN\n2013-01-01 00:00:04  NaN  NaN  NaN   NaN\n\nIn [5]: data = {'time' :'2013-01-01 00:00:02', 'stock' : 'BLAH', 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}\n\nIn [6]: data_ = pd.Series(data)\n\nIn [7]: df.loc[data['time']] = data_\n\nIn [8]: df\nOut[8]: \n                    high  low open close\n2013-01-01 00:00:00  NaN  NaN  NaN   NaN\n2013-01-01 00:00:01  NaN  NaN  NaN   NaN\n2013-01-01 00:00:02    4    3    2     1\n2013-01-01 00:00:03  NaN  NaN  NaN   NaN\n2013-01-01 00:00:04  NaN  NaN  NaN   NaN\n"", ""In [9]: ls = []\n\nIn [10]: for n in range(5):\n   .....:     # Naive stuff ahead =)\n   .....:     time = '2013-01-01 00:00:0' + str(n)\n   .....:     d = {'time' : time, 'stock' : 'BLAH', 'high' : np.random.rand()*10, 'low' : np.random.rand()*10, 'open' : np.random.rand()*10, 'close' : np.random.rand()*10}\n   .....:     ls.append(d)\n\nIn [11]: df = pd.DataFrame(ls[1:3]).set_index('time')\n\nIn [12]: df\nOut[12]: \n                        close      high       low      open stock\ntime                                                             \n2013-01-01 00:00:01  3.270078  1.008289  7.486118  2.180683  BLAH\n2013-01-01 00:00:02  3.883586  2.215645  0.051799  2.310823  BLAH\n""]";"[""{'time' :'2013-01-01 00:00:00', 'stock' : 'BLAH',\n 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}\n"", ""In [1]: index = pd.DatetimeIndex(start='2013-01-01 00:00:00', freq='S', periods=5)\n\nIn [2]: columns = ['high', 'low', 'open', 'close']\n\nIn [3]: df = pd.DataFrame(index=t, columns=columns)\n\nIn [4]: df\nOut[4]: \n                    high  low open close\n2013-01-01 00:00:00  NaN  NaN  NaN   NaN\n2013-01-01 00:00:01  NaN  NaN  NaN   NaN\n2013-01-01 00:00:02  NaN  NaN  NaN   NaN\n2013-01-01 00:00:03  NaN  NaN  NaN   NaN\n2013-01-01 00:00:04  NaN  NaN  NaN   NaN\n\nIn [5]: data = {'time' :'2013-01-01 00:00:02', 'stock' : 'BLAH', 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}\n\nIn [6]: data_ = pd.Series(data)\n\nIn [7]: df.loc[data['time']] = data_\n\nIn [8]: df\nOut[8]: \n                    high  low open close\n2013-01-01 00:00:00  NaN  NaN  NaN   NaN\n2013-01-01 00:00:01  NaN  NaN  NaN   NaN\n2013-01-01 00:00:02    4    3    2     1\n2013-01-01 00:00:03  NaN  NaN  NaN   NaN\n2013-01-01 00:00:04  NaN  NaN  NaN   NaN\n"", ""In [9]: ls = []\n\nIn [10]: for n in range(5):\n   .....:     # Naive stuff ahead =)\n   .....:     time = '2013-01-01 00:00:0' + str(n)\n   .....:     d = {'time' : time, 'stock' : 'BLAH', 'high' : np.random.rand()*10, 'low' : np.random.rand()*10, 'open' : np.random.rand()*10, 'close' : np.random.rand()*10}\n   .....:     ls.append(d)\n\nIn [11]: df = pd.DataFrame(ls[1:3]).set_index('time')\n\nIn [12]: df\nOut[12]: \n                        close      high       low      open stock\ntime                                                             \n2013-01-01 00:00:01  3.270078  1.008289  7.486118  2.180683  BLAH\n2013-01-01 00:00:02  3.883586  2.215645  0.051799  2.310823  BLAH\n""]"
193;2.0;1;16777570;;1;22;<python><dataframe><pandas>;Calculate time difference between Pandas Dataframe indices;"<p>I am trying to add a column of deltaT to a dataframe where deltaT is the time difference between the successive rows (as indexed in the timeseries).</p>

<pre><code>time                 value

2012-03-16 23:50:00      1
2012-03-16 23:56:00      2
2012-03-17 00:08:00      3
2012-03-17 00:10:00      4
2012-03-17 00:12:00      5
2012-03-17 00:20:00      6
2012-03-20 00:43:00      7
</code></pre>

<p>Desired result is something like the following (deltaT units shown in minutes):</p>

<pre><code>time                 value  deltaT

2012-03-16 23:50:00      1       0
2012-03-16 23:56:00      2       6
2012-03-17 00:08:00      3      12
2012-03-17 00:10:00      4       2
2012-03-17 00:12:00      5       2
2012-03-17 00:20:00      6       8
2012-03-20 00:43:00      7      23
</code></pre>
";19835.0;['time                 value\n\n2012-03-16 23:50:00      1\n2012-03-16 23:56:00      2\n2012-03-17 00:08:00      3\n2012-03-17 00:10:00      4\n2012-03-17 00:12:00      5\n2012-03-17 00:20:00      6\n2012-03-20 00:43:00      7\n', 'time                 value  deltaT\n\n2012-03-16 23:50:00      1       0\n2012-03-16 23:56:00      2       6\n2012-03-17 00:08:00      3      12\n2012-03-17 00:10:00      4       2\n2012-03-17 00:12:00      5       2\n2012-03-17 00:20:00      6       8\n2012-03-20 00:43:00      7      23\n'];['time                 value\n\n2012-03-16 23:50:00      1\n2012-03-16 23:56:00      2\n2012-03-17 00:08:00      3\n2012-03-17 00:10:00      4\n2012-03-17 00:12:00      5\n2012-03-17 00:20:00      6\n2012-03-20 00:43:00      7\n', 'time                 value  deltaT\n\n2012-03-16 23:50:00      1       0\n2012-03-16 23:56:00      2       6\n2012-03-17 00:08:00      3      12\n2012-03-17 00:10:00      4       2\n2012-03-17 00:12:00      5       2\n2012-03-17 00:20:00      6       8\n2012-03-20 00:43:00      7      23\n']
194;1.0;1;16782323;;1;33;<python><pandas>;Python pandas: Keep selected column as DataFrame instead of Series;"<p>When selecting a single column from a pandas DataFrame(say <code>df.iloc[:, 0]</code>, <code>df['A']</code>, or <code>df.A</code>, etc), the resulting vector is automatically converted to a Series instead of a single-column DataFrame. However, I am writing some functions that takes a DataFrame as an input argument. Therefore, I prefer to deal with single-column DataFrame instead of Series so that the function can assume say df.columns is accessible. Right now I have to explicitly convert the Series into a DataFrame by using something like <code>pd.DataFrame(df.iloc[:, 0])</code>. This doesn't seem like the most clean method. Is there a more elegant way to index from a DataFrame directly so that the result is a single-column DataFrame instead of Series?</p>
";15889.0;[];"['df.iloc[:, 0]', ""df['A']"", 'df.A', 'pd.DataFrame(df.iloc[:, 0])']"
195;4.0;0;16852911;;1;42;<python><date><pandas>;How do I convert dates in a Pandas data frame to a 'date' data type?;"<p>I have a Pandas data frame, one of the columns of which contains date strings in the format 'YYYY-MM-DD' e.g. '2013-10-28'.</p>

<p>At the moment the dtype of the column is 'object'.</p>

<p>How do I convert the column values to Pandas date format?</p>
";54111.0;[];[]
196;1.0;1;16888888;;1;28;<python><pandas><ipython><ipython-notebook><dataframe>;How to read a .xlsx file using the pandas Library in iPython?;"<p>I want to read a .xlsx file using the Pandas Library of python and port the data to a postgreSQL table. <br/></p>

<p>All I could do up until now is:<br/></p>

<pre><code>import pandas as pd
data = pd.ExcelFile(""*File Name*"")
</code></pre>

<p>
Now I know that the step got executed successfully, but I want to know how i can parse the excel file that has been read so that I can understand how the data in the excel maps to the data in the variable data. <br/>
I learnt that data is a Dataframe object if I'm not wrong. So How do i parse this dataframe object to extract each line row by row.</p>
";25755.0;"['import pandas as pd\ndata = pd.ExcelFile(""*File Name*"")\n']";"['import pandas as pd\ndata = pd.ExcelFile(""*File Name*"")\n']"
197;5.0;0;16923281;;1;214;<python><csv><pandas><dataframe>;Pandas writing dataframe to CSV file;"<p>I have a dataframe in pandas which I would like to write to a CSV file. I am doing this using:</p>

<pre><code>df.to_csv('out.csv')
</code></pre>

<p>And getting the error:</p>

<pre><code>UnicodeEncodeError: 'ascii' codec can't encode character u'\u03b1' in position 20: ordinal not in range(128)
</code></pre>

<p>Is there any way to get around this easily (i.e. I have unicode characters in my data frame)? And is there a way to write to a tab delimited file instead of a CSV using e.g. a 'to-tab' method (that I dont think exists)?</p>
";296632.0;"[""df.to_csv('out.csv')\n"", ""UnicodeEncodeError: 'ascii' codec can't encode character u'\\u03b1' in position 20: ordinal not in range(128)\n""]";"[""df.to_csv('out.csv')\n"", ""UnicodeEncodeError: 'ascii' codec can't encode character u'\\u03b1' in position 20: ordinal not in range(128)\n""]"
198;3.0;0;16947336;;1;31;<python><numpy><pandas>;binning a dataframe in pandas in Python;"<p>given the following dataframe in pandas:</p>

<pre><code>import numpy as np
df = pandas.DataFrame({""a"": np.random.random(100), ""b"": np.random.random(100), ""id"": np.arange(100)})
</code></pre>

<p>where <code>id</code> is an id for each point consisting of an <code>a</code> and <code>b</code> value, how can I bin <code>a</code> and <code>b</code> into a specified set of bins (so that I can then take the median/average value of <code>a</code> and <code>b</code> in each bin)?  <code>df</code> might have <code>NaN</code> values for <code>a</code> or <code>b</code> (or both) for any given row in <code>df</code>. thanks.</p>

<p>Here's a better example using Joe Kington's solution with a more realistic df. The thing I'm unsure about is how to access the df.b elements for each df.a group below:</p>

<pre><code>a = np.random.random(20)
df = pandas.DataFrame({""a"": a, ""b"": a + 10})
# bins for df.a
bins = np.linspace(0, 1, 10)
# bin df according to a
groups = df.groupby(np.digitize(df.a,bins))
# Get the mean of a in each group
print groups.mean()
## But how to get the mean of b for each group of a?
# ...
</code></pre>
";34913.0;"['import numpy as np\ndf = pandas.DataFrame({""a"": np.random.random(100), ""b"": np.random.random(100), ""id"": np.arange(100)})\n', 'a = np.random.random(20)\ndf = pandas.DataFrame({""a"": a, ""b"": a + 10})\n# bins for df.a\nbins = np.linspace(0, 1, 10)\n# bin df according to a\ngroups = df.groupby(np.digitize(df.a,bins))\n# Get the mean of a in each group\nprint groups.mean()\n## But how to get the mean of b for each group of a?\n# ...\n']";"['import numpy as np\ndf = pandas.DataFrame({""a"": np.random.random(100), ""b"": np.random.random(100), ""id"": np.arange(100)})\n', 'id', 'a', 'b', 'a', 'b', 'a', 'b', 'df', 'NaN', 'a', 'b', 'df', 'a = np.random.random(20)\ndf = pandas.DataFrame({""a"": a, ""b"": a + 10})\n# bins for df.a\nbins = np.linspace(0, 1, 10)\n# bin df according to a\ngroups = df.groupby(np.digitize(df.a,bins))\n# Get the mean of a in each group\nprint groups.mean()\n## But how to get the mean of b for each group of a?\n# ...\n']"
199;3.0;2;17001389;;1;113;<python><documentation><pandas>;pandas resample documentation;"<p>So I completely understand how to use <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.resample.html"">resample</a>, but the documentation does not do a good job explaining the options.</p>

<p>So most options in the <code>resample</code> function are pretty straight forward except for these two:</p>

<ul>
<li>rule : the offset string or object representing target conversion</li>
<li>how : string, method for down- or re-sampling, default to mean</li>
</ul>

<p>So from looking at as many examples as I found online I can see for rule you can do <code>'D'</code> for day, <code>'xMin'</code> for minutes, <code>'xL'</code> for milliseconds, but that is all I could find.</p>

<p>for how I have seen the following: <code>'first'</code>, <code>np.max</code>, <code>'last'</code>, <code>'mean'</code>, and <code>'n1n2n3n4...nx'</code> where nx is the first letter of each column index.</p>

<p>So is there somewhere in the documentation that I am missing that displays every option for <code>pandas.resample</code>'s rule and how inputs? If yes, where because I could not find it. If no, <strong>what are all the options for them?</strong></p>
";59119.0;[];"['resample', ""'D'"", ""'xMin'"", ""'xL'"", ""'first'"", 'np.max', ""'last'"", ""'mean'"", ""'n1n2n3n4...nx'"", 'pandas.resample']"
200;4.0;0;17063458;;1;56;<python><python-2.7><pandas>;Reading an Excel file in python using pandas;"<p>I am trying to read an excel file this way :</p>

<pre><code>newFile = pd.ExcelFile(PATH\FileName.xlsx)
ParsedData = pd.io.parsers.ExcelFile.parse(newFile)
</code></pre>

<p>which throws an error that says two arguments expected, I don't know what the second argument is and also what I am trying to achieve here is to convert an Excel file to a DataFrame, Am I doing it the right way? or is there any other way to do this using pandas?</p>
";114839.0;['newFile = pd.ExcelFile(PATH\\FileName.xlsx)\nParsedData = pd.io.parsers.ExcelFile.parse(newFile)\n'];['newFile = pd.ExcelFile(PATH\\FileName.xlsx)\nParsedData = pd.io.parsers.ExcelFile.parse(newFile)\n']
201;9.0;0;17071871;;1;375;<python><pandas><dataframe>;Select rows from a DataFrame based on values in a column in pandas;"<p>How to select rows from a DataFrame based on values in some column in pandas?<br>
In SQL I would use: </p>

<pre><code>select * from table where colume_name = some_value. 
</code></pre>

<p><em>I tried to look at pandas documentation but did not immediately find the answer.</em></p>
";420943.0;['select * from table where colume_name = some_value. \n'];['select * from table where colume_name = some_value. \n']
202;4.0;0;17091769;;1;54;<python><dataframe><row><pandas>;Python pandas: fill a dataframe row by row;"<p>The simple task of adding a row to a <code>pandas.DataFrame</code> object seems to be hard to accomplish. There are 3 stackoverflow questions relating to this, none of which give a working answer.</p>

<p>Here is what I'm trying to do. I have a DataFrame of which I already know the shape as well as the names of the rows and columns.</p>

<pre><code>&gt;&gt;&gt; df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'])
&gt;&gt;&gt; df
     a    b    c    d
x  NaN  NaN  NaN  NaN
y  NaN  NaN  NaN  NaN
z  NaN  NaN  NaN  NaN
</code></pre>

<p>Now, I have a function to compute the values of the rows iteratively. How can I fill in one of the rows with either a dictionary or a <code>pandas.Series</code> ? Here are various attempts that have failed:</p>

<pre><code>&gt;&gt;&gt; y = {'a':1, 'b':5, 'c':2, 'd':3} 
&gt;&gt;&gt; df['y'] = y
AssertionError: Length of values does not match length of index
</code></pre>

<p>Apparently it tried to add a column instead of a row.</p>

<pre><code>&gt;&gt;&gt; y = {'a':1, 'b':5, 'c':2, 'd':3} 
&gt;&gt;&gt; df.join(y)
AttributeError: 'builtin_function_or_method' object has no attribute 'is_unique'
</code></pre>

<p>Very uninformative error message.</p>

<pre><code>&gt;&gt;&gt; y = {'a':1, 'b':5, 'c':2, 'd':3} 
&gt;&gt;&gt; df.set_value(index='y', value=y)
TypeError: set_value() takes exactly 4 arguments (3 given)
</code></pre>

<p>Apparently that is only for setting individual values in the dataframe.</p>

<pre><code>&gt;&gt;&gt; y = {'a':1, 'b':5, 'c':2, 'd':3} 
&gt;&gt;&gt; df.append(y)
Exception: Can only append a Series if ignore_index=True
</code></pre>

<p>Well, I don't want to ignore the index, otherwise here is the result:</p>

<pre><code>&gt;&gt;&gt; df.append(y, ignore_index=True)
     a    b    c    d
0  NaN  NaN  NaN  NaN
1  NaN  NaN  NaN  NaN
2  NaN  NaN  NaN  NaN
3    1    5    2    3
</code></pre>

<p>It did align the column names with the values, but lost the row labels.</p>

<pre><code>&gt;&gt;&gt; y = {'a':1, 'b':5, 'c':2, 'd':3} 
&gt;&gt;&gt; df.ix['y'] = y
&gt;&gt;&gt; df
                                  a                                 b  \
x                               NaN                               NaN
y  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}
z                               NaN                               NaN

                                  c                                 d
x                               NaN                               NaN
y  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}
z                               NaN                               NaN
</code></pre>

<p>That also failed miserably.</p>

<p>So how do you do it ?</p>
";53561.0;"["">>> df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'])\n>>> df\n     a    b    c    d\nx  NaN  NaN  NaN  NaN\ny  NaN  NaN  NaN  NaN\nz  NaN  NaN  NaN  NaN\n"", "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df['y'] = y\nAssertionError: Length of values does not match length of index\n"", "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.join(y)\nAttributeError: 'builtin_function_or_method' object has no attribute 'is_unique'\n"", "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.set_value(index='y', value=y)\nTypeError: set_value() takes exactly 4 arguments (3 given)\n"", "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.append(y)\nException: Can only append a Series if ignore_index=True\n"", '>>> df.append(y, ignore_index=True)\n     a    b    c    d\n0  NaN  NaN  NaN  NaN\n1  NaN  NaN  NaN  NaN\n2  NaN  NaN  NaN  NaN\n3    1    5    2    3\n', "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.ix['y'] = y\n>>> df\n                                  a                                 b  \\\nx                               NaN                               NaN\ny  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}\nz                               NaN                               NaN\n\n                                  c                                 d\nx                               NaN                               NaN\ny  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}\nz                               NaN                               NaN\n""]";"['pandas.DataFrame', "">>> df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'])\n>>> df\n     a    b    c    d\nx  NaN  NaN  NaN  NaN\ny  NaN  NaN  NaN  NaN\nz  NaN  NaN  NaN  NaN\n"", 'pandas.Series', "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df['y'] = y\nAssertionError: Length of values does not match length of index\n"", "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.join(y)\nAttributeError: 'builtin_function_or_method' object has no attribute 'is_unique'\n"", "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.set_value(index='y', value=y)\nTypeError: set_value() takes exactly 4 arguments (3 given)\n"", "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.append(y)\nException: Can only append a Series if ignore_index=True\n"", '>>> df.append(y, ignore_index=True)\n     a    b    c    d\n0  NaN  NaN  NaN  NaN\n1  NaN  NaN  NaN  NaN\n2  NaN  NaN  NaN  NaN\n3    1    5    2    3\n', "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.ix['y'] = y\n>>> df\n                                  a                                 b  \\\nx                               NaN                               NaN\ny  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}\nz                               NaN                               NaN\n\n                                  c                                 d\nx                               NaN                               NaN\ny  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}\nz                               NaN                               NaN\n""]"
203;8.0;1;17095101;;1;48;<python><html><dataframe><pandas>;Outputting difference in two Pandas dataframes side by side - highlighting the difference;"<p>I am trying to highlight exactly what changed between two dataframes.</p>

<p>Suppose I have two Python Pandas dataframes:</p>

<pre><code>""StudentRoster Jan-1"":
id   Name   score                    isEnrolled           Comment
111  Jack   2.17                     True                 He was late to class
112  Nick   1.11                     False                Graduated
113  Zoe    4.12                     True       

""StudentRoster Jan-2"":
id   Name   score                    isEnrolled           Comment
111  Jack   2.17                     True                 He was late to class
112  Nick   1.21                     False                Graduated
113  Zoe    4.12                     False                On vacation
</code></pre>

<p>My goal is to output an HTML table that:</p>

<ol>
<li>Identifies rows that have changed (could be int, float, boolean, string)</li>
<li><p>Outputs rows with same, OLD and NEW values (ideally into an HTML table) so the consumer can clearly see what changed between two dataframes: </p>

<pre><code>""StudentRoster Difference Jan-1 - Jan-2"":  
id   Name   score                    isEnrolled           Comment
112  Nick   was 1.11| now 1.21       False                Graduated
113  Zoe    4.12                     was True | now False was """" | now   ""On   vacation""
</code></pre></li>
</ol>

<p>I suppose I could do a row by row and column by column comparison, but is there an easier way?</p>
";51910.0;"['""StudentRoster Jan-1"":\nid   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.11                     False                Graduated\n113  Zoe    4.12                     True       \n\n""StudentRoster Jan-2"":\nid   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.21                     False                Graduated\n113  Zoe    4.12                     False                On vacation\n', '""StudentRoster Difference Jan-1 - Jan-2"":  \nid   Name   score                    isEnrolled           Comment\n112  Nick   was 1.11| now 1.21       False                Graduated\n113  Zoe    4.12                     was True | now False was """" | now   ""On   vacation""\n']";"['""StudentRoster Jan-1"":\nid   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.11                     False                Graduated\n113  Zoe    4.12                     True       \n\n""StudentRoster Jan-2"":\nid   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.21                     False                Graduated\n113  Zoe    4.12                     False                On vacation\n', '""StudentRoster Difference Jan-1 - Jan-2"":  \nid   Name   score                    isEnrolled           Comment\n112  Nick   was 1.11| now 1.21       False                Graduated\n113  Zoe    4.12                     was True | now False was """" | now   ""On   vacation""\n']"
204;2.0;5;17097236;;1;29;<python><replace><pandas><nan><nonetype>;How to replace values with None in Pandas data frame in Python?;"<p>Is there any method to replace values with <code>None</code> in Pandas in Python?</p>

<p>You can use <code>df.replace('pre', 'post')</code> and can replace a value with another, but this can't be done if you want to replace with <code>None</code> value, which if you try, you get a strange result.</p>

<p>So here's an example:</p>

<pre><code>df = DataFrame(['-',3,2,5,1,-5,-1,'-',9])
df.replace('-', 0)
</code></pre>

<p>which returns a successful result.</p>

<p>But,</p>

<pre><code>df.replace('-', None)
</code></pre>

<p>which returns a following result:</p>

<pre><code>0
0   - // this isn't replaced
1   3
2   2
3   5
4   1
5  -5
6  -1
7  -1 // this is changed to `-1`...
8   9
</code></pre>

<p>Why does such a strange result be returned?</p>

<p>Since I want to pour this data frame into MySQL database, I can't put <code>NaN</code> values into any element in my data frame and instead want to put <code>None</code>. Surely, you can first change <code>'-'</code> to <code>NaN</code> and then convert <code>NaN</code> to <code>None</code>, but I want to know why the dataframe acts in such a terrible way.</p>
";53211.0;"[""df = DataFrame(['-',3,2,5,1,-5,-1,'-',9])\ndf.replace('-', 0)\n"", ""df.replace('-', None)\n"", ""0\n0   - // this isn't replaced\n1   3\n2   2\n3   5\n4   1\n5  -5\n6  -1\n7  -1 // this is changed to `-1`...\n8   9\n""]";"['None', ""df.replace('pre', 'post')"", 'None', ""df = DataFrame(['-',3,2,5,1,-5,-1,'-',9])\ndf.replace('-', 0)\n"", ""df.replace('-', None)\n"", ""0\n0   - // this isn't replaced\n1   3\n2   2\n3   5\n4   1\n5  -5\n6  -1\n7  -1 // this is changed to `-1`...\n8   9\n"", 'NaN', 'None', ""'-'"", 'NaN', 'NaN', 'None']"
205;3.0;0;17097643;;1;23;<python><pandas><contains>;"search for ""does-not-contain"" on a dataframe in pandas";"<p>I've done some searching and can't figure out how to filter a dataframe by <code>df[""col""].str.contains(word)</code>, however I'm wondering if there is a way to do the reverse: filter a dataframe by that set's compliment. eg: to the effect of <code>!(df[""col""].str.contains(word))</code>. </p>

<p>Can this be done through a <code>DataFrame</code> method?</p>
";13033.0;[];"['df[""col""].str.contains(word)', '!(df[""col""].str.contains(word))', 'DataFrame']"
206;5.0;0;17098654;;1;117;<python><pandas><dataframe>;How to store a dataframe using Pandas;"<p>Right now I'm importing a fairly large <code>CSV</code> as a dataframe every time I run the script. Is there a good solution for keeping that dataframe constantly available in between runs so I don't have to spend all that time waiting for the script to run?</p>
";78448.0;[];['CSV']
207;3.0;0;17116814;;1;95;<python><pandas><dataframe>;pandas: How do I split text in a column into multiple rows?;"<p>I'm working with a large csv file and the next to last column has a string of text that I want to split by a specific delimiter. I was wondering if there is a simple way to do this using pandas or python?</p>

<pre><code>CustNum  CustomerName     ItemQty  Item   Seatblocks                 ItemExt
32363    McCartney, Paul      3     F04    2:218:10:4,6                   60
31316    Lennon, John        25     F01    1:13:36:1,12 1:13:37:1,13     300
</code></pre>

<p>I want to split by the space<code>(' ')</code> and then the colon<code>(':')</code> in the <code>Seatblocks</code> column, but each cell would result in a different number of columns. I have a function to rearrange the columns so the <code>Seatblocks</code> column is at the end of the sheet, but I'm not sure what to do from there. I can do it in excel with the built in <code>text-to-columns</code> function and a quick macro, but my dataset has too many records for excel to handle.</p>

<p>Ultimately, I want to take records such John Lennon's and create multiple lines, with the info from each set of seats on a separate line.</p>
";70528.0;['CustNum  CustomerName     ItemQty  Item   Seatblocks                 ItemExt\n32363    McCartney, Paul      3     F04    2:218:10:4,6                   60\n31316    Lennon, John        25     F01    1:13:36:1,12 1:13:37:1,13     300\n'];"['CustNum  CustomerName     ItemQty  Item   Seatblocks                 ItemExt\n32363    McCartney, Paul      3     F04    2:218:10:4,6                   60\n31316    Lennon, John        25     F01    1:13:36:1,12 1:13:37:1,13     300\n', ""(' ')"", ""(':')"", 'Seatblocks', 'Seatblocks', 'text-to-columns']"
208;2.0;0;17134716;;1;92;<python><pandas><dataframe>;Convert DataFrame column type from string to datetime;"<p>How can I convert a DataFrame column of strings (in dd/mm/yyyy format) to datetimes?</p>
";85223.0;[];[]
209;2.0;1;17141558;;1;65;<python><python-2.7><pandas><data-analysis>;How to sort a dataFrame in python pandas by two or more columns?;"<p>Suppose I have a data-Frame with columns a b &amp; c, I want to sort the data-Frame by column b in ascending, and by column c in descending, how do I do this?</p>
";73105.0;[];[]
210;2.0;0;17142304;;1;33;<python><replace><dataframe><pandas>;replace string/value in entire dataframe;"<p>I have a very large dataset were I want to replace strings with numbers. I would like to operate on the dataset without typing a mapping function for each key (column) in the dataset. (similar to the fillna method, but replace specific string with assosiated value).
Is there anyway to do this?</p>

<p>Here is an example of my dataset</p>

<pre><code>data
   resp          A          B          C
0     1       poor       poor       good
1     2       good       poor       good
2     3  very good  very good  very good
3     4       bad        poor       bad 
4     5   very bad   very bad   very bad
5     6       poor       good   very bad
6     7       good       good       good
7     8  very good  very good  very good
8     9       bad        bad    very bad
9    10   very bad   very bad   very bad
</code></pre>

<p>The desired result:</p>

<pre><code> data
   resp  A  B  C
0      1  3  3  4
1     2  4  3  4
2     3  5  5  5
3     4  2  3  2
4     5  1  1  1
5     6  3  4  1
6     7  4  4  4
7     8  5  5  5
8     9  2  2  1
9    10  1  1  1
</code></pre>

<p>very bad=1, bad=2, poor=3, good=4, very good=5</p>

<p>//Jonas</p>
";35525.0;['data\n   resp          A          B          C\n0     1       poor       poor       good\n1     2       good       poor       good\n2     3  very good  very good  very good\n3     4       bad        poor       bad \n4     5   very bad   very bad   very bad\n5     6       poor       good   very bad\n6     7       good       good       good\n7     8  very good  very good  very good\n8     9       bad        bad    very bad\n9    10   very bad   very bad   very bad\n', ' data\n   resp  A  B  C\n0      1  3  3  4\n1     2  4  3  4\n2     3  5  5  5\n3     4  2  3  2\n4     5  1  1  1\n5     6  3  4  1\n6     7  4  4  4\n7     8  5  5  5\n8     9  2  2  1\n9    10  1  1  1\n'];['data\n   resp          A          B          C\n0     1       poor       poor       good\n1     2       good       poor       good\n2     3  very good  very good  very good\n3     4       bad        poor       bad \n4     5   very bad   very bad   very bad\n5     6       poor       good   very bad\n6     7       good       good       good\n7     8  very good  very good  very good\n8     9       bad        bad    very bad\n9    10   very bad   very bad   very bad\n', ' data\n   resp  A  B  C\n0      1  3  3  4\n1     2  4  3  4\n2     3  5  5  5\n3     4  2  3  2\n4     5  1  1  1\n5     6  3  4  1\n6     7  4  4  4\n7     8  5  5  5\n8     9  2  2  1\n9    10  1  1  1\n']
211;4.0;0;17241004;;1;105;<python><pandas>;Pandas - how to get the data frame index as an array;"<p>Do you know how to get the index column of an dataframe as an array? I have a list of accession numbers in the ""Accession"" column of a CSV file, which I imported into Pandas, and during the import, I set the index to the ""Accession"" column. Now, I need the ""Accession"" column to be a set of labels in a later step, but I don't know how to extract it standalone.</p>
";158874.0;[];[]
212;4.0;4;17326973;;1;21;<python><excel><pandas><openpyxl>;Is there a way to auto-adjust Excel column widths with pandas.ExcelWriter?;"<p>I am being asked to generate some Excel reports. I am currently using pandas quite heavily for my data, so naturally I would like to use the pandas.ExcelWriter method to generate these reports.  However the fixed column widths are a problem.   </p>

<p>The code I have so far is simple enough.  Say I have a dataframe called 'df':</p>

<pre><code>writer = pd.ExcelWriter(excel_file_path)
df.to_excel(writer, sheet_name=""Summary"")
</code></pre>

<p>I was looking over the pandas code, and I don't really see any options to set column widths.  Is there a trick out there in the universe to make it such that the columns auto-adjust to the data? Or is there something I can do after the fact to the xlsx file to adjust the column widths? </p>

<p>(I am using the OpenPyXL library, and generating .xlsx files - if that makes any difference.)</p>

<p>Thank you.</p>
";11512.0;"['writer = pd.ExcelWriter(excel_file_path)\ndf.to_excel(writer, sheet_name=""Summary"")\n']";"['writer = pd.ExcelWriter(excel_file_path)\ndf.to_excel(writer, sheet_name=""Summary"")\n']"
213;4.0;1;17383094;;1;30;<python><numpy><pandas>;python pandas/numpy True/False to 1/0 mapping;"<p>I have a column in python pandas DataFrame that has boolean True/False values, but for further calculations I need 1/0 representation. Is there a quick pandas/numpy way to do that?</p>

<p>EDIT:
The answers below do not seem to hold in the case of numpy that, given an array with both integers and True/False values, returns <code>dtype=object</code> on such array. In order to proceed with further calculations in numpy, I had to set explicitly <code>np_values    = np.array(df.values, dtype = np.float64)</code>.</p>
";27133.0;[];['dtype=object', 'np_values    = np.array(df.values, dtype = np.float64)']
214;2.0;1;17438906;;1;30;<python><pandas>;Combining rows in pandas;"<p>I have a DataFrame with an index called <code>city_id</code> of cities in the format <code>[city],[state]</code> (e.g., <code>new york,ny</code> containing integer counts in the columns. The problem is that I have multiple rows for the same city, and I want to collapse the rows sharing a <code>city_id</code> by adding their column values. I looked at <code>groupby()</code> but it wasn't immediately obvious how to apply it to this problem.</p>

<p>Edit:</p>

<p>An example: I'd like to change this:</p>

<pre><code>city_id    val1 val2 val3
houston,tx    1    2    0
houston,tx    0    0    1
houston,tx    2    1    1
</code></pre>

<p>into this:</p>

<pre><code>city_id    val1 val2 val3
houston,tx    3    3    2
</code></pre>

<p>if there are ~10-20k rows.</p>
";26878.0;['city_id    val1 val2 val3\nhouston,tx    1    2    0\nhouston,tx    0    0    1\nhouston,tx    2    1    1\n', 'city_id    val1 val2 val3\nhouston,tx    3    3    2\n'];['city_id', '[city],[state]', 'new york,ny', 'city_id', 'groupby()', 'city_id    val1 val2 val3\nhouston,tx    1    2    0\nhouston,tx    0    0    1\nhouston,tx    2    1    1\n', 'city_id    val1 val2 val3\nhouston,tx    3    3    2\n']
215;4.0;0;17465045;;1;40;<python><date><types><dataframe><pandas>;Can pandas automatically recognize dates?;"<p>Today I was positively surprised by the fact that while reading data from a data file (for example) pandas is able to recognize types of values:</p>

<pre><code>df = pandas.read_csv('test.dat', delimiter=r""\s+"", names=['col1','col2','col3'])
</code></pre>

<p>For example it can be checked in this way:</p>

<pre><code>for i, r in df.iterrows():
    print type(r['col1']), type(r['col2']), type(r['col3'])
</code></pre>

<p>In particular integer, floats and strings were recognized correctly. However, I have a column that has dates in the following format: <code>2013-6-4</code>. These dates were recognized as strings (not as python date-objects). Is there a way to ""learn"" pandas to recognized dates?</p>
";49798.0;"['df = pandas.read_csv(\'test.dat\', delimiter=r""\\s+"", names=[\'col1\',\'col2\',\'col3\'])\n', ""for i, r in df.iterrows():\n    print type(r['col1']), type(r['col2']), type(r['col3'])\n""]";"['df = pandas.read_csv(\'test.dat\', delimiter=r""\\s+"", names=[\'col1\',\'col2\',\'col3\'])\n', ""for i, r in df.iterrows():\n    print type(r['col1']), type(r['col2']), type(r['col3'])\n"", '2013-6-4']"
216;4.0;0;17477979;;1;60;<python><numpy><scipy><pandas>;dropping infinite values from dataframes in pandas?;"<p>what is the quickest/simplest way to drop nan and inf/-inf values from a pandas DataFrame without resetting <code>mode.use_inf_as_null</code>? I'd like to be able to use the <code>subset</code> and <code>how</code> arguments of <code>dropna</code>, except with <code>inf</code> values considered missing, like:</p>

<pre><code>df.dropna(subset=[""col1"", ""col2""], how=""all"", with_inf=True)
</code></pre>

<p>is this possible? Is there a way to tell <code>dropna</code> to include <code>inf</code> in its definition of missing values?</p>
";45786.0;"['df.dropna(subset=[""col1"", ""col2""], how=""all"", with_inf=True)\n']";"['mode.use_inf_as_null', 'subset', 'how', 'dropna', 'inf', 'df.dropna(subset=[""col1"", ""col2""], how=""all"", with_inf=True)\n', 'dropna', 'inf']"
217;4.0;1;17530542;;1;55;<csv><pandas>;How to add pandas data to an existing csv file?;"<p>I want to know if it is possible to use the pandas <code>to_csv()</code> function to add a dataframe to an existing csv file. The csv file has the same structure as the loaded data. </p>
";48770.0;[];['to_csv()']
218;4.0;3;17534106;;1;43;<python><numpy><pandas><nan>;What is the difference between NaN and None?;"<p>I am reading two columns of a csv file using pandas <code>readcsv()</code> and then assigning the values to a dictionary. The columns contain strings of numbers and letters. Occasionally there are cases where a cell is empty. In my opinion, the value read to that dictionary entry should be <code>None</code> but instead <code>nan</code> is assigned. Surely <code>None</code> is more descriptive of an empty cell as it has a null value, whereas <code>nan</code> just says that the value read is not a number.</p>

<p>Is my understanding correct, what IS the difference between <code>None</code> and <code>nan</code>? Why is <code>nan</code> assigned instead of <code>None</code>?</p>

<p>Also, my dictionary check for any empty cells has been using <code>numpy.isnan()</code>:</p>

<pre><code>for k, v in my_dict.iteritems():
    if np.isnan(v):
</code></pre>

<p>But this gives me an error saying that I cannot use this check for <code>v</code>. I guess it is because an integer or float variable, not a string is meant to be used. If this is true, how can I check <code>v</code> for an ""empty cell""/<code>nan</code> case?</p>
";23404.0;['for k, v in my_dict.iteritems():\n    if np.isnan(v):\n'];['readcsv()', 'None', 'nan', 'None', 'nan', 'None', 'nan', 'nan', 'None', 'numpy.isnan()', 'for k, v in my_dict.iteritems():\n    if np.isnan(v):\n', 'v', 'v', 'nan']
219;5.0;17;17557074;;1;61;<python><windows><pandas>;Memory error when using pandas read_csv;"<p>I am trying to do something fairly simple, reading a large csv file into a pandas dataframe.</p>

<pre><code>data = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2)
</code></pre>

<p>The code either fails with a <code>MemoryError</code>, or just never finishes.</p>

<p>Mem usage in the task manager stopped at 506 Mb and after 5 minutes of no change and no CPU activity in the process I stopped it.</p>

<p>I am using pandas version 0.11.0.</p>

<p>I am aware that there used to be a memory problem with the file parser, but according to <strong><a href=""http://wesmckinney.com/blog/?p=543"">http://wesmckinney.com/blog/?p=543</a></strong> this should have been fixed.</p>

<p>The file I am trying to read is 366 Mb, the code above works if I cut the file down to something short (25 Mb).</p>

<p>It has also happened that I get a pop up telling me that it can't write to address 0x1e0baf93... </p>

<p>Stacktrace:</p>

<pre><code>Traceback (most recent call last):
  File ""F:\QA ALM\Python\new WIM data\new WIM data\new_WIM_data.py"", line 25, in
 &lt;module&gt;
    wimdata = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2
)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\io\parsers.py""
, line 401, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\io\parsers.py""
, line 216, in _read
    return parser.read()
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\io\parsers.py""
, line 643, in read
    df = DataFrame(col_dict, columns=columns, index=index)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\core\frame.py""
, line 394, in __init__
    mgr = self._init_dict(data, index, columns, dtype=dtype)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\core\frame.py""
, line 525, in _init_dict
    dtype=dtype)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\core\frame.py""
, line 5338, in _arrays_to_mgr
    return create_block_manager_from_arrays(arrays, arr_names, axes)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\core\internals
.py"", line 1820, in create_block_manager_from_arrays
    blocks = form_blocks(arrays, names, axes)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\core\internals
.py"", line 1872, in form_blocks
    float_blocks = _multi_blockify(float_items, items)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\core\internals
.py"", line 1930, in _multi_blockify
    block_items, values = _stack_arrays(list(tup_block), ref_items, dtype)
  File ""C:\Program Files\Python\Anaconda\lib\site-packages\pandas\core\internals
.py"", line 1962, in _stack_arrays
    stacked = np.empty(shape, dtype=dtype)
MemoryError
Press any key to continue . . .
</code></pre>

<hr>

<p>A bit of background - I am trying to convince people that Python can do the same as R. For this I am trying to replicate an R script that does</p>

<pre><code>data &lt;- read.table(paste(INPUTDIR,config[i,]$TOEXTRACT,sep=""""), HASHEADER, DELIMITER,skip=2,fill=TRUE)
</code></pre>

<p>R not only manages to read the above file just fine, it even reads several of these files in a for loop (and then does some stuff with the data). If Python does have a problem with files of that size I might be fighting a loosing battle...</p>
";17699.0;"['data = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2)\n', 'Traceback (most recent call last):\n  File ""F:\\QA ALM\\Python\\new WIM data\\new WIM data\\new_WIM_data.py"", line 25, in\n <module>\n    wimdata = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2\n)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 401, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 216, in _read\n    return parser.read()\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 643, in read\n    df = DataFrame(col_dict, columns=columns, index=index)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 394, in __init__\n    mgr = self._init_dict(data, index, columns, dtype=dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 525, in _init_dict\n    dtype=dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 5338, in _arrays_to_mgr\n    return create_block_manager_from_arrays(arrays, arr_names, axes)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1820, in create_block_manager_from_arrays\n    blocks = form_blocks(arrays, names, axes)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1872, in form_blocks\n    float_blocks = _multi_blockify(float_items, items)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1930, in _multi_blockify\n    block_items, values = _stack_arrays(list(tup_block), ref_items, dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1962, in _stack_arrays\n    stacked = np.empty(shape, dtype=dtype)\nMemoryError\nPress any key to continue . . .\n', 'data <- read.table(paste(INPUTDIR,config[i,]$TOEXTRACT,sep=""""), HASHEADER, DELIMITER,skip=2,fill=TRUE)\n']";"['data = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2)\n', 'MemoryError', 'Traceback (most recent call last):\n  File ""F:\\QA ALM\\Python\\new WIM data\\new WIM data\\new_WIM_data.py"", line 25, in\n <module>\n    wimdata = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2\n)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 401, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 216, in _read\n    return parser.read()\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 643, in read\n    df = DataFrame(col_dict, columns=columns, index=index)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 394, in __init__\n    mgr = self._init_dict(data, index, columns, dtype=dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 525, in _init_dict\n    dtype=dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 5338, in _arrays_to_mgr\n    return create_block_manager_from_arrays(arrays, arr_names, axes)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1820, in create_block_manager_from_arrays\n    blocks = form_blocks(arrays, names, axes)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1872, in form_blocks\n    float_blocks = _multi_blockify(float_items, items)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1930, in _multi_blockify\n    block_items, values = _stack_arrays(list(tup_block), ref_items, dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1962, in _stack_arrays\n    stacked = np.empty(shape, dtype=dtype)\nMemoryError\nPress any key to continue . . .\n', 'data <- read.table(paste(INPUTDIR,config[i,]$TOEXTRACT,sep=""""), HASHEADER, DELIMITER,skip=2,fill=TRUE)\n']"
220;6.0;1;17618981;;1;35;<python><sorting><dataframe><pandas>;How to sort pandas data frame using values from several columns?;"<p>I have the following data frame:</p>

<pre><code>df = pandas.DataFrame([{'c1':3,'c2':10},{'c1':2, 'c2':30},{'c1':1,'c2':20},{'c1':2,'c2':15},{'c1':2,'c2':100}])
</code></pre>

<p>Or, in human readable form:</p>

<pre><code>   c1   c2
0   3   10
1   2   30
2   1   20
3   2   15
4   2  100
</code></pre>

<p>The following sorting-command works as expected:</p>

<pre><code>df.sort(['c1','c2'], ascending=False)
</code></pre>

<p>Output:</p>

<pre><code>   c1   c2
0   3   10
4   2  100
1   2   30
3   2   15
2   1   20
</code></pre>

<p>But the following command:</p>

<pre><code>df.sort(['c1','c2'], ascending=[False,True])
</code></pre>

<p>results in</p>

<pre><code>   c1   c2
2   1   20
3   2   15
1   2   30
4   2  100
0   3   10
</code></pre>

<p>and this is not what I expect. I expect to have the values in the first column ordered from largest to smallest, and if there are identical values in the first column, order by the ascending values from the second column.</p>

<p>Does anybody know why it does not work as expected?</p>

<p><strong>ADDED</strong></p>

<p>This is copy-paste:</p>

<pre><code>&gt;&gt;&gt; df.sort(['c1','c2'], ascending=[False,True])
   c1   c2
2   1   20
3   2   15
1   2   30
4   2  100
0   3   10
</code></pre>
";82354.0;"[""df = pandas.DataFrame([{'c1':3,'c2':10},{'c1':2, 'c2':30},{'c1':1,'c2':20},{'c1':2,'c2':15},{'c1':2,'c2':100}])\n"", '   c1   c2\n0   3   10\n1   2   30\n2   1   20\n3   2   15\n4   2  100\n', ""df.sort(['c1','c2'], ascending=False)\n"", '   c1   c2\n0   3   10\n4   2  100\n1   2   30\n3   2   15\n2   1   20\n', ""df.sort(['c1','c2'], ascending=[False,True])\n"", '   c1   c2\n2   1   20\n3   2   15\n1   2   30\n4   2  100\n0   3   10\n', "">>> df.sort(['c1','c2'], ascending=[False,True])\n   c1   c2\n2   1   20\n3   2   15\n1   2   30\n4   2  100\n0   3   10\n""]";"[""df = pandas.DataFrame([{'c1':3,'c2':10},{'c1':2, 'c2':30},{'c1':1,'c2':20},{'c1':2,'c2':15},{'c1':2,'c2':100}])\n"", '   c1   c2\n0   3   10\n1   2   30\n2   1   20\n3   2   15\n4   2  100\n', ""df.sort(['c1','c2'], ascending=False)\n"", '   c1   c2\n0   3   10\n4   2  100\n1   2   30\n3   2   15\n2   1   20\n', ""df.sort(['c1','c2'], ascending=[False,True])\n"", '   c1   c2\n2   1   20\n3   2   15\n1   2   30\n4   2  100\n0   3   10\n', "">>> df.sort(['c1','c2'], ascending=[False,True])\n   c1   c2\n2   1   20\n3   2   15\n1   2   30\n4   2  100\n0   3   10\n""]"
221;5.0;3;17627219;;1;29;<python><numpy><pandas><similarity><cosine-similarity>;What's the fastest way in Python to calculate cosine similarity given sparse matrix data?;"<p>Given a sparse matrix listing, what's the best way to calculate the cosine similarity between each of the columns (or rows) in the matrix? I would rather not iterate n-choose-two times.</p>

<p>Say the input matrix is:</p>

<pre><code>A= 
[0 1 0 0 1
 0 0 1 1 1
 1 1 0 1 0]
</code></pre>

<p>The sparse representation is:</p>

<pre><code>A = 
0, 1
0, 4
1, 2
1, 3
1, 4
2, 0
2, 1
2, 3
</code></pre>

<p>In Python, it's straightforward to work with the matrix-input format:</p>

<pre><code>import numpy as np
from sklearn.metrics import pairwise_distances
from scipy.spatial.distance import cosine

A = np.array(
[[0, 1, 0, 0, 1],
[0, 0, 1, 1, 1],
[1, 1, 0, 1, 0]])

dist_out = 1-pairwise_distances(A, metric=""cosine"")
dist_out
</code></pre>

<p>Gives:</p>

<pre><code>array([[ 1.        ,  0.40824829,  0.40824829],
       [ 0.40824829,  1.        ,  0.33333333],
       [ 0.40824829,  0.33333333,  1.        ]])
</code></pre>

<p>That's fine for a full-matrix input, but I really want to start with the sparse representation (due to the size and sparsity of my matrix). Any ideas about how this could best be accomplished? Thanks in advance.</p>
";33178.0;"['A= \n[0 1 0 0 1\n 0 0 1 1 1\n 1 1 0 1 0]\n', 'A = \n0, 1\n0, 4\n1, 2\n1, 3\n1, 4\n2, 0\n2, 1\n2, 3\n', 'import numpy as np\nfrom sklearn.metrics import pairwise_distances\nfrom scipy.spatial.distance import cosine\n\nA = np.array(\n[[0, 1, 0, 0, 1],\n[0, 0, 1, 1, 1],\n[1, 1, 0, 1, 0]])\n\ndist_out = 1-pairwise_distances(A, metric=""cosine"")\ndist_out\n', 'array([[ 1.        ,  0.40824829,  0.40824829],\n       [ 0.40824829,  1.        ,  0.33333333],\n       [ 0.40824829,  0.33333333,  1.        ]])\n']";"['A= \n[0 1 0 0 1\n 0 0 1 1 1\n 1 1 0 1 0]\n', 'A = \n0, 1\n0, 4\n1, 2\n1, 3\n1, 4\n2, 0\n2, 1\n2, 3\n', 'import numpy as np\nfrom sklearn.metrics import pairwise_distances\nfrom scipy.spatial.distance import cosine\n\nA = np.array(\n[[0, 1, 0, 0, 1],\n[0, 0, 1, 1, 1],\n[1, 1, 0, 1, 0]])\n\ndist_out = 1-pairwise_distances(A, metric=""cosine"")\ndist_out\n', 'array([[ 1.        ,  0.40824829,  0.40824829],\n       [ 0.40824829,  1.        ,  0.33333333],\n       [ 0.40824829,  0.33333333,  1.        ]])\n']"
222;5.0;1;17679089;;1;41;<python><pandas><dataframe>;Pandas DataFrame Groupby two columns and get counts;"<p>I have a pandas dataframe in the following format:</p>

<pre><code>df = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T
df.columns = ['col1','col2','col3','col4','col5']
</code></pre>

<p>df:</p>

<pre><code>   col1 col2 col3     col4 col5
0   1.1    A  1.1    x/y/z    1
1   1.1    A  1.7      x/y    3
2   1.1    A  2.5  x/y/z/n    3
3   2.6    B  2.6      x/u    2
4   2.5    B  3.3        x    4
5   3.4    B  3.8    x/u/v    2
6   2.6    B    4    x/y/z    5
7   2.6    A  4.2        x    3
8   3.4    B  4.3  x/u/v/b    6
9   3.4    C  4.5        -    3
10  2.6    B  4.6      x/y    5
11  1.1    D  4.7    x/y/z    1
12  1.1    D  4.7        x    1
13  3.3    D  4.8  x/u/v/w    1
</code></pre>

<p>Now I want to group this by two columns like following:</p>

<pre><code>df.groupby(['col5','col2']).reset_index()
</code></pre>

<p>OutPut:</p>

<pre><code>             index col1 col2 col3     col4 col5
col5 col2                                      
1    A    0      0  1.1    A  1.1    x/y/z    1
     D    0     11  1.1    D  4.7    x/y/z    1
          1     12  1.1    D  4.7        x    1
          2     13  3.3    D  4.8  x/u/v/w    1
2    B    0      3  2.6    B  2.6      x/u    2
          1      5  3.4    B  3.8    x/u/v    2
3    A    0      1  1.1    A  1.7      x/y    3
          1      2  1.1    A  2.5  x/y/z/n    3
          2      7  2.6    A  4.2        x    3
     C    0      9  3.4    C  4.5        -    3
4    B    0      4  2.5    B  3.3        x    4
5    B    0      6  2.6    B    4    x/y/z    5
          1     10  2.6    B  4.6      x/y    5
6    B    0      8  3.4    B  4.3  x/u/v/b    6
</code></pre>

<p>I want to get the count by each row like following.
Expected Output:</p>

<pre><code>col5 col2 count
1    A      1
     D      3
2    B      2
etc...
</code></pre>

<p>How to get my expected output? And I want to find largest count for each 'col2' value?</p>
";65459.0;"[""df = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T\ndf.columns = ['col1','col2','col3','col4','col5']\n"", '   col1 col2 col3     col4 col5\n0   1.1    A  1.1    x/y/z    1\n1   1.1    A  1.7      x/y    3\n2   1.1    A  2.5  x/y/z/n    3\n3   2.6    B  2.6      x/u    2\n4   2.5    B  3.3        x    4\n5   3.4    B  3.8    x/u/v    2\n6   2.6    B    4    x/y/z    5\n7   2.6    A  4.2        x    3\n8   3.4    B  4.3  x/u/v/b    6\n9   3.4    C  4.5        -    3\n10  2.6    B  4.6      x/y    5\n11  1.1    D  4.7    x/y/z    1\n12  1.1    D  4.7        x    1\n13  3.3    D  4.8  x/u/v/w    1\n', ""df.groupby(['col5','col2']).reset_index()\n"", '             index col1 col2 col3     col4 col5\ncol5 col2                                      \n1    A    0      0  1.1    A  1.1    x/y/z    1\n     D    0     11  1.1    D  4.7    x/y/z    1\n          1     12  1.1    D  4.7        x    1\n          2     13  3.3    D  4.8  x/u/v/w    1\n2    B    0      3  2.6    B  2.6      x/u    2\n          1      5  3.4    B  3.8    x/u/v    2\n3    A    0      1  1.1    A  1.7      x/y    3\n          1      2  1.1    A  2.5  x/y/z/n    3\n          2      7  2.6    A  4.2        x    3\n     C    0      9  3.4    C  4.5        -    3\n4    B    0      4  2.5    B  3.3        x    4\n5    B    0      6  2.6    B    4    x/y/z    5\n          1     10  2.6    B  4.6      x/y    5\n6    B    0      8  3.4    B  4.3  x/u/v/b    6\n', 'col5 col2 count\n1    A      1\n     D      3\n2    B      2\netc...\n']";"[""df = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T\ndf.columns = ['col1','col2','col3','col4','col5']\n"", '   col1 col2 col3     col4 col5\n0   1.1    A  1.1    x/y/z    1\n1   1.1    A  1.7      x/y    3\n2   1.1    A  2.5  x/y/z/n    3\n3   2.6    B  2.6      x/u    2\n4   2.5    B  3.3        x    4\n5   3.4    B  3.8    x/u/v    2\n6   2.6    B    4    x/y/z    5\n7   2.6    A  4.2        x    3\n8   3.4    B  4.3  x/u/v/b    6\n9   3.4    C  4.5        -    3\n10  2.6    B  4.6      x/y    5\n11  1.1    D  4.7    x/y/z    1\n12  1.1    D  4.7        x    1\n13  3.3    D  4.8  x/u/v/w    1\n', ""df.groupby(['col5','col2']).reset_index()\n"", '             index col1 col2 col3     col4 col5\ncol5 col2                                      \n1    A    0      0  1.1    A  1.1    x/y/z    1\n     D    0     11  1.1    D  4.7    x/y/z    1\n          1     12  1.1    D  4.7        x    1\n          2     13  3.3    D  4.8  x/u/v/w    1\n2    B    0      3  2.6    B  2.6      x/u    2\n          1      5  3.4    B  3.8    x/u/v    2\n3    A    0      1  1.1    A  1.7      x/y    3\n          1      2  1.1    A  2.5  x/y/z/n    3\n          2      7  2.6    A  4.2        x    3\n     C    0      9  3.4    C  4.5        -    3\n4    B    0      4  2.5    B  3.3        x    4\n5    B    0      6  2.6    B    4    x/y/z    5\n          1     10  2.6    B  4.6      x/y    5\n6    B    0      8  3.4    B  4.3  x/u/v/b    6\n', 'col5 col2 count\n1    A      1\n     D      3\n2    B      2\netc...\n']"
223;3.0;0;17682613;;1;41;<python><arrays><numpy><pandas><scikit-learn>;How to convert a pandas DataFrame subset of columns AND rows into a numpy array?;"<p>I'm wondering if there is a simpler, memory efficient way to select a subset of rows and columns from a pandas DataFrame.</p>

<p>For instance, given this dataframe:</p>

<pre>
df = DataFrame(np.random.rand(4,5), columns = list('abcde'))
print df

          a         b         c         d         e
0  0.945686  0.000710  0.909158  0.892892  0.326670
1  0.919359  0.667057  0.462478  0.008204  0.473096
2  0.976163  0.621712  0.208423  0.980471  0.048334
3  0.459039  0.788318  0.309892  0.100539  0.753992
</pre>

<p>I want only those rows in which the value for column 'c' is greater than 0.5, but I only need columns 'b' and 'e' for those rows.</p>

<p>This is the method that I've come up with - perhaps there is a better ""pandas"" way?</p>

<pre>
locs = [df.columns.get_loc(_) for _ in ['a', 'd']]
print df[df.c > 0.5][locs]

          a         d
0  0.945686  0.892892
</pre>

<p>My final goal is to convert the result to a numpy array to pass into an sklearn regression algorithm, so I will use the code above like this:</p>

<pre>
training_set = array(df[df.c > 0.5][locs])
</pre>

<p>... and that peeves me since I end up with a huge array copy in memory. Perhaps there's a better way for that too?</p>
";99966.0;"[""\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint df\n\n          a         b         c         d         e\n0  0.945686  0.000710  0.909158  0.892892  0.326670\n1  0.919359  0.667057  0.462478  0.008204  0.473096\n2  0.976163  0.621712  0.208423  0.980471  0.048334\n3  0.459039  0.788318  0.309892  0.100539  0.753992\n"", ""\nlocs = [df.columns.get_loc(_) for _ in ['a', 'd']]\nprint df[df.c > 0.5][locs]\n\n          a         d\n0  0.945686  0.892892\n"", '\ntraining_set = array(df[df.c > 0.5][locs])\n']";[]
224;2.0;0;17690738;;1;41;<python><datetime><pandas>;In Pandas how do I convert a string of date strings to datetime objects and put them in a DataFrame?;"<pre><code>import pandas as pd
date_stngs = ('2008-12-20','2008-12-21','2008-12-22','2008-12-23')

a = pd.Series(range(4),index = (range(4)))

for idx, date in enumerate(date_stngs):
    a[idx]= pd.to_datetime(date)
</code></pre>

<p>This code bit produces error: </p>

<blockquote>
  <p>TypeError:"" 'int' object is not iterable""</p>
</blockquote>

<p>Can anyone tell me how to get this series of date time strings into a DataFrame as <code>DateTime</code> objects?</p>
";50026.0;"[""import pandas as pd\ndate_stngs = ('2008-12-20','2008-12-21','2008-12-22','2008-12-23')\n\na = pd.Series(range(4),index = (range(4)))\n\nfor idx, date in enumerate(date_stngs):\n    a[idx]= pd.to_datetime(date)\n""]";"[""import pandas as pd\ndate_stngs = ('2008-12-20','2008-12-21','2008-12-22','2008-12-23')\n\na = pd.Series(range(4),index = (range(4)))\n\nfor idx, date in enumerate(date_stngs):\n    a[idx]= pd.to_datetime(date)\n"", 'DateTime']"
225;2.0;0;17691447;;1;22;<python><pandas><dataframe>;Get count of values across columns-Pandas DataFrame;"<p>I have a Pandas DataFrame like following:</p>

<pre><code>               A              B              C
0   192.168.2.85   192.168.2.85  124.43.113.22
1  192.248.8.183  192.248.8.183   192.168.2.85
2  192.168.2.161            NaN  192.248.8.183
3   66.249.74.52            NaN  192.168.2.161
4            NaN            NaN   66.249.74.52
</code></pre>

<p>I want to get the count of a certain values across columns. So my expected output is something like:</p>

<pre><code>IP          Count
192.168.2.85 3 #Since this value is there in all coulmns
192.248.8.183 3
192.168.2.161 2
66.249.74.52 2
124.43.113.22 1
</code></pre>

<p>I know how to this across rows, but doing this for columns is bit strange?Help me to solve this? Thanks.</p>
";21387.0;['               A              B              C\n0   192.168.2.85   192.168.2.85  124.43.113.22\n1  192.248.8.183  192.248.8.183   192.168.2.85\n2  192.168.2.161            NaN  192.248.8.183\n3   66.249.74.52            NaN  192.168.2.161\n4            NaN            NaN   66.249.74.52\n', 'IP          Count\n192.168.2.85 3 #Since this value is there in all coulmns\n192.248.8.183 3\n192.168.2.161 2\n66.249.74.52 2\n124.43.113.22 1\n'];['               A              B              C\n0   192.168.2.85   192.168.2.85  124.43.113.22\n1  192.248.8.183  192.248.8.183   192.168.2.85\n2  192.168.2.161            NaN  192.248.8.183\n3   66.249.74.52            NaN  192.168.2.161\n4            NaN            NaN   66.249.74.52\n', 'IP          Count\n192.168.2.85 3 #Since this value is there in all coulmns\n192.248.8.183 3\n192.168.2.161 2\n66.249.74.52 2\n124.43.113.22 1\n']
226;10.0;7;17709641;;1;79;<python><numpy><install><pandas><statsmodels>;ValueError: numpy.dtype has the wrong size, try recompiling;"<p>I just installed pandas and statsmodels package on my python 2.7
When I tried  ""import pandas as pd"", this error message comes out.
Can anyone help? Thanks!!!</p>

<pre><code>numpy.dtype has the wrong size, try recompiling
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\analytics\ext\python27\lib\site-packages\statsmodels-0.5.0-py2.7-win32.egg\statsmodels\formula\__init__.py"",
line 4, in &lt;module&gt;
    from formulatools import handle_formula_data
  File ""C:\analytics\ext\python27\lib\site-packages\statsmodels-0.5.0-py2.7-win32.egg\statsmodels\formula\formulatools.p
y"", line 1, in &lt;module&gt;
    import statsmodels.tools.data as data_util
  File ""C:\analytics\ext\python27\lib\site-packages\statsmodels-0.5.0-py2.7-win32.egg\statsmodels\tools\__init__.py"", li
ne 1, in &lt;module&gt;
    from tools import add_constant, categorical
  File ""C:\analytics\ext\python27\lib\site-packages\statsmodels-0.5.0-py2.7-win32.egg\statsmodels\tools\tools.py"", line
14, in &lt;module&gt;
    from pandas import DataFrame
  File ""C:\analytics\ext\python27\lib\site-packages\pandas\__init__.py"", line 6, in &lt;module&gt;
    from . import hashtable, tslib, lib
  File ""numpy.pxd"", line 157, in init pandas.tslib (pandas\tslib.c:49133)
ValueError: numpy.dtype has the wrong size, try recompiling
</code></pre>
";64914.0;"['numpy.dtype has the wrong size, try recompiling\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\formula\\__init__.py"",\nline 4, in <module>\n    from formulatools import handle_formula_data\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\formula\\formulatools.p\ny"", line 1, in <module>\n    import statsmodels.tools.data as data_util\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\tools\\__init__.py"", li\nne 1, in <module>\n    from tools import add_constant, categorical\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\tools\\tools.py"", line\n14, in <module>\n    from pandas import DataFrame\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\pandas\\__init__.py"", line 6, in <module>\n    from . import hashtable, tslib, lib\n  File ""numpy.pxd"", line 157, in init pandas.tslib (pandas\\tslib.c:49133)\nValueError: numpy.dtype has the wrong size, try recompiling\n']";"['numpy.dtype has the wrong size, try recompiling\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\formula\\__init__.py"",\nline 4, in <module>\n    from formulatools import handle_formula_data\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\formula\\formulatools.p\ny"", line 1, in <module>\n    import statsmodels.tools.data as data_util\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\tools\\__init__.py"", li\nne 1, in <module>\n    from tools import add_constant, categorical\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\tools\\tools.py"", line\n14, in <module>\n    from pandas import DataFrame\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\pandas\\__init__.py"", line 6, in <module>\n    from . import hashtable, tslib, lib\n  File ""numpy.pxd"", line 157, in init pandas.tslib (pandas\\tslib.c:49133)\nValueError: numpy.dtype has the wrong size, try recompiling\n']"
227;2.0;0;17812978;;1;30;<python><matplotlib><plot><pandas><dataframe>;How to plot two columns of a pandas data frame using points?;"<p>I have a pandas data frame and would like to plot values from one column versus the values from another column. Fortunately, there is <code>plot</code> method associated with the data-frames that seems to do what I need:</p>

<pre><code>df.plot(x='col_name_1', y='col_name_2')
</code></pre>

<p>Unfortunately, it looks like among the plot styles (listed <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.plot.html"" rel=""noreferrer"">here</a> after the <code>kind</code> parameter) there are not points. I can use lines or bars or even density but not points. Is there a work around that can help to solve this problem.</p>
";74379.0;"[""df.plot(x='col_name_1', y='col_name_2')\n""]";"['plot', ""df.plot(x='col_name_1', y='col_name_2')\n"", 'kind']"
228;3.0;1;17818783;;1;25;<python><numpy><scipy><pandas><sparse-matrix>;Populate a Pandas SparseDataFrame from a SciPy Sparse Matrix;"<p>I noticed Pandas now has <a href=""http://pandas.pydata.org/pandas-docs/dev/sparse.html"">support for Sparse Matrices and Arrays</a>.  Currently, I create <code>DataFrame()</code>s like this:</p>

<pre><code>return DataFrame(matrix.toarray(), columns=features, index=observations)
</code></pre>

<p>Is there a way to create a <code>SparseDataFrame()</code> with a <code>scipy.sparse.csc_matrix()</code> or <code>csr_matrix()</code>? Converting to dense format kills RAM badly. Thanks!</p>
";9661.0;['return DataFrame(matrix.toarray(), columns=features, index=observations)\n'];['DataFrame()', 'return DataFrame(matrix.toarray(), columns=features, index=observations)\n', 'SparseDataFrame()', 'scipy.sparse.csc_matrix()', 'csr_matrix()']
229;6.0;0;17839973;;1;64;<python><pandas><dataframe>;construct pandas DataFrame from values in variables;"<p>This may be a simple question, but I can not figure out how to do this. Lets say that I have two variables as follows.</p>

<pre><code>a = 2
b = 3
</code></pre>

<p>I want to construct a DataFrame from this:</p>

<pre><code>df2 = pd.DataFrame({'A':a,'B':b})
</code></pre>

<p>This generates an error:  </p>

<blockquote>
  <p>ValueError: If using all scalar values, you must pass an index</p>
</blockquote>

<p>I tried this also:</p>

<pre><code>df2 = (pd.DataFrame({'a':a,'b':b})).reset_index()
</code></pre>

<p>This gives the same error message.</p>
";53479.0;"['a = 2\nb = 3\n', ""df2 = pd.DataFrame({'A':a,'B':b})\n"", ""df2 = (pd.DataFrame({'a':a,'b':b})).reset_index()\n""]";"['a = 2\nb = 3\n', ""df2 = pd.DataFrame({'A':a,'B':b})\n"", ""df2 = (pd.DataFrame({'a':a,'b':b})).reset_index()\n""]"
230;4.0;0;17841149;;1;37;<python><pandas>;Pandas groupby: How to get a union of strings;"<p>I have a dataframe like this:</p>

<pre><code>   A         B       C
0  1  0.749065    This
1  2  0.301084      is
2  3  0.463468       a
3  4  0.643961  random
4  1  0.866521  string
5  2  0.120737       !
</code></pre>

<p>Calling </p>

<pre><code>In [10]: print df.groupby(""A"")[""B""].sum()
</code></pre>

<p>will return </p>

<pre><code>A
1    1.615586
2    0.421821
3    0.463468
4    0.643961
</code></pre>

<p>Now I would like to do ""the same"" for column ""C"". Because that column contains strings, sum() doesn't work (although you might think that it would concatenate the strings). What I would really like to see is a list or set of the strings for each group, i.e. </p>

<pre><code>A
1    {This, string}
2    {is, !}
3    {a}
4    {random}
</code></pre>

<p>I have been trying to find ways to do this. </p>

<p>Series.unique() (<a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html</a>) doesn't work, although</p>

<pre><code>df.groupby(""A"")[""B""]
</code></pre>

<p>is a</p>

<pre><code>pandas.core.groupby.SeriesGroupBy object
</code></pre>

<p>so I was hoping any Series method would work. Any ideas?</p>

<p>Thanks,
Anne</p>
";24290.0;"['   A         B       C\n0  1  0.749065    This\n1  2  0.301084      is\n2  3  0.463468       a\n3  4  0.643961  random\n4  1  0.866521  string\n5  2  0.120737       !\n', 'In [10]: print df.groupby(""A"")[""B""].sum()\n', 'A\n1    1.615586\n2    0.421821\n3    0.463468\n4    0.643961\n', 'A\n1    {This, string}\n2    {is, !}\n3    {a}\n4    {random}\n', 'df.groupby(""A"")[""B""]\n', 'pandas.core.groupby.SeriesGroupBy object\n']";"['   A         B       C\n0  1  0.749065    This\n1  2  0.301084      is\n2  3  0.463468       a\n3  4  0.643961  random\n4  1  0.866521  string\n5  2  0.120737       !\n', 'In [10]: print df.groupby(""A"")[""B""].sum()\n', 'A\n1    1.615586\n2    0.421821\n3    0.463468\n4    0.643961\n', 'A\n1    {This, string}\n2    {is, !}\n3    {a}\n4    {random}\n', 'df.groupby(""A"")[""B""]\n', 'pandas.core.groupby.SeriesGroupBy object\n']"
231;4.0;3;17874063;;1;22;<python><pandas><matplotlib>;Is there a parameter in matplotlib/pandas to have the Y axis of a histogram as percentage?;"<p>I would like to compare two histograms by having the Y axis show the percentage of each column from the overall dataset size instead of an absolute value. Is that possible? I am using Pandas and matplotlib.
Thanks</p>
";15779.0;[];[]
232;3.0;1;17950374;;1;38;<string><int><pandas>;Converting a column within pandas dataframe from int to string;"<p>I'm just starting to work with pandas.
I have a dataframe in pandas with mixed int and str data columns. I want to concatenate first columns within the dataframe, to do that I have to convert <code>int</code> column to <code>str</code>. 
I've tried to do that like that:</p>

<pre><code>mtrx['X.3'] = mtrx.to_string(columns = ['X.3'])
</code></pre>

<p>or like that </p>

<pre><code>mtrx['X.3'] = mtrx['X.3'].astype(str)
</code></pre>

<p>but in both cases it's not working and I'm getting an error saying ""cannot concatenate 'str' and 'int' objects"". Concat for two <code>str</code> columns is working perfectly fine.</p>

<p>Any help would be greatly appreciated! Thanks!</p>
";71902.0;"[""mtrx['X.3'] = mtrx.to_string(columns = ['X.3'])\n"", ""mtrx['X.3'] = mtrx['X.3'].astype(str)\n""]";"['int', 'str', ""mtrx['X.3'] = mtrx.to_string(columns = ['X.3'])\n"", ""mtrx['X.3'] = mtrx['X.3'].astype(str)\n"", 'str']"
233;2.0;0;17972938;;1;21;<python><python-2.7><pandas>;check if string in pandas dataframe column is in list;"<p>If I have a frame like this</p>

<pre><code>frame = pd.DataFrame({'a' : ['the cat is blue', 'the sky is green', 'the dog is black']})
</code></pre>

<p>and I want to check if any of those rows contain a certain word I just have to do this.</p>

<pre><code>frame['b'] = frame.a.str.contains(""dog"") | frame.a.str.contains(""cat"") | frame.a.str.contains(""fish"")
</code></pre>

<p><code>frame['b']</code> outputs:</p>

<pre><code>True
False
True
</code></pre>

<p>If I decide to make a list </p>

<pre><code>mylist =['dog', 'cat', 'fish']
</code></pre>

<p>how would I check that the rows contain a certain word in the list?  </p>
";27027.0;"[""frame = pd.DataFrame({'a' : ['the cat is blue', 'the sky is green', 'the dog is black']})\n"", 'frame[\'b\'] = frame.a.str.contains(""dog"") | frame.a.str.contains(""cat"") | frame.a.str.contains(""fish"")\n', 'True\nFalse\nTrue\n', ""mylist =['dog', 'cat', 'fish']\n""]";"[""frame = pd.DataFrame({'a' : ['the cat is blue', 'the sky is green', 'the dog is black']})\n"", 'frame[\'b\'] = frame.a.str.contains(""dog"") | frame.a.str.contains(""cat"") | frame.a.str.contains(""fish"")\n', ""frame['b']"", 'True\nFalse\nTrue\n', ""mylist =['dog', 'cat', 'fish']\n""]"
234;2.0;0;17977540;;1;32;<python><pandas>;Pandas: Looking up the list of sheets in an excel file;"<p>The new version of Pandas uses <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.io.excel.read_excel.html#pandas.io.excel.read_excel"" rel=""noreferrer"">the following interface</a> to load Excel files:</p>

<pre><code>read_excel('path_to_file.xls', 'Sheet1', index_col=None, na_values=['NA'])
</code></pre>

<p>but what if I don't know the sheets that are available? </p>

<p>For example, I am working with excel files that the following sheets</p>

<blockquote>
  <p>Data 1, Data 2 ..., Data N, foo, bar</p>
</blockquote>

<p>but I don't know <code>N</code> a priori.</p>

<p>Is there any way to get the list of sheets from an excel document in Pandas?</p>
";16531.0;"[""read_excel('path_to_file.xls', 'Sheet1', index_col=None, na_values=['NA'])\n""]";"[""read_excel('path_to_file.xls', 'Sheet1', index_col=None, na_values=['NA'])\n"", 'N']"
235;4.0;1;17978092;;1;30;<python><pandas>;Combine Date and Time columns using python pandas;"<p>I have a pandas dataframe with the following columns;</p>

<pre><code>Date              Time
01-06-2013      23:00:00
02-06-2013      01:00:00
02-06-2013      21:00:00
02-06-2013      22:00:00
02-06-2013      23:00:00
03-06-2013      01:00:00
03-06-2013      21:00:00
03-06-2013      22:00:00
03-06-2013      23:00:00
04-06-2013      01:00:00
</code></pre>

<p>How do I combine data['Date'] &amp; data['Time']  to get the following? Is there a way of doing it using <code>pd.to_datetime</code>?</p>

<pre><code>Date
01-06-2013 23:00:00
02-06-2013 01:00:00
02-06-2013 21:00:00
02-06-2013 22:00:00
02-06-2013 23:00:00
03-06-2013 01:00:00
03-06-2013 21:00:00
03-06-2013 22:00:00
03-06-2013 23:00:00
04-06-2013 01:00:00
</code></pre>
";16717.0;['Date              Time\n01-06-2013      23:00:00\n02-06-2013      01:00:00\n02-06-2013      21:00:00\n02-06-2013      22:00:00\n02-06-2013      23:00:00\n03-06-2013      01:00:00\n03-06-2013      21:00:00\n03-06-2013      22:00:00\n03-06-2013      23:00:00\n04-06-2013      01:00:00\n', 'Date\n01-06-2013 23:00:00\n02-06-2013 01:00:00\n02-06-2013 21:00:00\n02-06-2013 22:00:00\n02-06-2013 23:00:00\n03-06-2013 01:00:00\n03-06-2013 21:00:00\n03-06-2013 22:00:00\n03-06-2013 23:00:00\n04-06-2013 01:00:00\n'];['Date              Time\n01-06-2013      23:00:00\n02-06-2013      01:00:00\n02-06-2013      21:00:00\n02-06-2013      22:00:00\n02-06-2013      23:00:00\n03-06-2013      01:00:00\n03-06-2013      21:00:00\n03-06-2013      22:00:00\n03-06-2013      23:00:00\n04-06-2013      01:00:00\n', 'pd.to_datetime', 'Date\n01-06-2013 23:00:00\n02-06-2013 01:00:00\n02-06-2013 21:00:00\n02-06-2013 22:00:00\n02-06-2013 23:00:00\n03-06-2013 01:00:00\n03-06-2013 21:00:00\n03-06-2013 22:00:00\n03-06-2013 23:00:00\n04-06-2013 01:00:00\n']
236;4.0;1;17978133;;1;24;<python><merge><pandas>;Python Pandas merge only certain columns;"<p>Is it possible to only merge some columns? I have a DataFrame df1 with columns x, y, z, and df2 with columns x, a ,b, c, d, e, f, etc.</p>

<p>I want to merge the two DataFrames on x, but I only want to merge columns df2.a, df2.b - not the entire DataFrame. </p>

<p>The result would be a DataFrame with x, y, z, a, b.</p>

<p>I could merge then delete the unwanted columns, but it seems like there is a better method.</p>
";15297.0;[];[]
237;6.0;0;18022845;;1;100;<python><pandas><dataframe><columnname>;Pandas index column title or name;"<p>How do I get the index column name in python pandas?  Here's an example dataframe:</p>

<pre><code>             Column 1
Index Title          
Apples              1
Oranges             2
Puppies             3
Ducks               4  
</code></pre>

<p>What I'm trying to do is get/set the dataframe index title.  Here is what i tried:</p>

<pre><code>import pandas as pd
data = {'Column 1'     : [1., 2., 3., 4.],
        'Index Title'  : [""Apples"", ""Oranges"", ""Puppies"", ""Ducks""]}
df = pd.DataFrame(data)
df.index = df[""Index Title""]
del df[""Index Title""]
print df
</code></pre>

<p>Anyone know how to do this? </p>
";114634.0;"['             Column 1\nIndex Title          \nApples              1\nOranges             2\nPuppies             3\nDucks               4  \n', 'import pandas as pd\ndata = {\'Column 1\'     : [1., 2., 3., 4.],\n        \'Index Title\'  : [""Apples"", ""Oranges"", ""Puppies"", ""Ducks""]}\ndf = pd.DataFrame(data)\ndf.index = df[""Index Title""]\ndel df[""Index Title""]\nprint df\n']";"['             Column 1\nIndex Title          \nApples              1\nOranges             2\nPuppies             3\nDucks               4  \n', 'import pandas as pd\ndata = {\'Column 1\'     : [1., 2., 3., 4.],\n        \'Index Title\'  : [""Apples"", ""Oranges"", ""Puppies"", ""Ducks""]}\ndf = pd.DataFrame(data)\ndf.index = df[""Index Title""]\ndel df[""Index Title""]\nprint df\n']"
238;11.0;1;18039057;;1;84;<python><csv><pandas>;Python Pandas Error tokenizing data;"<p>I'm trying to use pandas to manipulate a .csv file but I get this error:</p>

<pre><code>pandas.parser.CParserError: Error tokenizing data. C error: Expected 2 fields in line 3,  saw 12
</code></pre>

<p>I have tried to read the pandas docs, but found nothing.</p>

<p>My code is simple:</p>

<pre><code>path = 'GOOG Key Ratios.csv'
#print(open(path).read())
data = pd.read_csv(path)
</code></pre>

<p>How can I resolve this? Should I use the <code>csv</code> module or another language ?</p>

<p>Thanks folks.</p>

<p>File is from <a href=""http://financials.morningstar.com/ratios/r.html?t=GOOG&amp;region=usa&amp;culture=en-US"" rel=""noreferrer"">Morningstar</a></p>
";84465.0;"['pandas.parser.CParserError: Error tokenizing data. C error: Expected 2 fields in line 3,  saw 12\n', ""path = 'GOOG Key Ratios.csv'\n#print(open(path).read())\ndata = pd.read_csv(path)\n""]";"['pandas.parser.CParserError: Error tokenizing data. C error: Expected 2 fields in line 3,  saw 12\n', ""path = 'GOOG Key Ratios.csv'\n#print(open(path).read())\ndata = pd.read_csv(path)\n"", 'csv']"
239;5.0;0;18062135;;1;113;<python><pandas><series><dataframe>;Combining two Series into a DataFrame in pandas;"<p>I have two Series <code>s1</code> and <code>s2</code> with the same (non-consecutive) indices. How do I combine <code>s1</code> and <code>s2</code> to being two columns in a DataFrame and keep one of the indices as a third column?</p>
";90070.0;[];['s1', 's2', 's1', 's2']
240;5.0;0;18079563;;1;22;<python><pandas><series>;Finding the intersection between two series in Pandas;"<p>I have two series s1 and s2 in pandas/python and want to compute the intersection i.e. where all of the values of the series are common.</p>

<p>How would I use the concat function to do this? I have been trying to work it out but have been unable to (I don't want to compute the intersection on the indices of s1 and S2, but on the values).</p>

<p>Thanks in advance.</p>
";23715.0;[];[]
241;6.0;4;18089667;;1;42;<python><pandas>;How to estimate how much memory a Pandas' DataFrame will need?;"<p>I have been wondering... If I am reading, say, a 400MB csv file into a pandas dataframe (using read_csv or read_table), is there any way to guesstimate how much memory this will need? Just trying to get a better feel of data frames and  memory...</p>
";17587.0;[];[]
242;1.0;0;18171739;;1;82;<python><csv><pandas><unicode>;UnicodeDecodeError when reading CSV file in Pandas with Python;"<p>I'm running a program which is processing 30,000 similar files. A random number of them are stopping and producing this error...</p>

<pre><code>   File ""C:\Importer\src\dfman\importer.py"", line 26, in import_chr
     data = pd.read_csv(filepath, names=fields)
   File ""C:\Python33\lib\site-packages\pandas\io\parsers.py"", line 400, in parser_f
     return _read(filepath_or_buffer, kwds)
   File ""C:\Python33\lib\site-packages\pandas\io\parsers.py"", line 205, in _read
     return parser.read()
   File ""C:\Python33\lib\site-packages\pandas\io\parsers.py"", line 608, in read
     ret = self._engine.read(nrows)
   File ""C:\Python33\lib\site-packages\pandas\io\parsers.py"", line 1028, in read
     data = self._reader.read(nrows)
   File ""parser.pyx"", line 706, in pandas.parser.TextReader.read (pandas\parser.c:6745)
   File ""parser.pyx"", line 728, in pandas.parser.TextReader._read_low_memory (pandas\parser.c:6964)
   File ""parser.pyx"", line 804, in pandas.parser.TextReader._read_rows (pandas\parser.c:7780)
   File ""parser.pyx"", line 890, in pandas.parser.TextReader._convert_column_data (pandas\parser.c:8793)
   File ""parser.pyx"", line 950, in pandas.parser.TextReader._convert_tokens (pandas\parser.c:9484)
   File ""parser.pyx"", line 1026, in pandas.parser.TextReader._convert_with_dtype (pandas\parser.c:10642)
   File ""parser.pyx"", line 1046, in pandas.parser.TextReader._string_convert (pandas\parser.c:10853)
   File ""parser.pyx"", line 1278, in pandas.parser._string_box_utf8 (pandas\parser.c:15657)
 UnicodeDecodeError: 'utf-8' codec can't decode byte 0xda in position 6: invalid    continuation byte
</code></pre>

<p>The source/creation of these files all come from the same place. What's the best way to correct this to proceed with the import?</p>
";46551.0;"['   File ""C:\\Importer\\src\\dfman\\importer.py"", line 26, in import_chr\n     data = pd.read_csv(filepath, names=fields)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 400, in parser_f\n     return _read(filepath_or_buffer, kwds)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 205, in _read\n     return parser.read()\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 608, in read\n     ret = self._engine.read(nrows)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 1028, in read\n     data = self._reader.read(nrows)\n   File ""parser.pyx"", line 706, in pandas.parser.TextReader.read (pandas\\parser.c:6745)\n   File ""parser.pyx"", line 728, in pandas.parser.TextReader._read_low_memory (pandas\\parser.c:6964)\n   File ""parser.pyx"", line 804, in pandas.parser.TextReader._read_rows (pandas\\parser.c:7780)\n   File ""parser.pyx"", line 890, in pandas.parser.TextReader._convert_column_data (pandas\\parser.c:8793)\n   File ""parser.pyx"", line 950, in pandas.parser.TextReader._convert_tokens (pandas\\parser.c:9484)\n   File ""parser.pyx"", line 1026, in pandas.parser.TextReader._convert_with_dtype (pandas\\parser.c:10642)\n   File ""parser.pyx"", line 1046, in pandas.parser.TextReader._string_convert (pandas\\parser.c:10853)\n   File ""parser.pyx"", line 1278, in pandas.parser._string_box_utf8 (pandas\\parser.c:15657)\n UnicodeDecodeError: \'utf-8\' codec can\'t decode byte 0xda in position 6: invalid    continuation byte\n']";"['   File ""C:\\Importer\\src\\dfman\\importer.py"", line 26, in import_chr\n     data = pd.read_csv(filepath, names=fields)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 400, in parser_f\n     return _read(filepath_or_buffer, kwds)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 205, in _read\n     return parser.read()\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 608, in read\n     ret = self._engine.read(nrows)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 1028, in read\n     data = self._reader.read(nrows)\n   File ""parser.pyx"", line 706, in pandas.parser.TextReader.read (pandas\\parser.c:6745)\n   File ""parser.pyx"", line 728, in pandas.parser.TextReader._read_low_memory (pandas\\parser.c:6964)\n   File ""parser.pyx"", line 804, in pandas.parser.TextReader._read_rows (pandas\\parser.c:7780)\n   File ""parser.pyx"", line 890, in pandas.parser.TextReader._convert_column_data (pandas\\parser.c:8793)\n   File ""parser.pyx"", line 950, in pandas.parser.TextReader._convert_tokens (pandas\\parser.c:9484)\n   File ""parser.pyx"", line 1026, in pandas.parser.TextReader._convert_with_dtype (pandas\\parser.c:10642)\n   File ""parser.pyx"", line 1046, in pandas.parser.TextReader._string_convert (pandas\\parser.c:10853)\n   File ""parser.pyx"", line 1278, in pandas.parser._string_box_utf8 (pandas\\parser.c:15657)\n UnicodeDecodeError: \'utf-8\' codec can\'t decode byte 0xda in position 6: invalid    continuation byte\n']"
243;4.0;1;18172851;;1;194;<python><pandas>;Deleting DataFrame row in Pandas based on column value;"<p>I have the following DataFrame:</p>

<pre><code>             daysago  line_race rating        rw    wrating
 line_date                                                 
 2007-03-31       62         11     56  1.000000  56.000000
 2007-03-10       83         11     67  1.000000  67.000000
 2007-02-10      111          9     66  1.000000  66.000000
 2007-01-13      139         10     83  0.880678  73.096278
 2006-12-23      160         10     88  0.793033  69.786942
 2006-11-09      204          9     52  0.636655  33.106077
 2006-10-22      222          8     66  0.581946  38.408408
 2006-09-29      245          9     70  0.518825  36.317752
 2006-09-16      258         11     68  0.486226  33.063381
 2006-08-30      275          8     72  0.446667  32.160051
 2006-02-11      475          5     65  0.164591  10.698423
 2006-01-13      504          0     70  0.142409   9.968634
 2006-01-02      515          0     64  0.134800   8.627219
 2005-12-06      542          0     70  0.117803   8.246238
 2005-11-29      549          0     70  0.113758   7.963072
 2005-11-22      556          0     -1  0.109852  -0.109852
 2005-11-01      577          0     -1  0.098919  -0.098919
 2005-10-20      589          0     -1  0.093168  -0.093168
 2005-09-27      612          0     -1  0.083063  -0.083063
 2005-09-07      632          0     -1  0.075171  -0.075171
 2005-06-12      719          0     69  0.048690   3.359623
 2005-05-29      733          0     -1  0.045404  -0.045404
 2005-05-02      760          0     -1  0.039679  -0.039679
 2005-04-02      790          0     -1  0.034160  -0.034160
 2005-03-13      810          0     -1  0.030915  -0.030915
 2004-11-09      934          0     -1  0.016647  -0.016647
</code></pre>

<p>I need to remove the rows where <code>line_race</code> is equal to <code>0</code>. What's the most efficient way to do this?</p>
";264894.0;['             daysago  line_race rating        rw    wrating\n line_date                                                 \n 2007-03-31       62         11     56  1.000000  56.000000\n 2007-03-10       83         11     67  1.000000  67.000000\n 2007-02-10      111          9     66  1.000000  66.000000\n 2007-01-13      139         10     83  0.880678  73.096278\n 2006-12-23      160         10     88  0.793033  69.786942\n 2006-11-09      204          9     52  0.636655  33.106077\n 2006-10-22      222          8     66  0.581946  38.408408\n 2006-09-29      245          9     70  0.518825  36.317752\n 2006-09-16      258         11     68  0.486226  33.063381\n 2006-08-30      275          8     72  0.446667  32.160051\n 2006-02-11      475          5     65  0.164591  10.698423\n 2006-01-13      504          0     70  0.142409   9.968634\n 2006-01-02      515          0     64  0.134800   8.627219\n 2005-12-06      542          0     70  0.117803   8.246238\n 2005-11-29      549          0     70  0.113758   7.963072\n 2005-11-22      556          0     -1  0.109852  -0.109852\n 2005-11-01      577          0     -1  0.098919  -0.098919\n 2005-10-20      589          0     -1  0.093168  -0.093168\n 2005-09-27      612          0     -1  0.083063  -0.083063\n 2005-09-07      632          0     -1  0.075171  -0.075171\n 2005-06-12      719          0     69  0.048690   3.359623\n 2005-05-29      733          0     -1  0.045404  -0.045404\n 2005-05-02      760          0     -1  0.039679  -0.039679\n 2005-04-02      790          0     -1  0.034160  -0.034160\n 2005-03-13      810          0     -1  0.030915  -0.030915\n 2004-11-09      934          0     -1  0.016647  -0.016647\n'];['             daysago  line_race rating        rw    wrating\n line_date                                                 \n 2007-03-31       62         11     56  1.000000  56.000000\n 2007-03-10       83         11     67  1.000000  67.000000\n 2007-02-10      111          9     66  1.000000  66.000000\n 2007-01-13      139         10     83  0.880678  73.096278\n 2006-12-23      160         10     88  0.793033  69.786942\n 2006-11-09      204          9     52  0.636655  33.106077\n 2006-10-22      222          8     66  0.581946  38.408408\n 2006-09-29      245          9     70  0.518825  36.317752\n 2006-09-16      258         11     68  0.486226  33.063381\n 2006-08-30      275          8     72  0.446667  32.160051\n 2006-02-11      475          5     65  0.164591  10.698423\n 2006-01-13      504          0     70  0.142409   9.968634\n 2006-01-02      515          0     64  0.134800   8.627219\n 2005-12-06      542          0     70  0.117803   8.246238\n 2005-11-29      549          0     70  0.113758   7.963072\n 2005-11-22      556          0     -1  0.109852  -0.109852\n 2005-11-01      577          0     -1  0.098919  -0.098919\n 2005-10-20      589          0     -1  0.093168  -0.093168\n 2005-09-27      612          0     -1  0.083063  -0.083063\n 2005-09-07      632          0     -1  0.075171  -0.075171\n 2005-06-12      719          0     69  0.048690   3.359623\n 2005-05-29      733          0     -1  0.045404  -0.045404\n 2005-05-02      760          0     -1  0.039679  -0.039679\n 2005-04-02      790          0     -1  0.034160  -0.034160\n 2005-03-13      810          0     -1  0.030915  -0.030915\n 2004-11-09      934          0     -1  0.016647  -0.016647\n', 'line_race', '0']
244;1.0;0;18196203;;1;22;<python><pandas>;How to conditionally update DataFrame column in Pandas;"<p>With this DataFrame, how can I conditionally set <code>rating</code> to 0 when <code>line_race</code> is equal to zero? </p>

<pre><code>    line_track  line_race  rating foreign
 25        MTH         10     84    False
 26        MTH          6     88    False
 27        TAM          5     87    False
 28         GP          2     86    False
 29         GP          7     59    False
 30        LCH          0    103     True
 31        LEO          0    125     True
 32        YOR          0    126     True
 33        ASC          0    124     True
</code></pre>

<p>In other words, what is the proper way on a DataFrame to say if ColumnA = x then ColumnB = y else ColumnB = ColumnB</p>
";20823.0;['    line_track  line_race  rating foreign\n 25        MTH         10     84    False\n 26        MTH          6     88    False\n 27        TAM          5     87    False\n 28         GP          2     86    False\n 29         GP          7     59    False\n 30        LCH          0    103     True\n 31        LEO          0    125     True\n 32        YOR          0    126     True\n 33        ASC          0    124     True\n'];['rating', 'line_race', '    line_track  line_race  rating foreign\n 25        MTH         10     84    False\n 26        MTH          6     88    False\n 27        TAM          5     87    False\n 28         GP          2     86    False\n 29         GP          7     59    False\n 30        LCH          0    103     True\n 31        LEO          0    125     True\n 32        YOR          0    126     True\n 33        ASC          0    124     True\n']
245;3.0;1;18215317;;1;36;<python><numpy><pandas>;extracting days from a numpy.timedelta64 value;"<p>I am using pandas/python and I have two date time series s1 and s2, that have been generated using the 'to_datetime' function on a field of the df containing dates/times.</p>

<p>When I subtract s1 from s2</p>

<blockquote>
  <p>s3 = s2 - s1</p>
</blockquote>

<p>I get a series, s3, of type </p>

<blockquote>
  <p>timedelta64[ns]</p>
</blockquote>

<pre><code>0    385 days, 04:10:36
1     57 days, 22:54:00
2    642 days, 21:15:23
3    615 days, 00:55:44
4    160 days, 22:13:35
5    196 days, 23:06:49
6     23 days, 22:57:17
7      2 days, 22:17:31
8    622 days, 01:29:25
9     79 days, 20:15:14
10    23 days, 22:46:51
11   268 days, 19:23:04
12                  NaT
13                  NaT
14   583 days, 03:40:39
</code></pre>

<p>How do I look at 1 element of the series:</p>

<blockquote>
  <p>s3[10]</p>
</blockquote>

<p>I get something like this:</p>

<blockquote>
  <p>numpy.timedelta64(2069211000000000,'ns')</p>
</blockquote>

<p>How do I extract days from s3 and maybe keep them as integers(not so interested in hours/mins etc.)?</p>

<p>Thanks in advance for any help.</p>
";32906.0;['0    385 days, 04:10:36\n1     57 days, 22:54:00\n2    642 days, 21:15:23\n3    615 days, 00:55:44\n4    160 days, 22:13:35\n5    196 days, 23:06:49\n6     23 days, 22:57:17\n7      2 days, 22:17:31\n8    622 days, 01:29:25\n9     79 days, 20:15:14\n10    23 days, 22:46:51\n11   268 days, 19:23:04\n12                  NaT\n13                  NaT\n14   583 days, 03:40:39\n'];['0    385 days, 04:10:36\n1     57 days, 22:54:00\n2    642 days, 21:15:23\n3    615 days, 00:55:44\n4    160 days, 22:13:35\n5    196 days, 23:06:49\n6     23 days, 22:57:17\n7      2 days, 22:17:31\n8    622 days, 01:29:25\n9     79 days, 20:15:14\n10    23 days, 22:46:51\n11   268 days, 19:23:04\n12                  NaT\n13                  NaT\n14   583 days, 03:40:39\n']
246;4.0;1;18316211;;1;33;<python><pandas>;Access index in pandas.Series.apply;"<p>Lets say I have a MultiIndex Series <code>s</code>:</p>

<pre><code>&gt;&gt;&gt; s
     values
a b
1 2  0.1 
3 6  0.3
4 4  0.7
</code></pre>

<p>and I want to apply a function which uses the index of the row:</p>

<pre><code>def f(x):
   # conditions or computations using the indexes
   if x.index[0] and ...: 
   other = sum(x.index) + ...
   return something
</code></pre>

<p>How can I do <code>s.apply(f)</code> for such a function? What is the recommended way to make this kind of operations? I expect to obtain a new Series with the values resulting from this function applied on each row and the same MultiIndex.</p>
";16141.0;['>>> s\n     values\na b\n1 2  0.1 \n3 6  0.3\n4 4  0.7\n', 'def f(x):\n   # conditions or computations using the indexes\n   if x.index[0] and ...: \n   other = sum(x.index) + ...\n   return something\n'];['s', '>>> s\n     values\na b\n1 2  0.1 \n3 6  0.3\n4 4  0.7\n', 'def f(x):\n   # conditions or computations using the indexes\n   if x.index[0] and ...: \n   other = sum(x.index) + ...\n   return something\n', 's.apply(f)']
247;6.0;0;18327624;;1;43;<python><pandas>;Find element's index in pandas Series;"<p>I know this is a very basic question but for some reason I can't find an answer. How can I get the index of certain element of a Series in python pandas? (first occurrence would suffice)</p>

<p>I.e., I'd like something like:</p>

<pre><code>import pandas as pd
myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])
print myseries.find(7) # should output 3
</code></pre>

<p>Certainly, it is possible to define such a method with a loop:</p>

<pre><code>def find(s, el):
    for i in s.index:
        if s[i] == el: 
            return i
    return None

print find(myseries, 7)
</code></pre>

<p>but I assume there should be a better way. Is there?</p>
";81639.0;['import pandas as pd\nmyseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])\nprint myseries.find(7) # should output 3\n', 'def find(s, el):\n    for i in s.index:\n        if s[i] == el: \n            return i\n    return None\n\nprint find(myseries, 7)\n'];['import pandas as pd\nmyseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])\nprint myseries.find(7) # should output 3\n', 'def find(s, el):\n    for i in s.index:\n        if s[i] == el: \n            return i\n    return None\n\nprint find(myseries, 7)\n']
248;2.0;4;18358938;;1;25;<python><list><pandas><indexing>;Get index values of Pandas DataFrame as list?;"<p>I'm probably using poor search terms when trying to find this answer. Right now, before indexing a DataFrame, I'm getting a list of values in a column this way...</p>

<pre><code> list = list(df['column']) 
</code></pre>

<p>...then I'll <code>set_index</code> on the column. This seems like a wasted step. When trying the above on an index, I get a key error.</p>

<p>How can I grab the values in an index (both single and multi) and put them in a list or a list of tuples?</p>
";35190.0;"["" list = list(df['column']) \n""]";"["" list = list(df['column']) \n"", 'set_index']"
249;6.0;4;18429491;;1;40;<pandas><grouping><nan>;groupby columns with NaN (missing) values;"<p>I have a DataFrame with many missing values in columns which I wish to groupby:</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame({'a': ['1', '2', '3'], 'b': ['4', np.NaN, '6']})

In [4]: df.groupby('b').groups
Out[4]: {'4': [0], '6': [2]}
</code></pre>

<p>see that Pandas has dropped the rows with NaN target values. (I want to include these rows!)</p>

<p><em>Since I need many such operations (many cols have missing values), and use more complicated functions than just medians (typically random forests), I want to avoid writing too complicated pieces of code.</em></p>

<p>Any suggestions? Should I write a function for this or is there a simple solution?</p>
";20547.0;"[""import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'a': ['1', '2', '3'], 'b': ['4', np.NaN, '6']})\n\nIn [4]: df.groupby('b').groups\nOut[4]: {'4': [0], '6': [2]}\n""]";"[""import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'a': ['1', '2', '3'], 'b': ['4', np.NaN, '6']})\n\nIn [4]: df.groupby('b').groups\nOut[4]: {'4': [0], '6': [2]}\n""]"
250;2.0;0;18504967;;1;21;<python><pandas><calculated-columns>;pandas dataframe create new columns and fill with calculated values from same df;"<p>Here is a simplified example of my df:</p>

<pre><code>ds = pd.DataFrame(np.abs(randn(3, 4)), index=[1,2,3], columns=['A','B','C','D'])
ds
      A         B         C         D
1  1.099679  0.042043  0.083903  0.410128
2  0.268205  0.718933  1.459374  0.758887
3  0.680566  0.538655  0.038236  1.169403
</code></pre>

<p>I would like to sum the data in the columns row wise:</p>

<pre><code>ds['sum']=ds.sum(axis=1)
ds
      A         B         C         D       sum
1  0.095389  0.556978  1.646888  1.959295  4.258550
2  1.076190  2.668270  0.825116  1.477040  6.046616
3  0.245034  1.066285  0.967124  0.791606  3.070049
</code></pre>

<p>Now, here comes my question! I would like to create 4 new columns and calculate the percentage value from the total (sum) in every row. So first value in the first new column should be (0.095389/4.258550), first value in the second new column (0.556978/4.258550)...and so on...
Help please </p>
";37638.0;"[""ds = pd.DataFrame(np.abs(randn(3, 4)), index=[1,2,3], columns=['A','B','C','D'])\nds\n      A         B         C         D\n1  1.099679  0.042043  0.083903  0.410128\n2  0.268205  0.718933  1.459374  0.758887\n3  0.680566  0.538655  0.038236  1.169403\n"", ""ds['sum']=ds.sum(axis=1)\nds\n      A         B         C         D       sum\n1  0.095389  0.556978  1.646888  1.959295  4.258550\n2  1.076190  2.668270  0.825116  1.477040  6.046616\n3  0.245034  1.066285  0.967124  0.791606  3.070049\n""]";"[""ds = pd.DataFrame(np.abs(randn(3, 4)), index=[1,2,3], columns=['A','B','C','D'])\nds\n      A         B         C         D\n1  1.099679  0.042043  0.083903  0.410128\n2  0.268205  0.718933  1.459374  0.758887\n3  0.680566  0.538655  0.038236  1.169403\n"", ""ds['sum']=ds.sum(axis=1)\nds\n      A         B         C         D       sum\n1  0.095389  0.556978  1.646888  1.959295  4.258550\n2  1.076190  2.668270  0.825116  1.477040  6.046616\n3  0.245034  1.066285  0.967124  0.791606  3.070049\n""]"
251;2.0;0;18554920;;1;39;<python><pandas>;Pandas aggregate count distinct;"<p>Let's say I have a log of user activity and I want to generate a report of total duration and the number of unique users per day.</p>

<pre><code>import numpy as np
import pandas as pd
df = pd.DataFrame({'date': ['2013-04-01','2013-04-01','2013-04-01','2013-04-02', '2013-04-02'],
    'user_id': ['0001', '0001', '0002', '0002', '0002'],
    'duration': [30, 15, 20, 15, 30]})
</code></pre>

<p>Aggregating duration is pretty straightforward:</p>

<pre><code>group = df.groupby('date')
agg = group.aggregate({'duration': np.sum})
agg
            duration
date
2013-04-01        65
2013-04-02        45
</code></pre>

<p>What I'd like to do is sum the duration and count distincts at the same time, but I can't seem to find an equivalent for count_distinct:</p>

<pre><code>agg = group.aggregate({ 'duration': np.sum, 'user_id': count_distinct})
</code></pre>

<p>This works, but surely there's a better way, no?</p>

<pre><code>group = df.groupby('date')
agg = group.aggregate({'duration': np.sum})
agg['uv'] = df.groupby('date').user_id.nunique()
agg
            duration  uv
date
2013-04-01        65   2
2013-04-02        45   1
</code></pre>

<p>I'm thinking I just need to provide a function that returns the count of distinct items of a Series object to the aggregate function, but I don't have a lot of exposure to the various libraries at my disposal. Also, it seems that the groupby object already knows this information, so wouldn't I just be duplicating effort?</p>
";33368.0;"[""import numpy as np\nimport pandas as pd\ndf = pd.DataFrame({'date': ['2013-04-01','2013-04-01','2013-04-01','2013-04-02', '2013-04-02'],\n    'user_id': ['0001', '0001', '0002', '0002', '0002'],\n    'duration': [30, 15, 20, 15, 30]})\n"", ""group = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg\n            duration\ndate\n2013-04-01        65\n2013-04-02        45\n"", ""agg = group.aggregate({ 'duration': np.sum, 'user_id': count_distinct})\n"", ""group = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg['uv'] = df.groupby('date').user_id.nunique()\nagg\n            duration  uv\ndate\n2013-04-01        65   2\n2013-04-02        45   1\n""]";"[""import numpy as np\nimport pandas as pd\ndf = pd.DataFrame({'date': ['2013-04-01','2013-04-01','2013-04-01','2013-04-02', '2013-04-02'],\n    'user_id': ['0001', '0001', '0002', '0002', '0002'],\n    'duration': [30, 15, 20, 15, 30]})\n"", ""group = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg\n            duration\ndate\n2013-04-01        65\n2013-04-02        45\n"", ""agg = group.aggregate({ 'duration': np.sum, 'user_id': count_distinct})\n"", ""group = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg['uv'] = df.groupby('date').user_id.nunique()\nagg\n            duration  uv\ndate\n2013-04-01        65   2\n2013-04-02        45   1\n""]"
252;2.0;0;18594469;;1;27;<python><pandas><normalization><dataframe>;Normalizing a pandas DataFrame by row;"<p>What is the most idiomatic way to normalize each row of a pandas DataFrame? Normalizing the columns is easy, so one (very ugly!) option is:</p>

<pre><code>(df.T / df.T.sum()).T
</code></pre>

<p>Pandas broadcasting rules prevent <code>df / df.sum(axis=1)</code> from doing this</p>
";11082.0;['(df.T / df.T.sum()).T\n'];['(df.T / df.T.sum()).T\n', 'df / df.sum(axis=1)']
253;2.0;0;18674064;;1;61;<python><indexing><pandas>;how do I insert a column at a specific column index in pandas?;"<p>Can I insert a column at a specific column index in pandas? </p>

<pre><code>import pandas as pd
df = pd.DataFrame({'l':['a','b','c','d'], 'v':[1,2,1,2]})
df['n'] = 0
</code></pre>

<p>This will put column <code>n</code> as the last column of <code>df</code>, but isn't there a way to tell <code>df</code> to put <code>n</code> at the beginning?</p>
";47504.0;"[""import pandas as pd\ndf = pd.DataFrame({'l':['a','b','c','d'], 'v':[1,2,1,2]})\ndf['n'] = 0\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'l':['a','b','c','d'], 'v':[1,2,1,2]})\ndf['n'] = 0\n"", 'n', 'df', 'df', 'n']"
254;2.0;2;18689512;;1;55;<python><numpy><pandas>;Efficiently checking if arbitrary object is NaN in Python / numpy / pandas?;"<p>My numpy arrays use <code>np.nan</code> to designate missing values. As I iterate over the data set, I need to detect such missing values and handle them in special ways.</p>

<p>Naively I used <code>numpy.isnan(val)</code>, which works well unless <code>val</code> isn't among the subset of types supported by <code>numpy.isnan()</code>. For example, missing data can occur in string fields, in which case I get:</p>

<pre><code>&gt;&gt;&gt; np.isnan('some_string')
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: Not implemented for this type
</code></pre>

<p>Other than writing an expensive wrapper that catches the exception and returns <code>False</code>, is there a way to handle this elegantly and efficiently?</p>
";68431.0;"['>>> np.isnan(\'some_string\')\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nTypeError: Not implemented for this type\n']";"['np.nan', 'numpy.isnan(val)', 'val', 'numpy.isnan()', '>>> np.isnan(\'some_string\')\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nTypeError: Not implemented for this type\n', 'False']"
255;6.0;0;18689823;;1;29;<python><pandas><nan>;pandas DataFrame: replace nan values with average of columns;"<p>I've got a pandas DataFrame filled mostly with real numbers, but there is a few <code>nan</code> values in it as well.</p>

<p>How can I replace the <code>nan</code>s with averages of columns where they are?</p>

<p>This question is very similar to this one: <a href=""https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns"">numpy array: replace nan values with average of columns</a>  but, unfortunately, the solution given there doesn't work for a pandas DataFrame.</p>
";36285.0;[];['nan', 'nan']
256;4.0;4;18695605;;1;42;<python><dictionary><pandas>;python pandas dataframe to dictionary;"<p>I've a two columns dataframe, and intend to convert it to python dictionary - the first column will be the key and the second will be the value. Thank you in advance. </p>

<p>Dataframe:</p>

<pre><code>    id    value
0    0     10.2
1    1      5.7
2    2      7.4
</code></pre>
";63301.0;['    id    value\n0    0     10.2\n1    1      5.7\n2    2      7.4\n'];['    id    value\n0    0     10.2\n1    1      5.7\n2    2      7.4\n']
257;3.0;0;18792918;;1;28;<python><pandas><left-join><dataframe>;Pandas Combining 2 Data Frames (join on a common column);"<p>I have 2 dataframes:</p>

<p>restaurant_ids_dataframe </p>

<pre><code>Data columns (total 13 columns):
business_id      4503  non-null values
categories       4503  non-null values
city             4503  non-null values
full_address     4503  non-null values
latitude         4503  non-null values
longitude        4503  non-null values
name             4503  non-null values
neighborhoods    4503  non-null values
open             4503  non-null values
review_count     4503  non-null values
stars            4503  non-null values
state            4503  non-null values
type             4503  non-null values
dtypes: bool(1), float64(3), int64(1), object(8)`
</code></pre>

<p>and </p>

<p>restaurant_review_frame</p>

<pre><code>Int64Index: 158430 entries, 0 to 229905
Data columns (total 8 columns):
business_id    158430  non-null values
date           158430  non-null values
review_id      158430  non-null values
stars          158430  non-null values
text           158430  non-null values
type           158430  non-null values
user_id        158430  non-null values
votes          158430  non-null values
dtypes: int64(1), object(7)
</code></pre>

<p>I would like to join these two DataFrames to make them into a single dataframe using the DataFrame.join() command in pandas.</p>

<p>I have tried the following line of code:</p>

<pre><code>#the following line of code creates a left join of restaurant_ids_frame and   restaurant_review_frame on the column 'business_id'
restaurant_review_frame.join(other=restaurant_ids_dataframe,on='business_id',how='left')
</code></pre>

<p>But when I try this I get the following error:</p>

<pre><code>Exception: columns overlap: Index([business_id, stars, type], dtype=object)
</code></pre>

<p>I am very new to pandas and have no clue what I am doing wrong as far as executing the join statement is concerned.</p>

<p>any help would be much appreciated.</p>
";63764.0;"['Data columns (total 13 columns):\nbusiness_id      4503  non-null values\ncategories       4503  non-null values\ncity             4503  non-null values\nfull_address     4503  non-null values\nlatitude         4503  non-null values\nlongitude        4503  non-null values\nname             4503  non-null values\nneighborhoods    4503  non-null values\nopen             4503  non-null values\nreview_count     4503  non-null values\nstars            4503  non-null values\nstate            4503  non-null values\ntype             4503  non-null values\ndtypes: bool(1), float64(3), int64(1), object(8)`\n', 'Int64Index: 158430 entries, 0 to 229905\nData columns (total 8 columns):\nbusiness_id    158430  non-null values\ndate           158430  non-null values\nreview_id      158430  non-null values\nstars          158430  non-null values\ntext           158430  non-null values\ntype           158430  non-null values\nuser_id        158430  non-null values\nvotes          158430  non-null values\ndtypes: int64(1), object(7)\n', ""#the following line of code creates a left join of restaurant_ids_frame and   restaurant_review_frame on the column 'business_id'\nrestaurant_review_frame.join(other=restaurant_ids_dataframe,on='business_id',how='left')\n"", 'Exception: columns overlap: Index([business_id, stars, type], dtype=object)\n']";"['Data columns (total 13 columns):\nbusiness_id      4503  non-null values\ncategories       4503  non-null values\ncity             4503  non-null values\nfull_address     4503  non-null values\nlatitude         4503  non-null values\nlongitude        4503  non-null values\nname             4503  non-null values\nneighborhoods    4503  non-null values\nopen             4503  non-null values\nreview_count     4503  non-null values\nstars            4503  non-null values\nstate            4503  non-null values\ntype             4503  non-null values\ndtypes: bool(1), float64(3), int64(1), object(8)`\n', 'Int64Index: 158430 entries, 0 to 229905\nData columns (total 8 columns):\nbusiness_id    158430  non-null values\ndate           158430  non-null values\nreview_id      158430  non-null values\nstars          158430  non-null values\ntext           158430  non-null values\ntype           158430  non-null values\nuser_id        158430  non-null values\nvotes          158430  non-null values\ndtypes: int64(1), object(7)\n', ""#the following line of code creates a left join of restaurant_ids_frame and   restaurant_review_frame on the column 'business_id'\nrestaurant_review_frame.join(other=restaurant_ids_dataframe,on='business_id',how='left')\n"", 'Exception: columns overlap: Index([business_id, stars, type], dtype=object)\n']"
258;2.0;2;18835077;;1;34;<python><pandas>;selecting from multi-index pandas;"<p>I have a multi-index data frame with columns  'A' and 'B'. </p>

<p>Is there is a way to select rows by filtering on one column of the multi-index without reseting the index to single column index. </p>

<p>For Example.</p>

<pre><code># has multi-index (A,B)
df
#can i do this? I know this doesnt work because index is multi-index so I need to     specify a tuple

df.ix[df.A ==1]
</code></pre>
";34779.0;['# has multi-index (A,B)\ndf\n#can i do this? I know this doesnt work because index is multi-index so I need to     specify a tuple\n\ndf.ix[df.A ==1]\n'];['# has multi-index (A,B)\ndf\n#can i do this? I know this doesnt work because index is multi-index so I need to     specify a tuple\n\ndf.ix[df.A ==1]\n']
259;8.0;4;18837262;;1;78;<python><pandas><dataframe>;Convert Python dict into a dataframe;"<p>I have a Python dictionary like the following:</p>

<pre><code>{u'2012-06-08': 388,
 u'2012-06-09': 388,
 u'2012-06-10': 388,
 u'2012-06-11': 389,
 u'2012-06-12': 389,
 u'2012-06-13': 389,
 u'2012-06-14': 389,
 u'2012-06-15': 389,
 u'2012-06-16': 389,
 u'2012-06-17': 389,
 u'2012-06-18': 390,
 u'2012-06-19': 390,
 u'2012-06-20': 390,
 u'2012-06-21': 390,
 u'2012-06-22': 390,
 u'2012-06-23': 390,
 u'2012-06-24': 390,
 u'2012-06-25': 391,
 u'2012-06-26': 391,
 u'2012-06-27': 391,
 u'2012-06-28': 391,
 u'2012-06-29': 391,
 u'2012-06-30': 391,
 u'2012-07-01': 391,
 u'2012-07-02': 392,
 u'2012-07-03': 392,
 u'2012-07-04': 392,
 u'2012-07-05': 392,
 u'2012-07-06': 392}
</code></pre>

<p>The keys are <a href=""http://en.wikipedia.org/wiki/Unicode"" rel=""noreferrer"">Unicode</a> dates and the values are integers. I would like to convert this into a pandas dataframe by having the dates and their corresponding values as two separate columns. Example: col1: Dates col2: DateValue (the dates are still Unicode and datevalues are still integers)</p>

<pre><code>     Date         DateValue
0    2012-07-01    391
1    2012-07-02    392
2    2012-07-03    392
.    2012-07-04    392
.    ...           ...
.    ...           ...
</code></pre>

<p>Any help in this direction would be much appreciated. I am unable to find resources on the pandas docs to help me with this.</p>

<p>I know one solution might be to convert each key-value pair in this dict, into a dict so the entire structure becomes a dict of dicts, and then we can add each row individually to the dataframe. But I want to know if there is an easier way and a more direct way to do this.</p>

<p>So far I have tried converting the dict into a series object but this doesn't seem to maintain the relationship between the columns:</p>

<pre><code>s  = Series(my_dict,index=my_dict.keys())
</code></pre>
";151737.0;"[""{u'2012-06-08': 388,\n u'2012-06-09': 388,\n u'2012-06-10': 388,\n u'2012-06-11': 389,\n u'2012-06-12': 389,\n u'2012-06-13': 389,\n u'2012-06-14': 389,\n u'2012-06-15': 389,\n u'2012-06-16': 389,\n u'2012-06-17': 389,\n u'2012-06-18': 390,\n u'2012-06-19': 390,\n u'2012-06-20': 390,\n u'2012-06-21': 390,\n u'2012-06-22': 390,\n u'2012-06-23': 390,\n u'2012-06-24': 390,\n u'2012-06-25': 391,\n u'2012-06-26': 391,\n u'2012-06-27': 391,\n u'2012-06-28': 391,\n u'2012-06-29': 391,\n u'2012-06-30': 391,\n u'2012-07-01': 391,\n u'2012-07-02': 392,\n u'2012-07-03': 392,\n u'2012-07-04': 392,\n u'2012-07-05': 392,\n u'2012-07-06': 392}\n"", '     Date         DateValue\n0    2012-07-01    391\n1    2012-07-02    392\n2    2012-07-03    392\n.    2012-07-04    392\n.    ...           ...\n.    ...           ...\n', 's  = Series(my_dict,index=my_dict.keys())\n']";"[""{u'2012-06-08': 388,\n u'2012-06-09': 388,\n u'2012-06-10': 388,\n u'2012-06-11': 389,\n u'2012-06-12': 389,\n u'2012-06-13': 389,\n u'2012-06-14': 389,\n u'2012-06-15': 389,\n u'2012-06-16': 389,\n u'2012-06-17': 389,\n u'2012-06-18': 390,\n u'2012-06-19': 390,\n u'2012-06-20': 390,\n u'2012-06-21': 390,\n u'2012-06-22': 390,\n u'2012-06-23': 390,\n u'2012-06-24': 390,\n u'2012-06-25': 391,\n u'2012-06-26': 391,\n u'2012-06-27': 391,\n u'2012-06-28': 391,\n u'2012-06-29': 391,\n u'2012-06-30': 391,\n u'2012-07-01': 391,\n u'2012-07-02': 392,\n u'2012-07-03': 392,\n u'2012-07-04': 392,\n u'2012-07-05': 392,\n u'2012-07-06': 392}\n"", '     Date         DateValue\n0    2012-07-01    391\n1    2012-07-02    392\n2    2012-07-03    392\n.    2012-07-04    392\n.    ...           ...\n.    ...           ...\n', 's  = Series(my_dict,index=my_dict.keys())\n']"
260;3.0;1;18876022;;1;25;<python><html><pandas><ipython>;How to format IPython html display of Pandas dataframe?;"<p>How can I format IPython html display of pandas dataframes so that</p>

<ol>
<li>numbers are right justified</li>
<li>numbers have commas as thousands separator</li>
<li>large floats have no decimal places</li>
</ol>

<p>I understand that <code>numpy</code> has the facility of <code>set_printoptions</code> where I can do:</p>

<pre><code>int_frmt:lambda x : '{:,}'.format(x)
np.set_printoptions(formatter={'int_kind':int_frmt})
</code></pre>

<p>and similarly for other data types. </p>

<p>But IPython does not pick up these formatting options when displaying dataframes in html. I still need to have</p>

<pre><code>pd.set_option('display.notebook_repr_html', True)
</code></pre>

<p>but with 1, 2, 3 as in above.</p>

<p><strong>Edit:</strong> Below is my solution for 2 &amp; 3 ( not sure this is the best way ), but I still need to figure out how to make number columns right justified.</p>

<pre><code>from IPython.display import HTML
int_frmt = lambda x: '{:,}'.format(x)
float_frmt = lambda x: '{:,.0f}'.format(x) if x &gt; 1e3 else '{:,.2f}'.format(x)
frmt_map = {np.dtype('int64'):int_frmt, np.dtype('float64'):float_frmt}
frmt = {col:frmt_map[df.dtypes[col]] for col in df.columns if df.dtypes[col] in frmt_map.keys()}
HTML(df.to_html(formatters=frmt))
</code></pre>
";11860.0;"[""int_frmt:lambda x : '{:,}'.format(x)\nnp.set_printoptions(formatter={'int_kind':int_frmt})\n"", ""pd.set_option('display.notebook_repr_html', True)\n"", ""from IPython.display import HTML\nint_frmt = lambda x: '{:,}'.format(x)\nfloat_frmt = lambda x: '{:,.0f}'.format(x) if x > 1e3 else '{:,.2f}'.format(x)\nfrmt_map = {np.dtype('int64'):int_frmt, np.dtype('float64'):float_frmt}\nfrmt = {col:frmt_map[df.dtypes[col]] for col in df.columns if df.dtypes[col] in frmt_map.keys()}\nHTML(df.to_html(formatters=frmt))\n""]";"['numpy', 'set_printoptions', ""int_frmt:lambda x : '{:,}'.format(x)\nnp.set_printoptions(formatter={'int_kind':int_frmt})\n"", ""pd.set_option('display.notebook_repr_html', True)\n"", ""from IPython.display import HTML\nint_frmt = lambda x: '{:,}'.format(x)\nfloat_frmt = lambda x: '{:,.0f}'.format(x) if x > 1e3 else '{:,.2f}'.format(x)\nfrmt_map = {np.dtype('int64'):int_frmt, np.dtype('float64'):float_frmt}\nfrmt = {col:frmt_map[df.dtypes[col]] for col in df.columns if df.dtypes[col] in frmt_map.keys()}\nHTML(df.to_html(formatters=frmt))\n""]"
261;3.0;0;18885175;;1;29;<python><zip><pandas>;Read a zipped file as a pandas DataFrame;"<p>I'm trying to unzip a csv file and pass it into pandas so I can work on the file.<br>
The code I have tried so far is: </p>

<pre><code>import requests, zipfile, StringIO
r = requests.get('http://data.octo.dc.gov/feeds/crime_incidents/archive/crime_incidents_2013_CSV.zip')
z = zipfile.ZipFile(StringIO.StringIO(r.content))
crime2013 = pandas.read_csv(z.read('crime_incidents_2013_CSV.csv'))
</code></pre>

<p><em>After the last line, although python is able to get the file, I get a ""does not exist"" at the end of the error.</em></p>

<p>Can someone tell me what I'm doing incorrectly?</p>
";18856.0;"[""import requests, zipfile, StringIO\nr = requests.get('http://data.octo.dc.gov/feeds/crime_incidents/archive/crime_incidents_2013_CSV.zip')\nz = zipfile.ZipFile(StringIO.StringIO(r.content))\ncrime2013 = pandas.read_csv(z.read('crime_incidents_2013_CSV.csv'))\n""]";"[""import requests, zipfile, StringIO\nr = requests.get('http://data.octo.dc.gov/feeds/crime_incidents/archive/crime_incidents_2013_CSV.zip')\nz = zipfile.ZipFile(StringIO.StringIO(r.content))\ncrime2013 = pandas.read_csv(z.read('crime_incidents_2013_CSV.csv'))\n""]"
262;3.0;3;18889588;;1;21;<python><pandas><dummy-data><categorical-data>;Create dummies from column with multiple values in pandas;"<p>I am looking for for a pythonic way to handle the following problem.</p>

<p>The <code>pandas.get_dummies()</code> method is great to create dummies from a categorical column of a dataframe. For example, if the column has values in <code>['A', 'B']</code>, <code>get_dummies()</code> creates 2 dummy variables and assigns 0 or 1 accordingly.</p>

<p>Now, I need to handle this situation. A single column, let's call it 'label', has values like <code>['A', 'B', 'C', 'D', 'A*C', 'C*D']</code> . <code>get_dummies()</code> creates 6 dummies, but I only want 4 of them, so that a row could have multiple 1s. </p>

<p>Is there a way to handle this in a pythonic way? I could only think of some step-by-step algorithm to get it, but that would not include get_dummies(). 
Thanks</p>

<p>Edited, hope it is more clear!</p>
";14589.0;[];"['pandas.get_dummies()', ""['A', 'B']"", 'get_dummies()', ""['A', 'B', 'C', 'D', 'A*C', 'C*D']"", 'get_dummies()']"
263;1.0;1;18942506;;1;41;<python><pandas><dataframe>;Add new column in Pandas DataFrame Python;"<p>I have dataframe in Pandas for example:</p>

<pre><code>Col1 Col2
A     1 
B     2
C     3
</code></pre>

<p>Now if I would like to add one more column named Col3 and the value is based on Col2. In formula, if Col2 > 1, then Col3 is 0, otherwise would be 1. So, in the example above. The output would be:</p>

<pre><code>Col1 Col2 Col3
A    1    1
B    2    0
C    3    0
</code></pre>

<p>Any idea on how to achieve this?</p>
";83369.0;['Col1 Col2\nA     1 \nB     2\nC     3\n', 'Col1 Col2 Col3\nA    1    1\nB    2    0\nC    3    0\n'];['Col1 Col2\nA     1 \nB     2\nC     3\n', 'Col1 Col2 Col3\nA    1    1\nB    2    0\nC    3    0\n']
264;3.0;1;18973404;;1;34;<python><matplotlib><pandas><bar-chart>;Setting Different Bar color in matplotlib Python;"<p>Supposely, I have the bar chart as below:</p>

<p><img src=""https://i.stack.imgur.com/wy1lP.png"" alt=""BarPlot""></p>

<p>Any ideas on how to set different colors for each carrier? As for example, AK would be Red, GA would be Green, etc?</p>

<p>I am using Pandas and matplotlib in Python</p>

<pre><code>&gt;&gt;&gt; f=plt.figure()
&gt;&gt;&gt; ax=f.add_subplot(1,1,1)
&gt;&gt;&gt; ax.bar([1,2,3,4], [1,2,3,4])
&lt;Container object of 4 artists&gt;
&gt;&gt;&gt; ax.get_children()
[&lt;matplotlib.axis.XAxis object at 0x6529850&gt;, &lt;matplotlib.axis.YAxis object at 0x78460d0&gt;,  &lt;matplotlib.patches.Rectangle object at 0x733cc50&gt;, &lt;matplotlib.patches.Rectangle object at 0x733cdd0&gt;, &lt;matplotlib.patches.Rectangle object at 0x777f290&gt;, &lt;matplotlib.patches.Rectangle object at 0x777f710&gt;, &lt;matplotlib.text.Text object at 0x7836450&gt;, &lt;matplotlib.patches.Rectangle object at 0x7836390&gt;, &lt;matplotlib.spines.Spine object at 0x6529950&gt;, &lt;matplotlib.spines.Spine object at 0x69aef50&gt;, &lt;matplotlib.spines.Spine object at 0x69ae310&gt;, &lt;matplotlib.spines.Spine object at 0x69aea50&gt;]
&gt;&gt;&gt; ax.get_children()[2].set_color('r') #You can also try to locate the first patches.Rectangle object instead of direct calling the index.
</code></pre>

<p>For the suggestions above, how do exactly we could enumerate ax.get_children() and check if the object type is rectangle? So if the object is rectangle, we would assign different random color?</p>
";46910.0;"["">>> f=plt.figure()\n>>> ax=f.add_subplot(1,1,1)\n>>> ax.bar([1,2,3,4], [1,2,3,4])\n<Container object of 4 artists>\n>>> ax.get_children()\n[<matplotlib.axis.XAxis object at 0x6529850>, <matplotlib.axis.YAxis object at 0x78460d0>,  <matplotlib.patches.Rectangle object at 0x733cc50>, <matplotlib.patches.Rectangle object at 0x733cdd0>, <matplotlib.patches.Rectangle object at 0x777f290>, <matplotlib.patches.Rectangle object at 0x777f710>, <matplotlib.text.Text object at 0x7836450>, <matplotlib.patches.Rectangle object at 0x7836390>, <matplotlib.spines.Spine object at 0x6529950>, <matplotlib.spines.Spine object at 0x69aef50>, <matplotlib.spines.Spine object at 0x69ae310>, <matplotlib.spines.Spine object at 0x69aea50>]\n>>> ax.get_children()[2].set_color('r') #You can also try to locate the first patches.Rectangle object instead of direct calling the index.\n""]";"["">>> f=plt.figure()\n>>> ax=f.add_subplot(1,1,1)\n>>> ax.bar([1,2,3,4], [1,2,3,4])\n<Container object of 4 artists>\n>>> ax.get_children()\n[<matplotlib.axis.XAxis object at 0x6529850>, <matplotlib.axis.YAxis object at 0x78460d0>,  <matplotlib.patches.Rectangle object at 0x733cc50>, <matplotlib.patches.Rectangle object at 0x733cdd0>, <matplotlib.patches.Rectangle object at 0x777f290>, <matplotlib.patches.Rectangle object at 0x777f710>, <matplotlib.text.Text object at 0x7836450>, <matplotlib.patches.Rectangle object at 0x7836390>, <matplotlib.spines.Spine object at 0x6529950>, <matplotlib.spines.Spine object at 0x69aef50>, <matplotlib.spines.Spine object at 0x69ae310>, <matplotlib.spines.Spine object at 0x69aea50>]\n>>> ax.get_children()[2].set_color('r') #You can also try to locate the first patches.Rectangle object instead of direct calling the index.\n""]"
265;1.0;0;18992086;;1;31;<python><pandas><histogram>;save a pandas.Series histogram plot to file;"<p>In ipython Notebook, first create a pandas Series object, then by calling the instance method .hist(), the browser displays the figure. </p>

<p>I am wondering how to save this figure to a file (I mean not by right click and save as, but the commands needed in the script).</p>
";26802.0;[];[]
266;3.0;1;19078325;;1;22;<python><group-by><pandas><aggregate-functions>;Naming returned columns in Pandas aggregate function?;"<p>I'm having trouble with Pandas' groupby functionality. I've read <a href=""http://pandas.pydata.org/pandas-docs/dev/groupby.html"" rel=""noreferrer"">the documentation</a>, but I can't see to figure out how to apply aggregate functions to multiple columns <em>and</em> have custom names for those columns.</p>

<p>This comes very close, but the data structure returned has nested column headings:</p>

<pre><code>data.groupby(""Country"").agg(
        {""column1"": {""foo"": sum()}, ""column2"": {""mean"": np.mean, ""std"": np.std}})
</code></pre>

<p>(ie. I want to take the mean and std of column2, but return those columns as ""mean"" and ""std"")</p>

<p>What am I missing?</p>
";15533.0;"['data.groupby(""Country"").agg(\n        {""column1"": {""foo"": sum()}, ""column2"": {""mean"": np.mean, ""std"": np.std}})\n']";"['data.groupby(""Country"").agg(\n        {""column1"": {""foo"": sum()}, ""column2"": {""mean"": np.mean, ""std"": np.std}})\n']"
267;2.0;0;19112398;;1;65;<python><pandas><datanitro>;Getting list of lists into pandas DataFrame;"<p>I am reading contents of a spreadsheet into pandas.   DataNitro has a method that returns a rectangular selection of cells as a list of lists.   So</p>

<pre><code>table = Cell(""A1"").table
</code></pre>

<p>gives</p>

<pre><code>table = [['Heading1', 'Heading2'], [1 , 2], [3, 4]]

headers = table.pop(0) # gives the headers as list and leaves data
</code></pre>

<p>I am busy writing code to translate this, but my guess is that it is such a simple use that there must be method to do this.    Cant seem to find it in documentation.   Any pointers to the method that would simplify this?</p>
";72639.0;"['table = Cell(""A1"").table\n', ""table = [['Heading1', 'Heading2'], [1 , 2], [3, 4]]\n\nheaders = table.pop(0) # gives the headers as list and leaves data\n""]";"['table = Cell(""A1"").table\n', ""table = [['Heading1', 'Heading2'], [1 , 2], [3, 4]]\n\nheaders = table.pop(0) # gives the headers as list and leaves data\n""]"
268;4.0;4;19124601;;1;138;<python><pandas><dataframe>;Is there a way to (pretty) print the entire Pandas Series / DataFrame?;"<p>I work with Series and DataFrames on the terminal a lot. The default <code>__repr__</code> for a Series returns a reducted sample, with some head and tail values, but the rest missing.</p>

<p>Is there a builtin way to pretty-print the entire Series / DataFrame?  Ideally, it would support proper alignment, perhaps borders between columns, and maybe even color-coding for the different columns.</p>
";94738.0;[];['__repr__']
269;2.0;4;19125091;;1;22;<python><pandas>;Pandas Merge - How to avoid duplicating columns;"<p>I am attempting a merge between two data frames.  Each data frame has two index levels (date, cusip).  In the columns, some columns match between the two (currency, adj date) for example.</p>

<p>What is the best way to merge these by index, but to not take two copies of currency and adj date.</p>

<p>Each data frame is 90 columns, so I am trying to avoid writing everything out by hand.</p>

<pre><code>df:                 currency  adj_date   data_col1 ...
date        cusip
2012-01-01  XSDP      USD      2012-01-03   0.45
...

df2:                currency  adj_date   data_col2 ...
date        cusip
2012-01-01  XSDP      USD      2012-01-03   0.45
...
</code></pre>

<p>If I do:</p>

<pre><code>dfNew = merge(df, df2, left_index=True, right_index=True, how='outer')
</code></pre>

<p>I get </p>

<pre><code>dfNew:              currency_x  adj_date_x   data_col2 ... currency_y adj_date_y
date        cusip
2012-01-01  XSDP      USD      2012-01-03   0.45             USD         2012-01-03
</code></pre>

<p>Thank you!
    ...</p>
";21524.0;"['df:                 currency  adj_date   data_col1 ...\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45\n...\n\ndf2:                currency  adj_date   data_col2 ...\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45\n...\n', ""dfNew = merge(df, df2, left_index=True, right_index=True, how='outer')\n"", 'dfNew:              currency_x  adj_date_x   data_col2 ... currency_y adj_date_y\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45             USD         2012-01-03\n']";"['df:                 currency  adj_date   data_col1 ...\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45\n...\n\ndf2:                currency  adj_date   data_col2 ...\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45\n...\n', ""dfNew = merge(df, df2, left_index=True, right_index=True, how='outer')\n"", 'dfNew:              currency_x  adj_date_x   data_col2 ... currency_y adj_date_y\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45             USD         2012-01-03\n']"
270;2.0;0;19155718;;1;23;<python><pandas>;Select Pandas rows based on list index;"<p>I have a dataframe df :</p>

<pre><code>   20060930  10.103       NaN     10.103   7.981
   20061231  15.915       NaN     15.915  12.686
   20070331   3.196       NaN      3.196   2.710
   20070630   7.907       NaN      7.907   6.459
</code></pre>

<p>Then I want to select rows with certain sequence numbers which indicated in a list, suppose here is [1,3], then left:</p>

<pre><code>   20061231  15.915       NaN     15.915  12.686
   20070630   7.907       NaN      7.907   6.459
</code></pre>

<p>How or what function can do that ?</p>
";38383.0;['   20060930  10.103       NaN     10.103   7.981\n   20061231  15.915       NaN     15.915  12.686\n   20070331   3.196       NaN      3.196   2.710\n   20070630   7.907       NaN      7.907   6.459\n', '   20061231  15.915       NaN     15.915  12.686\n   20070630   7.907       NaN      7.907   6.459\n'];['   20060930  10.103       NaN     10.103   7.981\n   20061231  15.915       NaN     15.915  12.686\n   20070331   3.196       NaN      3.196   2.710\n   20070630   7.907       NaN      7.907   6.459\n', '   20061231  15.915       NaN     15.915  12.686\n   20070630   7.907       NaN      7.907   6.459\n']
271;2.0;0;19213789;;1;44;<python><matplotlib><plot><pandas>;How do you plot a vertical line on a time series plot in Pandas?;"<p>How do you plot a vertical line (vlines) in a Pandas series plot?  I am using Pandas to plot rolling means, etc and would like to mark important positions with a vertical line.  Is it possible to use vlines or something similar to accomplish this?  If so, could someone please provide an example?  In this case, the x axis is date-time.</p>
";39922.0;[];[]
272;2.0;0;19231871;;1;30;<python><pandas><unix-timestamp><dataframe>;Convert unix time to readable date in pandas DataFrame;"<p>I have a data frame with unix times and prices in it. I want to convert the index column so that it shows in human readable dates. So for instance i have ""date"" as 1349633705 in the index column but I'd want it to show as 10/07/2012 (or at least 10/07/2012 18:15). For some context, here is the code I'm working with and what I've tried already:</p>

<pre><code>import json
import urllib2
from datetime import datetime
response = urllib2.urlopen('http://blockchain.info/charts/market-price?&amp;format=json')
data = json.load(response)   
df = DataFrame(data['values'])
df.columns = [""date"",""price""]
#convert dates 
df.date = df.date.apply(lambda d: datetime.strptime(d, ""%Y-%m-%d""))
df.index = df.date   
df
</code></pre>

<p>As you can see I'm using
<code>df.date = df.date.apply(lambda d: datetime.strptime(d, ""%Y-%m-%d""))</code> here which doesn't work since I'm working with integers, not strings. I think I need to use <code>datetime.date.fromtimestamp</code> but I'm not quite sure how to apply this to the whole of df.date. Thanks.</p>
";24317.0;"['import json\nimport urllib2\nfrom datetime import datetime\nresponse = urllib2.urlopen(\'http://blockchain.info/charts/market-price?&format=json\')\ndata = json.load(response)   \ndf = DataFrame(data[\'values\'])\ndf.columns = [""date"",""price""]\n#convert dates \ndf.date = df.date.apply(lambda d: datetime.strptime(d, ""%Y-%m-%d""))\ndf.index = df.date   \ndf\n']";"['import json\nimport urllib2\nfrom datetime import datetime\nresponse = urllib2.urlopen(\'http://blockchain.info/charts/market-price?&format=json\')\ndata = json.load(response)   \ndf = DataFrame(data[\'values\'])\ndf.columns = [""date"",""price""]\n#convert dates \ndf.date = df.date.apply(lambda d: datetime.strptime(d, ""%Y-%m-%d""))\ndf.index = df.date   \ndf\n', 'df.date = df.date.apply(lambda d: datetime.strptime(d, ""%Y-%m-%d""))', 'datetime.date.fromtimestamp']"
273;3.0;0;19237878;;1;30;<pandas><subset>;subsetting a Python DataFrame;"<p>I am transitioning from R to Python. I just began using Pandas. I have an R code that subsets nicely:</p>

<pre><code>k1 &lt;- subset(data, Product = p.id &amp; Month &lt; mn &amp; Year == yr, select = c(Time, Product))
</code></pre>

<p>Now, I want to do similar stuff in Python. this is what I have got so far:</p>

<pre><code>import pandas as pd
data = pd.read_csv(""../data/monthly_prod_sales.csv"")


#first, index the dataset by Product. And, get all that matches a given 'p.id' and time.
 data.set_index('Product')
 k = data.ix[[p.id, 'Time']]

# then, index this subset with Time and do more subsetting..
</code></pre>

<p>I am beginning to feel that I am doing this the wrong way. perhaps, there is an elegant solution. Can anyone help? I need to extract month and year from the timestamp I have and do subsetting. Perhaps there is a one-liner that will accomplish all this:</p>

<pre><code>k1 &lt;- subset(data, Product = p.id &amp; Time &gt;= start_time &amp; Time &lt; end_time, select = c(Time, Product))
</code></pre>

<p>thanks.</p>
";71327.0;"['k1 <- subset(data, Product = p.id & Month < mn & Year == yr, select = c(Time, Product))\n', 'import pandas as pd\ndata = pd.read_csv(""../data/monthly_prod_sales.csv"")\n\n\n#first, index the dataset by Product. And, get all that matches a given \'p.id\' and time.\n data.set_index(\'Product\')\n k = data.ix[[p.id, \'Time\']]\n\n# then, index this subset with Time and do more subsetting..\n', 'k1 <- subset(data, Product = p.id & Time >= start_time & Time < end_time, select = c(Time, Product))\n']";"['k1 <- subset(data, Product = p.id & Month < mn & Year == yr, select = c(Time, Product))\n', 'import pandas as pd\ndata = pd.read_csv(""../data/monthly_prod_sales.csv"")\n\n\n#first, index the dataset by Product. And, get all that matches a given \'p.id\' and time.\n data.set_index(\'Product\')\n k = data.ix[[p.id, \'Time\']]\n\n# then, index this subset with Time and do more subsetting..\n', 'k1 <- subset(data, Product = p.id & Time >= start_time & Time < end_time, select = c(Time, Product))\n']"
274;5.0;0;19324453;;1;32;<python><date><plot><pandas><dataframe>;Add missing dates to pandas dataframe;"<p>My data can have multiple events on a given date or NO events on a date. I take these events, get a count by date and plot them.  However, when I plot them, my two series don't always match.</p>

<pre><code>idx = pd.date_range(df['simpleDate'].min(), df['simpleDate'].max())
s = df.groupby(['simpleDate']).size()
</code></pre>

<p>In the above code <strong>idx</strong> becomes a range of say 30 dates. 09-01-2013 to 09-30-2013
However <strong>S</strong> may only have 25 or 26 days because no events happened for a given date. I then get an AssertionError as the sizes dont match.</p>

<p>What's the proper way to tackle this? Do I want to remove dates with no values from <strong>IDX</strong> or (which I'd rather do) is add to the series the missing date with a count of 0. I'd rather have a full graph of 30 days with 0 values. If this approach is right, any suggestions on how to get started? Do I need some sort of dynamic <code>reindex</code> function?</p>

<p>Here's a snippet of <strong>S</strong> ( <code>df.groupby(['simpleDate']).size()</code>  ), notice no entries for 04 and 05.</p>

<pre><code>09-02-2013     2
09-03-2013    10
09-06-2013     5
09-07-2013     1
</code></pre>
";17676.0;"[""idx = pd.date_range(df['simpleDate'].min(), df['simpleDate'].max())\ns = df.groupby(['simpleDate']).size()\n"", '09-02-2013     2\n09-03-2013    10\n09-06-2013     5\n09-07-2013     1\n']";"[""idx = pd.date_range(df['simpleDate'].min(), df['simpleDate'].max())\ns = df.groupby(['simpleDate']).size()\n"", 'reindex', ""df.groupby(['simpleDate']).size()"", '09-02-2013     2\n09-03-2013    10\n09-06-2013     5\n09-07-2013     1\n']"
275;3.0;0;19365513;;1;22;<python><pandas>;How to add an extra row to a pandas dataframe;"<p>If I have an empty dataframe as such:</p>

<pre><code>columns = ['Date', 'Name', 'Action','ID']
df = pd.DataFrame(columns=columns) 
</code></pre>

<p>is there a way to append a new row to this newly created dataframe? Currently I have to create a dictionary, populate it, then append the dictionary to the dataframe at the end. Is there a more direct way? </p>
";80190.0;"[""columns = ['Date', 'Name', 'Action','ID']\ndf = pd.DataFrame(columns=columns) \n""]";"[""columns = ['Date', 'Name', 'Action','ID']\ndf = pd.DataFrame(columns=columns) \n""]"
276;8.0;1;19377969;;1;103;<python><pandas><dataframe>;Combine two columns of text in dataframe in pandas/python;"<p>I have a 20 x 4000 dataframe in python using pandas. Two of these columns are named Year and quarter. I'd like to create a variable called period that makes Year = 2000 and quarter= q2 into 2000q2</p>

<p>Can anyone help with that?</p>
";112188.0;[];[]
277;2.0;0;19384532;;1;93;<python><group-by><pandas><distinct>;How to count number of rows in a group in pandas group by object?;"<p>I have a data frame <code>df</code> and I use several columns from it to <code>groupby</code>:</p>

<pre><code>df['col1','col2','col3','col4'].groupby(['col1','col2']).mean()
</code></pre>

<p>In the above way I almost get the table (data frame) that I need. What is missing is an additional column that contains number of rows in each group. In other words, I have mean but I also would like to know how many number were used to get these means. For example in the first group there are 8 values and in the second one 10 and so on.</p>
";144711.0;"[""df['col1','col2','col3','col4'].groupby(['col1','col2']).mean()\n""]";"['df', 'groupby', ""df['col1','col2','col3','col4'].groupby(['col1','col2']).mean()\n""]"
278;12.0;1;19482970;;1;304;<python><pandas><dataframe>;Get list from pandas DataFrame column headers;"<p>I want to get a list of the column headers from a pandas DataFrame.  The DataFrame will come from user input so I won't know how many columns there will be or what they will be called.</p>

<p>For example, if I'm given a DataFrame like this:</p>

<pre><code>&gt;&gt;&gt; my_dataframe
    y  gdp  cap
0   1    2    5
1   2    3    9
2   8    7    2
3   3    4    7
4   6    7    7
5   4    8    3
6   8    2    8
7   9    9   10
8   6    6    4
9  10   10    7
</code></pre>

<p>I would want to get a list like this:</p>

<pre><code>&gt;&gt;&gt; header_list
[y, gdp, cap]
</code></pre>
";438665.0;['>>> my_dataframe\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n5   4    8    3\n6   8    2    8\n7   9    9   10\n8   6    6    4\n9  10   10    7\n', '>>> header_list\n[y, gdp, cap]\n'];['>>> my_dataframe\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n5   4    8    3\n6   8    2    8\n7   9    9   10\n8   6    6    4\n9  10   10    7\n', '>>> header_list\n[y, gdp, cap]\n']
279;5.0;0;19530568;;1;25;<python><pandas>;Can pandas groupby aggregate into a list, rather than sum, mean, etc?;"<p>I've had success using the groupby function to sum or average a given variable by groups, but is there a way to aggregate into a list of values, rather than to get a single result? (And would this still be called aggregation?) </p>

<p>I am not entirely sure this is the approach I should be taking anyhow, so below is an example of the transformation I'd like to make, with toy data. </p>

<p>That is, if the data look something like this:</p>

<pre><code>    A    B    C  
    1    10   22
    1    12   20
    1    11   8
    1    10   10
    2    11   13
    2    12   10 
    3    14   0
</code></pre>

<p>What I am trying to end up with is something like the following. I am not totally sure whether this can be done through groupby aggregating into lists, and am rather lost as to where to go from here. </p>

<p>Hypothetical output:</p>

<pre><code>     A    B    C  New1  New2  New3  New4  New5  New6
    1    10   22  12    20    11    8     10    10
    2    11   13  12    10 
    3    14   0
</code></pre>

<p>Perhaps I should be pursuing pivots instead? The order by which the data are put into columns does not matter - all columns B through New6 in this example are equivalent. All suggestions/corrections are much appreciated.</p>
";16908.0;['    A    B    C  \n    1    10   22\n    1    12   20\n    1    11   8\n    1    10   10\n    2    11   13\n    2    12   10 \n    3    14   0\n', '     A    B    C  New1  New2  New3  New4  New5  New6\n    1    10   22  12    20    11    8     10    10\n    2    11   13  12    10 \n    3    14   0\n'];['    A    B    C  \n    1    10   22\n    1    12   20\n    1    11   8\n    1    10   10\n    2    11   13\n    2    12   10 \n    3    14   0\n', '     A    B    C  New1  New2  New3  New4  New5  New6\n    1    10   22  12    20    11    8     10    10\n    2    11   13  12    10 \n    3    14   0\n']
280;3.0;2;19555525;;1;28;<python><matplotlib><pandas>;Saving plots (AxesSubPlot) generated from python pandas with matplotlib's savefig;"<p>I'm using pandas to generate a plot from a dataframe, which I would like to save to a file:</p>

<pre><code>dtf = pd.DataFrame.from_records(d,columns=h)
fig = plt.figure()
ax = dtf2.plot()
ax = fig.add_subplot(ax)
fig.savefig('~/Documents/output.png')
</code></pre>

<p>It seems like the last line, using matplotlib's savefig, should do the trick.  But that code produces the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""./testgraph.py"", line 76, in &lt;module&gt;
    ax = fig.add_subplot(ax)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/figure.py"", line 890, in add_subplot
    assert(a.get_figure() is self)
AssertionError
</code></pre>

<p>Alternatively, trying to call savefig directly on the plot also errors out:</p>

<pre><code>dtf2.plot().savefig('~/Documents/output.png')


  File ""./testgraph.py"", line 79, in &lt;module&gt;
    dtf2.plot().savefig('~/Documents/output.png')
AttributeError: 'AxesSubplot' object has no attribute 'savefig'
</code></pre>

<p>I think I need to somehow add the subplot returned by plot() to a figure in order to use savefig.  I also wonder if perhaps this has to do with the <a href=""https://stackoverflow.com/questions/11690597/there-is-a-class-matplotlib-axes-axessubplot-but-the-module-matplotlib-axes-has"">magic</a> behind the AxesSubPlot class.</p>

<p>EDIT: </p>

<p>the following works (raising no error), but leaves me with a blank page image....</p>

<pre><code>fig = plt.figure()
dtf2.plot()
fig.savefig('output.png')
</code></pre>
";25060.0;"[""dtf = pd.DataFrame.from_records(d,columns=h)\nfig = plt.figure()\nax = dtf2.plot()\nax = fig.add_subplot(ax)\nfig.savefig('~/Documents/output.png')\n"", 'Traceback (most recent call last):\n  File ""./testgraph.py"", line 76, in <module>\n    ax = fig.add_subplot(ax)\n  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/figure.py"", line 890, in add_subplot\n    assert(a.get_figure() is self)\nAssertionError\n', 'dtf2.plot().savefig(\'~/Documents/output.png\')\n\n\n  File ""./testgraph.py"", line 79, in <module>\n    dtf2.plot().savefig(\'~/Documents/output.png\')\nAttributeError: \'AxesSubplot\' object has no attribute \'savefig\'\n', ""fig = plt.figure()\ndtf2.plot()\nfig.savefig('output.png')\n""]";"[""dtf = pd.DataFrame.from_records(d,columns=h)\nfig = plt.figure()\nax = dtf2.plot()\nax = fig.add_subplot(ax)\nfig.savefig('~/Documents/output.png')\n"", 'Traceback (most recent call last):\n  File ""./testgraph.py"", line 76, in <module>\n    ax = fig.add_subplot(ax)\n  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/figure.py"", line 890, in add_subplot\n    assert(a.get_figure() is self)\nAssertionError\n', 'dtf2.plot().savefig(\'~/Documents/output.png\')\n\n\n  File ""./testgraph.py"", line 79, in <module>\n    dtf2.plot().savefig(\'~/Documents/output.png\')\nAttributeError: \'AxesSubplot\' object has no attribute \'savefig\'\n', ""fig = plt.figure()\ndtf2.plot()\nfig.savefig('output.png')\n""]"
281;3.0;0;19584029;;1;27;<python><pandas><histogram>;Plotting histograms from grouped data in a pandas DataFrame;"<p>I need some guidance in working out how to plot a block of histograms from grouped data in a pandas dataframe. Here's an example to illustrate my question:</p>

<pre><code>from pandas import DataFrame
import numpy as np
x = ['A']*300 + ['B']*400 + ['C']*300
y = np.random.randn(1000)
df = DataFrame({'Letter':x, 'N':y})
grouped = df.groupby('Letter')
</code></pre>

<p>In my ignorance I tried this code command:</p>

<pre><code>df.groupby('Letter').hist()
</code></pre>

<p>which failed with the error message ""TypeError: cannot concatenate 'str' and 'float' objects""</p>

<p>Any help most appreciated.</p>
";47761.0;"[""from pandas import DataFrame\nimport numpy as np\nx = ['A']*300 + ['B']*400 + ['C']*300\ny = np.random.randn(1000)\ndf = DataFrame({'Letter':x, 'N':y})\ngrouped = df.groupby('Letter')\n"", ""df.groupby('Letter').hist()\n""]";"[""from pandas import DataFrame\nimport numpy as np\nx = ['A']*300 + ['B']*400 + ['C']*300\ny = np.random.randn(1000)\ndf = DataFrame({'Letter':x, 'N':y})\ngrouped = df.groupby('Letter')\n"", ""df.groupby('Letter').hist()\n""]"
282;3.0;0;19585280;;1;21;<python><pandas>;Convert a row in pandas into list;"<p>I have a pandas data frame like this:  </p>

<pre><code>admit   gpa  gre  rank   
0  3.61  380     3  
1  3.67  660     3  
1  3.19  640     4  
0  2.93  520     4
</code></pre>

<p>Now I want to get a list of rows in pandas like:  </p>

<pre><code>[[0,3.61,380,3], [1,3.67,660,3], [1,3.19,640,4], [0,2.93,520,4]]   
</code></pre>

<p>How can I do it? please help me. 
Thanks a lot. </p>
";21327.0;['admit   gpa  gre  rank   \n0  3.61  380     3  \n1  3.67  660     3  \n1  3.19  640     4  \n0  2.93  520     4\n', '[[0,3.61,380,3], [1,3.67,660,3], [1,3.19,640,4], [0,2.93,520,4]]   \n'];['admit   gpa  gre  rank   \n0  3.61  380     3  \n1  3.67  660     3  \n1  3.19  640     4  \n0  2.93  520     4\n', '[[0,3.61,380,3], [1,3.67,660,3], [1,3.19,640,4], [0,2.93,520,4]]   \n']
283;2.0;0;19611729;;1;23;<python><pandas><google-spreadsheet><google-apps>;Getting Google Spreadsheet CSV into A Pandas Dataframe;"<p>I uploaded a file to Google spreadsheets (to make a publically accessible example IPython Notebook, with data) I was using the file in it's native form could be read into a Pandas Dataframe. So now I use the following code to read the spreadsheet, works fine but just comes in as string,, and I'm not having any luck trying to get it back into a dataframe (you can get the data)</p>

<pre><code>import requests
r = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&amp;output=csv')
data = r.content
</code></pre>

<p>The data ends up looking like: (1st row headers)</p>

<pre><code>',City,region,Res_Comm,mkt_type,Quradate,National_exp,Alabama_exp,Sales_exp,Inventory_exp,Price_exp,Credit_exp\n0,Dothan,South_Central-Montgomery-Auburn-Wiregrass-Dothan,Residential,Rural,1/15/2010,2,2,3,2,3,3\n10,Foley,South_Mobile-Baldwin,Residential,Suburban_Urban,1/15/2010,4,4,4,4,4,3\n12,Birmingham,North_Central-Birmingham-Tuscaloosa-Anniston,Commercial,Suburban_Urban,1/15/2010,2,2,3,2,2,3\n
</code></pre>

<p>The native pandas code that brings in the disk resident file looks like:</p>

<pre><code>df = pd.io.parsers.read_csv('/home/tom/Dropbox/Projects/annonallanswerswithmaster1012013.csv',index_col=0,parse_dates=['Quradate'])
</code></pre>

<p>A ""clean"" solution would be helpful to many to provide an easy way to share datasets for Pandas use! I tried a bunch of alternative with no success and I'm pretty sure I'm missing something obvious again.</p>

<p>Just a Update note The new Google spreadsheet has a different URL pattern Just use this in place of the URL in the above example and or the below answer and you should be fine here is an example: </p>

<pre><code>https://docs.google.com/spreadsheets/d/177_dFZ0i-duGxLiyg6tnwNDKruAYE-_Dd8vAQziipJQ/export?format=csv&amp;id
</code></pre>

<p>see solution below from @Max Ghenis which just used pd.read_csv, no need for StringIO or requests...</p>
";7482.0;"[""import requests\nr = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&output=csv')\ndata = r.content\n"", ""',City,region,Res_Comm,mkt_type,Quradate,National_exp,Alabama_exp,Sales_exp,Inventory_exp,Price_exp,Credit_exp\\n0,Dothan,South_Central-Montgomery-Auburn-Wiregrass-Dothan,Residential,Rural,1/15/2010,2,2,3,2,3,3\\n10,Foley,South_Mobile-Baldwin,Residential,Suburban_Urban,1/15/2010,4,4,4,4,4,3\\n12,Birmingham,North_Central-Birmingham-Tuscaloosa-Anniston,Commercial,Suburban_Urban,1/15/2010,2,2,3,2,2,3\\n\n"", ""df = pd.io.parsers.read_csv('/home/tom/Dropbox/Projects/annonallanswerswithmaster1012013.csv',index_col=0,parse_dates=['Quradate'])\n"", 'https://docs.google.com/spreadsheets/d/177_dFZ0i-duGxLiyg6tnwNDKruAYE-_Dd8vAQziipJQ/export?format=csv&id\n']";"[""import requests\nr = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&output=csv')\ndata = r.content\n"", ""',City,region,Res_Comm,mkt_type,Quradate,National_exp,Alabama_exp,Sales_exp,Inventory_exp,Price_exp,Credit_exp\\n0,Dothan,South_Central-Montgomery-Auburn-Wiregrass-Dothan,Residential,Rural,1/15/2010,2,2,3,2,3,3\\n10,Foley,South_Mobile-Baldwin,Residential,Suburban_Urban,1/15/2010,4,4,4,4,4,3\\n12,Birmingham,North_Central-Birmingham-Tuscaloosa-Anniston,Commercial,Suburban_Urban,1/15/2010,2,2,3,2,2,3\\n\n"", ""df = pd.io.parsers.read_csv('/home/tom/Dropbox/Projects/annonallanswerswithmaster1012013.csv',index_col=0,parse_dates=['Quradate'])\n"", 'https://docs.google.com/spreadsheets/d/177_dFZ0i-duGxLiyg6tnwNDKruAYE-_Dd8vAQziipJQ/export?format=csv&id\n']"
284;3.0;0;19618912;;1;21;<python><python-2.7><pandas><dataframe><intersect>;Finding common rows (intersection) in two Pandas dataframes;"<p>Assume I have two dataframes of this format (call them <code>df1</code> and <code>df2</code>):</p>

<pre><code>+------------------------+------------------------+--------+
|        user_id         |      business_id       | rating |
+------------------------+------------------------+--------+
| rLtl8ZkDX5vH5nAx9C3q5Q | eIxSLxzIlfExI6vgAbn2JA |      4 |
| C6IOtaaYdLIT5fWd7ZYIuA | eIxSLxzIlfExI6vgAbn2JA |      5 |
| mlBC3pN9GXlUUfQi1qBBZA | KoIRdcIfh3XWxiCeV1BDmA |      3 |
+------------------------+------------------------+--------+
</code></pre>

<p>I'm looking to get a dataframe of all the rows that have a common <code>user_id</code> in <code>df1</code> and <code>df2</code>. (ie. if a <code>user_id</code> is in both <code>df1</code> and <code>df2</code>, include the two rows in the output dataframe)</p>

<p>I can think of many ways to approach this, but they all strike me as clunky. For example, we could find all the unique <code>user_id</code>s in each dataframe, create a set of each, find their intersection, filter the two dataframes with the resulting set and concatenate the two filtered dataframes.</p>

<p>Maybe that's the best approach, but I know Pandas is clever. Is there a simpler way to do this? I've looked at <code>merge</code> but I don't think that's what I need.</p>
";23593.0;['+------------------------+------------------------+--------+\n|        user_id         |      business_id       | rating |\n+------------------------+------------------------+--------+\n| rLtl8ZkDX5vH5nAx9C3q5Q | eIxSLxzIlfExI6vgAbn2JA |      4 |\n| C6IOtaaYdLIT5fWd7ZYIuA | eIxSLxzIlfExI6vgAbn2JA |      5 |\n| mlBC3pN9GXlUUfQi1qBBZA | KoIRdcIfh3XWxiCeV1BDmA |      3 |\n+------------------------+------------------------+--------+\n'];['df1', 'df2', '+------------------------+------------------------+--------+\n|        user_id         |      business_id       | rating |\n+------------------------+------------------------+--------+\n| rLtl8ZkDX5vH5nAx9C3q5Q | eIxSLxzIlfExI6vgAbn2JA |      4 |\n| C6IOtaaYdLIT5fWd7ZYIuA | eIxSLxzIlfExI6vgAbn2JA |      5 |\n| mlBC3pN9GXlUUfQi1qBBZA | KoIRdcIfh3XWxiCeV1BDmA |      3 |\n+------------------------+------------------------+--------+\n', 'user_id', 'df1', 'df2', 'user_id', 'df1', 'df2', 'user_id', 'merge']
285;2.0;1;19632075;;1;25;<python><pandas>;how to read file with space separated values;"<p>I try to read the file into pandas.
The file has values separated by space, but with different number of spaces
I tried:</p>

<pre><code>pd.read_csv('file.csv', delimiter=' ')
</code></pre>

<p>but it doesn't work</p>
";17660.0;"[""pd.read_csv('file.csv', delimiter=' ')\n""]";"[""pd.read_csv('file.csv', delimiter=' ')\n""]"
286;4.0;4;19726663;;1;27;<python><matplotlib><pandas>;How to save the Pandas dataframe/series data as a figure?;"<p>It sounds somewhat weird? but I need to save the Pandas console output string to png pics. For example:</p>

<pre><code>&gt;&gt;&gt; df
                   sales  net_pft     ROE    ROIC
STK_ID RPT_Date                                  
600809 20120331  22.1401   4.9253  0.1651  0.6656
       20120630  38.1565   7.8684  0.2567  1.0385
       20120930  52.5098  12.4338  0.3587  1.2867
       20121231  64.7876  13.2731  0.3736  1.2205
       20130331  27.9517   7.5182  0.1745  0.3723
       20130630  40.6460   9.8572  0.2560  0.4290
       20130930  53.0501  11.8605  0.2927  0.4369 
</code></pre>

<p>Is there any way like <code>df.output_as_png(filename='df_data.png')</code> to generate a pic file which just display above content inside?</p>
";22214.0;['>>> df\n                   sales  net_pft     ROE    ROIC\nSTK_ID RPT_Date                                  \n600809 20120331  22.1401   4.9253  0.1651  0.6656\n       20120630  38.1565   7.8684  0.2567  1.0385\n       20120930  52.5098  12.4338  0.3587  1.2867\n       20121231  64.7876  13.2731  0.3736  1.2205\n       20130331  27.9517   7.5182  0.1745  0.3723\n       20130630  40.6460   9.8572  0.2560  0.4290\n       20130930  53.0501  11.8605  0.2927  0.4369 \n'];"['>>> df\n                   sales  net_pft     ROE    ROIC\nSTK_ID RPT_Date                                  \n600809 20120331  22.1401   4.9253  0.1651  0.6656\n       20120630  38.1565   7.8684  0.2567  1.0385\n       20120930  52.5098  12.4338  0.3587  1.2867\n       20121231  64.7876  13.2731  0.3736  1.2205\n       20130331  27.9517   7.5182  0.1745  0.3723\n       20130630  40.6460   9.8572  0.2560  0.4290\n       20130930  53.0501  11.8605  0.2927  0.4369 \n', ""df.output_as_png(filename='df_data.png')""]"
287;3.0;0;19736080;;1;23;<python><pandas>;Creating dataframe from a dictionary where entries have different lengths;"<p>Say I have a dictionary with 10 key-value pairs. Each entry holds a numpy array. However, the length of the array is not the same for all of them.</p>

<p>How can I create a dataframe where each column holds a different entry?</p>

<p>When I try:</p>

<pre><code>pd.DataFrame(my_dict)
</code></pre>

<p>I get:</p>

<pre><code>ValueError: arrays must all be the same length
</code></pre>

<p>Any way to overcome this? I am happy to have Pandas use <code>NaN</code> to pad those columns for the shorter entries.</p>
";17482.0;['pd.DataFrame(my_dict)\n', 'ValueError: arrays must all be the same length\n'];['pd.DataFrame(my_dict)\n', 'ValueError: arrays must all be the same length\n', 'NaN']
288;2.0;2;19758364;;1;80;<python><pandas><dataframe>;python: rename single column header in pandas dataframe;"<p>I've got a dataframe called <code>data</code>. How would I rename the only one column header? For example <code>gdp</code> to <code>log(gdp)</code>?</p>

<pre><code>data =
    y  gdp  cap
0   1    2    5
1   2    3    9
2   8    7    2
3   3    4    7
4   6    7    7
5   4    8    3
6   8    2    8
7   9    9   10
8   6    6    4
9  10   10    7
</code></pre>
";50105.0;['data =\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n5   4    8    3\n6   8    2    8\n7   9    9   10\n8   6    6    4\n9  10   10    7\n'];['data', 'gdp', 'log(gdp)', 'data =\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n5   4    8    3\n6   8    2    8\n7   9    9   10\n8   6    6    4\n9  10   10    7\n']
289;6.0;3;19790790;;1;33;<python><split><pandas><dataframe>;Splitting dataframe into multiple dataframes;"<p>I have a very large dataframe (around 1 million rows) with data from an experiment (60 respondents).
I would like to split the dataframe into 60 dataframes (a dataframe for each participant). </p>

<p>In the dataframe (called = data) there is a variable called 'name' which is the unique code for each participant.</p>

<p>I have tried the following, but nothing happens (or the does not stop within an hour). What I intend to do is to split the dataframe (data) into smaller dataframes and append these to a list (datalist):</p>

<pre><code>import pandas as pd

def splitframe(data, name='name'):

    n = data[name][0]

    df = pd.DataFrame(columns=data.columns)

    datalist = []

    for i in range(len(data)):
        if data[name][i] == n:
            df = df.append(data.iloc[i])
        else:
            datalist.append(df)
            df = pd.DataFrame(columns=data.columns)
            n = data[name][i]
            df = df.append(data.iloc[i])

    return datalist
</code></pre>

<p>I do not get an error message, the script just seems to run forever!</p>

<p>Is there a smart way to do it?</p>
";55592.0;"[""import pandas as pd\n\ndef splitframe(data, name='name'):\n\n    n = data[name][0]\n\n    df = pd.DataFrame(columns=data.columns)\n\n    datalist = []\n\n    for i in range(len(data)):\n        if data[name][i] == n:\n            df = df.append(data.iloc[i])\n        else:\n            datalist.append(df)\n            df = pd.DataFrame(columns=data.columns)\n            n = data[name][i]\n            df = df.append(data.iloc[i])\n\n    return datalist\n""]";"[""import pandas as pd\n\ndef splitframe(data, name='name'):\n\n    n = data[name][0]\n\n    df = pd.DataFrame(columns=data.columns)\n\n    datalist = []\n\n    for i in range(len(data)):\n        if data[name][i] == n:\n            df = df.append(data.iloc[i])\n        else:\n            datalist.append(df)\n            df = pd.DataFrame(columns=data.columns)\n            n = data[name][i]\n            df = df.append(data.iloc[i])\n\n    return datalist\n""]"
290;6.0;0;19798153;;1;189;<python><numpy><pandas><vectorization>;Difference between map, applymap and apply methods in Pandas;"<p>Can you tell me when to use these vectorization methods with basic examples? I see that <code>map</code> is a <code>Series</code> method whereas the rest are <code>DataFrame</code> methods. I got confused about <code>apply</code> and <code>applymap</code> methods though. Why do we have two methods for applying a function to a DataFrame? Again, simple examples which illustrate the usage would be great!</p>

<p>Thanks!</p>
";98032.0;[];['map', 'Series', 'DataFrame', 'apply', 'applymap']
291;3.0;1;19828822;;1;111;<python><pandas>;How to check whether a pandas DataFrame is empty?;"<p>How to check whether a pandas <code>DataFrame</code> is empty? In my case I want to print some message in terminal if the <code>DataFrame</code> is empty. </p>
";70044.0;[];['DataFrame', 'DataFrame']
292;4.0;0;19851005;;1;43;<python><pandas><dataframe>;Rename Pandas DataFrame Index;"<p>I've a csv file without header, with a DateTime index. I want to rename the index and column name, but with df.rename() only the column name is renamed. Bug? I'm on version 0.12.0</p>

<pre><code>In [2]: df = pd.read_csv(r'D:\Data\DataTimeSeries_csv//seriesSM.csv', header=None, parse_dates=[[0]], index_col=[0] )

In [3]: df.head()
Out[3]: 
                   1
0                   
2002-06-18  0.112000
2002-06-22  0.190333
2002-06-26  0.134000
2002-06-30  0.093000
2002-07-04  0.098667

In [4]: df.rename(index={0:'Date'}, columns={1:'SM'}, inplace=True)

In [5]: df.head()
Out[5]: 
                  SM
0                   
2002-06-18  0.112000
2002-06-22  0.190333
2002-06-26  0.134000
2002-06-30  0.093000
2002-07-04  0.098667
</code></pre>
";76528.0;"[""In [2]: df = pd.read_csv(r'D:\\Data\\DataTimeSeries_csv//seriesSM.csv', header=None, parse_dates=[[0]], index_col=[0] )\n\nIn [3]: df.head()\nOut[3]: \n                   1\n0                   \n2002-06-18  0.112000\n2002-06-22  0.190333\n2002-06-26  0.134000\n2002-06-30  0.093000\n2002-07-04  0.098667\n\nIn [4]: df.rename(index={0:'Date'}, columns={1:'SM'}, inplace=True)\n\nIn [5]: df.head()\nOut[5]: \n                  SM\n0                   \n2002-06-18  0.112000\n2002-06-22  0.190333\n2002-06-26  0.134000\n2002-06-30  0.093000\n2002-07-04  0.098667\n""]";"[""In [2]: df = pd.read_csv(r'D:\\Data\\DataTimeSeries_csv//seriesSM.csv', header=None, parse_dates=[[0]], index_col=[0] )\n\nIn [3]: df.head()\nOut[3]: \n                   1\n0                   \n2002-06-18  0.112000\n2002-06-22  0.190333\n2002-06-26  0.134000\n2002-06-30  0.093000\n2002-07-04  0.098667\n\nIn [4]: df.rename(index={0:'Date'}, columns={1:'SM'}, inplace=True)\n\nIn [5]: df.head()\nOut[5]: \n                  SM\n0                   \n2002-06-18  0.112000\n2002-06-22  0.190333\n2002-06-26  0.134000\n2002-06-30  0.093000\n2002-07-04  0.098667\n""]"
293;5.0;1;19913659;;1;85;<python><pandas>;Pandas conditional creation of a series/dataframe column;"<p>I have a dataframe along the lines of the below:</p>

<pre><code>    Type       Set
1    A          Z
2    B          Z           
3    B          X
4    C          Y
</code></pre>

<p>I want to add another column to the dataframe (or generate a series) of the same length as the dataframe (= equal number of records/rows) which sets a colour green if Set = 'Z' and 'red' if Set = otherwise.</p>

<p>What's the best way to do this?</p>
";70108.0;['    Type       Set\n1    A          Z\n2    B          Z           \n3    B          X\n4    C          Y\n'];['    Type       Set\n1    A          Z\n2    B          Z           \n3    B          X\n4    C          Y\n']
294;4.0;0;19914937;;1;52;<python><pandas>;Applying function with multiple arguments to create a new pandas column;"<p>I want to create a new column in a <code>pandas</code> data frame by applying a function to two existing columns. Following this <a href=""https://stackoverflow.com/a/14603893/2327821"">answer</a> I've been able to create a new column when I only need one column as an argument:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({""A"": [10,20,30], ""B"": [20, 30, 10]})

def fx(x):
    return x * x

print(df)
df['newcolumn'] = df.A.apply(fx)
print(df)
</code></pre>

<p>However, I cannot figure out how to do the same thing when the function requires multiple arguments. For example, how do I create a new column by passing column A and column B to the function below?</p>

<pre><code>def fxy(x, y):
    return x * y
</code></pre>
";45281.0;"['import pandas as pd\ndf = pd.DataFrame({""A"": [10,20,30], ""B"": [20, 30, 10]})\n\ndef fx(x):\n    return x * x\n\nprint(df)\ndf[\'newcolumn\'] = df.A.apply(fx)\nprint(df)\n', 'def fxy(x, y):\n    return x * y\n']";"['pandas', 'import pandas as pd\ndf = pd.DataFrame({""A"": [10,20,30], ""B"": [20, 30, 10]})\n\ndef fx(x):\n    return x * x\n\nprint(df)\ndf[\'newcolumn\'] = df.A.apply(fx)\nprint(df)\n', 'def fxy(x, y):\n    return x * y\n']"
295;4.0;0;19928284;;1;23;<pandas>;Pandas dataframe values equality test;"<p>Another Pandas question!</p>

<p>I am writing some unit tests that test two data frames for equality, however, the test does not appear to look at the values of the data frame, only the structure:</p>

<pre><code>dates = pd.date_range('20130101', periods=6)

df1 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))
df2 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))

print df1
print df2
self.assertItemsEqual(df1, df2)
</code></pre>

<p>-->True</p>

<p>Do I need to convert the data frames to another data structure before asserting equality?</p>
";7575.0;"[""dates = pd.date_range('20130101', periods=6)\n\ndf1 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\ndf2 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n\nprint df1\nprint df2\nself.assertItemsEqual(df1, df2)\n""]";"[""dates = pd.date_range('20130101', periods=6)\n\ndf1 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\ndf2 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n\nprint df1\nprint df2\nself.assertItemsEqual(df1, df2)\n""]"
296;4.0;2;19960077;;1;84;<python><pandas><dataframe><sql-function>;How to implement 'in' and 'not in' for Pandas dataframe;"<p>How can I achieve the equivalents of SQL's <code>IN</code> and <code>NOT IN</code>?</p>

<p>I have a list with the required values.
Here's the scenario:</p>

<pre><code>df = pd.DataFrame({'countries':['US','UK','Germany','China']})
countries = ['UK','China']

# pseudo-code:
df[df['countries'] not in countries]
</code></pre>

<p>My current way of doing this is as follows:</p>

<pre><code>df = pd.DataFrame({'countries':['US','UK','Germany','China']})
countries = pd.DataFrame({'countries':['UK','China'], 'matched':True})

# IN
df.merge(countries,how='inner',on='countries')

# NOT IN
not_in = df.merge(countries,how='left',on='countries')
not_in = not_in[pd.isnull(not_in['matched'])]
</code></pre>

<p>But this seems like a horrible kludge. Can anyone improve on it?</p>
";72686.0;"[""df = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = ['UK','China']\n\n# pseudo-code:\ndf[df['countries'] not in countries]\n"", ""df = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = pd.DataFrame({'countries':['UK','China'], 'matched':True})\n\n# IN\ndf.merge(countries,how='inner',on='countries')\n\n# NOT IN\nnot_in = df.merge(countries,how='left',on='countries')\nnot_in = not_in[pd.isnull(not_in['matched'])]\n""]";"['IN', 'NOT IN', ""df = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = ['UK','China']\n\n# pseudo-code:\ndf[df['countries'] not in countries]\n"", ""df = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = pd.DataFrame({'countries':['UK','China'], 'matched':True})\n\n# IN\ndf.merge(countries,how='inner',on='countries')\n\n# NOT IN\nnot_in = df.merge(countries,how='left',on='countries')\nnot_in = not_in[pd.isnull(not_in['matched'])]\n""]"
297;2.0;3;19961490;;1;43;<python><python-2.7><pandas>;Construct pandas DataFrame from list of tuples;"<p>I have a list of tuples like</p>

<pre><code>data = [
('r1', 'c1', avg11, stdev11),
('r1', 'c2', avg12, stdev12),
('r2', 'c1', avg21, stdev21),
('r2', 'c2', avg22, stdev22)
]
</code></pre>

<p>and I would like to put them into a pandas DataFrame with rows named by the first column and columns named by the 2nd column. It seems the way to take care of the row names is something like <code>pandas.DataFrame([x[1:] for x in data], index = [x[0] for x in data])</code> but how do I take care of the columns to get a 2x2 matrix (the output from the previous set is 3x4)? Is there a more intelligent way of taking care of row labels as well, instead of explicitly omitting them?</p>

<p><strong>EDIT</strong> It seems I will need 2 DataFrames - one for averages and one for standard deviations, is that correct? Or can I store a list of values in each ""cell""?</p>
";96105.0;"[""data = [\n('r1', 'c1', avg11, stdev11),\n('r1', 'c2', avg12, stdev12),\n('r2', 'c1', avg21, stdev21),\n('r2', 'c2', avg22, stdev22)\n]\n""]";"[""data = [\n('r1', 'c1', avg11, stdev11),\n('r1', 'c2', avg12, stdev12),\n('r2', 'c1', avg21, stdev21),\n('r2', 'c2', avg22, stdev22)\n]\n"", 'pandas.DataFrame([x[1:] for x in data], index = [x[0] for x in data])']"
298;4.0;0;19991445;;1;60;<python><pandas><scikit-learn><regression><statsmodels>;Run an OLS regression with Pandas Data Frame;"<p>I have a <code>pandas</code> data frame and I would like to able to predict the values of column A from the values in columns B and C. Here is a toy example:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({""A"": [10,20,30,40,50], 
                   ""B"": [20, 30, 10, 40, 50], 
                   ""C"": [32, 234, 23, 23, 42523]})
</code></pre>

<p>Ideally, I would have something like <code>ols(A ~ B + C, data = df)</code> but when I look at the <a href=""http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html"" rel=""noreferrer"">examples</a> from algorithm libraries like <code>scikit-learn</code> it appears to feed the data to the model with a list of rows instead of columns. This would require me to reformat the data into lists inside lists, which seems to defeat the purpose of using pandas in the first place. What is the most pythonic way to run an OLS regression (or any machine learning algorithm more generally) on data in a pandas data frame? </p>
";82103.0;"['import pandas as pd\ndf = pd.DataFrame({""A"": [10,20,30,40,50], \n                   ""B"": [20, 30, 10, 40, 50], \n                   ""C"": [32, 234, 23, 23, 42523]})\n']";"['pandas', 'import pandas as pd\ndf = pd.DataFrame({""A"": [10,20,30,40,50], \n                   ""B"": [20, 30, 10, 40, 50], \n                   ""C"": [32, 234, 23, 23, 42523]})\n', 'ols(A ~ B + C, data = df)', 'scikit-learn']"
299;5.0;2;20003290;;1;22;<python><csv><numpy><floating-point><pandas>;Print different precision by column with pandas.DataFrame.to_csv()?;"<h2>Question</h2>

<p>Is it possible to specify a float precision specifically for each column to be printed by the Python <code>pandas</code> package method <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.to_csv.html"" rel=""noreferrer"">pandas.DataFrame.to_csv</a>?</p>

<h2>Background</h2>

<p>If I have a <code>pandas</code> dataframe that is arranged like this:</p>

<pre><code>In [53]: df_data[:5]
Out[53]: 
    year  month  day       lats       lons  vals
0   2012      6   16  81.862745 -29.834254   0.0
1   2012      6   16  81.862745 -29.502762   0.1
2   2012      6   16  81.862745 -29.171271   0.0
3   2012      6   16  81.862745 -28.839779   0.2
4   2012      6   16  81.862745 -28.508287   0.0
</code></pre>

<p>There is the <code>float_format</code> option that can be used to specify a precision, but this applys that precision to all columns of the dataframe when printed.</p>

<p>When I use that like so:</p>

<pre><code>df_data.to_csv(outfile, index=False,
                   header=False, float_format='%11.6f')
</code></pre>

<p>I get the following, where <code>vals</code> is given an inaccurate precision:</p>

<pre><code>2012,6,16,  81.862745, -29.834254,   0.000000
2012,6,16,  81.862745, -29.502762,   0.100000
2012,6,16,  81.862745, -29.171270,   0.000000
2012,6,16,  81.862745, -28.839779,   0.200000
2012,6,16,  81.862745, -28.508287,   0.000000
</code></pre>
";16557.0;"['In [53]: df_data[:5]\nOut[53]: \n    year  month  day       lats       lons  vals\n0   2012      6   16  81.862745 -29.834254   0.0\n1   2012      6   16  81.862745 -29.502762   0.1\n2   2012      6   16  81.862745 -29.171271   0.0\n3   2012      6   16  81.862745 -28.839779   0.2\n4   2012      6   16  81.862745 -28.508287   0.0\n', ""df_data.to_csv(outfile, index=False,\n                   header=False, float_format='%11.6f')\n"", '2012,6,16,  81.862745, -29.834254,   0.000000\n2012,6,16,  81.862745, -29.502762,   0.100000\n2012,6,16,  81.862745, -29.171270,   0.000000\n2012,6,16,  81.862745, -28.839779,   0.200000\n2012,6,16,  81.862745, -28.508287,   0.000000\n']";"['pandas', 'pandas', 'In [53]: df_data[:5]\nOut[53]: \n    year  month  day       lats       lons  vals\n0   2012      6   16  81.862745 -29.834254   0.0\n1   2012      6   16  81.862745 -29.502762   0.1\n2   2012      6   16  81.862745 -29.171271   0.0\n3   2012      6   16  81.862745 -28.839779   0.2\n4   2012      6   16  81.862745 -28.508287   0.0\n', 'float_format', ""df_data.to_csv(outfile, index=False,\n                   header=False, float_format='%11.6f')\n"", 'vals', '2012,6,16,  81.862745, -29.834254,   0.000000\n2012,6,16,  81.862745, -29.502762,   0.100000\n2012,6,16,  81.862745, -29.171270,   0.000000\n2012,6,16,  81.862745, -28.839779,   0.200000\n2012,6,16,  81.862745, -28.508287,   0.000000\n']"
300;3.0;5;20025325;;1;29;<python><pandas><indexing><dataframe>;Apply Function on DataFrame Index;"<p>What is the best way to apply a function over the index of a Pandas <code>DataFrame</code>?
Currently I am using this verbose approach:</p>

<pre><code>pd.DataFrame({""Month"": df.reset_index().Date.apply(foo)})
</code></pre>

<p>where <code>Date</code> is the name of the index and <code>foo</code> is the name of the function that I am applying.</p>
";15303.0;"['pd.DataFrame({""Month"": df.reset_index().Date.apply(foo)})\n']";"['DataFrame', 'pd.DataFrame({""Month"": df.reset_index().Date.apply(foo)})\n', 'Date', 'foo']"
301;2.0;0;20033111;;1;37;<python><python-2.7><pandas><max>;Python Pandas max value of selected columns;"<pre><code>data = {'name' : ['bill', 'joe', 'steve'],
    'test1' : [85, 75, 85],
    'test2' : [35, 45, 83],
     'test3' : [51, 61, 45]}
frame = pd.DataFrame(data)
</code></pre>

<p>I would like to add a new column that shows the max value for each row.</p>

<p>desired output:</p>

<pre><code> name test1 test2 test3 HighScore
 bill  75    75    85    85
 joe   35    45    83    83 
 steve  51   61    45    61 
</code></pre>

<p><strong>Sometimes</strong>  </p>

<pre><code>frame['HighScore'] = max(data['test1'], data['test2'], data['test3'])
</code></pre>

<p>works but most of the time gives this error:</p>

<p><strong>ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()</strong></p>

<p>Why does it only work sometimes?  Is there another way of doing it?</p>
";43953.0;"[""data = {'name' : ['bill', 'joe', 'steve'],\n    'test1' : [85, 75, 85],\n    'test2' : [35, 45, 83],\n     'test3' : [51, 61, 45]}\nframe = pd.DataFrame(data)\n"", ' name test1 test2 test3 HighScore\n bill  75    75    85    85\n joe   35    45    83    83 \n steve  51   61    45    61 \n', ""frame['HighScore'] = max(data['test1'], data['test2'], data['test3'])\n""]";"[""data = {'name' : ['bill', 'joe', 'steve'],\n    'test1' : [85, 75, 85],\n    'test2' : [35, 45, 83],\n     'test3' : [51, 61, 45]}\nframe = pd.DataFrame(data)\n"", ' name test1 test2 test3 HighScore\n bill  75    75    85    85\n joe   35    45    83    83 \n steve  51   61    45    61 \n', ""frame['HighScore'] = max(data['test1'], data['test2'], data['test3'])\n""]"
302;3.0;0;20067636;;1;38;<python><pandas><dataframe>;Pandas dataframe get first row of each group;"<p>I have a pandas <code>DataFrame</code> like following.</p>

<pre><code>df = pd.DataFrame({'id' : [1,1,1,2,2,3,3,3,3,4,4,5,6,6,6,7,7],
                'value'  : [""first"",""second"",""second"",""first"",
                            ""second"",""first"",""third"",""fourth"",
                            ""fifth"",""second"",""fifth"",""first"",
                            ""first"",""second"",""third"",""fourth"",""fifth""]})
</code></pre>

<p>I want to group this by [""id"",""value""] and get the first row of each group.</p>

<pre><code>        id   value
0        1   first
1        1  second
2        1  second
3        2   first
4        2  second
5        3   first
6        3   third
7        3  fourth
8        3   fifth
9        4  second
10       4   fifth
11       5   first
12       6   first
13       6  second
14       6   third
15       7  fourth
16       7   fifth
</code></pre>

<p>Expected outcome</p>

<pre><code>    id   value
     1   first
     2   first
     3   first
     4  second
     5  first
     6  first
     7  fourth
</code></pre>

<p>I tried following which only gives the first row of the <code>DataFrame</code>. Any help regarding this is appreciated.</p>

<pre><code>In [25]: for index, row in df.iterrows():
   ....:     df2 = pd.DataFrame(df.groupby(['id','value']).reset_index().ix[0])
</code></pre>
";35554.0;"['df = pd.DataFrame({\'id\' : [1,1,1,2,2,3,3,3,3,4,4,5,6,6,6,7,7],\n                \'value\'  : [""first"",""second"",""second"",""first"",\n                            ""second"",""first"",""third"",""fourth"",\n                            ""fifth"",""second"",""fifth"",""first"",\n                            ""first"",""second"",""third"",""fourth"",""fifth""]})\n', '        id   value\n0        1   first\n1        1  second\n2        1  second\n3        2   first\n4        2  second\n5        3   first\n6        3   third\n7        3  fourth\n8        3   fifth\n9        4  second\n10       4   fifth\n11       5   first\n12       6   first\n13       6  second\n14       6   third\n15       7  fourth\n16       7   fifth\n', '    id   value\n     1   first\n     2   first\n     3   first\n     4  second\n     5  first\n     6  first\n     7  fourth\n', ""In [25]: for index, row in df.iterrows():\n   ....:     df2 = pd.DataFrame(df.groupby(['id','value']).reset_index().ix[0])\n""]";"['DataFrame', 'df = pd.DataFrame({\'id\' : [1,1,1,2,2,3,3,3,3,4,4,5,6,6,6,7,7],\n                \'value\'  : [""first"",""second"",""second"",""first"",\n                            ""second"",""first"",""third"",""fourth"",\n                            ""fifth"",""second"",""fifth"",""first"",\n                            ""first"",""second"",""third"",""fourth"",""fifth""]})\n', '        id   value\n0        1   first\n1        1  second\n2        1  second\n3        2   first\n4        2  second\n5        3   first\n6        3   third\n7        3  fourth\n8        3   fifth\n9        4  second\n10       4   fifth\n11       5   first\n12       6   first\n13       6  second\n14       6   third\n15       7  fourth\n16       7   fifth\n', '    id   value\n     1   first\n     2   first\n     3   first\n     4  second\n     5  first\n     6  first\n     7  fourth\n', 'DataFrame', ""In [25]: for index, row in df.iterrows():\n   ....:     df2 = pd.DataFrame(df.groupby(['id','value']).reset_index().ix[0])\n""]"
303;2.0;1;20069009;;1;54;<python><pandas><greatest-n-per-group><window-functions><top-n>;Pandas good approach to get top-n records within each group;"<p>Suppose I have pandas DataFrame like this:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'id':[1,1,1,2,2,2,2,3,4],'value':[1,2,3,1,2,3,4,1,1]})
&gt;&gt;&gt; df
   id  value
0   1      1
1   1      2
2   1      3
3   2      1
4   2      2
5   2      3
6   2      4
7   3      1
8   4      1
</code></pre>

<p>I want to get new DataFrame with top 2 records for each id, like this:</p>

<pre><code>   id  value
0   1      1
1   1      2
3   2      1
4   2      2
7   3      1
8   4      1
</code></pre>

<p>I can do it with numbering records within group after group by:</p>

<pre><code>&gt;&gt;&gt; dfN = df.groupby('id').apply(lambda x:x['value'].reset_index()).reset_index()
&gt;&gt;&gt; dfN
   id  level_1  index  value
0   1        0      0      1
1   1        1      1      2
2   1        2      2      3
3   2        0      3      1
4   2        1      4      2
5   2        2      5      3
6   2        3      6      4
7   3        0      7      1
8   4        0      8      1
&gt;&gt;&gt; dfN[dfN['level_1'] &lt;= 1][['id', 'value']]
   id  value
0   1      1
1   1      2
3   2      1
4   2      2
7   3      1
8   4      1
</code></pre>

<p>But is there more effective/elegant approach to do this? And also is there more elegant approach to number records within each group (like SQL window function <a href=""http://msdn.microsoft.com/en-us/library/ms186734.aspx"" rel=""noreferrer"">row_number()</a>).</p>

<p>Thanks in advance.</p>
";41787.0;"["">>> df = pd.DataFrame({'id':[1,1,1,2,2,2,2,3,4],'value':[1,2,3,1,2,3,4,1,1]})\n>>> df\n   id  value\n0   1      1\n1   1      2\n2   1      3\n3   2      1\n4   2      2\n5   2      3\n6   2      4\n7   3      1\n8   4      1\n"", '   id  value\n0   1      1\n1   1      2\n3   2      1\n4   2      2\n7   3      1\n8   4      1\n', "">>> dfN = df.groupby('id').apply(lambda x:x['value'].reset_index()).reset_index()\n>>> dfN\n   id  level_1  index  value\n0   1        0      0      1\n1   1        1      1      2\n2   1        2      2      3\n3   2        0      3      1\n4   2        1      4      2\n5   2        2      5      3\n6   2        3      6      4\n7   3        0      7      1\n8   4        0      8      1\n>>> dfN[dfN['level_1'] <= 1][['id', 'value']]\n   id  value\n0   1      1\n1   1      2\n3   2      1\n4   2      2\n7   3      1\n8   4      1\n""]";"["">>> df = pd.DataFrame({'id':[1,1,1,2,2,2,2,3,4],'value':[1,2,3,1,2,3,4,1,1]})\n>>> df\n   id  value\n0   1      1\n1   1      2\n2   1      3\n3   2      1\n4   2      2\n5   2      3\n6   2      4\n7   3      1\n8   4      1\n"", '   id  value\n0   1      1\n1   1      2\n3   2      1\n4   2      2\n7   3      1\n8   4      1\n', "">>> dfN = df.groupby('id').apply(lambda x:x['value'].reset_index()).reset_index()\n>>> dfN\n   id  level_1  index  value\n0   1        0      0      1\n1   1        1      1      2\n2   1        2      2      3\n3   2        0      3      1\n4   2        1      4      2\n5   2        2      5      3\n6   2        3      6      4\n7   3        0      7      1\n8   4        0      8      1\n>>> dfN[dfN['level_1'] <= 1][['id', 'value']]\n   id  value\n0   1      1\n1   1      2\n3   2      1\n4   2      2\n7   3      1\n8   4      1\n""]"
304;2.0;0;20076195;;1;27;<pandas>;what is the most efficient way of counting occurrences in pandas?;"<p>I have a large (about 12M rows) dataframe df with say:</p>

<pre><code>df.columns = ['word','documents','frequency']
</code></pre>

<p>So the following ran in a timely fashion:</p>

<pre><code>word_grouping = df[['word','frequency']].groupby('word')
MaxFrequency_perWord = word_grouping[['frequency']].max().reset_index()
MaxFrequency_perWord.columns = ['word','MaxFrequency']
</code></pre>

<p>However, this is taking an unexpected long time to run:</p>

<pre><code>Occurrences_of_Words = word_grouping[['word']].count().reset_index()
</code></pre>

<p>What am I doing wrong here?  Is there a better way to count occurences in a large dataframe?</p>

<pre><code>df.word.describe()
</code></pre>

<p>ran pretty well, so I really did not expect this Occurrences_of_Words dataframe to take very long to build.</p>

<p>ps: If the answer is obvious and you feel the need to penalize me for asking this question, please include the answer as well.  thank you.</p>
";34623.0;"[""df.columns = ['word','documents','frequency']\n"", ""word_grouping = df[['word','frequency']].groupby('word')\nMaxFrequency_perWord = word_grouping[['frequency']].max().reset_index()\nMaxFrequency_perWord.columns = ['word','MaxFrequency']\n"", ""Occurrences_of_Words = word_grouping[['word']].count().reset_index()\n"", 'df.word.describe()\n']";"[""df.columns = ['word','documents','frequency']\n"", ""word_grouping = df[['word','frequency']].groupby('word')\nMaxFrequency_perWord = word_grouping[['frequency']].max().reset_index()\nMaxFrequency_perWord.columns = ['word','MaxFrequency']\n"", ""Occurrences_of_Words = word_grouping[['word']].count().reset_index()\n"", 'df.word.describe()\n']"
305;2.0;1;20083098;;1;29;<python><performance><pandas><hdf5><pytables>;Improve pandas (PyTables?) HDF5 table write performance;"<p>I've been using pandas for research now for about two months to great effect. With large numbers of medium-sized trace event datasets, pandas + PyTables (the HDF5 interface) does a tremendous job of allowing me to process heterogenous data using all the Python tools I know and love.</p>

<p>Generally speaking, I use the Fixed (formerly ""Storer"") format in PyTables, as my workflow is write-once, read-many, and many of my datasets are sized such that I can load 50-100 of them into memory at a time with no serious disadvantages. (NB: I do much of my work on Opteron server-class machines with 128GB+ system memory.)</p>

<p>However, for large datasets (500MB and greater), I would like to be able to use the more scalable random-access and query abilities of the PyTables ""Tables"" format, so that I can perform my queries out-of-memory and then load the much smaller result set into memory for processing. The big hurdle here, however, is the write performance. Yes, as I said, my workflow is write-once, read-many, but the relative times are still unacceptable. </p>

<p>As an example, I recently ran a large Cholesky factorization that took 3 minutes, 8 seconds (188 seconds) on my 48 core machine. This generated a trace file of ~2.2 GB - the trace is generated in parallel with the program, so there is no additional ""trace creation time.""</p>

<p>The initial conversion of my binary trace file into the pandas/PyTables format takes a decent chunk of time, but largely because the binary format is deliberately out-of-order in order to reduce the performance impact of the trace generator itself. This is also irrelevant to the performance loss when moving from the Storer format to the Table format.</p>

<p>My tests were initially run with pandas 0.12, numpy 1.7.1, PyTables 2.4.0, and numexpr 0.20.1. My 48 core machine runs at 2.8GHz per core, and I am writing to an ext3 filesystem which is probably (but not certainly) on a SSD.</p>

<p>I can write the entire dataset to a Storer format HDF5 file (resulting filesize: 3.3GB) in 7.1 seconds. The same dataset, written to the Table format (resulting file size is also 3.3GB), takes 178.7 seconds to write. </p>

<p>The code is as follows:</p>

<pre><code>with Timer() as t:
    store = pd.HDFStore('test_storer.h5', 'w')
    store.put('events', events_dataset, table=False, append=False)
print('Fixed format write took ' + str(t.interval))
with Timer() as t:
    store = pd.HDFStore('test_table.h5', 'w')
    store.put('events', events_dataset, table=True, append=False)
print('Table format write took ' + str(t.interval))
</code></pre>

<p>and the output is simply</p>

<pre><code>Fixed format write took 7.1
Table format write took 178.7
</code></pre>

<p>My dataset has 28,880,943 rows, and the columns are basic datatypes:</p>

<pre><code>node_id           int64
thread_id         int64
handle_id         int64
type              int64
begin             int64
end               int64
duration          int64
flags             int64
unique_id         int64
id                int64
DSTL_LS_FULL    float64
L2_DMISS        float64
L3_MISS         float64
kernel_type     float64
dtype: object
</code></pre>

<p>...so I don't think there should be any data-specific issues with the write speed. </p>

<p>I've also tried adding BLOSC compression, to rule out any strange I/O issues that might affect one scenario or the other, but compression seems to decrease the performance of both equally.</p>

<p>Now, I realize that the pandas documentation says that the Storer format offers significantly faster writes, and slightly faster reads. (I do experience the faster reads, as a read of the Storer format seems to take around 2.5 seconds, while a read of the Table format takes around 10 seconds.) But it really seems excessive that the Table format write should take 25 times as long as the Storer format write.</p>

<p>Can any of the folks involved with PyTables or pandas explain the architectural (or otherwise) reasons why writing to the queryable format (which clearly requires very little extra data) should take an order of magnitude longer? And is there any hope for improving this in the future? I'd love to jump in to contributing to one project or the other, as my field is high performance computing and I see a significant use case for both projects in this domain.... but it would be helpful to get some clarification on the issues involved first, and/or some advice on how to speed things up from those who know how the system is built.</p>

<p>EDIT:</p>

<p>Running the former tests with %prun in IPython gives the following (somewhat reduced for readability) profile output for the Storer/Fixed format:</p>

<pre><code>%prun -l 20 profile.events.to_hdf('test.h5', 'events', table=False, append=False)

3223 function calls (3222 primitive calls) in 7.385 seconds

Ordered by: internal time
List reduced from 208 to 20 due to restriction &lt;20&gt;

ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    6    7.127    1.188    7.128    1.188 {method '_createArray' of 'tables.hdf5Extension.Array' objects}
    1    0.242    0.242    0.242    0.242 {method '_closeFile' of 'tables.hdf5Extension.File' objects}
    1    0.003    0.003    0.003    0.003 {method '_g_new' of 'tables.hdf5Extension.File' objects}
   46    0.001    0.000    0.001    0.000 {method 'reduce' of 'numpy.ufunc' objects}
</code></pre>

<p>and the following for the Tables format:</p>

<pre><code>   %prun -l 40 profile.events.to_hdf('test.h5', 'events', table=True, append=False, chunksize=1000000)

   499082 function calls (499040 primitive calls) in 188.981 seconds

   Ordered by: internal time
   List reduced from 526 to 40 due to restriction &lt;40&gt;

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
       29   92.018    3.173   92.018    3.173 {pandas.lib.create_hdf_rows_2d}
      640   20.987    0.033   20.987    0.033 {method '_append' of 'tables.hdf5Extension.Array' objects}
       29   19.256    0.664   19.256    0.664 {method '_append_records' of 'tables.tableExtension.Table' objects}
      406   19.182    0.047   19.182    0.047 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}
    14244   10.646    0.001   10.646    0.001 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}
      472   10.359    0.022   10.359    0.022 {method 'copy' of 'numpy.ndarray' objects}
       80    3.409    0.043    3.409    0.043 {tables.indexesExtension.keysort}
        2    3.023    1.512    3.023    1.512 common.py:134(_isnull_ndarraylike)
       41    2.489    0.061    2.533    0.062 {method '_fillCol' of 'tables.tableExtension.Row' objects}
       87    2.401    0.028    2.401    0.028 {method 'astype' of 'numpy.ndarray' objects}
       30    1.880    0.063    1.880    0.063 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}
      282    0.824    0.003    0.824    0.003 {method 'reduce' of 'numpy.ufunc' objects}
       41    0.537    0.013    0.668    0.016 index.py:607(final_idx32)
    14490    0.385    0.000    0.712    0.000 array.py:342(_interpret_indexing)
       39    0.279    0.007   19.635    0.503 index.py:1219(reorder_slice)
        2    0.256    0.128   10.063    5.031 index.py:1099(get_neworder)
        1    0.090    0.090  119.392  119.392 pytables.py:3016(write_data)
    57842    0.087    0.000    0.087    0.000 {numpy.core.multiarray.empty}
    28570    0.062    0.000    0.107    0.000 utils.py:42(is_idx)
    14164    0.062    0.000    7.181    0.001 array.py:711(_readSlice)
</code></pre>

<p>EDIT 2:</p>

<p>Running again with a pre-release copy of pandas 0.13 (pulled Nov 20 2013 at about 11:00 EST), write times for the Tables format improve significantly but still don't compare ""reasonably"" to the write speeds of the Storer/Fixed format.</p>

<pre><code>%prun -l 40 profile.events.to_hdf('test.h5', 'events', table=True, append=False, chunksize=1000000)

         499748 function calls (499720 primitive calls) in 117.187 seconds

   Ordered by: internal time
   List reduced from 539 to 20 due to restriction &lt;20&gt;

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      640   22.010    0.034   22.010    0.034 {method '_append' of 'tables.hdf5Extension.Array' objects}
       29   20.782    0.717   20.782    0.717 {method '_append_records' of 'tables.tableExtension.Table' objects}
      406   19.248    0.047   19.248    0.047 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}
    14244   10.685    0.001   10.685    0.001 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}
      472   10.439    0.022   10.439    0.022 {method 'copy' of 'numpy.ndarray' objects}
       30    7.356    0.245    7.356    0.245 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}
       29    7.161    0.247   37.609    1.297 pytables.py:3498(write_data_chunk)
        2    3.888    1.944    3.888    1.944 common.py:197(_isnull_ndarraylike)
       80    3.581    0.045    3.581    0.045 {tables.indexesExtension.keysort}
       41    3.248    0.079    3.294    0.080 {method '_fillCol' of 'tables.tableExtension.Row' objects}
       34    2.744    0.081    2.744    0.081 {method 'ravel' of 'numpy.ndarray' objects}
      115    2.591    0.023    2.591    0.023 {method 'astype' of 'numpy.ndarray' objects}
      270    0.875    0.003    0.875    0.003 {method 'reduce' of 'numpy.ufunc' objects}
       41    0.560    0.014    0.732    0.018 index.py:607(final_idx32)
    14490    0.387    0.000    0.712    0.000 array.py:342(_interpret_indexing)
       39    0.303    0.008   19.617    0.503 index.py:1219(reorder_slice)
        2    0.288    0.144   10.299    5.149 index.py:1099(get_neworder)
    57871    0.087    0.000    0.087    0.000 {numpy.core.multiarray.empty}
        1    0.084    0.084   45.266   45.266 pytables.py:3424(write_data)
        1    0.080    0.080   55.542   55.542 pytables.py:3385(write)
</code></pre>

<p>I noticed while running these tests that there are long periods where writing seems to ""pause"" (the file on disk is not actively growing), and yet there is also low CPU usage during some of these periods.</p>

<p>I begin to suspect that some known ext3 limitations may interact badly with either pandas or PyTables. Ext3 and other non-extent-based filesystems sometimes struggle to unlink large files promptly, and similar system performance (low CPU usage, but long wait times) is apparent even during a simple 'rm' of a 1GB file, for instance. </p>

<p>To clarify, in each test case, I made sure to remove the existing file, if any, before starting the test, so as not to incur any ext3 file removal/overwrite penalty.</p>

<p>However, when re-running this test with index=None, performance improves drastically (~50s vs the ~120 when indexing). So it would seem that either this process continues to be CPU-bound (my system has relatively old AMD Opteron Istanbul CPUs running @ 2.8GHz, though it does also have 8 sockets with 6 core CPUs in each, all but one of which, of course, sit idle during the write), or that there is some conflict between the way PyTables or pandas attempts to manipulate/read/analyze the file when already partially or fully on the filesystem that causes pathologically bad I/O behavior when the indexing is occurring.</p>

<p>EDIT 3:</p>

<p>@Jeff's suggested tests on a smaller dataset (1.3 GB on disk), after upgrading PyTables from 2.4 to 3.0.0, have gotten me here:</p>

<pre><code>In [7]: %timeit f(df)
1 loops, best of 3: 3.7 s per loop

In [8]: %timeit f2(df) # where chunksize= 2 000 000
1 loops, best of 3: 13.8 s per loop

In [9]: %timeit f3(df) # where chunksize= 2 000 000
1 loops, best of 3: 43.4 s per loop
</code></pre>

<p>In fact, my performance seems to beat his in all scenarios except for when indexing is turned on (the default). However, indexing still seems to be a killer, and if the way I'm interpreting the output from <code>top</code> and <code>ls</code> as I run these tests is correct, there remain periods of time when there is neither significant processing nor any file writing happening (i.e., CPU usage for the Python process is near 0, and the filesize remains constant). I can only assume these are file reads. Why file reads would be causing slowdowns is hard for me to understand, as I can reliably load an entire 3+ GB file from this disk into memory in under 3 seconds. If they're not file reads, then what is the system 'waiting' on? (No one else is logged into the machine, and there is no other filesystem activity.)</p>

<p>At this point, with upgraded versions of the relevant python modules, the performance for my original dataset is down to the following figures. Of special interest are the system time, which I assume is at least an upper-bound on the time spent performing IO, and the Wall time, which seems to perhaps account for these mysterious periods of no write/no CPU activity.</p>

<pre><code>In [28]: %time f(profile.events)
CPU times: user 0 ns, sys: 7.16 s, total: 7.16 s
Wall time: 7.51 s

In [29]: %time f2(profile.events)
CPU times: user 18.7 s, sys: 14 s, total: 32.7 s
Wall time: 47.2 s

In [31]: %time f3(profile.events)
CPU times: user 1min 18s, sys: 14.4 s, total: 1min 32s
Wall time: 2min 5s
</code></pre>

<p>Nevertheless, it would appears that indexing causes significant slowdown for my use case. Perhaps I should attempt limiting the fields indexed instead of simply performing the default case (which may very well be indexing on all of the fields in the DataFrame)? I am not sure how this is likely to affect query times, especially in the cases where a query selects based on a non-indexed field.</p>

<p>Per Jeff's request, a ptdump of the resulting file.</p>

<pre><code>ptdump -av test.h5
/ (RootGroup) ''
  /._v_attrs (AttributeSet), 4 attributes:
   [CLASS := 'GROUP',
    PYTABLES_FORMAT_VERSION := '2.1',
    TITLE := '',
    VERSION := '1.0']
/df (Group) ''
  /df._v_attrs (AttributeSet), 14 attributes:
   [CLASS := 'GROUP',
    TITLE := '',
    VERSION := '1.0',
    data_columns := [],
    encoding := None,
    index_cols := [(0, 'index')],
    info := {1: {'type': 'Index', 'names': [None]}, 'index': {}},
    levels := 1,
    nan_rep := 'nan',
    non_index_axes := 
    [(1, ['node_id', 'thread_id', 'handle_id', 'type', 'begin', 'end', 'duration', 'flags', 'unique_id', 'id', 'DSTL_LS_FULL', 'L2_DMISS', 'L3_MISS', 'kernel_type'])],
    pandas_type := 'frame_table',
    pandas_version := '0.10.1',
    table_type := 'appendable_frame',
    values_cols := ['values_block_0', 'values_block_1']]
/df/table (Table(28880943,)) ''
  description := {
  ""index"": Int64Col(shape=(), dflt=0, pos=0),
  ""values_block_0"": Int64Col(shape=(10,), dflt=0, pos=1),
  ""values_block_1"": Float64Col(shape=(4,), dflt=0.0, pos=2)}
  byteorder := 'little'
  chunkshape := (4369,)
  autoindex := True
  colindexes := {
    ""index"": Index(6, medium, shuffle, zlib(1)).is_csi=False}
  /df/table._v_attrs (AttributeSet), 15 attributes:
   [CLASS := 'TABLE',
    FIELD_0_FILL := 0,
    FIELD_0_NAME := 'index',
    FIELD_1_FILL := 0,
    FIELD_1_NAME := 'values_block_0',
    FIELD_2_FILL := 0.0,
    FIELD_2_NAME := 'values_block_1',
    NROWS := 28880943,
    TITLE := '',
    VERSION := '2.7',
    index_kind := 'integer',
    values_block_0_dtype := 'int64',
    values_block_0_kind := ['node_id', 'thread_id', 'handle_id', 'type', 'begin', 'end', 'duration', 'flags', 'unique_id', 'id'],
    values_block_1_dtype := 'float64',
    values_block_1_kind := ['DSTL_LS_FULL', 'L2_DMISS', 'L3_MISS', 'kernel_type']]
</code></pre>

<p>and another %prun with the updated modules and the full dataset:</p>

<pre><code>%prun -l 25  %time f3(profile.events)
CPU times: user 1min 14s, sys: 16.2 s, total: 1min 30s
Wall time: 1min 48s

        542678 function calls (542650 primitive calls) in 108.678 seconds

   Ordered by: internal time
   List reduced from 629 to 25 due to restriction &lt;25&gt;

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      640   23.633    0.037   23.633    0.037 {method '_append' of 'tables.hdf5extension.Array' objects}
       15   20.852    1.390   20.852    1.390 {method '_append_records' of 'tables.tableextension.Table' objects}
      406   19.584    0.048   19.584    0.048 {method '_g_write_slice' of 'tables.hdf5extension.Array' objects}
    14244   10.591    0.001   10.591    0.001 {method '_g_read_slice' of 'tables.hdf5extension.Array' objects}
      458    9.693    0.021    9.693    0.021 {method 'copy' of 'numpy.ndarray' objects}
       15    6.350    0.423   30.989    2.066 pytables.py:3498(write_data_chunk)
       80    3.496    0.044    3.496    0.044 {tables.indexesextension.keysort}
       41    3.335    0.081    3.376    0.082 {method '_fill_col' of 'tables.tableextension.Row' objects}
       20    2.551    0.128    2.551    0.128 {method 'ravel' of 'numpy.ndarray' objects}
      101    2.449    0.024    2.449    0.024 {method 'astype' of 'numpy.ndarray' objects}
       16    1.789    0.112    1.789    0.112 {method '_g_flush' of 'tables.hdf5extension.Leaf' objects}
        2    1.728    0.864    1.728    0.864 common.py:197(_isnull_ndarraylike)
       41    0.586    0.014    0.842    0.021 index.py:637(final_idx32)
    14490    0.292    0.000    0.616    0.000 array.py:368(_interpret_indexing)
        2    0.283    0.142   10.267    5.134 index.py:1158(get_neworder)
      274    0.251    0.001    0.251    0.001 {method 'reduce' of 'numpy.ufunc' objects}
       39    0.174    0.004   19.373    0.497 index.py:1280(reorder_slice)
    57857    0.085    0.000    0.085    0.000 {numpy.core.multiarray.empty}
        1    0.083    0.083   35.657   35.657 pytables.py:3424(write_data)
        1    0.065    0.065   45.338   45.338 pytables.py:3385(write)
    14164    0.065    0.000    7.831    0.001 array.py:615(__getitem__)
    28570    0.062    0.000    0.108    0.000 utils.py:47(is_idx)
       47    0.055    0.001    0.055    0.001 {numpy.core.multiarray.arange}
    28570    0.050    0.000    0.090    0.000 leaf.py:397(_process_range)
    87797    0.048    0.000    0.048    0.000 {isinstance}
</code></pre>
";10552.0;"[""with Timer() as t:\n    store = pd.HDFStore('test_storer.h5', 'w')\n    store.put('events', events_dataset, table=False, append=False)\nprint('Fixed format write took ' + str(t.interval))\nwith Timer() as t:\n    store = pd.HDFStore('test_table.h5', 'w')\n    store.put('events', events_dataset, table=True, append=False)\nprint('Table format write took ' + str(t.interval))\n"", 'Fixed format write took 7.1\nTable format write took 178.7\n', 'node_id           int64\nthread_id         int64\nhandle_id         int64\ntype              int64\nbegin             int64\nend               int64\nduration          int64\nflags             int64\nunique_id         int64\nid                int64\nDSTL_LS_FULL    float64\nL2_DMISS        float64\nL3_MISS         float64\nkernel_type     float64\ndtype: object\n', ""%prun -l 20 profile.events.to_hdf('test.h5', 'events', table=False, append=False)\n\n3223 function calls (3222 primitive calls) in 7.385 seconds\n\nOrdered by: internal time\nList reduced from 208 to 20 due to restriction <20>\n\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n    6    7.127    1.188    7.128    1.188 {method '_createArray' of 'tables.hdf5Extension.Array' objects}\n    1    0.242    0.242    0.242    0.242 {method '_closeFile' of 'tables.hdf5Extension.File' objects}\n    1    0.003    0.003    0.003    0.003 {method '_g_new' of 'tables.hdf5Extension.File' objects}\n   46    0.001    0.000    0.001    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n"", ""   %prun -l 40 profile.events.to_hdf('test.h5', 'events', table=True, append=False, chunksize=1000000)\n\n   499082 function calls (499040 primitive calls) in 188.981 seconds\n\n   Ordered by: internal time\n   List reduced from 526 to 40 due to restriction <40>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n       29   92.018    3.173   92.018    3.173 {pandas.lib.create_hdf_rows_2d}\n      640   20.987    0.033   20.987    0.033 {method '_append' of 'tables.hdf5Extension.Array' objects}\n       29   19.256    0.664   19.256    0.664 {method '_append_records' of 'tables.tableExtension.Table' objects}\n      406   19.182    0.047   19.182    0.047 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n    14244   10.646    0.001   10.646    0.001 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n      472   10.359    0.022   10.359    0.022 {method 'copy' of 'numpy.ndarray' objects}\n       80    3.409    0.043    3.409    0.043 {tables.indexesExtension.keysort}\n        2    3.023    1.512    3.023    1.512 common.py:134(_isnull_ndarraylike)\n       41    2.489    0.061    2.533    0.062 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       87    2.401    0.028    2.401    0.028 {method 'astype' of 'numpy.ndarray' objects}\n       30    1.880    0.063    1.880    0.063 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n      282    0.824    0.003    0.824    0.003 {method 'reduce' of 'numpy.ufunc' objects}\n       41    0.537    0.013    0.668    0.016 index.py:607(final_idx32)\n    14490    0.385    0.000    0.712    0.000 array.py:342(_interpret_indexing)\n       39    0.279    0.007   19.635    0.503 index.py:1219(reorder_slice)\n        2    0.256    0.128   10.063    5.031 index.py:1099(get_neworder)\n        1    0.090    0.090  119.392  119.392 pytables.py:3016(write_data)\n    57842    0.087    0.000    0.087    0.000 {numpy.core.multiarray.empty}\n    28570    0.062    0.000    0.107    0.000 utils.py:42(is_idx)\n    14164    0.062    0.000    7.181    0.001 array.py:711(_readSlice)\n"", ""%prun -l 40 profile.events.to_hdf('test.h5', 'events', table=True, append=False, chunksize=1000000)\n\n         499748 function calls (499720 primitive calls) in 117.187 seconds\n\n   Ordered by: internal time\n   List reduced from 539 to 20 due to restriction <20>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      640   22.010    0.034   22.010    0.034 {method '_append' of 'tables.hdf5Extension.Array' objects}\n       29   20.782    0.717   20.782    0.717 {method '_append_records' of 'tables.tableExtension.Table' objects}\n      406   19.248    0.047   19.248    0.047 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n    14244   10.685    0.001   10.685    0.001 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n      472   10.439    0.022   10.439    0.022 {method 'copy' of 'numpy.ndarray' objects}\n       30    7.356    0.245    7.356    0.245 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n       29    7.161    0.247   37.609    1.297 pytables.py:3498(write_data_chunk)\n        2    3.888    1.944    3.888    1.944 common.py:197(_isnull_ndarraylike)\n       80    3.581    0.045    3.581    0.045 {tables.indexesExtension.keysort}\n       41    3.248    0.079    3.294    0.080 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       34    2.744    0.081    2.744    0.081 {method 'ravel' of 'numpy.ndarray' objects}\n      115    2.591    0.023    2.591    0.023 {method 'astype' of 'numpy.ndarray' objects}\n      270    0.875    0.003    0.875    0.003 {method 'reduce' of 'numpy.ufunc' objects}\n       41    0.560    0.014    0.732    0.018 index.py:607(final_idx32)\n    14490    0.387    0.000    0.712    0.000 array.py:342(_interpret_indexing)\n       39    0.303    0.008   19.617    0.503 index.py:1219(reorder_slice)\n        2    0.288    0.144   10.299    5.149 index.py:1099(get_neworder)\n    57871    0.087    0.000    0.087    0.000 {numpy.core.multiarray.empty}\n        1    0.084    0.084   45.266   45.266 pytables.py:3424(write_data)\n        1    0.080    0.080   55.542   55.542 pytables.py:3385(write)\n"", 'In [7]: %timeit f(df)\n1 loops, best of 3: 3.7 s per loop\n\nIn [8]: %timeit f2(df) # where chunksize= 2 000 000\n1 loops, best of 3: 13.8 s per loop\n\nIn [9]: %timeit f3(df) # where chunksize= 2 000 000\n1 loops, best of 3: 43.4 s per loop\n', 'In [28]: %time f(profile.events)\nCPU times: user 0 ns, sys: 7.16 s, total: 7.16 s\nWall time: 7.51 s\n\nIn [29]: %time f2(profile.events)\nCPU times: user 18.7 s, sys: 14 s, total: 32.7 s\nWall time: 47.2 s\n\nIn [31]: %time f3(profile.events)\nCPU times: user 1min 18s, sys: 14.4 s, total: 1min 32s\nWall time: 2min 5s\n', 'ptdump -av test.h5\n/ (RootGroup) \'\'\n  /._v_attrs (AttributeSet), 4 attributes:\n   [CLASS := \'GROUP\',\n    PYTABLES_FORMAT_VERSION := \'2.1\',\n    TITLE := \'\',\n    VERSION := \'1.0\']\n/df (Group) \'\'\n  /df._v_attrs (AttributeSet), 14 attributes:\n   [CLASS := \'GROUP\',\n    TITLE := \'\',\n    VERSION := \'1.0\',\n    data_columns := [],\n    encoding := None,\n    index_cols := [(0, \'index\')],\n    info := {1: {\'type\': \'Index\', \'names\': [None]}, \'index\': {}},\n    levels := 1,\n    nan_rep := \'nan\',\n    non_index_axes := \n    [(1, [\'node_id\', \'thread_id\', \'handle_id\', \'type\', \'begin\', \'end\', \'duration\', \'flags\', \'unique_id\', \'id\', \'DSTL_LS_FULL\', \'L2_DMISS\', \'L3_MISS\', \'kernel_type\'])],\n    pandas_type := \'frame_table\',\n    pandas_version := \'0.10.1\',\n    table_type := \'appendable_frame\',\n    values_cols := [\'values_block_0\', \'values_block_1\']]\n/df/table (Table(28880943,)) \'\'\n  description := {\n  ""index"": Int64Col(shape=(), dflt=0, pos=0),\n  ""values_block_0"": Int64Col(shape=(10,), dflt=0, pos=1),\n  ""values_block_1"": Float64Col(shape=(4,), dflt=0.0, pos=2)}\n  byteorder := \'little\'\n  chunkshape := (4369,)\n  autoindex := True\n  colindexes := {\n    ""index"": Index(6, medium, shuffle, zlib(1)).is_csi=False}\n  /df/table._v_attrs (AttributeSet), 15 attributes:\n   [CLASS := \'TABLE\',\n    FIELD_0_FILL := 0,\n    FIELD_0_NAME := \'index\',\n    FIELD_1_FILL := 0,\n    FIELD_1_NAME := \'values_block_0\',\n    FIELD_2_FILL := 0.0,\n    FIELD_2_NAME := \'values_block_1\',\n    NROWS := 28880943,\n    TITLE := \'\',\n    VERSION := \'2.7\',\n    index_kind := \'integer\',\n    values_block_0_dtype := \'int64\',\n    values_block_0_kind := [\'node_id\', \'thread_id\', \'handle_id\', \'type\', \'begin\', \'end\', \'duration\', \'flags\', \'unique_id\', \'id\'],\n    values_block_1_dtype := \'float64\',\n    values_block_1_kind := [\'DSTL_LS_FULL\', \'L2_DMISS\', \'L3_MISS\', \'kernel_type\']]\n', ""%prun -l 25  %time f3(profile.events)\nCPU times: user 1min 14s, sys: 16.2 s, total: 1min 30s\nWall time: 1min 48s\n\n        542678 function calls (542650 primitive calls) in 108.678 seconds\n\n   Ordered by: internal time\n   List reduced from 629 to 25 due to restriction <25>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      640   23.633    0.037   23.633    0.037 {method '_append' of 'tables.hdf5extension.Array' objects}\n       15   20.852    1.390   20.852    1.390 {method '_append_records' of 'tables.tableextension.Table' objects}\n      406   19.584    0.048   19.584    0.048 {method '_g_write_slice' of 'tables.hdf5extension.Array' objects}\n    14244   10.591    0.001   10.591    0.001 {method '_g_read_slice' of 'tables.hdf5extension.Array' objects}\n      458    9.693    0.021    9.693    0.021 {method 'copy' of 'numpy.ndarray' objects}\n       15    6.350    0.423   30.989    2.066 pytables.py:3498(write_data_chunk)\n       80    3.496    0.044    3.496    0.044 {tables.indexesextension.keysort}\n       41    3.335    0.081    3.376    0.082 {method '_fill_col' of 'tables.tableextension.Row' objects}\n       20    2.551    0.128    2.551    0.128 {method 'ravel' of 'numpy.ndarray' objects}\n      101    2.449    0.024    2.449    0.024 {method 'astype' of 'numpy.ndarray' objects}\n       16    1.789    0.112    1.789    0.112 {method '_g_flush' of 'tables.hdf5extension.Leaf' objects}\n        2    1.728    0.864    1.728    0.864 common.py:197(_isnull_ndarraylike)\n       41    0.586    0.014    0.842    0.021 index.py:637(final_idx32)\n    14490    0.292    0.000    0.616    0.000 array.py:368(_interpret_indexing)\n        2    0.283    0.142   10.267    5.134 index.py:1158(get_neworder)\n      274    0.251    0.001    0.251    0.001 {method 'reduce' of 'numpy.ufunc' objects}\n       39    0.174    0.004   19.373    0.497 index.py:1280(reorder_slice)\n    57857    0.085    0.000    0.085    0.000 {numpy.core.multiarray.empty}\n        1    0.083    0.083   35.657   35.657 pytables.py:3424(write_data)\n        1    0.065    0.065   45.338   45.338 pytables.py:3385(write)\n    14164    0.065    0.000    7.831    0.001 array.py:615(__getitem__)\n    28570    0.062    0.000    0.108    0.000 utils.py:47(is_idx)\n       47    0.055    0.001    0.055    0.001 {numpy.core.multiarray.arange}\n    28570    0.050    0.000    0.090    0.000 leaf.py:397(_process_range)\n    87797    0.048    0.000    0.048    0.000 {isinstance}\n""]";"[""with Timer() as t:\n    store = pd.HDFStore('test_storer.h5', 'w')\n    store.put('events', events_dataset, table=False, append=False)\nprint('Fixed format write took ' + str(t.interval))\nwith Timer() as t:\n    store = pd.HDFStore('test_table.h5', 'w')\n    store.put('events', events_dataset, table=True, append=False)\nprint('Table format write took ' + str(t.interval))\n"", 'Fixed format write took 7.1\nTable format write took 178.7\n', 'node_id           int64\nthread_id         int64\nhandle_id         int64\ntype              int64\nbegin             int64\nend               int64\nduration          int64\nflags             int64\nunique_id         int64\nid                int64\nDSTL_LS_FULL    float64\nL2_DMISS        float64\nL3_MISS         float64\nkernel_type     float64\ndtype: object\n', ""%prun -l 20 profile.events.to_hdf('test.h5', 'events', table=False, append=False)\n\n3223 function calls (3222 primitive calls) in 7.385 seconds\n\nOrdered by: internal time\nList reduced from 208 to 20 due to restriction <20>\n\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n    6    7.127    1.188    7.128    1.188 {method '_createArray' of 'tables.hdf5Extension.Array' objects}\n    1    0.242    0.242    0.242    0.242 {method '_closeFile' of 'tables.hdf5Extension.File' objects}\n    1    0.003    0.003    0.003    0.003 {method '_g_new' of 'tables.hdf5Extension.File' objects}\n   46    0.001    0.000    0.001    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n"", ""   %prun -l 40 profile.events.to_hdf('test.h5', 'events', table=True, append=False, chunksize=1000000)\n\n   499082 function calls (499040 primitive calls) in 188.981 seconds\n\n   Ordered by: internal time\n   List reduced from 526 to 40 due to restriction <40>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n       29   92.018    3.173   92.018    3.173 {pandas.lib.create_hdf_rows_2d}\n      640   20.987    0.033   20.987    0.033 {method '_append' of 'tables.hdf5Extension.Array' objects}\n       29   19.256    0.664   19.256    0.664 {method '_append_records' of 'tables.tableExtension.Table' objects}\n      406   19.182    0.047   19.182    0.047 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n    14244   10.646    0.001   10.646    0.001 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n      472   10.359    0.022   10.359    0.022 {method 'copy' of 'numpy.ndarray' objects}\n       80    3.409    0.043    3.409    0.043 {tables.indexesExtension.keysort}\n        2    3.023    1.512    3.023    1.512 common.py:134(_isnull_ndarraylike)\n       41    2.489    0.061    2.533    0.062 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       87    2.401    0.028    2.401    0.028 {method 'astype' of 'numpy.ndarray' objects}\n       30    1.880    0.063    1.880    0.063 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n      282    0.824    0.003    0.824    0.003 {method 'reduce' of 'numpy.ufunc' objects}\n       41    0.537    0.013    0.668    0.016 index.py:607(final_idx32)\n    14490    0.385    0.000    0.712    0.000 array.py:342(_interpret_indexing)\n       39    0.279    0.007   19.635    0.503 index.py:1219(reorder_slice)\n        2    0.256    0.128   10.063    5.031 index.py:1099(get_neworder)\n        1    0.090    0.090  119.392  119.392 pytables.py:3016(write_data)\n    57842    0.087    0.000    0.087    0.000 {numpy.core.multiarray.empty}\n    28570    0.062    0.000    0.107    0.000 utils.py:42(is_idx)\n    14164    0.062    0.000    7.181    0.001 array.py:711(_readSlice)\n"", ""%prun -l 40 profile.events.to_hdf('test.h5', 'events', table=True, append=False, chunksize=1000000)\n\n         499748 function calls (499720 primitive calls) in 117.187 seconds\n\n   Ordered by: internal time\n   List reduced from 539 to 20 due to restriction <20>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      640   22.010    0.034   22.010    0.034 {method '_append' of 'tables.hdf5Extension.Array' objects}\n       29   20.782    0.717   20.782    0.717 {method '_append_records' of 'tables.tableExtension.Table' objects}\n      406   19.248    0.047   19.248    0.047 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n    14244   10.685    0.001   10.685    0.001 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n      472   10.439    0.022   10.439    0.022 {method 'copy' of 'numpy.ndarray' objects}\n       30    7.356    0.245    7.356    0.245 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n       29    7.161    0.247   37.609    1.297 pytables.py:3498(write_data_chunk)\n        2    3.888    1.944    3.888    1.944 common.py:197(_isnull_ndarraylike)\n       80    3.581    0.045    3.581    0.045 {tables.indexesExtension.keysort}\n       41    3.248    0.079    3.294    0.080 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       34    2.744    0.081    2.744    0.081 {method 'ravel' of 'numpy.ndarray' objects}\n      115    2.591    0.023    2.591    0.023 {method 'astype' of 'numpy.ndarray' objects}\n      270    0.875    0.003    0.875    0.003 {method 'reduce' of 'numpy.ufunc' objects}\n       41    0.560    0.014    0.732    0.018 index.py:607(final_idx32)\n    14490    0.387    0.000    0.712    0.000 array.py:342(_interpret_indexing)\n       39    0.303    0.008   19.617    0.503 index.py:1219(reorder_slice)\n        2    0.288    0.144   10.299    5.149 index.py:1099(get_neworder)\n    57871    0.087    0.000    0.087    0.000 {numpy.core.multiarray.empty}\n        1    0.084    0.084   45.266   45.266 pytables.py:3424(write_data)\n        1    0.080    0.080   55.542   55.542 pytables.py:3385(write)\n"", 'In [7]: %timeit f(df)\n1 loops, best of 3: 3.7 s per loop\n\nIn [8]: %timeit f2(df) # where chunksize= 2 000 000\n1 loops, best of 3: 13.8 s per loop\n\nIn [9]: %timeit f3(df) # where chunksize= 2 000 000\n1 loops, best of 3: 43.4 s per loop\n', 'top', 'ls', 'In [28]: %time f(profile.events)\nCPU times: user 0 ns, sys: 7.16 s, total: 7.16 s\nWall time: 7.51 s\n\nIn [29]: %time f2(profile.events)\nCPU times: user 18.7 s, sys: 14 s, total: 32.7 s\nWall time: 47.2 s\n\nIn [31]: %time f3(profile.events)\nCPU times: user 1min 18s, sys: 14.4 s, total: 1min 32s\nWall time: 2min 5s\n', 'ptdump -av test.h5\n/ (RootGroup) \'\'\n  /._v_attrs (AttributeSet), 4 attributes:\n   [CLASS := \'GROUP\',\n    PYTABLES_FORMAT_VERSION := \'2.1\',\n    TITLE := \'\',\n    VERSION := \'1.0\']\n/df (Group) \'\'\n  /df._v_attrs (AttributeSet), 14 attributes:\n   [CLASS := \'GROUP\',\n    TITLE := \'\',\n    VERSION := \'1.0\',\n    data_columns := [],\n    encoding := None,\n    index_cols := [(0, \'index\')],\n    info := {1: {\'type\': \'Index\', \'names\': [None]}, \'index\': {}},\n    levels := 1,\n    nan_rep := \'nan\',\n    non_index_axes := \n    [(1, [\'node_id\', \'thread_id\', \'handle_id\', \'type\', \'begin\', \'end\', \'duration\', \'flags\', \'unique_id\', \'id\', \'DSTL_LS_FULL\', \'L2_DMISS\', \'L3_MISS\', \'kernel_type\'])],\n    pandas_type := \'frame_table\',\n    pandas_version := \'0.10.1\',\n    table_type := \'appendable_frame\',\n    values_cols := [\'values_block_0\', \'values_block_1\']]\n/df/table (Table(28880943,)) \'\'\n  description := {\n  ""index"": Int64Col(shape=(), dflt=0, pos=0),\n  ""values_block_0"": Int64Col(shape=(10,), dflt=0, pos=1),\n  ""values_block_1"": Float64Col(shape=(4,), dflt=0.0, pos=2)}\n  byteorder := \'little\'\n  chunkshape := (4369,)\n  autoindex := True\n  colindexes := {\n    ""index"": Index(6, medium, shuffle, zlib(1)).is_csi=False}\n  /df/table._v_attrs (AttributeSet), 15 attributes:\n   [CLASS := \'TABLE\',\n    FIELD_0_FILL := 0,\n    FIELD_0_NAME := \'index\',\n    FIELD_1_FILL := 0,\n    FIELD_1_NAME := \'values_block_0\',\n    FIELD_2_FILL := 0.0,\n    FIELD_2_NAME := \'values_block_1\',\n    NROWS := 28880943,\n    TITLE := \'\',\n    VERSION := \'2.7\',\n    index_kind := \'integer\',\n    values_block_0_dtype := \'int64\',\n    values_block_0_kind := [\'node_id\', \'thread_id\', \'handle_id\', \'type\', \'begin\', \'end\', \'duration\', \'flags\', \'unique_id\', \'id\'],\n    values_block_1_dtype := \'float64\',\n    values_block_1_kind := [\'DSTL_LS_FULL\', \'L2_DMISS\', \'L3_MISS\', \'kernel_type\']]\n', ""%prun -l 25  %time f3(profile.events)\nCPU times: user 1min 14s, sys: 16.2 s, total: 1min 30s\nWall time: 1min 48s\n\n        542678 function calls (542650 primitive calls) in 108.678 seconds\n\n   Ordered by: internal time\n   List reduced from 629 to 25 due to restriction <25>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      640   23.633    0.037   23.633    0.037 {method '_append' of 'tables.hdf5extension.Array' objects}\n       15   20.852    1.390   20.852    1.390 {method '_append_records' of 'tables.tableextension.Table' objects}\n      406   19.584    0.048   19.584    0.048 {method '_g_write_slice' of 'tables.hdf5extension.Array' objects}\n    14244   10.591    0.001   10.591    0.001 {method '_g_read_slice' of 'tables.hdf5extension.Array' objects}\n      458    9.693    0.021    9.693    0.021 {method 'copy' of 'numpy.ndarray' objects}\n       15    6.350    0.423   30.989    2.066 pytables.py:3498(write_data_chunk)\n       80    3.496    0.044    3.496    0.044 {tables.indexesextension.keysort}\n       41    3.335    0.081    3.376    0.082 {method '_fill_col' of 'tables.tableextension.Row' objects}\n       20    2.551    0.128    2.551    0.128 {method 'ravel' of 'numpy.ndarray' objects}\n      101    2.449    0.024    2.449    0.024 {method 'astype' of 'numpy.ndarray' objects}\n       16    1.789    0.112    1.789    0.112 {method '_g_flush' of 'tables.hdf5extension.Leaf' objects}\n        2    1.728    0.864    1.728    0.864 common.py:197(_isnull_ndarraylike)\n       41    0.586    0.014    0.842    0.021 index.py:637(final_idx32)\n    14490    0.292    0.000    0.616    0.000 array.py:368(_interpret_indexing)\n        2    0.283    0.142   10.267    5.134 index.py:1158(get_neworder)\n      274    0.251    0.001    0.251    0.001 {method 'reduce' of 'numpy.ufunc' objects}\n       39    0.174    0.004   19.373    0.497 index.py:1280(reorder_slice)\n    57857    0.085    0.000    0.085    0.000 {numpy.core.multiarray.empty}\n        1    0.083    0.083   35.657   35.657 pytables.py:3424(write_data)\n        1    0.065    0.065   45.338   45.338 pytables.py:3385(write)\n    14164    0.065    0.000    7.831    0.001 array.py:615(__getitem__)\n    28570    0.062    0.000    0.108    0.000 utils.py:47(is_idx)\n       47    0.055    0.001    0.055    0.001 {numpy.core.multiarray.arange}\n    28570    0.050    0.000    0.090    0.000 leaf.py:397(_process_range)\n    87797    0.048    0.000    0.048    0.000 {isinstance}\n""]"
306;2.0;0;20084382;;1;48;<python><pandas><dataframe>;Find unique values in a Pandas dataframe, irrespective of row or column location;"<p>I have a Pandas dataframe and I want to find all the unique values in that dataframe...irrespective of row/columns. If I have a 10 x 10 dataframe, and suppose they have 84 unique values, I need to find them - Not the count.</p>

<p>I can create a set and add the values of each rows by iterating over the rows of the dataframe. But, I feel that it may be inefficient (cannot justify that). Is there an efficient way to find it? Is there a predefined function?</p>
";81292.0;[];[]
307;3.0;0;20095673;;1;26;<python><pandas><dataframe>;python: shift column in pandas dataframe up by one;"<p>I've got a pandas dataframe. I want to 'lag' one of my columns. Meaning, for example, shifting the entire column 'gdp' up by one, and then removing all the excess data at the bottom of the remaining rows so that all columns are of equal length again.</p>

<pre><code>df =
    y  gdp  cap
0   1    2    5
1   2    3    9
2   8    7    2
3   3    4    7
4   6    7    7

df_lag =
    y  gdp  cap
0   1    3    5
1   2    7    9
2   8    4    2
3   3    7    7
</code></pre>

<p>Anyway to do this?</p>
";24501.0;['df =\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n\ndf_lag =\n    y  gdp  cap\n0   1    3    5\n1   2    7    9\n2   8    4    2\n3   3    7    7\n'];['df =\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n\ndf_lag =\n    y  gdp  cap\n0   1    3    5\n1   2    7    9\n2   8    4    2\n3   3    7    7\n']
308;5.0;0;20107570;;1;27;<python><pandas>;Removing index column in pandas;"<p>I have the following code which imports a CSV file.  There are 3 columns and I want to set the first two of them to variables.  When I set the second column to the variable ""efficiency"" the index column is also tacked on.  How can I get rid of the index column?</p>

<pre><code>df = pd.DataFrame.from_csv('Efficiency_Data.csv', header=0, parse_dates=False)
energy = df.index
efficiency = df.Efficiency
print efficiency
</code></pre>

<p>I tried using </p>

<pre><code>del df['index']
</code></pre>

<p>after I set </p>

<pre><code>energy = df.index
</code></pre>

<p>which I found in another post but that results in ""KeyError: 'index' ""</p>
";70802.0;"[""df = pd.DataFrame.from_csv('Efficiency_Data.csv', header=0, parse_dates=False)\nenergy = df.index\nefficiency = df.Efficiency\nprint efficiency\n"", ""del df['index']\n"", 'energy = df.index\n']";"[""df = pd.DataFrame.from_csv('Efficiency_Data.csv', header=0, parse_dates=False)\nenergy = df.index\nefficiency = df.Efficiency\nprint efficiency\n"", ""del df['index']\n"", 'energy = df.index\n']"
309;5.0;3;20109391;;1;105;<python><pandas>;How to make good reproducible pandas examples;"<p>Having spent a decent amount of time watching both the <a href=""/questions/tagged/r"" class=""post-tag"" title=""show questions tagged &#39;r&#39;"" rel=""tag"">r</a> and <a href=""/questions/tagged/pandas"" class=""post-tag"" title=""show questions tagged &#39;pandas&#39;"" rel=""tag"">pandas</a> tags on SO, the impression that I get is that <code>pandas</code> questions are less likely to contain reproducible data. This is something that the R community has been pretty good about encouraging, and thanks to guides like <a href=""https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example"">this</a>, newcomers are able to get some help on putting together these examples. People who are able to read these guides and come back with reproducible data will often have much better luck getting answers to their questions.</p>

<p>How can we create good reproducible examples for <code>pandas</code> questions? Simple dataframes can be put together, e.g.:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'user': ['Bob', 'Jane', 'Alice'], 
                   'income': [40000, 50000, 42000]})
</code></pre>

<p>But many example datasets need more complicated structure, e.g.:</p>

<ul>
<li><code>datetime</code> indices or data</li>
<li>Multiple categorical variables (is there an equivalent to R's <code>expand.grid()</code> function, which produces all possible combinations of some given variables?)</li>
<li>MultiIndex or Panel data</li>
</ul>

<p>For datasets that are hard to mock up using a few lines of code, is there an equivalent to R's <code>dput()</code> that allows you to generate copy-pasteable code to regenerate your datastructure?</p>
";3594.0;"[""import pandas as pd\ndf = pd.DataFrame({'user': ['Bob', 'Jane', 'Alice'], \n                   'income': [40000, 50000, 42000]})\n""]";"['pandas', 'pandas', ""import pandas as pd\ndf = pd.DataFrame({'user': ['Bob', 'Jane', 'Alice'], \n                   'income': [40000, 50000, 42000]})\n"", 'datetime', 'expand.grid()', 'dput()']"
310;1.0;3;20110170;;1;63;<python><pandas><dataframe><flatten><multi-index>;Turn Pandas Multi-Index into column;"<p>I have a dataframe with 2 index levels:</p>

<pre><code>                         value
Trial    measurement
    1              0        13
                   1         3
                   2         4
    2              0       NaN
                   1        12
    3              0        34 
</code></pre>

<p>Which I want to turn into this:</p>

<pre><code>Trial    measurement       value

    1              0        13
    1              1         3
    1              2         4
    2              0       NaN
    2              1        12
    3              0        34 
</code></pre>

<p>How can I best do this?   </p>

<p>I need this because I want to aggregate the data <a href=""https://stackoverflow.com/a/20108446/1893275"">as instructed here</a>, but I can't select my columns like that if they are in use as indices.</p>
";31159.0;['                         value\nTrial    measurement\n    1              0        13\n                   1         3\n                   2         4\n    2              0       NaN\n                   1        12\n    3              0        34 \n', 'Trial    measurement       value\n\n    1              0        13\n    1              1         3\n    1              2         4\n    2              0       NaN\n    2              1        12\n    3              0        34 \n'];['                         value\nTrial    measurement\n    1              0        13\n                   1         3\n                   2         4\n    2              0       NaN\n                   1        12\n    3              0        34 \n', 'Trial    measurement       value\n\n    1              0        13\n    1              1         3\n    1              2         4\n    2              0       NaN\n    2              1        12\n    3              0        34 \n']
311;4.0;5;20158597;;1;29;<python><pandas>;How to qcut with non unique bin edges?;"<p>My question is the same as this previous one:</p>

<p><a href=""https://stackoverflow.com/questions/18921570/binning-with-zero-values-in-pandas"">Binning with zero values in pandas</a></p>

<p>however, I still want to include the 0 values in a fractile.  Is there a way to do this?  In other words, if I have 600 values, 50% of which are 0, and the rest are let's say between 1 and 100, how would I categorize all the 0 values in fractile 1, and then the rest of the non-zero values in fractile labels 2 to 10 (assuming I want 10 fractiles).  Could I convert the 0's to nan, qcut the remaining non nan data into 9 fractiles (1 to 9), then add 1 to each label (now 2 to 10) and label all the 0 values as fractile 1 manually?  Even this is tricky, because in my data set in addition to the 600 values, I also have another couple hundred which may already be nan before I would convert the 0s to nan.</p>

<p>Update 1/26/14:</p>

<p>I came up with the following interim solution.  The problem with this code though, is if the high frequency value is not on the edges of the distribution, then it inserts an extra bin in the middle of the existing set of bins and throws everything a little (or a lot) off.</p>

<pre><code>def fractile_cut(ser, num_fractiles):
    num_valid = ser.valid().shape[0]
    remain_fractiles = num_fractiles
    vcounts = ser.value_counts()
    high_freq = []
    i = 0
    while vcounts.iloc[i] &gt; num_valid/ float(remain_fractiles):
        curr_val = vcounts.index[i]
        high_freq.append(curr_val)
        remain_fractiles -= 1
        num_valid = num_valid - vcounts[i]
        i += 1
    curr_ser = ser.copy()
    curr_ser = curr_ser[~curr_ser.isin(high_freq)]
    qcut = pd.qcut(curr_ser, remain_fractiles, retbins=True)
    qcut_bins = qcut[1]
    all_bins = list(qcut_bins)
    for val in high_freq:
        bisect.insort(all_bins, val)
    cut = pd.cut(ser, bins=all_bins)
    ser_fractiles = pd.Series(cut.labels + 1, index=ser.index)
    return ser_fractiles
</code></pre>
";8987.0;['def fractile_cut(ser, num_fractiles):\n    num_valid = ser.valid().shape[0]\n    remain_fractiles = num_fractiles\n    vcounts = ser.value_counts()\n    high_freq = []\n    i = 0\n    while vcounts.iloc[i] > num_valid/ float(remain_fractiles):\n        curr_val = vcounts.index[i]\n        high_freq.append(curr_val)\n        remain_fractiles -= 1\n        num_valid = num_valid - vcounts[i]\n        i += 1\n    curr_ser = ser.copy()\n    curr_ser = curr_ser[~curr_ser.isin(high_freq)]\n    qcut = pd.qcut(curr_ser, remain_fractiles, retbins=True)\n    qcut_bins = qcut[1]\n    all_bins = list(qcut_bins)\n    for val in high_freq:\n        bisect.insort(all_bins, val)\n    cut = pd.cut(ser, bins=all_bins)\n    ser_fractiles = pd.Series(cut.labels + 1, index=ser.index)\n    return ser_fractiles\n'];['def fractile_cut(ser, num_fractiles):\n    num_valid = ser.valid().shape[0]\n    remain_fractiles = num_fractiles\n    vcounts = ser.value_counts()\n    high_freq = []\n    i = 0\n    while vcounts.iloc[i] > num_valid/ float(remain_fractiles):\n        curr_val = vcounts.index[i]\n        high_freq.append(curr_val)\n        remain_fractiles -= 1\n        num_valid = num_valid - vcounts[i]\n        i += 1\n    curr_ser = ser.copy()\n    curr_ser = curr_ser[~curr_ser.isin(high_freq)]\n    qcut = pd.qcut(curr_ser, remain_fractiles, retbins=True)\n    qcut_bins = qcut[1]\n    all_bins = list(qcut_bins)\n    for val in high_freq:\n        bisect.insort(all_bins, val)\n    cut = pd.cut(ser, bins=all_bins)\n    ser_fractiles = pd.Series(cut.labels + 1, index=ser.index)\n    return ser_fractiles\n']
312;5.0;5;20219254;;1;46;<python><excel><python-2.7><pandas>;How to write to an existing excel file without overwriting data (using pandas)?;"<p>I use pandas to write to excel file in the following fashion:</p>

<pre><code>import pandas

writer = pandas.ExcelWriter('Masterfile.xlsx') 

data_filtered.to_excel(writer, ""Main"", cols=['Diff1', 'Diff2'])

writer.save()
</code></pre>

<p>Masterfile.xlsx already consists of number of different tabs.</p>

<p>Pandas correctly writes to ""Main"" sheet, unfortunately it also deletes all other tabs.</p>
";29344.0;"['import pandas\n\nwriter = pandas.ExcelWriter(\'Masterfile.xlsx\') \n\ndata_filtered.to_excel(writer, ""Main"", cols=[\'Diff1\', \'Diff2\'])\n\nwriter.save()\n']";"['import pandas\n\nwriter = pandas.ExcelWriter(\'Masterfile.xlsx\') \n\ndata_filtered.to_excel(writer, ""Main"", cols=[\'Diff1\', \'Diff2\'])\n\nwriter.save()\n']"
313;4.0;0;20230326;;1;29;<python><pandas><dataframe>;Retrieve DataFrame of all but one specified column;"<p>Is there a way to select all but one column in a pandas DataFrame object? I've seen ways to delete a column, but I don't want to do that.</p>
";17402.0;[];[]
314;1.0;4;20233071;;1;21;<python><pandas>;Filter Pandas DataFrame by time index;"<p>I have a pandas DataFrame from 6:36 AM to 5:31 PM.  I want to remove all observations where the time is less than 8:00:00 AM.  Here is my attempt:</p>

<pre><code>df = df[df.index &lt; '2013-10-16 08:00:00']
</code></pre>

<p>This does nothing, please help.</p>
";15944.0;"[""df = df[df.index < '2013-10-16 08:00:00']\n""]";"[""df = df[df.index < '2013-10-16 08:00:00']\n""]"
315;1.0;0;20235401;;1;34;<python><pandas><series>;Remove NaN from pandas series;"<p>Is there a way to remove a NaN values from a panda series? I have a series that may or may not have some NaN values in it, and I'd like to return a copy of the series with all the NaNs removed.</p>
";43902.0;[];[]
316;3.0;0;20250771;;1;69;<python><dictionary><pandas><remap>;Remap values in pandas column with a dict;"<p>I have a dictionary which looks like this: <code>di = {1: ""A"", 2: ""B""}</code></p>

<p>I would like to apply it to the ""col1"" column of a dataframe similar to:</p>

<pre><code>     col1   col2
0       w      a
1       1      2
2       2    NaN
</code></pre>

<p>to get:</p>

<pre><code>     col1   col2
0       w      a
1       A      2
2       B    NaN
</code></pre>

<p>How can I best do this? For some reason googling terms relating to this only shows me links about how to make columns from dicts and vice-versa :-/ </p>
";48507.0;['     col1   col2\n0       w      a\n1       1      2\n2       2    NaN\n', '     col1   col2\n0       w      a\n1       A      2\n2       B    NaN\n'];"['di = {1: ""A"", 2: ""B""}', '     col1   col2\n0       w      a\n1       1      2\n2       2    NaN\n', '     col1   col2\n0       w      a\n1       A      2\n2       B    NaN\n']"
317;3.0;1;20297317;;1;37;<python><pandas><dataframe>;python dataframe pandas drop column using int;"<p>I understand that to drop a column you use df.drop('column name', axis=1). Is there a way to drop a column using a numerical index instead of the column name?</p>
";28907.0;[];[]
318;3.0;1;20297332;;1;52;<python><pandas><dataframe>;Python pandas dataframe: retrieve number of columns;"<p>How do you programmatically retrieve the number of columns in a pandas dataframe? I was hoping for something like:</p>

<pre><code>df.num_columns
</code></pre>
";66459.0;['df.num_columns\n'];['df.num_columns\n']
319;2.0;0;20375561;;1;28;<python><pandas><dataframe>;Joining pandas dataframes by column names;"<p>I have two dataframes with the following column names:</p>

<pre><code>frame_1:
event_id, date, time, county_ID

frame_2:
countyid, state
</code></pre>

<p>I would like to get a dataframe with the following columns by joining (left) on <code>county_ID = countyid</code>:</p>

<pre><code>joined_dataframe
event_id, date, time, county, state
</code></pre>

<p>I cannot figure out how to do it if the columns on which I want to join are not the index. What's the easiest way? Thanks!</p>
";16792.0;['frame_1:\nevent_id, date, time, county_ID\n\nframe_2:\ncountyid, state\n', 'joined_dataframe\nevent_id, date, time, county, state\n'];['frame_1:\nevent_id, date, time, county_ID\n\nframe_2:\ncountyid, state\n', 'county_ID = countyid', 'joined_dataframe\nevent_id, date, time, county, state\n']
320;3.0;0;20383647;;1;26;<python><pandas>;Pandas selecting by label sometimes return series, sometimes returns dataframe;"<p>In Pandas, when I select a label that only has one entry in the index I get back a Series, but when I select an entry that has more then one entry I get back a data frame.</p>

<p>Why is that?  Is there a way to ensure I always get back a data frame?</p>

<pre><code>In [1]: import pandas as pd

In [2]: df = pd.DataFrame(data=range(5), index=[1, 2, 3, 3, 3])

In [3]: type(df.loc[3])
Out[3]: pandas.core.frame.DataFrame

In [4]: type(df.loc[1])
Out[4]: pandas.core.series.Series
</code></pre>
";7456.0;['In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame(data=range(5), index=[1, 2, 3, 3, 3])\n\nIn [3]: type(df.loc[3])\nOut[3]: pandas.core.frame.DataFrame\n\nIn [4]: type(df.loc[1])\nOut[4]: pandas.core.series.Series\n'];['In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame(data=range(5), index=[1, 2, 3, 3, 3])\n\nIn [3]: type(df.loc[3])\nOut[3]: pandas.core.frame.DataFrame\n\nIn [4]: type(df.loc[1])\nOut[4]: pandas.core.series.Series\n']
321;1.0;5;20444087;;1;28;<python><pandas><reverse>;Right way to reverse pandas.DataFrame?;"<p>Here is my code:</p>

<pre><code>import pandas as pd

data = pd.DataFrame({'Odd':[1,3,5,6,7,9], 'Even':[0,2,4,6,8,10]})

for i in reversed(data):
    print(data['Odd'], data['Even'])
</code></pre>

<p>When I run this code, i get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Python33\lib\site-packages\pandas\core\generic.py"", line 665, in _get_item_cache
    return cache[item]
KeyError: 5

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\*****\Documents\******\********\****.py"", line 5, in &lt;module&gt;
    for i in reversed(data):
  File ""C:\Python33\lib\site-packages\pandas\core\frame.py"", line 2003, in __getitem__
    return self._get_item_cache(key)
  File ""C:\Python33\lib\site-packages\pandas\core\generic.py"", line 667, in _get_item_cache
    values = self._data.get(item)
  File ""C:\Python33\lib\site-packages\pandas\core\internals.py"", line 1656, in get
    _, block = self._find_block(item)
  File ""C:\Python33\lib\site-packages\pandas\core\internals.py"", line 1936, in _find_block
    self._check_have(item)
  File ""C:\Python33\lib\site-packages\pandas\core\internals.py"", line 1943, in _check_have
    raise KeyError('no item named %s' % com.pprint_thing(item))
KeyError: 'no item named 5'
</code></pre>

<p><strong>Why am I getting this error?<br>
How can I fix that?<br>
What is the right way to reverse <code>pandas.DataFrame</code>?</strong></p>
";31880.0;"[""import pandas as pd\n\ndata = pd.DataFrame({'Odd':[1,3,5,6,7,9], 'Even':[0,2,4,6,8,10]})\n\nfor i in reversed(data):\n    print(data['Odd'], data['Even'])\n"", 'Traceback (most recent call last):\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\generic.py"", line 665, in _get_item_cache\n    return cache[item]\nKeyError: 5\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""C:\\Users\\*****\\Documents\\******\\********\\****.py"", line 5, in <module>\n    for i in reversed(data):\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\frame.py"", line 2003, in __getitem__\n    return self._get_item_cache(key)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\generic.py"", line 667, in _get_item_cache\n    values = self._data.get(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1656, in get\n    _, block = self._find_block(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1936, in _find_block\n    self._check_have(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1943, in _check_have\n    raise KeyError(\'no item named %s\' % com.pprint_thing(item))\nKeyError: \'no item named 5\'\n']";"[""import pandas as pd\n\ndata = pd.DataFrame({'Odd':[1,3,5,6,7,9], 'Even':[0,2,4,6,8,10]})\n\nfor i in reversed(data):\n    print(data['Odd'], data['Even'])\n"", 'Traceback (most recent call last):\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\generic.py"", line 665, in _get_item_cache\n    return cache[item]\nKeyError: 5\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""C:\\Users\\*****\\Documents\\******\\********\\****.py"", line 5, in <module>\n    for i in reversed(data):\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\frame.py"", line 2003, in __getitem__\n    return self._get_item_cache(key)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\generic.py"", line 667, in _get_item_cache\n    values = self._data.get(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1656, in get\n    _, block = self._find_block(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1936, in _find_block\n    self._check_have(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1943, in _check_have\n    raise KeyError(\'no item named %s\' % com.pprint_thing(item))\nKeyError: \'no item named 5\'\n', 'pandas.DataFrame']"
322;2.0;0;20461165;;1;132;<python><pandas>;How to convert pandas index in a dataframe to a column?;"<p>This seems rather obvious, but I can't seem to figure out how do I convert an index of data frame to a column?</p>

<p>For example:</p>

<pre><code>df=
           gi  ptt_loc
 0  384444683      593  
 1  384444684      594 
 2  384444686      596  
</code></pre>

<p>To,</p>

<pre><code>df=
    index1       gi    ptt_loc
 0  0     384444683      593  
 1  1     384444684      594 
 2  2     384444686      596  
</code></pre>
";106030.0;['df=\n           gi  ptt_loc\n 0  384444683      593  \n 1  384444684      594 \n 2  384444686      596  \n', 'df=\n    index1       gi    ptt_loc\n 0  0     384444683      593  \n 1  1     384444684      594 \n 2  2     384444686      596  \n'];['df=\n           gi  ptt_loc\n 0  384444683      593  \n 1  384444684      594 \n 2  384444686      596  \n', 'df=\n    index1       gi    ptt_loc\n 0  0     384444683      593  \n 1  1     384444684      594 \n 2  2     384444686      596  \n']
323;2.0;0;20490274;;1;113;<python><indexing><pandas><dataframe>;How to reset index in a pandas data frame?;"<p>I have a data frame from which I remove some rows. As a result, I get a data frame in which index is something like that: <code>[1,5,6,10,11]</code> and I would like to reset it to <code>[0,1,2,3,4]</code>. How can I do it?</p>

<p><strong>ADDED</strong></p>

<p>The following seems to work:</p>

<pre><code>df = df.reset_index()
del df['index']
</code></pre>

<p>The following does not work:</p>

<pre><code>df = df.reindex()
</code></pre>
";101981.0;"[""df = df.reset_index()\ndel df['index']\n"", 'df = df.reindex()\n']";"['[1,5,6,10,11]', '[0,1,2,3,4]', ""df = df.reset_index()\ndel df['index']\n"", 'df = df.reindex()\n']"
324;3.0;0;20574257;;1;24;<python><pandas><statistics>;Constructing a co-occurrence matrix in python pandas;"<p>I know you how to do this in <a href=""https://stackoverflow.com/questions/10622730/constructing-a-co-occurrence-matrix-from-dummycoded-observations-in-r"">R</a>. But, is there any functions in pandas that transforms a dataframe to an nxn co-occurrence matrix containing the counts of two aspects co-occurring. </p>

<p>For example an matrix df:</p>

<pre><code>import pandas as pd

df = pd.DataFrame({'TFD' : ['AA', 'SL', 'BB', 'D0', 'Dk', 'FF'],
                    'Snack' : ['1', '0', '1', '1', '0', '0'],
                    'Trans' : ['1', '1', '1', '0', '0', '1'],
                    'Dop' : ['1', '0', '1', '0', '1', '1']}).set_index('TFD')

print df

&gt;&gt;&gt; 
    Dop Snack Trans
TFD                
AA    1     1     1
SL    0     0     1
BB    1     1     1
D0    0     1     0
Dk    1     0     0
FF    1     0     1

[6 rows x 3 columns]
</code></pre>

<p>would yield:</p>

<pre><code>    Dop Snack Trans

Dop   0     2     3
Snack 2     0     2
Trans 3     2     0
</code></pre>

<p>Since the matrix is mirrored on the diagonal I guess there would be a way to optimize code.</p>
";8487.0;"[""import pandas as pd\n\ndf = pd.DataFrame({'TFD' : ['AA', 'SL', 'BB', 'D0', 'Dk', 'FF'],\n                    'Snack' : ['1', '0', '1', '1', '0', '0'],\n                    'Trans' : ['1', '1', '1', '0', '0', '1'],\n                    'Dop' : ['1', '0', '1', '0', '1', '1']}).set_index('TFD')\n\nprint df\n\n>>> \n    Dop Snack Trans\nTFD                \nAA    1     1     1\nSL    0     0     1\nBB    1     1     1\nD0    0     1     0\nDk    1     0     0\nFF    1     0     1\n\n[6 rows x 3 columns]\n"", '    Dop Snack Trans\n\nDop   0     2     3\nSnack 2     0     2\nTrans 3     2     0\n']";"[""import pandas as pd\n\ndf = pd.DataFrame({'TFD' : ['AA', 'SL', 'BB', 'D0', 'Dk', 'FF'],\n                    'Snack' : ['1', '0', '1', '1', '0', '0'],\n                    'Trans' : ['1', '1', '1', '0', '0', '1'],\n                    'Dop' : ['1', '0', '1', '0', '1', '1']}).set_index('TFD')\n\nprint df\n\n>>> \n    Dop Snack Trans\nTFD                \nAA    1     1     1\nSL    0     0     1\nBB    1     1     1\nD0    0     1     0\nDk    1     0     0\nFF    1     0     1\n\n[6 rows x 3 columns]\n"", '    Dop Snack Trans\n\nDop   0     2     3\nSnack 2     0     2\nTrans 3     2     0\n']"
325;3.0;0;20612645;;1;103;<python><pandas>;How to find the installed pandas version;"<p>I am having trouble with some of pandas functionalities. How do I check what is my installation version?</p>
";58878.0;[];[]
326;5.0;1;20625582;;1;170;<python><parsing><pandas><dataframe>;How to deal with SettingWithCopyWarning in Pandas?;"<h2>Background</h2>

<p>I just upgraded my Pandas from 0.11 to 0.13.0rc1. Now, the application is popping out many new warnings. One of them like this:</p>

<pre><code>E:\FinReporter\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead
  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE
</code></pre>

<p>I want to know what exactly it means?  Do I need to change something?</p>

<p>How should I suspend the warning if I insist to use <code>quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE</code>?</p>

<h2>The function that gives errors</h2>

<pre><code>def _decode_stock_quote(list_of_150_stk_str):
    """"""decode the webpage and return dataframe""""""

    from cStringIO import StringIO

    str_of_all = """".join(list_of_150_stk_str)

    quote_df = pd.read_csv(StringIO(str_of_all), sep=',', names=list('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg')) #dtype={'A': object, 'B': object, 'C': np.float64}
    quote_df.rename(columns={'A':'STK', 'B':'TOpen', 'C':'TPCLOSE', 'D':'TPrice', 'E':'THigh', 'F':'TLow', 'I':'TVol', 'J':'TAmt', 'e':'TDate', 'f':'TTime'}, inplace=True)
    quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]
    quote_df['TClose'] = quote_df['TPrice']
    quote_df['RT']     = 100 * (quote_df['TPrice']/quote_df['TPCLOSE'] - 1)
    quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE
    quote_df['TAmt']   = quote_df['TAmt']/TAMT_SCALE
    quote_df['STK_ID'] = quote_df['STK'].str.slice(13,19)
    quote_df['STK_Name'] = quote_df['STK'].str.slice(21,30)#.decode('gb2312')
    quote_df['TDate']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])

    return quote_df
</code></pre>

<h2>More error messages</h2>

<pre><code>E:\FinReporter\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead
  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE
E:\FinReporter\FM_EXT.py:450: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead
  quote_df['TAmt']   = quote_df['TAmt']/TAMT_SCALE
E:\FinReporter\FM_EXT.py:453: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead
  quote_df['TDate']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])
</code></pre>
";145172.0;"[""E:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE\n"", 'def _decode_stock_quote(list_of_150_stk_str):\n    """"""decode the webpage and return dataframe""""""\n\n    from cStringIO import StringIO\n\n    str_of_all = """".join(list_of_150_stk_str)\n\n    quote_df = pd.read_csv(StringIO(str_of_all), sep=\',\', names=list(\'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg\')) #dtype={\'A\': object, \'B\': object, \'C\': np.float64}\n    quote_df.rename(columns={\'A\':\'STK\', \'B\':\'TOpen\', \'C\':\'TPCLOSE\', \'D\':\'TPrice\', \'E\':\'THigh\', \'F\':\'TLow\', \'I\':\'TVol\', \'J\':\'TAmt\', \'e\':\'TDate\', \'f\':\'TTime\'}, inplace=True)\n    quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\n    quote_df[\'TClose\'] = quote_df[\'TPrice\']\n    quote_df[\'RT\']     = 100 * (quote_df[\'TPrice\']/quote_df[\'TPCLOSE\'] - 1)\n    quote_df[\'TVol\']   = quote_df[\'TVol\']/TVOL_SCALE\n    quote_df[\'TAmt\']   = quote_df[\'TAmt\']/TAMT_SCALE\n    quote_df[\'STK_ID\'] = quote_df[\'STK\'].str.slice(13,19)\n    quote_df[\'STK_Name\'] = quote_df[\'STK\'].str.slice(21,30)#.decode(\'gb2312\')\n    quote_df[\'TDate\']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])\n\n    return quote_df\n', ""E:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE\nE:\\FinReporter\\FM_EXT.py:450: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TAmt']   = quote_df['TAmt']/TAMT_SCALE\nE:\\FinReporter\\FM_EXT.py:453: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TDate']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])\n""]";"[""E:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE\n"", ""quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE"", 'def _decode_stock_quote(list_of_150_stk_str):\n    """"""decode the webpage and return dataframe""""""\n\n    from cStringIO import StringIO\n\n    str_of_all = """".join(list_of_150_stk_str)\n\n    quote_df = pd.read_csv(StringIO(str_of_all), sep=\',\', names=list(\'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg\')) #dtype={\'A\': object, \'B\': object, \'C\': np.float64}\n    quote_df.rename(columns={\'A\':\'STK\', \'B\':\'TOpen\', \'C\':\'TPCLOSE\', \'D\':\'TPrice\', \'E\':\'THigh\', \'F\':\'TLow\', \'I\':\'TVol\', \'J\':\'TAmt\', \'e\':\'TDate\', \'f\':\'TTime\'}, inplace=True)\n    quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\n    quote_df[\'TClose\'] = quote_df[\'TPrice\']\n    quote_df[\'RT\']     = 100 * (quote_df[\'TPrice\']/quote_df[\'TPCLOSE\'] - 1)\n    quote_df[\'TVol\']   = quote_df[\'TVol\']/TVOL_SCALE\n    quote_df[\'TAmt\']   = quote_df[\'TAmt\']/TAMT_SCALE\n    quote_df[\'STK_ID\'] = quote_df[\'STK\'].str.slice(13,19)\n    quote_df[\'STK_Name\'] = quote_df[\'STK\'].str.slice(21,30)#.decode(\'gb2312\')\n    quote_df[\'TDate\']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])\n\n    return quote_df\n', ""E:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE\nE:\\FinReporter\\FM_EXT.py:450: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TAmt']   = quote_df['TAmt']/TAMT_SCALE\nE:\\FinReporter\\FM_EXT.py:453: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TDate']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])\n""]"
327;3.0;2;20637439;;1;29;<python><csv><pandas>;Skip rows during csv import pandas;"<p>I'm trying to import a .csv file using <code>pandas.read_csv()</code>, however I don't want to import the 2nd row of the data file (the row with index = 1 for 0-indexing).</p>

<p>I can't see how not to import it because the arguments used with the command seem ambiguous:</p>

<p>From the pandas website: </p>

<p>""skiprows : list-like or integer</p>

<p>Row numbers to skip (0-indexed) or number of rows to skip (int) at the start of the file.""</p>

<p>If I put <code>skiprows=1</code> in the arguments, how does it know whether to skip the first row or skip the row with index 1?</p>

<p>Cheers.</p>
";35315.0;[];['pandas.read_csv()', 'skiprows=1']
328;3.0;0;20638006;;1;221;<python><dictionary><pandas><dataframe>;Convert list of dictionaries to Dataframe;"<p>I have a list of dictionaries like this:</p>

<pre><code>[{'points': 50, 'time': '5:00', 'year': 2010}, 
{'points': 25, 'time': '6:00', 'month': ""february""}, 
{'points':90, 'time': '9:00', 'month': 'january'}, 
{'points_h1':20, 'month': 'june'}]
</code></pre>

<p>and I want to turn this into a pandas <code>DataFrame</code> like this:</p>

<pre><code>      month  points  points_h1  time  year
0       NaN      50        NaN  5:00  2010
1  february      25        NaN  6:00   NaN
2   january      90        NaN  9:00   NaN
3      june     NaN         20   NaN   NaN
</code></pre>

<p><em>Note: Order of the columns does not matter.</em></p>

<p>Ultimately, the goal is to write this to a text file and this seems like the best solution I could find.  How can I turn the list of dictionaries into a panda DataFrame as shown above?</p>
";57095.0;"['[{\'points\': 50, \'time\': \'5:00\', \'year\': 2010}, \n{\'points\': 25, \'time\': \'6:00\', \'month\': ""february""}, \n{\'points\':90, \'time\': \'9:00\', \'month\': \'january\'}, \n{\'points_h1\':20, \'month\': \'june\'}]\n', '      month  points  points_h1  time  year\n0       NaN      50        NaN  5:00  2010\n1  february      25        NaN  6:00   NaN\n2   january      90        NaN  9:00   NaN\n3      june     NaN         20   NaN   NaN\n']";"['[{\'points\': 50, \'time\': \'5:00\', \'year\': 2010}, \n{\'points\': 25, \'time\': \'6:00\', \'month\': ""february""}, \n{\'points\':90, \'time\': \'9:00\', \'month\': \'january\'}, \n{\'points_h1\':20, \'month\': \'june\'}]\n', 'DataFrame', '      month  points  points_h1  time  year\n0       NaN      50        NaN  5:00  2010\n1  february      25        NaN  6:00   NaN\n2   january      90        NaN  9:00   NaN\n3      june     NaN         20   NaN   NaN\n']"
329;1.0;3;20656663;;1;36;<python><matplotlib><pandas><histogram>;Matplotlib/Pandas error using histogram;"<p>I have a problem making histograms from pandas series objects and I can't understand why it does not work. The code has worked fine before but now it does not.</p>

<p>Here is a bit of my code (specifically, a pandas series object I'm trying to make a histogram of):</p>

<pre><code>type(dfj2_MARKET1['VSPD2_perc'])
</code></pre>

<p>which outputs the result: 
    <code>pandas.core.series.Series</code></p>

<p>Here's my plotting code:</p>

<pre><code>fig, axes = plt.subplots(1, 7, figsize=(30,4))
axes[0].hist(dfj2_MARKET1['VSPD1_perc'],alpha=0.9, color='blue')
axes[0].grid(True)
axes[0].set_title(MARKET1 + '  5-40 km / h')
</code></pre>

<p>Error message:</p>

<pre><code>    AttributeError                            Traceback (most recent call last)
    &lt;ipython-input-75-3810c361db30&gt; in &lt;module&gt;()
      1 fig, axes = plt.subplots(1, 7, figsize=(30,4))
      2 
    ----&gt; 3 axes[1].hist(dfj2_MARKET1['VSPD2_perc'],alpha=0.9, color='blue')
      4 axes[1].grid(True)
      5 axes[1].set_xlabel('Time spent [%]')

    C:\Python27\lib\site-packages\matplotlib\axes.pyc in hist(self, x, bins, range, normed,          weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label,    stacked, **kwargs)
   8322             # this will automatically overwrite bins,
   8323             # so that each histogram uses the same bins
-&gt; 8324             m, bins = np.histogram(x[i], bins, weights=w[i], **hist_kwargs)
   8325             m = m.astype(float) # causes problems later if it's an int
   8326             if mlast is None:

    C:\Python27\lib\site-packages\numpy\lib\function_base.pyc in histogram(a, bins, range,     normed, weights, density)
    158         if (mn &gt; mx):
    159             raise AttributeError(
--&gt; 160                 'max must be larger than min in range parameter.')
    161 
    162     if not iterable(bins):

AttributeError: max must be larger than min in range parameter.
</code></pre>
";24955.0;"[""type(dfj2_MARKET1['VSPD2_perc'])\n"", ""fig, axes = plt.subplots(1, 7, figsize=(30,4))\naxes[0].hist(dfj2_MARKET1['VSPD1_perc'],alpha=0.9, color='blue')\naxes[0].grid(True)\naxes[0].set_title(MARKET1 + '  5-40 km / h')\n"", ""    AttributeError                            Traceback (most recent call last)\n    <ipython-input-75-3810c361db30> in <module>()\n      1 fig, axes = plt.subplots(1, 7, figsize=(30,4))\n      2 \n    ----> 3 axes[1].hist(dfj2_MARKET1['VSPD2_perc'],alpha=0.9, color='blue')\n      4 axes[1].grid(True)\n      5 axes[1].set_xlabel('Time spent [%]')\n\n    C:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in hist(self, x, bins, range, normed,          weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label,    stacked, **kwargs)\n   8322             # this will automatically overwrite bins,\n   8323             # so that each histogram uses the same bins\n-> 8324             m, bins = np.histogram(x[i], bins, weights=w[i], **hist_kwargs)\n   8325             m = m.astype(float) # causes problems later if it's an int\n   8326             if mlast is None:\n\n    C:\\Python27\\lib\\site-packages\\numpy\\lib\\function_base.pyc in histogram(a, bins, range,     normed, weights, density)\n    158         if (mn > mx):\n    159             raise AttributeError(\n--> 160                 'max must be larger than min in range parameter.')\n    161 \n    162     if not iterable(bins):\n\nAttributeError: max must be larger than min in range parameter.\n""]";"[""type(dfj2_MARKET1['VSPD2_perc'])\n"", 'pandas.core.series.Series', ""fig, axes = plt.subplots(1, 7, figsize=(30,4))\naxes[0].hist(dfj2_MARKET1['VSPD1_perc'],alpha=0.9, color='blue')\naxes[0].grid(True)\naxes[0].set_title(MARKET1 + '  5-40 km / h')\n"", ""    AttributeError                            Traceback (most recent call last)\n    <ipython-input-75-3810c361db30> in <module>()\n      1 fig, axes = plt.subplots(1, 7, figsize=(30,4))\n      2 \n    ----> 3 axes[1].hist(dfj2_MARKET1['VSPD2_perc'],alpha=0.9, color='blue')\n      4 axes[1].grid(True)\n      5 axes[1].set_xlabel('Time spent [%]')\n\n    C:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in hist(self, x, bins, range, normed,          weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label,    stacked, **kwargs)\n   8322             # this will automatically overwrite bins,\n   8323             # so that each histogram uses the same bins\n-> 8324             m, bins = np.histogram(x[i], bins, weights=w[i], **hist_kwargs)\n   8325             m = m.astype(float) # causes problems later if it's an int\n   8326             if mlast is None:\n\n    C:\\Python27\\lib\\site-packages\\numpy\\lib\\function_base.pyc in histogram(a, bins, range,     normed, weights, density)\n    158         if (mn > mx):\n    159             raise AttributeError(\n--> 160                 'max must be larger than min in range parameter.')\n    161 \n    162     if not iterable(bins):\n\nAttributeError: max must be larger than min in range parameter.\n""]"
330;2.0;1;20763012;;1;62;<numpy><pandas>;Creating a Pandas DataFrame from a Numpy array: How do I specify the index column and column headers?;"<p>I have a Numpy array consisting of a list of lists, representing a two-dimensional array with row labels and column names as shown below:</p>

<pre><code>data = array([['','Col1','Col2'],['Row1',1,2],['Row2',3,4]])
</code></pre>

<p>I'd like the resulting DataFrame to have Row1 and Row2 as index values, and Col1, Col2 as header values</p>

<p>I can specify the index as follows:</p>

<pre><code>df = pd.DataFrame(data,index=data[:,0]),
</code></pre>

<p>however I am unsure how to best assign column headers.</p>
";128433.0;"[""data = array([['','Col1','Col2'],['Row1',1,2],['Row2',3,4]])\n"", 'df = pd.DataFrame(data,index=data[:,0]),\n']";"[""data = array([['','Col1','Col2'],['Row1',1,2],['Row2',3,4]])\n"", 'df = pd.DataFrame(data,index=data[:,0]),\n']"
331;3.0;2;20845213;;1;100;<python><csv><indexing><pandas>;How to avoid Python/Pandas creating an index in a saved csv?;"<p>I am trying to save a csv to a folder after making some edits to the file. </p>

<p>Every time I use <code>pd.to_csv('C:/Path of file.csv')</code> the csv file has a separate column of indexes. I want to avoid printing the index to csv.</p>

<p>I tried: </p>

<pre><code>pd.read_csv('C:/Path to file to edit.csv', index_col = False)
</code></pre>

<p>And to save the file...</p>

<pre><code>pd.to_csv('C:/Path to save edited file.csv', index_col = False)
</code></pre>

<p>However, I still got the unwanted index column. How can I avoid this when I save my files?</p>
";47670.0;"[""pd.read_csv('C:/Path to file to edit.csv', index_col = False)\n"", ""pd.to_csv('C:/Path to save edited file.csv', index_col = False)\n""]";"[""pd.to_csv('C:/Path of file.csv')"", ""pd.read_csv('C:/Path to file to edit.csv', index_col = False)\n"", ""pd.to_csv('C:/Path to save edited file.csv', index_col = False)\n""]"
332;4.0;6;20853474;;1;59;<python><pandas><pip>;ImportError: No module named dateutil.parser;"<p>I am receiving the following error when importing <code>pandas</code> in a <code>Python</code> program</p>

<pre><code>monas-mbp:book mona$ sudo pip install python-dateutil
Requirement already satisfied (use --upgrade to upgrade): python-dateutil in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python
Cleaning up...
monas-mbp:book mona$ python t1.py
No module named dateutil.parser
Traceback (most recent call last):
  File ""t1.py"", line 4, in &lt;module&gt;
    import pandas as pd
  File ""/Library/Python/2.7/site-packages/pandas/__init__.py"", line 6, in &lt;module&gt;
    from . import hashtable, tslib, lib
  File ""tslib.pyx"", line 31, in init pandas.tslib (pandas/tslib.c:48782)
ImportError: No module named dateutil.parser
</code></pre>

<p>Also here's the program:</p>

<pre><code>import codecs 
from math import sqrt
import numpy as np
import pandas as pd

users = {""Angelica"": {""Blues Traveler"": 3.5, ""Broken Bells"": 2.0,
                      ""Norah Jones"": 4.5, ""Phoenix"": 5.0,
                      ""Slightly Stoopid"": 1.5,
                      ""The Strokes"": 2.5, ""Vampire Weekend"": 2.0},

         ""Bill"":{""Blues Traveler"": 2.0, ""Broken Bells"": 3.5,
                 ""Deadmau5"": 4.0, ""Phoenix"": 2.0,
                 ""Slightly Stoopid"": 3.5, ""Vampire Weekend"": 3.0},

         ""Chan"": {""Blues Traveler"": 5.0, ""Broken Bells"": 1.0,
                  ""Deadmau5"": 1.0, ""Norah Jones"": 3.0, ""Phoenix"": 5,
                  ""Slightly Stoopid"": 1.0},

         ""Dan"": {""Blues Traveler"": 3.0, ""Broken Bells"": 4.0,
                 ""Deadmau5"": 4.5, ""Phoenix"": 3.0,
                 ""Slightly Stoopid"": 4.5, ""The Strokes"": 4.0,
                 ""Vampire Weekend"": 2.0},

         ""Hailey"": {""Broken Bells"": 4.0, ""Deadmau5"": 1.0,
                    ""Norah Jones"": 4.0, ""The Strokes"": 4.0,
                    ""Vampire Weekend"": 1.0},

         ""Jordyn"":  {""Broken Bells"": 4.5, ""Deadmau5"": 4.0,
                     ""Norah Jones"": 5.0, ""Phoenix"": 5.0,
                     ""Slightly Stoopid"": 4.5, ""The Strokes"": 4.0,
                     ""Vampire Weekend"": 4.0},

         ""Sam"": {""Blues Traveler"": 5.0, ""Broken Bells"": 2.0,
                 ""Norah Jones"": 3.0, ""Phoenix"": 5.0,
                 ""Slightly Stoopid"": 4.0, ""The Strokes"": 5.0},

         ""Veronica"": {""Blues Traveler"": 3.0, ""Norah Jones"": 5.0,
                      ""Phoenix"": 4.0, ""Slightly Stoopid"": 2.5,
                      ""The Strokes"": 3.0}
        }



class recommender:

    def __init__(self, data, k=1, metric='pearson', n=5):
        """""" initialize recommender
        currently, if data is dictionary the recommender is initialized
        to it.
        For all other data types of data, no initialization occurs
        k is the k value for k nearest neighbor
        metric is which distance formula to use
        n is the maximum number of recommendations to make""""""
        self.k = k
        self.n = n
        self.username2id = {}
        self.userid2name = {}
        self.productid2name = {}
        # for some reason I want to save the name of the metric
        self.metric = metric
        if self.metric == 'pearson':
            self.fn = self.pearson
        #
        # if data is dictionary set recommender data to it
        #
        if type(data).__name__ == 'dict':
            self.data = data

    def convertProductID2name(self, id):
        """"""Given product id number return product name""""""
        if id in self.productid2name:
            return self.productid2name[id]
        else:
            return id


    def userRatings(self, id, n):
        """"""Return n top ratings for user with id""""""
        print (""Ratings for "" + self.userid2name[id])
        ratings = self.data[id]
        print(len(ratings))
        ratings = list(ratings.items())
        ratings = [(self.convertProductID2name(k), v)
                   for (k, v) in ratings]
        # finally sort and return
        ratings.sort(key=lambda artistTuple: artistTuple[1],
                     reverse = True)
        ratings = ratings[:n]
        for rating in ratings:
            print(""%s\t%i"" % (rating[0], rating[1]))




    def loadBookDB(self, path=''):
        """"""loads the BX book dataset. Path is where the BX files are
        located""""""
        self.data = {}
        i = 0
        #
        # First load book ratings into self.data
        #
        f = codecs.open(path + ""BX-Book-Ratings.csv"", 'r', 'utf8')
        for line in f:
            i += 1
            #separate line into fields
            fields = line.split(';')
            user = fields[0].strip('""')
            book = fields[1].strip('""')
            rating = int(fields[2].strip().strip('""'))
            if user in self.data:
                currentRatings = self.data[user]
            else:
                currentRatings = {}
            currentRatings[book] = rating
            self.data[user] = currentRatings
        f.close()
        #
        # Now load books into self.productid2name
        # Books contains isbn, title, and author among other fields
        #
        f = codecs.open(path + ""BX-Books.csv"", 'r', 'utf8')
        for line in f:
            i += 1
            #separate line into fields
            fields = line.split(';')
            isbn = fields[0].strip('""')
            title = fields[1].strip('""')
            author = fields[2].strip().strip('""')
            title = title + ' by ' + author
            self.productid2name[isbn] = title
        f.close()
        #
        #  Now load user info into both self.userid2name and
        #  self.username2id
        #
        f = codecs.open(path + ""BX-Users.csv"", 'r', 'utf8')
        for line in f:
            i += 1
            #print(line)
            #separate line into fields
            fields = line.split(';')
            userid = fields[0].strip('""')
            location = fields[1].strip('""')
            if len(fields) &gt; 3:
                age = fields[2].strip().strip('""')
            else:
                age = 'NULL'
            if age != 'NULL':
                value = location + '  (age: ' + age + ')'
            else:
                value = location
            self.userid2name[userid] = value
            self.username2id[location] = userid
        f.close()
        print(i)


    def pearson(self, rating1, rating2):
        sum_xy = 0
        sum_x = 0
        sum_y = 0
        sum_x2 = 0
        sum_y2 = 0
        n = 0
        for key in rating1:
            if key in rating2:
                n += 1
                x = rating1[key]
                y = rating2[key]
                sum_xy += x * y
                sum_x += x
                sum_y += y
                sum_x2 += pow(x, 2)
                sum_y2 += pow(y, 2)
        if n == 0:
            return 0
        # now compute denominator
        denominator = (sqrt(sum_x2 - pow(sum_x, 2) / n)
                       * sqrt(sum_y2 - pow(sum_y, 2) / n))
        if denominator == 0:
            return 0
        else:
            return (sum_xy - (sum_x * sum_y) / n) / denominator


    def computeNearestNeighbor(self, username):
        """"""creates a sorted list of users based on their distance to
        username""""""
        distances = []
        for instance in self.data:
            if instance != username:
                distance = self.fn(self.data[username],
                                   self.data[instance])
                distances.append((instance, distance))
        # sort based on distance -- closest first
        distances.sort(key=lambda artistTuple: artistTuple[1],
                       reverse=True)
        return distances

    def recommend(self, user):
       """"""Give list of recommendations""""""
       recommendations = {}
       # first get list of users  ordered by nearness
       nearest = self.computeNearestNeighbor(user)
       #
       # now get the ratings for the user
       #
       userRatings = self.data[user]
       #
       # determine the total distance
       totalDistance = 0.0
       for i in range(self.k):
          totalDistance += nearest[i][1]
       # now iterate through the k nearest neighbors
       # accumulating their ratings
       for i in range(self.k):
          # compute slice of pie 
          weight = nearest[i][1] / totalDistance
          # get the name of the person
          name = nearest[i][0]
          # get the ratings for this person
          neighborRatings = self.data[name]
          # get the name of the person
          # now find bands neighbor rated that user didn't
          for artist in neighborRatings:
             if not artist in userRatings:
                if artist not in recommendations:
                   recommendations[artist] = (neighborRatings[artist]
                                              * weight)
                else:
                   recommendations[artist] = (recommendations[artist]
                                              + neighborRatings[artist]
                                              * weight)
       # now make list from dictionary
       recommendations = list(recommendations.items())
       recommendations = [(self.convertProductID2name(k), v)
                          for (k, v) in recommendations]
       # finally sort and return
       recommendations.sort(key=lambda artistTuple: artistTuple[1],
                            reverse = True)
       # Return the first n items
       return recommendations[:self.n]

r = recommender(users)
# The author implementation
r.loadBookDB('/Users/mona/Downloads/BX-Dump/')

ratings = pd.read_csv('/Users/danialt/BX-CSV-Dump/BX-Book-Ratings.csv', sep="";"", quotechar=""\"""", escapechar=""\\"")
books = pd.read_csv('/Users/danialt/BX-CSV-Dump/BX-Books.csv', sep="";"", quotechar=""\"""", escapechar=""\\"")
users = pd.read_csv('/Users/danialt/BX-CSV-Dump/BX-Users.csv', sep="";"", quotechar=""\"""", escapechar=""\\"")



pivot_rating = ratings.pivot(index='User-ID', columns='ISBN', values='Book-Rating')
</code></pre>
";72499.0;"['monas-mbp:book mona$ sudo pip install python-dateutil\nRequirement already satisfied (use --upgrade to upgrade): python-dateutil in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python\nCleaning up...\nmonas-mbp:book mona$ python t1.py\nNo module named dateutil.parser\nTraceback (most recent call last):\n  File ""t1.py"", line 4, in <module>\n    import pandas as pd\n  File ""/Library/Python/2.7/site-packages/pandas/__init__.py"", line 6, in <module>\n    from . import hashtable, tslib, lib\n  File ""tslib.pyx"", line 31, in init pandas.tslib (pandas/tslib.c:48782)\nImportError: No module named dateutil.parser\n', 'import codecs \nfrom math import sqrt\nimport numpy as np\nimport pandas as pd\n\nusers = {""Angelica"": {""Blues Traveler"": 3.5, ""Broken Bells"": 2.0,\n                      ""Norah Jones"": 4.5, ""Phoenix"": 5.0,\n                      ""Slightly Stoopid"": 1.5,\n                      ""The Strokes"": 2.5, ""Vampire Weekend"": 2.0},\n\n         ""Bill"":{""Blues Traveler"": 2.0, ""Broken Bells"": 3.5,\n                 ""Deadmau5"": 4.0, ""Phoenix"": 2.0,\n                 ""Slightly Stoopid"": 3.5, ""Vampire Weekend"": 3.0},\n\n         ""Chan"": {""Blues Traveler"": 5.0, ""Broken Bells"": 1.0,\n                  ""Deadmau5"": 1.0, ""Norah Jones"": 3.0, ""Phoenix"": 5,\n                  ""Slightly Stoopid"": 1.0},\n\n         ""Dan"": {""Blues Traveler"": 3.0, ""Broken Bells"": 4.0,\n                 ""Deadmau5"": 4.5, ""Phoenix"": 3.0,\n                 ""Slightly Stoopid"": 4.5, ""The Strokes"": 4.0,\n                 ""Vampire Weekend"": 2.0},\n\n         ""Hailey"": {""Broken Bells"": 4.0, ""Deadmau5"": 1.0,\n                    ""Norah Jones"": 4.0, ""The Strokes"": 4.0,\n                    ""Vampire Weekend"": 1.0},\n\n         ""Jordyn"":  {""Broken Bells"": 4.5, ""Deadmau5"": 4.0,\n                     ""Norah Jones"": 5.0, ""Phoenix"": 5.0,\n                     ""Slightly Stoopid"": 4.5, ""The Strokes"": 4.0,\n                     ""Vampire Weekend"": 4.0},\n\n         ""Sam"": {""Blues Traveler"": 5.0, ""Broken Bells"": 2.0,\n                 ""Norah Jones"": 3.0, ""Phoenix"": 5.0,\n                 ""Slightly Stoopid"": 4.0, ""The Strokes"": 5.0},\n\n         ""Veronica"": {""Blues Traveler"": 3.0, ""Norah Jones"": 5.0,\n                      ""Phoenix"": 4.0, ""Slightly Stoopid"": 2.5,\n                      ""The Strokes"": 3.0}\n        }\n\n\n\nclass recommender:\n\n    def __init__(self, data, k=1, metric=\'pearson\', n=5):\n        """""" initialize recommender\n        currently, if data is dictionary the recommender is initialized\n        to it.\n        For all other data types of data, no initialization occurs\n        k is the k value for k nearest neighbor\n        metric is which distance formula to use\n        n is the maximum number of recommendations to make""""""\n        self.k = k\n        self.n = n\n        self.username2id = {}\n        self.userid2name = {}\n        self.productid2name = {}\n        # for some reason I want to save the name of the metric\n        self.metric = metric\n        if self.metric == \'pearson\':\n            self.fn = self.pearson\n        #\n        # if data is dictionary set recommender data to it\n        #\n        if type(data).__name__ == \'dict\':\n            self.data = data\n\n    def convertProductID2name(self, id):\n        """"""Given product id number return product name""""""\n        if id in self.productid2name:\n            return self.productid2name[id]\n        else:\n            return id\n\n\n    def userRatings(self, id, n):\n        """"""Return n top ratings for user with id""""""\n        print (""Ratings for "" + self.userid2name[id])\n        ratings = self.data[id]\n        print(len(ratings))\n        ratings = list(ratings.items())\n        ratings = [(self.convertProductID2name(k), v)\n                   for (k, v) in ratings]\n        # finally sort and return\n        ratings.sort(key=lambda artistTuple: artistTuple[1],\n                     reverse = True)\n        ratings = ratings[:n]\n        for rating in ratings:\n            print(""%s\\t%i"" % (rating[0], rating[1]))\n\n\n\n\n    def loadBookDB(self, path=\'\'):\n        """"""loads the BX book dataset. Path is where the BX files are\n        located""""""\n        self.data = {}\n        i = 0\n        #\n        # First load book ratings into self.data\n        #\n        f = codecs.open(path + ""BX-Book-Ratings.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #separate line into fields\n            fields = line.split(\';\')\n            user = fields[0].strip(\'""\')\n            book = fields[1].strip(\'""\')\n            rating = int(fields[2].strip().strip(\'""\'))\n            if user in self.data:\n                currentRatings = self.data[user]\n            else:\n                currentRatings = {}\n            currentRatings[book] = rating\n            self.data[user] = currentRatings\n        f.close()\n        #\n        # Now load books into self.productid2name\n        # Books contains isbn, title, and author among other fields\n        #\n        f = codecs.open(path + ""BX-Books.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #separate line into fields\n            fields = line.split(\';\')\n            isbn = fields[0].strip(\'""\')\n            title = fields[1].strip(\'""\')\n            author = fields[2].strip().strip(\'""\')\n            title = title + \' by \' + author\n            self.productid2name[isbn] = title\n        f.close()\n        #\n        #  Now load user info into both self.userid2name and\n        #  self.username2id\n        #\n        f = codecs.open(path + ""BX-Users.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #print(line)\n            #separate line into fields\n            fields = line.split(\';\')\n            userid = fields[0].strip(\'""\')\n            location = fields[1].strip(\'""\')\n            if len(fields) > 3:\n                age = fields[2].strip().strip(\'""\')\n            else:\n                age = \'NULL\'\n            if age != \'NULL\':\n                value = location + \'  (age: \' + age + \')\'\n            else:\n                value = location\n            self.userid2name[userid] = value\n            self.username2id[location] = userid\n        f.close()\n        print(i)\n\n\n    def pearson(self, rating1, rating2):\n        sum_xy = 0\n        sum_x = 0\n        sum_y = 0\n        sum_x2 = 0\n        sum_y2 = 0\n        n = 0\n        for key in rating1:\n            if key in rating2:\n                n += 1\n                x = rating1[key]\n                y = rating2[key]\n                sum_xy += x * y\n                sum_x += x\n                sum_y += y\n                sum_x2 += pow(x, 2)\n                sum_y2 += pow(y, 2)\n        if n == 0:\n            return 0\n        # now compute denominator\n        denominator = (sqrt(sum_x2 - pow(sum_x, 2) / n)\n                       * sqrt(sum_y2 - pow(sum_y, 2) / n))\n        if denominator == 0:\n            return 0\n        else:\n            return (sum_xy - (sum_x * sum_y) / n) / denominator\n\n\n    def computeNearestNeighbor(self, username):\n        """"""creates a sorted list of users based on their distance to\n        username""""""\n        distances = []\n        for instance in self.data:\n            if instance != username:\n                distance = self.fn(self.data[username],\n                                   self.data[instance])\n                distances.append((instance, distance))\n        # sort based on distance -- closest first\n        distances.sort(key=lambda artistTuple: artistTuple[1],\n                       reverse=True)\n        return distances\n\n    def recommend(self, user):\n       """"""Give list of recommendations""""""\n       recommendations = {}\n       # first get list of users  ordered by nearness\n       nearest = self.computeNearestNeighbor(user)\n       #\n       # now get the ratings for the user\n       #\n       userRatings = self.data[user]\n       #\n       # determine the total distance\n       totalDistance = 0.0\n       for i in range(self.k):\n          totalDistance += nearest[i][1]\n       # now iterate through the k nearest neighbors\n       # accumulating their ratings\n       for i in range(self.k):\n          # compute slice of pie \n          weight = nearest[i][1] / totalDistance\n          # get the name of the person\n          name = nearest[i][0]\n          # get the ratings for this person\n          neighborRatings = self.data[name]\n          # get the name of the person\n          # now find bands neighbor rated that user didn\'t\n          for artist in neighborRatings:\n             if not artist in userRatings:\n                if artist not in recommendations:\n                   recommendations[artist] = (neighborRatings[artist]\n                                              * weight)\n                else:\n                   recommendations[artist] = (recommendations[artist]\n                                              + neighborRatings[artist]\n                                              * weight)\n       # now make list from dictionary\n       recommendations = list(recommendations.items())\n       recommendations = [(self.convertProductID2name(k), v)\n                          for (k, v) in recommendations]\n       # finally sort and return\n       recommendations.sort(key=lambda artistTuple: artistTuple[1],\n                            reverse = True)\n       # Return the first n items\n       return recommendations[:self.n]\n\nr = recommender(users)\n# The author implementation\nr.loadBookDB(\'/Users/mona/Downloads/BX-Dump/\')\n\nratings = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Book-Ratings.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nbooks = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Books.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nusers = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Users.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\n\n\n\npivot_rating = ratings.pivot(index=\'User-ID\', columns=\'ISBN\', values=\'Book-Rating\')\n']";"['pandas', 'Python', 'monas-mbp:book mona$ sudo pip install python-dateutil\nRequirement already satisfied (use --upgrade to upgrade): python-dateutil in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python\nCleaning up...\nmonas-mbp:book mona$ python t1.py\nNo module named dateutil.parser\nTraceback (most recent call last):\n  File ""t1.py"", line 4, in <module>\n    import pandas as pd\n  File ""/Library/Python/2.7/site-packages/pandas/__init__.py"", line 6, in <module>\n    from . import hashtable, tslib, lib\n  File ""tslib.pyx"", line 31, in init pandas.tslib (pandas/tslib.c:48782)\nImportError: No module named dateutil.parser\n', 'import codecs \nfrom math import sqrt\nimport numpy as np\nimport pandas as pd\n\nusers = {""Angelica"": {""Blues Traveler"": 3.5, ""Broken Bells"": 2.0,\n                      ""Norah Jones"": 4.5, ""Phoenix"": 5.0,\n                      ""Slightly Stoopid"": 1.5,\n                      ""The Strokes"": 2.5, ""Vampire Weekend"": 2.0},\n\n         ""Bill"":{""Blues Traveler"": 2.0, ""Broken Bells"": 3.5,\n                 ""Deadmau5"": 4.0, ""Phoenix"": 2.0,\n                 ""Slightly Stoopid"": 3.5, ""Vampire Weekend"": 3.0},\n\n         ""Chan"": {""Blues Traveler"": 5.0, ""Broken Bells"": 1.0,\n                  ""Deadmau5"": 1.0, ""Norah Jones"": 3.0, ""Phoenix"": 5,\n                  ""Slightly Stoopid"": 1.0},\n\n         ""Dan"": {""Blues Traveler"": 3.0, ""Broken Bells"": 4.0,\n                 ""Deadmau5"": 4.5, ""Phoenix"": 3.0,\n                 ""Slightly Stoopid"": 4.5, ""The Strokes"": 4.0,\n                 ""Vampire Weekend"": 2.0},\n\n         ""Hailey"": {""Broken Bells"": 4.0, ""Deadmau5"": 1.0,\n                    ""Norah Jones"": 4.0, ""The Strokes"": 4.0,\n                    ""Vampire Weekend"": 1.0},\n\n         ""Jordyn"":  {""Broken Bells"": 4.5, ""Deadmau5"": 4.0,\n                     ""Norah Jones"": 5.0, ""Phoenix"": 5.0,\n                     ""Slightly Stoopid"": 4.5, ""The Strokes"": 4.0,\n                     ""Vampire Weekend"": 4.0},\n\n         ""Sam"": {""Blues Traveler"": 5.0, ""Broken Bells"": 2.0,\n                 ""Norah Jones"": 3.0, ""Phoenix"": 5.0,\n                 ""Slightly Stoopid"": 4.0, ""The Strokes"": 5.0},\n\n         ""Veronica"": {""Blues Traveler"": 3.0, ""Norah Jones"": 5.0,\n                      ""Phoenix"": 4.0, ""Slightly Stoopid"": 2.5,\n                      ""The Strokes"": 3.0}\n        }\n\n\n\nclass recommender:\n\n    def __init__(self, data, k=1, metric=\'pearson\', n=5):\n        """""" initialize recommender\n        currently, if data is dictionary the recommender is initialized\n        to it.\n        For all other data types of data, no initialization occurs\n        k is the k value for k nearest neighbor\n        metric is which distance formula to use\n        n is the maximum number of recommendations to make""""""\n        self.k = k\n        self.n = n\n        self.username2id = {}\n        self.userid2name = {}\n        self.productid2name = {}\n        # for some reason I want to save the name of the metric\n        self.metric = metric\n        if self.metric == \'pearson\':\n            self.fn = self.pearson\n        #\n        # if data is dictionary set recommender data to it\n        #\n        if type(data).__name__ == \'dict\':\n            self.data = data\n\n    def convertProductID2name(self, id):\n        """"""Given product id number return product name""""""\n        if id in self.productid2name:\n            return self.productid2name[id]\n        else:\n            return id\n\n\n    def userRatings(self, id, n):\n        """"""Return n top ratings for user with id""""""\n        print (""Ratings for "" + self.userid2name[id])\n        ratings = self.data[id]\n        print(len(ratings))\n        ratings = list(ratings.items())\n        ratings = [(self.convertProductID2name(k), v)\n                   for (k, v) in ratings]\n        # finally sort and return\n        ratings.sort(key=lambda artistTuple: artistTuple[1],\n                     reverse = True)\n        ratings = ratings[:n]\n        for rating in ratings:\n            print(""%s\\t%i"" % (rating[0], rating[1]))\n\n\n\n\n    def loadBookDB(self, path=\'\'):\n        """"""loads the BX book dataset. Path is where the BX files are\n        located""""""\n        self.data = {}\n        i = 0\n        #\n        # First load book ratings into self.data\n        #\n        f = codecs.open(path + ""BX-Book-Ratings.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #separate line into fields\n            fields = line.split(\';\')\n            user = fields[0].strip(\'""\')\n            book = fields[1].strip(\'""\')\n            rating = int(fields[2].strip().strip(\'""\'))\n            if user in self.data:\n                currentRatings = self.data[user]\n            else:\n                currentRatings = {}\n            currentRatings[book] = rating\n            self.data[user] = currentRatings\n        f.close()\n        #\n        # Now load books into self.productid2name\n        # Books contains isbn, title, and author among other fields\n        #\n        f = codecs.open(path + ""BX-Books.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #separate line into fields\n            fields = line.split(\';\')\n            isbn = fields[0].strip(\'""\')\n            title = fields[1].strip(\'""\')\n            author = fields[2].strip().strip(\'""\')\n            title = title + \' by \' + author\n            self.productid2name[isbn] = title\n        f.close()\n        #\n        #  Now load user info into both self.userid2name and\n        #  self.username2id\n        #\n        f = codecs.open(path + ""BX-Users.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #print(line)\n            #separate line into fields\n            fields = line.split(\';\')\n            userid = fields[0].strip(\'""\')\n            location = fields[1].strip(\'""\')\n            if len(fields) > 3:\n                age = fields[2].strip().strip(\'""\')\n            else:\n                age = \'NULL\'\n            if age != \'NULL\':\n                value = location + \'  (age: \' + age + \')\'\n            else:\n                value = location\n            self.userid2name[userid] = value\n            self.username2id[location] = userid\n        f.close()\n        print(i)\n\n\n    def pearson(self, rating1, rating2):\n        sum_xy = 0\n        sum_x = 0\n        sum_y = 0\n        sum_x2 = 0\n        sum_y2 = 0\n        n = 0\n        for key in rating1:\n            if key in rating2:\n                n += 1\n                x = rating1[key]\n                y = rating2[key]\n                sum_xy += x * y\n                sum_x += x\n                sum_y += y\n                sum_x2 += pow(x, 2)\n                sum_y2 += pow(y, 2)\n        if n == 0:\n            return 0\n        # now compute denominator\n        denominator = (sqrt(sum_x2 - pow(sum_x, 2) / n)\n                       * sqrt(sum_y2 - pow(sum_y, 2) / n))\n        if denominator == 0:\n            return 0\n        else:\n            return (sum_xy - (sum_x * sum_y) / n) / denominator\n\n\n    def computeNearestNeighbor(self, username):\n        """"""creates a sorted list of users based on their distance to\n        username""""""\n        distances = []\n        for instance in self.data:\n            if instance != username:\n                distance = self.fn(self.data[username],\n                                   self.data[instance])\n                distances.append((instance, distance))\n        # sort based on distance -- closest first\n        distances.sort(key=lambda artistTuple: artistTuple[1],\n                       reverse=True)\n        return distances\n\n    def recommend(self, user):\n       """"""Give list of recommendations""""""\n       recommendations = {}\n       # first get list of users  ordered by nearness\n       nearest = self.computeNearestNeighbor(user)\n       #\n       # now get the ratings for the user\n       #\n       userRatings = self.data[user]\n       #\n       # determine the total distance\n       totalDistance = 0.0\n       for i in range(self.k):\n          totalDistance += nearest[i][1]\n       # now iterate through the k nearest neighbors\n       # accumulating their ratings\n       for i in range(self.k):\n          # compute slice of pie \n          weight = nearest[i][1] / totalDistance\n          # get the name of the person\n          name = nearest[i][0]\n          # get the ratings for this person\n          neighborRatings = self.data[name]\n          # get the name of the person\n          # now find bands neighbor rated that user didn\'t\n          for artist in neighborRatings:\n             if not artist in userRatings:\n                if artist not in recommendations:\n                   recommendations[artist] = (neighborRatings[artist]\n                                              * weight)\n                else:\n                   recommendations[artist] = (recommendations[artist]\n                                              + neighborRatings[artist]\n                                              * weight)\n       # now make list from dictionary\n       recommendations = list(recommendations.items())\n       recommendations = [(self.convertProductID2name(k), v)\n                          for (k, v) in recommendations]\n       # finally sort and return\n       recommendations.sort(key=lambda artistTuple: artistTuple[1],\n                            reverse = True)\n       # Return the first n items\n       return recommendations[:self.n]\n\nr = recommender(users)\n# The author implementation\nr.loadBookDB(\'/Users/mona/Downloads/BX-Dump/\')\n\nratings = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Book-Ratings.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nbooks = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Books.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nusers = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Users.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\n\n\n\npivot_rating = ratings.pivot(index=\'User-ID\', columns=\'ISBN\', values=\'Book-Rating\')\n']"
333;3.0;0;20868394;;1;99;<python><pandas>;Changing a specific column name in pandas DataFrame;"<p>I was looking for an elegant way to change a specified column name in a <code>DataFrame</code>.</p>

<p>play data ...</p>

<pre><code>import pandas as pd
d = {
         'one': [1, 2, 3, 4, 5],
         'two': [9, 8, 7, 6, 5],
         'three': ['a', 'b', 'c', 'd', 'e']
    }
df = pd.DataFrame(d)
</code></pre>

<p>The most elegant solution I have found so far ...</p>

<pre><code>names = df.columns.tolist()
names[names.index('two')] = 'new_name'
df.columns = names
</code></pre>

<p>I was hoping for a simple one-liner ... this attempt failed ...</p>

<pre><code>df.columns[df.columns.tolist().index('one')] = 'another_name'
</code></pre>

<p>Any hints gratefully received.</p>
";115075.0;"[""import pandas as pd\nd = {\n         'one': [1, 2, 3, 4, 5],\n         'two': [9, 8, 7, 6, 5],\n         'three': ['a', 'b', 'c', 'd', 'e']\n    }\ndf = pd.DataFrame(d)\n"", ""names = df.columns.tolist()\nnames[names.index('two')] = 'new_name'\ndf.columns = names\n"", ""df.columns[df.columns.tolist().index('one')] = 'another_name'\n""]";"['DataFrame', ""import pandas as pd\nd = {\n         'one': [1, 2, 3, 4, 5],\n         'two': [9, 8, 7, 6, 5],\n         'three': ['a', 'b', 'c', 'd', 'e']\n    }\ndf = pd.DataFrame(d)\n"", ""names = df.columns.tolist()\nnames[names.index('two')] = 'new_name'\ndf.columns = names\n"", ""df.columns[df.columns.tolist().index('one')] = 'another_name'\n""]"
334;6.0;14;20906474;;1;104;<python><csv><pandas><concatenation>;Import multiple csv files into pandas and concatenate into one DataFrame;"<p>I would like to read several csv files from a directory into pandas and concatenate them into one big DataFrame. I have not been able to figure it out though. Here is what I have so far:</p>

<pre><code>import glob
import pandas as pd

# get data file names
path =r'C:\DRO\DCL_rawdata_files'
filenames = glob.glob(path + ""/*.csv"")

dfs = []
for filename in filenames:
    dfs.append(pd.read_csv(filename))

# Concatenate all data into one DataFrame
big_frame = pd.concat(dfs, ignore_index=True)
</code></pre>

<p>I guess I need some help within the for loop???</p>
";80289.0;"['import glob\nimport pandas as pd\n\n# get data file names\npath =r\'C:\\DRO\\DCL_rawdata_files\'\nfilenames = glob.glob(path + ""/*.csv"")\n\ndfs = []\nfor filename in filenames:\n    dfs.append(pd.read_csv(filename))\n\n# Concatenate all data into one DataFrame\nbig_frame = pd.concat(dfs, ignore_index=True)\n']";"['import glob\nimport pandas as pd\n\n# get data file names\npath =r\'C:\\DRO\\DCL_rawdata_files\'\nfilenames = glob.glob(path + ""/*.csv"")\n\ndfs = []\nfor filename in filenames:\n    dfs.append(pd.read_csv(filename))\n\n# Concatenate all data into one DataFrame\nbig_frame = pd.concat(dfs, ignore_index=True)\n']"
335;3.0;2;20937538;;1;60;<python><python-2.7><pandas><ipython><dataframe>;How to display pandas DataFrame of floats using a format string for columns?;"<p>I would like to display a pandas dataframe with a given format using <code>print()</code> and the IPython <code>display()</code>. For example:</p>

<pre><code>df = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],
                  index=['foo','bar','baz','quux'],
                  columns=['cost'])
print df

         cost
foo   123.4567
bar   234.5678
baz   345.6789
quux  456.7890
</code></pre>

<p>I would like to somehow coerce this into printing</p>

<pre><code>         cost
foo   $123.46
bar   $234.57
baz   $345.68
quux  $456.79
</code></pre>

<p>without having to modify the data itself or create a copy, just change the way it is displayed.</p>

<p>How can I do this?</p>
";56902.0;"[""df = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\nprint df\n\n         cost\nfoo   123.4567\nbar   234.5678\nbaz   345.6789\nquux  456.7890\n"", '         cost\nfoo   $123.46\nbar   $234.57\nbaz   $345.68\nquux  $456.79\n']";"['print()', 'display()', ""df = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\nprint df\n\n         cost\nfoo   123.4567\nbar   234.5678\nbaz   345.6789\nquux  456.7890\n"", '         cost\nfoo   $123.46\nbar   $234.57\nbaz   $345.68\nquux  $456.79\n']"
336;1.0;0;20965046;;1;23;<python><pandas><dataframe><cumulative-sum>;Cumulative sum and percentage on column?;"<p>I have a <code>DataFrame</code> like this:</p>

<p><code>df</code>:</p>

<pre><code> fruit    val1 val2
0 orange    15    3
1 apple     10   13
2 mango     5    5 
</code></pre>

<p>How do I get Pandas to give me a cumulative sum and percentage column on only <code>val1</code>?</p>

<p>Desired output:</p>

<p><code>df_with_cumsum</code>:</p>

<pre><code> fruit    val1 val2   cum_sum    cum_perc
0 orange    15    3    15          50.00
1 apple     10   13    25          83.33
2 mango     5    5     30          100.00
</code></pre>

<p>I tried <code>df.cumsum()</code>, but it's giving me this error:</p>

<blockquote>
  <p>TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''</p>
</blockquote>
";27193.0;[' fruit    val1 val2\n0 orange    15    3\n1 apple     10   13\n2 mango     5    5 \n', ' fruit    val1 val2   cum_sum    cum_perc\n0 orange    15    3    15          50.00\n1 apple     10   13    25          83.33\n2 mango     5    5     30          100.00\n'];['DataFrame', 'df', ' fruit    val1 val2\n0 orange    15    3\n1 apple     10   13\n2 mango     5    5 \n', 'val1', 'df_with_cumsum', ' fruit    val1 val2   cum_sum    cum_perc\n0 orange    15    3    15          50.00\n1 apple     10   13    25          83.33\n2 mango     5    5     30          100.00\n', 'df.cumsum()']
337;1.0;0;20970279;;1;24;<python><pandas>;how to do a left,right and mid of a string in a pandas dataframe;"<p>in a pandas dataframe how can I apply a sort of excel left('state',2) to only take the first two letters. Ideally I want to learn how to use left,right and mid in a dataframe too. So need an equivalent and not a ""trick"" for this specific example.</p>

<pre><code>data = {'state': ['Auckland', 'Otago', 'Wellington', 'Dunedin', 'Hamilton'],
'year': [2000, 2001, 2002, 2001, 2002],
'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}
df = pd.DataFrame(data)

print df

     pop       state  year
 0  1.5    Auckland  2000
 1  1.7       Otago  2001
 2  3.6  Wellington  2002
 3  2.4     Dunedin  2001
 4  2.9    Hamilton  2002
</code></pre>

<p>I want to get this:</p>

<pre><code>    pop       state     year  StateInitial
 0  1.5       Auckland    2000     Au
 1  1.7       Otago       2001     Ot
 2  3.6       Wellington  2002     We
 3  2.4       Dunedin     2001     Du
 4  2.9       Hamilton    2002     Ha
</code></pre>
";23529.0;"[""data = {'state': ['Auckland', 'Otago', 'Wellington', 'Dunedin', 'Hamilton'],\n'year': [2000, 2001, 2002, 2001, 2002],\n'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\ndf = pd.DataFrame(data)\n\nprint df\n\n     pop       state  year\n 0  1.5    Auckland  2000\n 1  1.7       Otago  2001\n 2  3.6  Wellington  2002\n 3  2.4     Dunedin  2001\n 4  2.9    Hamilton  2002\n"", '    pop       state     year  StateInitial\n 0  1.5       Auckland    2000     Au\n 1  1.7       Otago       2001     Ot\n 2  3.6       Wellington  2002     We\n 3  2.4       Dunedin     2001     Du\n 4  2.9       Hamilton    2002     Ha\n']";"[""data = {'state': ['Auckland', 'Otago', 'Wellington', 'Dunedin', 'Hamilton'],\n'year': [2000, 2001, 2002, 2001, 2002],\n'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\ndf = pd.DataFrame(data)\n\nprint df\n\n     pop       state  year\n 0  1.5    Auckland  2000\n 1  1.7       Otago  2001\n 2  3.6  Wellington  2002\n 3  2.4     Dunedin  2001\n 4  2.9    Hamilton  2002\n"", '    pop       state     year  StateInitial\n 0  1.5       Auckland    2000     Au\n 1  1.7       Otago       2001     Ot\n 2  3.6       Wellington  2002     We\n 3  2.4       Dunedin     2001     Du\n 4  2.9       Hamilton    2002     Ha\n']"
338;2.0;0;20995196;;1;23;<python><pandas><sum>;Python Pandas counting and summing specific conditions;"<p>Are there single functions in pandas to perform the equivalents of <a href=""http://office.microsoft.com/en-us/excel-help/sumifs-function-HA010047504.aspx"" rel=""noreferrer"">SUMIF</a>, which sums over a specific condition and <a href=""http://office.microsoft.com/en-us/excel-help/countifs-function-HA010047494.aspx"" rel=""noreferrer"">COUNTIF</a>, which counts values of specific conditions from Excel?</p>

<p>I know that there are many multiple step functions that can be used for</p>

<p>for example for <code>sumif</code> I can use <code>(df.map(lambda x: condition), or df.size())</code> then use <code>.sum()</code></p>

<p>and for <code>countif</code> I can use <code>(groupby functions</code> and look for my answer or use a filter and the <code>.count())</code> </p>

<p>Is there simple one step process to do these functions where you enter the condition and the data frame and you get the sum or counted results?</p>
";38942.0;[];['sumif', '(df.map(lambda x: condition), or df.size())', '.sum()', 'countif', '(groupby functions', '.count())']
339;1.0;0;21018654;;1;51;<python><types><pandas>;Strings in a DataFrame, but dtype is object;"<p>Why does Pandas tell me that I have objects, although every item in the selected column is a string  even after explicit conversion.</p>

<p>This is my DataFrame:</p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 56992 entries, 0 to 56991
Data columns (total 7 columns):
id            56992  non-null values
attr1         56992  non-null values
attr2         56992  non-null values
attr3         56992  non-null values
attr4         56992  non-null values
attr5         56992  non-null values
attr6         56992  non-null values
dtypes: int64(2), object(5)
</code></pre>

<p>Five of them are <code>dtype object</code>. I explicitly convert those objects to strings:</p>

<pre><code>for c in df.columns:
    if df[c].dtype == object:
        print ""convert "", df[c].name, "" to string""
        df[c] = df[c].astype(str)
</code></pre>

<p>Then, <code>df[""attr2""]</code> still has <code>dtype object</code>, although <code>type(df[""attr2""].ix[0]</code> reveals <code>str</code>, which is correct.</p>

<p>Pandas distinguishes between <code>int64</code> and <code>float64</code> and <code>object</code>. What is the logic behind it when there is no <code>dtype str</code>? Why is a <code>str</code> covered by <code>object</code>?</p>
";21985.0;"[""<class 'pandas.core.frame.DataFrame'>\nInt64Index: 56992 entries, 0 to 56991\nData columns (total 7 columns):\nid            56992  non-null values\nattr1         56992  non-null values\nattr2         56992  non-null values\nattr3         56992  non-null values\nattr4         56992  non-null values\nattr5         56992  non-null values\nattr6         56992  non-null values\ndtypes: int64(2), object(5)\n"", 'for c in df.columns:\n    if df[c].dtype == object:\n        print ""convert "", df[c].name, "" to string""\n        df[c] = df[c].astype(str)\n']";"[""<class 'pandas.core.frame.DataFrame'>\nInt64Index: 56992 entries, 0 to 56991\nData columns (total 7 columns):\nid            56992  non-null values\nattr1         56992  non-null values\nattr2         56992  non-null values\nattr3         56992  non-null values\nattr4         56992  non-null values\nattr5         56992  non-null values\nattr6         56992  non-null values\ndtypes: int64(2), object(5)\n"", 'dtype object', 'for c in df.columns:\n    if df[c].dtype == object:\n        print ""convert "", df[c].name, "" to string""\n        df[c] = df[c].astype(str)\n', 'df[""attr2""]', 'dtype object', 'type(df[""attr2""].ix[0]', 'str', 'int64', 'float64', 'object', 'dtype str', 'str', 'object']"
340;4.0;0;21104592;;1;44;<python><json><google-maps><pandas>;JSON to pandas DataFrame;"<p>What I am trying to do is extract elevation data from a google maps API along a path specified by latitude and longitude coordinates as follows:</p>

<pre><code>from urllib2 import Request, urlopen
import json

path1 = '42.974049,-81.205203|42.974298,-81.195755'
request=Request('http://maps.googleapis.com/maps/api/elevation/json?locations='+path1+'&amp;sensor=false')
response = urlopen(request)
elevations = response.read()
</code></pre>

<p>This gives me a data that looks like this:</p>

<pre><code>elevations.splitlines()

['{',
 '   ""results"" : [',
 '      {',
 '         ""elevation"" : 243.3462677001953,',
 '         ""location"" : {',
 '            ""lat"" : 42.974049,',
 '            ""lng"" : -81.205203',
 '         },',
 '         ""resolution"" : 19.08790397644043',
 '      },',
 '      {',
 '         ""elevation"" : 244.1318664550781,',
 '         ""location"" : {',
 '            ""lat"" : 42.974298,',
 '            ""lng"" : -81.19575500000001',
 '         },',
 '         ""resolution"" : 19.08790397644043',
 '      }',
 '   ],',
 '   ""status"" : ""OK""',
 '}']
</code></pre>

<p>when putting into as DataFrame here is what I get:</p>

<p><img src=""https://i.stack.imgur.com/t9aSp.png"" alt=""enter image description here""></p>

<pre><code>pd.read_json(elevations)
</code></pre>

<p>and here is what I want:</p>

<p><img src=""https://i.stack.imgur.com/4Kdlg.png"" alt=""enter image description here""></p>

<p>I'm not sure if this is possible, but mainly what I am looking for is a way to be able to put the elevation, latitude and longitude data together in a pandas dataframe (doesn't have to have fancy mutiline headers).</p>

<p>If any one can help or give some advice on working with this data that would be great! If you can't tell I haven't worked much with json data before...</p>

<p>EDIT:</p>

<p>This method isn't all that attractive but seems to work:</p>

<pre><code>data = json.loads(elevations)
lat,lng,el = [],[],[]
for result in data['results']:
    lat.append(result[u'location'][u'lat'])
    lng.append(result[u'location'][u'lng'])
    el.append(result[u'elevation'])
df = pd.DataFrame([lat,lng,el]).T
</code></pre>

<p>ends up dataframe having columns latitude, longitude, elevation</p>

<p><img src=""https://i.stack.imgur.com/seht1.png"" alt=""enter image description here""></p>
";64504.0;"[""from urllib2 import Request, urlopen\nimport json\n\npath1 = '42.974049,-81.205203|42.974298,-81.195755'\nrequest=Request('http://maps.googleapis.com/maps/api/elevation/json?locations='+path1+'&sensor=false')\nresponse = urlopen(request)\nelevations = response.read()\n"", 'elevations.splitlines()\n\n[\'{\',\n \'   ""results"" : [\',\n \'      {\',\n \'         ""elevation"" : 243.3462677001953,\',\n \'         ""location"" : {\',\n \'            ""lat"" : 42.974049,\',\n \'            ""lng"" : -81.205203\',\n \'         },\',\n \'         ""resolution"" : 19.08790397644043\',\n \'      },\',\n \'      {\',\n \'         ""elevation"" : 244.1318664550781,\',\n \'         ""location"" : {\',\n \'            ""lat"" : 42.974298,\',\n \'            ""lng"" : -81.19575500000001\',\n \'         },\',\n \'         ""resolution"" : 19.08790397644043\',\n \'      }\',\n \'   ],\',\n \'   ""status"" : ""OK""\',\n \'}\']\n', 'pd.read_json(elevations)\n', ""data = json.loads(elevations)\nlat,lng,el = [],[],[]\nfor result in data['results']:\n    lat.append(result[u'location'][u'lat'])\n    lng.append(result[u'location'][u'lng'])\n    el.append(result[u'elevation'])\ndf = pd.DataFrame([lat,lng,el]).T\n""]";"[""from urllib2 import Request, urlopen\nimport json\n\npath1 = '42.974049,-81.205203|42.974298,-81.195755'\nrequest=Request('http://maps.googleapis.com/maps/api/elevation/json?locations='+path1+'&sensor=false')\nresponse = urlopen(request)\nelevations = response.read()\n"", 'elevations.splitlines()\n\n[\'{\',\n \'   ""results"" : [\',\n \'      {\',\n \'         ""elevation"" : 243.3462677001953,\',\n \'         ""location"" : {\',\n \'            ""lat"" : 42.974049,\',\n \'            ""lng"" : -81.205203\',\n \'         },\',\n \'         ""resolution"" : 19.08790397644043\',\n \'      },\',\n \'      {\',\n \'         ""elevation"" : 244.1318664550781,\',\n \'         ""location"" : {\',\n \'            ""lat"" : 42.974298,\',\n \'            ""lng"" : -81.19575500000001\',\n \'         },\',\n \'         ""resolution"" : 19.08790397644043\',\n \'      }\',\n \'   ],\',\n \'   ""status"" : ""OK""\',\n \'}\']\n', 'pd.read_json(elevations)\n', ""data = json.loads(elevations)\nlat,lng,el = [],[],[]\nfor result in data['results']:\n    lat.append(result[u'location'][u'lat'])\n    lng.append(result[u'location'][u'lng'])\n    el.append(result[u'elevation'])\ndf = pd.DataFrame([lat,lng,el]).T\n""]"
341;1.0;3;21137150;;1;36;<python><pandas><floating-point><scientific-notation><numeric-format>;Format / Suppress Scientific Notation from Python Pandas Aggregation Results;"<p>How can one modify the format for the output from a groupby operation in pandas that produces scientific notation for very large numbers. I know how to do string formatting in pythong but I'm at a loss when it comes to applying it here. </p>

<pre><code>df1.groupby('dept')['data1'].sum()

dept
value1       1.192433e+08
value2       1.293066e+08
value3       1.077142e+08
</code></pre>

<p>This suppresses the scientific notation if I convert to string but now I'm just wondering how to string format and add decimals. </p>

<pre><code>sum_sales_dept.astype(str)
</code></pre>
";25062.0;"[""df1.groupby('dept')['data1'].sum()\n\ndept\nvalue1       1.192433e+08\nvalue2       1.293066e+08\nvalue3       1.077142e+08\n"", 'sum_sales_dept.astype(str)\n']";"[""df1.groupby('dept')['data1'].sum()\n\ndept\nvalue1       1.192433e+08\nvalue2       1.293066e+08\nvalue3       1.077142e+08\n"", 'sum_sales_dept.astype(str)\n']"
342;5.0;4;21197774;;1;48;<python><pandas>;Assign pandas dataframe column dtypes;"<p>I want to set the <code>dtype</code>s of multiple columns in <code>pd.Dataframe</code> (I have a file that I've had to manually parse into a list of lists, as the file was not amenable for <code>pd.read_csv</code>)</p>

<pre><code>import pandas as pd
print pd.DataFrame([['a','1'],['b','2']],
                   dtype={'x':'object','y':'int'},
                   columns=['x','y'])
</code></pre>

<p>I get</p>

<pre><code>ValueError: entry not a 2- or 3- tuple
</code></pre>

<p>The only way I can set them is by looping through each column variable and recasting with <code>astype</code>. </p>

<pre><code>dtypes = {'x':'object','y':'int'}
mydata = pd.DataFrame([['a','1'],['b','2']],
                      columns=['x','y'])
for c in mydata.columns:
    mydata[c] = mydata[c].astype(dtypes[c])
print mydata['y'].dtype   #=&gt; int64
</code></pre>

<p>Is there a better way?</p>
";50855.0;"[""import pandas as pd\nprint pd.DataFrame([['a','1'],['b','2']],\n                   dtype={'x':'object','y':'int'},\n                   columns=['x','y'])\n"", 'ValueError: entry not a 2- or 3- tuple\n', ""dtypes = {'x':'object','y':'int'}\nmydata = pd.DataFrame([['a','1'],['b','2']],\n                      columns=['x','y'])\nfor c in mydata.columns:\n    mydata[c] = mydata[c].astype(dtypes[c])\nprint mydata['y'].dtype   #=> int64\n""]";"['dtype', 'pd.Dataframe', 'pd.read_csv', ""import pandas as pd\nprint pd.DataFrame([['a','1'],['b','2']],\n                   dtype={'x':'object','y':'int'},\n                   columns=['x','y'])\n"", 'ValueError: entry not a 2- or 3- tuple\n', 'astype', ""dtypes = {'x':'object','y':'int'}\nmydata = pd.DataFrame([['a','1'],['b','2']],\n                      columns=['x','y'])\nfor c in mydata.columns:\n    mydata[c] = mydata[c].astype(dtypes[c])\nprint mydata['y'].dtype   #=> int64\n""]"
343;3.0;0;21269399;;1;24;<python><csv><datetime><pandas><dataframe>;datetime dtypes in pandas read_csv;"<p>I'm reading in a csv file with multiple datetime columns.  I'd need to set the data types upon reading in the file, but datetimes appear to be a problem.  For instance:</p>

<pre><code>headers = ['col1', 'col2', 'col3', 'col4']
dtypes = ['datetime', 'datetime', 'str', 'float']
pd.read_csv(file, sep='\t', header=None, names=headers, dtype=dtypes)
</code></pre>

<p>When run gives a error:</p>

<pre><code>TypeError: data type ""datetime"" not understood
</code></pre>

<p>Converting columns after the fact, via pandas.to_datetime() isn't an option I can't know which columns will be datetime objects.  That information can change and comes from whatever informs my dtypes list.</p>

<p>Alternatively, I've tried to load the csv file with numpy.genfromtxt, set the dtypes in that function, and then convert to a pandas.dataframe but it garbles the data.  Any help is greatly appreciated!</p>
";28062.0;"[""headers = ['col1', 'col2', 'col3', 'col4']\ndtypes = ['datetime', 'datetime', 'str', 'float']\npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes)\n"", 'TypeError: data type ""datetime"" not understood\n']";"[""headers = ['col1', 'col2', 'col3', 'col4']\ndtypes = ['datetime', 'datetime', 'str', 'float']\npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes)\n"", 'TypeError: data type ""datetime"" not understood\n']"
344;2.0;0;21285380;;1;31;<string><python-3.x><pandas><find>;Pandas: find column whose name contains a specific string;"<p>So, I have a dataframe with column names, and I want to find the one that contains a certain string, but does not exactly match it. I'm searching for <code>'spike'</code> in column names like <code>'spike-2'</code>, <code>'hey spike'</code>, <code>'spiked-in'</code> (the <code>'spike'</code> part is always continuous). </p>

<p>I want the column name to be returned as a string or a variable, so I access the column later with <code>df['name']</code> or <code>df[name]</code> as normal. I've tried to find ways to do this, to no avail. Any tips?</p>
";28697.0;[];"[""'spike'"", ""'spike-2'"", ""'hey spike'"", ""'spiked-in'"", ""'spike'"", ""df['name']"", 'df[name]']"
345;3.0;7;21287624;;1;46;<python><pandas><na>;Convert Pandas column containing NaNs to dtype `int`;"<p>I read data from a .csv file to a Pandas dataframe as below. For one of the columns, namely <code>id</code>, I want to specify the column type as <code>int</code>. The problem is the <code>id</code> series has missing/empty values.</p>

<p>When I try to cast the <code>id</code> column to integer while reading the .csv, I get:</p>

<pre><code>df= pd.read_csv(""data.csv"", dtype={'id': int}) 
error: Integer column has NA values
</code></pre>

<p>Alternatively, I tried to convert the column type after reading as below, but this time I get:</p>

<pre><code>df= pd.read_csv(""data.csv"") 
df[['id']] = df[['id']].astype(int)
error: Cannot convert NA to integer
</code></pre>

<p>How can I tackle this?</p>
";42262.0;"['df= pd.read_csv(""data.csv"", dtype={\'id\': int}) \nerror: Integer column has NA values\n', 'df= pd.read_csv(""data.csv"") \ndf[[\'id\']] = df[[\'id\']].astype(int)\nerror: Cannot convert NA to integer\n']";"['id', 'int', 'id', 'id', 'df= pd.read_csv(""data.csv"", dtype={\'id\': int}) \nerror: Integer column has NA values\n', 'df= pd.read_csv(""data.csv"") \ndf[[\'id\']] = df[[\'id\']].astype(int)\nerror: Cannot convert NA to integer\n']"
346;5.0;2;21291259;;1;58;<python><pandas>;Convert floats to ints in Pandas?;"<p>I've working with data imported from a CSV. Pandas changed some columns to float, so now the numbers in these columns get displayed as floating points! However, I need them to be displayed as integers, or, without comma. Is there a way to convert them to integers or not diplay the comma?</p>
";84803.0;[];[]
347;2.0;0;21319929;;1;26;<python><pandas>;How to determine whether a Pandas Column contains a particular value;"<p>I am trying to determine whether there is an entry in a Pandas column that has a particular value. I tried to do this with <code>if x in df['id']</code>. I thought this was working, except when I fed it a value that I knew was not in the column <code>43 in df['id']</code> it still returned <code>True</code>. When I subset to a data frame only containing entries matching the missing id <code>df[df['id'] == 43]</code> there are, obviously, no entries in it. How to I determine if a column in a Pandas data frame contains a particular value and why doesn't my current method work? (FYI, I have the same problem when I use the implementation in this <a href=""https://stackoverflow.com/a/19630449/2327821"">answer</a> to a similar question).</p>
";42502.0;[];"[""if x in df['id']"", ""43 in df['id']"", 'True', ""df[df['id'] == 43]""]"
348;4.0;0;21360361;;1;43;<python><matplotlib><pandas><ipython><ipython-notebook>;how to dynamically update a plot in a loop in ipython notebook (within one cell);"<p>Environment: Python 2.7, matplotlib 1.3, IPython notebook 1.1, linux, chrome. The code is in one single input cell, using <code>--pylab=inline</code></p>

<p>I want to use IPython notebook and pandas to consume a stream and dynamically update a plot every 5 seconds. </p>

<p>When I just use print statement to print the data in text format, it works perfectly fine: the output cell just keeps printing data and adding new rows. But when I try to plot the data (and then update it in a loop), the plot never show up in the output cell. But if I remove the loop, just plot it once. It works fine.</p>

<p>Then I did some simple test:</p>

<pre><code>i = pd.date_range('2013-1-1',periods=100,freq='s')
while True:
    plot(pd.Series(data=np.random.randn(100), index=i))
    #pd.Series(data=np.random.randn(100), index=i).plot() also tried this one
    time.sleep(5)
</code></pre>

<p>The output will not show anything until I manually interrupt the process (ctrl+m+i). And after I interrupt it, the plot shows correctly as multiple overlapped lines. But what I really want is a plot that shows up and gets updated every 5 seconds (or whenever the <code>plot()</code> function gets called, just like what print statement outputs I mentioned above, which works well). Only showing the final chart after the cell is completely done is NOT what i want.</p>

<p>I even tried to explicitly add draw() function after each <code>plot()</code>, etc. None of them works. Wonder how to dynamically update a plot by a for/while loop within one cell in IPython notebook.</p>
";30112.0;"[""i = pd.date_range('2013-1-1',periods=100,freq='s')\nwhile True:\n    plot(pd.Series(data=np.random.randn(100), index=i))\n    #pd.Series(data=np.random.randn(100), index=i).plot() also tried this one\n    time.sleep(5)\n""]";"['--pylab=inline', ""i = pd.date_range('2013-1-1',periods=100,freq='s')\nwhile True:\n    plot(pd.Series(data=np.random.randn(100), index=i))\n    #pd.Series(data=np.random.randn(100), index=i).plot() also tried this one\n    time.sleep(5)\n"", 'plot()', 'plot()']"
349;1.0;3;21415661;;1;32;<python><pandas><boolean-logic>;Logic operator for boolean indexing in Pandas;"<p>I'm working with boolean index in Pandas.
The question is why the statement:</p>

<pre><code>a[(a['some_column']==some_number) &amp; (a['some_other_column']==some_other_number)]
</code></pre>

<p>works fine whereas</p>

<pre><code>a[(a['some_column']==some_number) and (a['some_other_column']==some_other_number)]
</code></pre>

<p>exists with error?</p>

<p>Example:</p>

<pre><code>a=pd.DataFrame({'x':[1,1],'y':[10,20]})

In: a[(a['x']==1)&amp;(a['y']==10)]
Out:    x   y
     0  1  10

In: a[(a['x']==1) and (a['y']==10)]
Out: ValueError: The truth value of an array with more than one element is ambiguous.     Use a.any() or a.all()
</code></pre>
";38894.0;"[""a[(a['some_column']==some_number) & (a['some_other_column']==some_other_number)]\n"", ""a[(a['some_column']==some_number) and (a['some_other_column']==some_other_number)]\n"", ""a=pd.DataFrame({'x':[1,1],'y':[10,20]})\n\nIn: a[(a['x']==1)&(a['y']==10)]\nOut:    x   y\n     0  1  10\n\nIn: a[(a['x']==1) and (a['y']==10)]\nOut: ValueError: The truth value of an array with more than one element is ambiguous.     Use a.any() or a.all()\n""]";"[""a[(a['some_column']==some_number) & (a['some_other_column']==some_other_number)]\n"", ""a[(a['some_column']==some_number) and (a['some_other_column']==some_other_number)]\n"", ""a=pd.DataFrame({'x':[1,1],'y':[10,20]})\n\nIn: a[(a['x']==1)&(a['y']==10)]\nOut:    x   y\n     0  1  10\n\nIn: a[(a['x']==1) and (a['y']==10)]\nOut: ValueError: The truth value of an array with more than one element is ambiguous.     Use a.any() or a.all()\n""]"
350;2.0;0;21441259;;1;33;<python><group-by><pandas>;Pandas Groupby Range of Values;"<p>Is there an easy method in pandas to invoke <code>groupby</code> on a range of values increments? For instance given the example below can I bin and group column <code>B</code> with a <code>0.155</code> increment so that for example, the first couple of groups in column <code>B</code> are divided into ranges between <code>0, -0.155, 0.155 - 0.31 ...</code></p>

<pre><code>import numpy as np
import pandas as pd
df=pd.DataFrame({'A':np.random.random(20),'B':np.random.random(20)})

     A         B
0  0.383493  0.250785
1  0.572949  0.139555
2  0.652391  0.401983
3  0.214145  0.696935
4  0.848551  0.516692
</code></pre>

<p>Alternatively I could first categorize the data by those increments into a new column and subsequently use <code>groupby</code> to determine any relevant statistics that may be applicable in column <code>A</code>?</p>
";13283.0;"[""import numpy as np\nimport pandas as pd\ndf=pd.DataFrame({'A':np.random.random(20),'B':np.random.random(20)})\n\n     A         B\n0  0.383493  0.250785\n1  0.572949  0.139555\n2  0.652391  0.401983\n3  0.214145  0.696935\n4  0.848551  0.516692\n""]";"['groupby', 'B', '0.155', 'B', '0, -0.155, 0.155 - 0.31 ...', ""import numpy as np\nimport pandas as pd\ndf=pd.DataFrame({'A':np.random.random(20),'B':np.random.random(20)})\n\n     A         B\n0  0.383493  0.250785\n1  0.572949  0.139555\n2  0.652391  0.401983\n3  0.214145  0.696935\n4  0.848551  0.516692\n"", 'groupby', 'A']"
351;5.0;0;21487329;;1;62;<python><matplotlib><pandas>;Add x and y labels to a pandas plot;"<p>Suppose I have the following code that plots something very simple using pandas:</p>

<pre><code>import pandas as pd
values = [[1,2], [2,5]]
df2 = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])
df2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')
</code></pre>

<p><img src=""https://i.stack.imgur.com/LIkH3.png"" alt=""Output""></p>

<p>How do I easily set x and y-labels while preserving my ability to use specific colormaps? I noticed that the plot() wrapper for pandas dataframes doesn't take any parameters specific for that.</p>
";62710.0;"[""import pandas as pd\nvalues = [[1,2], [2,5]]\ndf2 = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])\ndf2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')\n""]";"[""import pandas as pd\nvalues = [[1,2], [2,5]]\ndf2 = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])\ndf2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')\n""]"
352;2.0;0;21606987;;1;26;<pandas>;How can I strip the whitespace from Pandas DataFrame headers?;"<p>I am parsing data from an Excel file that has extra white space in some of the column headings.</p>

<p>When I check the columns of the resulting dataframe, like so:</p>

<p><code>df.columns</code></p>

<p>The result looks like this:</p>

<p><code>Index(['Year', 'Month ', 'Value'])</code></p>

<p>Consequently, I can't run </p>

<p><code>df[""Month""]</code></p>

<p>Because it will tell me the column is not found, as I asked for ""Month"", not ""Month "".</p>

<p>My question, then, is how can I strip out the unwanted white space from the column headings?</p>
";14508.0;[];"['df.columns', ""Index(['Year', 'Month ', 'Value'])"", 'df[""Month""]']"
353;2.0;1;21608228;;1;26;<python><replace><pandas><conditional>;Conditional Replace Pandas;"<p>I'm probably doing something very stupid, but I'm stumped.</p>

<p>I have a dataframe and I want to replace the values in a particular column that exceed a value with zero. I had thought this was a way of achieving this:</p>

<pre><code>df[df.my_channel &gt; 20000].my_channel = 0
</code></pre>

<p>If I copy the channel into a new data frame it's simple:</p>

<pre><code>df2 = df.my_channel 

df2[df2 &gt; 20000] = 0
</code></pre>

<p>this does exactly what I want, but seems not to work with the channel as part of the original dataframe.</p>

<p>Thanks is advance.</p>

<p>Ben</p>
";30186.0;['df[df.my_channel > 20000].my_channel = 0\n', 'df2 = df.my_channel \n\ndf2[df2 > 20000] = 0\n'];['df[df.my_channel > 20000].my_channel = 0\n', 'df2 = df.my_channel \n\ndf2[df2 > 20000] = 0\n']
354;4.0;0;21654635;;1;42;<python><matplotlib><pandas>;Scatter plots in Pandas/Pyplot: How to plot by category;"<p>I am trying to make a simple scatter plot in pyplot using a Pandas DataFrame object, but want an efficient way of plotting two variables but have the symbols dictated by a third column (key). I have tried various ways using df.groupby, but not successfully. A sample df script is below. This colours the markers according to 'key1', but Id like to see a legend with 'key1' categories. Am I close? Thanks.</p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range('2010-01-01', freq = 'M', periods = 10), columns = ('one', 'two', 'three'))
df['key1'] = (4,4,4,6,6,6,8,8,8,8)
fig1 = plt.figure(1)
ax1 = fig1.add_subplot(111)
ax1.scatter(df['one'], df['two'], marker = 'o', c = df['key1'], alpha = 0.8)
plt.show()
</code></pre>
";44949.0;"[""import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range('2010-01-01', freq = 'M', periods = 10), columns = ('one', 'two', 'three'))\ndf['key1'] = (4,4,4,6,6,6,8,8,8,8)\nfig1 = plt.figure(1)\nax1 = fig1.add_subplot(111)\nax1.scatter(df['one'], df['two'], marker = 'o', c = df['key1'], alpha = 0.8)\nplt.show()\n""]";"[""import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range('2010-01-01', freq = 'M', periods = 10), columns = ('one', 'two', 'three'))\ndf['key1'] = (4,4,4,6,6,6,8,8,8,8)\nfig1 = plt.figure(1)\nax1 = fig1.add_subplot(111)\nax1.scatter(df['one'], df['two'], marker = 'o', c = df['key1'], alpha = 0.8)\nplt.show()\n""]"
355;2.0;0;21733893;;1;22;<python><lambda>;Pandas dataframe add a field based on multiple if statements;"<p>I'm quite new to Python and Pandas so this might be an obvious question.  </p>

<p>I have a dataframe with ages listed in it.  I want to create a new field with an age banding.  I can use the lambda statement to capture a single if / else statement but I want to use multiple if's e.g. <code>if age &lt; 18 then 'under 18' elif age &lt; 40 then 'under 40' else '&gt;40'</code>.</p>

<p>I don't think I can do this using lambda but am not sure how to do it in a different way.  I have this code so far:</p>

<pre><code>import pandas as pd
import numpy as n

d = {'Age' : pd.Series([36., 42., 6., 66., 38.]) }

df = pd.DataFrame(d)

df['Age_Group'] =  df['Age'].map(lambda x: '&lt;18' if x &lt; 19 else '&gt;18')

print(df)
</code></pre>
";32357.0;"[""import pandas as pd\nimport numpy as n\n\nd = {'Age' : pd.Series([36., 42., 6., 66., 38.]) }\n\ndf = pd.DataFrame(d)\n\ndf['Age_Group'] =  df['Age'].map(lambda x: '<18' if x < 19 else '>18')\n\nprint(df)\n""]";"[""if age < 18 then 'under 18' elif age < 40 then 'under 40' else '>40'"", ""import pandas as pd\nimport numpy as n\n\nd = {'Age' : pd.Series([36., 42., 6., 66., 38.]) }\n\ndf = pd.DataFrame(d)\n\ndf['Age_Group'] =  df['Age'].map(lambda x: '<18' if x < 19 else '>18')\n\nprint(df)\n""]"
356;1.0;0;21786490;;1;30;<python><sql><merge><pandas>;Pandas left outer join multiple dataframes on multiple columns;"<p>I am new to using DataFrame and I would like to know how to perform a SQL equivalent of left outer join on multiple columns on a series of tables</p>

<p>Example:</p>

<pre><code>df1: 
Year    Week    Colour    Val1 
2014       A       Red      50
2014       B       Red      60
2014       B     Black      70
2014       C       Red      10
2014       D     Green      20

df2:
Year    Week    Colour    Val2
2014       A     Black      30
2014       B     Black     100
2014       C     Green      50
2014       C       Red      20
2014       D       Red      40

df3:
Year    Week    Colour    Val3
2013       B       Red      60
2013       C     Black      80
2013       B     Black      10
2013       D     Green      20
2013       D       Red      50
</code></pre>

<p>Essentially I want to do something like this SQL code (Notice that df3 is not joined on Year):</p>

<pre><code>SELECT df1.*, df2.Val2, df3.Val3
FROM df1
  LEFT OUTER JOIN df2
    ON df1.Year = df2.Year
    AND df1.Week = df2.Week
    AND df1.Colour = df2.Colour
  LEFT OUTER JOIN df3
    ON df1.Week = df3.Week
    AND df1.Colour = df3.Colour
</code></pre>

<p>The result should look like:</p>

<pre><code>Year    Week    Colour    Val1    Val2    Val3
2014       A       Red      50    Null    Null
2014       B       Red      60    Null      60
2014       B     Black      70     100    Null
2014       C       Red      10      20    Null
2014       D     Green      20    Null    Null
</code></pre>

<p>I have tried using merge and join but can't figure out how to do it on multiple tables and when there are multiple joints involved. Could someone help me on this please?</p>

<p>Thanks</p>
";45960.0;['df1: \nYear    Week    Colour    Val1 \n2014       A       Red      50\n2014       B       Red      60\n2014       B     Black      70\n2014       C       Red      10\n2014       D     Green      20\n\ndf2:\nYear    Week    Colour    Val2\n2014       A     Black      30\n2014       B     Black     100\n2014       C     Green      50\n2014       C       Red      20\n2014       D       Red      40\n\ndf3:\nYear    Week    Colour    Val3\n2013       B       Red      60\n2013       C     Black      80\n2013       B     Black      10\n2013       D     Green      20\n2013       D       Red      50\n', 'SELECT df1.*, df2.Val2, df3.Val3\nFROM df1\n  LEFT OUTER JOIN df2\n    ON df1.Year = df2.Year\n    AND df1.Week = df2.Week\n    AND df1.Colour = df2.Colour\n  LEFT OUTER JOIN df3\n    ON df1.Week = df3.Week\n    AND df1.Colour = df3.Colour\n', 'Year    Week    Colour    Val1    Val2    Val3\n2014       A       Red      50    Null    Null\n2014       B       Red      60    Null      60\n2014       B     Black      70     100    Null\n2014       C       Red      10      20    Null\n2014       D     Green      20    Null    Null\n'];['df1: \nYear    Week    Colour    Val1 \n2014       A       Red      50\n2014       B       Red      60\n2014       B     Black      70\n2014       C       Red      10\n2014       D     Green      20\n\ndf2:\nYear    Week    Colour    Val2\n2014       A     Black      30\n2014       B     Black     100\n2014       C     Green      50\n2014       C       Red      20\n2014       D       Red      40\n\ndf3:\nYear    Week    Colour    Val3\n2013       B       Red      60\n2013       C     Black      80\n2013       B     Black      10\n2013       D     Green      20\n2013       D       Red      50\n', 'SELECT df1.*, df2.Val2, df3.Val3\nFROM df1\n  LEFT OUTER JOIN df2\n    ON df1.Year = df2.Year\n    AND df1.Week = df2.Week\n    AND df1.Colour = df2.Colour\n  LEFT OUTER JOIN df3\n    ON df1.Week = df3.Week\n    AND df1.Colour = df3.Colour\n', 'Year    Week    Colour    Val1    Val2    Val3\n2014       A       Red      50    Null    Null\n2014       B       Red      60    Null      60\n2014       B     Black      70     100    Null\n2014       C       Red      10      20    Null\n2014       D     Green      20    Null    Null\n']
357;2.0;0;21800169;;1;66;<python><indexing><pandas>;Python Pandas: Get index of rows which column matches certain value;"<p>Given a DataFrame with a column ""BoolCol"", we want to find the indexes of the DataFrame in which the values for ""BoolCol"" == True</p>

<p>I currently have the iterating way to do it, which works perfectly:</p>

<pre><code>for i in range(100,3000):
    if df.iloc[i]['BoolCol']== True:
         print i,df.iloc[i]['BoolCol']
</code></pre>

<p>But this is not the correct panda's way to do it.
After some research, I am currently using this code:</p>

<pre><code>df[df['BoolCol'] == True].index.tolist()
</code></pre>

<p>This one gives me a list of indexes, but they dont match, when I check them by doing:</p>

<pre><code>df.iloc[i]['BoolCol']
</code></pre>

<p>The result is actually False!!</p>

<p>Which would be the correct Pandas way to do this?</p>
";149908.0;"[""for i in range(100,3000):\n    if df.iloc[i]['BoolCol']== True:\n         print i,df.iloc[i]['BoolCol']\n"", ""df[df['BoolCol'] == True].index.tolist()\n"", ""df.iloc[i]['BoolCol']\n""]";"[""for i in range(100,3000):\n    if df.iloc[i]['BoolCol']== True:\n         print i,df.iloc[i]['BoolCol']\n"", ""df[df['BoolCol'] == True].index.tolist()\n"", ""df.iloc[i]['BoolCol']\n""]"
358;2.0;0;22005911;;1;42;<python><numpy><pandas>;Convert Columns to String in Pandas;"<p>I have the following DataFrame from a SQL query:</p>

<pre><code>(Pdb) pp total_rows
     ColumnID  RespondentCount
0          -1                2
1  3030096843                1
2  3030096845                1
</code></pre>

<p>and I want to pivot it like this:</p>

<pre><code>total_data = total_rows.pivot_table(cols=['ColumnID'])

(Pdb) pp total_data
ColumnID         -1            3030096843   3030096845
RespondentCount            2            1            1

[1 rows x 3 columns]


total_rows.pivot_table(cols=['ColumnID']).to_dict('records')[0]

{3030096843: 1, 3030096845: 1, -1: 2}
</code></pre>

<p>but I want to make sure the 303 columns are casted as strings instead of integers so that I get this:</p>

<pre><code>{'3030096843': 1, '3030096845': 1, -1: 2}
</code></pre>
";74333.0;"['(Pdb) pp total_rows\n     ColumnID  RespondentCount\n0          -1                2\n1  3030096843                1\n2  3030096845                1\n', ""total_data = total_rows.pivot_table(cols=['ColumnID'])\n\n(Pdb) pp total_data\nColumnID         -1            3030096843   3030096845\nRespondentCount            2            1            1\n\n[1 rows x 3 columns]\n\n\ntotal_rows.pivot_table(cols=['ColumnID']).to_dict('records')[0]\n\n{3030096843: 1, 3030096845: 1, -1: 2}\n"", ""{'3030096843': 1, '3030096845': 1, -1: 2}\n""]";"['(Pdb) pp total_rows\n     ColumnID  RespondentCount\n0          -1                2\n1  3030096843                1\n2  3030096845                1\n', ""total_data = total_rows.pivot_table(cols=['ColumnID'])\n\n(Pdb) pp total_data\nColumnID         -1            3030096843   3030096845\nRespondentCount            2            1            1\n\n[1 rows x 3 columns]\n\n\ntotal_rows.pivot_table(cols=['ColumnID']).to_dict('records')[0]\n\n{3030096843: 1, 3030096845: 1, -1: 2}\n"", ""{'3030096843': 1, '3030096845': 1, -1: 2}\n""]"
359;5.0;0;22084338;;1;28;<python><dictionary><pandas>;Pandas DataFrame performance;"<p>Pandas is really great, but I am really surprised by how inefficient it is to retrieve values from a Pandas.DataFrame.  In the following toy example, even the DataFrame.iloc method is more than 100 times slower than a dictionary.  </p>

<p>The question: Is the lesson here just that dictionaries are the better way to look up values?  Yes, I get that that is precisely what they were made for.  But I just wonder if there is something I am missing about DataFrame lookup performance.</p>

<p>I realize this question is more ""musing"" than ""asking"" but I will accept an answer that provides insight or perspective on this.  Thanks.</p>

<pre><code>import timeit

setup = '''
import numpy, pandas
df = pandas.DataFrame(numpy.zeros(shape=[10, 10]))
dictionary = df.to_dict()
'''

f = ['value = dictionary[5][5]', 'value = df.loc[5, 5]', 'value = df.iloc[5, 5]']

for func in f:
    print func
    print min(timeit.Timer(func, setup).repeat(3, 100000))
</code></pre>

<blockquote>
  <p>value = dictionary[5][5]</p>
  
  <p>0.130625009537</p>
  
  <p>value = df.loc[5, 5]</p>
  
  <p>19.4681699276</p>
  
  <p>value = df.iloc[5, 5]</p>
  
  <p>17.2575249672</p>
</blockquote>
";13142.0;"[""import timeit\n\nsetup = '''\nimport numpy, pandas\ndf = pandas.DataFrame(numpy.zeros(shape=[10, 10]))\ndictionary = df.to_dict()\n'''\n\nf = ['value = dictionary[5][5]', 'value = df.loc[5, 5]', 'value = df.iloc[5, 5]']\n\nfor func in f:\n    print func\n    print min(timeit.Timer(func, setup).repeat(3, 100000))\n""]";"[""import timeit\n\nsetup = '''\nimport numpy, pandas\ndf = pandas.DataFrame(numpy.zeros(shape=[10, 10]))\ndictionary = df.to_dict()\n'''\n\nf = ['value = dictionary[5][5]', 'value = df.loc[5, 5]', 'value = df.iloc[5, 5]']\n\nfor func in f:\n    print func\n    print min(timeit.Timer(func, setup).repeat(3, 100000))\n""]"
360;2.0;1;22086116;;1;31;<python><filter><pandas>;how do you filter pandas dataframes by multiple columns;"<p>To filter a dataframe (df) by a single column, if we consider data with male and females we might:</p>

<pre><code>males = df[df[Gender]=='Male']
</code></pre>

<p>Question 1 - But what if the data spanned multiple years and i wanted to only see males for 2014?</p>

<p>In other languages I might do something like: </p>

<pre><code>if A = ""Male"" and if B = ""2014"" then 
</code></pre>

<p>(except I want to do this and get a subset of the original dataframe in a new dataframe object)</p>

<p>Question 2. How do I do this in a loop, and create a dataframe object for each unique sets of year and gender (i.e. a df for: 2013-Male, 2013-Female, 2014-Male, and 2014-Female</p>

<pre><code>for y in year:

for g in gender:

df = .....
</code></pre>
";29798.0;"[""males = df[df[Gender]=='Male']\n"", 'if A = ""Male"" and if B = ""2014"" then \n', 'for y in year:\n\nfor g in gender:\n\ndf = .....\n']";"[""males = df[df[Gender]=='Male']\n"", 'if A = ""Male"" and if B = ""2014"" then \n', 'for y in year:\n\nfor g in gender:\n\ndf = .....\n']"
361;1.0;2;22127569;;1;24;<python><pandas>;Opposite of melt in python pandas;"<p>I cannot figure out how to do ""reverse melt"" using Pandas in python.
This is my starting data</p>

<pre><code>import pandas as pd

from StringIO import StringIO

origin = pd.read_table(StringIO('''label    type    value
x   a   1
x   b   2
x   c   3
y   a   4
y   b   5
y   c   6
z   a   7
z   b   8
z   c   9'''))

origin
Out[5]: 
  label type  value
0     x    a      1
1     x    b      2
2     x    c      3
3     y    a      4
4     y    b      5
5     y    c      6
6     z    a      7
7     z    b      8
8     z    c      9
</code></pre>

<p>This is what I would like to have:</p>

<pre><code>    label   a   b   c
        x   1   2   3
        y   4   5   6
        z   7   8   9
</code></pre>

<p>I'm sure there is an easy way to do this, but I don't know how.</p>
";3241.0;"[""import pandas as pd\n\nfrom StringIO import StringIO\n\norigin = pd.read_table(StringIO('''label    type    value\nx   a   1\nx   b   2\nx   c   3\ny   a   4\ny   b   5\ny   c   6\nz   a   7\nz   b   8\nz   c   9'''))\n\norigin\nOut[5]: \n  label type  value\n0     x    a      1\n1     x    b      2\n2     x    c      3\n3     y    a      4\n4     y    b      5\n5     y    c      6\n6     z    a      7\n7     z    b      8\n8     z    c      9\n"", '    label   a   b   c\n        x   1   2   3\n        y   4   5   6\n        z   7   8   9\n']";"[""import pandas as pd\n\nfrom StringIO import StringIO\n\norigin = pd.read_table(StringIO('''label    type    value\nx   a   1\nx   b   2\nx   c   3\ny   a   4\ny   b   5\ny   c   6\nz   a   7\nz   b   8\nz   c   9'''))\n\norigin\nOut[5]: \n  label type  value\n0     x    a      1\n1     x    b      2\n2     x    c      3\n3     y    a      4\n4     y    b      5\n5     y    c      6\n6     z    a      7\n7     z    b      8\n8     z    c      9\n"", '    label   a   b   c\n        x   1   2   3\n        y   4   5   6\n        z   7   8   9\n']"
362;1.0;0;22137723;;1;23;<python><pandas>;Convert number strings with commas in pandas DataFrame to float;"<p>I have a DataFrame that contains numbers as strings with commas for the thousands marker. I need to convert them to floats.</p>

<pre><code>a = [['1,200', '4,200'], ['7,000', '-0.03'], [ '5', '0']]
df=pandas.DataFrame(a)
</code></pre>

<p>I am guessing I need to use locale.atof. Indeed </p>

<pre><code>df[0].apply(locale.atof)
</code></pre>

<p>works as expected. I get a Series of floats.</p>

<p>But when I apply it to the DataFrame, I get an error.</p>

<pre><code>df.apply(locale.atof)
</code></pre>

<p>TypeError: (""cannot convert the series to "", u'occurred at index 0')</p>

<p>and</p>

<pre><code>df[0:1].apply(locale.atof)
</code></pre>

<p>gives the error</p>

<p>ValueError: ('invalid literal for float(): 1,200', u'occurred at index 0')</p>

<p>So, how do I convert this DataFrame of strings to a DataFrame of floats?</p>
";15553.0;"[""a = [['1,200', '4,200'], ['7,000', '-0.03'], [ '5', '0']]\ndf=pandas.DataFrame(a)\n"", 'df[0].apply(locale.atof)\n', 'df.apply(locale.atof)\n', 'df[0:1].apply(locale.atof)\n']";"[""a = [['1,200', '4,200'], ['7,000', '-0.03'], [ '5', '0']]\ndf=pandas.DataFrame(a)\n"", 'df[0].apply(locale.atof)\n', 'df.apply(locale.atof)\n', 'df[0:1].apply(locale.atof)\n']"
363;6.0;0;22149584;;1;86;<python><pandas>;What does axis in pandas mean?;"<p>Here is my code to generate a dataframe:</p>

<pre><code>import pandas as pd
import numpy as np

dff = pd.DataFrame(np.random.randn(1,2),columns=list('AB'))
</code></pre>

<p>then I got the dataframe:</p>

<pre><code>+------------+---------+--------+
|            |  A      |  B     |
+------------+---------+---------
|      0     | 0.626386| 1.52325|
+------------+---------+--------+
</code></pre>

<p>When I type the commmand :</p>

<pre><code>dff.mean(axis=1)
</code></pre>

<p>I got :</p>

<pre><code>0    1.074821
dtype: float64
</code></pre>

<p>According to the reference of pandas, axis=1 stands for columns and I expect the result of the command to be</p>

<pre><code>A    0.626386
B    1.523255
dtype: float64
</code></pre>

<p>So here is my question: what does axis in pandas mean?</p>
";39852.0;"[""import pandas as pd\nimport numpy as np\n\ndff = pd.DataFrame(np.random.randn(1,2),columns=list('AB'))\n"", '+------------+---------+--------+\n|            |  A      |  B     |\n+------------+---------+---------\n|      0     | 0.626386| 1.52325|\n+------------+---------+--------+\n', 'dff.mean(axis=1)\n', '0    1.074821\ndtype: float64\n', 'A    0.626386\nB    1.523255\ndtype: float64\n']";"[""import pandas as pd\nimport numpy as np\n\ndff = pd.DataFrame(np.random.randn(1,2),columns=list('AB'))\n"", '+------------+---------+--------+\n|            |  A      |  B     |\n+------------+---------+---------\n|      0     | 0.626386| 1.52325|\n+------------+---------+--------+\n', 'dff.mean(axis=1)\n', '0    1.074821\ndtype: float64\n', 'A    0.626386\nB    1.523255\ndtype: float64\n']"
364;2.0;4;22180993;;1;22;<python><pandas><flask>;Pandas Dataframe display on a webpage;"<p>I am using Flask but this probably applies to a lot of similar frameworks.</p>

<p>I construct a pandas Dataframe, e.g.</p>

<pre><code>@app.route('/analysis/&lt;filename&gt;')
def analysis(filename):
    x = pd.DataFrame(np.random.randn(20, 5))
    return render_template(""analysis.html"", name=filename, data=x)
</code></pre>

<p>The template analysis.html looks like</p>

<pre><code>{% extends ""base.html"" %}
{% block content %}
&lt;h1&gt;{{name}}&lt;/h1&gt;
{{data}}
{% endblock %}
</code></pre>

<p>This works but the output looks horrible. It doesn't use linebreaks etc.
I have played with <code>data.to_html()</code> and <code>data.to_string()</code>
What's the easiest way to display a frame?</p>
";13560.0;"['@app.route(\'/analysis/<filename>\')\ndef analysis(filename):\n    x = pd.DataFrame(np.random.randn(20, 5))\n    return render_template(""analysis.html"", name=filename, data=x)\n', '{% extends ""base.html"" %}\n{% block content %}\n<h1>{{name}}</h1>\n{{data}}\n{% endblock %}\n']";"['@app.route(\'/analysis/<filename>\')\ndef analysis(filename):\n    x = pd.DataFrame(np.random.randn(20, 5))\n    return render_template(""analysis.html"", name=filename, data=x)\n', '{% extends ""base.html"" %}\n{% block content %}\n<h1>{{name}}</h1>\n{{data}}\n{% endblock %}\n', 'data.to_html()', 'data.to_string()']"
365;2.0;0;22211737;;1;27;<python><pandas>;Python, pandas: how to sort dataframe by index;"<p>When there is an DataFrame like the following:</p>

<pre><code>import pandas as pd
df = pd.DataFrame([1, 1, 1, 1, 1], index=[100, 29, 234, 1, 150], columns=['A'])
</code></pre>

<p>How can I sort this dataframe by index with each combination of index and column value intact?</p>
";32238.0;"[""import pandas as pd\ndf = pd.DataFrame([1, 1, 1, 1, 1], index=[100, 29, 234, 1, 150], columns=['A'])\n""]";"[""import pandas as pd\ndf = pd.DataFrame([1, 1, 1, 1, 1], index=[100, 29, 234, 1, 150], columns=['A'])\n""]"
366;3.0;3;22219004;;1;42;<python><pandas>;grouping rows in list in pandas groupby;"<p>I have a pandas data frame like:</p>

<pre><code>A 1
A 2
B 5
B 5
B 4
C 6
</code></pre>

<p>I want to group by the first column and get second column as lists in rows:</p>

<pre><code>A [1,2]
B [5,5,4]
C [6]
</code></pre>

<p>Is it possible to do something like this using pandas groupby?</p>
";24488.0;['A 1\nA 2\nB 5\nB 5\nB 4\nC 6\n', 'A [1,2]\nB [5,5,4]\nC [6]\n'];['A 1\nA 2\nB 5\nB 5\nB 4\nC 6\n', 'A [1,2]\nB [5,5,4]\nC [6]\n']
367;4.0;0;22233488;;1;67;<python><pandas>;Pandas: drop a level from a multi-level column index?;"<p>If I've got a multi-level column index:</p>

<pre><code>&gt;&gt;&gt; cols = pd.MultiIndex.from_tuples([(""a"", ""b""), (""a"", ""c"")])
&gt;&gt;&gt; pd.DataFrame([[1,2], [3,4]], columns=cols)
</code></pre>

<pre>
    a
   ---+--
    b | c
--+---+--
0 | 1 | 2
1 | 3 | 4
</pre>

<p>How can I drop the ""a"" level of that index, so I end up with:</p>

<pre>
    b | c
--+---+--
0 | 1 | 2
1 | 3 | 4
</pre>
";36471.0;"['>>> cols = pd.MultiIndex.from_tuples([(""a"", ""b""), (""a"", ""c"")])\n>>> pd.DataFrame([[1,2], [3,4]], columns=cols)\n', '\n    a\n   ---+--\n    b | c\n--+---+--\n0 | 1 | 2\n1 | 3 | 4\n', '\n    b | c\n--+---+--\n0 | 1 | 2\n1 | 3 | 4\n']";"['>>> cols = pd.MultiIndex.from_tuples([(""a"", ""b""), (""a"", ""c"")])\n>>> pd.DataFrame([[1,2], [3,4]], columns=cols)\n']"
368;1.0;1;22235245;;1;21;<python><csv><pandas><dataframe>;Calculate summary statistics of columns in dataframe;"<p>I have a dataframe of the following form (for example)</p>

<pre><code>shopper_num,is_martian,number_of_items,count_pineapples,birth_country,tranpsortation_method
1,FALSE,0,0,MX,
2,FALSE,1,0,MX,
3,FALSE,0,0,MX,
4,FALSE,22,0,MX,
5,FALSE,0,0,MX,
6,FALSE,0,0,MX,
7,FALSE,5,0,MX,
8,FALSE,0,0,MX,
9,FALSE,4,0,MX,
10,FALSE,2,0,MX,
11,FALSE,0,0,MX,
12,FALSE,13,0,MX,
13,FALSE,0,0,CA,
14,FALSE,0,0,US,
</code></pre>

<p>How can I use Pandas to calculate summary statistics of each column (column data types are variable, some columns have no information </p>

<p>And then return the a dataframe of the form:</p>

<pre><code>columnname, max, min, median,

is_martian, NA, NA, FALSE
</code></pre>

<p>So on and so on</p>
";35787.0;['shopper_num,is_martian,number_of_items,count_pineapples,birth_country,tranpsortation_method\n1,FALSE,0,0,MX,\n2,FALSE,1,0,MX,\n3,FALSE,0,0,MX,\n4,FALSE,22,0,MX,\n5,FALSE,0,0,MX,\n6,FALSE,0,0,MX,\n7,FALSE,5,0,MX,\n8,FALSE,0,0,MX,\n9,FALSE,4,0,MX,\n10,FALSE,2,0,MX,\n11,FALSE,0,0,MX,\n12,FALSE,13,0,MX,\n13,FALSE,0,0,CA,\n14,FALSE,0,0,US,\n', 'columnname, max, min, median,\n\nis_martian, NA, NA, FALSE\n'];['shopper_num,is_martian,number_of_items,count_pineapples,birth_country,tranpsortation_method\n1,FALSE,0,0,MX,\n2,FALSE,1,0,MX,\n3,FALSE,0,0,MX,\n4,FALSE,22,0,MX,\n5,FALSE,0,0,MX,\n6,FALSE,0,0,MX,\n7,FALSE,5,0,MX,\n8,FALSE,0,0,MX,\n9,FALSE,4,0,MX,\n10,FALSE,2,0,MX,\n11,FALSE,0,0,MX,\n12,FALSE,13,0,MX,\n13,FALSE,0,0,CA,\n14,FALSE,0,0,US,\n', 'columnname, max, min, median,\n\nis_martian, NA, NA, FALSE\n']
369;2.0;7;22341271;;1;60;<python><list><pandas>;get list from pandas dataframe column;"<p>I have an excel document which looks like this..</p>

<pre><code>cluster load_date   budget  actual  fixed_price
A   1/1/2014    1000    4000    Y
A   2/1/2014    12000   10000   Y
A   3/1/2014    36000   2000    Y
B   4/1/2014    15000   10000   N
B   4/1/2014    12000   11500   N
B   4/1/2014    90000   11000   N
C   7/1/2014    22000   18000   N
C   8/1/2014    30000   28960   N
C   9/1/2014    53000   51200   N
</code></pre>

<p>I want to be able to return the contents of column 1 - cluster as a list, so I can run a for loop over it, and create an excel worksheet for every cluster.</p>

<p>Is it also possible, to return the contents of a whole row to a list? e.g.</p>

<pre><code>list = [], list[column1] or list[df.ix(row1)]
</code></pre>
";117410.0;['cluster load_date   budget  actual  fixed_price\nA   1/1/2014    1000    4000    Y\nA   2/1/2014    12000   10000   Y\nA   3/1/2014    36000   2000    Y\nB   4/1/2014    15000   10000   N\nB   4/1/2014    12000   11500   N\nB   4/1/2014    90000   11000   N\nC   7/1/2014    22000   18000   N\nC   8/1/2014    30000   28960   N\nC   9/1/2014    53000   51200   N\n', 'list = [], list[column1] or list[df.ix(row1)]\n'];['cluster load_date   budget  actual  fixed_price\nA   1/1/2014    1000    4000    Y\nA   2/1/2014    12000   10000   Y\nA   3/1/2014    36000   2000    Y\nB   4/1/2014    15000   10000   N\nB   4/1/2014    12000   11500   N\nB   4/1/2014    90000   11000   N\nC   7/1/2014    22000   18000   N\nC   8/1/2014    30000   28960   N\nC   9/1/2014    53000   51200   N\n', 'list = [], list[column1] or list[df.ix(row1)]\n']
370;9.0;7;22391433;;1;51;<python><pandas>;count the frequency that a value occurs in a dataframe column;"<p>I have a dataset</p>

<pre><code>|category|
cat a
cat b
cat a
</code></pre>

<p>I'd like to be able to return something like (showing unique values and frequency)</p>

<pre><code>category | freq |
cat a       2
cat b       1
</code></pre>
";98963.0;['|category|\ncat a\ncat b\ncat a\n', 'category | freq |\ncat a       2\ncat b       1\n'];['|category|\ncat a\ncat b\ncat a\n', 'category | freq |\ncat a       2\ncat b       1\n']
371;1.0;3;22403469;;1;21;<python><datetime><pandas>;Locate first and last non NaN values in a Pandas DataFrame;"<p>I have a Pandas <code>DataFrame</code> indexed by date. There a number of columns but many columns are only populated for part of the time series. I'd like to find where the first and last values non-<code>NaN</code> values are located so that I can extracts the dates and see how long the time series is for a particular column.</p>

<p>Could somebody point me in the right direction as to how I could go about doing something like this? Thanks in advance.</p>
";9357.0;[];['DataFrame', 'NaN']
372;5.0;0;22470690;;1;65;<python><pandas>;get list of pandas dataframe columns based on data type;"<p>If I have a dataframe with the following columns: </p>

<pre><code>1. NAME                                     object
2. On_Time                                      object
3. On_Budget                                    object
4. %actual_hr                                  float64
5. Baseline Start Date                  datetime64[ns]
6. Forecast Start Date                  datetime64[ns] 
</code></pre>

<p>I would like to be able to say: here is a dataframe, give me a list of the columns which are of type Object or of type DateTime?</p>

<p>I have a function which converts numbers (Float64) to two decimal places, and I would like to use this list of dataframe columns, of a particular type, and run it through this function to convert them all to 2dp.</p>

<p>Maybe:</p>

<pre><code>For c in col_list: if c.dtype = ""Something""
list[]
List.append(c)?
</code></pre>
";133397.0;"['1. NAME                                     object\n2. On_Time                                      object\n3. On_Budget                                    object\n4. %actual_hr                                  float64\n5. Baseline Start Date                  datetime64[ns]\n6. Forecast Start Date                  datetime64[ns] \n', 'For c in col_list: if c.dtype = ""Something""\nlist[]\nList.append(c)?\n']";"['1. NAME                                     object\n2. On_Time                                      object\n3. On_Budget                                    object\n4. %actual_hr                                  float64\n5. Baseline Start Date                  datetime64[ns]\n6. Forecast Start Date                  datetime64[ns] \n', 'For c in col_list: if c.dtype = ""Something""\nlist[]\nList.append(c)?\n']"
373;4.0;0;22483588;;1;38;<python><matplotlib><pandas>;How can I plot separate Pandas DataFrames as subplots?;"<p>I have a few Pandas DataFrames sharing the same value scale, but having different columns and indices. When invoking <code>df.plot()</code>, I get separate plot images. what I really want is to have them all in the same plot as subplots, but I'm unfortunately failing to come up with a solution to how and would highly appreciate some help. </p>
";34397.0;[];['df.plot()']
374;4.0;3;22543208;;1;21;<python><matplotlib><pandas><ipython>;ggplot styles in Python;"<p>When I look at the <a href=""http://pandas.pydata.org/pandas-docs/dev/visualization.html"">plotting style in the Pandas documentation</a>, the plots look different from the default one. It seems to mimic the ggplot ""look and feel"".</p>

<p>Same thing with the <a href=""http://nbviewer.ipython.org/github/mwaskom/seaborn/blob/master/examples/plotting_distributions.ipynb"">seaborn's package</a>.</p>

<p>How can I load that style? (even if I am not using a notebook?)</p>
";12579.0;[];[]
375;2.0;0;22546425;;1;28;<python><pandas>;using pandas to select rows conditional on multiple equivalencies;"<p>I have a pandas df and would like to accomplish something along these lines (in SQL terms):</p>

<pre><code>SELECT * FROM df WHERE column1 = 'a' OR column2 = 'b' OR column3 = 'c' etc...
</code></pre>

<p>Now this works, for one column/value pair:</p>

<pre><code>foo = df.ix[df['column']==value]
</code></pre>

<p>However, I'm not sure how to expand that to multiple column/value pairs</p>
";35625.0;"[""SELECT * FROM df WHERE column1 = 'a' OR column2 = 'b' OR column3 = 'c' etc...\n"", ""foo = df.ix[df['column']==value]\n""]";"[""SELECT * FROM df WHERE column1 = 'a' OR column2 = 'b' OR column3 = 'c' etc...\n"", ""foo = df.ix[df['column']==value]\n""]"
376;1.0;0;22551403;;1;38;<python><pandas><dataframe>;Python pandas Filtering out nan from a data selection of a column of strings;"<p>Without using <code>groupby</code> how would I filter out data without <code>NaN</code>?</p>

<p>Let say I have a matrix where customers will fill in 'N/A','n/a' or any of its variations and others leave it blank:</p>

<pre><code>import pandas as pd
import numpy as np


df = pd.DataFrame({'movie': ['thg', 'thg', 'mol', 'mol', 'lob', 'lob'],
                  'rating': [3., 4., 5., np.nan, np.nan, np.nan],
                  'name': ['John', np.nan, 'N/A', 'Graham', np.nan, np.nan]})

nbs = df['name'].str.extract('^(N/A|NA|na|n/a)')
nms=df[(df['name'] != nbs) ]
</code></pre>

<p>output:</p>

<pre><code>&gt;&gt;&gt; nms
  movie    name  rating
0   thg    John       3
1   thg     NaN       4
3   mol  Graham     NaN
4   lob     NaN     NaN
5   lob     NaN     NaN
</code></pre>

<p>How would I filter out NaN values so I can get results to work with like this:</p>

<pre><code>  movie    name  rating
0   thg    John       3
3   mol  Graham     NaN
</code></pre>

<p>I am guessing I need something like <code>~np.isnan</code> but the tilda does not work with strings.</p>
";55836.0;"[""import pandas as pd\nimport numpy as np\n\n\ndf = pd.DataFrame({'movie': ['thg', 'thg', 'mol', 'mol', 'lob', 'lob'],\n                  'rating': [3., 4., 5., np.nan, np.nan, np.nan],\n                  'name': ['John', np.nan, 'N/A', 'Graham', np.nan, np.nan]})\n\nnbs = df['name'].str.extract('^(N/A|NA|na|n/a)')\nnms=df[(df['name'] != nbs) ]\n"", '>>> nms\n  movie    name  rating\n0   thg    John       3\n1   thg     NaN       4\n3   mol  Graham     NaN\n4   lob     NaN     NaN\n5   lob     NaN     NaN\n', '  movie    name  rating\n0   thg    John       3\n3   mol  Graham     NaN\n']";"['groupby', 'NaN', ""import pandas as pd\nimport numpy as np\n\n\ndf = pd.DataFrame({'movie': ['thg', 'thg', 'mol', 'mol', 'lob', 'lob'],\n                  'rating': [3., 4., 5., np.nan, np.nan, np.nan],\n                  'name': ['John', np.nan, 'N/A', 'Graham', np.nan, np.nan]})\n\nnbs = df['name'].str.extract('^(N/A|NA|na|n/a)')\nnms=df[(df['name'] != nbs) ]\n"", '>>> nms\n  movie    name  rating\n0   thg    John       3\n1   thg     NaN       4\n3   mol  Graham     NaN\n4   lob     NaN     NaN\n5   lob     NaN     NaN\n', '  movie    name  rating\n0   thg    John       3\n3   mol  Graham     NaN\n', '~np.isnan']"
377;4.0;0;22588316;;1;25;<python><regex><pandas>;pandas applying regex to replace values;"<p>I have read some pricing data into a pandas dataframe the values appear as:</p>

<pre><code>$40,000*
$40000 conditions attached
</code></pre>

<p>I want to strip it down to just the numeric values.
I know I can loop through and apply regex </p>

<pre><code>[0-9]+
</code></pre>

<p>to each field then join the resulting list back together but is there a not loopy way?</p>

<p>Thanks</p>
";22689.0;['$40,000*\n$40000 conditions attached\n', '[0-9]+\n'];['$40,000*\n$40000 conditions attached\n', '[0-9]+\n']
378;2.0;1;22591174;;1;26;<python><pandas><boolean-logic>;pandas: multiple conditions while indexing data frame - unexpected behavior;"<p>I am filtering rows in a dataframe by values in two columns.</p>

<p>For some reason the OR operator behaves like I would expect AND operator to behave and vice versa.</p>

<p>My test code:</p>

<pre><code>import pandas as pd

df = pd.DataFrame({'a': range(5), 'b': range(5) })

# let's insert some -1 values
df['a'][1] = -1
df['b'][1] = -1
df['a'][3] = -1
df['b'][4] = -1

df1 = df[(df.a != -1) &amp; (df.b != -1)]
df2 = df[(df.a != -1) | (df.b != -1)]

print pd.concat([df, df1, df2], axis=1,
                keys = [ 'original df', 'using AND (&amp;)', 'using OR (|)',])
</code></pre>

<p>And the result:</p>

<pre><code>      original df      using AND (&amp;)      using OR (|)    
             a  b              a   b             a   b
0            0  0              0   0             0   0
1           -1 -1            NaN NaN           NaN NaN
2            2  2              2   2             2   2
3           -1  3            NaN NaN            -1   3
4            4 -1            NaN NaN             4  -1

[5 rows x 6 columns]
</code></pre>

<p>As you can see, the <code>AND</code> operator drops every row in which at least one value equals <code>-1</code>. On the other hand, the <code>OR</code> operator requires both values to be equal to <code>-1</code> to drop them. I would expect exactly the opposite result. Could anyone explain this behavior, please?</p>

<p>I am using pandas 0.13.1.</p>
";47480.0;"[""import pandas as pd\n\ndf = pd.DataFrame({'a': range(5), 'b': range(5) })\n\n# let's insert some -1 values\ndf['a'][1] = -1\ndf['b'][1] = -1\ndf['a'][3] = -1\ndf['b'][4] = -1\n\ndf1 = df[(df.a != -1) & (df.b != -1)]\ndf2 = df[(df.a != -1) | (df.b != -1)]\n\nprint pd.concat([df, df1, df2], axis=1,\n                keys = [ 'original df', 'using AND (&)', 'using OR (|)',])\n"", '      original df      using AND (&)      using OR (|)    \n             a  b              a   b             a   b\n0            0  0              0   0             0   0\n1           -1 -1            NaN NaN           NaN NaN\n2            2  2              2   2             2   2\n3           -1  3            NaN NaN            -1   3\n4            4 -1            NaN NaN             4  -1\n\n[5 rows x 6 columns]\n']";"[""import pandas as pd\n\ndf = pd.DataFrame({'a': range(5), 'b': range(5) })\n\n# let's insert some -1 values\ndf['a'][1] = -1\ndf['b'][1] = -1\ndf['a'][3] = -1\ndf['b'][4] = -1\n\ndf1 = df[(df.a != -1) & (df.b != -1)]\ndf2 = df[(df.a != -1) | (df.b != -1)]\n\nprint pd.concat([df, df1, df2], axis=1,\n                keys = [ 'original df', 'using AND (&)', 'using OR (|)',])\n"", '      original df      using AND (&)      using OR (|)    \n             a  b              a   b             a   b\n0            0  0              0   0             0   0\n1           -1 -1            NaN NaN           NaN NaN\n2            2  2              2   2             2   2\n3           -1  3            NaN NaN            -1   3\n4            4 -1            NaN NaN             4  -1\n\n[5 rows x 6 columns]\n', 'AND', '-1', 'OR', '-1']"
379;1.0;0;22604564;;1;76;<python><csv><pandas>;How to create a Pandas DataFrame from String;"<p>In order to test some functionality I would like to create a <code>DataFrame</code> from a string. Let's say my testdata looks like:</p>

<pre><code>TESTDATA=""""""col1;col2;col3
1;4.4;99
2;4.5;200
3;4.7;65
4;3.2;140
""""""
</code></pre>

<p>What is the simplest way to read that data into a Pandas <code>DataFrame</code>?</p>
";32697.0;"['TESTDATA=""""""col1;col2;col3\n1;4.4;99\n2;4.5;200\n3;4.7;65\n4;3.2;140\n""""""\n']";"['DataFrame', 'TESTDATA=""""""col1;col2;col3\n1;4.4;99\n2;4.5;200\n3;4.7;65\n4;3.2;140\n""""""\n', 'DataFrame']"
380;7.0;0;22649693;;1;22;<python><pandas>;Drop rows with all zeros in pandas data frame;"<p>I can use <code>pandas</code> <code>dropna()</code> functionality to remove rows with some or all columns set as <code>NA</code>'s. Is there an equivalent function for dropping rows with all columns having value 0?</p>

<pre><code>P   kt  b   tt  mky depth
1   0   0   0   0   0
2   0   0   0   0   0
3   0   0   0   0   0
4   0   0   0   0   0
5   1.1 3   4.5 2.3 9.0
</code></pre>

<p>In this example, we would like to drop the first 4 rows from the data frame.</p>

<p>thanks!</p>
";25347.0;['P   kt  b   tt  mky depth\n1   0   0   0   0   0\n2   0   0   0   0   0\n3   0   0   0   0   0\n4   0   0   0   0   0\n5   1.1 3   4.5 2.3 9.0\n'];['pandas', 'dropna()', 'NA', 'P   kt  b   tt  mky depth\n1   0   0   0   0   0\n2   0   0   0   0   0\n3   0   0   0   0   0\n4   0   0   0   0   0\n5   1.1 3   4.5 2.3 9.0\n']
381;4.0;1;22676081;;1;45;<python><pandas>;Pandas - The difference between join and merge;"<p>Suppose I have two DataFrames like so:</p>

<pre><code>left = pd.DataFrame({'key1': ['foo', 'bar'], 'lval': [1, 2]})

right = pd.DataFrame({'key2': ['foo', 'bar'], 'rval': [4, 5]})
</code></pre>

<p>I want to merge them, so I try something like this:</p>

<pre><code>pd.merge(left, right, left_on='key1', right_on='key2')
</code></pre>

<p>And I'm happy</p>

<pre><code>    key1    lval    key2    rval
0   foo     1       foo     4
1   bar     2       bar     5
</code></pre>

<p>But I'm trying to use the join method, which I've been lead to believe is pretty similar. </p>

<pre><code>left.join(right, on=['key1', 'key2'])
</code></pre>

<p>And I get this:</p>

<pre><code>//anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _validate_specification(self)
    406             if self.right_index:
    407                 if not ((len(self.left_on) == self.right.index.nlevels)):
--&gt; 408                     raise AssertionError()
    409                 self.right_on = [None] * n
    410         elif self.right_on is not None:

AssertionError: 
</code></pre>

<p>What am I missing?</p>
";22067.0;"[""left = pd.DataFrame({'key1': ['foo', 'bar'], 'lval': [1, 2]})\n\nright = pd.DataFrame({'key2': ['foo', 'bar'], 'rval': [4, 5]})\n"", ""pd.merge(left, right, left_on='key1', right_on='key2')\n"", '    key1    lval    key2    rval\n0   foo     1       foo     4\n1   bar     2       bar     5\n', ""left.join(right, on=['key1', 'key2'])\n"", '//anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _validate_specification(self)\n    406             if self.right_index:\n    407                 if not ((len(self.left_on) == self.right.index.nlevels)):\n--> 408                     raise AssertionError()\n    409                 self.right_on = [None] * n\n    410         elif self.right_on is not None:\n\nAssertionError: \n']";"[""left = pd.DataFrame({'key1': ['foo', 'bar'], 'lval': [1, 2]})\n\nright = pd.DataFrame({'key2': ['foo', 'bar'], 'rval': [4, 5]})\n"", ""pd.merge(left, right, left_on='key1', right_on='key2')\n"", '    key1    lval    key2    rval\n0   foo     1       foo     4\n1   bar     2       bar     5\n', ""left.join(right, on=['key1', 'key2'])\n"", '//anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _validate_specification(self)\n    406             if self.right_index:\n    407                 if not ((len(self.left_on) == self.right.index.nlevels)):\n--> 408                     raise AssertionError()\n    409                 self.right_on = [None] * n\n    410         elif self.right_on is not None:\n\nAssertionError: \n']"
382;4.0;3;22691010;;1;25;<python><pandas>;How to print a groupby object;"<p>I want to print the result of grouping with Pandas.</p>

<p>I have a dataframe:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'A': ['one', 'one', 'two', 'three', 'three', 'one'], 'B': range(6)})
print df

       A  B
0    one  0
1    one  1
2    two  2
3  three  3
4  three  4
5    one  5
</code></pre>

<p>When printing after grouping by 'A' I have the following:</p>

<pre><code>print df.groupby('A')

&lt;pandas.core.groupby.DataFrameGroupBy object at 0x05416E90&gt;
</code></pre>

<p>How can I print the dataframe grouped?</p>

<p>If I do:</p>

<pre><code>print df.groupby('A').head()
</code></pre>

<p>I obtain the dataframe as if it was not grouped:</p>

<pre><code>             A  B
A                
one   0    one  0
      1    one  1
two   2    two  2
three 3  three  3
      4  three  4
one   5    one  5
</code></pre>

<p>I was expecting something like:</p>

<pre><code>             A  B
A                
one   0    one  0
      1    one  1
      5    one  5
two   2    two  2
three 3  three  3
      4  three  4
</code></pre>
";15872.0;"[""import pandas as pd\ndf = pd.DataFrame({'A': ['one', 'one', 'two', 'three', 'three', 'one'], 'B': range(6)})\nprint df\n\n       A  B\n0    one  0\n1    one  1\n2    two  2\n3  three  3\n4  three  4\n5    one  5\n"", ""print df.groupby('A')\n\n<pandas.core.groupby.DataFrameGroupBy object at 0x05416E90>\n"", ""print df.groupby('A').head()\n"", '             A  B\nA                \none   0    one  0\n      1    one  1\ntwo   2    two  2\nthree 3  three  3\n      4  three  4\none   5    one  5\n', '             A  B\nA                \none   0    one  0\n      1    one  1\n      5    one  5\ntwo   2    two  2\nthree 3  three  3\n      4  three  4\n']";"[""import pandas as pd\ndf = pd.DataFrame({'A': ['one', 'one', 'two', 'three', 'three', 'one'], 'B': range(6)})\nprint df\n\n       A  B\n0    one  0\n1    one  1\n2    two  2\n3  three  3\n4  three  4\n5    one  5\n"", ""print df.groupby('A')\n\n<pandas.core.groupby.DataFrameGroupBy object at 0x05416E90>\n"", ""print df.groupby('A').head()\n"", '             A  B\nA                \none   0    one  0\n      1    one  1\ntwo   2    two  2\nthree 3  three  3\n      4  three  4\none   5    one  5\n', '             A  B\nA                \none   0    one  0\n      1    one  1\n      5    one  5\ntwo   2    two  2\nthree 3  three  3\n      4  three  4\n']"
383;3.0;1;22697773;;1;31;<python><pandas>;how to check the dtype of a column in python pandas;"<p>I need to use different functions to treat numeric columns and string columns. What I am doing now is really dumb:</p>

<pre><code>allc = list((agg.loc[:, (agg.dtypes==np.float64)|(agg.dtypes==np.int)]).columns)
for y in allc:
    treat_numeric(agg[y])    

allc = list((agg.loc[:, (agg.dtypes!=np.float64)&amp;(agg.dtypes!=np.int)]).columns)
for y in allc:
    treat_str(agg[y])    
</code></pre>

<p>Is there a more elegant way to do this? E.g.</p>

<pre><code>for y in agg.columns:
    if(dtype(agg[y]) == 'string'):
          treat_str(agg[y])
    elif(dtype(agg[y]) != 'string'):
          treat_numeric(agg[y])
</code></pre>
";62516.0;"['allc = list((agg.loc[:, (agg.dtypes==np.float64)|(agg.dtypes==np.int)]).columns)\nfor y in allc:\n    treat_numeric(agg[y])    \n\nallc = list((agg.loc[:, (agg.dtypes!=np.float64)&(agg.dtypes!=np.int)]).columns)\nfor y in allc:\n    treat_str(agg[y])    \n', ""for y in agg.columns:\n    if(dtype(agg[y]) == 'string'):\n          treat_str(agg[y])\n    elif(dtype(agg[y]) != 'string'):\n          treat_numeric(agg[y])\n""]";"['allc = list((agg.loc[:, (agg.dtypes==np.float64)|(agg.dtypes==np.int)]).columns)\nfor y in allc:\n    treat_numeric(agg[y])    \n\nallc = list((agg.loc[:, (agg.dtypes!=np.float64)&(agg.dtypes!=np.int)]).columns)\nfor y in allc:\n    treat_str(agg[y])    \n', ""for y in agg.columns:\n    if(dtype(agg[y]) == 'string'):\n          treat_str(agg[y])\n    elif(dtype(agg[y]) != 'string'):\n          treat_numeric(agg[y])\n""]"
384;4.0;0;22787209;;1;24;<python><pandas><matplotlib><plot><seaborn>;How to have clusters of stacked bars with python (Pandas);"<p>So here is how my data set looks like :</p>

<pre><code>In [1]: df1=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])

In [2]: df2=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])

In [3]: df1
Out[3]: 
          I         J
A  0.675616  0.177597
B  0.675693  0.598682
C  0.631376  0.598966
D  0.229858  0.378817

In [4]: df2
Out[4]: 
          I         J
A  0.939620  0.984616
B  0.314818  0.456252
C  0.630907  0.656341
D  0.020994  0.538303
</code></pre>

<p><strong>I want to have stacked bar plot for each dataframe but since they have same index, I'd like to have 2 stacked bars per index.</strong></p>

<p>I've tried to plot both on the same axes :</p>

<pre><code>In [5]: ax = df1.plot(kind=""bar"", stacked=True)

In [5]: ax2 = df2.plot(kind=""bar"", stacked=True, ax = ax)
</code></pre>

<p>But it overlaps.</p>

<p>Then I tried to concat the two dataset first :</p>

<pre><code>pd.concat(dict(df1 = df1, df2 = df2),axis = 1).plot(kind=""bar"", stacked=True)
</code></pre>

<p>but here everything is stacked</p>

<p>My best try is :</p>

<pre><code> pd.concat(dict(df1 = df1, df2 = df2),axis = 0).plot(kind=""bar"", stacked=True)
</code></pre>

<p>Which gives :</p>

<p><img src=""https://i.stack.imgur.com/dSSsI.png"" alt=""enter image description here""></p>

<p>This is basically what I want, except that I want the bar ordered as</p>

<p>(df1,A) (df2,A) (df1,B) (df2,B) etc...</p>

<p>I guess there is a trick but I can't found it !</p>

<hr>

<p>After @bgschiller's answer I got this :</p>

<p><img src=""https://i.stack.imgur.com/8Uk5l.png"" alt=""enter image description here""> </p>

<p>Which is almost what I want. I would like the bar to be <strong>clustered by index</strong>, in order to have something visually clear.</p>

<p><em>Bonus</em> : Having the x-label not redundant, something like : </p>

<pre><code>df1 df2    df1 df2
_______    _______ ...
   A          B
</code></pre>

<p>Thanks for helping.</p>
";7938.0;"['In [1]: df1=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])\n\nIn [2]: df2=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])\n\nIn [3]: df1\nOut[3]: \n          I         J\nA  0.675616  0.177597\nB  0.675693  0.598682\nC  0.631376  0.598966\nD  0.229858  0.378817\n\nIn [4]: df2\nOut[4]: \n          I         J\nA  0.939620  0.984616\nB  0.314818  0.456252\nC  0.630907  0.656341\nD  0.020994  0.538303\n', 'In [5]: ax = df1.plot(kind=""bar"", stacked=True)\n\nIn [5]: ax2 = df2.plot(kind=""bar"", stacked=True, ax = ax)\n', 'pd.concat(dict(df1 = df1, df2 = df2),axis = 1).plot(kind=""bar"", stacked=True)\n', ' pd.concat(dict(df1 = df1, df2 = df2),axis = 0).plot(kind=""bar"", stacked=True)\n', 'df1 df2    df1 df2\n_______    _______ ...\n   A          B\n']";"['In [1]: df1=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])\n\nIn [2]: df2=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])\n\nIn [3]: df1\nOut[3]: \n          I         J\nA  0.675616  0.177597\nB  0.675693  0.598682\nC  0.631376  0.598966\nD  0.229858  0.378817\n\nIn [4]: df2\nOut[4]: \n          I         J\nA  0.939620  0.984616\nB  0.314818  0.456252\nC  0.630907  0.656341\nD  0.020994  0.538303\n', 'In [5]: ax = df1.plot(kind=""bar"", stacked=True)\n\nIn [5]: ax2 = df2.plot(kind=""bar"", stacked=True, ax = ax)\n', 'pd.concat(dict(df1 = df1, df2 = df2),axis = 1).plot(kind=""bar"", stacked=True)\n', ' pd.concat(dict(df1 = df1, df2 = df2),axis = 0).plot(kind=""bar"", stacked=True)\n', 'df1 df2    df1 df2\n_______    _______ ...\n   A          B\n']"
385;4.0;0;22898824;;1;23;<python><datetime><pandas><filtering><dataframe>;filtering pandas dataframes on dates;"<p>I have a pandas data frame with a 'date' column. Now i need to filter out all rows in the dataframe that have dates outside of the next two months.
Essentially, I only need to retain the row that are within the next two months. 
What is the best way to achieve this.</p>
";50394.0;[];[]
386;2.0;0;22923775;;1;21;<python><datetime><pandas>;Calculate Pandas DataFrame Time Difference Between Two Columns in Hours and Minutes;"<p>I have two columns from and to date in a dataframe </p>

<p>when I try add new column diff with to find the difference between two date using</p>

<pre><code>df['diff'] = df['todate'] - df['fromdate']
</code></pre>

<p>I get the diff column in days if more than 24 hours.</p>

<pre><code>2014-01-24 13:03:12.050000,2014-01-26 23:41:21.870000,""2 days, 10:38:09.820000""
2014-01-27 11:57:18.240000,2014-01-27 15:38:22.540000,03:41:04.300000
2014-01-23 10:07:47.660000,2014-01-23 18:50:41.420000,08:42:53.760000
</code></pre>

<p>How do I convert my results only in hours and minutes ignoring days and even seconds. </p>
";31742.0;"[""df['diff'] = df['todate'] - df['fromdate']\n"", '2014-01-24 13:03:12.050000,2014-01-26 23:41:21.870000,""2 days, 10:38:09.820000""\n2014-01-27 11:57:18.240000,2014-01-27 15:38:22.540000,03:41:04.300000\n2014-01-23 10:07:47.660000,2014-01-23 18:50:41.420000,08:42:53.760000\n']";"[""df['diff'] = df['todate'] - df['fromdate']\n"", '2014-01-24 13:03:12.050000,2014-01-26 23:41:21.870000,""2 days, 10:38:09.820000""\n2014-01-27 11:57:18.240000,2014-01-27 15:38:22.540000,03:41:04.300000\n2014-01-23 10:07:47.660000,2014-01-23 18:50:41.420000,08:42:53.760000\n']"
387;4.0;3;22963263;;1;22;<python><pandas><dataframe>;Creating a zero-filled pandas data frame;"<p>What is the best way to create a zero-filled pandas data frame of a given size?</p>

<p>I have used: </p>

<pre><code>zero_data = np.zeros(shape=(len(data),len(feature_list)))
d = pd.DataFrame(zero_data, columns=feature_list)
</code></pre>

<p>Is there a better way to do it?</p>
";29486.0;['zero_data = np.zeros(shape=(len(data),len(feature_list)))\nd = pd.DataFrame(zero_data, columns=feature_list)\n'];['zero_data = np.zeros(shape=(len(data),len(feature_list)))\nd = pd.DataFrame(zero_data, columns=feature_list)\n']
388;2.0;0;23142967;;1;26;<pandas><dataframe><series>;Adding a column thats result of difference in consecutive rows in pandas;"<p>Lets say I have a dataframe like this</p>

<pre><code>    A   B
0   a   b
1   c   d
2   e   f 
3   g   h
</code></pre>

<p>0,1,2,3 are times, a, c, e, g is one time series and b, d, f, h is another time series.
I need to be able to add two columns to the orignal dataframe which is got by computing the differences of consecutive rows for certain columns. </p>

<p>So i need something like this</p>

<pre><code>    A   B   dA
0   a   b  (a-c)
1   c   d  (c-e)
2   e   f  (e-g)
3   g   h   Nan
</code></pre>

<p>I saw something called diff on the dataframe/series but that does it slightly differently as in first element will become Nan.</p>
";15505.0;['    A   B\n0   a   b\n1   c   d\n2   e   f \n3   g   h\n', '    A   B   dA\n0   a   b  (a-c)\n1   c   d  (c-e)\n2   e   f  (e-g)\n3   g   h   Nan\n'];['    A   B\n0   a   b\n1   c   d\n2   e   f \n3   g   h\n', '    A   B   dA\n0   a   b  (a-c)\n1   c   d  (c-e)\n2   e   f  (e-g)\n3   g   h   Nan\n']
389;6.0;1;23199796;;1;51;<python><pandas><filtering><dataframe><outliers>;Detect and exclude outliers in Pandas dataframe;"<p>I have a pandas dataframe with few columns.</p>

<p>Now I know that certain rows are outliers based on a certain column value.</p>

<p>For instance columns - 'Vol' has all values around 12.xx and one value which is 4000</p>

<p>Now I would like to exclude those rows that have Vol Column like this.</p>

<p>So essentially I need to put a filter such that we select all rows wehre the values of a certain column are within say 3 standard deviations from mean.</p>

<p>Whats an elegant way to achieve this. </p>
";40223.0;[];[]
390;1.0;1;23282130;;1;25;<python><pandas><pca><scientific-computing><principal-components>;Principal components analysis using pandas dataframe;"<p>How can I calculate Principal Components Analysis from data in a pandas dataframe?</p>
";13077.0;[];[]
391;1.0;0;23296282;;1;39;<python><pandas><indexing><dataframe><slice>;What rules does Pandas use to generate a view vs a copy?;"<p>I'm confused about the rules Pandas uses when deciding that a selection from a dataframe is a copy of the original dataframe, or a view on the original.</p>

<p>If I have, for example,</p>

<pre><code>df = pd.DataFrame(np.random.randn(8,8), columns=list('ABCDEFGH'), index=[1, 2, 3, 4, 5, 6, 7, 8])
</code></pre>

<p>I understand that a <code>query</code> returns a copy so that something like</p>

<pre><code>foo = df.query('2 &lt; index &lt;= 5')
foo.loc[:,'E'] = 40
</code></pre>

<p>will have no effect on the original dataframe, <code>df</code>. I also understand that scalar or named slices return a view, so that assignments to these, such as </p>

<pre><code>df.iloc[3] = 70
</code></pre>

<p>or </p>

<pre><code>df.ix[1,'B':'E'] = 222
</code></pre>

<p>will change <code>df</code>. But I'm lost when it comes to more complicated cases. For example, </p>

<pre><code>df[df.C &lt;= df.B]  = 7654321
</code></pre>

<p>changes <code>df</code>, but</p>

<pre><code>df[df.C &lt;= df.B].ix[:,'B':'E']
</code></pre>

<p>does not.</p>

<p>Is there a simple rule that Pandas is using that I'm just missing? What's going on in these specific cases; and in particular, how do I change all values (or a subset of values) in a dataframe that satisfy a particular query (as I'm attempting to do in the last example above)?</p>

<hr>

<p>Note: This is not the same as <a href=""https://stackoverflow.com/q/17960511/656912"">this question</a>; and I have read <a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html#returning-a-view-versus-a-copy"" rel=""noreferrer"">the documentation</a>, but am not enlightened by it. I've also read through the ""Related"" questions on this topic, but I'm still missing the simple rule Pandas is using, and how I'd apply it to  for example modify the values (or a subset of values) in a dataframe that satisfy a particular query.</p>
";8967.0;"[""df = pd.DataFrame(np.random.randn(8,8), columns=list('ABCDEFGH'), index=[1, 2, 3, 4, 5, 6, 7, 8])\n"", ""foo = df.query('2 < index <= 5')\nfoo.loc[:,'E'] = 40\n"", 'df.iloc[3] = 70\n', ""df.ix[1,'B':'E'] = 222\n"", 'df[df.C <= df.B]  = 7654321\n', ""df[df.C <= df.B].ix[:,'B':'E']\n""]";"[""df = pd.DataFrame(np.random.randn(8,8), columns=list('ABCDEFGH'), index=[1, 2, 3, 4, 5, 6, 7, 8])\n"", 'query', ""foo = df.query('2 < index <= 5')\nfoo.loc[:,'E'] = 40\n"", 'df', 'df.iloc[3] = 70\n', ""df.ix[1,'B':'E'] = 222\n"", 'df', 'df[df.C <= df.B]  = 7654321\n', 'df', ""df[df.C <= df.B].ix[:,'B':'E']\n""]"
392;6.0;1;23307301;;1;33;<python><pandas>;Pandas: Replacing column values in dataframe;"<p>I'm trying to replace the values in one column of a dataframe. The column ('female') only contains the values 'female' and 'male'. </p>

<p>I have tried the following:</p>

<pre><code>w['female']['female']='1'
w['female']['male']='0' 
</code></pre>

<p>But receive the exact same copy of the previous results.</p>

<p>I would ideally like to get some output which resembles the following loop element-wise.</p>

<pre><code>if w['female'] =='female':
    w['female'] = '1';
else:
    w['female'] = '0';
</code></pre>

<p>I've looked through the gotchas documentation (<a href=""http://pandas.pydata.org/pandas-docs/stable/gotchas.html"">http://pandas.pydata.org/pandas-docs/stable/gotchas.html</a>) but cannot figure out why nothing happens.</p>

<p>Any help will be appreciated.</p>
";84170.0;"[""w['female']['female']='1'\nw['female']['male']='0' \n"", ""if w['female'] =='female':\n    w['female'] = '1';\nelse:\n    w['female'] = '0';\n""]";"[""w['female']['female']='1'\nw['female']['male']='0' \n"", ""if w['female'] =='female':\n    w['female'] = '1';\nelse:\n    w['female'] = '0';\n""]"
393;2.0;0;23317342;;1;26;<python><split><pandas>;Pandas Dataframe: split column into multiple columns, right-align inconsistent cell entries;"<p>I have a pandas dataframe with a column named 'City, State, Country'. I want to separate this column into three new columns, 'City, 'State' and 'Country'.</p>

<pre><code>0                 HUN
1                 ESP
2                 GBR
3                 ESP
4                 FRA
5             ID, USA
6             GA, USA
7    Hoboken, NJ, USA
8             NJ, USA
9                 AUS
</code></pre>

<p>Splitting the column into three columns is trivial enough:</p>

<pre><code>location_df = df['City, State, Country'].apply(lambda x: pd.Series(x.split(',')))
</code></pre>

<p>However, this creates left-aligned data:</p>

<pre><code>     0       1       2
0    HUN     NaN     NaN
1    ESP     NaN     NaN
2    GBR     NaN     NaN
3    ESP     NaN     NaN
4    FRA     NaN     NaN
5    ID      USA     NaN
6    GA      USA     NaN
7    Hoboken  NJ     USA
8    NJ      USA     NaN
9    AUS     NaN     NaN
</code></pre>

<p>How would one go about creating the new columns with the data right-aligned? Would I need to iterate through every row, count the number of commas and handle the contents individually?</p>
";31355.0;"['0                 HUN\n1                 ESP\n2                 GBR\n3                 ESP\n4                 FRA\n5             ID, USA\n6             GA, USA\n7    Hoboken, NJ, USA\n8             NJ, USA\n9                 AUS\n', ""location_df = df['City, State, Country'].apply(lambda x: pd.Series(x.split(',')))\n"", '     0       1       2\n0    HUN     NaN     NaN\n1    ESP     NaN     NaN\n2    GBR     NaN     NaN\n3    ESP     NaN     NaN\n4    FRA     NaN     NaN\n5    ID      USA     NaN\n6    GA      USA     NaN\n7    Hoboken  NJ     USA\n8    NJ      USA     NaN\n9    AUS     NaN     NaN\n']";"['0                 HUN\n1                 ESP\n2                 GBR\n3                 ESP\n4                 FRA\n5             ID, USA\n6             GA, USA\n7    Hoboken, NJ, USA\n8             NJ, USA\n9                 AUS\n', ""location_df = df['City, State, Country'].apply(lambda x: pd.Series(x.split(',')))\n"", '     0       1       2\n0    HUN     NaN     NaN\n1    ESP     NaN     NaN\n2    GBR     NaN     NaN\n3    ESP     NaN     NaN\n4    FRA     NaN     NaN\n5    ID      USA     NaN\n6    GA      USA     NaN\n7    Hoboken  NJ     USA\n8    NJ      USA     NaN\n9    AUS     NaN     NaN\n']"
394;2.0;3;23330654;;1;38;<python><pandas><updates><dataframe>;Update a dataframe in pandas while iterating row by row;"<p>I have a pandas data frame that looks like this (its a pretty big one)</p>

<pre><code>           date      exer exp     ifor         mat  
1092  2014-03-17  American   M  528.205  2014-04-19 
1093  2014-03-17  American   M  528.205  2014-04-19 
1094  2014-03-17  American   M  528.205  2014-04-19 
1095  2014-03-17  American   M  528.205  2014-04-19    
1096  2014-03-17  American   M  528.205  2014-05-17 
</code></pre>

<p>now I would like to iterate row by row and as I go through each row, the value of <code>ifor</code>
in each row can change depending on some conditions and I need to lookup another dataframe.</p>

<p>Now, how do I update this as I iterate.
Tried a few things none of them worked.</p>

<pre><code>for i, row in df.iterrows():
    if &lt;something&gt;:
        row['ifor'] = x
    else:
        row['ifor'] = y

    df.ix[i]['ifor'] = x
</code></pre>

<p>None of these approaches seem to work. I don't see the values updated in the dataframe.</p>
";31600.0;"['           date      exer exp     ifor         mat  \n1092  2014-03-17  American   M  528.205  2014-04-19 \n1093  2014-03-17  American   M  528.205  2014-04-19 \n1094  2014-03-17  American   M  528.205  2014-04-19 \n1095  2014-03-17  American   M  528.205  2014-04-19    \n1096  2014-03-17  American   M  528.205  2014-05-17 \n', ""for i, row in df.iterrows():\n    if <something>:\n        row['ifor'] = x\n    else:\n        row['ifor'] = y\n\n    df.ix[i]['ifor'] = x\n""]";"['           date      exer exp     ifor         mat  \n1092  2014-03-17  American   M  528.205  2014-04-19 \n1093  2014-03-17  American   M  528.205  2014-04-19 \n1094  2014-03-17  American   M  528.205  2014-04-19 \n1095  2014-03-17  American   M  528.205  2014-04-19    \n1096  2014-03-17  American   M  528.205  2014-05-17 \n', 'ifor', ""for i, row in df.iterrows():\n    if <something>:\n        row['ifor'] = x\n    else:\n        row['ifor'] = y\n\n    df.ix[i]['ifor'] = x\n""]"
395;3.0;0;23377108;;1;29;<python><pandas>;Pandas percentage of total with groupby;"<p>This is obviously simple, but as a numpy newbe I'm getting stuck.</p>

<p>I have a CSV file that contains 3 columns, the State, the Office ID, and the Sales for that office.</p>

<p>I want to calculate the percentage of sales per office in a given state (total of all percentages in each state is 100%).</p>

<pre><code>df = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,
                   'office_id': range(1, 7) * 2,
                   'sales': [np.random.randint(100000, 999999)
                             for _ in range(12)]})

df.groupby(['state', 'office_id']).agg({'sales': 'sum'})
</code></pre>

<p>This returns:</p>

<pre><code>                  sales
state office_id        
AZ    2          839507
      4          373917
      6          347225
CA    1          798585
      3          890850
      5          454423
CO    1          819975
      3          202969
      5          614011
WA    2          163942
      4          369858
      6          959285
</code></pre>

<p>I can't seem to figure out how to ""reach up"" to the <code>state</code> level of the <code>groupby</code> to total up the <code>sales</code> for the entire <code>state</code> to calculate the fraction.</p>
";34307.0;"[""df = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,\n                   'office_id': range(1, 7) * 2,\n                   'sales': [np.random.randint(100000, 999999)\n                             for _ in range(12)]})\n\ndf.groupby(['state', 'office_id']).agg({'sales': 'sum'})\n"", '                  sales\nstate office_id        \nAZ    2          839507\n      4          373917\n      6          347225\nCA    1          798585\n      3          890850\n      5          454423\nCO    1          819975\n      3          202969\n      5          614011\nWA    2          163942\n      4          369858\n      6          959285\n']";"[""df = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,\n                   'office_id': range(1, 7) * 2,\n                   'sales': [np.random.randint(100000, 999999)\n                             for _ in range(12)]})\n\ndf.groupby(['state', 'office_id']).agg({'sales': 'sum'})\n"", '                  sales\nstate office_id        \nAZ    2          839507\n      4          373917\n      6          347225\nCA    1          798585\n      3          890850\n      5          454423\nCO    1          819975\n      3          202969\n      5          614011\nWA    2          163942\n      4          369858\n      6          959285\n', 'state', 'groupby', 'sales', 'state']"
396;4.0;1;23549231;;1;41;<python><pandas><ipython>;Check if a value exists in pandas dataframe index;"<p>I am sure there is an obvious way to do this but cant think of anything slick right now.</p>

<p>Basically instead of raising exception I would like to get <code>True</code> or <code>False</code> to see if a value exists in pandas <code>df</code> index.</p>

<pre><code>df = pandas.DataFrame({'test':[1,2,3,4]}, index=['a','b','c','d'])

df.loc['g']  # (should give False)
</code></pre>

<p>What I have working now is the following</p>

<pre><code>sum(df.index == 'g')
</code></pre>
";45304.0;"[""df = pandas.DataFrame({'test':[1,2,3,4]}, index=['a','b','c','d'])\n\ndf.loc['g']  # (should give False)\n"", ""sum(df.index == 'g')\n""]";"['True', 'False', 'df', ""df = pandas.DataFrame({'test':[1,2,3,4]}, index=['a','b','c','d'])\n\ndf.loc['g']  # (should give False)\n"", ""sum(df.index == 'g')\n""]"
397;5.0;3;23668427;;1;34;<python><pandas>;pandas joining multiple dataframes on columns;"<p>I have 3 CSV files. Each has the first column as the (string) names of people, while all the other columns in each dataframe are attributes of that person. </p>

<p>How can I ""join"" together all three CSV documents to create a single CSV with each row having all the attributes for each unique value of the person's string name?</p>

<p>The <code>join()</code> function in pandas specifies that I need a multiindex, but I'm confused about what a hierarchical indexing scheme has to do with making a join based on a single index. </p>
";46068.0;[];['join()']
398;4.0;1;23748995;;1;43;<python><pandas><tolist>;Pandas DataFrame to list;"<p>I am pulling a subset of data from a column based on conditions in another column being met.</p>

<p>I can get the correct values back but it is in pandas.core.frame.DataFrame.  How do I convert that to list?</p>

<pre><code>import pandas as pd

tst = pd.read_csv('C:\\SomeCSV.csv')

lookupValue = tst['SomeCol'] == ""SomeValue""
ID = tst[lookupValue][['SomeCol']]
#How To convert ID to a list
</code></pre>
";103159.0;"['import pandas as pd\n\ntst = pd.read_csv(\'C:\\\\SomeCSV.csv\')\n\nlookupValue = tst[\'SomeCol\'] == ""SomeValue""\nID = tst[lookupValue][[\'SomeCol\']]\n#How To convert ID to a list\n']";"['import pandas as pd\n\ntst = pd.read_csv(\'C:\\\\SomeCSV.csv\')\n\nlookupValue = tst[\'SomeCol\'] == ""SomeValue""\nID = tst[lookupValue][[\'SomeCol\']]\n#How To convert ID to a list\n']"
399;1.0;0;23853553;;1;23;<python><pandas>;Python Pandas: How to read only first n rows of CSV files in?;"<p>I have a very large data set and I can't afford to read the entire data set in. So, I'm thinking of reading only one chunk of it to train but I have no idea how to do it. Any thought will be appreciated.</p>
";18338.0;[];[]
400;1.0;5;24039023;;1;32;<python><pandas>;add column with constant value to pandas dataframe;"<p>I don't know why this puts NaN into 'new' column?</p>

<pre><code>df['new'] = pd.Series([0 for x in range(len(df.index))])
</code></pre>

<p>EDIT:</p>

<pre><code>df['new'] = 0 
</code></pre>

<p>works :)</p>
";32111.0;"[""df['new'] = pd.Series([0 for x in range(len(df.index))])\n"", ""df['new'] = 0 \n""]";"[""df['new'] = pd.Series([0 for x in range(len(df.index))])\n"", ""df['new'] = 0 \n""]"
401;1.0;2;24041436;;1;21;<python><pandas>;set multi index of an existing data frame in pandas;"<p>I have a <code>DataFrame</code> that looks like</p>

<pre><code>  Emp1    Empl2           date       Company
0    0        0     2012-05-01         apple
1    0        1     2012-05-29         apple
2    0        1     2013-05-02         apple
3    0        1     2013-11-22         apple
18   1        0     2011-09-09        google
19   1        0     2012-02-02        google
20   1        0     2012-11-26        google
21   1        0     2013-05-11        google
</code></pre>

<p>I want to pass the company and date for setting a <code>MultiIndex</code> for this <code>DataFrame</code>. Currently it has a default index. I am using <code>df.set_index(['Company', 'date'], inplace=True)</code></p>

<pre><code>df = pd.DataFrame()
for c in company_list:
        row = pd.DataFrame([dict(company = '%s' %s, date = datetime.date(2012, 05, 01))])
        df = df.append(row, ignore_index = True)
        for e in emp_list:
            dataset  = pd.read_sql(""select company, emp_name, date(date), count(*) from company_table where  = '""+s+""' and emp_name = '""+b+""' group by company, date, name LIMIT 5 "", con)
                if len(dataset) == 0:
                row = pd.DataFrame([dict(sitename='%s' %s, name = '%s' %b, date = datetime.date(2012, 05, 01), count = np.nan)])
                dataset = dataset.append(row, ignore_index=True)
            dataset = dataset.rename(columns = {'count': '%s' %b})
            dataset = dataset.groupby(['company', 'date', 'emp_name'], as_index = False).sum()

            dataset = dataset.drop('emp_name', 1)
            df = pd.merge(df, dataset, how = '')
            df = df.sort('date', ascending = True)
            df.fillna(0, inplace = True)

df.set_index(['Company', 'date'], inplace=True)            
print df
</code></pre>

<p>But when I print this <code>DataFrame</code>, it prints <code>None</code>. I saw this solution from stackoverflow it self. Is this not the correct way of doing it. Also I want to shuffle the positions of the columns company and date so that company becomes the first index, and date becomes the second in Hierarchy. Any ideas on this?</p>
";21648.0;"['  Emp1    Empl2           date       Company\n0    0        0     2012-05-01         apple\n1    0        1     2012-05-29         apple\n2    0        1     2013-05-02         apple\n3    0        1     2013-11-22         apple\n18   1        0     2011-09-09        google\n19   1        0     2012-02-02        google\n20   1        0     2012-11-26        google\n21   1        0     2013-05-11        google\n', 'df = pd.DataFrame()\nfor c in company_list:\n        row = pd.DataFrame([dict(company = \'%s\' %s, date = datetime.date(2012, 05, 01))])\n        df = df.append(row, ignore_index = True)\n        for e in emp_list:\n            dataset  = pd.read_sql(""select company, emp_name, date(date), count(*) from company_table where  = \'""+s+""\' and emp_name = \'""+b+""\' group by company, date, name LIMIT 5 "", con)\n                if len(dataset) == 0:\n                row = pd.DataFrame([dict(sitename=\'%s\' %s, name = \'%s\' %b, date = datetime.date(2012, 05, 01), count = np.nan)])\n                dataset = dataset.append(row, ignore_index=True)\n            dataset = dataset.rename(columns = {\'count\': \'%s\' %b})\n            dataset = dataset.groupby([\'company\', \'date\', \'emp_name\'], as_index = False).sum()\n\n            dataset = dataset.drop(\'emp_name\', 1)\n            df = pd.merge(df, dataset, how = \'\')\n            df = df.sort(\'date\', ascending = True)\n            df.fillna(0, inplace = True)\n\ndf.set_index([\'Company\', \'date\'], inplace=True)            \nprint df\n']";"['DataFrame', '  Emp1    Empl2           date       Company\n0    0        0     2012-05-01         apple\n1    0        1     2012-05-29         apple\n2    0        1     2013-05-02         apple\n3    0        1     2013-11-22         apple\n18   1        0     2011-09-09        google\n19   1        0     2012-02-02        google\n20   1        0     2012-11-26        google\n21   1        0     2013-05-11        google\n', 'MultiIndex', 'DataFrame', ""df.set_index(['Company', 'date'], inplace=True)"", 'df = pd.DataFrame()\nfor c in company_list:\n        row = pd.DataFrame([dict(company = \'%s\' %s, date = datetime.date(2012, 05, 01))])\n        df = df.append(row, ignore_index = True)\n        for e in emp_list:\n            dataset  = pd.read_sql(""select company, emp_name, date(date), count(*) from company_table where  = \'""+s+""\' and emp_name = \'""+b+""\' group by company, date, name LIMIT 5 "", con)\n                if len(dataset) == 0:\n                row = pd.DataFrame([dict(sitename=\'%s\' %s, name = \'%s\' %b, date = datetime.date(2012, 05, 01), count = np.nan)])\n                dataset = dataset.append(row, ignore_index=True)\n            dataset = dataset.rename(columns = {\'count\': \'%s\' %b})\n            dataset = dataset.groupby([\'company\', \'date\', \'emp_name\'], as_index = False).sum()\n\n            dataset = dataset.drop(\'emp_name\', 1)\n            df = pd.merge(df, dataset, how = \'\')\n            df = df.sort(\'date\', ascending = True)\n            df.fillna(0, inplace = True)\n\ndf.set_index([\'Company\', \'date\'], inplace=True)            \nprint df\n', 'DataFrame', 'None']"
402;1.0;0;24082784;;1;27;<python><datetime><pandas>;pandas dataframe groupby datetime month;"<p>Consider a csv file:</p>

<pre><code>string,date,number
a string,2/5/11 9:16am,1.0
a string,3/5/11 10:44pm,2.0
a string,4/22/11 12:07pm,3.0
a string,4/22/11 12:10pm,4.0
a string,4/29/11 11:59am,1.0
a string,5/2/11 1:41pm,2.0
a string,5/2/11 2:02pm,3.0
a string,5/2/11 2:56pm,4.0
a string,5/2/11 3:00pm,5.0
a string,5/2/14 3:02pm,6.0
a string,5/2/14 3:18pm,7.0
</code></pre>

<p>I can read this in, and reformat the date column into datetime format:</p>

<pre><code>b=pd.read_csv('b.dat')
b['date']=pd.to_datetime(b['date'],format='%m/%d/%y %I:%M%p')
</code></pre>

<p>I have been trying to group the data by month. It seems like there should be an obvious way of accessing the month and grouping by that. But I can't seem to do it. Does anyone know how?</p>

<p>What I am currently trying is re-indexing by the date:</p>

<pre><code>b.index=b['date']
</code></pre>

<p>I can access the month like so:</p>

<pre><code>b.index.month
</code></pre>

<p>However I can't seem to find a function to lump together by month.</p>
";26219.0;"['string,date,number\na string,2/5/11 9:16am,1.0\na string,3/5/11 10:44pm,2.0\na string,4/22/11 12:07pm,3.0\na string,4/22/11 12:10pm,4.0\na string,4/29/11 11:59am,1.0\na string,5/2/11 1:41pm,2.0\na string,5/2/11 2:02pm,3.0\na string,5/2/11 2:56pm,4.0\na string,5/2/11 3:00pm,5.0\na string,5/2/14 3:02pm,6.0\na string,5/2/14 3:18pm,7.0\n', ""b=pd.read_csv('b.dat')\nb['date']=pd.to_datetime(b['date'],format='%m/%d/%y %I:%M%p')\n"", ""b.index=b['date']\n"", 'b.index.month\n']";"['string,date,number\na string,2/5/11 9:16am,1.0\na string,3/5/11 10:44pm,2.0\na string,4/22/11 12:07pm,3.0\na string,4/22/11 12:10pm,4.0\na string,4/29/11 11:59am,1.0\na string,5/2/11 1:41pm,2.0\na string,5/2/11 2:02pm,3.0\na string,5/2/11 2:56pm,4.0\na string,5/2/11 3:00pm,5.0\na string,5/2/14 3:02pm,6.0\na string,5/2/14 3:18pm,7.0\n', ""b=pd.read_csv('b.dat')\nb['date']=pd.to_datetime(b['date'],format='%m/%d/%y %I:%M%p')\n"", ""b.index=b['date']\n"", 'b.index.month\n']"
403;12.0;0;24147278;;1;97;<python><python-2.7><pandas><dataframe>;How do I create test and train samples from one dataframe with pandas?;"<p>I have a fairly large dataset in the form of a dataframe and I was wondering how I would be able to split the dataframe into two random samples (80% and 20%) for training and testing.</p>

<p>Thanks!</p>
";81694.0;[];[]
404;5.0;0;24193174;;1;26;<python><matplotlib><pandas>;Reset color cycle in Matplotlib;"<p>Say I have data about 3 trading strategies, each with and without transaction costs.  I want to plot, on the same axes, the time series of each of the 6 variants (3 strategies * 2 trading costs).  I would like the ""with transaction cost"" lines to be plotted with <code>alpha=1</code> and <code>linewidth=1</code> while I want the ""no transaction costs"" to be plotted with <code>alpha=0.25</code> and <code>linewidth=5</code>.  But I would like the color to be the same for both versions of each strategy. </p>

<p>I would like something along the lines of:</p>

<pre><code>fig, ax = plt.subplots(1, 1, figsize=(10, 10))

for c in with_transaction_frame.columns:
    ax.plot(with_transaction_frame[c], label=c, alpha=1, linewidth=1)

****SOME MAGIC GOES HERE TO RESET THE COLOR CYCLE

for c in no_transaction_frame.columns:
    ax.plot(no_transaction_frame[c], label=c, alpha=0.25, linewidth=5)

ax.legend()
</code></pre>

<p>What is the appropriate code to put on the indicated line to reset the color cycle so it is ""back to the start"" when the second loop is invoked?</p>
";10416.0;['fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\nfor c in with_transaction_frame.columns:\n    ax.plot(with_transaction_frame[c], label=c, alpha=1, linewidth=1)\n\n****SOME MAGIC GOES HERE TO RESET THE COLOR CYCLE\n\nfor c in no_transaction_frame.columns:\n    ax.plot(no_transaction_frame[c], label=c, alpha=0.25, linewidth=5)\n\nax.legend()\n'];['alpha=1', 'linewidth=1', 'alpha=0.25', 'linewidth=5', 'fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\nfor c in with_transaction_frame.columns:\n    ax.plot(with_transaction_frame[c], label=c, alpha=1, linewidth=1)\n\n****SOME MAGIC GOES HERE TO RESET THE COLOR CYCLE\n\nfor c in no_transaction_frame.columns:\n    ax.plot(no_transaction_frame[c], label=c, alpha=0.25, linewidth=5)\n\nax.legend()\n']
405;1.0;0;24216425;;1;28;<python><pandas>;Adding a new pandas column with mapped value from a dictionary;"<p>I'm trying do something that should be really simple in pandas, but it seems anything but. I'm trying to add a column to an existing pandas dataframe that is a mapped value based on another (existing) column. Here is a small test case:</p>

<pre><code>import pandas as pd
equiv = {7001:1, 8001:2, 9001:3}
df = pd.DataFrame( {""A"": [7001, 8001, 9001]} )
df[""B""] = equiv(df[""A""])
print(df)
</code></pre>

<p>I was hoping the following would result:</p>

<pre><code>      A   B
0  7001   1
1  8001   2
2  9001   3
</code></pre>

<p>Instead, I get an error telling me that equiv is not a callable function. Fair enough, it's a dictionary, but even if I wrap it in a function I still get frustration. So I tried to use a map function that seems to work with other operations, but it also is defeated by use of a dictionary:</p>

<pre><code>df[""B""] = df[""A""].map(lambda x:equiv[x])
</code></pre>

<p>In this case I just get KeyError: 8001. I've read through documentation and previous posts, but have yet to come across anything that suggests how to mix dictionaries with pandas dataframes. Any suggestions would be greatly appreciated.</p>
";15904.0;"['import pandas as pd\nequiv = {7001:1, 8001:2, 9001:3}\ndf = pd.DataFrame( {""A"": [7001, 8001, 9001]} )\ndf[""B""] = equiv(df[""A""])\nprint(df)\n', '      A   B\n0  7001   1\n1  8001   2\n2  9001   3\n', 'df[""B""] = df[""A""].map(lambda x:equiv[x])\n']";"['import pandas as pd\nequiv = {7001:1, 8001:2, 9001:3}\ndf = pd.DataFrame( {""A"": [7001, 8001, 9001]} )\ndf[""B""] = equiv(df[""A""])\nprint(df)\n', '      A   B\n0  7001   1\n1  8001   2\n2  9001   3\n', 'df[""B""] = df[""A""].map(lambda x:equiv[x])\n']"
406;4.0;2;24251219;;1;81;<python><parsing><numpy><pandas><dataframe>;Pandas read_csv low_memory and dtype options;"<p>When calling</p>

<pre><code>df = pd.read_csv('somefile.csv')
</code></pre>

<p>I get:</p>

<blockquote>
  <p>/Users/josh/anaconda/envs/py27/lib/python2.7/site-packages/pandas/io/parsers.py:1130:
  DtypeWarning: Columns (4,5,7,16) have mixed types.  Specify dtype
  option on import or set low_memory=False.</p>
</blockquote>

<p>Why is the <code>dtype</code> option related to <code>low_memory</code>, and why would making it <code>False</code> help with this problem?</p>
";53257.0;"[""df = pd.read_csv('somefile.csv')\n""]";"[""df = pd.read_csv('somefile.csv')\n"", 'dtype', 'low_memory', 'False']"
407;3.0;2;24284342;;1;27;<python><pandas>;Insert a row to pandas dataframe;"<p>I have a dataframe..</p>

<pre><code>s1 = pd.Series([5, 6, 7])
s2 = pd.Series([7, 8, 9])

df = pd.DataFrame([list(s1), list(s2)],  columns =  [""A"", ""B"", ""C""])

   A  B  C
0  5  6  7
1  7  8  9

[2 rows x 3 columns]
</code></pre>

<p>and I need to add a first row [2, 3, 4] to get..</p>

<pre><code>   A  B  C
0  2  3  4
1  5  6  7
2  7  8  9
</code></pre>

<p>I've tried append() and concat() functions but somehow I can't find the right way how to do that.</p>

<p>Any ideas?
Is there any direct way how to add/insert series to dataframe?</p>
";87343.0;"['s1 = pd.Series([5, 6, 7])\ns2 = pd.Series([7, 8, 9])\n\ndf = pd.DataFrame([list(s1), list(s2)],  columns =  [""A"", ""B"", ""C""])\n\n   A  B  C\n0  5  6  7\n1  7  8  9\n\n[2 rows x 3 columns]\n', '   A  B  C\n0  2  3  4\n1  5  6  7\n2  7  8  9\n']";"['s1 = pd.Series([5, 6, 7])\ns2 = pd.Series([7, 8, 9])\n\ndf = pd.DataFrame([list(s1), list(s2)],  columns =  [""A"", ""B"", ""C""])\n\n   A  B  C\n0  5  6  7\n1  7  8  9\n\n[2 rows x 3 columns]\n', '   A  B  C\n0  2  3  4\n1  5  6  7\n2  7  8  9\n']"
408;9.0;2;24458645;;1;67;<python><pandas><scikit-learn>;Label encoding across multiple columns in scikit-learn;"<p>I'm trying to use scikit-learn's <code>LabelEncoder</code> to encode a pandas <code>DataFrame</code> of string labels. As the dataframe has many (50+) columns, I want to avoid creating a <code>LabelEncoder</code> object for each column; I'd rather just have one big <code>LabelEncoder</code> objects that works across <em>all</em> my columns of data.  </p>

<p>Throwing the entire <code>DataFrame</code> into <code>LabelEncoder</code> creates the below error.  Please bear in mind that I'm using dummy data here; in actuality I'm dealing with about 50 columns of string labeled data, so need a solution that doesn't reference any columns by name. </p>

<pre><code>import pandas
from sklearn import preprocessing 

df = pandas.DataFrame({'pets':['cat', 'dog', 'cat', 'monkey', 'dog', 'dog'], 'owner':['Champ', 'Ron', 'Brick', 'Champ', 'Veronica', 'Ron'], 'location':['San_Diego', 'New_York', 'New_York', 'San_Diego', 'San_Diego', 'New_York']})
le = preprocessing.LabelEncoder()

le.fit(df)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/label.py"", line 103, in fit
    y = column_or_1d(y, warn=True)
  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py"", line 306, in column_or_1d
    raise ValueError(""bad input shape {0}"".format(shape))
ValueError: bad input shape (6, 3)
</code></pre>

<p>Any thoughts on how to get around this problem? </p>
";27993.0;"['import pandas\nfrom sklearn import preprocessing \n\ndf = pandas.DataFrame({\'pets\':[\'cat\', \'dog\', \'cat\', \'monkey\', \'dog\', \'dog\'], \'owner\':[\'Champ\', \'Ron\', \'Brick\', \'Champ\', \'Veronica\', \'Ron\'], \'location\':[\'San_Diego\', \'New_York\', \'New_York\', \'San_Diego\', \'San_Diego\', \'New_York\']})\nle = preprocessing.LabelEncoder()\n\nle.fit(df)\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/label.py"", line 103, in fit\n    y = column_or_1d(y, warn=True)\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py"", line 306, in column_or_1d\n    raise ValueError(""bad input shape {0}"".format(shape))\nValueError: bad input shape (6, 3)\n']";"['LabelEncoder', 'DataFrame', 'LabelEncoder', 'LabelEncoder', 'DataFrame', 'LabelEncoder', 'import pandas\nfrom sklearn import preprocessing \n\ndf = pandas.DataFrame({\'pets\':[\'cat\', \'dog\', \'cat\', \'monkey\', \'dog\', \'dog\'], \'owner\':[\'Champ\', \'Ron\', \'Brick\', \'Champ\', \'Veronica\', \'Ron\'], \'location\':[\'San_Diego\', \'New_York\', \'New_York\', \'San_Diego\', \'San_Diego\', \'New_York\']})\nle = preprocessing.LabelEncoder()\n\nle.fit(df)\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/label.py"", line 103, in fit\n    y = column_or_1d(y, warn=True)\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py"", line 306, in column_or_1d\n    raise ValueError(""bad input shape {0}"".format(shape))\nValueError: bad input shape (6, 3)\n']"
409;3.0;1;24495695;;1;30;<python><pandas>;Pandas: Get unique MultiIndex level values by label;"<p>Say you have this MultiIndex-ed DataFrame:</p>

<pre><code>df = pd.DataFrame({'co':['DE','DE','FR','FR'],
                   'tp':['Lake','Forest','Lake','Forest'],
                   'area':[10,20,30,40],
                   'count':[7,5,2,3]})
df = df.set_index(['co','tp'])
</code></pre>

<p>Which looks like this:</p>

<pre><code>           area  count
co tp
DE Lake      10      7
   Forest    20      5
FR Lake      30      2
   Forest    40      3
</code></pre>

<p>I would like to <strong>retrieve the unique values per index level</strong>. This can be accomplished using</p>

<pre><code>df.index.levels[0]  # returns ['DE', 'FR]
df.index.levels[1]  # returns ['Lake', 'Forest']
</code></pre>

<p>What I would <em>really</em> like to do, is to retrieve these lists by <strong>addressing the levels by their name</strong>, i.e. <code>'co'</code> and <code>'tp'</code>. The shortest two ways I could find looks like this:</p>

<pre><code>list(set(df.index.get_level_values('co')))  # returns ['DE', 'FR']
df.index.levels[df.index.names.index('co')]  # returns ['DE', 'FR']
</code></pre>

<p>But non of them are very elegant. Is there a shorter way?</p>
";18489.0;"[""df = pd.DataFrame({'co':['DE','DE','FR','FR'],\n                   'tp':['Lake','Forest','Lake','Forest'],\n                   'area':[10,20,30,40],\n                   'count':[7,5,2,3]})\ndf = df.set_index(['co','tp'])\n"", '           area  count\nco tp\nDE Lake      10      7\n   Forest    20      5\nFR Lake      30      2\n   Forest    40      3\n', ""df.index.levels[0]  # returns ['DE', 'FR]\ndf.index.levels[1]  # returns ['Lake', 'Forest']\n"", ""list(set(df.index.get_level_values('co')))  # returns ['DE', 'FR']\ndf.index.levels[df.index.names.index('co')]  # returns ['DE', 'FR']\n""]";"[""df = pd.DataFrame({'co':['DE','DE','FR','FR'],\n                   'tp':['Lake','Forest','Lake','Forest'],\n                   'area':[10,20,30,40],\n                   'count':[7,5,2,3]})\ndf = df.set_index(['co','tp'])\n"", '           area  count\nco tp\nDE Lake      10      7\n   Forest    20      5\nFR Lake      30      2\n   Forest    40      3\n', ""df.index.levels[0]  # returns ['DE', 'FR]\ndf.index.levels[1]  # returns ['Lake', 'Forest']\n"", ""'co'"", ""'tp'"", ""list(set(df.index.get_level_values('co')))  # returns ['DE', 'FR']\ndf.index.levels[df.index.names.index('co')]  # returns ['DE', 'FR']\n""]"
410;3.0;4;24644656;;1;32;<python><datetime><pandas><dataframe>;How to print dataframe without index;"<p>I want to print the whole dataframe, but I don't want to print the index</p>

<p>Besides, one column is datetime type, I just want to print time, not date.</p>

<p>The dataframe looks like:</p>

<pre><code>   User ID           Enter Time   Activity Number
0      123  2014-07-08 00:09:00              1411
1      123  2014-07-08 00:18:00               893
2      123  2014-07-08 00:49:00              1041
</code></pre>

<p>I want it print as</p>

<pre><code>User ID   Enter Time   Activity Number
123         00:09:00              1411
123         00:18:00               893
123         00:49:00              1041
</code></pre>
";21964.0;['   User ID           Enter Time   Activity Number\n0      123  2014-07-08 00:09:00              1411\n1      123  2014-07-08 00:18:00               893\n2      123  2014-07-08 00:49:00              1041\n', 'User ID   Enter Time   Activity Number\n123         00:09:00              1411\n123         00:18:00               893\n123         00:49:00              1041\n'];['   User ID           Enter Time   Activity Number\n0      123  2014-07-08 00:09:00              1411\n1      123  2014-07-08 00:18:00               893\n2      123  2014-07-08 00:49:00              1041\n', 'User ID   Enter Time   Activity Number\n123         00:09:00              1411\n123         00:18:00               893\n123         00:49:00              1041\n']
411;4.0;3;24645153;;1;25;<python><pandas><scikit-learn><dataframe>;pandas dataframe columns scaling with sklearn;"<p>I have a pandas dataframe with mixed type columns, and I'd like to apply sklearn's min_max_scaler to some of the columns.  Ideally, I'd like to do these transformations in place, but haven't figured out a way to do that yet.  I've written the following code that works:</p>

<pre><code>import pandas as pd
import numpy as np
from sklearn import preprocessing

scaler = preprocessing.MinMaxScaler()

dfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],'B':[103.02,107.26,110.35,114.23,114.68], 'C':['big','small','big','small','small']})
min_max_scaler = preprocessing.MinMaxScaler()

def scaleColumns(df, cols_to_scale):
    for col in cols_to_scale:
        df[col] = pd.DataFrame(min_max_scaler.fit_transform(pd.DataFrame(dfTest[col])),columns=[col])
    return df

dfTest

    A   B   C
0    14.00   103.02  big
1    90.20   107.26  small
2    90.95   110.35  big
3    96.27   114.23  small
4    91.21   114.68  small

scaled_df = scaleColumns(dfTest,['A','B'])
scaled_df

A   B   C
0    0.000000    0.000000    big
1    0.926219    0.363636    small
2    0.935335    0.628645    big
3    1.000000    0.961407    small
4    0.938495    1.000000    small
</code></pre>

<p>I'm curious if this is the preferred/most efficient way to do this transformation.  Is there a way I could use df.apply that would be better?  </p>

<p>I'm also surprised I can't get the following code to work:</p>

<p><code>bad_output = min_max_scaler.fit_transform(dfTest['A'])</code></p>

<p>If I pass an entire dataframe to the scaler it works:</p>

<p><code>dfTest2 = dfTest.drop('C', axis = 1)
good_output = min_max_scaler.fit_transform(dfTest2)
good_output</code></p>

<p>I'm confused why passing a series to the scaler fails.  In my full working code above I had hoped to just pass a series to the scaler then set the dataframe column = to the scaled series.  I've seen this question asked a few other places, but haven't found a good answer.  Any help understanding what's going on here would be greatly appreciated!</p>
";19762.0;"[""import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\n\ndfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],'B':[103.02,107.26,110.35,114.23,114.68], 'C':['big','small','big','small','small']})\nmin_max_scaler = preprocessing.MinMaxScaler()\n\ndef scaleColumns(df, cols_to_scale):\n    for col in cols_to_scale:\n        df[col] = pd.DataFrame(min_max_scaler.fit_transform(pd.DataFrame(dfTest[col])),columns=[col])\n    return df\n\ndfTest\n\n    A   B   C\n0    14.00   103.02  big\n1    90.20   107.26  small\n2    90.95   110.35  big\n3    96.27   114.23  small\n4    91.21   114.68  small\n\nscaled_df = scaleColumns(dfTest,['A','B'])\nscaled_df\n\nA   B   C\n0    0.000000    0.000000    big\n1    0.926219    0.363636    small\n2    0.935335    0.628645    big\n3    1.000000    0.961407    small\n4    0.938495    1.000000    small\n""]";"[""import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\n\ndfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],'B':[103.02,107.26,110.35,114.23,114.68], 'C':['big','small','big','small','small']})\nmin_max_scaler = preprocessing.MinMaxScaler()\n\ndef scaleColumns(df, cols_to_scale):\n    for col in cols_to_scale:\n        df[col] = pd.DataFrame(min_max_scaler.fit_transform(pd.DataFrame(dfTest[col])),columns=[col])\n    return df\n\ndfTest\n\n    A   B   C\n0    14.00   103.02  big\n1    90.20   107.26  small\n2    90.95   110.35  big\n3    96.27   114.23  small\n4    91.21   114.68  small\n\nscaled_df = scaleColumns(dfTest,['A','B'])\nscaled_df\n\nA   B   C\n0    0.000000    0.000000    big\n1    0.926219    0.363636    small\n2    0.935335    0.628645    big\n3    1.000000    0.961407    small\n4    0.938495    1.000000    small\n"", ""bad_output = min_max_scaler.fit_transform(dfTest['A'])"", ""dfTest2 = dfTest.drop('C', axis = 1)\ngood_output = min_max_scaler.fit_transform(dfTest2)\ngood_output""]"
412;2.0;0;24775648;;1;32;<python><pandas><boolean-logic><logical-operators><boolean-operations>;Element-wise logical OR in Pandas;"<p>I would like the element-wise logical OR operator. I know ""or"" itself is not what I am looking for.</p>

<p>For AND I want to use &amp; as explained <a href=""https://stackoverflow.com/questions/21415661/logic-operator-for-boolean-indexing-in-pandas"" title=""here"">here</a>. For NOT I want to use np.invert() as explained <a href=""https://stackoverflow.com/questions/15998188/how-can-i-obtain-the-element-wise-logical-not-of-a-pandas-series"">here</a>. So what is the equivalent for OR?</p>
";17268.0;[];[]
413;3.0;0;24870306;;1;71;<python><pandas><dataframe>;How to check if a column exists in Pandas;"<p>Is there a way to check if a column exists in a Pandas DataFrame?</p>

<p>Suppose that I have the following DataFrame:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; from random import randint
&gt;&gt;&gt; df = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],
                       'B': [randint(1, 9)*10 for x in xrange(10)],
                       'C': [randint(1, 9)*100 for x in xrange(10)]})
&gt;&gt;&gt; df
   A   B    C
0  3  40  100
1  6  30  200
2  7  70  800
3  3  50  200
4  7  50  400
5  4  10  400
6  3  70  500
7  8  30  200
8  3  40  800
9  6  60  200
</code></pre>

<p>and I want to calculate <code>df['sum'] = df['A'] + df['C']</code></p>

<p>but first I want to check if <code>df['A']</code> exists, and if not, I want to calculate <code>df['sum'] = df['B'] + df['C']</code> instead.</p>

<p>Thanks for the help. </p>
";39441.0;"["">>> import pandas as pd\n>>> from random import randint\n>>> df = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],\n                       'B': [randint(1, 9)*10 for x in xrange(10)],\n                       'C': [randint(1, 9)*100 for x in xrange(10)]})\n>>> df\n   A   B    C\n0  3  40  100\n1  6  30  200\n2  7  70  800\n3  3  50  200\n4  7  50  400\n5  4  10  400\n6  3  70  500\n7  8  30  200\n8  3  40  800\n9  6  60  200\n""]";"["">>> import pandas as pd\n>>> from random import randint\n>>> df = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],\n                       'B': [randint(1, 9)*10 for x in xrange(10)],\n                       'C': [randint(1, 9)*100 for x in xrange(10)]})\n>>> df\n   A   B    C\n0  3  40  100\n1  6  30  200\n2  7  70  800\n3  3  50  200\n4  7  50  400\n5  4  10  400\n6  3  70  500\n7  8  30  200\n8  3  40  800\n9  6  60  200\n"", ""df['sum'] = df['A'] + df['C']"", ""df['A']"", ""df['sum'] = df['B'] + df['C']""]"
414;4.0;6;24870953;;1;22;<python><performance><pandas><iteration>;Does iterrows have performance issues?;"<p>I have noticed very poor performance when using iterrows from pandas.</p>

<p>Is this something that is experienced by others? Is it specific to iterrows and should this function be avoided for data of a certain size (I'm working with 2-3 million rows)?</p>

<p><a href=""https://github.com/pydata/pandas/issues/7683"">This discussion</a> on GitHub led me to believe it is caused when mixing dtypes in the dataframe, however the simple example below shows it is there even when using one dtype (float64). This takes 36 seconds on my machine:</p>

<pre><code>import pandas as pd
import numpy as np
import time

s1 = np.random.randn(2000000)
s2 = np.random.randn(2000000)
dfa = pd.DataFrame({'s1': s1, 's2': s2})

start = time.time()
i=0
for rowindex, row in dfa.iterrows():
    i+=1
end = time.time()
print end - start
</code></pre>

<p>Why are vectorized operations like apply so much quicker? I imagine there must be some row by row iteration going on there too. </p>

<p>I cannot figure out how to not use iterrows in my case (this I'll save for a future question). Therefore I would appreciate hearing if you have consistently been able to avoid this iteration. I'm making calculations based on data in separate dataframes. Thank you!</p>

<p>---Edit: simplified version of what I want to run has been added below---</p>

<pre><code>import pandas as pd
import numpy as np

#%% Create the original tables
t1 = {'letter':['a','b'],
      'number1':[50,-10]}

t2 = {'letter':['a','a','b','b'],
      'number2':[0.2,0.5,0.1,0.4]}

table1 = pd.DataFrame(t1)
table2 = pd.DataFrame(t2)

#%% Create the body of the new table
table3 = pd.DataFrame(np.nan, columns=['letter','number2'], index=[0])

#%% Iterate through filtering relevant data, optimizing, returning info
for row_index, row in table1.iterrows():   
    t2info = table2[table2.letter == row['letter']].reset_index()
    table3.ix[row_index,] = optimize(t2info,row['number1'])

#%% Define optimization
def optimize(t2info, t1info):
    calculation = []
    for index, r in t2info.iterrows():
        calculation.append(r['number2']*t1info)
    maxrow = calculation.index(max(calculation))
    return t2info.ix[maxrow]
</code></pre>
";7339.0;"[""import pandas as pd\nimport numpy as np\nimport time\n\ns1 = np.random.randn(2000000)\ns2 = np.random.randn(2000000)\ndfa = pd.DataFrame({'s1': s1, 's2': s2})\n\nstart = time.time()\ni=0\nfor rowindex, row in dfa.iterrows():\n    i+=1\nend = time.time()\nprint end - start\n"", ""import pandas as pd\nimport numpy as np\n\n#%% Create the original tables\nt1 = {'letter':['a','b'],\n      'number1':[50,-10]}\n\nt2 = {'letter':['a','a','b','b'],\n      'number2':[0.2,0.5,0.1,0.4]}\n\ntable1 = pd.DataFrame(t1)\ntable2 = pd.DataFrame(t2)\n\n#%% Create the body of the new table\ntable3 = pd.DataFrame(np.nan, columns=['letter','number2'], index=[0])\n\n#%% Iterate through filtering relevant data, optimizing, returning info\nfor row_index, row in table1.iterrows():   \n    t2info = table2[table2.letter == row['letter']].reset_index()\n    table3.ix[row_index,] = optimize(t2info,row['number1'])\n\n#%% Define optimization\ndef optimize(t2info, t1info):\n    calculation = []\n    for index, r in t2info.iterrows():\n        calculation.append(r['number2']*t1info)\n    maxrow = calculation.index(max(calculation))\n    return t2info.ix[maxrow]\n""]";"[""import pandas as pd\nimport numpy as np\nimport time\n\ns1 = np.random.randn(2000000)\ns2 = np.random.randn(2000000)\ndfa = pd.DataFrame({'s1': s1, 's2': s2})\n\nstart = time.time()\ni=0\nfor rowindex, row in dfa.iterrows():\n    i+=1\nend = time.time()\nprint end - start\n"", ""import pandas as pd\nimport numpy as np\n\n#%% Create the original tables\nt1 = {'letter':['a','b'],\n      'number1':[50,-10]}\n\nt2 = {'letter':['a','a','b','b'],\n      'number2':[0.2,0.5,0.1,0.4]}\n\ntable1 = pd.DataFrame(t1)\ntable2 = pd.DataFrame(t2)\n\n#%% Create the body of the new table\ntable3 = pd.DataFrame(np.nan, columns=['letter','number2'], index=[0])\n\n#%% Iterate through filtering relevant data, optimizing, returning info\nfor row_index, row in table1.iterrows():   \n    t2info = table2[table2.letter == row['letter']].reset_index()\n    table3.ix[row_index,] = optimize(t2info,row['number1'])\n\n#%% Define optimization\ndef optimize(t2info, t1info):\n    calculation = []\n    for index, r in t2info.iterrows():\n        calculation.append(r['number2']*t1info)\n    maxrow = calculation.index(max(calculation))\n    return t2info.ix[maxrow]\n""]"
415;5.0;1;25039626;;1;37;<python><types><pandas>;find numeric columns in pandas (python);"<p>say df is a pandas DataFrame.
I would like to find all columns of numeric type.
something like:</p>

<pre><code>isNumeric = is_numeric(df)
</code></pre>
";15852.0;['isNumeric = is_numeric(df)\n'];['isNumeric = is_numeric(df)\n']
416;1.0;0;25055712;;1;23;<pandas><resampling>;Pandas every nth row;"<p>Dataframe.resample() works only with timeseries data. I cannot find a way of getting every nth row from non-timeseries data. What is the best method?</p>
";10589.0;[];[]
417;7.0;1;25146121;;1;53;<python><pandas>;Extracting just Month and Year from Pandas Datetime column (Python);"<p>I have a Dataframe, df, with the following column:</p>

<pre><code>df['ArrivalDate'] =
...
936   2012-12-31
938   2012-12-29
965   2012-12-31
966   2012-12-31
967   2012-12-31
968   2012-12-31
969   2012-12-31
970   2012-12-29
971   2012-12-31
972   2012-12-29
973   2012-12-29
...
</code></pre>

<p>The elements of the column are pandas.tslib.Timestamp.</p>

<p>I want to just include the year and month.  I thought there would be simple way to do it, but I can't figure it out.</p>

<p>Here's what I've tried:</p>

<pre><code>df['ArrivalDate'].resample('M', how = 'mean')
</code></pre>

<p>I got the following error:</p>

<pre><code>Only valid with DatetimeIndex or PeriodIndex 
</code></pre>

<p>Then I tried:</p>

<pre><code>df['ArrivalDate'].apply(lambda(x):x[:-2])
</code></pre>

<p>I got the following error:</p>

<pre><code>'Timestamp' object has no attribute '__getitem__' 
</code></pre>

<p>Any suggestions?</p>

<p>Edit: I sort of figured it out.  </p>

<pre><code>df.index = df['ArrivalDate']
</code></pre>

<p>Then, I can resample another column using the index.</p>

<p>But I'd still like a method for reconfiguring the entire column.  Any ideas?</p>
";78275.0;"[""df['ArrivalDate'] =\n...\n936   2012-12-31\n938   2012-12-29\n965   2012-12-31\n966   2012-12-31\n967   2012-12-31\n968   2012-12-31\n969   2012-12-31\n970   2012-12-29\n971   2012-12-31\n972   2012-12-29\n973   2012-12-29\n...\n"", ""df['ArrivalDate'].resample('M', how = 'mean')\n"", 'Only valid with DatetimeIndex or PeriodIndex \n', ""df['ArrivalDate'].apply(lambda(x):x[:-2])\n"", ""'Timestamp' object has no attribute '__getitem__' \n"", ""df.index = df['ArrivalDate']\n""]";"[""df['ArrivalDate'] =\n...\n936   2012-12-31\n938   2012-12-29\n965   2012-12-31\n966   2012-12-31\n967   2012-12-31\n968   2012-12-31\n969   2012-12-31\n970   2012-12-29\n971   2012-12-31\n972   2012-12-29\n973   2012-12-29\n...\n"", ""df['ArrivalDate'].resample('M', how = 'mean')\n"", 'Only valid with DatetimeIndex or PeriodIndex \n', ""df['ArrivalDate'].apply(lambda(x):x[:-2])\n"", ""'Timestamp' object has no attribute '__getitem__' \n"", ""df.index = df['ArrivalDate']\n""]"
418;2.0;2;25212986;;1;30;<python><pandas><seaborn>;How to set some xlim and ylim in Seaborn lmplot facetgrid;"<p>I'm using Seaborn's lmplot to plot a linear regression, dividing my dataset into two groups with a categorical variable.</p>

<p>For both x and y, I'd like to manually set the <em>lower bound</em> on both plots, but leave the <em>upper bound</em> at the Seaborn default.
Here's a simple example:</p>

<pre><code>import pandas as pd
import seaborn as sns
import random

n = 200
random.seed(2014)
base_x = [random.random() for i in range(n)]
base_y = [2*i for i in base_x]
errors = [random.uniform(0,1) for i in range(n)]
y = [i+j for i,j in zip(base_y,errors)]

df = pd.DataFrame({'X': base_x,
                   'Y': y,
                   'Z': ['A','B']*(n/2)})

mask_for_b = df.Z == 'B'
df.loc[mask_for_b,['X','Y']] = df.loc[mask_for_b,] *2

sns.lmplot('X','Y',df,col='Z',sharex=False,sharey=False)
</code></pre>

<p>This outputs the following:
<img src=""https://i.stack.imgur.com/Pqw2j.png"" alt=""enter image description here""></p>

<p>But in this example, I'd like the xlim and the ylim to be (0,*) . I tried using sns.plt.ylim and sns.plt.xlim but those only affect the right-hand plot. 
Example:</p>

<pre><code>sns.plt.ylim(0,)
sns.plt.xlim(0,)
</code></pre>

<p><img src=""https://i.stack.imgur.com/s9jpF.png"" alt=""enter image description here""></p>

<p>How can I access the xlim and ylim for each plot in the FacetGrid?</p>
";25892.0;"[""import pandas as pd\nimport seaborn as sns\nimport random\n\nn = 200\nrandom.seed(2014)\nbase_x = [random.random() for i in range(n)]\nbase_y = [2*i for i in base_x]\nerrors = [random.uniform(0,1) for i in range(n)]\ny = [i+j for i,j in zip(base_y,errors)]\n\ndf = pd.DataFrame({'X': base_x,\n                   'Y': y,\n                   'Z': ['A','B']*(n/2)})\n\nmask_for_b = df.Z == 'B'\ndf.loc[mask_for_b,['X','Y']] = df.loc[mask_for_b,] *2\n\nsns.lmplot('X','Y',df,col='Z',sharex=False,sharey=False)\n"", 'sns.plt.ylim(0,)\nsns.plt.xlim(0,)\n']";"[""import pandas as pd\nimport seaborn as sns\nimport random\n\nn = 200\nrandom.seed(2014)\nbase_x = [random.random() for i in range(n)]\nbase_y = [2*i for i in base_x]\nerrors = [random.uniform(0,1) for i in range(n)]\ny = [i+j for i,j in zip(base_y,errors)]\n\ndf = pd.DataFrame({'X': base_x,\n                   'Y': y,\n                   'Z': ['A','B']*(n/2)})\n\nmask_for_b = df.Z == 'B'\ndf.loc[mask_for_b,['X','Y']] = df.loc[mask_for_b,] *2\n\nsns.lmplot('X','Y',df,col='Z',sharex=False,sharey=False)\n"", 'sns.plt.ylim(0,)\nsns.plt.xlim(0,)\n']"
419;4.0;3;25239958;;1;23;<python><pandas><scikit-learn>;Impute categorical missing values in scikit-learn;"<p>I've got pandas data with some columns of text type. There are some NaN values along with these text columns. What I'm trying to do is to impute those NaN's by sklearn.preprocessing. Imputer (replacing NaN by the most frequent value). The problem is in implementation.
Suppose there is a Pandas dataframe df with 30 columns, 10 of which are of categorical nature.
Once I run</p>

<pre><code>from sklearn.preprocessing import Imputer
imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)
imp.fit(df) 
</code></pre>

<p>Python generates an error: 'could not convert string to float: 'run1'', where 'run1' is an ordinary (non-missing) value from the first column with categorical data.</p>

<p>Any help would be very welcome</p>
";10847.0;"[""from sklearn.preprocessing import Imputer\nimp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\nimp.fit(df) \n""]";"[""from sklearn.preprocessing import Imputer\nimp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\nimp.fit(df) \n""]"
420;3.0;1;25254016;;1;62;<python><pandas>;Pandas - Get first row value of a given column;"<p>This seems like a ridiculously easy question... but I'm not seeing the easy answer I was expecting.</p>

<p>So, how do I get the value at an nth row of a given column in Pandas? (I am particularly interested in the first row, but would be interested in a more general practice as well).</p>

<p>For example, let's say I want to pull the 1.2 value in Btime as a variable. </p>

<p>Whats the right way to do this?</p>

<p>df_test = </p>

<pre><code>  ATime   X   Y   Z   Btime  C   D   E
0    1.2  2  15   2    1.2  12  25  12
1    1.4  3  12   1    1.3  13  22  11
2    1.5  1  10   6    1.4  11  20  16
3    1.6  2   9  10    1.7  12  29  12
4    1.9  1   1   9    1.9  11  21  19
5    2.0  0   0   0    2.0   8  10  11
6    2.4  0   0   0    2.4  10  12  15
</code></pre>
";90823.0;['  ATime   X   Y   Z   Btime  C   D   E\n0    1.2  2  15   2    1.2  12  25  12\n1    1.4  3  12   1    1.3  13  22  11\n2    1.5  1  10   6    1.4  11  20  16\n3    1.6  2   9  10    1.7  12  29  12\n4    1.9  1   1   9    1.9  11  21  19\n5    2.0  0   0   0    2.0   8  10  11\n6    2.4  0   0   0    2.4  10  12  15\n'];['  ATime   X   Y   Z   Btime  C   D   E\n0    1.2  2  15   2    1.2  12  25  12\n1    1.4  3  12   1    1.3  13  22  11\n2    1.5  1  10   6    1.4  11  20  16\n3    1.6  2   9  10    1.7  12  29  12\n4    1.9  1   1   9    1.9  11  21  19\n5    2.0  0   0   0    2.0   8  10  11\n6    2.4  0   0   0    2.4  10  12  15\n']
421;2.0;0;25351968;;1;21;<python><html><pandas>;How to display full (non-truncated) dataframe information in html when converting from pandas dataframe to html?;"<p>I converted a pandas dataframe to an html output using the <code>DataFrame.to_html</code> function. When I save this to a separate html file, the file shows truncated output.</p>

<p>For example, in my TEXT column, </p>

<p><code>df.head(1)</code> will show </p>

<p><i>The film was an excellent effort...</i></p>

<p>instead of </p>

<p><i>The film was an excellent effort in deconstructing the complex social sentiments that prevailed during this period.</i></p>

<p>This rendition is fine in the case of a screen-friendly format of a massive pandas dataframe, but I need an html file that will show complete tabular data contained in the dataframe, that is, something that will show the latter text element rather than the former text snippet. </p>

<p>How would I be able to show the complete, non-truncated text data for each element in my TEXT column in the html version of the information? I would imagine that the html table would have to display long cells to show the complete data, but as far as I understand, only column-width parameters can be passed into the <code>DataFrame.to_html</code> function.</p>
";13484.0;[];['DataFrame.to_html', 'df.head(1)', 'DataFrame.to_html']
422;2.0;0;25386870;;1;25;<python><matplotlib><pandas><multi-index>;Pandas Plotting with Multi-Index;"<p>After performing a groupby.sum() on a dataframe I'm having some trouble trying to create my intended plot.</p>

<p><img src=""https://i.stack.imgur.com/y9L5u.png"" alt=""grouped dataframe with multi-index""></p>

<p>How can I create a subplot (kind='bar') for each 'Code', where the x-axis is the 'Month' and the bars are ColA and ColB?</p>
";12929.0;[];[]
423;2.0;0;25493625;;1;22;<python><join><pandas><vlookup>;vlookup in Pandas using join;"<p>I have the following 2 dataframes </p>

<pre><code>Example1
sku loc flag  
122  61 True 
123  61 True
113  62 True 
122  62 True 
123  62 False
122  63 False
301  63 True 

Example2 
sku dept 
113 a
122 b
123 b
301 c 
</code></pre>

<p>I want to perform a merge, or join opertation using Pandas (or whichever Python operator is best) to produce the below data frame. </p>

<pre><code>Example3
sku loc flag   dept  
122  61 True   b
123  61 True   b
113  62 True   a
122  62 True   b
123  62 False  b
122  63 False  b
301  63 True   c

Both 
df_Example1.join(df_Example2,lsuffix='_ProdHier')
df_Example1.join(df_Example2,how='outer',lsuffix='_ProdHier')
</code></pre>

<p>Aren't working.
What am I doing wrong? </p>
";21751.0;"['Example1\nsku loc flag  \n122  61 True \n123  61 True\n113  62 True \n122  62 True \n123  62 False\n122  63 False\n301  63 True \n\nExample2 \nsku dept \n113 a\n122 b\n123 b\n301 c \n', ""Example3\nsku loc flag   dept  \n122  61 True   b\n123  61 True   b\n113  62 True   a\n122  62 True   b\n123  62 False  b\n122  63 False  b\n301  63 True   c\n\nBoth \ndf_Example1.join(df_Example2,lsuffix='_ProdHier')\ndf_Example1.join(df_Example2,how='outer',lsuffix='_ProdHier')\n""]";"['Example1\nsku loc flag  \n122  61 True \n123  61 True\n113  62 True \n122  62 True \n123  62 False\n122  63 False\n301  63 True \n\nExample2 \nsku dept \n113 a\n122 b\n123 b\n301 c \n', ""Example3\nsku loc flag   dept  \n122  61 True   b\n123  61 True   b\n113  62 True   a\n122  62 True   b\n123  62 False  b\n122  63 False  b\n301  63 True   c\n\nBoth \ndf_Example1.join(df_Example2,lsuffix='_ProdHier')\ndf_Example1.join(df_Example2,how='outer',lsuffix='_ProdHier')\n""]"
424;1.0;6;25537399;;1;21;<python><pandas><scipy><statsmodels><anova>;ANOVA in python using pandas dataframe with statsmodels or scipy?;"<p>I want to use the Pandas dataframe to breakdown the variance in one variable.</p>

<p>For example, if I have a column called 'Degrees', and I have this indexed for various dates, cities, and night vs. day, I want to find out what fraction of the variation in this series is coming from cross-sectional city variation, how much is coming from time series variation, and how much is coming from night vs. day. </p>

<p>In Stata I would use Fixed effects and look at the R^2.   Hopefully my question makes sense.</p>

<p>Basically, what I want to do, is find the ANOVA breakdown of ""Degrees"" by three other columns.  </p>
";10835.0;[];[]
425;1.0;16;25631076;;1;46;<python><performance><numpy><pandas>;Is this the fastest way to group in Pandas?;"<p>The following code works well. Just checking: am I using and timing Pandas correctly and is there any faster way? Thanks.</p>

<pre><code>$ python3
Python 3.4.0 (default, Apr 11 2014, 13:05:11) 
[GCC 4.8.2] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import timeit
&gt;&gt;&gt; pd.__version__
'0.14.1'

def randChar(f, numGrp, N) :
   things = [f%x for x in range(numGrp)]
   return [things[x] for x in np.random.choice(numGrp, N)]

def randFloat(numGrp, N) :
   things = [round(100*np.random.random(),4) for x in range(numGrp)]
   return [things[x] for x in np.random.choice(numGrp, N)]

N=int(1e8)
K=100
DF = pd.DataFrame({
  'id1' : randChar(""id%03d"", K, N),       # large groups (char)
  'id2' : randChar(""id%03d"", K, N),       # large groups (char)
  'id3' : randChar(""id%010d"", N//K, N),   # small groups (char)
  'id4' : np.random.choice(K, N),         # large groups (int)
  'id5' : np.random.choice(K, N),         # large groups (int)
  'id6' : np.random.choice(N//K, N),      # small groups (int)            
  'v1' :  np.random.choice(5, N),         # int in range [1,5]
  'v2' :  np.random.choice(5, N),         # int in range [1,5]
  'v3' :  randFloat(100,N)                # numeric e.g. 23.5749
})
</code></pre>

<p>Now time 5 different groupings, repeating each one twice to confirm the timing. [I realise <code>timeit(2)</code> runs it twice, but then it reports the total. I'm interested in the time of the first and second run separately.] Python uses about 10G of RAM according to <code>htop</code> during these tests.</p>

<pre><code>&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id1']).agg({'v1':'sum'})""                            ,""from __main__ import DF"").timeit(1)
5.604133386000285
&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id1']).agg({'v1':'sum'})""                            ,""from __main__ import DF"").timeit(1)
5.505057081000359

&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id1','id2']).agg({'v1':'sum'})""                      ,""from __main__ import DF"").timeit(1)
14.232032927000091
&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id1','id2']).agg({'v1':'sum'})""                      ,""from __main__ import DF"").timeit(1)
14.242601240999647

&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id3']).agg({'v1':'sum', 'v3':'mean'})""               ,""from __main__ import DF"").timeit(1)
22.87025260900009
&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id3']).agg({'v1':'sum', 'v3':'mean'})""               ,""from __main__ import DF"").timeit(1)
22.393589012999655

&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id4']).agg({'v1':'mean', 'v2':'mean', 'v3':'mean'})"" ,""from __main__ import DF"").timeit(1)
2.9725865330001398
&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id4']).agg({'v1':'mean', 'v2':'mean', 'v3':'mean'})"" ,""from __main__ import DF"").timeit(1)
2.9683854739996605

&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id6']).agg({'v1':'sum', 'v2':'sum', 'v3':'sum'})""    ,""from __main__ import DF"").timeit(1)
12.776488024999708
&gt;&gt;&gt; timeit.Timer(""DF.groupby(['id6']).agg({'v1':'sum', 'v2':'sum', 'v3':'sum'})""    ,""from __main__ import DF"").timeit(1)
13.558292575999076
</code></pre>

<p>Here is system info :</p>

<pre><code>$ lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                32
On-line CPU(s) list:   0-31
Thread(s) per core:    2
Core(s) per socket:    8
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 62
Stepping:              4
CPU MHz:               2500.048
BogoMIPS:              5066.38
Hypervisor vendor:     Xen
Virtualization type:   full
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              25600K
NUMA node0 CPU(s):     0-7,16-23
NUMA node1 CPU(s):     8-15,24-31

$ free -h
             total       used       free     shared    buffers     cached
Mem:          240G        74G       166G       372K        33M       550M
-/+ buffers/cache:        73G       166G
Swap:           0B         0B         0B
</code></pre>

<p>I don't believe it's relevant but just in case, the <code>randChar</code> function above is a workaround for a memory error in <code>mtrand.RandomState.choice</code> :</p>

<p><a href=""https://stackoverflow.com/questions/25627161/how-to-solve-memory-error-in-mtrand-randomstate-choice"">How to solve memory error in mtrand.RandomState.choice?</a></p>
";2357.0;"['$ python3\nPython 3.4.0 (default, Apr 11 2014, 13:05:11) \n[GCC 4.8.2] on linux\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n>>> import pandas as pd\n>>> import numpy as np\n>>> import timeit\n>>> pd.__version__\n\'0.14.1\'\n\ndef randChar(f, numGrp, N) :\n   things = [f%x for x in range(numGrp)]\n   return [things[x] for x in np.random.choice(numGrp, N)]\n\ndef randFloat(numGrp, N) :\n   things = [round(100*np.random.random(),4) for x in range(numGrp)]\n   return [things[x] for x in np.random.choice(numGrp, N)]\n\nN=int(1e8)\nK=100\nDF = pd.DataFrame({\n  \'id1\' : randChar(""id%03d"", K, N),       # large groups (char)\n  \'id2\' : randChar(""id%03d"", K, N),       # large groups (char)\n  \'id3\' : randChar(""id%010d"", N//K, N),   # small groups (char)\n  \'id4\' : np.random.choice(K, N),         # large groups (int)\n  \'id5\' : np.random.choice(K, N),         # large groups (int)\n  \'id6\' : np.random.choice(N//K, N),      # small groups (int)            \n  \'v1\' :  np.random.choice(5, N),         # int in range [1,5]\n  \'v2\' :  np.random.choice(5, N),         # int in range [1,5]\n  \'v3\' :  randFloat(100,N)                # numeric e.g. 23.5749\n})\n', '>>> timeit.Timer(""DF.groupby([\'id1\']).agg({\'v1\':\'sum\'})""                            ,""from __main__ import DF"").timeit(1)\n5.604133386000285\n>>> timeit.Timer(""DF.groupby([\'id1\']).agg({\'v1\':\'sum\'})""                            ,""from __main__ import DF"").timeit(1)\n5.505057081000359\n\n>>> timeit.Timer(""DF.groupby([\'id1\',\'id2\']).agg({\'v1\':\'sum\'})""                      ,""from __main__ import DF"").timeit(1)\n14.232032927000091\n>>> timeit.Timer(""DF.groupby([\'id1\',\'id2\']).agg({\'v1\':\'sum\'})""                      ,""from __main__ import DF"").timeit(1)\n14.242601240999647\n\n>>> timeit.Timer(""DF.groupby([\'id3\']).agg({\'v1\':\'sum\', \'v3\':\'mean\'})""               ,""from __main__ import DF"").timeit(1)\n22.87025260900009\n>>> timeit.Timer(""DF.groupby([\'id3\']).agg({\'v1\':\'sum\', \'v3\':\'mean\'})""               ,""from __main__ import DF"").timeit(1)\n22.393589012999655\n\n>>> timeit.Timer(""DF.groupby([\'id4\']).agg({\'v1\':\'mean\', \'v2\':\'mean\', \'v3\':\'mean\'})"" ,""from __main__ import DF"").timeit(1)\n2.9725865330001398\n>>> timeit.Timer(""DF.groupby([\'id4\']).agg({\'v1\':\'mean\', \'v2\':\'mean\', \'v3\':\'mean\'})"" ,""from __main__ import DF"").timeit(1)\n2.9683854739996605\n\n>>> timeit.Timer(""DF.groupby([\'id6\']).agg({\'v1\':\'sum\', \'v2\':\'sum\', \'v3\':\'sum\'})""    ,""from __main__ import DF"").timeit(1)\n12.776488024999708\n>>> timeit.Timer(""DF.groupby([\'id6\']).agg({\'v1\':\'sum\', \'v2\':\'sum\', \'v3\':\'sum\'})""    ,""from __main__ import DF"").timeit(1)\n13.558292575999076\n', '$ lscpu\nArchitecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                32\nOn-line CPU(s) list:   0-31\nThread(s) per core:    2\nCore(s) per socket:    8\nSocket(s):             2\nNUMA node(s):          2\nVendor ID:             GenuineIntel\nCPU family:            6\nModel:                 62\nStepping:              4\nCPU MHz:               2500.048\nBogoMIPS:              5066.38\nHypervisor vendor:     Xen\nVirtualization type:   full\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              256K\nL3 cache:              25600K\nNUMA node0 CPU(s):     0-7,16-23\nNUMA node1 CPU(s):     8-15,24-31\n\n$ free -h\n             total       used       free     shared    buffers     cached\nMem:          240G        74G       166G       372K        33M       550M\n-/+ buffers/cache:        73G       166G\nSwap:           0B         0B         0B\n']";"['$ python3\nPython 3.4.0 (default, Apr 11 2014, 13:05:11) \n[GCC 4.8.2] on linux\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n>>> import pandas as pd\n>>> import numpy as np\n>>> import timeit\n>>> pd.__version__\n\'0.14.1\'\n\ndef randChar(f, numGrp, N) :\n   things = [f%x for x in range(numGrp)]\n   return [things[x] for x in np.random.choice(numGrp, N)]\n\ndef randFloat(numGrp, N) :\n   things = [round(100*np.random.random(),4) for x in range(numGrp)]\n   return [things[x] for x in np.random.choice(numGrp, N)]\n\nN=int(1e8)\nK=100\nDF = pd.DataFrame({\n  \'id1\' : randChar(""id%03d"", K, N),       # large groups (char)\n  \'id2\' : randChar(""id%03d"", K, N),       # large groups (char)\n  \'id3\' : randChar(""id%010d"", N//K, N),   # small groups (char)\n  \'id4\' : np.random.choice(K, N),         # large groups (int)\n  \'id5\' : np.random.choice(K, N),         # large groups (int)\n  \'id6\' : np.random.choice(N//K, N),      # small groups (int)            \n  \'v1\' :  np.random.choice(5, N),         # int in range [1,5]\n  \'v2\' :  np.random.choice(5, N),         # int in range [1,5]\n  \'v3\' :  randFloat(100,N)                # numeric e.g. 23.5749\n})\n', 'timeit(2)', 'htop', '>>> timeit.Timer(""DF.groupby([\'id1\']).agg({\'v1\':\'sum\'})""                            ,""from __main__ import DF"").timeit(1)\n5.604133386000285\n>>> timeit.Timer(""DF.groupby([\'id1\']).agg({\'v1\':\'sum\'})""                            ,""from __main__ import DF"").timeit(1)\n5.505057081000359\n\n>>> timeit.Timer(""DF.groupby([\'id1\',\'id2\']).agg({\'v1\':\'sum\'})""                      ,""from __main__ import DF"").timeit(1)\n14.232032927000091\n>>> timeit.Timer(""DF.groupby([\'id1\',\'id2\']).agg({\'v1\':\'sum\'})""                      ,""from __main__ import DF"").timeit(1)\n14.242601240999647\n\n>>> timeit.Timer(""DF.groupby([\'id3\']).agg({\'v1\':\'sum\', \'v3\':\'mean\'})""               ,""from __main__ import DF"").timeit(1)\n22.87025260900009\n>>> timeit.Timer(""DF.groupby([\'id3\']).agg({\'v1\':\'sum\', \'v3\':\'mean\'})""               ,""from __main__ import DF"").timeit(1)\n22.393589012999655\n\n>>> timeit.Timer(""DF.groupby([\'id4\']).agg({\'v1\':\'mean\', \'v2\':\'mean\', \'v3\':\'mean\'})"" ,""from __main__ import DF"").timeit(1)\n2.9725865330001398\n>>> timeit.Timer(""DF.groupby([\'id4\']).agg({\'v1\':\'mean\', \'v2\':\'mean\', \'v3\':\'mean\'})"" ,""from __main__ import DF"").timeit(1)\n2.9683854739996605\n\n>>> timeit.Timer(""DF.groupby([\'id6\']).agg({\'v1\':\'sum\', \'v2\':\'sum\', \'v3\':\'sum\'})""    ,""from __main__ import DF"").timeit(1)\n12.776488024999708\n>>> timeit.Timer(""DF.groupby([\'id6\']).agg({\'v1\':\'sum\', \'v2\':\'sum\', \'v3\':\'sum\'})""    ,""from __main__ import DF"").timeit(1)\n13.558292575999076\n', '$ lscpu\nArchitecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                32\nOn-line CPU(s) list:   0-31\nThread(s) per core:    2\nCore(s) per socket:    8\nSocket(s):             2\nNUMA node(s):          2\nVendor ID:             GenuineIntel\nCPU family:            6\nModel:                 62\nStepping:              4\nCPU MHz:               2500.048\nBogoMIPS:              5066.38\nHypervisor vendor:     Xen\nVirtualization type:   full\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              256K\nL3 cache:              25600K\nNUMA node0 CPU(s):     0-7,16-23\nNUMA node1 CPU(s):     8-15,24-31\n\n$ free -h\n             total       used       free     shared    buffers     cached\nMem:          240G        74G       166G       372K        33M       550M\n-/+ buffers/cache:        73G       166G\nSwap:           0B         0B         0B\n', 'randChar', 'mtrand.RandomState.choice']"
426;2.0;1;25646200;;1;27;<python><pandas><timedelta>;Python: Convert timedelta to int in a dataframe;"<p>I would like to create a column in a pandas data frame that is an integer representation of the number of days in a timedelta column.  Is it possible to use 'datetime.days' or do I need to do something more manual?</p>

<p><strong>timedelta column</strong>                </p>

<blockquote>
  <p>7 days, 23:29:00</p>
</blockquote>

<p><strong>day integer column</strong></p>

<blockquote>
  <p>7</p>
</blockquote>
";31951.0;[];[]
427;2.0;0;25748683;;1;35;<python><pandas><dataframe><sum>;Pandas: sum DataFrame rows for given columns;"<p>I have the following DataFrame:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})
</code></pre>

<p>I would like to add a column 'e' which is the sum of column 'a', 'b' and 'd'.</p>

<p>Going across forums, I thought something like this would work:</p>

<pre><code>df['e'] = df[['a','b','d']].map(sum)
</code></pre>

<p>But no!</p>

<p>I would like to realize the operation having the list of columns <code>['a','b','d']</code> and <code>df</code> as inputs.</p>
";88000.0;"[""import pandas as pd\ndf = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})\n"", ""df['e'] = df[['a','b','d']].map(sum)\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})\n"", ""df['e'] = df[['a','b','d']].map(sum)\n"", ""['a','b','d']"", 'df']"
428;2.0;0;25773245;;1;58;<python><arrays><pandas><numpy><dataframe>;"Ambiguity in Pandas Dataframe / Numpy Array ""axis"" definition";"<p>I've been very confused about how python axes are defined, and whether they refer to a DataFrame's rows or columns. Consider the code below:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]], columns=[""col1"", ""col2"", ""col3"", ""col4""])
&gt;&gt;&gt; df
   col1  col2  col3  col4
0     1     1     1     1
1     2     2     2     2
2     3     3     3     3
</code></pre>

<p>So if we call <code>df.mean(axis=1)</code>, we'll get a mean across the rows:</p>

<pre><code>&gt;&gt;&gt; df.mean(axis=1)
0    1
1    2
2    3
</code></pre>

<p>However, if we call <code>df.drop(name, axis=1)</code>, we actually <strong>drop a column</strong>, not a row:</p>

<pre><code>&gt;&gt;&gt; df.drop(""col4"", axis=1)
   col1  col2  col3
0     1     1     1
1     2     2     2
2     3     3     3
</code></pre>

<p>Can someone help me understand what is meant by an ""axis"" in pandas/numpy/scipy?</p>

<p>A side note, <code>DataFrame.mean</code> just might be defined wrong. It says in the documentation for <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mean.html""><code>DataFrame.mean</code></a> that <code>axis=1</code> is supposed to mean a mean over the columns, not the rows...</p>
";11210.0;"['>>> df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]], columns=[""col1"", ""col2"", ""col3"", ""col4""])\n>>> df\n   col1  col2  col3  col4\n0     1     1     1     1\n1     2     2     2     2\n2     3     3     3     3\n', '>>> df.mean(axis=1)\n0    1\n1    2\n2    3\n', '>>> df.drop(""col4"", axis=1)\n   col1  col2  col3\n0     1     1     1\n1     2     2     2\n2     3     3     3\n']";"['>>> df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]], columns=[""col1"", ""col2"", ""col3"", ""col4""])\n>>> df\n   col1  col2  col3  col4\n0     1     1     1     1\n1     2     2     2     2\n2     3     3     3     3\n', 'df.mean(axis=1)', '>>> df.mean(axis=1)\n0    1\n1    2\n2    3\n', 'df.drop(name, axis=1)', '>>> df.drop(""col4"", axis=1)\n   col1  col2  col3\n0     1     1     1\n1     2     2     2\n2     3     3     3\n', 'DataFrame.mean', 'DataFrame.mean', 'axis=1']"
429;5.0;1;25962114;;1;37;<python><memory><numpy><pandas>;How to read a 6 GB csv file with pandas;"<p>I am trying to read a large csv file (aprox. 6 GB) in pandas and i am getting the following memory error:</p>

<pre><code>MemoryError                               Traceback (most recent call last)
&lt;ipython-input-58-67a72687871b&gt; in &lt;module&gt;()
----&gt; 1 data=pd.read_csv('aphro.csv',sep=';')

C:\Python27\lib\site-packages\pandas\io\parsers.pyc in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format)
    450                     infer_datetime_format=infer_datetime_format)
    451 
--&gt; 452         return _read(filepath_or_buffer, kwds)
    453 
    454     parser_f.__name__ = name

C:\Python27\lib\site-packages\pandas\io\parsers.pyc in _read(filepath_or_buffer, kwds)
    242         return parser
    243 
--&gt; 244     return parser.read()
    245 
    246 _parser_defaults = {

C:\Python27\lib\site-packages\pandas\io\parsers.pyc in read(self, nrows)
    693                 raise ValueError('skip_footer not supported for iteration')
    694 
--&gt; 695         ret = self._engine.read(nrows)
    696 
    697         if self.options.get('as_recarray'):

C:\Python27\lib\site-packages\pandas\io\parsers.pyc in read(self, nrows)
   1137 
   1138         try:
-&gt; 1139             data = self._reader.read(nrows)
   1140         except StopIteration:
   1141             if nrows is None:

C:\Python27\lib\site-packages\pandas\parser.pyd in pandas.parser.TextReader.read (pandas\parser.c:7145)()

C:\Python27\lib\site-packages\pandas\parser.pyd in pandas.parser.TextReader._read_low_memory (pandas\parser.c:7369)()

C:\Python27\lib\site-packages\pandas\parser.pyd in pandas.parser.TextReader._read_rows (pandas\parser.c:8194)()

C:\Python27\lib\site-packages\pandas\parser.pyd in pandas.parser.TextReader._convert_column_data (pandas\parser.c:9402)()

C:\Python27\lib\site-packages\pandas\parser.pyd in pandas.parser.TextReader._convert_tokens (pandas\parser.c:10057)()

C:\Python27\lib\site-packages\pandas\parser.pyd in pandas.parser.TextReader._convert_with_dtype (pandas\parser.c:10361)()

C:\Python27\lib\site-packages\pandas\parser.pyd in pandas.parser._try_int64 (pandas\parser.c:17806)()

MemoryError: 
</code></pre>

<p>Any help on this?? </p>
";40201.0;"[""MemoryError                               Traceback (most recent call last)\n<ipython-input-58-67a72687871b> in <module>()\n----> 1 data=pd.read_csv('aphro.csv',sep=';')\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format)\n    450                     infer_datetime_format=infer_datetime_format)\n    451 \n--> 452         return _read(filepath_or_buffer, kwds)\n    453 \n    454     parser_f.__name__ = name\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in _read(filepath_or_buffer, kwds)\n    242         return parser\n    243 \n--> 244     return parser.read()\n    245 \n    246 _parser_defaults = {\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in read(self, nrows)\n    693                 raise ValueError('skip_footer not supported for iteration')\n    694 \n--> 695         ret = self._engine.read(nrows)\n    696 \n    697         if self.options.get('as_recarray'):\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in read(self, nrows)\n   1137 \n   1138         try:\n-> 1139             data = self._reader.read(nrows)\n   1140         except StopIteration:\n   1141             if nrows is None:\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader.read (pandas\\parser.c:7145)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._read_low_memory (pandas\\parser.c:7369)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._read_rows (pandas\\parser.c:8194)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_column_data (pandas\\parser.c:9402)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_tokens (pandas\\parser.c:10057)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_with_dtype (pandas\\parser.c:10361)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser._try_int64 (pandas\\parser.c:17806)()\n\nMemoryError: \n""]";"[""MemoryError                               Traceback (most recent call last)\n<ipython-input-58-67a72687871b> in <module>()\n----> 1 data=pd.read_csv('aphro.csv',sep=';')\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format)\n    450                     infer_datetime_format=infer_datetime_format)\n    451 \n--> 452         return _read(filepath_or_buffer, kwds)\n    453 \n    454     parser_f.__name__ = name\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in _read(filepath_or_buffer, kwds)\n    242         return parser\n    243 \n--> 244     return parser.read()\n    245 \n    246 _parser_defaults = {\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in read(self, nrows)\n    693                 raise ValueError('skip_footer not supported for iteration')\n    694 \n--> 695         ret = self._engine.read(nrows)\n    696 \n    697         if self.options.get('as_recarray'):\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in read(self, nrows)\n   1137 \n   1138         try:\n-> 1139             data = self._reader.read(nrows)\n   1140         except StopIteration:\n   1141             if nrows is None:\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader.read (pandas\\parser.c:7145)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._read_low_memory (pandas\\parser.c:7369)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._read_rows (pandas\\parser.c:8194)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_column_data (pandas\\parser.c:9402)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_tokens (pandas\\parser.c:10057)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_with_dtype (pandas\\parser.c:10361)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser._try_int64 (pandas\\parser.c:17806)()\n\nMemoryError: \n""]"
430;2.0;4;26047209;;1;43;<python><pandas>;What is the difference between a pandas Series and a single-column DataFrame?;"<p>Why does pandas make a distinction between a <code>Series</code> and a single-column <code>DataFrame</code>?<br>
In other words: what is the reason of existence of the <code>Series</code> class? </p>

<p>I'm mainly using time series with datetime index, maybe that helps to set the context. </p>
";19282.0;[];['Series', 'DataFrame', 'Series']
431;4.0;0;26139423;;1;35;<matplotlib><pandas><visualization>;plot different color for different categorical levels using matplotlib;"<p>I have this data frame <code>diamonds</code> which is composed of variables like <code>(carat, price, color)</code>, and I want to draw a scatter plot of <code>price</code> to <code>carat</code> for each <code>color</code>, which means different <code>color</code> has different color in the plot.</p>

<p>This is easy in <code>R</code> with <code>ggplot</code>:</p>

<pre><code>ggplot(aes(x=carat, y=price, color=color),  #by setting color=color, ggplot automatically draw in different colors
       data=diamonds) + geom_point(stat='summary', fun.y=median)
</code></pre>

<p><img src=""https://i.stack.imgur.com/HW43K.png"" alt=""enter image description here""></p>

<p>I wonder how could this be done in Python using <code>matplotlib</code> ?</p>

<p>PS:</p>

<p>I know about auxiliary plotting packages, such as <code>seaborn</code> and <code>ggplot for python</code>, and I donot prefer them, just want to find out if it is possible to do the job using <code>matplotlib</code> alone, ;P</p>
";28342.0;"[""ggplot(aes(x=carat, y=price, color=color),  #by setting color=color, ggplot automatically draw in different colors\n       data=diamonds) + geom_point(stat='summary', fun.y=median)\n""]";"['diamonds', '(carat, price, color)', 'price', 'carat', 'color', 'color', 'R', 'ggplot', ""ggplot(aes(x=carat, y=price, color=color),  #by setting color=color, ggplot automatically draw in different colors\n       data=diamonds) + geom_point(stat='summary', fun.y=median)\n"", 'matplotlib', 'seaborn', 'ggplot for python', 'matplotlib']"
432;3.0;0;26187759;;1;22;<python><pandas><parallel-processing><rosetta>;Parallelize apply after pandas groupby;"<p>I have used rosetta.parallel.pandas_easy to parallelize apply after group by, for example:</p>

<pre><code>from rosetta.parallel.pandas_easy import groupby_to_series_to_frame
df = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])
groupby_to_series_to_frame(df, np.mean, n_jobs=8, use_apply=True, by=df.index)
</code></pre>

<p>However, has anyone figured out how to parallelize a function that returns a dataframe? This code fails for rosetta, as expected.</p>

<pre><code>def tmpFunc(df):
    df['c'] = df.a + df.b
    return df

df.groupby(df.index).apply(tmpFunc)
groupby_to_series_to_frame(df, tmpFunc, n_jobs=1, use_apply=True, by=df.index)
</code></pre>
";11709.0;"[""from rosetta.parallel.pandas_easy import groupby_to_series_to_frame\ndf = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])\ngroupby_to_series_to_frame(df, np.mean, n_jobs=8, use_apply=True, by=df.index)\n"", ""def tmpFunc(df):\n    df['c'] = df.a + df.b\n    return df\n\ndf.groupby(df.index).apply(tmpFunc)\ngroupby_to_series_to_frame(df, tmpFunc, n_jobs=1, use_apply=True, by=df.index)\n""]";"[""from rosetta.parallel.pandas_easy import groupby_to_series_to_frame\ndf = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])\ngroupby_to_series_to_frame(df, np.mean, n_jobs=8, use_apply=True, by=df.index)\n"", ""def tmpFunc(df):\n    df['c'] = df.a + df.b\n    return df\n\ndf.groupby(df.index).apply(tmpFunc)\ngroupby_to_series_to_frame(df, tmpFunc, n_jobs=1, use_apply=True, by=df.index)\n""]"
433;9.0;0;26266362;;1;110;<python><pandas>;How to count the Nan values in the column in Panda Data frame;"<p>I have data, in which I want to find number of NaN, so that if it is less than some threshold, I will drop this columns. I looked, but didn't able to find any function for this. there is count_values(), but it would be slow for me, because most of values are distinct and I want count of NaN only.</p>
";83763.0;[];[]
434;2.0;5;26277757;;1;32;<python><html><pandas>;Pandas to_html() truncates string contents;"<p>I have a Python Pandas <code>DataFrame</code> object containing textual data. My problem is, that when I use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_html.html"" rel=""nofollow noreferrer""><code>to_html()</code></a> function, it truncates the strings in the output.</p>

<p>For example:</p>

<pre class=""lang-py prettyprint-override""><code>import pandas
df = pandas.DataFrame({'text': ['Lorem ipsum dolor sit amet, consectetur adipiscing elit.']})
print (df.to_html())
</code></pre>

<p>The output is truncated at <code>adapis...</code></p>

<pre><code>&lt;table border=""1"" class=""dataframe""&gt;
  &lt;thead&gt;
    &lt;tr style=""text-align: right;""&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt; Lorem ipsum dolor sit amet, consectetur adipis...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</code></pre>

<p>There is a related question on SO, but it uses placeholders and search/replace functionality to postprocess the HTML, which I would like to avoid:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/25070758/writing-full-contents-of-pandas-dataframe-to-html-table"">Writing full contents of Pandas dataframe to HTML table</a></li>
</ul>

<p>Is there a simpler solution to this problem? I could not find anything related from the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_html.html"" rel=""nofollow noreferrer"">documentation</a>.</p>
";7560.0;"[""import pandas\ndf = pandas.DataFrame({'text': ['Lorem ipsum dolor sit amet, consectetur adipiscing elit.']})\nprint (df.to_html())\n"", '<table border=""1"" class=""dataframe"">\n  <thead>\n    <tr style=""text-align: right;"">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td> Lorem ipsum dolor sit amet, consectetur adipis...</td>\n    </tr>\n  </tbody>\n</table>\n']";"['DataFrame', 'to_html()', ""import pandas\ndf = pandas.DataFrame({'text': ['Lorem ipsum dolor sit amet, consectetur adipiscing elit.']})\nprint (df.to_html())\n"", 'adapis...', '<table border=""1"" class=""dataframe"">\n  <thead>\n    <tr style=""text-align: right;"">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td> Lorem ipsum dolor sit amet, consectetur adipis...</td>\n    </tr>\n  </tbody>\n</table>\n']"
435;5.0;1;26309962;;1;21;<python><pandas><append><dataframe>;Appending a list or series to a pandas DataFrame as a row?;"<p>So I have initialized an empty pandas DataFrame and I would like to iteratively append lists (or Series) as rows in this DataFrame. What is the best way of doing this?</p>
";40866.0;[];[]
436;1.0;0;26347412;;1;35;<python><pandas>;Drop multiple columns in pandas;"<p>I am trying to drop multiple columns (column 2 and 70 in my data set, indexed as 1 and 69 respectively) by index number in a pandas data frame with the following code:</p>

<pre><code>df.drop([df.columns[[1, 69]]], axis=1, inplace=True)
</code></pre>

<p>I get the following error: </p>

<pre><code>TypeError: unhashable type: 'Index'
</code></pre>

<p>And in my code the [1, 69] is highlighted and says:</p>

<pre><code>Expected type 'Integral', got 'list[int]' instead
</code></pre>

<p>The following code does what I want it to do successfully, but on two lines of repetitive code (first dropping col index 69, then 1, and order does matter because dropping earlier columns changes the index of later columns). I thought I could specify more than one column index simply as a list, but perhaps I have something wrong above?</p>

<pre><code>df.drop([df.columns[69]], axis=1, inplace=True)
df.drop([df.columns[1]], axis=1, inplace=True)
</code></pre>

<p>Is there a way that I can do this on one line similar to the first code snippet above?</p>
";66476.0;"['df.drop([df.columns[[1, 69]]], axis=1, inplace=True)\n', ""TypeError: unhashable type: 'Index'\n"", ""Expected type 'Integral', got 'list[int]' instead\n"", 'df.drop([df.columns[69]], axis=1, inplace=True)\ndf.drop([df.columns[1]], axis=1, inplace=True)\n']";"['df.drop([df.columns[[1, 69]]], axis=1, inplace=True)\n', ""TypeError: unhashable type: 'Index'\n"", ""Expected type 'Integral', got 'list[int]' instead\n"", 'df.drop([df.columns[69]], axis=1, inplace=True)\ndf.drop([df.columns[1]], axis=1, inplace=True)\n']"
437;14.0;4;26473681;;1;59;<python><numpy><pandas><pip>;"PIP Install Numpy throws an error ""ascii codec can't decode byte 0xe2""";"<p>I have a freshly installed Ubuntu on a freshly built computer. I just installed python-pip using apt-get. Now when I try to pip install Numpy and Pandas, it gives the following error.</p>

<p>I've seen this error mentioned in quite a few places on SO and Google, but I haven't been able to find a solution. Some people mention it's a bug, some threads are just dead... What's going on?</p>

<pre><code>Traceback (most recent call last):
  File ""/usr/bin/pip"", line 9, in &lt;module&gt;
    load_entry_point('pip==1.5.4', 'console_scripts', 'pip')()
  File ""/usr/lib/python2.7/dist-packages/pip/__init__.py"", line 185, in main
    return command.main(cmd_args)
  File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 161, in main
    text = '\n'.join(complete_log)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 72: ordinal not in range(128)
</code></pre>
";46327.0;"['Traceback (most recent call last):\n  File ""/usr/bin/pip"", line 9, in <module>\n    load_entry_point(\'pip==1.5.4\', \'console_scripts\', \'pip\')()\n  File ""/usr/lib/python2.7/dist-packages/pip/__init__.py"", line 185, in main\n    return command.main(cmd_args)\n  File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 161, in main\n    text = \'\\n\'.join(complete_log)\nUnicodeDecodeError: \'ascii\' codec can\'t decode byte 0xe2 in position 72: ordinal not in range(128)\n']";"['Traceback (most recent call last):\n  File ""/usr/bin/pip"", line 9, in <module>\n    load_entry_point(\'pip==1.5.4\', \'console_scripts\', \'pip\')()\n  File ""/usr/lib/python2.7/dist-packages/pip/__init__.py"", line 185, in main\n    return command.main(cmd_args)\n  File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 161, in main\n    text = \'\\n\'.join(complete_log)\nUnicodeDecodeError: \'ascii\' codec can\'t decode byte 0xe2 in position 72: ordinal not in range(128)\n']"
438;1.0;3;26483254;;1;22;<python><list><pandas><insert><dataframe>;Python pandas insert list into a cell;"<p>I have a list 'abc' and a dataframe 'df':</p>

<pre><code>abc = ['foo', 'bar']
df =
    A  B
0  12  NaN
1  23  NaN
</code></pre>

<p>I want to insert the list into cell 1B, so I want this result:</p>

<pre><code>    A  B
0  12  NaN
1  23  ['foo', 'bar']
</code></pre>

<p>Ho can I do that?</p>

<p>1) If I use this:</p>

<pre><code>df.ix[1,'B'] = abc
</code></pre>

<p>I get the following error message:</p>

<pre><code>ValueError: Must have equal len keys and value when setting with an iterable
</code></pre>

<p>because it tries to insert the list (that has two elements) into a row / column but not into a cell.</p>

<p>2) If I use this:</p>

<pre><code>df.ix[1,'B'] = [abc]
</code></pre>

<p>then it inserts a list that has only one element that is the 'abc' list ( <code>[['foo', 'bar']]</code> ).</p>

<p>3) If I use this:</p>

<pre><code>df.ix[1,'B'] = ', '.join(abc)
</code></pre>

<p>then it inserts a string: ( <code>foo, bar</code> ) but not a list.</p>

<p>4) If I use this:</p>

<pre><code>df.ix[1,'B'] = [', '.join(abc)]
</code></pre>

<p>then it inserts a list but it has only one element ( <code>['foo, bar']</code> ) but not two as I want ( <code>['foo', 'bar']</code> ).</p>

<p>Thanks for help!</p>

<hr>

<h2>EDIT</h2>

<p>My new dataframe and the old list:</p>

<pre><code>abc = ['foo', 'bar']
df2 =
    A    B         C
0  12  NaN      'bla'
1  23  NaN  'bla bla'
</code></pre>

<p>Another dataframe:</p>

<pre><code>df3 =
    A    B         C                    D
0  12  NaN      'bla'  ['item1', 'item2']
1  23  NaN  'bla bla'        [11, 12, 13]
</code></pre>

<p>I want insert the 'abc' list into <code>df2.loc[1,'B']</code> and/or <code>df3.loc[1,'B']</code>.</p>

<p>If the dataframe has columns only with integer values and/or NaN values and/or list values then inserting a list into a cell works perfectly. If the dataframe has columns only with string values and/or NaN values and/or list values then inserting a list into a cell works perfectly. But if the dataframe has columns with integer and string values and other columns then the error message appears if I use this: <code>df2.loc[1,'B'] = abc</code> or <code>df3.loc[1,'B'] = abc</code>.</p>

<p>Another dataframe:</p>

<pre><code>df4 =
          A     B
0      'bla'  NaN
1  'bla bla'  NaN
</code></pre>

<p>These inserts work perfectly: <code>df.loc[1,'B'] = abc</code> or <code>df4.loc[1,'B'] = abc</code>.</p>
";17970.0;"[""abc = ['foo', 'bar']\ndf =\n    A  B\n0  12  NaN\n1  23  NaN\n"", ""    A  B\n0  12  NaN\n1  23  ['foo', 'bar']\n"", ""df.ix[1,'B'] = abc\n"", 'ValueError: Must have equal len keys and value when setting with an iterable\n', ""df.ix[1,'B'] = [abc]\n"", ""df.ix[1,'B'] = ', '.join(abc)\n"", ""df.ix[1,'B'] = [', '.join(abc)]\n"", ""abc = ['foo', 'bar']\ndf2 =\n    A    B         C\n0  12  NaN      'bla'\n1  23  NaN  'bla bla'\n"", ""df3 =\n    A    B         C                    D\n0  12  NaN      'bla'  ['item1', 'item2']\n1  23  NaN  'bla bla'        [11, 12, 13]\n"", ""df4 =\n          A     B\n0      'bla'  NaN\n1  'bla bla'  NaN\n""]";"[""abc = ['foo', 'bar']\ndf =\n    A  B\n0  12  NaN\n1  23  NaN\n"", ""    A  B\n0  12  NaN\n1  23  ['foo', 'bar']\n"", ""df.ix[1,'B'] = abc\n"", 'ValueError: Must have equal len keys and value when setting with an iterable\n', ""df.ix[1,'B'] = [abc]\n"", ""[['foo', 'bar']]"", ""df.ix[1,'B'] = ', '.join(abc)\n"", 'foo, bar', ""df.ix[1,'B'] = [', '.join(abc)]\n"", ""['foo, bar']"", ""['foo', 'bar']"", ""abc = ['foo', 'bar']\ndf2 =\n    A    B         C\n0  12  NaN      'bla'\n1  23  NaN  'bla bla'\n"", ""df3 =\n    A    B         C                    D\n0  12  NaN      'bla'  ['item1', 'item2']\n1  23  NaN  'bla bla'        [11, 12, 13]\n"", ""df2.loc[1,'B']"", ""df3.loc[1,'B']"", ""df2.loc[1,'B'] = abc"", ""df3.loc[1,'B'] = abc"", ""df4 =\n          A     B\n0      'bla'  NaN\n1  'bla bla'  NaN\n"", ""df.loc[1,'B'] = abc"", ""df4.loc[1,'B'] = abc""]"
439;4.0;0;26640145;;1;24;<python><pandas><dataframe>;Python Pandas: How to get the row names from index of a dataframe?;"<p>So assume I have a dataframe with rownames that aren't a column of their own per se such as the following:</p>

<pre><code>        X  Y
 Row 1  0  5
 Row 2  8  1
 Row 3  3  0
</code></pre>

<p>How would I extract these row names as a list, if I have their index?
For example, it would look something like: </p>

<p>function_name(dataframe[indices])</p>

<blockquote>
  <p>['Row 1', 'Row 2']</p>
</blockquote>

<p>Thanks for your help!</p>
";41938.0;['        X  Y\n Row 1  0  5\n Row 2  8  1\n Row 3  3  0\n'];['        X  Y\n Row 1  0  5\n Row 2  8  1\n Row 3  3  0\n']
440;3.0;5;26645515;;1;36;<python><join><pandas>;Pandas join issue: columns overlap but no suffix specified;"<p>I have following 2 data frames:</p>

<pre><code>df_a =

     mukey  DI  PI
0   100000  35  14
1  1000005  44  14
2  1000006  44  14
3  1000007  43  13
4  1000008  43  13

df_b = 
    mukey  niccdcd
0  190236        4
1  190237        6
2  190238        7
3  190239        4
4  190240        7
</code></pre>

<p>When I try to join these 2 dataframes:</p>

<pre><code>join_df = df_a.join(df_b,on='mukey',how='left')
</code></pre>

<p>I get the error:</p>

<pre><code>*** ValueError: columns overlap but no suffix specified: Index([u'mukey'], dtype='object')
</code></pre>

<p>Why is this so? The dataframes do have common 'mukey' values.</p>
";30832.0;"['df_a =\n\n     mukey  DI  PI\n0   100000  35  14\n1  1000005  44  14\n2  1000006  44  14\n3  1000007  43  13\n4  1000008  43  13\n\ndf_b = \n    mukey  niccdcd\n0  190236        4\n1  190237        6\n2  190238        7\n3  190239        4\n4  190240        7\n', ""join_df = df_a.join(df_b,on='mukey',how='left')\n"", ""*** ValueError: columns overlap but no suffix specified: Index([u'mukey'], dtype='object')\n""]";"['df_a =\n\n     mukey  DI  PI\n0   100000  35  14\n1  1000005  44  14\n2  1000006  44  14\n3  1000007  43  13\n4  1000008  43  13\n\ndf_b = \n    mukey  niccdcd\n0  190236        4\n1  190237        6\n2  190238        7\n3  190239        4\n4  190240        7\n', ""join_df = df_a.join(df_b,on='mukey',how='left')\n"", ""*** ValueError: columns overlap but no suffix specified: Index([u'mukey'], dtype='object')\n""]"
441;1.0;2;26658240;;1;26;<python-2.7><pandas><dataframe>;getting the index of a row in a pandas apply function;"<p>I am trying to access the index of a row in a function applied across an entire <code>DataFrame</code> in Pandas. I have something like this:</p>

<pre><code>df = pandas.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])
&gt;&gt;&gt; df
   a  b  c
0  1  2  3
1  4  5  6
</code></pre>

<p>and I'll define a function that access elements with a given row</p>

<pre><code>def rowFunc(row):
    return row['a'] + row['b'] * row['c']
</code></pre>

<p>I can apply it like so:</p>

<pre><code>df['d'] = df.apply(rowFunc, axis=1)
&gt;&gt;&gt; df
   a  b  c   d
0  1  2  3   7
1  4  5  6  34
</code></pre>

<p>Awesome! Now what if I want to incorporate the index into my function?
The index of any given row in this <code>DataFrame</code> before adding <code>d</code> would be <code>Index([u'a', u'b', u'c', u'd'], dtype='object')</code>, but I want the 0 and 1. So I can't just access <code>row.index</code>.</p>

<p>I know I could create a temporary column in the table where I store the index, but I""m wondering if it is sotred in the row object somewhere.</p>
";9080.0;"[""df = pandas.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])\n>>> df\n   a  b  c\n0  1  2  3\n1  4  5  6\n"", ""def rowFunc(row):\n    return row['a'] + row['b'] * row['c']\n"", ""df['d'] = df.apply(rowFunc, axis=1)\n>>> df\n   a  b  c   d\n0  1  2  3   7\n1  4  5  6  34\n""]";"['DataFrame', ""df = pandas.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])\n>>> df\n   a  b  c\n0  1  2  3\n1  4  5  6\n"", ""def rowFunc(row):\n    return row['a'] + row['b'] * row['c']\n"", ""df['d'] = df.apply(rowFunc, axis=1)\n>>> df\n   a  b  c   d\n0  1  2  3   7\n1  4  5  6  34\n"", 'DataFrame', 'd', ""Index([u'a', u'b', u'c', u'd'], dtype='object')"", 'row.index']"
442;4.0;2;26716616;;1;28;<python><pandas><dictionary><dataframe>;Convert a Pandas DataFrame to a dictionary;"<p>I have a DataFrame with four columns. I want to convert this DataFrame to a python dictionary. I want the elements of first column be <code>keys</code> and the elements of other columns in same row be <code>values</code>. </p>

<p>DataFrame:   </p>

<pre><code>    ID   A   B   C
0   p    1   3   2
1   q    4   3   2
2   r    4   0   9  
</code></pre>

<p>Output should be like this:</p>

<p>Dictionary:</p>

<pre><code>{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}
</code></pre>
";34674.0;"['    ID   A   B   C\n0   p    1   3   2\n1   q    4   3   2\n2   r    4   0   9  \n', ""{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}\n""]";"['keys', 'values', '    ID   A   B   C\n0   p    1   3   2\n1   q    4   3   2\n2   r    4   0   9  \n', ""{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}\n""]"
443;3.0;0;26763344;;1;43;<python><datetime><pandas>;Convert Pandas Column to DateTime;"<p>I have one field in a pandas DataFrame that was imported as string format. 
It should be a datetime variable.
How do I convert it to a datetime column and then filter based on date.</p>

<p>Example:</p>

<ul>
<li>DataFrame Name: <strong>raw_data</strong>    </li>
<li>Column Name: <strong>Mycol</strong>    </li>
<li>Value
Format in Column: <strong>'05SEP2014:00:00:00.000'</strong></li>
</ul>
";63841.0;[];[]
444;2.0;0;26786960;;1;28;<python><csv><pandas>;pandas to_csv first extra column remove, how to?;"<p>I'm trying to create csv with pandas , but when I export to csv it gave me one extra row </p>

<pre><code>d = {'one' : pd.Series([1., 2., 3.]),'two' : pd.Series([1., 2., 3., 4.])}
df0_fa = pd.DataFrame(d)
df_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w')
</code></pre>

<p>so my result would be : </p>

<pre><code>,one,two
0,1.0,1.0
1,2.0,2.0
2,3.0,3.0
3,,4.0
</code></pre>

<p>But what I want is</p>

<pre><code>one,two
1.0,1.0
2.0,2.0
3.0,3.0
,4.0
</code></pre>
";14508.0;"[""d = {'one' : pd.Series([1., 2., 3.]),'two' : pd.Series([1., 2., 3., 4.])}\ndf0_fa = pd.DataFrame(d)\ndf_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w')\n"", ',one,two\n0,1.0,1.0\n1,2.0,2.0\n2,3.0,3.0\n3,,4.0\n', 'one,two\n1.0,1.0\n2.0,2.0\n3.0,3.0\n,4.0\n']";"[""d = {'one' : pd.Series([1., 2., 3.]),'two' : pd.Series([1., 2., 3., 4.])}\ndf0_fa = pd.DataFrame(d)\ndf_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w')\n"", ',one,two\n0,1.0,1.0\n1,2.0,2.0\n2,3.0,3.0\n3,,4.0\n', 'one,two\n1.0,1.0\n2.0,2.0\n3.0,3.0\n,4.0\n']"
445;3.0;1;26837998;;1;40;<python><pandas><nan>;Pandas Replace NaN with blank/empty string;"<p>I have a Pandas Dataframe as shown below:</p>

<pre><code>    1    2       3
 0  a  NaN    read
 1  b    l  unread
 2  c  NaN    read
</code></pre>

<p>I want to remove the NaN values with an empty string so that it looks like so:</p>

<pre><code>    1    2       3
 0  a   """"    read
 1  b    l  unread
 2  c   """"    read
</code></pre>
";36883.0;"['    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n', '    1    2       3\n 0  a   """"    read\n 1  b    l  unread\n 2  c   """"    read\n']";"['    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n', '    1    2       3\n 0  a   """"    read\n 1  b    l  unread\n 2  c   """"    read\n']"
446;4.0;2;26873127;;1;78;<pandas><printing><ipython-notebook><jupyter-notebook><display>;Show DataFrame as table in iPython Notebook;"<p>I am using iPython notebook.  When I do this:</p>

<pre><code>df
</code></pre>

<p>I get a beautiful table with cells.  However, if i do this:</p>

<pre><code>df1
df2 
</code></pre>

<p>it doesn't print the first beautiful table.  If I try this:</p>

<pre><code>print df1
print df2
</code></pre>

<p>It prints out the table in a different format that spills columns over and makes the output very tall.  </p>

<p>Is there a way to force it to print out the beautiful tables for both datasets?</p>
";52192.0;['df\n', 'df1\ndf2 \n', 'print df1\nprint df2\n'];['df\n', 'df1\ndf2 \n', 'print df1\nprint df2\n']
447;1.0;0;26886653;;1;52;<python><numpy><pandas>;pandas create new column based on values from other columns;"<p>I've tried different methods from other questions but still can't seem to find the right answer for my problem.  The critical piece of this is that if the person is counted as Hispanic they can't be counted as anything else.  Even if they have a ""1"" in another ethnicity column they still are counted as Hispanic not a two or more races.  Similarly, if the sum of all the ERI columns is greater than 1 they are counted as two or more races and can't be counted as a unique ethnicity(accept for Hispanic).  Hopefully this makes sense.  Any help will be greatly appreciated. </p>

<p>Its almost like doing a for loop through each row and if each record meets a criteria they are added to one list and eliminated from the original.  </p>

<p>From the dataframe below I need to calculate a new column based off of the following:</p>

<p>=========================  CRITERIA  ===============================</p>

<pre><code>IF [ERI_Hispanic] = 1 THEN RETURN Hispanic
ELSE IF SUM([ERI_AmerInd_AKNatv] + [ERI_Asian] + [ERI_Black_Afr.Amer] + [ERI_HI_PacIsl] + [ERI_White]) &gt; 1 THEN RETURN Two or More
ELSE IF [ERI_AmerInd_AKNatv] = 1 THEN RETURN A/I AK Native
ELSE IF [ERI_Asian] = 1 THEN RETURN Asian
ELSE IF [ERI_Black_Afr.Amer] = 1 THEN RETURN Black/AA
ELSE IF [ERI_HI_PacIsl] = 1 THEN RETURN Haw/Pac Isl.
ELSE IF [ERI_White] = 1 THEN RETURN White
</code></pre>

<p>Comment: If the ERI Flag for Hispanic is True (1), then employee is classified as Hispanic</p>

<p>Comment: If more than 1 non-Hispanic ERI Flag are true, return Two or More</p>

<p>======================  DATAFRAME ===========================</p>

<p>In [13]: df1</p>

<p>Out [13]: </p>

<pre><code>     lname          fname       rno_cd  eri_afr_amer    eri_asian   eri_hawaiian    eri_hispanic    eri_nat_amer    eri_white   rno_defined
0    MOST           JEFF        E       0               0           0               0               0               1           White
1    CRUISE         TOM         E       0               0           0               1               0               0           White
2    DEPP           JOHNNY              0               0           0               0               0               1           Unknown
3    DICAP          LEO                 0               0           0               0               0               1           Unknown
4    BRANDO         MARLON      E       0               0           0               0               0               0           White
5    HANKS          TOM         0                       0           0               0               0               1           Unknown
6    DENIRO         ROBERT      E       0               1           0               0               0               1           White
7    PACINO         AL          E       0               0           0               0               0               1           White
8    WILLIAMS       ROBIN       E       0               0           1               0               0               0           White
9    EASTWOOD       CLINT       E       0               0           0               0               0               1           White
</code></pre>
";68245.0;['IF [ERI_Hispanic] = 1 THEN RETURN \x93Hispanic\x94\nELSE IF SUM([ERI_AmerInd_AKNatv] + [ERI_Asian] + [ERI_Black_Afr.Amer] + [ERI_HI_PacIsl] + [ERI_White]) > 1 THEN RETURN \x93Two or More\x94\nELSE IF [ERI_AmerInd_AKNatv] = 1 THEN RETURN \x93A/I AK Native\x94\nELSE IF [ERI_Asian] = 1 THEN RETURN \x93Asian\x94\nELSE IF [ERI_Black_Afr.Amer] = 1 THEN RETURN \x93Black/AA\x94\nELSE IF [ERI_HI_PacIsl] = 1 THEN RETURN \x93Haw/Pac Isl.\x94\nELSE IF [ERI_White] = 1 THEN RETURN \x93White\x94\n', '     lname          fname       rno_cd  eri_afr_amer    eri_asian   eri_hawaiian    eri_hispanic    eri_nat_amer    eri_white   rno_defined\n0    MOST           JEFF        E       0               0           0               0               0               1           White\n1    CRUISE         TOM         E       0               0           0               1               0               0           White\n2    DEPP           JOHNNY              0               0           0               0               0               1           Unknown\n3    DICAP          LEO                 0               0           0               0               0               1           Unknown\n4    BRANDO         MARLON      E       0               0           0               0               0               0           White\n5    HANKS          TOM         0                       0           0               0               0               1           Unknown\n6    DENIRO         ROBERT      E       0               1           0               0               0               1           White\n7    PACINO         AL          E       0               0           0               0               0               1           White\n8    WILLIAMS       ROBIN       E       0               0           1               0               0               0           White\n9    EASTWOOD       CLINT       E       0               0           0               0               0               1           White\n'];['IF [ERI_Hispanic] = 1 THEN RETURN \x93Hispanic\x94\nELSE IF SUM([ERI_AmerInd_AKNatv] + [ERI_Asian] + [ERI_Black_Afr.Amer] + [ERI_HI_PacIsl] + [ERI_White]) > 1 THEN RETURN \x93Two or More\x94\nELSE IF [ERI_AmerInd_AKNatv] = 1 THEN RETURN \x93A/I AK Native\x94\nELSE IF [ERI_Asian] = 1 THEN RETURN \x93Asian\x94\nELSE IF [ERI_Black_Afr.Amer] = 1 THEN RETURN \x93Black/AA\x94\nELSE IF [ERI_HI_PacIsl] = 1 THEN RETURN \x93Haw/Pac Isl.\x94\nELSE IF [ERI_White] = 1 THEN RETURN \x93White\x94\n', '     lname          fname       rno_cd  eri_afr_amer    eri_asian   eri_hawaiian    eri_hispanic    eri_nat_amer    eri_white   rno_defined\n0    MOST           JEFF        E       0               0           0               0               0               1           White\n1    CRUISE         TOM         E       0               0           0               1               0               0           White\n2    DEPP           JOHNNY              0               0           0               0               0               1           Unknown\n3    DICAP          LEO                 0               0           0               0               0               1           Unknown\n4    BRANDO         MARLON      E       0               0           0               0               0               0           White\n5    HANKS          TOM         0                       0           0               0               0               1           Unknown\n6    DENIRO         ROBERT      E       0               1           0               0               0               1           White\n7    PACINO         AL          E       0               0           0               0               0               1           White\n8    WILLIAMS       ROBIN       E       0               0           1               0               0               0           White\n9    EASTWOOD       CLINT       E       0               0           0               0               0               1           White\n']
448;5.0;0;26977076;;1;35;<python><pandas><dataframe><unique>;pandas unique values multiple columns;"<pre><code>df = pd.DataFrame({'Col1': ['Bob', 'Joe', 'Bill', 'Mary', 'Joe'],
                   'Col2': ['Joe', 'Steve', 'Bob', 'Bob', 'Steve'],
                   'Col3': np.random.random(5)})
</code></pre>

<p>What is the best way to return the unique values of 'Col1' and 'Col2'?</p>

<p>The desired output is </p>

<pre><code>'Bob', 'Joe', 'Bill', 'Mary', 'Steve'
</code></pre>
";43537.0;"[""df = pd.DataFrame({'Col1': ['Bob', 'Joe', 'Bill', 'Mary', 'Joe'],\n                   'Col2': ['Joe', 'Steve', 'Bob', 'Bob', 'Steve'],\n                   'Col3': np.random.random(5)})\n"", ""'Bob', 'Joe', 'Bill', 'Mary', 'Steve'\n""]";"[""df = pd.DataFrame({'Col1': ['Bob', 'Joe', 'Bill', 'Mary', 'Joe'],\n                   'Col2': ['Joe', 'Steve', 'Bob', 'Bob', 'Steve'],\n                   'Col3': np.random.random(5)})\n"", ""'Bob', 'Joe', 'Bill', 'Mary', 'Steve'\n""]"
449;2.0;4;27236275;;1;56;<python><pandas>;What does `ValueError: cannot reindex from a duplicate axis` mean?;"<p>I am getting a <code>ValueError: cannot reindex from a duplicate axis</code> when I am trying to set an index to a certain value. I tried to reproduce this with a simple example, but I could not do it.</p>

<p>Here is my session inside of <code>ipdb</code> trace. I have a DataFrame with string index, and integer columns, float values. However when I try to create <code>sum</code> index for sum of all columns I am getting <code>ValueError: cannot reindex from a duplicate axis</code> error. I created a small DataFrame with the same characteristics, but was not able to reproduce the problem, what could I be missing?</p>

<p>I don't really understand what <code>ValueError: cannot reindex from a duplicate axis</code>means, what does this error message mean? Maybe this will help me diagnose the problem, and this is most answerable part of my question.</p>

<pre><code>ipdb&gt; type(affinity_matrix)
&lt;class 'pandas.core.frame.DataFrame'&gt;
ipdb&gt; affinity_matrix.shape
(333, 10)
ipdb&gt; affinity_matrix.columns
Int64Index([9315684, 9315597, 9316591, 9320520, 9321163, 9320615, 9321187, 9319487, 9319467, 9320484], dtype='int64')
ipdb&gt; affinity_matrix.index
Index([u'001', u'002', u'003', u'004', u'005', u'008', u'009', u'010', u'011', u'014', u'015', u'016', u'018', u'020', u'021', u'022', u'024', u'025', u'026', u'027', u'028', u'029', u'030', u'032', u'033', u'034', u'035', u'036', u'039', u'040', u'041', u'042', u'043', u'044', u'045', u'047', u'047', u'048', u'050', u'053', u'054', u'055', u'056', u'057', u'058', u'059', u'060', u'061', u'062', u'063', u'065', u'067', u'068', u'069', u'070', u'071', u'072', u'073', u'074', u'075', u'076', u'077', u'078', u'080', u'082', u'083', u'084', u'085', u'086', u'089', u'090', u'091', u'092', u'093', u'094', u'095', u'096', u'097', u'098', u'100', u'101', u'103', u'104', u'105', u'106', u'107', u'108', u'109', u'110', u'111', u'112', u'113', u'114', u'115', u'116', u'117', u'118', u'119', u'121', u'122', ...], dtype='object')

ipdb&gt; affinity_matrix.values.dtype
dtype('float64')
ipdb&gt; 'sums' in affinity_matrix.index
False
</code></pre>

<p>Here is the error:</p>

<pre><code>ipdb&gt; affinity_matrix.loc['sums'] = affinity_matrix.sum(axis=0)
*** ValueError: cannot reindex from a duplicate axis
</code></pre>

<p>I tried to reproduce this with a simple example, but I failed</p>

<pre><code>In [32]: import pandas as pd

In [33]: import numpy as np

In [34]: a = np.arange(35).reshape(5,7)

In [35]: df = pd.DataFrame(a, ['x', 'y', 'u', 'z', 'w'], range(10, 17))

In [36]: df.values.dtype
Out[36]: dtype('int64')

In [37]: df.loc['sums'] = df.sum(axis=0)

In [38]: df
Out[38]: 
      10  11  12  13  14  15   16
x      0   1   2   3   4   5    6
y      7   8   9  10  11  12   13
u     14  15  16  17  18  19   20
z     21  22  23  24  25  26   27
w     28  29  30  31  32  33   34
sums  70  75  80  85  90  95  100
</code></pre>
";48018.0;"[""ipdb> type(affinity_matrix)\n<class 'pandas.core.frame.DataFrame'>\nipdb> affinity_matrix.shape\n(333, 10)\nipdb> affinity_matrix.columns\nInt64Index([9315684, 9315597, 9316591, 9320520, 9321163, 9320615, 9321187, 9319487, 9319467, 9320484], dtype='int64')\nipdb> affinity_matrix.index\nIndex([u'001', u'002', u'003', u'004', u'005', u'008', u'009', u'010', u'011', u'014', u'015', u'016', u'018', u'020', u'021', u'022', u'024', u'025', u'026', u'027', u'028', u'029', u'030', u'032', u'033', u'034', u'035', u'036', u'039', u'040', u'041', u'042', u'043', u'044', u'045', u'047', u'047', u'048', u'050', u'053', u'054', u'055', u'056', u'057', u'058', u'059', u'060', u'061', u'062', u'063', u'065', u'067', u'068', u'069', u'070', u'071', u'072', u'073', u'074', u'075', u'076', u'077', u'078', u'080', u'082', u'083', u'084', u'085', u'086', u'089', u'090', u'091', u'092', u'093', u'094', u'095', u'096', u'097', u'098', u'100', u'101', u'103', u'104', u'105', u'106', u'107', u'108', u'109', u'110', u'111', u'112', u'113', u'114', u'115', u'116', u'117', u'118', u'119', u'121', u'122', ...], dtype='object')\n\nipdb> affinity_matrix.values.dtype\ndtype('float64')\nipdb> 'sums' in affinity_matrix.index\nFalse\n"", ""ipdb> affinity_matrix.loc['sums'] = affinity_matrix.sum(axis=0)\n*** ValueError: cannot reindex from a duplicate axis\n"", ""In [32]: import pandas as pd\n\nIn [33]: import numpy as np\n\nIn [34]: a = np.arange(35).reshape(5,7)\n\nIn [35]: df = pd.DataFrame(a, ['x', 'y', 'u', 'z', 'w'], range(10, 17))\n\nIn [36]: df.values.dtype\nOut[36]: dtype('int64')\n\nIn [37]: df.loc['sums'] = df.sum(axis=0)\n\nIn [38]: df\nOut[38]: \n      10  11  12  13  14  15   16\nx      0   1   2   3   4   5    6\ny      7   8   9  10  11  12   13\nu     14  15  16  17  18  19   20\nz     21  22  23  24  25  26   27\nw     28  29  30  31  32  33   34\nsums  70  75  80  85  90  95  100\n""]";"['ValueError: cannot reindex from a duplicate axis', 'ipdb', 'sum', 'ValueError: cannot reindex from a duplicate axis', 'ValueError: cannot reindex from a duplicate axis', ""ipdb> type(affinity_matrix)\n<class 'pandas.core.frame.DataFrame'>\nipdb> affinity_matrix.shape\n(333, 10)\nipdb> affinity_matrix.columns\nInt64Index([9315684, 9315597, 9316591, 9320520, 9321163, 9320615, 9321187, 9319487, 9319467, 9320484], dtype='int64')\nipdb> affinity_matrix.index\nIndex([u'001', u'002', u'003', u'004', u'005', u'008', u'009', u'010', u'011', u'014', u'015', u'016', u'018', u'020', u'021', u'022', u'024', u'025', u'026', u'027', u'028', u'029', u'030', u'032', u'033', u'034', u'035', u'036', u'039', u'040', u'041', u'042', u'043', u'044', u'045', u'047', u'047', u'048', u'050', u'053', u'054', u'055', u'056', u'057', u'058', u'059', u'060', u'061', u'062', u'063', u'065', u'067', u'068', u'069', u'070', u'071', u'072', u'073', u'074', u'075', u'076', u'077', u'078', u'080', u'082', u'083', u'084', u'085', u'086', u'089', u'090', u'091', u'092', u'093', u'094', u'095', u'096', u'097', u'098', u'100', u'101', u'103', u'104', u'105', u'106', u'107', u'108', u'109', u'110', u'111', u'112', u'113', u'114', u'115', u'116', u'117', u'118', u'119', u'121', u'122', ...], dtype='object')\n\nipdb> affinity_matrix.values.dtype\ndtype('float64')\nipdb> 'sums' in affinity_matrix.index\nFalse\n"", ""ipdb> affinity_matrix.loc['sums'] = affinity_matrix.sum(axis=0)\n*** ValueError: cannot reindex from a duplicate axis\n"", ""In [32]: import pandas as pd\n\nIn [33]: import numpy as np\n\nIn [34]: a = np.arange(35).reshape(5,7)\n\nIn [35]: df = pd.DataFrame(a, ['x', 'y', 'u', 'z', 'w'], range(10, 17))\n\nIn [36]: df.values.dtype\nOut[36]: dtype('int64')\n\nIn [37]: df.loc['sums'] = df.sum(axis=0)\n\nIn [38]: df\nOut[38]: \n      10  11  12  13  14  15   16\nx      0   1   2   3   4   5    6\ny      7   8   9  10  11  12   13\nu     14  15  16  17  18  19   20\nz     21  22  23  24  25  26   27\nw     28  29  30  31  32  33   34\nsums  70  75  80  85  90  95  100\n""]"
450;3.0;0;27263805;;1;31;<python><pandas>;pandas: When cell contents are lists, create a row for each element in the list;"<p>I have a dataframe where some cells contain lists of multiple values. Rather than storing multiple
values in a cell, I'd like to expand the dataframe so that each item in the list gets its own row (with the same values in all other columns). So if I have:</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(
    {'trial_num': [1, 2, 3, 1, 2, 3],
     'subject': [1, 1, 1, 2, 2, 2],
     'samples': [list(np.random.randn(3).round(2)) for i in range(6)]
    }
)

df
Out[10]: 
                 samples  subject  trial_num
0    [0.57, -0.83, 1.44]        1          1
1    [-0.01, 1.13, 0.36]        1          2
2   [1.18, -1.46, -0.94]        1          3
3  [-0.08, -4.22, -2.05]        2          1
4     [0.72, 0.79, 0.53]        2          2
5    [0.4, -0.32, -0.13]        2          3
</code></pre>

<p>How do I convert to long form, e.g.:</p>

<pre><code>   subject  trial_num  sample  sample_num
0        1          1    0.57           0
1        1          1   -0.83           1
2        1          1    1.44           2
3        1          2   -0.01           0
4        1          2    1.13           1
5        1          2    0.36           2
6        1          3    1.18           0
# etc.
</code></pre>

<p>The index is not important, it's OK to set existing
columns as the index and the final ordering isn't
important.</p>
";8222.0;"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {'trial_num': [1, 2, 3, 1, 2, 3],\n     'subject': [1, 1, 1, 2, 2, 2],\n     'samples': [list(np.random.randn(3).round(2)) for i in range(6)]\n    }\n)\n\ndf\nOut[10]: \n                 samples  subject  trial_num\n0    [0.57, -0.83, 1.44]        1          1\n1    [-0.01, 1.13, 0.36]        1          2\n2   [1.18, -1.46, -0.94]        1          3\n3  [-0.08, -4.22, -2.05]        2          1\n4     [0.72, 0.79, 0.53]        2          2\n5    [0.4, -0.32, -0.13]        2          3\n"", '   subject  trial_num  sample  sample_num\n0        1          1    0.57           0\n1        1          1   -0.83           1\n2        1          1    1.44           2\n3        1          2   -0.01           0\n4        1          2    1.13           1\n5        1          2    0.36           2\n6        1          3    1.18           0\n# etc.\n']";"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {'trial_num': [1, 2, 3, 1, 2, 3],\n     'subject': [1, 1, 1, 2, 2, 2],\n     'samples': [list(np.random.randn(3).round(2)) for i in range(6)]\n    }\n)\n\ndf\nOut[10]: \n                 samples  subject  trial_num\n0    [0.57, -0.83, 1.44]        1          1\n1    [-0.01, 1.13, 0.36]        1          2\n2   [1.18, -1.46, -0.94]        1          3\n3  [-0.08, -4.22, -2.05]        2          1\n4     [0.72, 0.79, 0.53]        2          2\n5    [0.4, -0.32, -0.13]        2          3\n"", '   subject  trial_num  sample  sample_num\n0        1          1    0.57           0\n1        1          1   -0.83           1\n2        1          1    1.44           2\n3        1          2   -0.01           0\n4        1          2    1.13           1\n5        1          2    0.36           2\n6        1          3    1.18           0\n# etc.\n']"
451;4.0;1;27365467;;1;32;<python><pandas><matplotlib><time-series>;python pandas: plot histogram of dates?;"<p>I've taken my Series and coerced it to a datetime column of dtype=<code>datetime64[ns]</code> (though only need day resolution...not sure how to change). </p>

<pre><code>import pandas as pd
df = pd.read_csv('somefile.csv')
column = df['date']
column = pd.to_datetime(column, coerce=True)
</code></pre>

<p>but plotting doesn't work:</p>

<pre><code>ipdb&gt; column.plot(kind='hist')
*** TypeError: ufunc add cannot use operands with types dtype('&lt;M8[ns]') and dtype('float64')
</code></pre>

<p>I'd like to plot a histogram that just <strong>shows the count of dates by week, month, or year</strong>.</p>

<p>Surely there is a way to do this in <code>pandas</code>?</p>
";17415.0;"[""import pandas as pd\ndf = pd.read_csv('somefile.csv')\ncolumn = df['date']\ncolumn = pd.to_datetime(column, coerce=True)\n"", ""ipdb> column.plot(kind='hist')\n*** TypeError: ufunc add cannot use operands with types dtype('<M8[ns]') and dtype('float64')\n""]";"['datetime64[ns]', ""import pandas as pd\ndf = pd.read_csv('somefile.csv')\ncolumn = df['date']\ncolumn = pd.to_datetime(column, coerce=True)\n"", ""ipdb> column.plot(kind='hist')\n*** TypeError: ufunc add cannot use operands with types dtype('<M8[ns]') and dtype('float64')\n"", 'pandas']"
452;2.0;7;27405483;;1;29;<python><pandas>;How to loop over grouped Pandas dataframe?;"<p>DataFrame:</p>

<pre><code>  c_os_family_ss c_os_major_is l_customer_id_i
0      Windows 7                         90418
1      Windows 7                         90418
2      Windows 7                         90418
</code></pre>

<p>Code:</p>

<pre><code>print df
for name, group in df.groupby('l_customer_id_i').agg(lambda x: ','.join(x)):
    print name
    print group
</code></pre>

<p>I'm trying to just loop over the aggregated data, but I get the error:</p>

<blockquote>
  <p>ValueError: too many values to unpack</p>
</blockquote>

<p>@EdChum, here's the expected output:</p>

<pre><code>                                                    c_os_family_ss  \
l_customer_id_i
131572           Windows 7,Windows 7,Windows 7,Windows 7,Window...
135467           Windows 7,Windows 7,Windows 7,Windows 7,Window...

                                                     c_os_major_is
l_customer_id_i
131572           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...
135467           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...
</code></pre>

<p>The output is not the problem, I wish to loop over every group.</p>
";27652.0;"['  c_os_family_ss c_os_major_is l_customer_id_i\n0      Windows 7                         90418\n1      Windows 7                         90418\n2      Windows 7                         90418\n', ""print df\nfor name, group in df.groupby('l_customer_id_i').agg(lambda x: ','.join(x)):\n    print name\n    print group\n"", '                                                    c_os_family_ss  \\\nl_customer_id_i\n131572           Windows 7,Windows 7,Windows 7,Windows 7,Window...\n135467           Windows 7,Windows 7,Windows 7,Windows 7,Window...\n\n                                                     c_os_major_is\nl_customer_id_i\n131572           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...\n135467           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...\n']";"['  c_os_family_ss c_os_major_is l_customer_id_i\n0      Windows 7                         90418\n1      Windows 7                         90418\n2      Windows 7                         90418\n', ""print df\nfor name, group in df.groupby('l_customer_id_i').agg(lambda x: ','.join(x)):\n    print name\n    print group\n"", '                                                    c_os_family_ss  \\\nl_customer_id_i\n131572           Windows 7,Windows 7,Windows 7,Windows 7,Window...\n135467           Windows 7,Windows 7,Windows 7,Windows 7,Window...\n\n                                                     c_os_major_is\nl_customer_id_i\n131572           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...\n135467           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...\n']"
453;1.0;2;27517425;;1;38;<python><pandas>;Apply vs transform on a group object;"<p>Consider the following dataframe:</p>

<pre><code>     A      B         C         D
0  foo    one  0.162003  0.087469
1  bar    one -1.156319 -1.526272
2  foo    two  0.833892 -1.666304
3  bar  three -2.026673 -0.322057
4  foo    two  0.411452 -0.954371
5  bar    two  0.765878 -0.095968
6  foo    one -0.654890  0.678091
7  foo  three -1.789842 -1.130922
</code></pre>

<p>The following commands work:</p>

<pre><code>&gt; df.groupby('A').apply(lambda x: (x['C'] - x['D']))
&gt; df.groupby('A').apply(lambda x: (x['C'] - x['D']).mean())
</code></pre>

<p>but none of the following work:</p>

<pre><code>&gt; df.groupby('A').transform(lambda x: (x['C'] - x['D']))
ValueError: could not broadcast input array from shape (5) into shape (5,3)

&gt; df.groupby('A').transform(lambda x: (x['C'] - x['D']).mean())
 TypeError: cannot concatenate a non-NDFrame object
</code></pre>

<p><strong>Why?</strong> <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#transformation"" rel=""noreferrer"">The example on the documentation</a> seems to suggest that calling <code>transform</code> on a group allows one to do row-wise operation processing:</p>

<pre><code># Note that the following suggests row-wise operation (x.mean is the column mean)
zscore = lambda x: (x - x.mean()) / x.std()
transformed = ts.groupby(key).transform(zscore)
</code></pre>

<p>In other words, I thought that transform is essentially a specific type of apply (the one that does not aggregate). Where am I wrong?</p>

<p>For reference, below is the construction of the original dataframe above:</p>

<pre><code>df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',
                          'foo', 'bar', 'foo', 'foo'],
                   'B' : ['one', 'one', 'two', 'three',
                         'two', 'two', 'one', 'three'],
                   'C' : randn(8), 'D' : randn(8)})
</code></pre>
";19045.0;"['     A      B         C         D\n0  foo    one  0.162003  0.087469\n1  bar    one -1.156319 -1.526272\n2  foo    two  0.833892 -1.666304\n3  bar  three -2.026673 -0.322057\n4  foo    two  0.411452 -0.954371\n5  bar    two  0.765878 -0.095968\n6  foo    one -0.654890  0.678091\n7  foo  three -1.789842 -1.130922\n', ""> df.groupby('A').apply(lambda x: (x['C'] - x['D']))\n> df.groupby('A').apply(lambda x: (x['C'] - x['D']).mean())\n"", ""> df.groupby('A').transform(lambda x: (x['C'] - x['D']))\nValueError: could not broadcast input array from shape (5) into shape (5,3)\n\n> df.groupby('A').transform(lambda x: (x['C'] - x['D']).mean())\n TypeError: cannot concatenate a non-NDFrame object\n"", '# Note that the following suggests row-wise operation (x.mean is the column mean)\nzscore = lambda x: (x - x.mean()) / x.std()\ntransformed = ts.groupby(key).transform(zscore)\n', ""df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n                          'foo', 'bar', 'foo', 'foo'],\n                   'B' : ['one', 'one', 'two', 'three',\n                         'two', 'two', 'one', 'three'],\n                   'C' : randn(8), 'D' : randn(8)})\n""]";"['     A      B         C         D\n0  foo    one  0.162003  0.087469\n1  bar    one -1.156319 -1.526272\n2  foo    two  0.833892 -1.666304\n3  bar  three -2.026673 -0.322057\n4  foo    two  0.411452 -0.954371\n5  bar    two  0.765878 -0.095968\n6  foo    one -0.654890  0.678091\n7  foo  three -1.789842 -1.130922\n', ""> df.groupby('A').apply(lambda x: (x['C'] - x['D']))\n> df.groupby('A').apply(lambda x: (x['C'] - x['D']).mean())\n"", ""> df.groupby('A').transform(lambda x: (x['C'] - x['D']))\nValueError: could not broadcast input array from shape (5) into shape (5,3)\n\n> df.groupby('A').transform(lambda x: (x['C'] - x['D']).mean())\n TypeError: cannot concatenate a non-NDFrame object\n"", 'transform', '# Note that the following suggests row-wise operation (x.mean is the column mean)\nzscore = lambda x: (x - x.mean()) / x.std()\ntransformed = ts.groupby(key).transform(zscore)\n', ""df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n                          'foo', 'bar', 'foo', 'foo'],\n                   'B' : ['one', 'one', 'two', 'three',\n                         'two', 'two', 'one', 'three'],\n                   'C' : randn(8), 'D' : randn(8)})\n""]"
454;1.0;1;27667759;;1;53;<python><r><pandas>;Is .ix() always better than .loc() and .iloc() since it is faster and supports integer and label access?;"<p>I'm learning the Python pandas library. Coming from an R background, the indexing and selecting functions seem more complicated than they need to be. My understanding it that .loc() is only label based and .iloc() is only integer based. </p>

<p><strong>Why should I ever use .loc() and .iloc() if .ix() is faster and supports integer and label access?</strong> </p>
";38433.0;[];[]
455;3.0;2;27673231;;1;36;<pandas>;why should I make a copy of a data frame in pandas;"<p>When you are selecting a sub dataframe from a parent dataframe. I notice some programmers are making a copy of the data frame using .copy() method. Why are they making a copy of the data frame? What will happen if I dont make a copy??</p>
";28042.0;[];[]
456;3.0;0;27842613;;1;43;<python><sorting><pandas><group-by>;pandas groupby sort within groups;"<p>I want to group my dataframe by two columns and then sort the aggregated results within the groups.</p>

<pre><code>In [167]:
df

Out[167]:
count   job source
0   2   sales   A
1   4   sales   B
2   6   sales   C
3   3   sales   D
4   7   sales   E
5   5   market  A
6   3   market  B
7   2   market  C
8   4   market  D
9   1   market  E

In [168]:
df.groupby(['job','source']).agg({'count':sum})

Out[168]:
            count
job     source  
market  A   5
        B   3
        C   2
        D   4
        E   1
sales   A   2
        B   4
        C   6
        D   3
        E   7
</code></pre>

<p>I would now like to sort the count column in descending order within each of the groups. And then take only the top three rows. To get something like:</p>

<pre><code>            count
job     source  
market  A   5
        D   4
        B   3
sales   E   7
        C   6
        B   4
</code></pre>
";37771.0;"[""In [167]:\ndf\n\nOut[167]:\ncount   job source\n0   2   sales   A\n1   4   sales   B\n2   6   sales   C\n3   3   sales   D\n4   7   sales   E\n5   5   market  A\n6   3   market  B\n7   2   market  C\n8   4   market  D\n9   1   market  E\n\nIn [168]:\ndf.groupby(['job','source']).agg({'count':sum})\n\nOut[168]:\n            count\njob     source  \nmarket  A   5\n        B   3\n        C   2\n        D   4\n        E   1\nsales   A   2\n        B   4\n        C   6\n        D   3\n        E   7\n"", '            count\njob     source  \nmarket  A   5\n        D   4\n        B   3\nsales   E   7\n        C   6\n        B   4\n']";"[""In [167]:\ndf\n\nOut[167]:\ncount   job source\n0   2   sales   A\n1   4   sales   B\n2   6   sales   C\n3   3   sales   D\n4   7   sales   E\n5   5   market  A\n6   3   market  B\n7   2   market  C\n8   4   market  D\n9   1   market  E\n\nIn [168]:\ndf.groupby(['job','source']).agg({'count':sum})\n\nOut[168]:\n            count\njob     source  \nmarket  A   5\n        B   3\n        C   2\n        D   4\n        E   1\nsales   A   2\n        B   4\n        C   6\n        D   3\n        E   7\n"", '            count\njob     source  \nmarket  A   5\n        D   4\n        B   3\nsales   E   7\n        C   6\n        B   4\n']"
457;5.0;0;27905295;;1;23;<python><python-3.x><pandas><dataframe><nan>;How to replace NaNs by preceding values in pandas DataFrame?;"<p>Suppose I have a DataFrame with some <code>NaN</code>s:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])
&gt;&gt;&gt; df
    0   1   2
0   1   2   3
1   4 NaN NaN
2 NaN NaN   9
</code></pre>

<p>What I need to do is replace every <code>NaN</code> with the first non-<code>NaN</code> value in the same column above it. It is assumed that the first row will never contain a <code>NaN</code>. So for the previous example the result would be</p>

<pre><code>   0  1  2
0  1  2  3
1  4  2  3
2  4  2  9
</code></pre>

<p>I can just loop through the whole DataFrame column-by-column, element-by-element and set the values directly, but is there an easy (optimally a loop-free) way of achieving this?</p>
";14351.0;['>>> import pandas as pd\n>>> df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])\n>>> df\n    0   1   2\n0   1   2   3\n1   4 NaN NaN\n2 NaN NaN   9\n', '   0  1  2\n0  1  2  3\n1  4  2  3\n2  4  2  9\n'];['NaN', '>>> import pandas as pd\n>>> df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])\n>>> df\n    0   1   2\n0   1   2   3\n1   4 NaN NaN\n2 NaN NaN   9\n', 'NaN', 'NaN', 'NaN', '   0  1  2\n0  1  2  3\n1  4  2  3\n2  4  2  9\n']
458;4.0;1;27975069;;1;26;<python><pandas>;How to filter rows containing a string pattern from a Pandas dataframe;"<p>Assume we have a data frame in Python Pandas that looks like this:</p>

<pre><code>df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': [u'aball', u'bball', u'cnut', u'fball']})
</code></pre>

<p>Or, in table form:</p>

<pre class=""lang-none prettyprint-override""><code>ids    vals
aball   1
bball   2
cnut    3
fball   4
</code></pre>

<p>How do I filter rows which contain the key word ""ball?"" For example, the output should be:</p>

<pre class=""lang-none prettyprint-override""><code>ids    vals
aball   1
bball   2
fball   4
</code></pre>
";35532.0;"[""df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': [u'aball', u'bball', u'cnut', u'fball']})\n"", 'ids    vals\naball   1\nbball   2\ncnut    3\nfball   4\n', 'ids    vals\naball   1\nbball   2\nfball   4\n']";"[""df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': [u'aball', u'bball', u'cnut', u'fball']})\n"", 'ids    vals\naball   1\nbball   2\ncnut    3\nfball   4\n', 'ids    vals\naball   1\nbball   2\nfball   4\n']"
459;2.0;1;28006793;;1;27;<python><pandas>;Pandas DataFrame to List of Lists;"<p>It's easy to turn a list of lists into a pandas dataframe:</p>

<pre><code>import pandas as pd
df = pd.DataFrame([[1,2,3],[3,4,5]])
</code></pre>

<p>But how do I turn df back into a list of lists?</p>

<pre><code>lol = df.what_to_do_now?
print lol
# [[1,2,3],[3,4,5]]
</code></pre>
";27158.0;['import pandas as pd\ndf = pd.DataFrame([[1,2,3],[3,4,5]])\n', 'lol = df.what_to_do_now?\nprint lol\n# [[1,2,3],[3,4,5]]\n'];['import pandas as pd\ndf = pd.DataFrame([[1,2,3],[3,4,5]])\n', 'lol = df.what_to_do_now?\nprint lol\n# [[1,2,3],[3,4,5]]\n']
460;7.0;1;28218698;;1;42;<python><pandas><statsmodels>;How to iterate over columns of pandas dataframe to run regression;"<p>I'm sure this is simple, but as a complete newbie to python, I'm having trouble figuring out how to iterate over variables in a <code>pandas</code> dataframe and run a regression with each.</p>

<p>Here's what I'm doing:</p>

<pre><code>all_data = {}
for ticker in ['FIUIX', 'FSAIX', 'FSAVX', 'FSTMX']:
    all_data[ticker] = web.get_data_yahoo(ticker, '1/1/2010', '1/1/2015')

prices = DataFrame({tic: data['Adj Close'] for tic, data in all_data.iteritems()})  
returns = prices.pct_change()
</code></pre>

<p>I know I can run a regression like this:</p>

<pre><code>regs = sm.OLS(returns.FIUIX,returns.FSTMX).fit()
</code></pre>

<p>but suppose I want to do this for each column in the dataframe. In particular, I want to regress FIUIX on FSTMX, and then FSAIX on FSTMX, and then FSAVX on FSTMX. After each regression I want to store the residuals.</p>

<p>I've tried various versions of the following, but I must be getting the syntax wrong:</p>

<pre><code>resids = {}
for k in returns.keys():
    reg = sm.OLS(returns[k],returns.FSTMX).fit()
    resids[k] = reg.resid
</code></pre>

<p>I think the problem is I don't know how to refer to the returns column by key, so <code>returns[k]</code> is probably wrong.</p>

<p>Any guidance on the best way to do this would be much appreciated. Perhaps there's a common pandas approach I'm missing.</p>
";71906.0;"[""all_data = {}\nfor ticker in ['FIUIX', 'FSAIX', 'FSAVX', 'FSTMX']:\n    all_data[ticker] = web.get_data_yahoo(ticker, '1/1/2010', '1/1/2015')\n\nprices = DataFrame({tic: data['Adj Close'] for tic, data in all_data.iteritems()})  \nreturns = prices.pct_change()\n"", 'regs = sm.OLS(returns.FIUIX,returns.FSTMX).fit()\n', 'resids = {}\nfor k in returns.keys():\n    reg = sm.OLS(returns[k],returns.FSTMX).fit()\n    resids[k] = reg.resid\n']";"['pandas', ""all_data = {}\nfor ticker in ['FIUIX', 'FSAIX', 'FSAVX', 'FSTMX']:\n    all_data[ticker] = web.get_data_yahoo(ticker, '1/1/2010', '1/1/2015')\n\nprices = DataFrame({tic: data['Adj Close'] for tic, data in all_data.iteritems()})  \nreturns = prices.pct_change()\n"", 'regs = sm.OLS(returns.FIUIX,returns.FSTMX).fit()\n', 'resids = {}\nfor k in returns.keys():\n    reg = sm.OLS(returns[k],returns.FSTMX).fit()\n    resids[k] = reg.resid\n', 'returns[k]']"
461;1.0;1;28236305;;1;21;<python><pandas><dataframe><data-analysis>;How do I sum values in a column that match a given condition using pandas?;"<p>Suppose I have a column like so:</p>

<pre><code>a   b  
1   5   
1   7
2   3
1   3
2   5
</code></pre>

<p>I want to sum up the values for <code>b</code> where <code>a = 1</code>, for example. This would give me <code>5 + 7 + 3 = 15</code>.</p>

<p>How do I do this in pandas?</p>
";27037.0;['a   b  \n1   5   \n1   7\n2   3\n1   3\n2   5\n'];['a   b  \n1   5   \n1   7\n2   3\n1   3\n2   5\n', 'b', 'a = 1', '5 + 7 + 3 = 15']
462;1.0;0;28311655;;1;24;<pandas>;Ignoring NaNs with str.contains;"<p>I want to find rows that contain a string, like so:</p>

<pre><code>DF[DF.col.str.contains(""foo"")]
</code></pre>

<p>However, this fails because some elements are NaN:</p>

<blockquote>
  <p>ValueError: cannot index with vector containing NA / NaN values</p>
</blockquote>

<p>So I resort to the obfuscated</p>

<pre><code>DF[DF.col.notnull()][DF.col.dropna().str.contains(""foo"")]
</code></pre>

<p>Is there a better way?</p>
";6219.0;"['DF[DF.col.str.contains(""foo"")]\n', 'DF[DF.col.notnull()][DF.col.dropna().str.contains(""foo"")]\n']";"['DF[DF.col.str.contains(""foo"")]\n', 'DF[DF.col.notnull()][DF.col.dropna().str.contains(""foo"")]\n']"
463;3.0;0;28679930;;1;23;<python><pandas>;How to drop rows from pandas data frame that contains a particular string in a particular column?;"<p>I have a very large data frame in python and I want to drop all rows that have a particular string inside a particular column.</p>

<p>For example, I want to drop all rows which have the string ""XYZ"" as a substring in the column C of the data frame.</p>

<p>Can this be implemented in an efficient way using .drop() method?</p>
";20370.0;[];[]
464;3.0;5;28757389;;1;71;<python><pandas>;Loc vs. iloc vs. ix vs. at vs. iat?;"<p>Recently began branching out from my safe place (R) into Python and and am a bit confused by the cell localization/selection in <code>Pandas</code>. I've read the documentation but I'm struggling to understand the practical implications of the various localization/selection options. </p>

<p>Is there a reason why I should ever use <code>.loc</code> or <code>.iloc</code> over the most general option <code>.ix</code>? </p>

<p>I understand that <code>.loc</code>, <code>iloc</code>, <code>at</code>, and <code>iat</code> may provide some guaranteed correctness that <code>.ix</code> can't offer, but I've also read where <code>.ix</code> tends to be the fastest solution across the board. </p>

<p>Can someone please explain the real world, best practices reasoning behind utilizing anything other than <code>.ix</code>?</p>
";31094.0;[];['Pandas', '.loc', '.iloc', '.ix', '.loc', 'iloc', 'at', 'iat', '.ix', '.ix', '.ix']
465;8.0;0;28901683;;1;44;<python><pandas>;pandas get rows which are NOT in other dataframe;"<p>I've two pandas data frames which have some rows in common.</p>

<p>Suppose dataframe2 is a subset of dataframe1.</p>

<p><strong>How can I get the rows of dataframe1 which are not in dataframe2?</strong></p>

<pre><code>df1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]}) 
df2 = pandas.DataFrame(data = {'col1' : [1, 2, 3], 'col2' : [10, 11, 12]})
</code></pre>
";26226.0;"[""df1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]}) \ndf2 = pandas.DataFrame(data = {'col1' : [1, 2, 3], 'col2' : [10, 11, 12]})\n""]";"[""df1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]}) \ndf2 = pandas.DataFrame(data = {'col1' : [1, 2, 3], 'col2' : [10, 11, 12]})\n""]"
466;1.0;0;28931224;;1;28;<python><python-2.7><pandas><matplotlib><data-visualization>;Adding value labels on a matplotlib bar chart;"<p>I got stuck on something that feels like should be relatively easy. The code I bring below is a sample based on a larger project I'm working on. I saw no reason to post all the details, so please accept the data structures I bring as is.</p>

<p>Basically, I'm creating a bar chart, and I just can figure out how to add value labels on the bars (in the center of the bar, or just above it). Been looking at samples around the web but with no success implementing on my own code. I believe the solution is either with 'text' or 'annotate', but I:
a) don't know which one to use (and generally speaking, haven't figured out when to use which).
b) can't see to get either to present the value labels.
Would appreciate your help, my code below.
Thanks in advance!</p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
pd.set_option('display.mpl_style', 'default') 
%matplotlib inline


frequencies = [6, 16, 75, 160, 244, 260, 145, 73, 16, 4, 1]   # bring some raw data

freq_series = pd.Series.from_array(frequencies)   # in my original code I create a series and run on that, so for consistency I create a series from the list.

x_labels = [108300.0, 110540.0, 112780.0, 115020.0, 117260.0, 119500.0, 121740.0, 123980.0, 126220.0, 128460.0, 130700.0]

# now to plot the figure...
plt.figure(figsize=(12, 8))
fig = freq_series.plot(kind='bar')
fig.set_title(""Amount Frequency"")
fig.set_xlabel(""Amount ($)"")
fig.set_ylabel(""Frequency"")
fig.set_xticklabels(x_labels)
</code></pre>
";35505.0;"['import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option(\'display.mpl_style\', \'default\') \n%matplotlib inline\n\n\nfrequencies = [6, 16, 75, 160, 244, 260, 145, 73, 16, 4, 1]   # bring some raw data\n\nfreq_series = pd.Series.from_array(frequencies)   # in my original code I create a series and run on that, so for consistency I create a series from the list.\n\nx_labels = [108300.0, 110540.0, 112780.0, 115020.0, 117260.0, 119500.0, 121740.0, 123980.0, 126220.0, 128460.0, 130700.0]\n\n# now to plot the figure...\nplt.figure(figsize=(12, 8))\nfig = freq_series.plot(kind=\'bar\')\nfig.set_title(""Amount Frequency"")\nfig.set_xlabel(""Amount ($)"")\nfig.set_ylabel(""Frequency"")\nfig.set_xticklabels(x_labels)\n']";"['import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option(\'display.mpl_style\', \'default\') \n%matplotlib inline\n\n\nfrequencies = [6, 16, 75, 160, 244, 260, 145, 73, 16, 4, 1]   # bring some raw data\n\nfreq_series = pd.Series.from_array(frequencies)   # in my original code I create a series and run on that, so for consistency I create a series from the list.\n\nx_labels = [108300.0, 110540.0, 112780.0, 115020.0, 117260.0, 119500.0, 121740.0, 123980.0, 126220.0, 128460.0, 130700.0]\n\n# now to plot the figure...\nplt.figure(figsize=(12, 8))\nfig = freq_series.plot(kind=\'bar\')\nfig.set_title(""Amount Frequency"")\nfig.set_xlabel(""Amount ($)"")\nfig.set_ylabel(""Frequency"")\nfig.set_xticklabels(x_labels)\n']"
467;2.0;0;29177498;;1;28;<python-2.7><pandas><dataframe><nan>;Python Pandas replace NaN in one column with value from corresponding row of second column;"<p>I am working with this Pandas DataFrame in Python 2.7.</p>

<pre><code>File    heat    Farheit Temp_Rating
   1    YesQ    75      N/A
   1    NoR     115     N/A
   1    YesA    63      N/A
   1    NoT     83      41
   1    NoY     100     80
   1    YesZ    56      12
   2    YesQ    111     N/A
   2    NoR     60      N/A
   2    YesA    19      N/A
   2    NoT     106     77
   2    NoY     45      21
   2    YesZ    40      54
   3    YesQ    84      N/A
   3    NoR     67      N/A
   3    YesA    94      N/A
   3    NoT     68      39
   3    NoY     63      46
   3    YesZ    34      81
</code></pre>

<p>I need to replace all NaNs in the <code>Temp_Rating</code> column with the value from the <code>Farheit</code> column.</p>

<p>This is what I need:</p>

<pre><code>File        heat    Observation
   1        YesQ    75
   1        NoR     115
   1        YesA    63
   1        YesQ    41
   1        NoR     80
   1        YesA    12
   2        YesQ    111
   2        NoR     60
   2        YesA    19
   2        NoT     77
   2        NoY     21
   2        YesZ    54
   3        YesQ    84
   3        NoR     67
   3        YesA    94
   3        NoT     39
   3        NoY     46
   3        YesZ    81
</code></pre>

<p>If I do a Boolean selection, I can pick out only one of these columns at a time. The problem is if I then try to join them, I am not able to do this while preserving the correct order.</p>

<p>How can I only find <code>Temp_Rating</code> rows with the <code>NaN</code>s and replace them with the value in the same row of the <code>Farheit</code> column?</p>
";14402.0;['File    heat    Farheit Temp_Rating\n   1    YesQ    75      N/A\n   1    NoR     115     N/A\n   1    YesA    63      N/A\n   1    NoT     83      41\n   1    NoY     100     80\n   1    YesZ    56      12\n   2    YesQ    111     N/A\n   2    NoR     60      N/A\n   2    YesA    19      N/A\n   2    NoT     106     77\n   2    NoY     45      21\n   2    YesZ    40      54\n   3    YesQ    84      N/A\n   3    NoR     67      N/A\n   3    YesA    94      N/A\n   3    NoT     68      39\n   3    NoY     63      46\n   3    YesZ    34      81\n', 'File        heat    Observation\n   1        YesQ    75\n   1        NoR     115\n   1        YesA    63\n   1        YesQ    41\n   1        NoR     80\n   1        YesA    12\n   2        YesQ    111\n   2        NoR     60\n   2        YesA    19\n   2        NoT     77\n   2        NoY     21\n   2        YesZ    54\n   3        YesQ    84\n   3        NoR     67\n   3        YesA    94\n   3        NoT     39\n   3        NoY     46\n   3        YesZ    81\n'];['File    heat    Farheit Temp_Rating\n   1    YesQ    75      N/A\n   1    NoR     115     N/A\n   1    YesA    63      N/A\n   1    NoT     83      41\n   1    NoY     100     80\n   1    YesZ    56      12\n   2    YesQ    111     N/A\n   2    NoR     60      N/A\n   2    YesA    19      N/A\n   2    NoT     106     77\n   2    NoY     45      21\n   2    YesZ    40      54\n   3    YesQ    84      N/A\n   3    NoR     67      N/A\n   3    YesA    94      N/A\n   3    NoT     68      39\n   3    NoY     63      46\n   3    YesZ    34      81\n', 'Temp_Rating', 'Farheit', 'File        heat    Observation\n   1        YesQ    75\n   1        NoR     115\n   1        YesA    63\n   1        YesQ    41\n   1        NoR     80\n   1        YesA    12\n   2        YesQ    111\n   2        NoR     60\n   2        YesA    19\n   2        NoT     77\n   2        NoY     21\n   2        YesZ    54\n   3        YesQ    84\n   3        NoR     67\n   3        YesA    94\n   3        NoT     39\n   3        NoY     46\n   3        YesZ    81\n', 'Temp_Rating', 'NaN', 'Farheit']
468;1.0;0;29226210;;1;21;<python><pandas><apache-spark><pyspark>;What is the Spark DataFrame method `toPandas` actually doing?;"<p>I'm a beginner of Spark-DataFrame API. </p>

<p>I use this code to load csv tab-separated into Spark Dataframe</p>

<pre><code>lines = sc.textFile('tail5.csv')
parts = lines.map(lambda l : l.strip().split('\t'))
fnames = *some name list*
schemaData = StructType([StructField(fname, StringType(), True) for fname in fnames])
ddf = sqlContext.createDataFrame(parts,schemaData)
</code></pre>

<p>Suppose I create DataFrame with Spark from new files, and convert it to pandas using built-in method toPandas(),</p>

<ul>
<li>Does it store the Pandas object to local memory?</li>
<li>Does Pandas low-level computation handled all by Spark?</li>
<li>Does it exposed all pandas dataframe functionality?(I guess yes)</li>
<li>Can I convert it toPandas and just be done with it, without so much touching DataFrame API? </li>
</ul>
";16117.0;"[""lines = sc.textFile('tail5.csv')\nparts = lines.map(lambda l : l.strip().split('\\t'))\nfnames = *some name list*\nschemaData = StructType([StructField(fname, StringType(), True) for fname in fnames])\nddf = sqlContext.createDataFrame(parts,schemaData)\n""]";"[""lines = sc.textFile('tail5.csv')\nparts = lines.map(lambda l : l.strip().split('\\t'))\nfnames = *some name list*\nschemaData = StructType([StructField(fname, StringType(), True) for fname in fnames])\nddf = sqlContext.createDataFrame(parts,schemaData)\n""]"
469;2.0;4;29287224;;1;31;<python><pandas>;Pandas read in table without headers;"<p>How can I read in a .csv file (with no headers) and when I only want a subset of the columns (say 4th and 7th out of a total of 20 columns), using pandas? I cannot seem to be able to do <code>usecols</code></p>
";32704.0;[];['usecols']
470;3.0;0;29370057;;1;42;<python><pandas>;Select dataframe rows between two dates;"<p>I am creating a dataframe from a csv as follows:</p>

<pre><code>stock = pd.read_csv('data_in/' + filename + '.csv', skipinitialspace=True)
</code></pre>

<p>The dataframe has a date column. Is there a way to create a new dataframe (or just overwrite the existing one) which only containes rows that fall between a specific date range?</p>
";55170.0;"[""stock = pd.read_csv('data_in/' + filename + '.csv', skipinitialspace=True)\n""]";"[""stock = pd.read_csv('data_in/' + filename + '.csv', skipinitialspace=True)\n""]"
471;4.0;0;29432629;;1;33;<python><pandas><matplotlib><data-visualization><information-visualization>;Correlation matrix using pandas;"<p>I have a data set with huge number of features, so analysing the correlation matrix has become very difficult. I want to plot a correlation matrix which we get using <code>dataframe.corr()</code> function from pandas library. Is there any built-in function provided by the pandas library to plot this matrix?</p>
";39252.0;[];['dataframe.corr()']
472;3.0;0;29517072;;1;28;<python><pandas>;Add column to dataframe with default value;"<p>I have an existing dataframe which I need to add an additional column to which will contain the same value for every row.</p>

<p>Existing df:</p>

<pre><code>Date, Open, High, Low, Close
01-01-2015, 565, 600, 400, 450
</code></pre>

<p>New df:</p>

<pre><code>Name, Date, Open, High, Low, Close
abc, 01-01-2015, 565, 600, 400, 450
</code></pre>

<p>I know how to append an existing series / dataframe column. But this is a different situation, because all I need is to add the 'Name' column and set every row to the same value, in this case 'abc'.</p>

<p>Im not entirely sure how to do that.</p>
";24041.0;['Date, Open, High, Low, Close\n01-01-2015, 565, 600, 400, 450\n', 'Name, Date, Open, High, Low, Close\nabc, 01-01-2015, 565, 600, 400, 450\n'];['Date, Open, High, Low, Close\n01-01-2015, 565, 600, 400, 450\n', 'Name, Date, Open, High, Low, Close\nabc, 01-01-2015, 565, 600, 400, 450\n']
473;4.0;0;29525808;;1;37;<python><pandas><sqlalchemy><flask-sqlalchemy>;SQLAlchemy ORM conversion to pandas DataFrame;"<p>This topic hasn't been addressed in a while, here or elsewhere. Is there a solution converting a SQLAlchemy <code>&lt;Query object&gt;</code> to a pandas DataFrame?</p>

<p>Pandas has the capability to use <code>pandas.read_sql</code> but this requires use of raw SQL. I have two reasons for wanting to avoid it: 1) I already have everything using the ORM (a good reason in and of itself) and 2) I'm using python lists as part of the query (eg: <code>.db.session.query(Item).filter(Item.symbol.in_(add_symbols)</code> where <code>Item</code> is my model class and <code>add_symbols</code> is a list). This is the equivalent of SQL <code>SELECT ... from ... WHERE ... IN</code>. </p>

<p>Is anything possible?</p>
";9483.0;[];['<Query object>', 'pandas.read_sql', '.db.session.query(Item).filter(Item.symbol.in_(add_symbols)', 'Item', 'add_symbols', 'SELECT ... from ... WHERE ... IN']
474;8.0;1;29530232;;1;113;<python><pandas><nan>;Python pandas: check if any value is NaN in DataFrame;"<p>In python pandas, what's the best way to check whether a DataFrame has one (or more) NaN values?</p>

<p>I know about the function <code>pd.isnan</code>, but this returns a DataFrame of booleans for each element. <a href=""https://stackoverflow.com/questions/27754891/python-nan-value-in-pandas"">This post</a> right here doesn't exactly answer my question either.</p>
";136260.0;[];['pd.isnan']
475;3.0;0;29576430;;1;60;<python><pandas><dataframe><permutation><shuffle>;Shuffle DataFrame rows;"<p>I have the following DataFrame:</p>

<pre><code>    Col1  Col2  Col3  Type
0      1     2     3     1
1      4     5     6     1
...
20     7     8     9     2
21    10    11    12     2
...
45    13    14    15     3
46    16    17    18     3
...
</code></pre>

<p>The DataFrame is read from a csv file. All rows which have <code>Type</code> 1 are on top, followed by the rows with <code>Type</code> 2, followed by the rows with <code>Type</code> 3, etc.</p>

<p>I would like to shuffle the DataFrame's rows, so that all <code>Type</code>'s are mixed. A possible result could be:</p>

<pre><code>    Col1  Col2  Col3  Type
0      7     8     9     2
1     13    14    15     3
...
20     1     2     3     1
21    10    11    12     2
...
45     4     5     6     1
46    16    17    18     3
...
</code></pre>

<p>As can be seen from the result, the order of the rows is shuffled, but the columns remain the same. I don't know if I am explaining this clearly. Let me know if I don't.</p>

<p>How can I achieve this?</p>
";36846.0;['    Col1  Col2  Col3  Type\n0      1     2     3     1\n1      4     5     6     1\n...\n20     7     8     9     2\n21    10    11    12     2\n...\n45    13    14    15     3\n46    16    17    18     3\n...\n', '    Col1  Col2  Col3  Type\n0      7     8     9     2\n1     13    14    15     3\n...\n20     1     2     3     1\n21    10    11    12     2\n...\n45     4     5     6     1\n46    16    17    18     3\n...\n'];['    Col1  Col2  Col3  Type\n0      1     2     3     1\n1      4     5     6     1\n...\n20     7     8     9     2\n21    10    11    12     2\n...\n45    13    14    15     3\n46    16    17    18     3\n...\n', 'Type', 'Type', 'Type', 'Type', '    Col1  Col2  Col3  Type\n0      7     8     9     2\n1     13    14    15     3\n...\n20     1     2     3     1\n21    10    11    12     2\n...\n45     4     5     6     1\n46    16    17    18     3\n...\n']
476;4.0;0;29763620;;1;29;<python><pandas>;How to select all columns, except one column in pandas using .ix;"<p>I have a dataframe look like this:</p>

<pre><code>    import pandas
    import numpy as np
    df = DataFrame(np.random.rand(4,4), columns = list('abcd'))
    df
          a         b         c         d
    0  0.418762  0.042369  0.869203  0.972314
    1  0.991058  0.510228  0.594784  0.534366
    2  0.407472  0.259811  0.396664  0.894202
    3  0.726168  0.139531  0.324932  0.906575
</code></pre>

<p>How I can get all columns except <code>column b</code> using <code>df.ix</code></p>
";26374.0;"[""    import pandas\n    import numpy as np\n    df = DataFrame(np.random.rand(4,4), columns = list('abcd'))\n    df\n          a         b         c         d\n    0  0.418762  0.042369  0.869203  0.972314\n    1  0.991058  0.510228  0.594784  0.534366\n    2  0.407472  0.259811  0.396664  0.894202\n    3  0.726168  0.139531  0.324932  0.906575\n""]";"[""    import pandas\n    import numpy as np\n    df = DataFrame(np.random.rand(4,4), columns = list('abcd'))\n    df\n          a         b         c         d\n    0  0.418762  0.042369  0.869203  0.972314\n    1  0.991058  0.510228  0.594784  0.534366\n    2  0.407472  0.259811  0.396664  0.894202\n    3  0.726168  0.139531  0.324932  0.906575\n"", 'column b', 'df.ix']"
477;3.0;1;29815129;;1;38;<python><list><dictionary><pandas><dataframe>;Pandas DataFrame to List of Dictionaries;"<p>I have the following DataFrame:</p>

<pre>
customer    item1      item2    item3
1           apple      milk     tomato
2           water      orange   potato
3           juice      mango    chips
</pre>

<p>which I want to translate it to list of dictionaries per row</p>

<pre><code>rows = [{'customer': 1, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},
    {'customer': 2, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},
    {'customer': 3, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]
</code></pre>
";15409.0;"['\ncustomer    item1      item2    item3\n1           apple      milk     tomato\n2           water      orange   potato\n3           juice      mango    chips\n', ""rows = [{'customer': 1, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},\n    {'customer': 2, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},\n    {'customer': 3, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]\n""]";"[""rows = [{'customer': 1, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},\n    {'customer': 2, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},\n    {'customer': 3, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]\n""]"
478;3.0;0;29919306;;1;30;<python><pandas><dataframe><max>;Find the column name which has the maximum value for each row;"<p>I have a DataFrame like this one:</p>

<pre><code>In [7]:
frame.head()
Out[7]:
Communications and Search   Business    General Lifestyle
0   0.745763    0.050847    0.118644    0.084746
0   0.333333    0.000000    0.583333    0.083333
0   0.617021    0.042553    0.297872    0.042553
0   0.435897    0.000000    0.410256    0.153846
0   0.358974    0.076923    0.410256    0.153846
</code></pre>

<p>In here, I want to ask how to get column name which has maximum value for each row, the desired output is like this:</p>

<pre><code>In [7]:
    frame.head()
    Out[7]:
    Communications and Search   Business    General Lifestyle   Max
    0   0.745763    0.050847    0.118644    0.084746           Communications 
    0   0.333333    0.000000    0.583333    0.083333           Business  
    0   0.617021    0.042553    0.297872    0.042553           Communications 
    0   0.435897    0.000000    0.410256    0.153846           Communications 
    0   0.358974    0.076923    0.410256    0.153846           Business 
</code></pre>
";10758.0;['In [7]:\nframe.head()\nOut[7]:\nCommunications and Search   Business    General Lifestyle\n0   0.745763    0.050847    0.118644    0.084746\n0   0.333333    0.000000    0.583333    0.083333\n0   0.617021    0.042553    0.297872    0.042553\n0   0.435897    0.000000    0.410256    0.153846\n0   0.358974    0.076923    0.410256    0.153846\n', 'In [7]:\n    frame.head()\n    Out[7]:\n    Communications and Search   Business    General Lifestyle   Max\n    0   0.745763    0.050847    0.118644    0.084746           Communications \n    0   0.333333    0.000000    0.583333    0.083333           Business  \n    0   0.617021    0.042553    0.297872    0.042553           Communications \n    0   0.435897    0.000000    0.410256    0.153846           Communications \n    0   0.358974    0.076923    0.410256    0.153846           Business \n'];['In [7]:\nframe.head()\nOut[7]:\nCommunications and Search   Business    General Lifestyle\n0   0.745763    0.050847    0.118644    0.084746\n0   0.333333    0.000000    0.583333    0.083333\n0   0.617021    0.042553    0.297872    0.042553\n0   0.435897    0.000000    0.410256    0.153846\n0   0.358974    0.076923    0.410256    0.153846\n', 'In [7]:\n    frame.head()\n    Out[7]:\n    Communications and Search   Business    General Lifestyle   Max\n    0   0.745763    0.050847    0.118644    0.084746           Communications \n    0   0.333333    0.000000    0.583333    0.083333           Business  \n    0   0.617021    0.042553    0.297872    0.042553           Communications \n    0   0.435897    0.000000    0.410256    0.153846           Communications \n    0   0.358974    0.076923    0.410256    0.153846           Business \n']
479;3.0;0;30522724;;1;30;<python><numpy><pandas>;Take multiple lists into dataframe;"<p>How do I take multiple lists and put them as different columns in a python dataframe? I tried following <a href=""https://stackoverflow.com/questions/29014618/read-lists-into-columns-of-pandas-dataframe"">Read lists into columns of pandas DataFrame</a> but had some trouble.</p>

<p>Attempt 1:</p>

<ul>
<li>Have three lists, and zip them together and use that res = zip(lst1,lst2,lst3)</li>
<li>Yields just one column</li>
</ul>

<p>Attempt 2:</p>

<pre><code>percentile_list = pd.DataFrame({'lst1Tite' : [lst1],
 'lst2Tite' : [lst2],
 'lst3Tite':[lst3]
  }, columns=['lst1Tite','lst1Tite', 'lst1Tite'])
</code></pre>

<p>- yields either one row by 3 columns (the way above) or if I transpose it is 3 rows and 1 column</p>

<p>How do I get a 100 row (length of each independent list) by 3 column (three lists) pandas dataframe? </p>
";38497.0;"[""percentile_list = pd.DataFrame({'lst1Tite' : [lst1],\n 'lst2Tite' : [lst2],\n 'lst3Tite':[lst3]\n  }, columns=['lst1Tite','lst1Tite', 'lst1Tite'])\n""]";"[""percentile_list = pd.DataFrame({'lst1Tite' : [lst1],\n 'lst2Tite' : [lst2],\n 'lst3Tite':[lst3]\n  }, columns=['lst1Tite','lst1Tite', 'lst1Tite'])\n""]"
480;6.0;3;30522982;;1;28;<python><pandas><dataset>;List with many dictionaries VS dictionary with few lists?;"<p>I am doing some exercises with datasets like so:</p>

<p><strong>List with many dictionaries</strong></p>

<pre><code>users = [
    {""id"": 0, ""name"": ""Ashley""},
    {""id"": 1, ""name"": ""Ben""},
    {""id"": 2, ""name"": ""Conrad""},
    {""id"": 3, ""name"": ""Doug""},
    {""id"": 4, ""name"": ""Evin""},
    {""id"": 5, ""name"": ""Florian""},
    {""id"": 6, ""name"": ""Gerald""}
]
</code></pre>

<p><strong>Dictionary with few lists</strong></p>

<pre><code>users2 = {
    ""id"": [0, 1, 2, 3, 4, 5, 6],
    ""name"": [""Ashley"", ""Ben"", ""Conrad"", ""Doug"",""Evin"", ""Florian"", ""Gerald""]
}
</code></pre>

<p><strong>Pandas dataframes</strong></p>

<pre><code>import pandas as pd
pd_users = pd.DataFrame(users)
pd_users2 = pd.DataFrame(users2)
print pd_users == pd_users2
</code></pre>

<p>Questions: </p>

<ol>
<li>Should I structure the datasets like users or like users2?</li>
<li>Are there performance differences?</li>
<li>Is one more readable than the other?</li>
<li>Is there a standard I should follow? </li>
<li>I usually convert these to pandas dataframes. When I do that, both versions are identical... right? </li>
<li>The output is true for each element so it doesn't matter if I work with panda df's right?</li>
</ol>
";1050.0;"['users = [\n    {""id"": 0, ""name"": ""Ashley""},\n    {""id"": 1, ""name"": ""Ben""},\n    {""id"": 2, ""name"": ""Conrad""},\n    {""id"": 3, ""name"": ""Doug""},\n    {""id"": 4, ""name"": ""Evin""},\n    {""id"": 5, ""name"": ""Florian""},\n    {""id"": 6, ""name"": ""Gerald""}\n]\n', 'users2 = {\n    ""id"": [0, 1, 2, 3, 4, 5, 6],\n    ""name"": [""Ashley"", ""Ben"", ""Conrad"", ""Doug"",""Evin"", ""Florian"", ""Gerald""]\n}\n', 'import pandas as pd\npd_users = pd.DataFrame(users)\npd_users2 = pd.DataFrame(users2)\nprint pd_users == pd_users2\n']";"['users = [\n    {""id"": 0, ""name"": ""Ashley""},\n    {""id"": 1, ""name"": ""Ben""},\n    {""id"": 2, ""name"": ""Conrad""},\n    {""id"": 3, ""name"": ""Doug""},\n    {""id"": 4, ""name"": ""Evin""},\n    {""id"": 5, ""name"": ""Florian""},\n    {""id"": 6, ""name"": ""Gerald""}\n]\n', 'users2 = {\n    ""id"": [0, 1, 2, 3, 4, 5, 6],\n    ""name"": [""Ashley"", ""Ben"", ""Conrad"", ""Doug"",""Evin"", ""Florian"", ""Gerald""]\n}\n', 'import pandas as pd\npd_users = pd.DataFrame(users)\npd_users2 = pd.DataFrame(users2)\nprint pd_users == pd_users2\n']"
481;2.0;2;30530663;;1;24;<python><pandas>;"How to ""select distinct"" across multiple data frame columns in pandas?";"<p>I'm looking for a way to do the equivalent to the sql </p>

<blockquote>
  <p>""SELECT DISTINCT col1, col2 FROM dataframe_table""</p>
</blockquote>

<p>The pandas sql comparison doesn't have anything about ""distinct""</p>

<p>.unique() only works for a single column, so I suppose I could concat the columns, or put them in a list/tuple and compare that way, but this seems like something pandas should do in a more native way.  </p>

<p>Am I missing something obvious, or is there no way to do this?</p>
";25025.0;[];[]
482;2.0;7;30631325;;1;27;<python><mysql><pandas><sqlalchemy><mysql-connector>;Writing to MySQL database with pandas using SQLAlchemy, to_sql;"<p>trying to write pandas dataframe to MySQL table using to_sql.  Previously been using flavor='mysql', however it will be depreciated in the future and wanted to start the transition to using SQLAlchemy engine.</p>

<p>sample code:</p>

<pre><code>import pandas as pd
import mysql.connector
from sqlalchemy import create_engine

engine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)
cnx = engine.raw_connection()
data = pd.read_sql('SELECT * FROM sample_table', cnx)
data.to_sql(name='sample_table2', con=cnx, if_exists = 'append', index=False)
</code></pre>

<p>The read works fine but the to_sql has an error:</p>

<p><strong>DatabaseError: Execution failed on sql 'SELECT name FROM sqlite_master WHERE type='table' AND name=?;': Wrong number of arguments during string formatting</strong></p>

<p>Why does it look like it is trying to use sqlite? What is the correct use of a sqlalchemy connection with mysql and specifically mysql.connector?</p>

<p>I also tried passing the engine in as the connection as well, and that gave me an error referencing no cursor object.</p>

<pre><code>data.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)
&gt;&gt;AttributeError: 'Engine' object has no attribute 'cursor'
</code></pre>
";22941.0;"[""import pandas as pd\nimport mysql.connector\nfrom sqlalchemy import create_engine\n\nengine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)\ncnx = engine.raw_connection()\ndata = pd.read_sql('SELECT * FROM sample_table', cnx)\ndata.to_sql(name='sample_table2', con=cnx, if_exists = 'append', index=False)\n"", ""data.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)\n>>AttributeError: 'Engine' object has no attribute 'cursor'\n""]";"[""import pandas as pd\nimport mysql.connector\nfrom sqlalchemy import create_engine\n\nengine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)\ncnx = engine.raw_connection()\ndata = pd.read_sql('SELECT * FROM sample_table', cnx)\ndata.to_sql(name='sample_table2', con=cnx, if_exists = 'append', index=False)\n"", ""data.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)\n>>AttributeError: 'Engine' object has no attribute 'cursor'\n""]"
483;3.0;0;30926670;;1;26;<python><pandas>;Pandas: Add multiple empty columns to DataFrame;"<p>This may be a stupid question, but how do I add multiple empty columns to a DataFrame from a list?</p>

<p>I can do:</p>

<pre><code>df[""B""] = None
df[""C""] = None
df[""D""] = None
</code></pre>

<p>But I can't do:</p>

<pre><code>df[[""B"", ""C"", ""D""]] = None

KeyError: ""['B' 'C' 'D'] not in index""
</code></pre>
";14668.0;"['df[""B""] = None\ndf[""C""] = None\ndf[""D""] = None\n', 'df[[""B"", ""C"", ""D""]] = None\n\nKeyError: ""[\'B\' \'C\' \'D\'] not in index""\n']";"['df[""B""] = None\ndf[""C""] = None\ndf[""D""] = None\n', 'df[[""B"", ""C"", ""D""]] = None\n\nKeyError: ""[\'B\' \'C\' \'D\'] not in index""\n']"
484;3.0;0;31029560;;1;25;<python><pandas>;Plotting categorical data with pandas and matplotlib;"<p>I have a data frame with categorical data:</p>

<pre><code>     colour  direction
1    red     up
2    blue    up
3    green   down
4    red     left
5    red     right
6    yellow  down
7    blue    down
</code></pre>

<p>and now I want to generate some graphs, like pie charts and histograns based on the categories. Is it possible without creating dummy numeric variables? Something like</p>

<pre><code>df.plot(kind='hist')
</code></pre>
";20647.0;"['     colour  direction\n1    red     up\n2    blue    up\n3    green   down\n4    red     left\n5    red     right\n6    yellow  down\n7    blue    down\n', ""df.plot(kind='hist')\n""]";"['     colour  direction\n1    red     up\n2    blue    up\n3    green   down\n4    red     left\n5    red     right\n6    yellow  down\n7    blue    down\n', ""df.plot(kind='hist')\n""]"
485;4.0;0;31357611;;1;21;<python><pandas><matplotlib><plot>;Format y axis as percent;"<p>I have an existing plot that was created with pandas like this:</p>

<pre><code>df['myvar'].plot(kind='bar')
</code></pre>

<p>The y axis is format as float and I want to change the y axis to percentages.  All of the solutions I found use ax.xyz syntax and <strong>I can only place code below the line above that creates the plot</strong> (I cannot add ax=ax to the line above.) </p>

<p><strong>How can I format the y axis as percentages without changing the line above?</strong></p>

<p>Here is the solution I found <strong>but requires that I redefine the plot</strong>:</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np
import matplotlib.ticker as mtick

data = [8,12,15,17,18,18.5]
perc = np.linspace(0,100,len(data))

fig = plt.figure(1, (7,4))
ax = fig.add_subplot(1,1,1)

ax.plot(perc, data)

fmt = '%.0f%%' # Format you want the ticks, e.g. '40%'
xticks = mtick.FormatStrFormatter(fmt)
ax.xaxis.set_major_formatter(xticks)

plt.show()
</code></pre>

<p>Link to the above solution: <a href=""https://stackoverflow.com/questions/26294360/pyplot-using-percentage-on-x-axis"">Pyplot: using percentage on x axis</a></p>
";18791.0;"[""df['myvar'].plot(kind='bar')\n"", ""import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.ticker as mtick\n\ndata = [8,12,15,17,18,18.5]\nperc = np.linspace(0,100,len(data))\n\nfig = plt.figure(1, (7,4))\nax = fig.add_subplot(1,1,1)\n\nax.plot(perc, data)\n\nfmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\nxticks = mtick.FormatStrFormatter(fmt)\nax.xaxis.set_major_formatter(xticks)\n\nplt.show()\n""]";"[""df['myvar'].plot(kind='bar')\n"", ""import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.ticker as mtick\n\ndata = [8,12,15,17,18,18.5]\nperc = np.linspace(0,100,len(data))\n\nfig = plt.figure(1, (7,4))\nax = fig.add_subplot(1,1,1)\n\nax.plot(perc, data)\n\nfmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\nxticks = mtick.FormatStrFormatter(fmt)\nax.xaxis.set_major_formatter(xticks)\n\nplt.show()\n""]"
486;2.0;3;31361721;;1;24;<python><pandas><parallel-processing><dask>;python dask DataFrame, support for (trivially parallelizable) row apply?;"<p>I recently found <a href=""http://dask.pydata.org/en/latest/index.html"">dask</a> module that aims to be an easy-to-use python parallel processing module. Big selling point for me is that it works with pandas.</p>

<p>After reading a bit on its manual page, I can't find a way to do this trivially parallelizable task: </p>

<pre><code>ts.apply(func) # for pandas series
df.apply(func, axis = 1) # for pandas DF row apply
</code></pre>

<p>At the moment, to achieve this in dask, AFAIK,</p>

<pre><code>ddf.assign(A=lambda df: df.apply(func, axis=1)).compute() # dask DataFrame
</code></pre>

<p>which is ugly syntax and is actually slower than outright</p>

<pre><code>df.apply(func, axis = 1) # for pandas DF row apply
</code></pre>

<p>Any suggestion?</p>

<p>Edit: Thanks @MRocklin for the map function. It seems to be slower than plain pandas apply. Is this related to pandas GIL releasing issue or am I doing it wrong?</p>

<pre><code>import dask.dataframe as dd
s = pd.Series([10000]*120)
ds = dd.from_pandas(s, npartitions = 3)

def slow_func(k):
    A = np.random.normal(size = k) # k = 10000
    s = 0
    for a in A:
        if a &gt; 0:
            s += 1
        else:
            s -= 1
    return s

s.apply(slow_func) # 0.43 sec
ds.map(slow_func).compute() # 2.04 sec
</code></pre>
";4089.0;['ts.apply(func) # for pandas series\ndf.apply(func, axis = 1) # for pandas DF row apply\n', 'ddf.assign(A=lambda df: df.apply(func, axis=1)).compute() # dask DataFrame\n', 'df.apply(func, axis = 1) # for pandas DF row apply\n', 'import dask.dataframe as dd\ns = pd.Series([10000]*120)\nds = dd.from_pandas(s, npartitions = 3)\n\ndef slow_func(k):\n    A = np.random.normal(size = k) # k = 10000\n    s = 0\n    for a in A:\n        if a > 0:\n            s += 1\n        else:\n            s -= 1\n    return s\n\ns.apply(slow_func) # 0.43 sec\nds.map(slow_func).compute() # 2.04 sec\n'];['ts.apply(func) # for pandas series\ndf.apply(func, axis = 1) # for pandas DF row apply\n', 'ddf.assign(A=lambda df: df.apply(func, axis=1)).compute() # dask DataFrame\n', 'df.apply(func, axis = 1) # for pandas DF row apply\n', 'import dask.dataframe as dd\ns = pd.Series([10000]*120)\nds = dd.from_pandas(s, npartitions = 3)\n\ndef slow_func(k):\n    A = np.random.normal(size = k) # k = 10000\n    s = 0\n    for a in A:\n        if a > 0:\n            s += 1\n        else:\n            s -= 1\n    return s\n\ns.apply(slow_func) # 0.43 sec\nds.map(slow_func).compute() # 2.04 sec\n']
487;2.0;2;31593201;;1;218;<python><pandas><indexing><dataframe>;pandas iloc vs ix vs loc explanation?;"<p>Can someone explain how these three methods of slicing are different?<br>
I've seen <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html"" rel=""noreferrer"">the docs</a>, 
and I've seen <a href=""https://stackoverflow.com/questions/28757389/loc-vs-iloc-vs-ix-vs-at-vs-iat"">these</a> <a href=""https://stackoverflow.com/questions/27667759/is-ix-always-better-than-loc-and-iloc-since-it-is-faster-and-supports-i"">answers</a>, but I still find myself unable to explain how the three are different.  To me, they seem interchangeable in large part, because they are at the lower levels of slicing.</p>

<p>For example, say we want to get the first five rows of a <code>DataFrame</code>.  How is it that all three of these work?</p>

<pre><code>df.loc[:5]
df.ix[:5]
df.iloc[:5]
</code></pre>

<p>Can someone present three cases where the distinction in uses are clearer?</p>
";118214.0;['df.loc[:5]\ndf.ix[:5]\ndf.iloc[:5]\n'];['DataFrame', 'df.loc[:5]\ndf.ix[:5]\ndf.iloc[:5]\n']
488;1.0;2;31609600;;1;30;<python><pandas><ipython>;Jupyter (IPython) notebook not plotting;"<p>I installed anaconda to use pandas and scipy. I reading and watching pandas tutorials and they all say to open the ipython notebook using</p>

<pre><code> ipython notebook --pylab==inline
</code></pre>

<p>but when I do that I get a message saying</p>

<pre><code>""Support for specifying --pylab on the command line has been removed. Please use '%pylab = inline' or '%matplotlib =inline' in the notebook itself""
</code></pre>

<p>But that does not work. Then when I try ""plot(arange(10))"" I get a message saying ""name 'plot' is not defined."" I trying plotting data from a .csv file and got</p>

<pre><code>""matplotlib.axes._subplots.AxesSubplot at 0xebf8b70"".
</code></pre>

<p>What should I do?</p>
";22330.0;"[' ipython notebook --pylab==inline\n', '""Support for specifying --pylab on the command line has been removed. Please use \'%pylab = inline\' or \'%matplotlib =inline\' in the notebook itself""\n', '""matplotlib.axes._subplots.AxesSubplot at 0xebf8b70"".\n']";"[' ipython notebook --pylab==inline\n', '""Support for specifying --pylab on the command line has been removed. Please use \'%pylab = inline\' or \'%matplotlib =inline\' in the notebook itself""\n', '""matplotlib.axes._subplots.AxesSubplot at 0xebf8b70"".\n']"
489;4.0;0;32011359;;1;24;<python><pandas>;Convert categorical data in pandas dataframe;"<p>I have a dataframe with this type of data (too many columns):</p>

<pre><code>col1        int64
col2        int64
col3        category
col4        category
col5        category
</code></pre>

<p>Columns seems like this:</p>

<pre><code>Name: col3, dtype: category
Categories (8, object): [B, C, E, G, H, N, S, W]
</code></pre>

<p>I want to convert all value in columns to integer like this:</p>

<pre><code>[1, 2, 3, 4, 5, 6, 7, 8]
</code></pre>

<p>I solved this for one column by this:</p>

<pre><code>dataframe['c'] = pandas.Categorical.from_array(dataframe.col3).codes
</code></pre>

<p>Now I have two columns in my dataframe - old 'col3' and new 'c' and need to drop old columns. </p>

<p>That's bad practice. It's work but in my dataframe many columns and I don't want do it manually.  </p>

<p>How do this pythonic and just cleverly?</p>
";29142.0;"['col1        int64\ncol2        int64\ncol3        category\ncol4        category\ncol5        category\n', 'Name: col3, dtype: category\nCategories (8, object): [B, C, E, G, H, N, S, W]\n', '[1, 2, 3, 4, 5, 6, 7, 8]\n', ""dataframe['c'] = pandas.Categorical.from_array(dataframe.col3).codes\n""]";"['col1        int64\ncol2        int64\ncol3        category\ncol4        category\ncol5        category\n', 'Name: col3, dtype: category\nCategories (8, object): [B, C, E, G, H, N, S, W]\n', '[1, 2, 3, 4, 5, 6, 7, 8]\n', ""dataframe['c'] = pandas.Categorical.from_array(dataframe.col3).codes\n""]"
490;2.0;0;32244019;;1;22;<python><pandas><matplotlib>;How to rotate x-axis tick labels in Pandas barplot;"<p>With the following code:</p>

<pre><code>import matplotlib
matplotlib.style.use('ggplot')
import matplotlib.pyplot as plt
import pandas as pd

df = pd.DataFrame({ 'celltype':[""foo"",""bar"",""qux"",""woz""], 's1':[5,9,1,7], 's2':[12,90,13,87]})
df = df[[""celltype"",""s1"",""s2""]]
df.set_index([""celltype""],inplace=True)
df.plot(kind='bar',alpha=0.75)
plt.xlabel("""")
</code></pre>

<p>I made this plot:</p>

<p><a href=""https://i.stack.imgur.com/6pLLq.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/6pLLq.jpg"" alt=""enter image description here""></a></p>

<p>How can I rotate the x-axis tick labels to 0 degrees?</p>

<p>I tried adding this but did not work:</p>

<pre><code>plt.set_xticklabels(df.index,rotation=90)
</code></pre>
";14916.0;"['import matplotlib\nmatplotlib.style.use(\'ggplot\')\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.DataFrame({ \'celltype\':[""foo"",""bar"",""qux"",""woz""], \'s1\':[5,9,1,7], \'s2\':[12,90,13,87]})\ndf = df[[""celltype"",""s1"",""s2""]]\ndf.set_index([""celltype""],inplace=True)\ndf.plot(kind=\'bar\',alpha=0.75)\nplt.xlabel("""")\n', 'plt.set_xticklabels(df.index,rotation=90)\n']";"['import matplotlib\nmatplotlib.style.use(\'ggplot\')\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.DataFrame({ \'celltype\':[""foo"",""bar"",""qux"",""woz""], \'s1\':[5,9,1,7], \'s2\':[12,90,13,87]})\ndf = df[[""celltype"",""s1"",""s2""]]\ndf.set_index([""celltype""],inplace=True)\ndf.plot(kind=\'bar\',alpha=0.75)\nplt.xlabel("""")\n', 'plt.set_xticklabels(df.index,rotation=90)\n']"
491;4.0;0;32244753;;1;35;<python><pandas><matplotlib><seaborn>;How to save a Seaborn plot into a file;"<p>I tried the following code (<code>test_seaborn.py</code>):</p>

<pre><code>import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
matplotlib.style.use('ggplot')
import seaborn as sns
sns.set()
df = sns.load_dataset('iris')
sns_plot = sns.pairplot(df, hue='species', size=2.5)
fig = sns_plot.get_figure()
fig.savefig(""output.png"")
#sns.plt.show()
</code></pre>

<p>But I get this error:</p>

<pre><code>  Traceback (most recent call last):
  File ""test_searborn.py"", line 11, in &lt;module&gt;
    fig = sns_plot.get_figure()
AttributeError: 'PairGrid' object has no attribute 'get_figure'
</code></pre>

<p>I expect the final <code>output.png</code> will exist and look like this:</p>

<p><a href=""https://i.stack.imgur.com/n6uXd.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/n6uXd.png"" alt=""enter image description here""></a></p>

<p>How can I resolve the problem?</p>
";27148.0;"['import matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nmatplotlib.style.use(\'ggplot\')\nimport seaborn as sns\nsns.set()\ndf = sns.load_dataset(\'iris\')\nsns_plot = sns.pairplot(df, hue=\'species\', size=2.5)\nfig = sns_plot.get_figure()\nfig.savefig(""output.png"")\n#sns.plt.show()\n', '  Traceback (most recent call last):\n  File ""test_searborn.py"", line 11, in <module>\n    fig = sns_plot.get_figure()\nAttributeError: \'PairGrid\' object has no attribute \'get_figure\'\n']";"['test_seaborn.py', 'import matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nmatplotlib.style.use(\'ggplot\')\nimport seaborn as sns\nsns.set()\ndf = sns.load_dataset(\'iris\')\nsns_plot = sns.pairplot(df, hue=\'species\', size=2.5)\nfig = sns_plot.get_figure()\nfig.savefig(""output.png"")\n#sns.plt.show()\n', '  Traceback (most recent call last):\n  File ""test_searborn.py"", line 11, in <module>\n    fig = sns_plot.get_figure()\nAttributeError: \'PairGrid\' object has no attribute \'get_figure\'\n', 'output.png']"
492;4.0;2;32400867;;1;22;<python><csv><pandas><request>;Pandas read_csv from url;"<p>I am using Python 3.4 with IPython and have the following code. I'm unable to read a csv-file from the given URL:</p>

<pre><code>import pandas as pd
import requests

url=""https://github.com/cs109/2014_data/blob/master/countries.csv""
s=requests.get(url).content
c=pd.read_csv(s)
</code></pre>

<p>I have the following error</p>

<blockquote>
  <p>""Expected file path name or file-like object, got  type""</p>
</blockquote>

<p>How can I fix this?</p>
";17280.0;"['import pandas as pd\nimport requests\n\nurl=""https://github.com/cs109/2014_data/blob/master/countries.csv""\ns=requests.get(url).content\nc=pd.read_csv(s)\n']";"['import pandas as pd\nimport requests\n\nurl=""https://github.com/cs109/2014_data/blob/master/countries.csv""\ns=requests.get(url).content\nc=pd.read_csv(s)\n']"
493;4.0;5;32468402;;1;22;<python><pandas><dataframe>;How to explode a list inside a Dataframe cell into separate rows;"<p>I'm looking to turn a pandas cell containing a list into rows for each of those values.</p>

<p>So, take this:</p>

<p><a href=""https://i.stack.imgur.com/j7lFk.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/j7lFk.png"" alt=""enter image description here""></a></p>

<p>If I'd like to unpack and stack the values in the 'nearest_neighbors"" column so that each value would be a row within each 'opponent' index, how would I best go about this? Are there pandas methods that are meant for operations like this? I'm just not aware.</p>

<p>Thanks in advance, guys.</p>
";8761.0;[];[]
494;1.0;6;33530753;;1;45;<python><arrays><numpy><pandas>;numpy: Reliable (non-conservative) indicator if numpy array is view;"<p>Looking for a way to reliably identify if a numpy object is a view. </p>

<p>Related questions have come up many times before (<a href=""https://stackoverflow.com/questions/11286864/is-there-a-way-to-check-if-numpy-arrays-share-the-same-data"">here</a>, <a href=""https://stackoverflow.com/questions/10747748/how-do-i-check-that-two-slices-of-numpy-arrays-are-the-same-or-overlapping"">here</a>, <a href=""https://stackoverflow.com/questions/28886731/numpy-reshape-on-view"">here</a>), and people have offered some solutions, but all seem to have problems:</p>

<ul>
<li>The test used in <code>pandas</code> now is to call something a view if <code>my_array.base is not None</code>. This seems to always catch views, but also offers lots of false positives (situations where it reports something is a view even if it isn't). </li>
<li><code>numpy.may_share_memory()</code> will check for two specific arrays, but won't answer generically 

<ul>
<li>(@RobertKurn says was best tool as of 2012 -- any changes?)</li>
</ul></li>
<li><code>flags['OWNDATA'])</code> is <a href=""https://stackoverflow.com/questions/11524664/how-can-i-tell-if-numpy-creates-a-view-or-a-copy"">reported (third comment first answer)</a> to fail in some cases. </li>
</ul>

<p>(The reason for my interest is that I'm working on implementing copy-on-write for pandas, and a conservative indicator is leading to over-copying.)</p>
";839.0;[];"['pandas', 'my_array.base is not None', 'numpy.may_share_memory()', ""flags['OWNDATA'])""]"
495;4.0;0;34001922;;1;21;<python><pandas><classification><tensorflow>;FailedPreconditionError: Attempting to use uninitialized in Tensorflow;"<p>I am working through the <a href=""https://www.tensorflow.org/tutorials/mnist/pros/index.html"" rel=""noreferrer"">tensor flow tutorial</a>, but am trying to use a numpy or pandas format for the data, so that I can compare it with Scikit-Learn results.</p>

<p>I get the digit recognition data from kaggle - <a href=""https://www.kaggle.com/c/digit-recognizer/data"" rel=""noreferrer"">here</a></p>

<p>The tutorial uses a weird format for uploading the data, where as I'm trying to compare with results from other libraries, so would like to keep it in numpy or pandas format.</p>

<p>Here is the standard tensor flow tutorial code (this all works fine):</p>

<pre><code># Stuff from tensorflow tutorial 
import tensorflow as tf
sess = tf.InteractiveSession()

x = tf.placeholder(""float"", shape=[None, 784])
y_ = tf.placeholder(""float"", shape=[None, 10])

W = tf.Variable(tf.zeros([784,10]))
b = tf.Variable(tf.zeros([10]))

y = tf.nn.softmax(tf.matmul(x,W) + b)

cross_entropy = -tf.reduce_sum(y_*tf.log(y))

train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)
</code></pre>

<p>Here I read the data, strip out the target variables and split the data into testing and training datasets (this all works fine):</p>

<pre><code># Read dataframe from training data
csvfile='train.csv'
from pandas import DataFrame, read_csv
df = read_csv(csvfile)

# Strip off the target data and make it a separate dataframe.
Target=df.label
del df[""label""]

# Split data into training and testing sets
msk = np.random.rand(len(df)) &lt; 0.8
dfTest = df[~msk]
TargetTest = Target[~msk]
df = df[msk]
Target = Target[msk]

# One hot encode the target
OHTarget=pd.get_dummies(Target)
OHTargetTest=pd.get_dummies(TargetTest)
</code></pre>

<p>Now, when I try to run the training step, I get a FailedPreconditionError:</p>

<pre><code>for i in range(100):
    batch = np.array(df[i*50:i*50+50].values)
    batch = np.multiply(batch, 1.0 / 255.0)
    Target_batch = np.array(OHTarget[i*50:i*50+50].values)
    Target_batch = np.multiply(Target_batch, 1.0 / 255.0)
    train_step.run(feed_dict={x: batch, y_: Target_batch})
</code></pre>

<p>Here's the full error:</p>

<pre><code>---------------------------------------------------------------------------
FailedPreconditionError                   Traceback (most recent call last)
&lt;ipython-input-82-967faab7d494&gt; in &lt;module&gt;()
      4     Target_batch = np.array(OHTarget[i*50:i*50+50].values)
      5     Target_batch = np.multiply(Target_batch, 1.0 / 255.0)
----&gt; 6     train_step.run(feed_dict={x: batch, y_: Target_batch})

/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in run(self, feed_dict, session)
   1265         none, the default session will be used.
   1266     """"""
-&gt; 1267     _run_using_default_session(self, feed_dict, self.graph, session)
   1268
   1269

/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in _run_using_default_session(operation, feed_dict, graph, session)
   2761                        ""the operation's graph is different from the session's ""
   2762                        ""graph."")
-&gt; 2763   session.run(operation, feed_dict)
   2764
   2765

/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict)
    343
    344     # Run request and get response.
--&gt; 345     results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
    346
    347     # User may have fetched the same tensor multiple times, but we

/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, target_list, fetch_list, feed_dict)
    417         # pylint: disable=protected-access
    418         raise errors._make_specific_exception(node_def, op, e.error_message,
--&gt; 419                                               e.code)
    420         # pylint: enable=protected-access
    421       raise e_type, e_value, e_traceback

FailedPreconditionError: Attempting to use uninitialized value Variable_1
     [[Node: gradients/add_grad/Shape_1 = Shape[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_1)]]
Caused by op u'gradients/add_grad/Shape_1', defined at:
  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main
    ...........

...which was originally created as op u'add', defined at:
  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
[elided 17 identical lines from previous traceback]
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 3066, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""&lt;ipython-input-45-59183d86e462&gt;"", line 1, in &lt;module&gt;
    y = tf.nn.softmax(tf.matmul(x,W) + b)
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 403, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 44, in add
    return _op_def_lib.apply_op(""Add"", x=x, y=y, name=name)
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 633, in apply_op
    op_def=op_def)
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1710, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 988, in __init__
    self._traceback = _extract_stack()
</code></pre>

<p>Any ideas as to how I can fix this?</p>
";20199.0;"['# Stuff from tensorflow tutorial \nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\nx = tf.placeholder(""float"", shape=[None, 784])\ny_ = tf.placeholder(""float"", shape=[None, 10])\n\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n\ny = tf.nn.softmax(tf.matmul(x,W) + b)\n\ncross_entropy = -tf.reduce_sum(y_*tf.log(y))\n\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n', '# Read dataframe from training data\ncsvfile=\'train.csv\'\nfrom pandas import DataFrame, read_csv\ndf = read_csv(csvfile)\n\n# Strip off the target data and make it a separate dataframe.\nTarget=df.label\ndel df[""label""]\n\n# Split data into training and testing sets\nmsk = np.random.rand(len(df)) < 0.8\ndfTest = df[~msk]\nTargetTest = Target[~msk]\ndf = df[msk]\nTarget = Target[msk]\n\n# One hot encode the target\nOHTarget=pd.get_dummies(Target)\nOHTargetTest=pd.get_dummies(TargetTest)\n', 'for i in range(100):\n    batch = np.array(df[i*50:i*50+50].values)\n    batch = np.multiply(batch, 1.0 / 255.0)\n    Target_batch = np.array(OHTarget[i*50:i*50+50].values)\n    Target_batch = np.multiply(Target_batch, 1.0 / 255.0)\n    train_step.run(feed_dict={x: batch, y_: Target_batch})\n', '---------------------------------------------------------------------------\nFailedPreconditionError                   Traceback (most recent call last)\n<ipython-input-82-967faab7d494> in <module>()\n      4     Target_batch = np.array(OHTarget[i*50:i*50+50].values)\n      5     Target_batch = np.multiply(Target_batch, 1.0 / 255.0)\n----> 6     train_step.run(feed_dict={x: batch, y_: Target_batch})\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in run(self, feed_dict, session)\n   1265         none, the default session will be used.\n   1266     """"""\n-> 1267     _run_using_default_session(self, feed_dict, self.graph, session)\n   1268\n   1269\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in _run_using_default_session(operation, feed_dict, graph, session)\n   2761                        ""the operation\'s graph is different from the session\'s ""\n   2762                        ""graph."")\n-> 2763   session.run(operation, feed_dict)\n   2764\n   2765\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict)\n    343\n    344     # Run request and get response.\n--> 345     results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n    346\n    347     # User may have fetched the same tensor multiple times, but we\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, target_list, fetch_list, feed_dict)\n    417         # pylint: disable=protected-access\n    418         raise errors._make_specific_exception(node_def, op, e.error_message,\n--> 419                                               e.code)\n    420         # pylint: enable=protected-access\n    421       raise e_type, e_value, e_traceback\n\nFailedPreconditionError: Attempting to use uninitialized value Variable_1\n     [[Node: gradients/add_grad/Shape_1 = Shape[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_1)]]\nCaused by op u\'gradients/add_grad/Shape_1\', defined at:\n  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main\n    ...........\n\n...which was originally created as op u\'add\', defined at:\n  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main\n    ""__main__"", fname, loader, pkg_name)\n[elided 17 identical lines from previous traceback]\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File ""<ipython-input-45-59183d86e462>"", line 1, in <module>\n    y = tf.nn.softmax(tf.matmul(x,W) + b)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 403, in binary_op_wrapper\n    return func(x, y, name=name)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 44, in add\n    return _op_def_lib.apply_op(""Add"", x=x, y=y, name=name)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 633, in apply_op\n    op_def=op_def)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1710, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 988, in __init__\n    self._traceback = _extract_stack()\n']";"['# Stuff from tensorflow tutorial \nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\nx = tf.placeholder(""float"", shape=[None, 784])\ny_ = tf.placeholder(""float"", shape=[None, 10])\n\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n\ny = tf.nn.softmax(tf.matmul(x,W) + b)\n\ncross_entropy = -tf.reduce_sum(y_*tf.log(y))\n\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n', '# Read dataframe from training data\ncsvfile=\'train.csv\'\nfrom pandas import DataFrame, read_csv\ndf = read_csv(csvfile)\n\n# Strip off the target data and make it a separate dataframe.\nTarget=df.label\ndel df[""label""]\n\n# Split data into training and testing sets\nmsk = np.random.rand(len(df)) < 0.8\ndfTest = df[~msk]\nTargetTest = Target[~msk]\ndf = df[msk]\nTarget = Target[msk]\n\n# One hot encode the target\nOHTarget=pd.get_dummies(Target)\nOHTargetTest=pd.get_dummies(TargetTest)\n', 'for i in range(100):\n    batch = np.array(df[i*50:i*50+50].values)\n    batch = np.multiply(batch, 1.0 / 255.0)\n    Target_batch = np.array(OHTarget[i*50:i*50+50].values)\n    Target_batch = np.multiply(Target_batch, 1.0 / 255.0)\n    train_step.run(feed_dict={x: batch, y_: Target_batch})\n', '---------------------------------------------------------------------------\nFailedPreconditionError                   Traceback (most recent call last)\n<ipython-input-82-967faab7d494> in <module>()\n      4     Target_batch = np.array(OHTarget[i*50:i*50+50].values)\n      5     Target_batch = np.multiply(Target_batch, 1.0 / 255.0)\n----> 6     train_step.run(feed_dict={x: batch, y_: Target_batch})\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in run(self, feed_dict, session)\n   1265         none, the default session will be used.\n   1266     """"""\n-> 1267     _run_using_default_session(self, feed_dict, self.graph, session)\n   1268\n   1269\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in _run_using_default_session(operation, feed_dict, graph, session)\n   2761                        ""the operation\'s graph is different from the session\'s ""\n   2762                        ""graph."")\n-> 2763   session.run(operation, feed_dict)\n   2764\n   2765\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict)\n    343\n    344     # Run request and get response.\n--> 345     results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n    346\n    347     # User may have fetched the same tensor multiple times, but we\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, target_list, fetch_list, feed_dict)\n    417         # pylint: disable=protected-access\n    418         raise errors._make_specific_exception(node_def, op, e.error_message,\n--> 419                                               e.code)\n    420         # pylint: enable=protected-access\n    421       raise e_type, e_value, e_traceback\n\nFailedPreconditionError: Attempting to use uninitialized value Variable_1\n     [[Node: gradients/add_grad/Shape_1 = Shape[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_1)]]\nCaused by op u\'gradients/add_grad/Shape_1\', defined at:\n  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main\n    ...........\n\n...which was originally created as op u\'add\', defined at:\n  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main\n    ""__main__"", fname, loader, pkg_name)\n[elided 17 identical lines from previous traceback]\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File ""<ipython-input-45-59183d86e462>"", line 1, in <module>\n    y = tf.nn.softmax(tf.matmul(x,W) + b)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 403, in binary_op_wrapper\n    return func(x, y, name=name)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 44, in add\n    return _op_def_lib.apply_op(""Add"", x=x, y=y, name=name)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 633, in apply_op\n    op_def=op_def)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1710, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 988, in __init__\n    self._traceback = _extract_stack()\n']"
496;2.0;0;34091877;;1;30;<python><csv><pandas><header>;How to add header row to a pandas DataFrame;"<p>I am reading a csv file into <code>pandas</code>. This csv file constists of four columns and some rows, but does not have a header row, which I want to add. I have been trying the following: </p>

<pre><code>Cov = pd.read_csv(""path/to/file.txt"", sep='\t')
Frame=pd.DataFrame([Cov], columns = [""Sequence"", ""Start"", ""End"", ""Coverage""])
Frame.to_csv(""path/to/file.txt"", sep='\t')
</code></pre>

<p>But when I apply the code, I get the following Error:</p>

<pre><code>ValueError: Shape of passed values is (1, 1), indices imply (4, 1)
</code></pre>

<p>What exactly does the error mean? And what would be a clean way in python to add a header row to my csv file/pandas df?</p>
";55423.0;"['Cov = pd.read_csv(""path/to/file.txt"", sep=\'\\t\')\nFrame=pd.DataFrame([Cov], columns = [""Sequence"", ""Start"", ""End"", ""Coverage""])\nFrame.to_csv(""path/to/file.txt"", sep=\'\\t\')\n', 'ValueError: Shape of passed values is (1, 1), indices imply (4, 1)\n']";"['pandas', 'Cov = pd.read_csv(""path/to/file.txt"", sep=\'\\t\')\nFrame=pd.DataFrame([Cov], columns = [""Sequence"", ""Start"", ""End"", ""Coverage""])\nFrame.to_csv(""path/to/file.txt"", sep=\'\\t\')\n', 'ValueError: Shape of passed values is (1, 1), indices imply (4, 1)\n']"
497;3.0;1;34962104;;1;32;<python><pandas><dataframe><python-3.5>;Pandas: How can I use the apply() function for a single column?;"<p>I have a pandas data frame with two columns. I need to change the values of the first column without affecting the second one and get back the whole data frame with just first column values changed. How can I do that using apply in pandas?</p>
";36873.0;[];[]
498;1.0;0;36519086;;1;29;<python><pandas><ipython>;Pandas: how to get rid of `Unnamed:` column in a dataframe;"<p>I have a situation wherein sometimes when I read a <code>csv</code> from <code>df</code> I get an unwanted index-like column named <code>unnamed:0</code>. This is very annoying! I have tried </p>

<pre><code>merge.to_csv('xy.df', mode = 'w', inplace=False)
</code></pre>

<p>which I thought was a solution to this, but I am still getting the <code>unnamed:0</code> column! Does anyone have an idea on this?</p>
";12496.0;"[""merge.to_csv('xy.df', mode = 'w', inplace=False)\n""]";"['csv', 'df', 'unnamed:0', ""merge.to_csv('xy.df', mode = 'w', inplace=False)\n"", 'unnamed:0']"
499;3.0;2;36921951;;1;26;<python><pandas><dataframe><boolean><filtering>;Truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all();"<p>Having issue filtering my result dataframe with an or condition. I want my result df to extract all column <em>var</em> values that are above 0.25 and below -0.25. This logic below gives me an ambiguous truth value however it work when I split this filtering in two separate operations. What is happening here? not sure where to use the suggested a.empty(), a.bool(), a.item(),a.any() or a.all().</p>

<pre><code> result = result[(result['var']&gt;0.25) or (result['var']&lt;-0.25)]
</code></pre>
";44469.0;"["" result = result[(result['var']>0.25) or (result['var']<-0.25)]\n""]";"["" result = result[(result['var']>0.25) or (result['var']<-0.25)]\n""]"
500;0.0;11;37078880;;1;29;<python><r><pandas><parallel-processing><mclapply>;Status of parallelization of pandas.apply();"<p>Over the last several years there have been several posts related to the <code>parallelization</code> of <code>pandas.apply()</code> or posts that describe problems that could be solved by structuring the data as a dataframe and using <code>pandas.apply()</code> if <code>parallelization</code> was implemented. </p>

<p>My question to the community of experts here - what is the status of this capability as <code>R</code> already has <code>mclapply</code>. </p>

<p>At the moment there is no clean standard solution. It is incredibly tedious to re-code entire functions and scripts to work with the proposed workarounds. </p>

<p><a href=""https://stackoverflow.com/questions/25510482/python-pandas-multiprocessing-apply"">Python Pandas Multiprocessing Apply</a></p>

<p><a href=""https://stackoverflow.com/questions/26187759/parallelize-apply-after-pandas-groupby"">Parallelize apply after pandas groupby</a></p>

<p><a href=""https://stackoverflow.com/questions/17054026/parallel-and-multicore-processing-in-r"">Parallel and Multicore Processing in R</a></p>

<p><a href=""https://stackoverflow.com/questions/5442910/python-multiprocessing-pool-map-for-multiple-arguments"">Python multiprocessing pool.map for multiple arguments</a></p>

<p><a href=""https://stackoverflow.com/questions/3842237/parallel-processing-in-python"">Parallel Processing in python</a></p>

<p><a href=""https://stackoverflow.com/questions/34031681/passing-kwargs-with-multiprocessing-pool-map"">passing kwargs with multiprocessing.pool.map</a></p>

<p><a href=""https://stackoverflow.com/questions/28943106/passing-arguments-and-manager-dict-to-pool-in-multiprocessing-in-python-2-7/28945479#28945479"">passing arguments and manager.dict to pool in multiprocessing in python 2.7</a></p>

<p><a href=""https://stackoverflow.com/questions/1704401/is-there-a-simple-process-based-parallel-map-for-python"">Is there a simple process-based parallel map for python?</a></p>

<p><a href=""https://stackoverflow.com/questions/29755787/pandas-with-rpy2-and-multiprocessing"">Pandas with rpy2 and multiprocessing</a></p>

<p><a href=""https://stackoverflow.com/questions/32787728/how-to-asynchronously-apply-function-via-spark-to-subsets-of-dataframe?s=5|0.1462"">How to asynchronously apply function via Spark to subsets of dataframe?</a></p>

<p><a href=""https://stackoverflow.com/questions/11728836/efficiently-applying-a-function-to-a-grouped-pandas-dataframe-in-parallel"">Efficiently applying a function to a grouped pandas DataFrame in parallel</a></p>

<p><a href=""https://stackoverflow.com/questions/31361721/python-dask-dataframe-support-for-trivially-parallelizable-row-apply"">python dask DataFrame, support for (trivially parallelizable) row apply?</a></p>

<p><a href=""https://stackoverflow.com/questions/22674950/python-multiprocessing-job-to-celery-task-but-attributeerror"">Python multiprocessing job to Celery task but AttributeError</a></p>

<p><a href=""https://stackoverflow.com/questions/27003430/parallelizing-apply-function-in-pandas-python-worked-on-groupby"">Parallelizing apply function in pandas python. worked on groupby</a></p>
";4502.0;[];['parallelization', 'pandas.apply()', 'pandas.apply()', 'parallelization', 'R', 'mclapply']
501;3.0;7;38250710;;1;34;<pandas><numpy><dataframe><machine-learning><scikit-learn>;How to split data into 3 sets (train, validation and test)?;"<p>I have a pandas dataframe and I wish to divide it to 3 separate sets. I know that using <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html"" rel=""noreferrer"">train_test_split</a> from <code>sklearn.cross_validation</code>, one can divide the data in two sets (train and test). However, I couldn't find any solution about splitting the data into three sets. Preferably, I'd like to have the indices of the original data. </p>

<p>I know that a workaround would be to use <code>train_test_split</code> two times and somehow adjust the indices. But is there a more standard / built-in way to split the data into 3 sets instead of 2?</p>
";12872.0;[];['sklearn.cross_validation', 'train_test_split']
502;9.0;3;42157944;;1;34;<python><regex><performance><parsing><pandas>;How can I speed up reading multiple files and putting the data into a dataframe?;"<p>I have a number of text files, say 50, that I need to read into a massive dataframe. At the moment, I am using the following steps.</p>

<ol>
<li>Read every file and check what the labels are. The information I need is often contained in the first few lines. The same labels just repeat for the rest of the file, with different types of data listed against them each time.</li>
<li>Create a dataframe with those labels.</li>
<li>Read the file again and fill the dataframe with values.</li>
<li>Concatenate that dataframe with a master dataframe.</li>
</ol>

<p>This works pretty well for files that are of the 100 KB size - a few minutes, but at 50 MB, it just takes hours, and is not practical. </p>

<p>How can I optimise my code? In particular -</p>

<ol>
<li>How can I identify what functions are taking the most time, which I need to optimise? Is it the reading of the file? Is it the writing to the dataframe? Where is my program spending time?</li>
<li>Should I consider multithreading or multiprocessing?</li>
<li>Can I improve the algorithm? 

<ul>
<li>Perhaps read the entire file in in one go into a list, rather than line by line,</li>
<li>Parse data in chunks/entire file, rather than line by line,</li>
<li>Assign data to the dataframe in chunks/one go, rather than row by row.</li>
</ul></li>
<li>Is there anything else that I can do to make my code execute faster?</li>
</ol>

<p>Here is an example code. My own code is a little more complex, as the text files are more complex such that I have to use about 10 regular expressions and multiple while loops to read the data in and allocate it to the right location in the right array. To keep the MWE simple, I haven't used repeating labels in the input files for the MWE either, so it would like I'm reading the file twice for no reason. I hope that makes sense!</p>

<pre><code>import re
import pandas as pd

df = pd.DataFrame()
paths = [""../gitignore/test1.txt"", ""../gitignore/test2.txt""]
reg_ex = re.compile('^(.+) (.+)\n')
# read all files to determine what indices are available
for path in paths:
    file_obj = open(path, 'r')
    print file_obj.readlines()

['a 1\n', 'b 2\n', 'end']
['c 3\n', 'd 4\n', 'end']

indices = []
for path in paths:
    index = []
    with open(path, 'r') as file_obj:
        line = True
        while line:
            try:
                line = file_obj.readline()
                match = reg_ex.match(line)
                index += match.group(1)
            except AttributeError:
                pass
    indices.append(index)
# read files again and put data into a master dataframe
for path, index in zip(paths, indices):
    subset_df = pd.DataFrame(index=index, columns=[""Number""])
    with open(path, 'r') as file_obj:
        line = True
        while line:
            try:
                line = file_obj.readline()
                match = reg_ex.match(line)
                subset_df.loc[[match.group(1)]] = match.group(2)
            except AttributeError:
                pass
    df = pd.concat([df, subset_df]).sort_index()
print df

  Number
a      1
b      2
c      3
d      4
</code></pre>

<p><strong>My input files:</strong></p>

<p>test1.txt</p>

<pre><code>a 1
b 2
end
</code></pre>

<p>test2.txt</p>

<pre><code>c 3
d 4
end
</code></pre>
";1476.0;"['import re\nimport pandas as pd\n\ndf = pd.DataFrame()\npaths = [""../gitignore/test1.txt"", ""../gitignore/test2.txt""]\nreg_ex = re.compile(\'^(.+) (.+)\\n\')\n# read all files to determine what indices are available\nfor path in paths:\n    file_obj = open(path, \'r\')\n    print file_obj.readlines()\n\n[\'a 1\\n\', \'b 2\\n\', \'end\']\n[\'c 3\\n\', \'d 4\\n\', \'end\']\n\nindices = []\nfor path in paths:\n    index = []\n    with open(path, \'r\') as file_obj:\n        line = True\n        while line:\n            try:\n                line = file_obj.readline()\n                match = reg_ex.match(line)\n                index += match.group(1)\n            except AttributeError:\n                pass\n    indices.append(index)\n# read files again and put data into a master dataframe\nfor path, index in zip(paths, indices):\n    subset_df = pd.DataFrame(index=index, columns=[""Number""])\n    with open(path, \'r\') as file_obj:\n        line = True\n        while line:\n            try:\n                line = file_obj.readline()\n                match = reg_ex.match(line)\n                subset_df.loc[[match.group(1)]] = match.group(2)\n            except AttributeError:\n                pass\n    df = pd.concat([df, subset_df]).sort_index()\nprint df\n\n  Number\na      1\nb      2\nc      3\nd      4\n', 'a 1\nb 2\nend\n', 'c 3\nd 4\nend\n']";"['import re\nimport pandas as pd\n\ndf = pd.DataFrame()\npaths = [""../gitignore/test1.txt"", ""../gitignore/test2.txt""]\nreg_ex = re.compile(\'^(.+) (.+)\\n\')\n# read all files to determine what indices are available\nfor path in paths:\n    file_obj = open(path, \'r\')\n    print file_obj.readlines()\n\n[\'a 1\\n\', \'b 2\\n\', \'end\']\n[\'c 3\\n\', \'d 4\\n\', \'end\']\n\nindices = []\nfor path in paths:\n    index = []\n    with open(path, \'r\') as file_obj:\n        line = True\n        while line:\n            try:\n                line = file_obj.readline()\n                match = reg_ex.match(line)\n                index += match.group(1)\n            except AttributeError:\n                pass\n    indices.append(index)\n# read files again and put data into a master dataframe\nfor path, index in zip(paths, indices):\n    subset_df = pd.DataFrame(index=index, columns=[""Number""])\n    with open(path, \'r\') as file_obj:\n        line = True\n        while line:\n            try:\n                line = file_obj.readline()\n                match = reg_ex.match(line)\n                subset_df.loc[[match.group(1)]] = match.group(2)\n            except AttributeError:\n                pass\n    df = pd.concat([df, subset_df]).sort_index()\nprint df\n\n  Number\na      1\nb      2\nc      3\nd      4\n', 'a 1\nb 2\nend\n', 'c 3\nd 4\nend\n']"
503;1.0;6;42347868;;1;24;<python>;Convert to date using formatters parameter in pandas to_string;"<p>I know there are several ways to convert a column to a date object, but what I am looking for is a way to do so while simultaneously formatting other columns. Say I have the following data frame:</p>

<pre><code>import pandas as pd

url = ""https://raw.github.com/pandas-dev/pandas/master/pandas/tests/data/tips.csv""
df = pd.read_csv(url)
df[""date""] = list(range(42005, 42005+len(df)))
</code></pre>

<p>What I'm trying to achieve is the ability to print these data using some formatting, so I might do something like the following:</p>

<pre><code>print(
  df
  .head(10)
  .to_string(
    formatters={""total_bill"": ""${:,.2f}"".format, 
                ""tip"": ""${:,.2f}"".format
    }
  )
)
</code></pre>

<p>But I also want to format the date in this step as well. I tried looking through <a href=""https://pyformat.info/#simple"" rel=""noreferrer"">here</a> for what I was looking for, but the datetime options didn't seem like they would work in what I'm trying to do, and building a custom option is a bit outside scope for my target audience.</p>

<p>Is it possible to do this in a simple manner?</p>
";839.0;"['import pandas as pd\n\nurl = ""https://raw.github.com/pandas-dev/pandas/master/pandas/tests/data/tips.csv""\ndf = pd.read_csv(url)\ndf[""date""] = list(range(42005, 42005+len(df)))\n', 'print(\n  df\n  .head(10)\n  .to_string(\n    formatters={""total_bill"": ""${:,.2f}"".format, \n                ""tip"": ""${:,.2f}"".format\n    }\n  )\n)\n']";"['import pandas as pd\n\nurl = ""https://raw.github.com/pandas-dev/pandas/master/pandas/tests/data/tips.csv""\ndf = pd.read_csv(url)\ndf[""date""] = list(range(42005, 42005+len(df)))\n', 'print(\n  df\n  .head(10)\n  .to_string(\n    formatters={""total_bill"": ""${:,.2f}"".format, \n                ""tip"": ""${:,.2f}"".format\n    }\n  )\n)\n']"
504;4.0;9;43423347;;1;23;<python><pandas>;why is blindly using df.copy() a bad idea to fix the SettingWithCopyWarning;"<p>There are countless questions about the dreaded <code>SettingWithCopyWarning</code></p>

<p>I've got a good handle on how it comes about. (Notice I said good, not great)</p>

<p>It happens when a dataframe <code>df</code> is ""attached"" to another dataframe via an attribute stored in <code>is_copy</code>.</p>

<p>Here's an example</p>

<pre><code>df = pd.DataFrame([[1]])

d1 = df[:]

d1.is_copy

&lt;weakref at 0x1115a4188; to 'DataFrame' at 0x1119bb0f0&gt;
</code></pre>

<p>We can either set that attribute to <code>None</code> or</p>

<pre><code>d1 = d1.copy()
</code></pre>

<p>I've seen devs like @Jeff and I can't remember who else, warn about doing that.  Citing that the <code>SettingWithCopyWarning</code> has a purpose.</p>

<p><strong><em>Question</em></strong><br>
Ok, so what is a concrete example that demonstrates why ignoring the warning by assigning a <code>copy</code> back to the original is a bad idea.</p>

<p>I'll define <em>""bad idea""</em> for clarification.</p>

<p><strong><em>Bad Idea</em></strong><br>
It is a <strong><em>bad idea</em></strong> to place code into production that will lead to getting a phone call in the middle of a Saturday night saying your code is broken and needs to be fixed.</p>

<p><strong>Now</strong> how can using <code>df = df.copy()</code> in order to bypass the <code>SettingWithCopyWarning</code> lead to getting that kind of phone call.  I want it spelled out because this is a source of confusion and I'm attempting to find clarity.  I want to see the edge case that blows up!</p>
";697.0;"[""df = pd.DataFrame([[1]])\n\nd1 = df[:]\n\nd1.is_copy\n\n<weakref at 0x1115a4188; to 'DataFrame' at 0x1119bb0f0>\n"", 'd1 = d1.copy()\n']";"['SettingWithCopyWarning', 'df', 'is_copy', ""df = pd.DataFrame([[1]])\n\nd1 = df[:]\n\nd1.is_copy\n\n<weakref at 0x1115a4188; to 'DataFrame' at 0x1119bb0f0>\n"", 'None', 'd1 = d1.copy()\n', 'SettingWithCopyWarning', 'copy', 'df = df.copy()', 'SettingWithCopyWarning']"
505;1.0;4;44380068;;1;21;<python><pandas><numpy><linear-regression><statsmodels>;Pandas rolling regression: alternatives to looping;"<p>I got good use out of pandas' <code>MovingOLS</code> class (source <a href=""https://github.com/pandas-dev/pandas/blob/v0.19.2/pandas/stats/ols.py"" rel=""nofollow noreferrer"">here</a>) within the deprecated <code>stats/ols</code> module.  Unfortunately, it was gutted completely with pandas 0.20.</p>

<p>The question of how to run rolling OLS regression in an efficient manner has been asked several times (<a href=""https://stackoverflow.com/questions/37317727/deprecated-rolling-window-option-in-ols-from-pandas-to-statsmodels"">here</a>, for instance), but phrased a little broadly and left without a great answer, in my view.</p>

<p>Here are my questions:  </p>

<ol>
<li><p>How can I best mimic the basic framework of pandas' <code>MovingOLS</code>?  The most attractive feature of this class was the ability to view multiple methods/attributes as separate time series--i.e. coefficients, r-squared, t-statistics, etc without needing to re-run regression.  For example, you could create something like <code>model = pd.MovingOLS(y, x)</code> and then call <code>.t_stat</code>, <code>.rmse</code>, <code>.std_err</code>, and the like.  In the example below, conversely, I don't see a way around being forced to compute each statistic separately.  Is there a method that doesn't involve creating sliding/rolling ""blocks"" (strides) and running regressions/using linear algebra to get model parameters for each?  </p></li>
<li><p>More broadly, what's going on under the hood in pandas that makes <code>rolling.apply</code> not able to take more complex functions?*  When you create a <code>.rolling</code> object, in layman's terms, what's going on internally--is it fundamentally different from looping over each window and creating a higher-dimensional array as I'm doing below?</p></li>
</ol>

<p>*Namely, <code>func</code> passed to <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.window.Rolling.apply.html"" rel=""nofollow noreferrer""><code>.apply</code></a>:</p>

<blockquote>
  <p>Must produce a single value from an ndarray input *args and **kwargs
  are passed to the function</p>
</blockquote>

<p>Here's where I'm currently at with some sample data, regressing percentage changes in the trade weighted dollar on interest rate spreads and the price of copper.  (This doesn't make a ton of sense; just picked these randomly.)  I've taken it out of a class-based implementation and tried to strip it down to a simpler script.</p>

<pre><code>from datetime import date
from pandas_datareader.data import DataReader
import statsmodels.formula.api as smf

syms = {'TWEXBMTH' : 'usd', 
        'T10Y2YM' : 'term_spread', 
        'PCOPPUSDM' : 'copper'
       }

start = date(2000, 1, 1)
data = (DataReader(syms.keys(), 'fred', start)
        .pct_change()
        .dropna())
data = data.rename(columns = syms)
data = data.assign(intercept = 1.) # required by statsmodels OLS

def sliding_windows(x, window):
    """"""Create rolling/sliding windows of length ~window~.

    Given an array of shape (y, z), it will return ""blocks"" of shape
    (x - window + 1, window, z).""""""

    return np.array([x[i:i + window] for i 
                    in range(0, x.shape[0] - window + 1)])

data.head(3)
Out[33]: 
                 usd  term_spread    copper  intercept
DATE                                                  
2000-02-01  0.012573    -1.409091 -0.019972        1.0
2000-03-01 -0.000079     2.000000 -0.037202        1.0
2000-04-01  0.005642     0.518519 -0.033275        1.0

window = 36
wins = sliding_windows(data.values, window=window)
y, x = wins[:, :, 0], wins[:, :, 1:]

coefs = []

for endog, exog in zip(y, x):
    model = smf.OLS(endog, exog).fit()
        # The full set of model attributes gets lost with each loop
    coefs.append(model.params)

df = pd.DataFrame(coefs, columns=data.iloc[:, 1:].columns,
                  index=data.index[window - 1:])

df.head(3) # rolling 36m coefficients
Out[70]: 
            term_spread    copper  intercept
DATE                                        
2003-01-01    -0.000122 -0.018426   0.001937
2003-02-01     0.000391 -0.015740   0.001597
2003-03-01     0.000655 -0.016811   0.001546
</code></pre>
";700.0;"['from datetime import date\nfrom pandas_datareader.data import DataReader\nimport statsmodels.formula.api as smf\n\nsyms = {\'TWEXBMTH\' : \'usd\', \n        \'T10Y2YM\' : \'term_spread\', \n        \'PCOPPUSDM\' : \'copper\'\n       }\n\nstart = date(2000, 1, 1)\ndata = (DataReader(syms.keys(), \'fred\', start)\n        .pct_change()\n        .dropna())\ndata = data.rename(columns = syms)\ndata = data.assign(intercept = 1.) # required by statsmodels OLS\n\ndef sliding_windows(x, window):\n    """"""Create rolling/sliding windows of length ~window~.\n\n    Given an array of shape (y, z), it will return ""blocks"" of shape\n    (x - window + 1, window, z).""""""\n\n    return np.array([x[i:i + window] for i \n                    in range(0, x.shape[0] - window + 1)])\n\ndata.head(3)\nOut[33]: \n                 usd  term_spread    copper  intercept\nDATE                                                  \n2000-02-01  0.012573    -1.409091 -0.019972        1.0\n2000-03-01 -0.000079     2.000000 -0.037202        1.0\n2000-04-01  0.005642     0.518519 -0.033275        1.0\n\nwindow = 36\nwins = sliding_windows(data.values, window=window)\ny, x = wins[:, :, 0], wins[:, :, 1:]\n\ncoefs = []\n\nfor endog, exog in zip(y, x):\n    model = smf.OLS(endog, exog).fit()\n        # The full set of model attributes gets lost with each loop\n    coefs.append(model.params)\n\ndf = pd.DataFrame(coefs, columns=data.iloc[:, 1:].columns,\n                  index=data.index[window - 1:])\n\ndf.head(3) # rolling 36m coefficients\nOut[70]: \n            term_spread    copper  intercept\nDATE                                        \n2003-01-01    -0.000122 -0.018426   0.001937\n2003-02-01     0.000391 -0.015740   0.001597\n2003-03-01     0.000655 -0.016811   0.001546\n']";"['MovingOLS', 'stats/ols', 'MovingOLS', 'model = pd.MovingOLS(y, x)', '.t_stat', '.rmse', '.std_err', 'rolling.apply', '.rolling', 'func', '.apply', 'from datetime import date\nfrom pandas_datareader.data import DataReader\nimport statsmodels.formula.api as smf\n\nsyms = {\'TWEXBMTH\' : \'usd\', \n        \'T10Y2YM\' : \'term_spread\', \n        \'PCOPPUSDM\' : \'copper\'\n       }\n\nstart = date(2000, 1, 1)\ndata = (DataReader(syms.keys(), \'fred\', start)\n        .pct_change()\n        .dropna())\ndata = data.rename(columns = syms)\ndata = data.assign(intercept = 1.) # required by statsmodels OLS\n\ndef sliding_windows(x, window):\n    """"""Create rolling/sliding windows of length ~window~.\n\n    Given an array of shape (y, z), it will return ""blocks"" of shape\n    (x - window + 1, window, z).""""""\n\n    return np.array([x[i:i + window] for i \n                    in range(0, x.shape[0] - window + 1)])\n\ndata.head(3)\nOut[33]: \n                 usd  term_spread    copper  intercept\nDATE                                                  \n2000-02-01  0.012573    -1.409091 -0.019972        1.0\n2000-03-01 -0.000079     2.000000 -0.037202        1.0\n2000-04-01  0.005642     0.518519 -0.033275        1.0\n\nwindow = 36\nwins = sliding_windows(data.values, window=window)\ny, x = wins[:, :, 0], wins[:, :, 1:]\n\ncoefs = []\n\nfor endog, exog in zip(y, x):\n    model = smf.OLS(endog, exog).fit()\n        # The full set of model attributes gets lost with each loop\n    coefs.append(model.params)\n\ndf = pd.DataFrame(coefs, columns=data.iloc[:, 1:].columns,\n                  index=data.index[window - 1:])\n\ndf.head(3) # rolling 36m coefficients\nOut[70]: \n            term_spread    copper  intercept\nDATE                                        \n2003-01-01    -0.000122 -0.018426   0.001937\n2003-02-01     0.000391 -0.015740   0.001597\n2003-03-01     0.000655 -0.016811   0.001546\n']"
506;;2;7779260;7776679.0;2;38;;;"<p>The <code>append</code> function has an optional argument <code>ignore_index</code> which you should use here to join the records together, since the index isn't meaningful for your application.</p>
";;[];['append', 'ignore_index']
507;;6;7837947;7837722.0;2;111;;;"<p>Pandas is based on NumPy arrays.
The key to speed with NumPy arrays is to perform your operations on the whole array at once, never row-by-row or item-by-item.</p>

<p>For example, if <code>close</code> is a 1-d array, and you want the day-over-day percent change,</p>

<pre><code>pct_change = close[1:]/close[:-1]
</code></pre>

<p>This computes the entire array of percent changes as one statement, instead of </p>

<pre><code>pct_change = []
for row in close:
    pct_change.append(...)
</code></pre>

<p>So try to avoid the Python loop <code>for i, row in enumerate(...)</code> entirely, and
think about how to perform your calculations with operations on the entire array (or dataframe) as a whole, rather than row-by-row.</p>
";;['pct_change = close[1:]/close[:-1]\n', 'pct_change = []\nfor row in close:\n    pct_change.append(...)\n'];['close', 'pct_change = close[1:]/close[:-1]\n', 'pct_change = []\nfor row in close:\n    pct_change.append(...)\n', 'for i, row in enumerate(...)']
508;;4;7849789;7837722.0;2;63;;;"<p>You can loop through the rows by transposing and then calling iteritems:</p>

<pre><code>for date, row in df.T.iteritems():
   # do some logic here
</code></pre>

<p>I am not certain about efficiency in that case. To get the best possible performance in an iterative algorithm, you might want to explore writing it in <a href=""http://cython.org"">Cython</a>, so you could do something like:</p>

<pre><code>def my_algo(ndarray[object] dates, ndarray[float64_t] open,
            ndarray[float64_t] low, ndarray[float64_t] high,
            ndarray[float64_t] close, ndarray[float64_t] volume):
    cdef:
        Py_ssize_t i, n
        float64_t foo
    n = len(dates)

    for i from 0 &lt;= i &lt; n:
        foo = close[i] - open[i] # will be extremely fast
</code></pre>

<p>I would recommend writing the algorithm in pure Python first, make sure it works and see how fast it is-- if it's not fast enough, convert things to Cython like this with minimal work to get something that's about as fast as hand-coded C/C++.</p>
";;['for date, row in df.T.iteritems():\n   # do some logic here\n', 'def my_algo(ndarray[object] dates, ndarray[float64_t] open,\n            ndarray[float64_t] low, ndarray[float64_t] high,\n            ndarray[float64_t] close, ndarray[float64_t] volume):\n    cdef:\n        Py_ssize_t i, n\n        float64_t foo\n    n = len(dates)\n\n    for i from 0 <= i < n:\n        foo = close[i] - open[i] # will be extremely fast\n'];['for date, row in df.T.iteritems():\n   # do some logic here\n', 'def my_algo(ndarray[object] dates, ndarray[float64_t] open,\n            ndarray[float64_t] low, ndarray[float64_t] high,\n            ndarray[float64_t] close, ndarray[float64_t] volume):\n    cdef:\n        Py_ssize_t i, n\n        float64_t foo\n    n = len(dates)\n\n    for i from 0 <= i < n:\n        foo = close[i] - open[i] # will be extremely fast\n']
509;;2;8916746;8916302.0;2;34;;;"<p>I encourage you to pose these questions on the <a href=""http://groups.google.com/group/pystatsmodels"" rel=""noreferrer"">mailing list</a>, but in any case, it's still a very much low level affair working with the underlying NumPy arrays. For example, to select rows where the value in any column exceed, say, 1.5 in this example:</p>

<pre><code>In [11]: df
Out[11]: 
            A        B        C        D      
2000-01-03 -0.59885 -0.18141 -0.68828 -0.77572
2000-01-04  0.83935  0.15993  0.95911 -1.12959
2000-01-05  2.80215 -0.10858 -1.62114 -0.20170
2000-01-06  0.71670 -0.26707  1.36029  1.74254
2000-01-07 -0.45749  0.22750  0.46291 -0.58431
2000-01-10 -0.78702  0.44006 -0.36881 -0.13884
2000-01-11  0.79577 -0.09198  0.14119  0.02668
2000-01-12 -0.32297  0.62332  1.93595  0.78024
2000-01-13  1.74683 -1.57738 -0.02134  0.11596
2000-01-14 -0.55613  0.92145 -0.22832  1.56631
2000-01-17 -0.55233 -0.28859 -1.18190 -0.80723
2000-01-18  0.73274  0.24387  0.88146 -0.94490
2000-01-19  0.56644 -0.49321  1.17584 -0.17585
2000-01-20  1.56441  0.62331 -0.26904  0.11952
2000-01-21  0.61834  0.17463 -1.62439  0.99103
2000-01-24  0.86378 -0.68111 -0.15788 -0.16670
2000-01-25 -1.12230 -0.16128  1.20401  1.08945
2000-01-26 -0.63115  0.76077 -0.92795 -2.17118
2000-01-27  1.37620 -1.10618 -0.37411  0.73780
2000-01-28 -1.40276  1.98372  1.47096 -1.38043
2000-01-31  0.54769  0.44100 -0.52775  0.84497
2000-02-01  0.12443  0.32880 -0.71361  1.31778
2000-02-02 -0.28986 -0.63931  0.88333 -2.58943
2000-02-03  0.54408  1.17928 -0.26795 -0.51681
2000-02-04 -0.07068 -1.29168 -0.59877 -1.45639
2000-02-07 -0.65483 -0.29584 -0.02722  0.31270
2000-02-08 -0.18529 -0.18701 -0.59132 -1.15239
2000-02-09 -2.28496  0.36352  1.11596  0.02293
2000-02-10  0.51054  0.97249  1.74501  0.20525
2000-02-11  0.10100  0.27722  0.65843  1.73591

In [12]: df[(df.values &gt; 1.5).any(1)]
Out[12]: 
            A       B       C        D     
2000-01-05  2.8021 -0.1086 -1.62114 -0.2017
2000-01-06  0.7167 -0.2671  1.36029  1.7425
2000-01-12 -0.3230  0.6233  1.93595  0.7802
2000-01-13  1.7468 -1.5774 -0.02134  0.1160
2000-01-14 -0.5561  0.9215 -0.22832  1.5663
2000-01-20  1.5644  0.6233 -0.26904  0.1195
2000-01-28 -1.4028  1.9837  1.47096 -1.3804
2000-02-10  0.5105  0.9725  1.74501  0.2052
2000-02-11  0.1010  0.2772  0.65843  1.7359
</code></pre>

<p>Multiple conditions have to be combined using <code>&amp;</code> or <code>|</code> (and parentheses!):</p>

<pre><code>In [13]: df[(df['A'] &gt; 1) | (df['B'] &lt; -1)]
Out[13]: 
            A        B       C        D     
2000-01-05  2.80215 -0.1086 -1.62114 -0.2017
2000-01-13  1.74683 -1.5774 -0.02134  0.1160
2000-01-20  1.56441  0.6233 -0.26904  0.1195
2000-01-27  1.37620 -1.1062 -0.37411  0.7378
2000-02-04 -0.07068 -1.2917 -0.59877 -1.4564
</code></pre>

<p>I'd be very interested to have some kind of query API to make these kinds of things easier</p>
";;"['In [11]: df\nOut[11]: \n            A        B        C        D      \n2000-01-03 -0.59885 -0.18141 -0.68828 -0.77572\n2000-01-04  0.83935  0.15993  0.95911 -1.12959\n2000-01-05  2.80215 -0.10858 -1.62114 -0.20170\n2000-01-06  0.71670 -0.26707  1.36029  1.74254\n2000-01-07 -0.45749  0.22750  0.46291 -0.58431\n2000-01-10 -0.78702  0.44006 -0.36881 -0.13884\n2000-01-11  0.79577 -0.09198  0.14119  0.02668\n2000-01-12 -0.32297  0.62332  1.93595  0.78024\n2000-01-13  1.74683 -1.57738 -0.02134  0.11596\n2000-01-14 -0.55613  0.92145 -0.22832  1.56631\n2000-01-17 -0.55233 -0.28859 -1.18190 -0.80723\n2000-01-18  0.73274  0.24387  0.88146 -0.94490\n2000-01-19  0.56644 -0.49321  1.17584 -0.17585\n2000-01-20  1.56441  0.62331 -0.26904  0.11952\n2000-01-21  0.61834  0.17463 -1.62439  0.99103\n2000-01-24  0.86378 -0.68111 -0.15788 -0.16670\n2000-01-25 -1.12230 -0.16128  1.20401  1.08945\n2000-01-26 -0.63115  0.76077 -0.92795 -2.17118\n2000-01-27  1.37620 -1.10618 -0.37411  0.73780\n2000-01-28 -1.40276  1.98372  1.47096 -1.38043\n2000-01-31  0.54769  0.44100 -0.52775  0.84497\n2000-02-01  0.12443  0.32880 -0.71361  1.31778\n2000-02-02 -0.28986 -0.63931  0.88333 -2.58943\n2000-02-03  0.54408  1.17928 -0.26795 -0.51681\n2000-02-04 -0.07068 -1.29168 -0.59877 -1.45639\n2000-02-07 -0.65483 -0.29584 -0.02722  0.31270\n2000-02-08 -0.18529 -0.18701 -0.59132 -1.15239\n2000-02-09 -2.28496  0.36352  1.11596  0.02293\n2000-02-10  0.51054  0.97249  1.74501  0.20525\n2000-02-11  0.10100  0.27722  0.65843  1.73591\n\nIn [12]: df[(df.values > 1.5).any(1)]\nOut[12]: \n            A       B       C        D     \n2000-01-05  2.8021 -0.1086 -1.62114 -0.2017\n2000-01-06  0.7167 -0.2671  1.36029  1.7425\n2000-01-12 -0.3230  0.6233  1.93595  0.7802\n2000-01-13  1.7468 -1.5774 -0.02134  0.1160\n2000-01-14 -0.5561  0.9215 -0.22832  1.5663\n2000-01-20  1.5644  0.6233 -0.26904  0.1195\n2000-01-28 -1.4028  1.9837  1.47096 -1.3804\n2000-02-10  0.5105  0.9725  1.74501  0.2052\n2000-02-11  0.1010  0.2772  0.65843  1.7359\n', ""In [13]: df[(df['A'] > 1) | (df['B'] < -1)]\nOut[13]: \n            A        B       C        D     \n2000-01-05  2.80215 -0.1086 -1.62114 -0.2017\n2000-01-13  1.74683 -1.5774 -0.02134  0.1160\n2000-01-20  1.56441  0.6233 -0.26904  0.1195\n2000-01-27  1.37620 -1.1062 -0.37411  0.7378\n2000-02-04 -0.07068 -1.2917 -0.59877 -1.4564\n""]";"['In [11]: df\nOut[11]: \n            A        B        C        D      \n2000-01-03 -0.59885 -0.18141 -0.68828 -0.77572\n2000-01-04  0.83935  0.15993  0.95911 -1.12959\n2000-01-05  2.80215 -0.10858 -1.62114 -0.20170\n2000-01-06  0.71670 -0.26707  1.36029  1.74254\n2000-01-07 -0.45749  0.22750  0.46291 -0.58431\n2000-01-10 -0.78702  0.44006 -0.36881 -0.13884\n2000-01-11  0.79577 -0.09198  0.14119  0.02668\n2000-01-12 -0.32297  0.62332  1.93595  0.78024\n2000-01-13  1.74683 -1.57738 -0.02134  0.11596\n2000-01-14 -0.55613  0.92145 -0.22832  1.56631\n2000-01-17 -0.55233 -0.28859 -1.18190 -0.80723\n2000-01-18  0.73274  0.24387  0.88146 -0.94490\n2000-01-19  0.56644 -0.49321  1.17584 -0.17585\n2000-01-20  1.56441  0.62331 -0.26904  0.11952\n2000-01-21  0.61834  0.17463 -1.62439  0.99103\n2000-01-24  0.86378 -0.68111 -0.15788 -0.16670\n2000-01-25 -1.12230 -0.16128  1.20401  1.08945\n2000-01-26 -0.63115  0.76077 -0.92795 -2.17118\n2000-01-27  1.37620 -1.10618 -0.37411  0.73780\n2000-01-28 -1.40276  1.98372  1.47096 -1.38043\n2000-01-31  0.54769  0.44100 -0.52775  0.84497\n2000-02-01  0.12443  0.32880 -0.71361  1.31778\n2000-02-02 -0.28986 -0.63931  0.88333 -2.58943\n2000-02-03  0.54408  1.17928 -0.26795 -0.51681\n2000-02-04 -0.07068 -1.29168 -0.59877 -1.45639\n2000-02-07 -0.65483 -0.29584 -0.02722  0.31270\n2000-02-08 -0.18529 -0.18701 -0.59132 -1.15239\n2000-02-09 -2.28496  0.36352  1.11596  0.02293\n2000-02-10  0.51054  0.97249  1.74501  0.20525\n2000-02-11  0.10100  0.27722  0.65843  1.73591\n\nIn [12]: df[(df.values > 1.5).any(1)]\nOut[12]: \n            A       B       C        D     \n2000-01-05  2.8021 -0.1086 -1.62114 -0.2017\n2000-01-06  0.7167 -0.2671  1.36029  1.7425\n2000-01-12 -0.3230  0.6233  1.93595  0.7802\n2000-01-13  1.7468 -1.5774 -0.02134  0.1160\n2000-01-14 -0.5561  0.9215 -0.22832  1.5663\n2000-01-20  1.5644  0.6233 -0.26904  0.1195\n2000-01-28 -1.4028  1.9837  1.47096 -1.3804\n2000-02-10  0.5105  0.9725  1.74501  0.2052\n2000-02-11  0.1010  0.2772  0.65843  1.7359\n', '&', '|', ""In [13]: df[(df['A'] > 1) | (df['B'] < -1)]\nOut[13]: \n            A        B       C        D     \n2000-01-05  2.80215 -0.1086 -1.62114 -0.2017\n2000-01-13  1.74683 -1.5774 -0.02134  0.1160\n2000-01-20  1.56441  0.6233 -0.26904  0.1195\n2000-01-27  1.37620 -1.1062 -0.37411  0.7378\n2000-02-04 -0.07068 -1.2917 -0.59877 -1.4564\n""]"
510;;8;8992714;8991709.0;2;185;;;"<p>The reason pandas is faster is because I came up with a better algorithm, which is implemented very carefully using <a href=""https://github.com/attractivechaos/klib"" rel=""noreferrer"">a fast hash table implementation - klib</a> and in C/<a href=""http://cython.org/"" rel=""noreferrer"">Cython</a> to avoid the Python interpreter overhead for the non-vectorizable parts. The algorithm is described in some detail in my presentation: <a href=""http://wesmckinney.com/blog/nycpython-1102012-a-look-inside-pandas-design-and-development/"" rel=""noreferrer""><em>A look inside pandas design and development</em></a>.</p>

<p>The comparison with <code>data.table</code> is actually a bit interesting because the whole point of R's <code>data.table</code> is that it contains <em>pre-computed indexes</em> for various columns to accelerate operations like data selection and merges. In this case (database joins) pandas' DataFrame contains <em>no pre-computed information</em> that is being used for the merge, so to speak it's a ""cold"" merge. If I had stored the factorized versions of the join keys, the join would be significantly faster - as factorizing is the biggest bottleneck for this algorithm.</p>

<p>I should also add that the internal design of pandas' DataFrame is much more amenable to these kinds of operations than R's data.frame (which is just a list of arrays internally).</p>
";;[];['data.table', 'data.table']
511;;6;8997908;8991709.0;2;93;;;"<p>It looks like Wes may have discovered a known issue in <code>data.table</code> when the number of unique strings (<em>levels</em>) is large: 10,000.</p>

<p>Does <code>Rprof()</code> reveal most of the time spent in the call <code>sortedmatch(levels(i[[lc]]), levels(x[[rc]])</code>?  This isn't really the join itself (the algorithm), but a preliminary step.</p>

<p>Recent efforts have gone into allowing character columns in keys, which should resolve that issue by integrating more closely with R's own global string hash table. Some benchmark results are already reported by <code>test.data.table()</code> but that code isn't hooked up yet to replace the levels to levels match.</p>

<p>Are pandas merges faster than <code>data.table</code> for regular integer columns?  That should be a way to isolate the algorithm itself vs factor issues.</p>

<p>Also, <code>data.table</code> has <em>time series merge</em> in mind. Two aspects to that: i) multi column <em>ordered</em> keys such as (id,datetime) ii) fast prevailing join (<code>roll=TRUE</code>) a.k.a. last observation carried forward.</p>

<p>I'll need some time to confirm as it's the first I've seen of the comparison to <code>data.table</code> as presented.</p>

<hr>

<p><strong>UPDATE from data.table v1.8.0 released July 2012</strong></p>

<ul>
<li>Internal function sortedmatch() removed and replaced with chmatch()
       when matching i levels to x levels for columns of type 'factor'. This
       preliminary step was causing a (known) significant slowdown when the number
       of levels of a factor column was large (e.g. >10,000). Exacerbated in
       tests of joining four such columns, as demonstrated by Wes McKinney
       (author of Python package Pandas). Matching 1 million strings of which
       of which 600,000 are unique is now reduced from 16s to 0.5s, for example.</li>
</ul>

<p>also in that release was :</p>

<ul>
<li><p>character columns are now allowed in keys and are preferred to
factor. data.table() and setkey() no longer coerce character to
factor. Factors are still supported. Implements FR#1493, FR#1224
and (partially) FR#951.</p></li>
<li><p>New functions chmatch() and %chin%, faster versions of match()
and %in% for character vectors. R's internal string cache is
utilised (no hash table is built). They are about 4 times faster
than match() on the example in ?chmatch.</p></li>
</ul>

<p>As of Sep 2013 data.table is v1.8.10 on CRAN and we're working on v1.9.0. <strong><a href=""https://r-forge.r-project.org/scm/viewvc.php/pkg/NEWS?view=markup&amp;root=datatable"" rel=""noreferrer"">NEWS</a></strong> is updated live.</p>

<hr>

<p>But as I wrote originally, above :</p>

<blockquote>
  <p><code>data.table</code> has <em>time series merge</em> in mind. Two aspects to that: i)
  multi column <em>ordered</em> keys such as (id,datetime) ii) fast prevailing
  join (<code>roll=TRUE</code>) a.k.a. last observation carried forward.</p>
</blockquote>

<p>So the Pandas equi join of two character columns is probably still faster than data.table. Since it sounds like it hashes the combined two columns. data.table doesn't hash the key because it has prevailing ordered joins in mind. A ""key"" in data.table is literally just the sort order (similar to a clustered index in SQL; i.e., that's how the data is ordered in RAM). On the list is to add secondary keys, for example.</p>

<p>In summary, the glaring speed difference highlighted by this particular two-character-column test with over 10,000 unique strings shouldn't be as bad now, since the known problem has been fixed.</p>
";;[];['data.table', 'Rprof()', 'sortedmatch(levels(i[[lc]]), levels(x[[rc]])', 'test.data.table()', 'data.table', 'data.table', 'roll=TRUE', 'data.table', 'data.table', 'roll=TRUE']
512;;1;9620832;9588331.0;2;12;;;"<p>Assuming that you have a file called 2010.csv with contents</p>

<pre><code>category,value
AB,100.00
AB,200.00
AC,150.00
AD,500.00
</code></pre>

<p>Then, using the ability to apply <a href=""http://pandas.pydata.org/pandas-docs/dev/groupby.html#applying-multiple-functions-at-once"" rel=""noreferrer"">multiple aggregation functions following a groupby</a>, you can say:</p>

<pre><code>import pandas
data_2010 = pandas.read_csv(""/path/to/2010.csv"")
data_2010.groupby(""category"").agg([len, sum])
</code></pre>

<p>You should get a result that looks something like</p>

<pre><code>          value     
            len  sum
category            
AB            2  300
AC            1  150
AD            1  500
</code></pre>

<p>Note that Wes will likely come by to point out that sum is optimized and that you should probably use np.sum.</p>
";;"['category,value\nAB,100.00\nAB,200.00\nAC,150.00\nAD,500.00\n', 'import pandas\ndata_2010 = pandas.read_csv(""/path/to/2010.csv"")\ndata_2010.groupby(""category"").agg([len, sum])\n', '          value     \n            len  sum\ncategory            \nAB            2  300\nAC            1  150\nAD            1  500\n']";"['category,value\nAB,100.00\nAB,200.00\nAC,150.00\nAD,500.00\n', 'import pandas\ndata_2010 = pandas.read_csv(""/path/to/2010.csv"")\ndata_2010.groupby(""category"").agg([len, sum])\n', '          value     \n            len  sum\ncategory            \nAB            2  300\nAC            1  150\nAD            1  500\n']"
513;;0;9623878;9588331.0;2;14;;;"<p>Thanks, Jeff. It is possible to do this using <code>pivot_table</code> for those interested:</p>

<pre><code>In [8]: df
Out[8]: 
  category  value
0       AB    100
1       AB    200
2       AC    150
3       AD    500

In [9]: df.pivot_table(rows='category', aggfunc=[len, np.sum])
Out[9]: 
            len    sum
          value  value
category              
AB            2    300
AC            1    150
AD            1    500
</code></pre>

<p>Note that the result's columns are hierarchically indexed. If you had multiple data columns, you would get a result like this:</p>

<pre><code>In [12]: df
Out[12]: 
  category  value  value2
0       AB    100       5
1       AB    200       5
2       AC    150       5
3       AD    500       5

In [13]: df.pivot_table(rows='category', aggfunc=[len, np.sum])
Out[13]: 
            len            sum        
          value  value2  value  value2
category                              
AB            2       2    300      10
AC            1       1    150       5
AD            1       1    500       5
</code></pre>

<p>The main reason to use <code>__builtin__.sum</code> vs. <code>np.sum</code> is that you get NA-handling from the latter. Probably could intercept the Python built-in, will make a note about that now.</p>
";;"[""In [8]: df\nOut[8]: \n  category  value\n0       AB    100\n1       AB    200\n2       AC    150\n3       AD    500\n\nIn [9]: df.pivot_table(rows='category', aggfunc=[len, np.sum])\nOut[9]: \n            len    sum\n          value  value\ncategory              \nAB            2    300\nAC            1    150\nAD            1    500\n"", ""In [12]: df\nOut[12]: \n  category  value  value2\n0       AB    100       5\n1       AB    200       5\n2       AC    150       5\n3       AD    500       5\n\nIn [13]: df.pivot_table(rows='category', aggfunc=[len, np.sum])\nOut[13]: \n            len            sum        \n          value  value2  value  value2\ncategory                              \nAB            2       2    300      10\nAC            1       1    150       5\nAD            1       1    500       5\n""]";"['pivot_table', ""In [8]: df\nOut[8]: \n  category  value\n0       AB    100\n1       AB    200\n2       AC    150\n3       AD    500\n\nIn [9]: df.pivot_table(rows='category', aggfunc=[len, np.sum])\nOut[9]: \n            len    sum\n          value  value\ncategory              \nAB            2    300\nAC            1    150\nAD            1    500\n"", ""In [12]: df\nOut[12]: \n  category  value  value2\n0       AB    100       5\n1       AB    200       5\n2       AC    150       5\n3       AD    500       5\n\nIn [13]: df.pivot_table(rows='category', aggfunc=[len, np.sum])\nOut[13]: \n            len            sum        \n          value  value2  value  value2\ncategory                              \nAB            2       2    300      10\nAC            1       1    150       5\nAD            1       1    500       5\n"", '__builtin__.sum', 'np.sum']"
514;;6;9652858;9652832.0;2;49;;;"<p>The documentation lists a <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.from_csv.html"" rel=""noreferrer"">.from_csv</a> function that appears to do what you want:</p>

<pre><code>DataFrame.from_csv('c:/~/trainSetRel3.txt', sep='\t')
</code></pre>

<p>If you have a header, you can pass <code>header=0</code>.</p>

<pre><code>DataFrame.from_csv('c:/~/trainSetRel3.txt', sep='\t', header=0)
</code></pre>
";;"[""DataFrame.from_csv('c:/~/trainSetRel3.txt', sep='\\t')\n"", ""DataFrame.from_csv('c:/~/trainSetRel3.txt', sep='\\t', header=0)\n""]";"[""DataFrame.from_csv('c:/~/trainSetRel3.txt', sep='\\t')\n"", 'header=0', ""DataFrame.from_csv('c:/~/trainSetRel3.txt', sep='\\t', header=0)\n""]"
515;;1;9656288;9652832.0;2;35;;;"<p>Use <code>read_table(filepath)</code>. The default separator is tab</p>
";;[];['read_table(filepath)']
516;;1;9762084;9758450.0;2;71;;;"<p>How about:</p>

<pre><code>subset = data_set[['data_date', 'data_1', 'data_2']]
tuples = [tuple(x) for x in subset.values]
</code></pre>
";;"[""subset = data_set[['data_date', 'data_1', 'data_2']]\ntuples = [tuple(x) for x in subset.values]\n""]";"[""subset = data_set[['data_date', 'data_1', 'data_2']]\ntuples = [tuple(x) for x in subset.values]\n""]"
517;;3;10202789;10202570.0;2;97;;;"<p>You just need the <code>argmax()</code> (<em>now called <code>idxmax</code></em>) function. It's straightforward:</p>

<pre><code>&gt;&gt;&gt; import pandas
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; df = pandas.DataFrame(np.random.randn(5,3),columns=['A','B','C'])
&gt;&gt;&gt; df
          A         B         C
0  1.232853 -1.979459 -0.573626
1  0.140767  0.394940  1.068890
2  0.742023  1.343977 -0.579745
3  2.125299 -0.649328 -0.211692
4 -0.187253  1.908618 -1.862934
&gt;&gt;&gt; df['A'].argmax()
3
&gt;&gt;&gt; df['B'].argmax()
4
&gt;&gt;&gt; df['C'].argmax()
1
</code></pre>

<p>This function was updated to the name <code>idxmax</code> in the Pandas API, though as of Pandas 0.16, <code>argmax</code> still exists and performs the same function (though appears to run more slowly than <code>idxmax</code>).</p>

<p>You can also just use <code>numpy.argmax</code>, such as <code>numpy.argmax(df['A'])</code> -- it provides the same thing as either of the two <code>pandas</code> functions, and appears at least as fast as <code>idxmax</code> in cursory observations.</p>

<p>Previously (as noted in the comments) it appeared that <code>argmax</code> would exist as a separate function which provided the <em>integer position</em> within the index of the row location of the maximum element. For example, if you have string values as your index labels, like rows 'a' through 'e', you might want to know that the max occurs in row 4 (not row 'd'). However, in pandas 0.16, all of the listed methods above only provide the <em>label</em> from the <code>Index</code> for the row in question, and if you want the position integer of that label within the <code>Index</code> you have to get it manually (which can be tricky now that duplicate row labels are allowed).</p>

<p>In general, I think the move to <code>idxmax</code>-like behavior for all three of the approaches (<code>argmax</code>, which still exists, <code>idxmax</code>, and <code>numpy.argmax</code>) is a bad thing, since it is very common to require the positional integer location of a maximum, perhaps even more common than desiring the <em>label</em> of that positional location within some index, especially in applications where duplicate row labels are common.</p>

<p>For example, consider this toy <code>DataFrame</code> with a duplicate row label:</p>

<pre><code>In [19]: dfrm
Out[19]: 
          A         B         C
a  0.143693  0.653810  0.586007
b  0.623582  0.312903  0.919076
c  0.165438  0.889809  0.000967
d  0.308245  0.787776  0.571195
e  0.870068  0.935626  0.606911
f  0.037602  0.855193  0.728495
g  0.605366  0.338105  0.696460
h  0.000000  0.090814  0.963927
i  0.688343  0.188468  0.352213
i  0.879000  0.105039  0.900260

In [20]: dfrm['A'].idxmax()
Out[20]: 'i'

In [21]: dfrm.ix[dfrm['A'].idxmax()]
Out[21]: 
          A         B         C
i  0.688343  0.188468  0.352213
i  0.879000  0.105039  0.900260
</code></pre>

<p>So here a naive use of <code>idxmax</code> is not sufficient, whereas the old form of <code>argmax</code> would correctly provide the <em>positional</em> location of the max row (in this case, position 9).</p>

<p>This is exactly one of those nasty kinds of bug-prone behaviors in dynamically typed languages that makes this sort of thing so unfortunate, and worth beating a dead horse over. If you are writing systems code and your system suddenly gets used on some data sets that are not cleaned properly before being joined, it's very easy to end up with duplicate row labels, especially string labels like a CUSIP or SEDOL identifier for financial assets. You can't easily use the type system to help you out, and you may not be able to enforce uniqueness on the index without running into unexpectedly missing data. </p>

<p>So you're left with hoping that your unit tests covered everything (they didn't, or more likely no one wrote any tests) -- otherwise (most likely) you're just left waiting to see if you happen to smack into this error at runtime, in which case you probably have to go drop many hours worth of work from the database you were outputting results to, bang your head against the wall in IPython trying to manually reproduce the problem, finally figuring out that it's because <code>idxmax</code> can <em>only</em> report the <em>label</em> of the max row, and then being disappointed that no standard function automatically gets the <em>positions</em> of the max row for you, writing a buggy implementation yourself, editing the code, and praying you don't run into the problem again.</p>
";;"["">>> import pandas\n>>> import numpy as np\n>>> df = pandas.DataFrame(np.random.randn(5,3),columns=['A','B','C'])\n>>> df\n          A         B         C\n0  1.232853 -1.979459 -0.573626\n1  0.140767  0.394940  1.068890\n2  0.742023  1.343977 -0.579745\n3  2.125299 -0.649328 -0.211692\n4 -0.187253  1.908618 -1.862934\n>>> df['A'].argmax()\n3\n>>> df['B'].argmax()\n4\n>>> df['C'].argmax()\n1\n"", ""In [19]: dfrm\nOut[19]: \n          A         B         C\na  0.143693  0.653810  0.586007\nb  0.623582  0.312903  0.919076\nc  0.165438  0.889809  0.000967\nd  0.308245  0.787776  0.571195\ne  0.870068  0.935626  0.606911\nf  0.037602  0.855193  0.728495\ng  0.605366  0.338105  0.696460\nh  0.000000  0.090814  0.963927\ni  0.688343  0.188468  0.352213\ni  0.879000  0.105039  0.900260\n\nIn [20]: dfrm['A'].idxmax()\nOut[20]: 'i'\n\nIn [21]: dfrm.ix[dfrm['A'].idxmax()]\nOut[21]: \n          A         B         C\ni  0.688343  0.188468  0.352213\ni  0.879000  0.105039  0.900260\n""]";"['argmax()', 'idxmax', "">>> import pandas\n>>> import numpy as np\n>>> df = pandas.DataFrame(np.random.randn(5,3),columns=['A','B','C'])\n>>> df\n          A         B         C\n0  1.232853 -1.979459 -0.573626\n1  0.140767  0.394940  1.068890\n2  0.742023  1.343977 -0.579745\n3  2.125299 -0.649328 -0.211692\n4 -0.187253  1.908618 -1.862934\n>>> df['A'].argmax()\n3\n>>> df['B'].argmax()\n4\n>>> df['C'].argmax()\n1\n"", 'idxmax', 'argmax', 'idxmax', 'numpy.argmax', ""numpy.argmax(df['A'])"", 'pandas', 'idxmax', 'argmax', 'Index', 'Index', 'idxmax', 'argmax', 'idxmax', 'numpy.argmax', 'DataFrame', ""In [19]: dfrm\nOut[19]: \n          A         B         C\na  0.143693  0.653810  0.586007\nb  0.623582  0.312903  0.919076\nc  0.165438  0.889809  0.000967\nd  0.308245  0.787776  0.571195\ne  0.870068  0.935626  0.606911\nf  0.037602  0.855193  0.728495\ng  0.605366  0.338105  0.696460\nh  0.000000  0.090814  0.963927\ni  0.688343  0.188468  0.352213\ni  0.879000  0.105039  0.900260\n\nIn [20]: dfrm['A'].idxmax()\nOut[20]: 'i'\n\nIn [21]: dfrm.ix[dfrm['A'].idxmax()]\nOut[21]: \n          A         B         C\ni  0.688343  0.188468  0.352213\ni  0.879000  0.105039  0.900260\n"", 'idxmax', 'argmax', 'idxmax']"
518;;3;10213167;10202570.0;2;52;;;"<p>You might also try <code>idxmax</code>:</p>

<pre><code>In [5]: df = pandas.DataFrame(np.random.randn(10,3),columns=['A','B','C'])

In [6]: df
Out[6]: 
          A         B         C
0  2.001289  0.482561  1.579985
1 -0.991646 -0.387835  1.320236
2  0.143826 -1.096889  1.486508
3 -0.193056 -0.499020  1.536540
4 -2.083647 -3.074591  0.175772
5 -0.186138 -1.949731  0.287432
6 -0.480790 -1.771560 -0.930234
7  0.227383 -0.278253  2.102004
8 -0.002592  1.434192 -1.624915
9  0.404911 -2.167599 -0.452900

In [7]: df.idxmax()
Out[7]: 
A    0
B    8
C    7
</code></pre>

<p>e.g.</p>

<pre><code>In [8]: df.ix[df['A'].idxmax()]
Out[8]: 
A    2.001289
B    0.482561
C    1.579985
</code></pre>
";;"[""In [5]: df = pandas.DataFrame(np.random.randn(10,3),columns=['A','B','C'])\n\nIn [6]: df\nOut[6]: \n          A         B         C\n0  2.001289  0.482561  1.579985\n1 -0.991646 -0.387835  1.320236\n2  0.143826 -1.096889  1.486508\n3 -0.193056 -0.499020  1.536540\n4 -2.083647 -3.074591  0.175772\n5 -0.186138 -1.949731  0.287432\n6 -0.480790 -1.771560 -0.930234\n7  0.227383 -0.278253  2.102004\n8 -0.002592  1.434192 -1.624915\n9  0.404911 -2.167599 -0.452900\n\nIn [7]: df.idxmax()\nOut[7]: \nA    0\nB    8\nC    7\n"", ""In [8]: df.ix[df['A'].idxmax()]\nOut[8]: \nA    2.001289\nB    0.482561\nC    1.579985\n""]";"['idxmax', ""In [5]: df = pandas.DataFrame(np.random.randn(10,3),columns=['A','B','C'])\n\nIn [6]: df\nOut[6]: \n          A         B         C\n0  2.001289  0.482561  1.579985\n1 -0.991646 -0.387835  1.320236\n2  0.143826 -1.096889  1.486508\n3 -0.193056 -0.499020  1.536540\n4 -2.083647 -3.074591  0.175772\n5 -0.186138 -1.949731  0.287432\n6 -0.480790 -1.771560 -0.930234\n7  0.227383 -0.278253  2.102004\n8 -0.002592  1.434192 -1.624915\n9  0.404911 -2.167599 -0.452900\n\nIn [7]: df.idxmax()\nOut[7]: \nA    0\nB    8\nC    7\n"", ""In [8]: df.ix[df['A'].idxmax()]\nOut[8]: \nA    2.001289\nB    0.482561\nC    1.579985\n""]"
519;;4;10374456;10373660.0;2;241;;;"<p><code>g1</code> here <em>is</em> a DataFrame. It has a hierarchical index, though:</p>

<pre><code>In [19]: type(g1)
Out[19]: pandas.core.frame.DataFrame

In [20]: g1.index
Out[20]: 
MultiIndex([('Alice', 'Seattle'), ('Bob', 'Seattle'), ('Mallory', 'Portland'),
       ('Mallory', 'Seattle')], dtype=object)
</code></pre>

<p>Perhaps you want something like this?</p>

<pre><code>In [21]: g1.add_suffix('_Count').reset_index()
Out[21]: 
      Name      City  City_Count  Name_Count
0    Alice   Seattle           1           1
1      Bob   Seattle           2           2
2  Mallory  Portland           2           2
3  Mallory   Seattle           1           1
</code></pre>

<p>Or something like:</p>

<pre><code>In [36]: DataFrame({'count' : df1.groupby( [ ""Name"", ""City""] ).size()}).reset_index()
Out[36]: 
      Name      City  count
0    Alice   Seattle      1
1      Bob   Seattle      2
2  Mallory  Portland      2
3  Mallory   Seattle      1
</code></pre>
";;"[""In [19]: type(g1)\nOut[19]: pandas.core.frame.DataFrame\n\nIn [20]: g1.index\nOut[20]: \nMultiIndex([('Alice', 'Seattle'), ('Bob', 'Seattle'), ('Mallory', 'Portland'),\n       ('Mallory', 'Seattle')], dtype=object)\n"", ""In [21]: g1.add_suffix('_Count').reset_index()\nOut[21]: \n      Name      City  City_Count  Name_Count\n0    Alice   Seattle           1           1\n1      Bob   Seattle           2           2\n2  Mallory  Portland           2           2\n3  Mallory   Seattle           1           1\n"", 'In [36]: DataFrame({\'count\' : df1.groupby( [ ""Name"", ""City""] ).size()}).reset_index()\nOut[36]: \n      Name      City  count\n0    Alice   Seattle      1\n1      Bob   Seattle      2\n2  Mallory  Portland      2\n3  Mallory   Seattle      1\n']";"['g1', ""In [19]: type(g1)\nOut[19]: pandas.core.frame.DataFrame\n\nIn [20]: g1.index\nOut[20]: \nMultiIndex([('Alice', 'Seattle'), ('Bob', 'Seattle'), ('Mallory', 'Portland'),\n       ('Mallory', 'Seattle')], dtype=object)\n"", ""In [21]: g1.add_suffix('_Count').reset_index()\nOut[21]: \n      Name      City  City_Count  Name_Count\n0    Alice   Seattle           1           1\n1      Bob   Seattle           2           2\n2  Mallory  Portland           2           2\n3  Mallory   Seattle           1           1\n"", 'In [36]: DataFrame({\'count\' : df1.groupby( [ ""Name"", ""City""] ).size()}).reset_index()\nOut[36]: \n      Name      City  count\n0    Alice   Seattle      1\n1      Bob   Seattle      2\n2  Mallory  Portland      2\n3  Mallory   Seattle      1\n']"
520;;1;10458386;10457584.0;2;168;;;"<p>Why don't you simply use <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#add-an-index-using-dataframe-columns"" rel=""noreferrer""><code>set_index</code></a> method?</p>

<pre><code>In : col = ['a','b','c']

In : data = DataFrame([[1,2,3],[10,11,12],[20,21,22]],columns=col)

In : data
Out:
    a   b   c
0   1   2   3
1  10  11  12
2  20  21  22

In : data2 = data.set_index('a')

In : data2
Out:
     b   c
a
1    2   3
10  11  12
20  21  22
</code></pre>
";;"[""In : col = ['a','b','c']\n\nIn : data = DataFrame([[1,2,3],[10,11,12],[20,21,22]],columns=col)\n\nIn : data\nOut:\n    a   b   c\n0   1   2   3\n1  10  11  12\n2  20  21  22\n\nIn : data2 = data.set_index('a')\n\nIn : data2\nOut:\n     b   c\na\n1    2   3\n10  11  12\n20  21  22\n""]";"['set_index', ""In : col = ['a','b','c']\n\nIn : data = DataFrame([[1,2,3],[10,11,12],[20,21,22]],columns=col)\n\nIn : data\nOut:\n    a   b   c\n0   1   2   3\n1  10  11  12\n2  20  21  22\n\nIn : data2 = data.set_index('a')\n\nIn : data2\nOut:\n     b   c\na\n1    2   3\n10  11  12\n20  21  22\n""]"
521;;5;10465162;10464738.0;2;30;;;"<p>You can use <code>DataFrame.apply</code> with <code>Series.interpolate</code> to get a linear interpolation. </p>

<pre><code>In : df = pandas.DataFrame(numpy.random.randn(5,3), index=['a','c','d','e','g'])

In : df
Out:
          0         1         2
a -1.987879 -2.028572  0.024493
c  2.092605 -1.429537  0.204811
d  0.767215  1.077814  0.565666
e -1.027733  1.330702 -0.490780
g -1.632493  0.938456  0.492695

In : df2 = df.reindex(['a','b','c','d','e','f','g'])

In : df2
Out:
          0         1         2
a -1.987879 -2.028572  0.024493
b       NaN       NaN       NaN
c  2.092605 -1.429537  0.204811
d  0.767215  1.077814  0.565666
e -1.027733  1.330702 -0.490780
f       NaN       NaN       NaN
g -1.632493  0.938456  0.492695

In : df2.apply(pandas.Series.interpolate)
Out:
          0         1         2
a -1.987879 -2.028572  0.024493
b  0.052363 -1.729055  0.114652
c  2.092605 -1.429537  0.204811
d  0.767215  1.077814  0.565666
e -1.027733  1.330702 -0.490780
f -1.330113  1.134579  0.000958
g -1.632493  0.938456  0.492695
</code></pre>

<p>For anything more complex, you need to roll-out your own function that will deal with a <code>Series</code> object and fill <code>NaN</code> values as you like and return another <code>Series</code> object.</p>
";;"[""In : df = pandas.DataFrame(numpy.random.randn(5,3), index=['a','c','d','e','g'])\n\nIn : df\nOut:\n          0         1         2\na -1.987879 -2.028572  0.024493\nc  2.092605 -1.429537  0.204811\nd  0.767215  1.077814  0.565666\ne -1.027733  1.330702 -0.490780\ng -1.632493  0.938456  0.492695\n\nIn : df2 = df.reindex(['a','b','c','d','e','f','g'])\n\nIn : df2\nOut:\n          0         1         2\na -1.987879 -2.028572  0.024493\nb       NaN       NaN       NaN\nc  2.092605 -1.429537  0.204811\nd  0.767215  1.077814  0.565666\ne -1.027733  1.330702 -0.490780\nf       NaN       NaN       NaN\ng -1.632493  0.938456  0.492695\n\nIn : df2.apply(pandas.Series.interpolate)\nOut:\n          0         1         2\na -1.987879 -2.028572  0.024493\nb  0.052363 -1.729055  0.114652\nc  2.092605 -1.429537  0.204811\nd  0.767215  1.077814  0.565666\ne -1.027733  1.330702 -0.490780\nf -1.330113  1.134579  0.000958\ng -1.632493  0.938456  0.492695\n""]";"['DataFrame.apply', 'Series.interpolate', ""In : df = pandas.DataFrame(numpy.random.randn(5,3), index=['a','c','d','e','g'])\n\nIn : df\nOut:\n          0         1         2\na -1.987879 -2.028572  0.024493\nc  2.092605 -1.429537  0.204811\nd  0.767215  1.077814  0.565666\ne -1.027733  1.330702 -0.490780\ng -1.632493  0.938456  0.492695\n\nIn : df2 = df.reindex(['a','b','c','d','e','f','g'])\n\nIn : df2\nOut:\n          0         1         2\na -1.987879 -2.028572  0.024493\nb       NaN       NaN       NaN\nc  2.092605 -1.429537  0.204811\nd  0.767215  1.077814  0.565666\ne -1.027733  1.330702 -0.490780\nf       NaN       NaN       NaN\ng -1.632493  0.938456  0.492695\n\nIn : df2.apply(pandas.Series.interpolate)\nOut:\n          0         1         2\na -1.987879 -2.028572  0.024493\nb  0.052363 -1.729055  0.114652\nc  2.092605 -1.429537  0.204811\nd  0.767215  1.077814  0.565666\ne -1.027733  1.330702 -0.490780\nf -1.330113  1.134579  0.000958\ng -1.632493  0.938456  0.492695\n"", 'Series', 'NaN', 'Series']"
522;;1;10511230;10511024.0;2;19;;;"<p>With your <code>import matplotlib.pyplot as plt</code> just add</p>

<pre><code>plt.show()
</code></pre>

<p>and it will show all stored plots.</p>
";;['plt.show()\n'];['import matplotlib.pyplot as plt', 'plt.show()\n']
523;;2;10511545;10511024.0;2;49;;;"<p>Ok, It seems the answer is to start ipython notebook with --pylab=inline. 
so ipython notebook --pylab=inline
This has it do what I saw earlier and what I wanted it to do. 
Sorry about the vague original question. </p>
";;[];[]
524;;4;10666301;10665889.0;2;23;;;"<p>You can slice along the columns of a <code>DataFrame</code> by referring to the names of each column in a list, like so:</p>

<pre><code>data = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))
data_ab = data[list('ab')]
data_cde = data[list('cde')]
</code></pre>
";;"[""data = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))\ndata_ab = data[list('ab')]\ndata_cde = data[list('cde')]\n""]";"['DataFrame', ""data = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))\ndata_ab = data[list('ab')]\ndata_cde = data[list('cde')]\n""]"
525;;5;10677896;10665889.0;2;137;;;"<p>The DataFrame.ix index is what you want to be accessing. It's a little confusing (I agree that Pandas indexing is perplexing at times!), but the following seems to do what you want:</p>

<pre><code>&gt;&gt;&gt; df = DataFrame(np.random.rand(4,5), columns = list('abcde'))
&gt;&gt;&gt; df.ix[:,'b':]
      b         c         d         e
0  0.418762  0.042369  0.869203  0.972314
1  0.991058  0.510228  0.594784  0.534366
2  0.407472  0.259811  0.396664  0.894202
3  0.726168  0.139531  0.324932  0.906575
</code></pre>

<p>where .ix[row slice, column slice] is what is being interpreted. More on Pandas indexing here: <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-advanced"">http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-advanced</a></p>
";;"["">>> df = DataFrame(np.random.rand(4,5), columns = list('abcde'))\n>>> df.ix[:,'b':]\n      b         c         d         e\n0  0.418762  0.042369  0.869203  0.972314\n1  0.991058  0.510228  0.594784  0.534366\n2  0.407472  0.259811  0.396664  0.894202\n3  0.726168  0.139531  0.324932  0.906575\n""]";"["">>> df = DataFrame(np.random.rand(4,5), columns = list('abcde'))\n>>> df.ix[:,'b':]\n      b         c         d         e\n0  0.418762  0.042369  0.869203  0.972314\n1  0.991058  0.510228  0.594784  0.534366\n2  0.407472  0.259811  0.396664  0.894202\n3  0.726168  0.139531  0.324932  0.906575\n""]"
526;;8;10716007;10715965.0;2;185;;;"<p>You could use <code>pandas.concat()</code> or <code>DataFrame.append()</code>. For details and examples, see <a href=""http://pandas.pydata.org/pandas-docs/stable/merging.html"" rel=""noreferrer"">Merge, join, and concatenate</a>.</p>
";;[];['pandas.concat()', 'DataFrame.append()']
527;;4;10739432;10729210.0;2;167;;;"<p>You should use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html""><code>df.iterrows()</code></a>. Though iterating row-by-row is not especially efficient since Series objects have to be created.</p>
";;[];['df.iterrows()']
528;;5;10762516;10751127.0;2;55;;;"<p>Returning a Series, rather than tuple, should produce a new multi-column DataFrame. For example,</p>

<pre><code>return pandas.Series({'pvalue': pvalue, 'mean_ratio': mean_ratio})
</code></pre>
";;"[""return pandas.Series({'pvalue': pvalue, 'mean_ratio': mean_ratio})\n""]";"[""return pandas.Series({'pvalue': pvalue, 'mean_ratio': mean_ratio})\n""]"
529;;1;10781413;10591000.0;2;12;;;"<p>It looks like you can't avoid pandas from trying to convert numeric/boolean values in the CSV file. Take a look at the source code of pandas for the IO parsers, in particular functions <code>_convert_to_ndarrays</code>, and <code>_convert_types</code>.
<a href=""https://github.com/pydata/pandas/blob/master/pandas/io/parsers.py"" rel=""noreferrer"">https://github.com/pydata/pandas/blob/master/pandas/io/parsers.py</a></p>

<p>You can always assign the type you want <em>after</em> you have read the file:</p>

<pre><code>df.phone = df.phone.astype(str)
</code></pre>
";;['df.phone = df.phone.astype(str)\n'];['_convert_to_ndarrays', '_convert_types', 'df.phone = df.phone.astype(str)\n']
530;;2;10859883;10857924.0;2;73;;;"<p>Yes, <code>dropna</code>. See <a href=""http://pandas.pydata.org/pandas-docs/stable/missing_data.html"">http://pandas.pydata.org/pandas-docs/stable/missing_data.html</a> and the <code>DataFrame.dropna</code> docstring:</p>

<pre><code>Definition: DataFrame.dropna(self, axis=0, how='any', thresh=None, subset=None)
Docstring:
Return object with labels on given axis omitted where alternately any
or all of the data are missing

Parameters
----------
axis : {0, 1}
how : {'any', 'all'}
    any : if any NA values are present, drop that label
    all : if all values are NA, drop that label
thresh : int, default None
    int value : require that many non-NA values
subset : array-like
    Labels along other axis to consider, e.g. if you are dropping rows
    these would be a list of columns to include

Returns
-------
dropped : DataFrame
</code></pre>

<p>The specific command to run would be:</p>

<pre><code>df=df.dropna(axis=1,how='all')
</code></pre>
";;"[""Definition: DataFrame.dropna(self, axis=0, how='any', thresh=None, subset=None)\nDocstring:\nReturn object with labels on given axis omitted where alternately any\nor all of the data are missing\n\nParameters\n----------\naxis : {0, 1}\nhow : {'any', 'all'}\n    any : if any NA values are present, drop that label\n    all : if all values are NA, drop that label\nthresh : int, default None\n    int value : require that many non-NA values\nsubset : array-like\n    Labels along other axis to consider, e.g. if you are dropping rows\n    these would be a list of columns to include\n\nReturns\n-------\ndropped : DataFrame\n"", ""df=df.dropna(axis=1,how='all')\n""]";"['dropna', 'DataFrame.dropna', ""Definition: DataFrame.dropna(self, axis=0, how='any', thresh=None, subset=None)\nDocstring:\nReturn object with labels on given axis omitted where alternately any\nor all of the data are missing\n\nParameters\n----------\naxis : {0, 1}\nhow : {'any', 'all'}\n    any : if any NA values are present, drop that label\n    all : if all values are NA, drop that label\nthresh : int, default None\n    int value : require that many non-NA values\nsubset : array-like\n    Labels along other axis to consider, e.g. if you are dropping rows\n    these would be a list of columns to include\n\nReturns\n-------\ndropped : DataFrame\n"", ""df=df.dropna(axis=1,how='all')\n""]"
531;;4;10964938;10951341.0;2;63;;;"<p>Yes; use the <code>.apply(...)</code> function, which will be called on each sub-<code>DataFrame</code>. For example:</p>

<pre><code>grouped = df.groupby(keys)

def wavg(group):
    d = group['data']
    w = group['weights']
    return (d * w).sum() / w.sum()

grouped.apply(wavg)
</code></pre>
";;"[""grouped = df.groupby(keys)\n\ndef wavg(group):\n    d = group['data']\n    w = group['weights']\n    return (d * w).sum() / w.sum()\n\ngrouped.apply(wavg)\n""]";"['.apply(...)', 'DataFrame', ""grouped = df.groupby(keys)\n\ndef wavg(group):\n    d = group['data']\n    w = group['weights']\n    return (d * w).sum() / w.sum()\n\ngrouped.apply(wavg)\n""]"
532;;1;10982198;10982089.0;2;70;;;"<pre><code>In [18]: a
Out[18]: 
   x1  x2
0   0   5
1   1   6
2   2   7
3   3   8
4   4   9

In [19]: a.x2 = a.x2.shift(1)

In [20]: a
Out[20]: 
   x1  x2
0   0 NaN
1   1   5
2   2   6
3   3   7
4   4   8
</code></pre>
";;['In [18]: a\nOut[18]: \n   x1  x2\n0   0   5\n1   1   6\n2   2   7\n3   3   8\n4   4   9\n\nIn [19]: a.x2 = a.x2.shift(1)\n\nIn [20]: a\nOut[20]: \n   x1  x2\n0   0 NaN\n1   1   5\n2   2   6\n3   3   7\n4   4   8\n'];['In [18]: a\nOut[18]: \n   x1  x2\n0   0   5\n1   1   6\n2   2   7\n3   3   8\n4   4   9\n\nIn [19]: a.x2 = a.x2.shift(1)\n\nIn [20]: a\nOut[20]: \n   x1  x2\n0   0 NaN\n1   1   5\n2   2   6\n3   3   7\n4   4   8\n']
533;;4;11005208;10867028.0;2;23;;;"<p>I added a ticket to add an option of some sort here:</p>

<p><a href=""https://github.com/pydata/pandas/issues/1450"">https://github.com/pydata/pandas/issues/1450</a></p>

<p>In the meantime, <code>result.fillna('')</code> should do what you want</p>

<p>EDIT: in the development version (to be 0.8.0 final) if you specify an empty list of <code>na_values</code>, empty strings will stay empty strings in the result</p>
";;[];"[""result.fillna('')"", 'na_values']"
534;;1;11067072;11067027.0;2;138;;;"<pre><code>df.reindex_axis(sorted(df.columns), axis=1)
</code></pre>

<p>This assumes that sorting the column names will give the order you want.  If your column names won't sort lexicographically (e.g., if you want column Q10.3 to appear after Q9.1), you'll need to sort differently, but that has nothing to do with pandas.</p>
";;['df.reindex_axis(sorted(df.columns), axis=1)\n'];['df.reindex_axis(sorted(df.columns), axis=1)\n']
535;;3;11073962;11073609.0;2;33;;;"<p>You can group on any array/Series of the same length as your DataFrame --- even a computed factor that's not actually a column of the DataFrame.  So to group by minute you can do:</p>

<pre><code>df.groupby(df.index.map(lambda t: t.minute))
</code></pre>

<p>If you want to group by minute and something else, just mix the above with the column you want to use:</p>

<pre><code>df.groupby([df.index.map(lambda t: t.minute), 'Source'])
</code></pre>

<p>Personally I find it useful to just add columns to the DataFrame to store some of these computed things (e.g., a ""Minute"" column) if I want to group by them often, since it makes the grouping code less verbose.</p>

<p>Or you could try something like this:</p>

<pre><code>df.groupby([df['Source'],pd.TimeGrouper(freq='Min')])
</code></pre>
";;"['df.groupby(df.index.map(lambda t: t.minute))\n', ""df.groupby([df.index.map(lambda t: t.minute), 'Source'])\n"", ""df.groupby([df['Source'],pd.TimeGrouper(freq='Min')])\n""]";"['df.groupby(df.index.map(lambda t: t.minute))\n', ""df.groupby([df.index.map(lambda t: t.minute), 'Source'])\n"", ""df.groupby([df['Source'],pd.TimeGrouper(freq='Min')])\n""]"
536;;0;11077060;11077023.0;2;36;;;"<p>Numpy is required by pandas (and by virtually all numerical tools for Python).  Scipy is not strictly required for pandas but is listed as an ""optional dependency"".  I wouldn't say that pandas is an alternative to Numpy and/or Scipy.  Rather, it's an extra tool that provides a more streamlined way of working with numerical and tabular data in Python.  You can use pandas data structures but freely draw on Numpy and Scipy functions to manipulate them.</p>
";;[];[]
537;;7;11077215;11077023.0;2;160;;;"<p>Indeed, pandas provides high level data manipulation tools built on top of NumPy. NumPy by itself is a fairly low-level tool, and will be very much similar to using MATLAB. pandas on the other hand provides rich time series functionality, data alignment, NA-friendly statistics, groupby, merge and join methods, and lots of other conveniences. It has become very popular in recent years in financial applications. I will have a chapter dedicated to financial data analysis using pandas in my upcoming book. </p>
";;[];[]
538;;2;11107627;11106823.0;2;6;;;"<p>If I understand you correctly, you want something like:</p>

<pre><code>(x.reindex_like(y).fillna(0) + y.fillna(0)).fillna(0)
</code></pre>

<p>This will give the sum of the two dataframes.  If a value is in one dataframe and not the other, the result at that position will be that existing value.  If a value is missing in both dataframes, the result at that position will be zero.</p>

<pre><code>&gt;&gt;&gt; x
   A   B   C
0  1   2 NaN
1  3 NaN   4
&gt;&gt;&gt; y
    A   B   C
0   8 NaN  88
1   2 NaN   5
2  10  11  12
&gt;&gt;&gt; (x.reindex_like(y).fillna(0) + y.fillna(0)).fillna(0)
    A   B   C
0   9   2  88
1   5   0   9
2  10  11  12
</code></pre>
";;['(x.reindex_like(y).fillna(0) + y.fillna(0)).fillna(0)\n', '>>> x\n   A   B   C\n0  1   2 NaN\n1  3 NaN   4\n>>> y\n    A   B   C\n0   8 NaN  88\n1   2 NaN   5\n2  10  11  12\n>>> (x.reindex_like(y).fillna(0) + y.fillna(0)).fillna(0)\n    A   B   C\n0   9   2  88\n1   5   0   9\n2  10  11  12\n'];['(x.reindex_like(y).fillna(0) + y.fillna(0)).fillna(0)\n', '>>> x\n   A   B   C\n0  1   2 NaN\n1  3 NaN   4\n>>> y\n    A   B   C\n0   8 NaN  88\n1   2 NaN   5\n2  10  11  12\n>>> (x.reindex_like(y).fillna(0) + y.fillna(0)).fillna(0)\n    A   B   C\n0   9   2  88\n1   5   0   9\n2  10  11  12\n']
539;;1;11112419;11106823.0;2;45;;;"<p>How about <code>x.add(y, fill_value=0)</code>?</p>

<pre><code>import pandas as pd

df1 = pd.DataFrame([(1,2),(3,4),(5,6)], columns=['a','b'])
Out: 
   a  b
0  1  2
1  3  4
2  5  6

df2 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['a','b'])
Out: 
     a    b
0  100  200
1  300  400
2  500  600

df_add = df1.add(df2, fill_value=0)
Out: 
     a    b
0  101  202
1  303  404
2  505  606
</code></pre>
";;"[""import pandas as pd\n\ndf1 = pd.DataFrame([(1,2),(3,4),(5,6)], columns=['a','b'])\nOut: \n   a  b\n0  1  2\n1  3  4\n2  5  6\n\ndf2 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['a','b'])\nOut: \n     a    b\n0  100  200\n1  300  400\n2  500  600\n\ndf_add = df1.add(df2, fill_value=0)\nOut: \n     a    b\n0  101  202\n1  303  404\n2  505  606\n""]";"['x.add(y, fill_value=0)', ""import pandas as pd\n\ndf1 = pd.DataFrame([(1,2),(3,4),(5,6)], columns=['a','b'])\nOut: \n   a  b\n0  1  2\n1  3  4\n2  5  6\n\ndf2 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['a','b'])\nOut: \n     a    b\n0  100  200\n1  300  400\n2  500  600\n\ndf_add = df1.add(df2, fill_value=0)\nOut: \n     a    b\n0  101  202\n1  303  404\n2  505  606\n""]"
540;;0;11138275;10065051.0;2;76;;;"<p>As Wes says, io/sql's read_sql will do it, once you've gotten a database connection using a DBI compatible library.  We can look at two short examples using the <code>MySQLdb</code> and <code>cx_Oracle</code> libraries to connect to Oracle and MySQL and query their data dictionaries. Here is the example for <code>cx_Oracle</code>:</p>

<pre><code>import pandas as pd
import cx_Oracle

ora_conn = cx_Oracle.connect('your_connection_string')
df_ora = pd.read_sql('select * from user_objects', con=ora_conn)    
print 'loaded dataframe from Oracle. # Records: ', len(df_ora)
ora_conn.close()
</code></pre>

<p>And here is the equivalent example for <code>MySQLdb</code>:</p>

<pre><code>import MySQLdb
mysql_cn= MySQLdb.connect(host='myhost', 
                port=3306,user='myusername', passwd='mypassword', 
                db='information_schema')
df_mysql = pd.read_sql('select * from VIEWS;', con=mysql_cn)    
print 'loaded dataframe from MySQL. records:', len(df_mysql)
mysql_cn.close()
</code></pre>
";;"[""import pandas as pd\nimport cx_Oracle\n\nora_conn = cx_Oracle.connect('your_connection_string')\ndf_ora = pd.read_sql('select * from user_objects', con=ora_conn)    \nprint 'loaded dataframe from Oracle. # Records: ', len(df_ora)\nora_conn.close()\n"", ""import MySQLdb\nmysql_cn= MySQLdb.connect(host='myhost', \n                port=3306,user='myusername', passwd='mypassword', \n                db='information_schema')\ndf_mysql = pd.read_sql('select * from VIEWS;', con=mysql_cn)    \nprint 'loaded dataframe from MySQL. records:', len(df_mysql)\nmysql_cn.close()\n""]";"['MySQLdb', 'cx_Oracle', 'cx_Oracle', ""import pandas as pd\nimport cx_Oracle\n\nora_conn = cx_Oracle.connect('your_connection_string')\ndf_ora = pd.read_sql('select * from user_objects', con=ora_conn)    \nprint 'loaded dataframe from Oracle. # Records: ', len(df_ora)\nora_conn.close()\n"", 'MySQLdb', ""import MySQLdb\nmysql_cn= MySQLdb.connect(host='myhost', \n                port=3306,user='myusername', passwd='mypassword', \n                db='information_schema')\ndf_mysql = pd.read_sql('select * from VIEWS;', con=mysql_cn)    \nprint 'loaded dataframe from MySQL. records:', len(df_mysql)\nmysql_cn.close()\n""]"
541;;12;11287278;11285613.0;2;490;;;"<p>The column names (which are strings) cannot be sliced in the manner you tried.</p>

<p>Here you have a couple of options. If you know from context which variables you want to slice out, you can just return a view of only those columns by passing a list into the <code>__getitem__</code> syntax (the []'s).</p>

<pre><code>df1 = df[['a','b']]
</code></pre>

<p>Alternatively, if it matters to index them numerically and not by their name (say your code should automatically do this without knowing the names of the first two columns) then you can do this instead:</p>

<pre><code>df1 = df.ix[:,0:2] # Remember that Python does not slice inclusive of the ending index.
</code></pre>

<p>Additionally, you should familiarize yourself with the idea of a view into a Pandas object vs. a copy of that object. The first of the above methods will return a new copy in memory of the desired sub-object (the desired slices). </p>

<p>Sometimes, however, there are indexing conventions in Pandas that don't do this and instead give you a new variable that just refers to the same chunk of memory as the sub-object or slice in the original object. This will happen with the second way of indexing, so you can modify it with the <code>copy()</code> function to get a regular copy. When this happens, changing what you think is the sliced object can sometimes alter the original object. Always good to be on the look out for this.</p>

<pre><code>df1 = df.ix[0,0:2].copy() # To avoid the case where changing df1 also changes df
</code></pre>
";;"[""df1 = df[['a','b']]\n"", 'df1 = df.ix[:,0:2] # Remember that Python does not slice inclusive of the ending index.\n', 'df1 = df.ix[0,0:2].copy() # To avoid the case where changing df1 also changes df\n']";"['__getitem__', ""df1 = df[['a','b']]\n"", 'df1 = df.ix[:,0:2] # Remember that Python does not slice inclusive of the ending index.\n', 'copy()', 'df1 = df.ix[0,0:2].copy() # To avoid the case where changing df1 also changes df\n']"
542;;9;11346337;11346283.0;2;768;;;"<p>Just assign it to the <code>.columns</code> attribute:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'$a':[1,2], '$b': [10,20]})
&gt;&gt;&gt; df.columns = ['a', 'b']
&gt;&gt;&gt; df
   a   b
0  1  10
1  2  20
</code></pre>
";;"["">>> df = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\n>>> df.columns = ['a', 'b']\n>>> df\n   a   b\n0  1  10\n1  2  20\n""]";"['.columns', "">>> df = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\n>>> df.columns = ['a', 'b']\n>>> df\n   a   b\n0  1  10\n1  2  20\n""]"
543;;6;11354850;11346283.0;2;1304;;;"<p>Use the rename function and refer the columns to be renamed. Not all the columns have to be renamed:</p>

<pre><code>df = df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'})
# OR
df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html</a></p>
";;"[""df = df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'})\n# OR\ndf.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n""]";"[""df = df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'})\n# OR\ndf.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n""]"
544;;1;11362056;11361985.0;2;36;;;"<p>There is too much data to be displayed on the screen, therefore a summary is displayed instead.</p>

<p>If you want to output the data anyway (it won't probably fit on a screen and does not look very well):</p>

<pre><code>print paramdata.values
</code></pre>

<p>converts the dataframe to its numpy-array matrix representation.</p>

<pre><code>paramdata.columns
</code></pre>

<p>stores the respective column names and</p>

<pre><code>paramdata.index
</code></pre>

<p>stores the respective index (row names).</p>
";;['print paramdata.values\n', 'paramdata.columns\n', 'paramdata.index\n'];['print paramdata.values\n', 'paramdata.columns\n', 'paramdata.index\n']
545;;0;11366429;11361985.0;2;9;;;"<p>you can also use <code>DataFrame.head(x)</code> / <code>.tail(x)</code> to display the first / last x rows of the DataFrame.</p>
";;[];['DataFrame.head(x)', '.tail(x)']
546;;1;11366706;11350770.0;2;7;;;"<p>Here's what I ended up doing for partial string matches.  If anyone has a more efficient way of doing this please let me know.</p>

<pre><code>def stringSearchColumn_DataFrame(df, colName, regex):
    newdf = DataFrame()
    for idx, record in df[colName].iteritems():

        if re.search(regex, record):
            newdf = concat([df[df[colName] == record], newdf], ignore_index=True)

    return newdf
</code></pre>
";;['def stringSearchColumn_DataFrame(df, colName, regex):\n    newdf = DataFrame()\n    for idx, record in df[colName].iteritems():\n\n        if re.search(regex, record):\n            newdf = concat([df[df[colName] == record], newdf], ignore_index=True)\n\n    return newdf\n'];['def stringSearchColumn_DataFrame(df, colName, regex):\n    newdf = DataFrame()\n    for idx, record in df[colName].iteritems():\n\n        if re.search(regex, record):\n            newdf = concat([df[df[colName] == record], newdf], ignore_index=True)\n\n    return newdf\n']
547;;2;11385335;11285613.0;2;35;;;"<pre><code>In [39]: df
Out[39]: 
   index  a  b  c
0      1  2  3  4
1      2  3  4  5

In [40]: df1 = df[['b', 'c']]

In [41]: df1
Out[41]: 
   b  c
0  3  4
1  4  5
</code></pre>
";;"[""In [39]: df\nOut[39]: \n   index  a  b  c\n0      1  2  3  4\n1      2  3  4  5\n\nIn [40]: df1 = df[['b', 'c']]\n\nIn [41]: df1\nOut[41]: \n   b  c\n0  3  4\n1  4  5\n""]";"[""In [39]: df\nOut[39]: \n   index  a  b  c\n0      1  2  3  4\n1      2  3  4  5\n\nIn [40]: df1 = df[['b', 'c']]\n\nIn [41]: df1\nOut[41]: \n   b  c\n0  3  4\n1  4  5\n""]"
548;;3;11385780;11067027.0;2;147;;;"<p>You can also do more succinctly:</p>

<p><code>df.sort_index(axis=1)</code></p>

<p><em>Edit</em>:</p>

<p>Make sure you hold the value </p>

<p><code>df = df.sort_index(axis=1)</code></p>

<p>Or do it in place</p>

<p><code>df.sort_index(axis=1, inplace=True)</code></p>
";;[];['df.sort_index(axis=1)', 'df = df.sort_index(axis=1)', 'df.sort_index(axis=1, inplace=True)']
549;;0;11395193;11391969.0;2;10;;;"<p>This should work:</p>

<pre><code>data.groupby(lambda x: data['date'][x].year)
</code></pre>
";;"[""data.groupby(lambda x: data['date'][x].year)\n""]";"[""data.groupby(lambda x: data['date'][x].year)\n""]"
550;;2;11397052;11391969.0;2;57;;;"<p>ecatmur's solution will work fine. This will be better performance on large datasets, though:</p>

<pre><code>data.groupby(data['date'].map(lambda x: x.year))
</code></pre>
";;"[""data.groupby(data['date'].map(lambda x: x.year))\n""]";"[""data.groupby(data['date'].map(lambda x: x.year))\n""]"
551;;1;11420594;11418192.0;2;9;;;"<p>Suppose I had a DataFrame as follows:</p>

<pre><code>In [39]: df
Out[39]: 
      mass1     mass2  velocity
0  1.461711 -0.404452  0.722502
1 -2.169377  1.131037  0.232047
2  0.009450 -0.868753  0.598470
3  0.602463  0.299249  0.474564
4 -0.675339 -0.816702  0.799289
</code></pre>

<p>I can use sin and DataFrame.prod to create a boolean mask:</p>

<pre><code>In [40]: mask = (np.sin(df.velocity) / df.ix[:, 0:2].prod(axis=1)) &gt; 0

In [41]: mask
Out[41]: 
0    False
1    False
2    False
3     True
4     True
</code></pre>

<p>Then use the mask to select from the DataFrame:</p>

<pre><code>In [42]: df[mask]
Out[42]: 
      mass1     mass2  velocity
3  0.602463  0.299249  0.474564
4 -0.675339 -0.816702  0.799289
</code></pre>
";;['In [39]: df\nOut[39]: \n      mass1     mass2  velocity\n0  1.461711 -0.404452  0.722502\n1 -2.169377  1.131037  0.232047\n2  0.009450 -0.868753  0.598470\n3  0.602463  0.299249  0.474564\n4 -0.675339 -0.816702  0.799289\n', 'In [40]: mask = (np.sin(df.velocity) / df.ix[:, 0:2].prod(axis=1)) > 0\n\nIn [41]: mask\nOut[41]: \n0    False\n1    False\n2    False\n3     True\n4     True\n', 'In [42]: df[mask]\nOut[42]: \n      mass1     mass2  velocity\n3  0.602463  0.299249  0.474564\n4 -0.675339 -0.816702  0.799289\n'];['In [39]: df\nOut[39]: \n      mass1     mass2  velocity\n0  1.461711 -0.404452  0.722502\n1 -2.169377  1.131037  0.232047\n2  0.009450 -0.868753  0.598470\n3  0.602463  0.299249  0.474564\n4 -0.675339 -0.816702  0.799289\n', 'In [40]: mask = (np.sin(df.velocity) / df.ix[:, 0:2].prod(axis=1)) > 0\n\nIn [41]: mask\nOut[41]: \n0    False\n1    False\n2    False\n3     True\n4     True\n', 'In [42]: df[mask]\nOut[42]: \n      mass1     mass2  velocity\n3  0.602463  0.299249  0.474564\n4 -0.675339 -0.816702  0.799289\n']
552;;0;11475486;11418192.0;2;56;;;"<p>You can do this using <code>DataFrame.apply</code>, which applies a function along a given axis,</p>

<pre><code>In [3]: df = pandas.DataFrame(np.random.randn(5, 3), columns=['a', 'b', 'c'])

In [4]: df
Out[4]: 
          a         b         c
0 -0.001968 -1.877945 -1.515674
1 -0.540628  0.793913 -0.983315
2 -1.313574  1.946410  0.826350
3  0.015763 -0.267860 -2.228350
4  0.563111  1.195459  0.343168

In [6]: df[df.apply(lambda x: x['b'] &gt; x['c'], axis=1)]
Out[6]: 
          a         b         c
1 -0.540628  0.793913 -0.983315
2 -1.313574  1.946410  0.826350
3  0.015763 -0.267860 -2.228350
4  0.563111  1.195459  0.343168
</code></pre>
";;"[""In [3]: df = pandas.DataFrame(np.random.randn(5, 3), columns=['a', 'b', 'c'])\n\nIn [4]: df\nOut[4]: \n          a         b         c\n0 -0.001968 -1.877945 -1.515674\n1 -0.540628  0.793913 -0.983315\n2 -1.313574  1.946410  0.826350\n3  0.015763 -0.267860 -2.228350\n4  0.563111  1.195459  0.343168\n\nIn [6]: df[df.apply(lambda x: x['b'] > x['c'], axis=1)]\nOut[6]: \n          a         b         c\n1 -0.540628  0.793913 -0.983315\n2 -1.313574  1.946410  0.826350\n3  0.015763 -0.267860 -2.228350\n4  0.563111  1.195459  0.343168\n""]";"['DataFrame.apply', ""In [3]: df = pandas.DataFrame(np.random.randn(5, 3), columns=['a', 'b', 'c'])\n\nIn [4]: df\nOut[4]: \n          a         b         c\n0 -0.001968 -1.877945 -1.515674\n1 -0.540628  0.793913 -0.983315\n2 -1.313574  1.946410  0.826350\n3  0.015763 -0.267860 -2.228350\n4  0.563111  1.195459  0.343168\n\nIn [6]: df[df.apply(lambda x: x['b'] > x['c'], axis=1)]\nOut[6]: \n          a         b         c\n1 -0.540628  0.793913 -0.983315\n2 -1.313574  1.946410  0.826350\n3  0.015763 -0.267860 -2.228350\n4  0.563111  1.195459  0.343168\n""]"
553;;3;11495086;11495051.0;2;15;;;"<p>Looks like Python does not add an intercept by default to your expression, whereas R does when you use the formula interface..</p>

<p>This means you did fit two different models. Try</p>

<pre><code>lm( y ~ x - 1, data)
</code></pre>

<p>in R to exclude the intercept, or in your case and with somewhat more standard notation</p>

<pre><code>lm(num_rx ~ ridageyr - 1, data=demoq)
</code></pre>
";;['lm( y ~ x - 1, data)\n', 'lm(num_rx ~ ridageyr - 1, data=demoq)\n'];['lm( y ~ x - 1, data)\n', 'lm(num_rx ~ ridageyr - 1, data=demoq)\n']
554;;5;11531402;11350770.0;2;267;;;"<p>Based on github issue <a href=""https://github.com/pydata/pandas/issues/620"" rel=""noreferrer"">#620</a>, it looks like you'll soon be able to do the following:</p>

<pre><code>df[df['A'].str.contains(""hello"")]
</code></pre>

<p>Update: <a href=""http://pandas.pydata.org/pandas-docs/stable/text.html#text-string-methods"" rel=""noreferrer"">vectorized string methods (i.e., Series.str)</a> are available in pandas 0.8.1 and up. </p>
";;"['df[df[\'A\'].str.contains(""hello"")]\n']";"['df[df[\'A\'].str.contains(""hello"")]\n']"
555;;7;11548224;11548005.0;2;49;;;"<p><code>NaN</code> can't be stored in an integer array. This is a known limitation of pandas at the moment; I have been waiting for progress to be made with NA values in NumPy (similar to NAs in R), but it will be at least 6 months to a year before NumPy gets these features, it seems:</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/gotchas.html#support-for-integer-na"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/gotchas.html#support-for-integer-na</a></p>
";;[];['NaN']
556;;4;11617194;7837722.0;2;209;;;"<p>The newest versions of pandas now include a built-in function for iterating over rows. </p>

<pre><code>for index, row in df.iterrows():

    # do some logic here
</code></pre>

<p>Or, if you want it faster use <code>itertuples()</code></p>

<p>But, unutbu's suggestion to use numpy functions to avoid iterating over rows will produce the fastest code. </p>
";;['for index, row in df.iterrows():\n\n    # do some logic here\n'];['for index, row in df.iterrows():\n\n    # do some logic here\n', 'itertuples()']
557;;3;11617682;11615504.0;2;27;;;"<p>If you pass a list to <code>index_col</code>, it means you want to create a hierarchical index out of the columns in the list.</p>

<p>In addition, the <code>parse_dates</code> keyword can be set to either True or a list/dict. If True, then it tries to parse individual columns as dates, otherwise it combines columns to parse a single date column.</p>

<p>In summary, what you want to do is:</p>

<pre><code>from datetime import datetime
import pandas as pd
parse = lambda x: datetime.strptime(x, '%Y%m%d %H')
pd.read_csv(""..\\file.csv"",  parse_dates = [['YYYYMMDD', 'HH']], 
            index_col = 0, 
            date_parser=parse)
</code></pre>
";;"['from datetime import datetime\nimport pandas as pd\nparse = lambda x: datetime.strptime(x, \'%Y%m%d %H\')\npd.read_csv(""..\\\\file.csv"",  parse_dates = [[\'YYYYMMDD\', \'HH\']], \n            index_col = 0, \n            date_parser=parse)\n']";"['index_col', 'parse_dates', 'from datetime import datetime\nimport pandas as pd\nparse = lambda x: datetime.strptime(x, \'%Y%m%d %H\')\npd.read_csv(""..\\\\file.csv"",  parse_dates = [[\'YYYYMMDD\', \'HH\']], \n            index_col = 0, \n            date_parser=parse)\n']"
558;;3;11622769;11622652.0;2;64;;;"<p>In principle it shouldn't run out of memory, but there are currently memory problems with <code>read_csv</code> on large files caused by some complex Python internal issues (this is vague but it's been known for a long time: <a href=""http://github.com/pydata/pandas/issues/407"">http://github.com/pydata/pandas/issues/407</a>). </p>

<p>At the moment there isn't a perfect solution (here's a tedious one: you could transcribe the file row-by-row into a pre-allocated NumPy array or memory-mapped file--<code>np.mmap</code>), but it's one I'll be working on in the near future. Another solution is to read the file in smaller pieces (use <code>iterator=True, chunksize=1000</code>) then concatenate then with <code>pd.concat</code>. The problem comes in when you pull the entire text file into memory in one big slurp.</p>
";;[];['read_csv', 'np.mmap', 'iterator=True, chunksize=1000', 'pd.concat']
559;;2;11643893;11640243.0;2;51;;;"<p>I think this might work:</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np
from pandas import DataFrame
df = DataFrame(np.random.randn(5, 3), columns=['A', 'B', 'C'])

fig, ax = plt.subplots()
ax2, ax3 = ax.twinx(), ax.twinx()
rspine = ax3.spines['right']
rspine.set_position(('axes', 1.25))
ax3.set_frame_on(True)
ax3.patch.set_visible(False)
fig.subplots_adjust(right=0.75)

df.A.plot(ax=ax, style='b-')
df.B.plot(ax=ax2, style='r-', secondary_y=True)
df.C.plot(ax=ax3, style='g-')
</code></pre>

<p>Output:</p>

<p><a href=""https://i.stack.imgur.com/XEkyp.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/XEkyp.png"" alt=""Output""></a></p>
";;"[""import matplotlib.pyplot as plt\nimport numpy as np\nfrom pandas import DataFrame\ndf = DataFrame(np.random.randn(5, 3), columns=['A', 'B', 'C'])\n\nfig, ax = plt.subplots()\nax2, ax3 = ax.twinx(), ax.twinx()\nrspine = ax3.spines['right']\nrspine.set_position(('axes', 1.25))\nax3.set_frame_on(True)\nax3.patch.set_visible(False)\nfig.subplots_adjust(right=0.75)\n\ndf.A.plot(ax=ax, style='b-')\ndf.B.plot(ax=ax2, style='r-', secondary_y=True)\ndf.C.plot(ax=ax3, style='g-')\n""]";"[""import matplotlib.pyplot as plt\nimport numpy as np\nfrom pandas import DataFrame\ndf = DataFrame(np.random.randn(5, 3), columns=['A', 'B', 'C'])\n\nfig, ax = plt.subplots()\nax2, ax3 = ax.twinx(), ax.twinx()\nrspine = ax3.spines['right']\nrspine.set_position(('axes', 1.25))\nax3.set_frame_on(True)\nax3.patch.set_visible(False)\nfig.subplots_adjust(right=0.75)\n\ndf.A.plot(ax=ax, style='b-')\ndf.B.plot(ax=ax2, style='r-', secondary_y=True)\ndf.C.plot(ax=ax3, style='g-')\n""]"
560;;4;11706782;7837722.0;2;20;;;"<p>I checked out <code>iterrows</code> after noticing <a href=""https://stackoverflow.com/users/475872/nick-crawford"">Nick Crawford's</a> answer, but found that it yields (index, Series) tuples. Not sure which would work best for you, but I ended up using the <code>itertuples</code> method for my problem, which yields (index, row_value1...) tuples.</p>

<p>There's also <code>iterkv</code>, which iterates through (column, series) tuples.</p>
";;[];['iterrows', 'itertuples', 'iterkv']
561;;1;11707706;11707586.0;2;15;;;"<p>You can use <code>print df.describe().to_string()</code> to force it to show the whole table.  (You can use <code>to_string()</code> like this for any DataFrame.  The result of <code>describe</code> is just a DataFrame itself.)</p>

<p>The 8 is the number of rows in the DataFrame holding the ""description"" (because <code>describe</code> computes 8 statistics, min, max, mean, etc.).</p>
";;[];['print df.describe().to_string()', 'to_string()', 'describe', 'describe']
562;;2;11708664;11707586.0;2;21;;;"<p>You can adjust pandas print options with <code>set_printoptions</code>.</p>

<pre><code>In [3]: df.describe()
Out[3]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
Index: 8 entries, count to max
Data columns:
x1    8  non-null values
x2    8  non-null values
x3    8  non-null values
x4    8  non-null values
x5    8  non-null values
x6    8  non-null values
x7    8  non-null values
dtypes: float64(7)

In [4]: pd.set_printoptions(precision=2)

In [5]: df.describe()
Out[5]: 
            x1       x2       x3       x4       x5       x6       x7
count      8.0      8.0      8.0      8.0      8.0      8.0      8.0
mean   69024.5  69025.5  69026.5  69027.5  69028.5  69029.5  69030.5
std       17.1     17.1     17.1     17.1     17.1     17.1     17.1
min    69000.0  69001.0  69002.0  69003.0  69004.0  69005.0  69006.0
25%    69012.2  69013.2  69014.2  69015.2  69016.2  69017.2  69018.2
50%    69024.5  69025.5  69026.5  69027.5  69028.5  69029.5  69030.5
75%    69036.8  69037.8  69038.8  69039.8  69040.8  69041.8  69042.8
max    69049.0  69050.0  69051.0  69052.0  69053.0  69054.0  69055.0
</code></pre>

<p>However this will not work in all cases as pandas detects your console width and it will only use <code>to_string</code> if the output fits in the console (see the docstring of <code>set_printoptions</code>). 
In this case you can explicitly call <code>to_string</code> as answered by <a href=""https://stackoverflow.com/a/11707706/1301710"">BrenBarn</a>.</p>

<p><strong>Update</strong></p>

<p>With version 0.10 the way wide dataframes are printed <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#wide-dataframe-print"" rel=""nofollow noreferrer"">changed</a>:</p>

<pre><code>In [3]: df.describe()
Out[3]: 
                 x1            x2            x3            x4            x5  \
count      8.000000      8.000000      8.000000      8.000000      8.000000   
mean   59832.361578  27356.711336  49317.281222  51214.837838  51254.839690   
std    22600.723536  26867.192716  28071.737509  21012.422793  33831.515761   
min    31906.695474   1648.359160     56.378115  16278.322271     43.745574   
25%    45264.625201  12799.540572  41429.628749  40374.273582  29789.643875   
50%    56340.214856  18666.456293  51995.661512  54894.562656  47667.684422   
75%    75587.003417  31375.610322  61069.190523  67811.893435  76014.884048   
max    98136.474782  84544.484627  91743.983895  75154.587156  99012.695717   

                 x6            x7  
count      8.000000      8.000000  
mean   41863.000717  33950.235126  
std    38709.468281  29075.745673  
min     3590.990740   1833.464154  
25%    15145.759625   6879.523949  
50%    22139.243042  33706.029946  
75%    72038.983496  51449.893980  
max    98601.190488  83309.051963  
</code></pre>

<p>Further more the API for setting pandas options changed:</p>

<pre><code>In [4]: pd.set_option('display.precision', 2)

In [5]: df.describe()
Out[5]: 
            x1       x2       x3       x4       x5       x6       x7
count      8.0      8.0      8.0      8.0      8.0      8.0      8.0
mean   59832.4  27356.7  49317.3  51214.8  51254.8  41863.0  33950.2
std    22600.7  26867.2  28071.7  21012.4  33831.5  38709.5  29075.7
min    31906.7   1648.4     56.4  16278.3     43.7   3591.0   1833.5
25%    45264.6  12799.5  41429.6  40374.3  29789.6  15145.8   6879.5
50%    56340.2  18666.5  51995.7  54894.6  47667.7  22139.2  33706.0
75%    75587.0  31375.6  61069.2  67811.9  76014.9  72039.0  51449.9
max    98136.5  84544.5  91744.0  75154.6  99012.7  98601.2  83309.1
</code></pre>
";;"[""In [3]: df.describe()\nOut[3]: \n<class 'pandas.core.frame.DataFrame'>\nIndex: 8 entries, count to max\nData columns:\nx1    8  non-null values\nx2    8  non-null values\nx3    8  non-null values\nx4    8  non-null values\nx5    8  non-null values\nx6    8  non-null values\nx7    8  non-null values\ndtypes: float64(7)\n\nIn [4]: pd.set_printoptions(precision=2)\n\nIn [5]: df.describe()\nOut[5]: \n            x1       x2       x3       x4       x5       x6       x7\ncount      8.0      8.0      8.0      8.0      8.0      8.0      8.0\nmean   69024.5  69025.5  69026.5  69027.5  69028.5  69029.5  69030.5\nstd       17.1     17.1     17.1     17.1     17.1     17.1     17.1\nmin    69000.0  69001.0  69002.0  69003.0  69004.0  69005.0  69006.0\n25%    69012.2  69013.2  69014.2  69015.2  69016.2  69017.2  69018.2\n50%    69024.5  69025.5  69026.5  69027.5  69028.5  69029.5  69030.5\n75%    69036.8  69037.8  69038.8  69039.8  69040.8  69041.8  69042.8\nmax    69049.0  69050.0  69051.0  69052.0  69053.0  69054.0  69055.0\n"", 'In [3]: df.describe()\nOut[3]: \n                 x1            x2            x3            x4            x5  \\\ncount      8.000000      8.000000      8.000000      8.000000      8.000000   \nmean   59832.361578  27356.711336  49317.281222  51214.837838  51254.839690   \nstd    22600.723536  26867.192716  28071.737509  21012.422793  33831.515761   \nmin    31906.695474   1648.359160     56.378115  16278.322271     43.745574   \n25%    45264.625201  12799.540572  41429.628749  40374.273582  29789.643875   \n50%    56340.214856  18666.456293  51995.661512  54894.562656  47667.684422   \n75%    75587.003417  31375.610322  61069.190523  67811.893435  76014.884048   \nmax    98136.474782  84544.484627  91743.983895  75154.587156  99012.695717   \n\n                 x6            x7  \ncount      8.000000      8.000000  \nmean   41863.000717  33950.235126  \nstd    38709.468281  29075.745673  \nmin     3590.990740   1833.464154  \n25%    15145.759625   6879.523949  \n50%    22139.243042  33706.029946  \n75%    72038.983496  51449.893980  \nmax    98601.190488  83309.051963  \n', ""In [4]: pd.set_option('display.precision', 2)\n\nIn [5]: df.describe()\nOut[5]: \n            x1       x2       x3       x4       x5       x6       x7\ncount      8.0      8.0      8.0      8.0      8.0      8.0      8.0\nmean   59832.4  27356.7  49317.3  51214.8  51254.8  41863.0  33950.2\nstd    22600.7  26867.2  28071.7  21012.4  33831.5  38709.5  29075.7\nmin    31906.7   1648.4     56.4  16278.3     43.7   3591.0   1833.5\n25%    45264.6  12799.5  41429.6  40374.3  29789.6  15145.8   6879.5\n50%    56340.2  18666.5  51995.7  54894.6  47667.7  22139.2  33706.0\n75%    75587.0  31375.6  61069.2  67811.9  76014.9  72039.0  51449.9\nmax    98136.5  84544.5  91744.0  75154.6  99012.7  98601.2  83309.1\n""]";"['set_printoptions', ""In [3]: df.describe()\nOut[3]: \n<class 'pandas.core.frame.DataFrame'>\nIndex: 8 entries, count to max\nData columns:\nx1    8  non-null values\nx2    8  non-null values\nx3    8  non-null values\nx4    8  non-null values\nx5    8  non-null values\nx6    8  non-null values\nx7    8  non-null values\ndtypes: float64(7)\n\nIn [4]: pd.set_printoptions(precision=2)\n\nIn [5]: df.describe()\nOut[5]: \n            x1       x2       x3       x4       x5       x6       x7\ncount      8.0      8.0      8.0      8.0      8.0      8.0      8.0\nmean   69024.5  69025.5  69026.5  69027.5  69028.5  69029.5  69030.5\nstd       17.1     17.1     17.1     17.1     17.1     17.1     17.1\nmin    69000.0  69001.0  69002.0  69003.0  69004.0  69005.0  69006.0\n25%    69012.2  69013.2  69014.2  69015.2  69016.2  69017.2  69018.2\n50%    69024.5  69025.5  69026.5  69027.5  69028.5  69029.5  69030.5\n75%    69036.8  69037.8  69038.8  69039.8  69040.8  69041.8  69042.8\nmax    69049.0  69050.0  69051.0  69052.0  69053.0  69054.0  69055.0\n"", 'to_string', 'set_printoptions', 'to_string', 'In [3]: df.describe()\nOut[3]: \n                 x1            x2            x3            x4            x5  \\\ncount      8.000000      8.000000      8.000000      8.000000      8.000000   \nmean   59832.361578  27356.711336  49317.281222  51214.837838  51254.839690   \nstd    22600.723536  26867.192716  28071.737509  21012.422793  33831.515761   \nmin    31906.695474   1648.359160     56.378115  16278.322271     43.745574   \n25%    45264.625201  12799.540572  41429.628749  40374.273582  29789.643875   \n50%    56340.214856  18666.456293  51995.661512  54894.562656  47667.684422   \n75%    75587.003417  31375.610322  61069.190523  67811.893435  76014.884048   \nmax    98136.474782  84544.484627  91743.983895  75154.587156  99012.695717   \n\n                 x6            x7  \ncount      8.000000      8.000000  \nmean   41863.000717  33950.235126  \nstd    38709.468281  29075.745673  \nmin     3590.990740   1833.464154  \n25%    15145.759625   6879.523949  \n50%    22139.243042  33706.029946  \n75%    72038.983496  51449.893980  \nmax    98601.190488  83309.051963  \n', ""In [4]: pd.set_option('display.precision', 2)\n\nIn [5]: df.describe()\nOut[5]: \n            x1       x2       x3       x4       x5       x6       x7\ncount      8.0      8.0      8.0      8.0      8.0      8.0      8.0\nmean   59832.4  27356.7  49317.3  51214.8  51254.8  41863.0  33950.2\nstd    22600.7  26867.2  28071.7  21012.4  33831.5  38709.5  29075.7\nmin    31906.7   1648.4     56.4  16278.3     43.7   3591.0   1833.5\n25%    45264.6  12799.5  41429.6  40374.3  29789.6  15145.8   6879.5\n50%    56340.2  18666.5  51995.7  54894.6  47667.7  22139.2  33706.0\n75%    75587.0  31375.6  61069.2  67811.9  76014.9  72039.0  51449.9\nmax    98136.5  84544.5  91744.0  75154.6  99012.7  98601.2  83309.1\n""]"
563;;8;11711637;11707586.0;2;228;;;"<p>As @bmu <a href=""https://stackoverflow.com/a/11708664/623735"">mentioned</a>, pandas auto detects (by default) the size of the display area, a summary view will be used when an object repr does not fit on the display. You mentioned resizing the IDLE window, to no effect. If you do <code>print df.describe().to_string()</code> does it fit on the IDLE window?</p>

<p>The terminal size is determined by <code>pandas.util.terminal.get_terminal_size()</code>, this returns a tuple containing the <code>(width, height)</code> of the display. Does the output match the size of your IDLE window? There might be an issue (there was one before when running a terminal in emacs).</p>

<p>Note that it is possible to bypass the autodetect, <code>pandas.set_printoptions(max_rows=200, max_columns=10)</code> will never switch to summary view if number of rows, columns does not exceed the given limits.</p>

<hr>

<p><strong>Update: Pandas 0.11.0 onwards</strong></p>

<p><code>pandas.set_printoptions(...)</code> is depracted. Instead, use <code>pandas.set_option</code>. Like:</p>

<pre><code>import pandas as pd
pd.set_option('display.height', 1000)
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
</code></pre>

<p>Here is the help:</p>

<pre>
set_option(pat,value) - Sets the value of the specified option

Available options:
display.[chop_threshold, colheader_justify, column_space, date_dayfirst,
         date_yearfirst, encoding, expand_frame_repr, float_format, height,
         line_width, max_columns, max_colwidth, max_info_columns, max_info_rows,
         max_rows, max_seq_items, mpl_style, multi_sparse, notebook_repr_html,
         pprint_nest_depth, precision, width]
mode.[sim_interactive, use_inf_as_null]

Parameters
----------
pat - str/regexp which should match a single option.

Note: partial matches are supported for convenience, but unless you use the
full option name (e.g. x.y.z.option_name), your code may break in future
versions if new options with similar names are introduced.

value - new value of option.

Returns
-------
None

Raises
------
KeyError if no such option exists

display.chop_threshold: [default: None] [currently: None]
: float or None
        if set to a float value, all float values smaller then the given threshold
        will be displayed as exactly 0 by repr and friends.
display.colheader_justify: [default: right] [currently: right]
: 'left'/'right'
        Controls the justification of column headers. used by DataFrameFormatter.
display.column_space: [default: 12] [currently: 12]No description available.

display.date_dayfirst: [default: False] [currently: False]
: boolean
        When True, prints and parses dates with the day first, eg 20/01/2005
display.date_yearfirst: [default: False] [currently: False]
: boolean
        When True, prints and parses dates with the year first, eg 2005/01/20
display.encoding: [default: UTF-8] [currently: UTF-8]
: str/unicode
        Defaults to the detected encoding of the console.
        Specifies the encoding to be used for strings returned by to_string,
        these are generally strings meant to be displayed on the console.
display.expand_frame_repr: [default: True] [currently: True]
: boolean
        Whether to print out the full DataFrame repr for wide DataFrames
        across multiple lines, `max_columns` is still respected, but the output will
        wrap-around across multiple ""pages"" if it's width exceeds `display.width`.
display.float_format: [default: None] [currently: None]
: callable
        The callable should accept a floating point number and return
        a string with the desired format of the number. This is used
        in some places like SeriesFormatter.
        See core.format.EngFormatter for an example.
display.height: [default: 60] [currently: 1000]
: int
        Deprecated.
        (Deprecated, use `display.height` instead.)

display.line_width: [default: 80] [currently: 1000]
: int
        Deprecated.
        (Deprecated, use `display.width` instead.)

display.max_columns: [default: 20] [currently: 500]
: int
        max_rows and max_columns are used in __repr__() methods to decide if
        to_string() or info() is used to render an object to a string.  In case
        python/IPython is running in a terminal this can be set to 0 and pandas
        will correctly auto-detect the width the terminal and swap to a smaller
        format in case all columns would not fit vertically. The IPython notebook,
        IPython qtconsole, or IDLE do not run in a terminal and hence it is not
        possible to do correct auto-detection.
        'None' value means unlimited.
display.max_colwidth: [default: 50] [currently: 50]
: int
        The maximum width in characters of a column in the repr of
        a pandas data structure. When the column overflows, a ""...""
        placeholder is embedded in the output.
display.max_info_columns: [default: 100] [currently: 100]
: int
        max_info_columns is used in DataFrame.info method to decide if
        per column information will be printed.
display.max_info_rows: [default: 1690785] [currently: 1690785]
: int or None
        max_info_rows is the maximum number of rows for which a frame will
        perform a null check on its columns when repr'ing To a console.
        The default is 1,000,000 rows. So, if a DataFrame has more
        1,000,000 rows there will be no null check performed on the
        columns and thus the representation will take much less time to
        display in an interactive session. A value of None means always
        perform a null check when repr'ing.
display.max_rows: [default: 60] [currently: 500]
: int
        This sets the maximum number of rows pandas should output when printing
        out various output. For example, this value determines whether the repr()
        for a dataframe prints out fully or just a summary repr.
        'None' value means unlimited.
display.max_seq_items: [default: None] [currently: None]
: int or None

        when pretty-printing a long sequence, no more then `max_seq_items`
        will be printed. If items are ommitted, they will be denoted by the addition
        of ""..."" to the resulting string.

        If set to None, the number of items to be printed is unlimited.
display.mpl_style: [default: None] [currently: None]
: bool

        Setting this to 'default' will modify the rcParams used by matplotlib
        to give plots a more pleasing visual style by default.
        Setting this to None/False restores the values to their initial value.
display.multi_sparse: [default: True] [currently: True]
: boolean
        ""sparsify"" MultiIndex display (don't display repeated
        elements in outer levels within groups)
display.notebook_repr_html: [default: True] [currently: True]
: boolean
        When True, IPython notebook will use html representation for
        pandas objects (if it is available).
display.pprint_nest_depth: [default: 3] [currently: 3]
: int
        Controls the number of nested levels to process when pretty-printing
display.precision: [default: 7] [currently: 7]
: int
        Floating point output precision (number of significant digits). This is
        only a suggestion
display.width: [default: 80] [currently: 1000]
: int
        Width of the display in characters. In case python/IPython is running in
        a terminal this can be set to None and pandas will correctly auto-detect the
        width.
        Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a
        terminal and hence it is not possible to correctly detect the width.
mode.sim_interactive: [default: False] [currently: False]
: boolean
        Whether to simulate interactive mode for purposes of testing
mode.use_inf_as_null: [default: False] [currently: False]
: boolean
        True means treat None, NaN, INF, -INF as null (old way),
        False means None and NaN are null, but INF, -INF are not null
        (new way).
Call def:   pd.set_option(self, *args, **kwds)
</pre>
";;"[""import pandas as pd\npd.set_option('display.height', 1000)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n"", '\nset_option(pat,value) - Sets the value of the specified option\n\nAvailable options:\ndisplay.[chop_threshold, colheader_justify, column_space, date_dayfirst,\n         date_yearfirst, encoding, expand_frame_repr, float_format, height,\n         line_width, max_columns, max_colwidth, max_info_columns, max_info_rows,\n         max_rows, max_seq_items, mpl_style, multi_sparse, notebook_repr_html,\n         pprint_nest_depth, precision, width]\nmode.[sim_interactive, use_inf_as_null]\n\nParameters\n----------\npat - str/regexp which should match a single option.\n\nNote: partial matches are supported for convenience, but unless you use the\nfull option name (e.g. x.y.z.option_name), your code may break in future\nversions if new options with similar names are introduced.\n\nvalue - new value of option.\n\nReturns\n-------\nNone\n\nRaises\n------\nKeyError if no such option exists\n\ndisplay.chop_threshold: [default: None] [currently: None]\n: float or None\n        if set to a float value, all float values smaller then the given threshold\n        will be displayed as exactly 0 by repr and friends.\ndisplay.colheader_justify: [default: right] [currently: right]\n: \'left\'/\'right\'\n        Controls the justification of column headers. used by DataFrameFormatter.\ndisplay.column_space: [default: 12] [currently: 12]No description available.\n\ndisplay.date_dayfirst: [default: False] [currently: False]\n: boolean\n        When True, prints and parses dates with the day first, eg 20/01/2005\ndisplay.date_yearfirst: [default: False] [currently: False]\n: boolean\n        When True, prints and parses dates with the year first, eg 2005/01/20\ndisplay.encoding: [default: UTF-8] [currently: UTF-8]\n: str/unicode\n        Defaults to the detected encoding of the console.\n        Specifies the encoding to be used for strings returned by to_string,\n        these are generally strings meant to be displayed on the console.\ndisplay.expand_frame_repr: [default: True] [currently: True]\n: boolean\n        Whether to print out the full DataFrame repr for wide DataFrames\n        across multiple lines, `max_columns` is still respected, but the output will\n        wrap-around across multiple ""pages"" if it\'s width exceeds `display.width`.\ndisplay.float_format: [default: None] [currently: None]\n: callable\n        The callable should accept a floating point number and return\n        a string with the desired format of the number. This is used\n        in some places like SeriesFormatter.\n        See core.format.EngFormatter for an example.\ndisplay.height: [default: 60] [currently: 1000]\n: int\n        Deprecated.\n        (Deprecated, use `display.height` instead.)\n\ndisplay.line_width: [default: 80] [currently: 1000]\n: int\n        Deprecated.\n        (Deprecated, use `display.width` instead.)\n\ndisplay.max_columns: [default: 20] [currently: 500]\n: int\n        max_rows and max_columns are used in __repr__() methods to decide if\n        to_string() or info() is used to render an object to a string.  In case\n        python/IPython is running in a terminal this can be set to 0 and pandas\n        will correctly auto-detect the width the terminal and swap to a smaller\n        format in case all columns would not fit vertically. The IPython notebook,\n        IPython qtconsole, or IDLE do not run in a terminal and hence it is not\n        possible to do correct auto-detection.\n        \'None\' value means unlimited.\ndisplay.max_colwidth: [default: 50] [currently: 50]\n: int\n        The maximum width in characters of a column in the repr of\n        a pandas data structure. When the column overflows, a ""...""\n        placeholder is embedded in the output.\ndisplay.max_info_columns: [default: 100] [currently: 100]\n: int\n        max_info_columns is used in DataFrame.info method to decide if\n        per column information will be printed.\ndisplay.max_info_rows: [default: 1690785] [currently: 1690785]\n: int or None\n        max_info_rows is the maximum number of rows for which a frame will\n        perform a null check on its columns when repr\'ing To a console.\n        The default is 1,000,000 rows. So, if a DataFrame has more\n        1,000,000 rows there will be no null check performed on the\n        columns and thus the representation will take much less time to\n        display in an interactive session. A value of None means always\n        perform a null check when repr\'ing.\ndisplay.max_rows: [default: 60] [currently: 500]\n: int\n        This sets the maximum number of rows pandas should output when printing\n        out various output. For example, this value determines whether the repr()\n        for a dataframe prints out fully or just a summary repr.\n        \'None\' value means unlimited.\ndisplay.max_seq_items: [default: None] [currently: None]\n: int or None\n\n        when pretty-printing a long sequence, no more then `max_seq_items`\n        will be printed. If items are ommitted, they will be denoted by the addition\n        of ""..."" to the resulting string.\n\n        If set to None, the number of items to be printed is unlimited.\ndisplay.mpl_style: [default: None] [currently: None]\n: bool\n\n        Setting this to \'default\' will modify the rcParams used by matplotlib\n        to give plots a more pleasing visual style by default.\n        Setting this to None/False restores the values to their initial value.\ndisplay.multi_sparse: [default: True] [currently: True]\n: boolean\n        ""sparsify"" MultiIndex display (don\'t display repeated\n        elements in outer levels within groups)\ndisplay.notebook_repr_html: [default: True] [currently: True]\n: boolean\n        When True, IPython notebook will use html representation for\n        pandas objects (if it is available).\ndisplay.pprint_nest_depth: [default: 3] [currently: 3]\n: int\n        Controls the number of nested levels to process when pretty-printing\ndisplay.precision: [default: 7] [currently: 7]\n: int\n        Floating point output precision (number of significant digits). This is\n        only a suggestion\ndisplay.width: [default: 80] [currently: 1000]\n: int\n        Width of the display in characters. In case python/IPython is running in\n        a terminal this can be set to None and pandas will correctly auto-detect the\n        width.\n        Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a\n        terminal and hence it is not possible to correctly detect the width.\nmode.sim_interactive: [default: False] [currently: False]\n: boolean\n        Whether to simulate interactive mode for purposes of testing\nmode.use_inf_as_null: [default: False] [currently: False]\n: boolean\n        True means treat None, NaN, INF, -INF as null (old way),\n        False means None and NaN are null, but INF, -INF are not null\n        (new way).\nCall def:   pd.set_option(self, *args, **kwds)\n']";"['print df.describe().to_string()', 'pandas.util.terminal.get_terminal_size()', '(width, height)', 'pandas.set_printoptions(max_rows=200, max_columns=10)', 'pandas.set_printoptions(...)', 'pandas.set_option', ""import pandas as pd\npd.set_option('display.height', 1000)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n""]"
564;;0;11858532;11858472.0;2;74;;;"<p><code>df['bar'] = df.bar.map(str) + "" is "" + df.foo</code>.</p>
";;[];"['df[\'bar\'] = df.bar.map(str) + "" is "" + df.foo']"
565;;3;11872393;11869910.0;2;186;;;"<p>I'm not entirely sure what you want, and your last line of code does not help either, but anyway:</p>

<p>""Chained"" filtering is done by ""chaining"" the criteria in the boolean index.</p>

<pre><code>In [96]: df
Out[96]:
   A  B  C  D
a  1  4  9  1
b  4  5  0  2
c  5  5  1  0
d  1  3  9  6

In [99]: df[(df.A == 1) &amp; (df.D == 6)]
Out[99]:
   A  B  C  D
d  1  3  9  6
</code></pre>

<p>If you want to chain methods, you can add your own mask method and use that one.</p>

<pre><code>In [90]: def mask(df, key, value):
   ....:     return df[df[key] == value]
   ....:

In [92]: pandas.DataFrame.mask = mask

In [93]: df = pandas.DataFrame(np.random.randint(0, 10, (4,4)), index=list('abcd'), columns=list('ABCD'))

In [95]: df.ix['d','A'] = df.ix['a', 'A']

In [96]: df
Out[96]:
   A  B  C  D
a  1  4  9  1
b  4  5  0  2
c  5  5  1  0
d  1  3  9  6

In [97]: df.mask('A', 1)
Out[97]:
   A  B  C  D
a  1  4  9  1
d  1  3  9  6

In [98]: df.mask('A', 1).mask('D', 6)
Out[98]:
   A  B  C  D
d  1  3  9  6
</code></pre>
";;"['In [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [99]: df[(df.A == 1) & (df.D == 6)]\nOut[99]:\n   A  B  C  D\nd  1  3  9  6\n', ""In [90]: def mask(df, key, value):\n   ....:     return df[df[key] == value]\n   ....:\n\nIn [92]: pandas.DataFrame.mask = mask\n\nIn [93]: df = pandas.DataFrame(np.random.randint(0, 10, (4,4)), index=list('abcd'), columns=list('ABCD'))\n\nIn [95]: df.ix['d','A'] = df.ix['a', 'A']\n\nIn [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [97]: df.mask('A', 1)\nOut[97]:\n   A  B  C  D\na  1  4  9  1\nd  1  3  9  6\n\nIn [98]: df.mask('A', 1).mask('D', 6)\nOut[98]:\n   A  B  C  D\nd  1  3  9  6\n""]";"['In [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [99]: df[(df.A == 1) & (df.D == 6)]\nOut[99]:\n   A  B  C  D\nd  1  3  9  6\n', ""In [90]: def mask(df, key, value):\n   ....:     return df[df[key] == value]\n   ....:\n\nIn [92]: pandas.DataFrame.mask = mask\n\nIn [93]: df = pandas.DataFrame(np.random.randint(0, 10, (4,4)), index=list('abcd'), columns=list('ABCD'))\n\nIn [95]: df.ix['d','A'] = df.ix['a', 'A']\n\nIn [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [97]: df.mask('A', 1)\nOut[97]:\n   A  B  C  D\na  1  4  9  1\nd  1  3  9  6\n\nIn [98]: df.mask('A', 1).mask('D', 6)\nOut[98]:\n   A  B  C  D\nd  1  3  9  6\n""]"
566;;0;11874590;11858472.0;2;30;;;"<p>The problem in your code is that you want to apply the operation on every row. The way you've written it though takes the whole 'bar' and 'foo' columns, converts them to strings and gives you back one big string. You can write it like:</p>

<pre><code>df.apply(lambda x:'%s is %s' % (x['bar'],x['foo']),axis=1)
</code></pre>

<p>It's longer than the other answer but is more generic (can be used with values that are not strings).</p>
";;"[""df.apply(lambda x:'%s is %s' % (x['bar'],x['foo']),axis=1)\n""]";"[""df.apply(lambda x:'%s is %s' % (x['bar'],x['foo']),axis=1)\n""]"
567;;2;11893375;11869910.0;2;49;;;"<p>The answer from @lodagro is great. I would extend it by generalizing the mask function as:</p>

<pre><code>def mask(df, f):
  return df[f(df)]
</code></pre>

<p>Then you can do stuff like:</p>

<pre><code>df.mask(lambda x: x[0] &lt; 0).mask(lambda x: x[1] &gt; 0)
</code></pre>
";;['def mask(df, f):\n  return df[f(df)]\n', 'df.mask(lambda x: x[0] < 0).mask(lambda x: x[1] > 0)\n'];['def mask(df, f):\n  return df[f(df)]\n', 'df.mask(lambda x: x[0] < 0).mask(lambda x: x[1] > 0)\n']
568;;8;11927922;11927715.0;2;61;;;"<p>You can specify the <code>color</code> option as a list directly to the <code>plot</code> function.</p>

<pre><code>from matplotlib import pyplot as plt
from itertools import cycle, islice
import pandas, numpy as np  # I find np.random.randint to be better

# Make the data
x = [{i:np.random.randint(1,5)} for i in range(10)]
df = pandas.DataFrame(x)

# Make a list by cycling through the colors you care about
# to match the length of your data.
my_colors = list(islice(cycle(['b', 'r', 'g', 'y', 'k']), None, len(df)))

# Specify this list of colors as the `color` option to `plot`.
df.plot(kind='bar', stacked=True, color=my_colors)
</code></pre>

<p>To define your own custom list, you can do a few of the following, or just look up the Matplotlib techniques for defining a color item by its RGB values, etc. You can get as complicated as you want with this.</p>

<pre><code>my_colors = ['g', 'b']*5 # &lt;-- this concatenates the list to itself 5 times.
my_colors = [(0.5,0.4,0.5), (0.75, 0.75, 0.25)]*5 # &lt;-- make two custom RGBs and repeat/alternate them over all the bar elements.
my_colors = [(x/10.0, x/20.0, 0.75) for x in range(len(df))] # &lt;-- Quick gradient example along the Red/Green dimensions.
</code></pre>

<p>The last example yields the follow simple gradient of colors for me:</p>

<p><img src=""https://i.stack.imgur.com/D0hRr.png"" alt=""enter image description here""></p>

<p>I didn't play with it long enough to figure out how to force the legend to pick up the defined colors, but I'm sure you can do it.</p>

<p>In general, though, a big piece of advice is to just use the functions from Matplotlib directly. Calling them from Pandas is OK, but I find you get better options and performance calling them straight from Matplotlib.</p>
";;"[""from matplotlib import pyplot as plt\nfrom itertools import cycle, islice\nimport pandas, numpy as np  # I find np.random.randint to be better\n\n# Make the data\nx = [{i:np.random.randint(1,5)} for i in range(10)]\ndf = pandas.DataFrame(x)\n\n# Make a list by cycling through the colors you care about\n# to match the length of your data.\nmy_colors = list(islice(cycle(['b', 'r', 'g', 'y', 'k']), None, len(df)))\n\n# Specify this list of colors as the `color` option to `plot`.\ndf.plot(kind='bar', stacked=True, color=my_colors)\n"", ""my_colors = ['g', 'b']*5 # <-- this concatenates the list to itself 5 times.\nmy_colors = [(0.5,0.4,0.5), (0.75, 0.75, 0.25)]*5 # <-- make two custom RGBs and repeat/alternate them over all the bar elements.\nmy_colors = [(x/10.0, x/20.0, 0.75) for x in range(len(df))] # <-- Quick gradient example along the Red/Green dimensions.\n""]";"['color', 'plot', ""from matplotlib import pyplot as plt\nfrom itertools import cycle, islice\nimport pandas, numpy as np  # I find np.random.randint to be better\n\n# Make the data\nx = [{i:np.random.randint(1,5)} for i in range(10)]\ndf = pandas.DataFrame(x)\n\n# Make a list by cycling through the colors you care about\n# to match the length of your data.\nmy_colors = list(islice(cycle(['b', 'r', 'g', 'y', 'k']), None, len(df)))\n\n# Specify this list of colors as the `color` option to `plot`.\ndf.plot(kind='bar', stacked=True, color=my_colors)\n"", ""my_colors = ['g', 'b']*5 # <-- this concatenates the list to itself 5 times.\nmy_colors = [(0.5,0.4,0.5), (0.75, 0.75, 0.25)]*5 # <-- make two custom RGBs and repeat/alternate them over all the bar elements.\nmy_colors = [(x/10.0, x/20.0, 0.75) for x in range(len(df))] # <-- Quick gradient example along the Red/Green dimensions.\n""]"
569;;10;11941772;11941492.0;2;9;;;"<p>Syntax like the following will work:</p>

<pre><code>df.ix['a']
df.ix['a'].ix['c']
</code></pre>

<p>since <code>group1</code> and <code>group2</code> are indices. Please forgive my previous attempt!</p>

<p>To get at the second index only, I think you have to swap indices:</p>

<pre><code>df.swaplevel(0,1).ix['c']
</code></pre>

<p>But I'm sure Wes will correct me if I'm wrong.</p>
";;"[""df.ix['a']\ndf.ix['a'].ix['c']\n"", ""df.swaplevel(0,1).ix['c']\n""]";"[""df.ix['a']\ndf.ix['a'].ix['c']\n"", 'group1', 'group2', ""df.swaplevel(0,1).ix['c']\n""]"
570;;9;11942697;11941492.0;2;47;;;"<p>Try using <code>xs</code> to be very precise:</p>

<pre><code>In [5]: df.xs('a', level=0)
Out[5]: 
        value1  value2
group2                
c          1.1     7.1
c          2.0     8.0
d          3.0     9.0

In [6]: df.xs('c', level='group2')
Out[6]: 
        value1  value2
group1                
a          1.1     7.1
a          2.0     8.0
</code></pre>
";;"[""In [5]: df.xs('a', level=0)\nOut[5]: \n        value1  value2\ngroup2                \nc          1.1     7.1\nc          2.0     8.0\nd          3.0     9.0\n\nIn [6]: df.xs('c', level='group2')\nOut[6]: \n        value1  value2\ngroup1                \na          1.1     7.1\na          2.0     8.0\n""]";"['xs', ""In [5]: df.xs('a', level=0)\nOut[5]: \n        value1  value2\ngroup2                \nc          1.1     7.1\nc          2.0     8.0\nd          3.0     9.0\n\nIn [6]: df.xs('c', level='group2')\nOut[6]: \n        value1  value2\ngroup1                \na          1.1     7.1\na          2.0     8.0\n""]"
571;;5;11982843;11976503.0;2;50;;;"<pre><code>In [5]: a.reset_index().merge(b, how=""left"").set_index('index')
Out[5]:
       col1  to_merge_on  col2
index
a         1            1     1
b         2            3     2
c         3            4   NaN
</code></pre>
";;"['In [5]: a.reset_index().merge(b, how=""left"").set_index(\'index\')\nOut[5]:\n       col1  to_merge_on  col2\nindex\na         1            1     1\nb         2            3     2\nc         3            4   NaN\n']";"['In [5]: a.reset_index().merge(b, how=""left"").set_index(\'index\')\nOut[5]:\n       col1  to_merge_on  col2\nindex\na         1            1     1\nb         2            3     2\nc         3            4   NaN\n']"
572;;0;12022047;12021730.0;2;18;;;"<p>I think there's just a missing <code>\</code> in the docs (maybe because it was interpreted as an escape marker at some point?)  It's a regexp, after all:</p>

<pre><code>In [68]: data = read_table('sample.txt', skiprows=3, header=None, sep=r""\s*"")

In [69]: data
Out[69]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 7 entries, 0 to 6
Data columns:
X.1     7  non-null values
X.2     7  non-null values
X.3     7  non-null values
X.4     7  non-null values
X.5     7  non-null values
X.6     7  non-null values
[...]
X.23    7  non-null values
X.24    7  non-null values
X.25    5  non-null values
X.26    3  non-null values
dtypes: float64(8), int64(10), object(8)
</code></pre>

<p>Because of the delimiter problem noted by @MRAB, it has some trouble with the last few columns:</p>

<pre><code>In [73]: data.ix[:,20:]
Out[73]: 
   X.21  X.22           X.23                   X.24            X.25    X.26
0   315  0.95            ABC            transporter   transmembrane  region
1   527  0.93            ABC            transporter            None    None
2   408  0.86  RecF/RecN/SMC                      N        terminal  domain
3   575  0.85  RecF/RecN/SMC                      N        terminal  domain
4   556  0.72            AAA                 ATPase          domain    None
5   275  0.85      YceG-like                 family            None    None
6   200  0.85       Pyridine  nucleotide-disulphide  oxidoreductase    None
</code></pre>

<p>but that can be patched up at the end.</p>
";;"['In [68]: data = read_table(\'sample.txt\', skiprows=3, header=None, sep=r""\\s*"")\n\nIn [69]: data\nOut[69]: \n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 7 entries, 0 to 6\nData columns:\nX.1     7  non-null values\nX.2     7  non-null values\nX.3     7  non-null values\nX.4     7  non-null values\nX.5     7  non-null values\nX.6     7  non-null values\n[...]\nX.23    7  non-null values\nX.24    7  non-null values\nX.25    5  non-null values\nX.26    3  non-null values\ndtypes: float64(8), int64(10), object(8)\n', 'In [73]: data.ix[:,20:]\nOut[73]: \n   X.21  X.22           X.23                   X.24            X.25    X.26\n0   315  0.95            ABC            transporter   transmembrane  region\n1   527  0.93            ABC            transporter            None    None\n2   408  0.86  RecF/RecN/SMC                      N        terminal  domain\n3   575  0.85  RecF/RecN/SMC                      N        terminal  domain\n4   556  0.72            AAA                 ATPase          domain    None\n5   275  0.85      YceG-like                 family            None    None\n6   200  0.85       Pyridine  nucleotide-disulphide  oxidoreductase    None\n']";"['\\', 'In [68]: data = read_table(\'sample.txt\', skiprows=3, header=None, sep=r""\\s*"")\n\nIn [69]: data\nOut[69]: \n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 7 entries, 0 to 6\nData columns:\nX.1     7  non-null values\nX.2     7  non-null values\nX.3     7  non-null values\nX.4     7  non-null values\nX.5     7  non-null values\nX.6     7  non-null values\n[...]\nX.23    7  non-null values\nX.24    7  non-null values\nX.25    5  non-null values\nX.26    3  non-null values\ndtypes: float64(8), int64(10), object(8)\n', 'In [73]: data.ix[:,20:]\nOut[73]: \n   X.21  X.22           X.23                   X.24            X.25    X.26\n0   315  0.95            ABC            transporter   transmembrane  region\n1   527  0.93            ABC            transporter            None    None\n2   408  0.86  RecF/RecN/SMC                      N        terminal  domain\n3   575  0.85  RecF/RecN/SMC                      N        terminal  domain\n4   556  0.72            AAA                 ATPase          domain    None\n5   275  0.85      YceG-like                 family            None    None\n6   200  0.85       Pyridine  nucleotide-disulphide  oxidoreductase    None\n']"
573;;0;12036847;10636024.0;2;11;;;"<p>I use <code>QTableWidget</code> from PyQt to display a <code>DataFrame</code>. I create a <code>QTableWidgetObject</code> and then populate with <code>QTableWidgetItems</code> created with <code>DataFrame</code> values.
Following is the snippet of code that reads a CSV file ,create a <code>DataFrame</code>, then display in a GUI:</p>

<pre><code>df  = read_csv(filename, index_col = 0,header = 0)
self.datatable = QtGui.QTableWidget(parent=self)
self.datatable.setColumnCount(len(df.columns))
self.datatable.setRowCount(len(df.index))
for i in range(len(df.index)):
    for j in range(len(df.columns)):
        self.datatable.setItem(i,j,QtGui.QTableWidgetItem(str(df.iget_value(i, j))))
</code></pre>
";;['df  = read_csv(filename, index_col = 0,header = 0)\nself.datatable = QtGui.QTableWidget(parent=self)\nself.datatable.setColumnCount(len(df.columns))\nself.datatable.setRowCount(len(df.index))\nfor i in range(len(df.index)):\n    for j in range(len(df.columns)):\n        self.datatable.setItem(i,j,QtGui.QTableWidgetItem(str(df.iget_value(i, j))))\n'];['QTableWidget', 'DataFrame', 'QTableWidgetObject', 'QTableWidgetItems', 'DataFrame', 'DataFrame', 'df  = read_csv(filename, index_col = 0,header = 0)\nself.datatable = QtGui.QTableWidget(parent=self)\nself.datatable.setColumnCount(len(df.columns))\nself.datatable.setRowCount(len(df.index))\nfor i in range(len(df.index)):\n    for j in range(len(df.columns)):\n        self.datatable.setItem(i,j,QtGui.QTableWidgetItem(str(df.iget_value(i, j))))\n']
574;;3;12056933;12047193.0;2;16;;;"<h3>Edit 2014-09-30:</h3>

<p>pandas now has a <code>read_sql</code> function. You definitely want to use that instead.</p>

<h3>Original answer:</h3>

<p>I can't help you with SQLAlchemy -- I always use pyodbc, MySQLdb, or psychopg2 as needed. But when doing so, a function as simple as the one below tends to suit my needs:</p>

<pre><code>import decimal

import pydobc
import numpy as np
import pandas

cnn, cur = myConnectToDBfunction()
cmd = ""SELECT * FROM myTable""
cur.execute(cmd)
dataframe = __processCursor(cur, dataframe=True)

def __processCursor(cur, dataframe=False, index=None):
    '''
    Processes a database cursor with data on it into either
    a structured numpy array or a pandas dataframe.

    input:
    cur - a pyodbc cursor that has just received data
    dataframe - bool. if false, a numpy record array is returned
                if true, return a pandas dataframe
    index - list of column(s) to use as index in a pandas dataframe
    '''
    datatypes = []
    colinfo = cur.description
    for col in colinfo:
        if col[1] == unicode:
            datatypes.append((col[0], 'U%d' % col[3]))
        elif col[1] == str:
            datatypes.append((col[0], 'S%d' % col[3]))
        elif col[1] in [float, decimal.Decimal]:
            datatypes.append((col[0], 'f4'))
        elif col[1] == datetime.datetime:
            datatypes.append((col[0], 'O4'))
        elif col[1] == int:
            datatypes.append((col[0], 'i4'))

    data = []
    for row in cur:
        data.append(tuple(row))

    array = np.array(data, dtype=datatypes)
    if dataframe:
        output = pandas.DataFrame.from_records(array)

        if index is not None:
            output = output.set_index(index)

    else:
        output = array

    return output
</code></pre>
";;"['import decimal\n\nimport pydobc\nimport numpy as np\nimport pandas\n\ncnn, cur = myConnectToDBfunction()\ncmd = ""SELECT * FROM myTable""\ncur.execute(cmd)\ndataframe = __processCursor(cur, dataframe=True)\n\ndef __processCursor(cur, dataframe=False, index=None):\n    \'\'\'\n    Processes a database cursor with data on it into either\n    a structured numpy array or a pandas dataframe.\n\n    input:\n    cur - a pyodbc cursor that has just received data\n    dataframe - bool. if false, a numpy record array is returned\n                if true, return a pandas dataframe\n    index - list of column(s) to use as index in a pandas dataframe\n    \'\'\'\n    datatypes = []\n    colinfo = cur.description\n    for col in colinfo:\n        if col[1] == unicode:\n            datatypes.append((col[0], \'U%d\' % col[3]))\n        elif col[1] == str:\n            datatypes.append((col[0], \'S%d\' % col[3]))\n        elif col[1] in [float, decimal.Decimal]:\n            datatypes.append((col[0], \'f4\'))\n        elif col[1] == datetime.datetime:\n            datatypes.append((col[0], \'O4\'))\n        elif col[1] == int:\n            datatypes.append((col[0], \'i4\'))\n\n    data = []\n    for row in cur:\n        data.append(tuple(row))\n\n    array = np.array(data, dtype=datatypes)\n    if dataframe:\n        output = pandas.DataFrame.from_records(array)\n\n        if index is not None:\n            output = output.set_index(index)\n\n    else:\n        output = array\n\n    return output\n']";"['read_sql', 'import decimal\n\nimport pydobc\nimport numpy as np\nimport pandas\n\ncnn, cur = myConnectToDBfunction()\ncmd = ""SELECT * FROM myTable""\ncur.execute(cmd)\ndataframe = __processCursor(cur, dataframe=True)\n\ndef __processCursor(cur, dataframe=False, index=None):\n    \'\'\'\n    Processes a database cursor with data on it into either\n    a structured numpy array or a pandas dataframe.\n\n    input:\n    cur - a pyodbc cursor that has just received data\n    dataframe - bool. if false, a numpy record array is returned\n                if true, return a pandas dataframe\n    index - list of column(s) to use as index in a pandas dataframe\n    \'\'\'\n    datatypes = []\n    colinfo = cur.description\n    for col in colinfo:\n        if col[1] == unicode:\n            datatypes.append((col[0], \'U%d\' % col[3]))\n        elif col[1] == str:\n            datatypes.append((col[0], \'S%d\' % col[3]))\n        elif col[1] in [float, decimal.Decimal]:\n            datatypes.append((col[0], \'f4\'))\n        elif col[1] == datetime.datetime:\n            datatypes.append((col[0], \'O4\'))\n        elif col[1] == int:\n            datatypes.append((col[0], \'i4\'))\n\n    data = []\n    for row in cur:\n        data.append(tuple(row))\n\n    array = np.array(data, dtype=datatypes)\n    if dataframe:\n        output = pandas.DataFrame.from_records(array)\n\n        if index is not None:\n            output = output.set_index(index)\n\n    else:\n        output = array\n\n    return output\n']"
575;;1;12060886;12047193.0;2;46;;;"<p>Here's the shortest code that will do the job:</p>

<pre><code>from pandas import DataFrame
df = DataFrame(resoverall.fetchall())
df.columns = resoverall.keys()
</code></pre>

<p>You can go fancier and parse the types as in Paul's answer.</p>
";;['from pandas import DataFrame\ndf = DataFrame(resoverall.fetchall())\ndf.columns = resoverall.keys()\n'];['from pandas import DataFrame\ndf = DataFrame(resoverall.fetchall())\ndf.columns = resoverall.keys()\n']
576;;12;12065904;12065885.0;2;326;;;"<p>Use the <code>isin</code> method.  <code>rpt[rpt['STK_ID'].isin(stk_list)]</code>.</p>
";;[];"['isin', ""rpt[rpt['STK_ID'].isin(stk_list)]""]"
577;;6;12098586;12096252.0;2;363;;;"<p>This is indeed a duplicate of <a href=""https://stackoverflow.com/questions/12065885/how-to-filter-the-dataframe-rows-of-pandas-by-within-in"">how to filter the dataframe rows of pandas by ""within""/""in""?</a>, translating the response to your example gives:</p>

<pre><code>In [5]: df = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})

In [6]: df
Out[6]:
   A  B
0  5  1
1  6  2
2  3  3
3  4  5

In [7]: df[df['A'].isin([3, 6])]
Out[7]:
   A  B
1  6  2
2  3  3
</code></pre>
";;"[""In [5]: df = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})\n\nIn [6]: df\nOut[6]:\n   A  B\n0  5  1\n1  6  2\n2  3  3\n3  4  5\n\nIn [7]: df[df['A'].isin([3, 6])]\nOut[7]:\n   A  B\n1  6  2\n2  3  3\n""]";"[""In [5]: df = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})\n\nIn [6]: df\nOut[6]:\n   A  B\n0  5  1\n1  6  2\n2  3  3\n3  4  5\n\nIn [7]: df[df['A'].isin([3, 6])]\nOut[7]:\n   A  B\n1  6  2\n2  3  3\n""]"
578;;3;12183507;12182744.0;2;65;;;"<p>The documentation explains this clearly. The apply method accepts a python function which should have a single parameter. If you want to pass more parameters you should use <code>functools.partial</code> as suggested by Joel Cornett in his comment.</p>

<p>An example:</p>

<pre><code>&gt;&gt;&gt; import functools
&gt;&gt;&gt; import operator
&gt;&gt;&gt; add_3 = functools.partial(operator.add,3)
&gt;&gt;&gt; add_3(2)
5
&gt;&gt;&gt; add_3(7)
10
</code></pre>

<p>You can also pass keyword arguments using <code>partial</code>.</p>

<p>Another way would be to create a lambda:</p>

<pre><code>my_series.apply((lambda x: your_func(a,b,c,d,...,x)))
</code></pre>

<p>But I think using <code>partial</code> is better.</p>

<hr>

<p>Note that newer versions of pandas <em>do</em> allow you to pass extra arguments (see the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html#pandas.Series.apply"" rel=""noreferrer"">new documentation</a>). So now you can do:</p>

<pre><code>my_series.apply(your_function, args=(2,3,4), extra_kw=1)
</code></pre>

<p>The positional arguments are added <em>after</em> the element of the series.</p>
";;['>>> import functools\n>>> import operator\n>>> add_3 = functools.partial(operator.add,3)\n>>> add_3(2)\n5\n>>> add_3(7)\n10\n', 'my_series.apply((lambda x: your_func(a,b,c,d,...,x)))\n', 'my_series.apply(your_function, args=(2,3,4), extra_kw=1)\n'];['functools.partial', '>>> import functools\n>>> import operator\n>>> add_3 = functools.partial(operator.add,3)\n>>> add_3(2)\n5\n>>> add_3(7)\n10\n', 'partial', 'my_series.apply((lambda x: your_func(a,b,c,d,...,x)))\n', 'partial', 'my_series.apply(your_function, args=(2,3,4), extra_kw=1)\n']
579;;5;12192021;12190874.0;2;74;;;"<p>What version of pandas are you using? For me your code works fine (i`m on git master).</p>

<p>Another approach could be:</p>

<pre><code>In [117]: import pandas

In [118]: import random

In [119]: df = pandas.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))

In [120]: rows = random.sample(df.index, 10)

In [121]: df_10 = df.ix[rows]

In [122]: df_90 = df.drop(rows)
</code></pre>

<p>Newer version (from 0.16.1 on) supports this directly: 
<a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html</a></p>
";;"[""In [117]: import pandas\n\nIn [118]: import random\n\nIn [119]: df = pandas.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))\n\nIn [120]: rows = random.sample(df.index, 10)\n\nIn [121]: df_10 = df.ix[rows]\n\nIn [122]: df_90 = df.drop(rows)\n""]";"[""In [117]: import pandas\n\nIn [118]: import random\n\nIn [119]: df = pandas.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))\n\nIn [120]: rows = random.sample(df.index, 10)\n\nIn [121]: df_10 = df.ix[rows]\n\nIn [122]: df_90 = df.drop(rows)\n""]"
580;;11;12193309;11622652.0;2;66;;;"<p>Wes is of course right! I'm just chiming in to provide a little more complete example code. I had the same issue with a 129 Mb file, which was solved by:</p>

<pre><code>from pandas import *

tp = read_csv('large_dataset.csv', iterator=True, chunksize=1000)  # gives TextFileReader, which is iterable with chunks of 1000 rows.
df = concat(tp, ignore_index=True)  # df is DataFrame. If errors, do `list(tp)` instead of `tp`
</code></pre>
";;"[""from pandas import *\n\ntp = read_csv('large_dataset.csv', iterator=True, chunksize=1000)  # gives TextFileReader, which is iterable with chunks of 1000 rows.\ndf = concat(tp, ignore_index=True)  # df is DataFrame. If errors, do `list(tp)` instead of `tp`\n""]";"[""from pandas import *\n\ntp = read_csv('large_dataset.csv', iterator=True, chunksize=1000)  # gives TextFileReader, which is iterable with chunks of 1000 rows.\ndf = concat(tp, ignore_index=True)  # df is DataFrame. If errors, do `list(tp)` instead of `tp`\n""]"
581;;7;12201723;12200693.0;2;30;;;"<pre><code>In [97]: df = pandas.DataFrame({'month': np.random.randint(0,11, 100), 'A': np.random.randn(100), 'B': np.random.randn(100)})

In [98]: df.join(df.groupby('month')['A'].sum(), on='month', rsuffix='_r')
Out[98]:
           A         B  month       A_r
0  -0.040710  0.182269      0 -0.331816
1  -0.004867  0.642243      1  2.448232
2  -0.162191  0.442338      4  2.045909
3  -0.979875  1.367018      5 -2.736399
4  -1.126198  0.338946      5 -2.736399
5  -0.992209 -1.343258      1  2.448232
6  -1.450310  0.021290      0 -0.331816
7  -0.675345 -1.359915      9  2.722156
</code></pre>
";;"[""In [97]: df = pandas.DataFrame({'month': np.random.randint(0,11, 100), 'A': np.random.randn(100), 'B': np.random.randn(100)})\n\nIn [98]: df.join(df.groupby('month')['A'].sum(), on='month', rsuffix='_r')\nOut[98]:\n           A         B  month       A_r\n0  -0.040710  0.182269      0 -0.331816\n1  -0.004867  0.642243      1  2.448232\n2  -0.162191  0.442338      4  2.045909\n3  -0.979875  1.367018      5 -2.736399\n4  -1.126198  0.338946      5 -2.736399\n5  -0.992209 -1.343258      1  2.448232\n6  -1.450310  0.021290      0 -0.331816\n7  -0.675345 -1.359915      9  2.722156\n""]";"[""In [97]: df = pandas.DataFrame({'month': np.random.randint(0,11, 100), 'A': np.random.randn(100), 'B': np.random.randn(100)})\n\nIn [98]: df.join(df.groupby('month')['A'].sum(), on='month', rsuffix='_r')\nOut[98]:\n           A         B  month       A_r\n0  -0.040710  0.182269      0 -0.331816\n1  -0.004867  0.642243      1  2.448232\n2  -0.162191  0.442338      4  2.045909\n3  -0.979875  1.367018      5 -2.736399\n4  -1.126198  0.338946      5 -2.736399\n5  -0.992209 -1.343258      1  2.448232\n6  -1.450310  0.021290      0 -0.331816\n7  -0.675345 -1.359915      9  2.722156\n""]"
582;;10;12207352;12207326.0;2;102;;;"<p>Maybe <code>.value_counts()</code>?</p>

<pre><code>&gt;&gt;&gt; import pandas
&gt;&gt;&gt; my_series = pandas.Series([1,2,2,3,3,3, ""fred"", 1.8, 1.8])
&gt;&gt;&gt; my_series
0       1
1       2
2       2
3       3
4       3
5       3
6    fred
7     1.8
8     1.8
&gt;&gt;&gt; counts = my_series.value_counts()
&gt;&gt;&gt; counts
3       3
2       2
1.8     2
fred    1
1       1
&gt;&gt;&gt; len(counts)
5
&gt;&gt;&gt; sum(counts)
9
&gt;&gt;&gt; counts[""fred""]
1
&gt;&gt;&gt; dict(counts)
{1.8: 2, 2: 2, 3: 3, 1: 1, 'fred': 1}
</code></pre>
";;"['>>> import pandas\n>>> my_series = pandas.Series([1,2,2,3,3,3, ""fred"", 1.8, 1.8])\n>>> my_series\n0       1\n1       2\n2       2\n3       3\n4       3\n5       3\n6    fred\n7     1.8\n8     1.8\n>>> counts = my_series.value_counts()\n>>> counts\n3       3\n2       2\n1.8     2\nfred    1\n1       1\n>>> len(counts)\n5\n>>> sum(counts)\n9\n>>> counts[""fred""]\n1\n>>> dict(counts)\n{1.8: 2, 2: 2, 3: 3, 1: 1, \'fred\': 1}\n']";"['.value_counts()', '>>> import pandas\n>>> my_series = pandas.Series([1,2,2,3,3,3, ""fred"", 1.8, 1.8])\n>>> my_series\n0       1\n1       2\n2       2\n3       3\n4       3\n5       3\n6    fred\n7     1.8\n8     1.8\n>>> counts = my_series.value_counts()\n>>> counts\n3       3\n2       2\n1.8     2\nfred    1\n1       1\n>>> len(counts)\n5\n>>> sum(counts)\n9\n>>> counts[""fred""]\n1\n>>> dict(counts)\n{1.8: 2, 2: 2, 3: 3, 1: 1, \'fred\': 1}\n']"
583;;5;12286958;12286607.0;2;37;;;"<p>You want <code>matplotlib.pcolor</code>:</p>

<pre><code>import numpy as np 
from pandas import DataFrame
import matplotlib.pyplot as plt

Index= ['aaa', 'bbb', 'ccc', 'ddd', 'eee']
Cols = ['A', 'B', 'C', 'D']
df = DataFrame(abs(np.random.randn(5, 4)), index=Index, columns=Cols)

plt.pcolor(df)
plt.yticks(np.arange(0.5, len(df.index), 1), df.index)
plt.xticks(np.arange(0.5, len(df.columns), 1), df.columns)
plt.show()
</code></pre>
";;"[""import numpy as np \nfrom pandas import DataFrame\nimport matplotlib.pyplot as plt\n\nIndex= ['aaa', 'bbb', 'ccc', 'ddd', 'eee']\nCols = ['A', 'B', 'C', 'D']\ndf = DataFrame(abs(np.random.randn(5, 4)), index=Index, columns=Cols)\n\nplt.pcolor(df)\nplt.yticks(np.arange(0.5, len(df.index), 1), df.index)\nplt.xticks(np.arange(0.5, len(df.columns), 1), df.columns)\nplt.show()\n""]";"['matplotlib.pcolor', ""import numpy as np \nfrom pandas import DataFrame\nimport matplotlib.pyplot as plt\n\nIndex= ['aaa', 'bbb', 'ccc', 'ddd', 'eee']\nCols = ['A', 'B', 'C', 'D']\ndf = DataFrame(abs(np.random.randn(5, 4)), index=Index, columns=Cols)\n\nplt.pcolor(df)\nplt.yticks(np.arange(0.5, len(df.index), 1), df.index)\nplt.xticks(np.arange(0.5, len(df.columns), 1), df.columns)\nplt.show()\n""]"
584;;11;12307162;12307099.0;2;148;;;"<p>Try this:</p>

<pre><code>df.ix[df.A==0, 'B'] = np.nan
</code></pre>

<p>the <code>df.A==0</code> expression creates a boolean series that indexes the rows, <code>'B'</code> selects the column. You can also use this to transform a subset of a column, e.g.:</p>

<pre><code>df.ix[df.A==0, 'B'] = df.ix[df.A==0, 'B'] / 2
</code></pre>
";;"[""df.ix[df.A==0, 'B'] = np.nan\n"", ""df.ix[df.A==0, 'B'] = df.ix[df.A==0, 'B'] / 2\n""]";"[""df.ix[df.A==0, 'B'] = np.nan\n"", 'df.A==0', ""'B'"", ""df.ix[df.A==0, 'B'] = df.ix[df.A==0, 'B'] / 2\n""]"
585;;3;12322877;12322779.0;2;9;;;"<p>Figured out one way to do it by reading the <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#splitting-an-object-into-groups"" rel=""noreferrer"">split-apply-combine</a> documentation examples. </p>

<pre><code>df = pandas.DataFrame({'b':[2,2,4,5], 'c': [3,3,0,9]}, index=[1,1,3,7])
df_unique = df.groupby(level=0).first()

df
   b  c
1  2  3
1  2  3
3  4  0
7  5  9

df_unique
   b  c
1  2  3
3  4  0
7  5  9
</code></pre>
";;"[""df = pandas.DataFrame({'b':[2,2,4,5], 'c': [3,3,0,9]}, index=[1,1,3,7])\ndf_unique = df.groupby(level=0).first()\n\ndf\n   b  c\n1  2  3\n1  2  3\n3  4  0\n7  5  9\n\ndf_unique\n   b  c\n1  2  3\n3  4  0\n7  5  9\n""]";"[""df = pandas.DataFrame({'b':[2,2,4,5], 'c': [3,3,0,9]}, index=[1,1,3,7])\ndf_unique = df.groupby(level=0).first()\n\ndf\n   b  c\n1  2  3\n1  2  3\n3  4  0\n7  5  9\n\ndf_unique\n   b  c\n1  2  3\n3  4  0\n7  5  9\n""]"
586;;4;12323599;12322779.0;2;48;;;"<pre><code>In [29]: df.drop_duplicates()
Out[29]: 
   b  c
1  2  3
3  4  0
7  5  9
</code></pre>
";;['In [29]: df.drop_duplicates()\nOut[29]: \n   b  c\n1  2  3\n3  4  0\n7  5  9\n'];['In [29]: df.drop_duplicates()\nOut[29]: \n   b  c\n1  2  3\n3  4  0\n7  5  9\n']
587;;0;12332974;12200693.0;2;20;;;"<p>While I'm still exploring all of the incredibly smart ways that <code>apply</code> concatenates the pieces it's given, here's another way to add a new column in the parent after a groupby operation.</p>

<pre><code>In [236]: df
Out[236]: 
  yearmonth    return
0    201202  0.922132
1    201202  0.220270
2    201202  0.228856
3    201203  0.277170
4    201203  0.747347

In [237]: def add_mkt_return(grp):
   .....:     grp['mkt_return'] = grp['return'].sum()
   .....:     return grp
   .....: 

In [238]: df.groupby('yearmonth').apply(add_mkt_return)
Out[238]: 
  yearmonth    return  mkt_return
0    201202  0.922132    1.371258
1    201202  0.220270    1.371258
2    201202  0.228856    1.371258
3    201203  0.277170    1.024516
4    201203  0.747347    1.024516
</code></pre>
";;"[""In [236]: df\nOut[236]: \n  yearmonth    return\n0    201202  0.922132\n1    201202  0.220270\n2    201202  0.228856\n3    201203  0.277170\n4    201203  0.747347\n\nIn [237]: def add_mkt_return(grp):\n   .....:     grp['mkt_return'] = grp['return'].sum()\n   .....:     return grp\n   .....: \n\nIn [238]: df.groupby('yearmonth').apply(add_mkt_return)\nOut[238]: \n  yearmonth    return  mkt_return\n0    201202  0.922132    1.371258\n1    201202  0.220270    1.371258\n2    201202  0.228856    1.371258\n3    201203  0.277170    1.024516\n4    201203  0.747347    1.024516\n""]";"['apply', ""In [236]: df\nOut[236]: \n  yearmonth    return\n0    201202  0.922132\n1    201202  0.220270\n2    201202  0.228856\n3    201203  0.277170\n4    201203  0.747347\n\nIn [237]: def add_mkt_return(grp):\n   .....:     grp['mkt_return'] = grp['return'].sum()\n   .....:     return grp\n   .....: \n\nIn [238]: df.groupby('yearmonth').apply(add_mkt_return)\nOut[238]: \n  yearmonth    return  mkt_return\n0    201202  0.922132    1.371258\n1    201202  0.220270    1.371258\n2    201202  0.228856    1.371258\n3    201203  0.277170    1.024516\n4    201203  0.747347    1.024516\n""]"
588;;4;12335016;12200693.0;2;8;;;"<p>May I suggest the <code>transform</code> method (instead of aggregate)? If you use it in your original example it should do what you want (the broadcasting).</p>
";;[];['transform']
589;;3;12356541;12356501.0;2;92;;;"<p>I'd just use <code>zip</code>:</p>

<pre><code>In [1]: from pandas import *

In [2]: def calculate(x):
   ...:     return x*2, x*3
   ...: 

In [3]: df = DataFrame({'a': [1,2,3], 'b': [2,3,4]})

In [4]: df
Out[4]: 
   a  b
0  1  2
1  2  3
2  3  4

In [5]: df[""A1""], df[""A2""] = zip(*df[""a""].map(calculate))

In [6]: df
Out[6]: 
   a  b  A1  A2
0  1  2   2   3
1  2  3   4   6
2  3  4   6   9
</code></pre>
";;"['In [1]: from pandas import *\n\nIn [2]: def calculate(x):\n   ...:     return x*2, x*3\n   ...: \n\nIn [3]: df = DataFrame({\'a\': [1,2,3], \'b\': [2,3,4]})\n\nIn [4]: df\nOut[4]: \n   a  b\n0  1  2\n1  2  3\n2  3  4\n\nIn [5]: df[""A1""], df[""A2""] = zip(*df[""a""].map(calculate))\n\nIn [6]: df\nOut[6]: \n   a  b  A1  A2\n0  1  2   2   3\n1  2  3   4   6\n2  3  4   6   9\n']";"['zip', 'In [1]: from pandas import *\n\nIn [2]: def calculate(x):\n   ...:     return x*2, x*3\n   ...: \n\nIn [3]: df = DataFrame({\'a\': [1,2,3], \'b\': [2,3,4]})\n\nIn [4]: df\nOut[4]: \n   a  b\n0  1  2\n1  2  3\n2  3  4\n\nIn [5]: df[""A1""], df[""A2""] = zip(*df[""a""].map(calculate))\n\nIn [6]: df\nOut[6]: \n   a  b  A1  A2\n0  1  2   2   3\n1  2  3   4   6\n2  3  4   6   9\n']"
590;;1;12377080;12376863.0;2;27;;;"<p>You could have <code>is_hammer</code> in terms of <code>row[""Open""]</code> etc. as follows</p>

<pre><code>def is_hammer(rOpen,rLow,rClose,rHigh):
    return lower_wick_at_least_twice_real_body(rOpen,rLow,rClose) \
       and closed_in_top_half_of_range(rHigh,rLow,rClose)
</code></pre>

<p>Then you can use map:</p>

<pre><code>df[""isHammer""] = map(is_hammer, df[""Open""], df[""Low""], df[""Close""], df[""High""])
</code></pre>
";;"['def is_hammer(rOpen,rLow,rClose,rHigh):\n    return lower_wick_at_least_twice_real_body(rOpen,rLow,rClose) \\\n       and closed_in_top_half_of_range(rHigh,rLow,rClose)\n', 'df[""isHammer""] = map(is_hammer, df[""Open""], df[""Low""], df[""Close""], df[""High""])\n']";"['is_hammer', 'row[""Open""]', 'def is_hammer(rOpen,rLow,rClose,rHigh):\n    return lower_wick_at_least_twice_real_body(rOpen,rLow,rClose) \\\n       and closed_in_top_half_of_range(rHigh,rLow,rClose)\n', 'df[""isHammer""] = map(is_hammer, df[""Open""], df[""Low""], df[""Close""], df[""High""])\n']"
591;;3;12377083;12376863.0;2;51;;;"<p>The exact code will vary for each of the columns you want to do, but it's likely you'll want to use the <code>map</code> and <code>apply</code> functions.  In some cases you can just compute using the existing columns directly, since the columns are Pandas Series objects, which also work as Numpy arrays, which automatically work element-wise for usual mathematical operations.</p>

<pre><code>&gt;&gt;&gt; d
    A   B  C
0  11  13  5
1   6   7  4
2   8   3  6
3   4   8  7
4   0   1  7
&gt;&gt;&gt; (d.A + d.B) / d.C
0    4.800000
1    3.250000
2    1.833333
3    1.714286
4    0.142857
&gt;&gt;&gt; d.A &gt; d.C
0     True
1     True
2     True
3    False
4    False
</code></pre>

<p>If you need to use operations like max and min within a row, you can use <code>apply</code> with <code>axis=1</code> to apply any function you like to each row.  Here's an example that computes <code>min(A, B)-C</code>, which seems to be like your ""lower wick"":</p>

<pre><code>&gt;&gt;&gt; d.apply(lambda row: min([row['A'], row['B']])-row['C'], axis=1)
0    6
1    2
2   -3
3   -3
4   -7
</code></pre>

<p>Hopefully that gives you some idea of how to proceed.</p>

<p>Edit: to compare rows against neighboring rows, the simplest approach is to slice the columns you want to compare, leaving off the beginning/end, and then compare the resulting slices.  For instance, this will tell you for which rows the element in column A is less than the next row's element in column C:</p>

<pre><code>d['A'][:-1] &lt; d['C'][1:]
</code></pre>

<p>and this does it the other way, telling you which rows have A less than the preceding row's C:</p>

<pre><code>d['A'][1:] &lt; d['C'][:-1]
</code></pre>

<p>Doing <code>['A""][:-1]</code> slices off the last element of column A, and doing <code>['C'][1:]</code> slices off the first element of column C, so when you line these two up and compare them, you're comparing each element in A with the C from the following row.</p>
";;"['>>> d\n    A   B  C\n0  11  13  5\n1   6   7  4\n2   8   3  6\n3   4   8  7\n4   0   1  7\n>>> (d.A + d.B) / d.C\n0    4.800000\n1    3.250000\n2    1.833333\n3    1.714286\n4    0.142857\n>>> d.A > d.C\n0     True\n1     True\n2     True\n3    False\n4    False\n', "">>> d.apply(lambda row: min([row['A'], row['B']])-row['C'], axis=1)\n0    6\n1    2\n2   -3\n3   -3\n4   -7\n"", ""d['A'][:-1] < d['C'][1:]\n"", ""d['A'][1:] < d['C'][:-1]\n""]";"['map', 'apply', '>>> d\n    A   B  C\n0  11  13  5\n1   6   7  4\n2   8   3  6\n3   4   8  7\n4   0   1  7\n>>> (d.A + d.B) / d.C\n0    4.800000\n1    3.250000\n2    1.833333\n3    1.714286\n4    0.142857\n>>> d.A > d.C\n0     True\n1     True\n2     True\n3    False\n4    False\n', 'apply', 'axis=1', 'min(A, B)-C', "">>> d.apply(lambda row: min([row['A'], row['B']])-row['C'], axis=1)\n0    6\n1    2\n2   -3\n3   -3\n4   -7\n"", ""d['A'][:-1] < d['C'][1:]\n"", ""d['A'][1:] < d['C'][:-1]\n"", '[\'A""][:-1]', ""['C'][1:]""]"
592;;5;12394122;12389898.0;2;30;;;"<p>What problems are you running into with <code>apply</code>? It works for this toy example here and the group lengths are different:</p>

<pre><code>In [82]: df
Out[82]: 
   X         Y
0  0 -0.631214
1  0  0.783142
2  0  0.526045
3  1 -1.750058
4  1  1.163868
5  1  1.625538
6  1  0.076105
7  2  0.183492
8  2  0.541400
9  2 -0.672809

In [83]: def func(x):
   ....:     x['NewCol'] = np.nan
   ....:     return x
   ....: 

In [84]: df.groupby('X').apply(func)
Out[84]: 
   X         Y  NewCol
0  0 -0.631214     NaN
1  0  0.783142     NaN
2  0  0.526045     NaN
3  1 -1.750058     NaN
4  1  1.163868     NaN
5  1  1.625538     NaN
6  1  0.076105     NaN
7  2  0.183492     NaN
8  2  0.541400     NaN
9  2 -0.672809     NaN
</code></pre>
";;"[""In [82]: df\nOut[82]: \n   X         Y\n0  0 -0.631214\n1  0  0.783142\n2  0  0.526045\n3  1 -1.750058\n4  1  1.163868\n5  1  1.625538\n6  1  0.076105\n7  2  0.183492\n8  2  0.541400\n9  2 -0.672809\n\nIn [83]: def func(x):\n   ....:     x['NewCol'] = np.nan\n   ....:     return x\n   ....: \n\nIn [84]: df.groupby('X').apply(func)\nOut[84]: \n   X         Y  NewCol\n0  0 -0.631214     NaN\n1  0  0.783142     NaN\n2  0  0.526045     NaN\n3  1 -1.750058     NaN\n4  1  1.163868     NaN\n5  1  1.625538     NaN\n6  1  0.076105     NaN\n7  2  0.183492     NaN\n8  2  0.541400     NaN\n9  2 -0.672809     NaN\n""]";"['apply', ""In [82]: df\nOut[82]: \n   X         Y\n0  0 -0.631214\n1  0  0.783142\n2  0  0.526045\n3  1 -1.750058\n4  1  1.163868\n5  1  1.625538\n6  1  0.076105\n7  2  0.183492\n8  2  0.541400\n9  2 -0.672809\n\nIn [83]: def func(x):\n   ....:     x['NewCol'] = np.nan\n   ....:     return x\n   ....: \n\nIn [84]: df.groupby('X').apply(func)\nOut[84]: \n   X         Y  NewCol\n0  0 -0.631214     NaN\n1  0  0.783142     NaN\n2  0  0.526045     NaN\n3  1 -1.750058     NaN\n4  1  1.163868     NaN\n5  1  1.625538     NaN\n6  1  0.076105     NaN\n7  2  0.183492     NaN\n8  2  0.541400     NaN\n9  2 -0.672809     NaN\n""]"
593;;0;12433236;12433076.0;2;32;;;"<p>Short answer: Yes. Use Python's <a href=""http://docs.python.org/library/urllib.html"">urllib</a> to pull the historical data pages for the stocks you want. Go with Yahoo! Finance; Google is both less reliable, has less data coverage, and is more restrictive in how you can use it once you have it. Also, I believe Google specifically prohibits you from scraping the data in their ToS.</p>

<p>Longer answer: This is the script I use to pull all the historical data on a particular company. It pulls the historical data page for a particular ticker symbol, then saves it to a csv file named by that symbol. You'll have to provide your own list of ticker symbols that you want to pull.</p>

<pre><code>import urllib

base_url = ""http://ichart.finance.yahoo.com/table.csv?s=""
def make_url(ticker_symbol):
    return base_url + ticker_symbol

output_path = ""C:/path/to/output/directory""
def make_filename(ticker_symbol, directory=""S&amp;P""):
    return output_path + ""/"" + directory + ""/"" + ticker_symbol + "".csv""

def pull_historical_data(ticker_symbol, directory=""S&amp;P""):
    try:
        urllib.urlretrieve(make_url(ticker_symbol), make_filename(ticker_symbol, directory))
    except urllib.ContentTooShortError as e:
        outfile = open(make_filename(ticker_symbol, directory), ""w"")
        outfile.write(e.content)
        outfile.close()
</code></pre>
";;"['import urllib\n\nbase_url = ""http://ichart.finance.yahoo.com/table.csv?s=""\ndef make_url(ticker_symbol):\n    return base_url + ticker_symbol\n\noutput_path = ""C:/path/to/output/directory""\ndef make_filename(ticker_symbol, directory=""S&P""):\n    return output_path + ""/"" + directory + ""/"" + ticker_symbol + "".csv""\n\ndef pull_historical_data(ticker_symbol, directory=""S&P""):\n    try:\n        urllib.urlretrieve(make_url(ticker_symbol), make_filename(ticker_symbol, directory))\n    except urllib.ContentTooShortError as e:\n        outfile = open(make_filename(ticker_symbol, directory), ""w"")\n        outfile.write(e.content)\n        outfile.close()\n']";"['import urllib\n\nbase_url = ""http://ichart.finance.yahoo.com/table.csv?s=""\ndef make_url(ticker_symbol):\n    return base_url + ticker_symbol\n\noutput_path = ""C:/path/to/output/directory""\ndef make_filename(ticker_symbol, directory=""S&P""):\n    return output_path + ""/"" + directory + ""/"" + ticker_symbol + "".csv""\n\ndef pull_historical_data(ticker_symbol, directory=""S&P""):\n    try:\n        urllib.urlretrieve(make_url(ticker_symbol), make_filename(ticker_symbol, directory))\n    except urllib.ContentTooShortError as e:\n        outfile = open(make_filename(ticker_symbol, directory), ""w"")\n        outfile.write(e.content)\n        outfile.close()\n']"
594;;3;12497577;12497402.0;2;7;;;"<p>Try this:</p>

<pre><code>df.groupby(['A']).max()
</code></pre>
";;"[""df.groupby(['A']).max()\n""]";"[""df.groupby(['A']).max()\n""]"
595;;1;12505089;12504976.0;2;28;;;"<p>You could use the <code>tolist</code> method as an intermediary:</p>

<pre><code>In [99]: import pandas as pd

In [100]: d1 = pd.DataFrame({'ticker' : ['spx 5/25/2001 p500', 'spx 5/25/2001 p600', 'spx 5/25/2001 p700']})

In [101]: d1.ticker.str.split().tolist()
Out[101]: 
[['spx', '5/25/2001', 'p500'],
 ['spx', '5/25/2001', 'p600'],
 ['spx', '5/25/2001', 'p700']]
</code></pre>

<p>From which you could make a new DataFrame:</p>

<pre><code>In [102]: d2 = pd.DataFrame(d1.ticker.str.split().tolist(), 
   .....:                   columns=""symbol date price"".split())

In [103]: d2
Out[103]: 
  symbol       date price
0    spx  5/25/2001  p500
1    spx  5/25/2001  p600
2    spx  5/25/2001  p700
</code></pre>

<p>For good measure, you could fix the price:</p>

<pre><code>In [104]: d2[""price""] = d2[""price""].str.replace(""p"","""").astype(float)

In [105]: d2
Out[105]: 
  symbol       date  price
0    spx  5/25/2001    500
1    spx  5/25/2001    600
2    spx  5/25/2001    700
</code></pre>

<p>PS: but if you <em>really</em> just want the last column, <code>apply</code> would suffice:</p>

<pre><code>In [113]: temp2.apply(lambda x: x[2])
Out[113]: 
0    p500
1    p600
2    p700
Name: ticker
</code></pre>
";;"[""In [99]: import pandas as pd\n\nIn [100]: d1 = pd.DataFrame({'ticker' : ['spx 5/25/2001 p500', 'spx 5/25/2001 p600', 'spx 5/25/2001 p700']})\n\nIn [101]: d1.ticker.str.split().tolist()\nOut[101]: \n[['spx', '5/25/2001', 'p500'],\n ['spx', '5/25/2001', 'p600'],\n ['spx', '5/25/2001', 'p700']]\n"", 'In [102]: d2 = pd.DataFrame(d1.ticker.str.split().tolist(), \n   .....:                   columns=""symbol date price"".split())\n\nIn [103]: d2\nOut[103]: \n  symbol       date price\n0    spx  5/25/2001  p500\n1    spx  5/25/2001  p600\n2    spx  5/25/2001  p700\n', 'In [104]: d2[""price""] = d2[""price""].str.replace(""p"","""").astype(float)\n\nIn [105]: d2\nOut[105]: \n  symbol       date  price\n0    spx  5/25/2001    500\n1    spx  5/25/2001    600\n2    spx  5/25/2001    700\n', 'In [113]: temp2.apply(lambda x: x[2])\nOut[113]: \n0    p500\n1    p600\n2    p700\nName: ticker\n']";"['tolist', ""In [99]: import pandas as pd\n\nIn [100]: d1 = pd.DataFrame({'ticker' : ['spx 5/25/2001 p500', 'spx 5/25/2001 p600', 'spx 5/25/2001 p700']})\n\nIn [101]: d1.ticker.str.split().tolist()\nOut[101]: \n[['spx', '5/25/2001', 'p500'],\n ['spx', '5/25/2001', 'p600'],\n ['spx', '5/25/2001', 'p700']]\n"", 'In [102]: d2 = pd.DataFrame(d1.ticker.str.split().tolist(), \n   .....:                   columns=""symbol date price"".split())\n\nIn [103]: d2\nOut[103]: \n  symbol       date price\n0    spx  5/25/2001  p500\n1    spx  5/25/2001  p600\n2    spx  5/25/2001  p700\n', 'In [104]: d2[""price""] = d2[""price""].str.replace(""p"","""").astype(float)\n\nIn [105]: d2\nOut[105]: \n  symbol       date  price\n0    spx  5/25/2001    500\n1    spx  5/25/2001    600\n2    spx  5/25/2001    700\n', 'apply', 'In [113]: temp2.apply(lambda x: x[2])\nOut[113]: \n0    p500\n1    p600\n2    p700\nName: ticker\n']"
596;;3;12510334;12433076.0;2;85;;;"<p>When you're going to work with such time series in Python, <code>pandas</code> is indispensable. And here's the good news: it comes with a historical data downloader for Yahoo: <code>pandas.io.data.DataReader</code>.</p>

<pre><code>from pandas.io.data import DataReader
from datetime import datetime

ibm = DataReader('IBM',  'yahoo', datetime(2000, 1, 1), datetime(2012, 1, 1))
print(ibm['Adj Close'])
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#yahoo-finance"">Here's an example from the <code>pandas</code> documentation.</a></p>

<p><strong>Update for pandas >= 0.19:</strong></p>

<p>The <code>pandas.io.data</code> module has been removed from <code>pandas&gt;=0.19</code> onwards. Instead, you should use the separate <a href=""https://github.com/pydata/pandas-datareader""><code>pandas-datareader</code> package</a>. Install with:</p>

<pre><code>pip install pandas-datareader
</code></pre>

<p>And then you can do this in Python:</p>

<pre><code>import pandas_datareader as pdr
from datetime import datetime

ibm = pdr.get_data_yahoo(symbols='IBM', start=datetime(2000, 1, 1), end=datetime(2012, 1, 1))
print(ibm['Adj Close'])
</code></pre>

<p><a href=""http://pandas-datareader.readthedocs.io/en/latest/remote_data.html#google-finance"">Downloading from Google Finance is also supported.</a></p>

<p><a href=""http://pandas-datareader.readthedocs.io"">There's more in the documentation of <code>pandas-datareader</code>.</a></p>
";;"[""from pandas.io.data import DataReader\nfrom datetime import datetime\n\nibm = DataReader('IBM',  'yahoo', datetime(2000, 1, 1), datetime(2012, 1, 1))\nprint(ibm['Adj Close'])\n"", 'pip install pandas-datareader\n', ""import pandas_datareader as pdr\nfrom datetime import datetime\n\nibm = pdr.get_data_yahoo(symbols='IBM', start=datetime(2000, 1, 1), end=datetime(2012, 1, 1))\nprint(ibm['Adj Close'])\n""]";"['pandas', 'pandas.io.data.DataReader', ""from pandas.io.data import DataReader\nfrom datetime import datetime\n\nibm = DataReader('IBM',  'yahoo', datetime(2000, 1, 1), datetime(2012, 1, 1))\nprint(ibm['Adj Close'])\n"", 'pandas', 'pandas.io.data', 'pandas>=0.19', 'pandas-datareader', 'pip install pandas-datareader\n', ""import pandas_datareader as pdr\nfrom datetime import datetime\n\nibm = pdr.get_data_yahoo(symbols='IBM', start=datetime(2000, 1, 1), end=datetime(2012, 1, 1))\nprint(ibm['Adj Close'])\n"", 'pandas-datareader']"
597;;7;12525836;12525722.0;2;124;;;"<pre><code>In [92]: df
Out[92]:
           a         b          c         d
A  -0.488816  0.863769   4.325608 -4.721202
B -11.937097  2.993993 -12.916784 -1.086236
C  -5.569493  4.672679  -2.168464 -9.315900
D   8.892368  0.932785   4.535396  0.598124

In [93]: df_norm = (df - df.mean()) / (df.max() - df.min())

In [94]: df_norm
Out[94]:
          a         b         c         d
A  0.085789 -0.394348  0.337016 -0.109935
B -0.463830  0.164926 -0.650963  0.256714
C -0.158129  0.605652 -0.035090 -0.573389
D  0.536170 -0.376229  0.349037  0.426611

In [95]: df_norm.mean()
Out[95]:
a   -2.081668e-17
b    4.857226e-17
c    1.734723e-17
d   -1.040834e-17

In [96]: df_norm.max() - df_norm.min()
Out[96]:
a    1
b    1
c    1
d    1
</code></pre>
";;['In [92]: df\nOut[92]:\n           a         b          c         d\nA  -0.488816  0.863769   4.325608 -4.721202\nB -11.937097  2.993993 -12.916784 -1.086236\nC  -5.569493  4.672679  -2.168464 -9.315900\nD   8.892368  0.932785   4.535396  0.598124\n\nIn [93]: df_norm = (df - df.mean()) / (df.max() - df.min())\n\nIn [94]: df_norm\nOut[94]:\n          a         b         c         d\nA  0.085789 -0.394348  0.337016 -0.109935\nB -0.463830  0.164926 -0.650963  0.256714\nC -0.158129  0.605652 -0.035090 -0.573389\nD  0.536170 -0.376229  0.349037  0.426611\n\nIn [95]: df_norm.mean()\nOut[95]:\na   -2.081668e-17\nb    4.857226e-17\nc    1.734723e-17\nd   -1.040834e-17\n\nIn [96]: df_norm.max() - df_norm.min()\nOut[96]:\na    1\nb    1\nc    1\nd    1\n'];['In [92]: df\nOut[92]:\n           a         b          c         d\nA  -0.488816  0.863769   4.325608 -4.721202\nB -11.937097  2.993993 -12.916784 -1.086236\nC  -5.569493  4.672679  -2.168464 -9.315900\nD   8.892368  0.932785   4.535396  0.598124\n\nIn [93]: df_norm = (df - df.mean()) / (df.max() - df.min())\n\nIn [94]: df_norm\nOut[94]:\n          a         b         c         d\nA  0.085789 -0.394348  0.337016 -0.109935\nB -0.463830  0.164926 -0.650963  0.256714\nC -0.158129  0.605652 -0.035090 -0.573389\nD  0.536170 -0.376229  0.349037  0.426611\n\nIn [95]: df_norm.mean()\nOut[95]:\na   -2.081668e-17\nb    4.857226e-17\nc    1.734723e-17\nd   -1.040834e-17\n\nIn [96]: df_norm.max() - df_norm.min()\nOut[96]:\na    1\nb    1\nc    1\nd    1\n']
598;;2;12555491;12555323.0;2;29;;;"<p>Doing this directly via <a href=""http://en.wikipedia.org/wiki/NumPy"">NumPy</a> will be the most efficient:</p>

<pre><code>df1['e'] = np.random.randn(sLength)
</code></pre>

<hr>

<p>Note my original (very old) suggestion was to use <code>map</code> (which is much slower):</p>

<pre><code>df1['e'] = df1['a'].map(lambda x: np.random.random())
</code></pre>
";;"[""df1['e'] = np.random.randn(sLength)\n"", ""df1['e'] = df1['a'].map(lambda x: np.random.random())\n""]";"[""df1['e'] = np.random.randn(sLength)\n"", 'map', ""df1['e'] = df1['a'].map(lambda x: np.random.random())\n""]"
599;;22;12555510;12555323.0;2;411;;;"<p>Use the original df1 indexes to create the series:</p>

<pre><code>df1['e'] = Series(np.random.randn(sLength), index=df1.index)
</code></pre>

<hr>

<hr>

<p><strong>Edit 2015</strong><br>
Some reported to get the <code>SettingWithCopyWarning</code> with this code.<br>
However, the code still runs perfect with the current pandas version 0.16.1.</p>

<pre><code>&gt;&gt;&gt; sLength = len(df1['a'])
&gt;&gt;&gt; df1
          a         b         c         d
6 -0.269221 -0.026476  0.997517  1.294385
8  0.917438  0.847941  0.034235 -0.448948

&gt;&gt;&gt; df1['e'] = p.Series(np.random.randn(sLength), index=df1.index)
&gt;&gt;&gt; df1
          a         b         c         d         e
6 -0.269221 -0.026476  0.997517  1.294385  1.757167
8  0.917438  0.847941  0.034235 -0.448948  2.228131

&gt;&gt;&gt; p.version.short_version
'0.16.1'
</code></pre>

<p>The <code>SettingWithCopyWarning</code> aims to inform of a possibly invalid assignment on a copy of the Dataframe. It doesn't necessarily say you did it wrong (it can trigger false positives) but from 0.13.0 it let you know there are more adequate methods for the same purpose. Then, if you get the warning, just follow its advise: <em>Try using .loc[row_index,col_indexer] = value instead</em></p>

<pre><code>&gt;&gt;&gt; df1.loc[:,'f'] = p.Series(np.random.randn(sLength), index=df1.index)
&gt;&gt;&gt; df1
          a         b         c         d         e         f
6 -0.269221 -0.026476  0.997517  1.294385  1.757167 -0.050927
8  0.917438  0.847941  0.034235 -0.448948  2.228131  0.006109
&gt;&gt;&gt; 
</code></pre>

<p>In fact, this is currently the more efficient method as <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy"">described in pandas docs</a></p>

<hr>

<hr>

<p><strong>Edit 2017</strong></p>

<p>As indicated in the comments and by @Alexander, currently the best method to add the values of a Series as a new column of a DataFrame could be using <code>assign</code>:</p>

<pre><code>df1 = df1.assign(e=p.Series(np.random.randn(sLength)).values)
</code></pre>
";;"[""df1['e'] = Series(np.random.randn(sLength), index=df1.index)\n"", "">>> sLength = len(df1['a'])\n>>> df1\n          a         b         c         d\n6 -0.269221 -0.026476  0.997517  1.294385\n8  0.917438  0.847941  0.034235 -0.448948\n\n>>> df1['e'] = p.Series(np.random.randn(sLength), index=df1.index)\n>>> df1\n          a         b         c         d         e\n6 -0.269221 -0.026476  0.997517  1.294385  1.757167\n8  0.917438  0.847941  0.034235 -0.448948  2.228131\n\n>>> p.version.short_version\n'0.16.1'\n"", "">>> df1.loc[:,'f'] = p.Series(np.random.randn(sLength), index=df1.index)\n>>> df1\n          a         b         c         d         e         f\n6 -0.269221 -0.026476  0.997517  1.294385  1.757167 -0.050927\n8  0.917438  0.847941  0.034235 -0.448948  2.228131  0.006109\n>>> \n"", 'df1 = df1.assign(e=p.Series(np.random.randn(sLength)).values)\n']";"[""df1['e'] = Series(np.random.randn(sLength), index=df1.index)\n"", 'SettingWithCopyWarning', "">>> sLength = len(df1['a'])\n>>> df1\n          a         b         c         d\n6 -0.269221 -0.026476  0.997517  1.294385\n8  0.917438  0.847941  0.034235 -0.448948\n\n>>> df1['e'] = p.Series(np.random.randn(sLength), index=df1.index)\n>>> df1\n          a         b         c         d         e\n6 -0.269221 -0.026476  0.997517  1.294385  1.757167\n8  0.917438  0.847941  0.034235 -0.448948  2.228131\n\n>>> p.version.short_version\n'0.16.1'\n"", 'SettingWithCopyWarning', "">>> df1.loc[:,'f'] = p.Series(np.random.randn(sLength), index=df1.index)\n>>> df1\n          a         b         c         d         e         f\n6 -0.269221 -0.026476  0.997517  1.294385  1.757167 -0.050927\n8  0.917438  0.847941  0.034235 -0.448948  2.228131  0.006109\n>>> \n"", 'assign', 'df1 = df1.assign(e=p.Series(np.random.randn(sLength)).values)\n']"
600;;0;12570410;12569730.0;2;43;;;"<p>I found the answer.</p>

<p>Using the data, dataframe from the question:</p>

<pre><code>from pandas import *

P1Channels = data.filter(regex=""P1"")
P1Sum = P1Channels.sum(axis=1)
</code></pre>
";;"['from pandas import *\n\nP1Channels = data.filter(regex=""P1"")\nP1Sum = P1Channels.sum(axis=1)\n']";"['from pandas import *\n\nP1Channels = data.filter(regex=""P1"")\nP1Sum = P1Channels.sum(axis=1)\n']"
601;;11;12605055;12604909.0;2;54;;;"<p>As @DSM points out, you can do this more directly using the <a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#vectorized-string-methods"">vectorised string methods</a>:</p>

<pre><code>df['Date'].str[-4:].astype(int)
</code></pre>

<p>Or using extract (assuming there is only one set of digits of length 4 somewhere in each string):</p>

<pre><code>df['Date'].str.extract('(?P&lt;year&gt;\d{4})').astype(int)
</code></pre>

<p>An alternative slightly more flexible way, might be to use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html""><code>apply</code></a> (or equivalently <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html""><code>map</code></a>) to do this:</p>

<pre><code>df['Date'] = df['Date'].apply(lambda x: int(str(x)[-4:]))
             #  converts the last 4 characters of the string to an integer
</code></pre>

<p>The lambda function, is taking the input from the <code>Date</code> and converting it to a year.<br>
You could (and perhaps should) write this more verbosely as:</p>

<pre><code>def convert_to_year(date_in_some_format);
    date_as_string = str(date_in_some_format)
    year_as_string = date_in_some_format[-4:] # last four characters
    return int(year_as_string)

df['Date'] = df['Date'].apply(convert_to_year)
</code></pre>

<p><em>Perhaps 'Year' is a better name for this column...</em></p>
";;"[""df['Date'].str[-4:].astype(int)\n"", ""df['Date'].str.extract('(?P<year>\\d{4})').astype(int)\n"", ""df['Date'] = df['Date'].apply(lambda x: int(str(x)[-4:]))\n             #  converts the last 4 characters of the string to an integer\n"", ""def convert_to_year(date_in_some_format);\n    date_as_string = str(date_in_some_format)\n    year_as_string = date_in_some_format[-4:] # last four characters\n    return int(year_as_string)\n\ndf['Date'] = df['Date'].apply(convert_to_year)\n""]";"[""df['Date'].str[-4:].astype(int)\n"", ""df['Date'].str.extract('(?P<year>\\d{4})').astype(int)\n"", 'apply', 'map', ""df['Date'] = df['Date'].apply(lambda x: int(str(x)[-4:]))\n             #  converts the last 4 characters of the string to an integer\n"", 'Date', ""def convert_to_year(date_in_some_format);\n    date_as_string = str(date_in_some_format)\n    year_as_string = date_in_some_format[-4:] # last four characters\n    return int(year_as_string)\n\ndf['Date'] = df['Date'].apply(convert_to_year)\n""]"
602;;3;12607018;12307099.0;2;55;;;"<p><a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-advanced"" rel=""noreferrer"">Here</a> is from pandas docs on advanced indexing: </p>

<p>The section after 'Assignment / setting values is possible when using ix:' will explain exactly what you need! Turns out <code>df.ix</code> can be used for cool slicing/dicing of a dataframe. And. It can also be used to set things. </p>

<pre><code>df.ix[selection criteria, columns I want] = value
</code></pre>

<p>So Bren's answer is saying 'find me all the places where <code>df.A == 0</code>, select column <code>B</code> and set it to <code>np.nan</code>'</p>
";;['df.ix[selection criteria, columns I want] = value\n'];['df.ix', 'df.ix[selection criteria, columns I want] = value\n', 'df.A == 0', 'B', 'np.nan']
603;;8;12681217;12680754.0;2;22;;;"<p>How about something like this:</p>

<pre><code>In [55]: pd.concat([Series(row['var2'], row['var1'].split(','))              
                    for _, row in a.iterrows()]).reset_index()
Out[55]: 
  index  0
0     a  1
1     b  1
2     c  1
3     d  2
4     e  2
5     f  2
</code></pre>

<p>Then you just have to rename the columns</p>
";;"[""In [55]: pd.concat([Series(row['var2'], row['var1'].split(','))              \n                    for _, row in a.iterrows()]).reset_index()\nOut[55]: \n  index  0\n0     a  1\n1     b  1\n2     c  1\n3     d  2\n4     e  2\n5     f  2\n""]";"[""In [55]: pd.concat([Series(row['var2'], row['var1'].split(','))              \n                    for _, row in a.iterrows()]).reset_index()\nOut[55]: \n  index  0\n0     a  1\n1     b  1\n2     c  1\n3     d  2\n4     e  2\n5     f  2\n""]"
604;;3;12726468;12725417.0;2;25;;;"<p>It`s a private method, but it will do the trick: source._get_numeric_data()</p>

<pre><code>In [2]: import pandas as pd

In [3]: source = pd.DataFrame({'A': ['foo', 'bar'], 'B': [1, 2], 'C': [(1,2), (3,4)]})

In [4]: source
Out[4]:
     A  B       C
0  foo  1  (1, 2)
1  bar  2  (3, 4)

In [5]: source._get_numeric_data()
Out[5]:
   B
0  1
1  2
</code></pre>
";;"[""In [2]: import pandas as pd\n\nIn [3]: source = pd.DataFrame({'A': ['foo', 'bar'], 'B': [1, 2], 'C': [(1,2), (3,4)]})\n\nIn [4]: source\nOut[4]:\n     A  B       C\n0  foo  1  (1, 2)\n1  bar  2  (3, 4)\n\nIn [5]: source._get_numeric_data()\nOut[5]:\n   B\n0  1\n1  2\n""]";"[""In [2]: import pandas as pd\n\nIn [3]: source = pd.DataFrame({'A': ['foo', 'bar'], 'B': [1, 2], 'C': [(1,2), (3,4)]})\n\nIn [4]: source\nOut[4]:\n     A  B       C\n0  foo  1  (1, 2)\n1  bar  2  (3, 4)\n\nIn [5]: source._get_numeric_data()\nOut[5]:\n   B\n0  1\n1  2\n""]"
605;;1;12741168;12741092.0;2;33;;;"<p>If I understand you right, you're looking for the <code>applymap</code> method.</p>

<pre><code>&gt;&gt;&gt; print df
   A  B  C
0 -1  0  0
1 -4  3 -1
2 -1  0  2
3  0  3  2
4  1 -1  0
&gt;&gt;&gt; print df.applymap(lambda x: x&gt;1)
       A      B      C
0  False  False  False
1  False   True  False
2  False  False   True
3  False   True   True
4  False  False  False
</code></pre>
";;['>>> print df\n   A  B  C\n0 -1  0  0\n1 -4  3 -1\n2 -1  0  2\n3  0  3  2\n4  1 -1  0\n>>> print df.applymap(lambda x: x>1)\n       A      B      C\n0  False  False  False\n1  False   True  False\n2  False  False   True\n3  False   True   True\n4  False  False  False\n'];['applymap', '>>> print df\n   A  B  C\n0 -1  0  0\n1 -4  3 -1\n2 -1  0  2\n3  0  3  2\n4  1 -1  0\n>>> print df.applymap(lambda x: x>1)\n       A      B      C\n0  False  False  False\n1  False   True  False\n2  False  False   True\n3  False   True   True\n4  False  False  False\n']
606;;0;12834193;10065051.0;2;8;;;"<p>The same syntax works for Ms SQL server using podbc also. </p>

<pre><code>import pyodbc
import pandas.io.sql as psql

cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER=servername;DATABASE=mydb;UID=username;PWD=password') 
cursor = cnxn.cursor()
sql = (""""""select * from mytable"""""")

df = psql.frame_query(sql, cnxn)
cnxn.close()
</code></pre>
";;"['import pyodbc\nimport pandas.io.sql as psql\n\ncnxn = pyodbc.connect(\'DRIVER={SQL Server};SERVER=servername;DATABASE=mydb;UID=username;PWD=password\') \ncursor = cnxn.cursor()\nsql = (""""""select * from mytable"""""")\n\ndf = psql.frame_query(sql, cnxn)\ncnxn.close()\n']";"['import pyodbc\nimport pandas.io.sql as psql\n\ncnxn = pyodbc.connect(\'DRIVER={SQL Server};SERVER=servername;DATABASE=mydb;UID=username;PWD=password\') \ncursor = cnxn.cursor()\nsql = (""""""select * from mytable"""""")\n\ndf = psql.frame_query(sql, cnxn)\ncnxn.close()\n']"
607;;2;12862196;12860421.0;2;37;;;"<p>Do you mean something like this?</p>

<pre><code>In [39]: df2.pivot_table(values='X', rows='Y', cols='Z', 
                         aggfunc=lambda x: len(x.unique()))
Out[39]: 
Z   Z1  Z2  Z3
Y             
Y1   1   1 NaN
Y2 NaN NaN   1
</code></pre>

<p>Note that using <code>len</code> assumes you don't have <code>NA</code>s in your DataFrame. You can do <code>x.value_counts().count()</code> or <code>len(x.dropna().unique())</code> otherwise.</p>
";;"[""In [39]: df2.pivot_table(values='X', rows='Y', cols='Z', \n                         aggfunc=lambda x: len(x.unique()))\nOut[39]: \nZ   Z1  Z2  Z3\nY             \nY1   1   1 NaN\nY2 NaN NaN   1\n""]";"[""In [39]: df2.pivot_table(values='X', rows='Y', cols='Z', \n                         aggfunc=lambda x: len(x.unique()))\nOut[39]: \nZ   Z1  Z2  Z3\nY             \nY1   1   1 NaN\nY2 NaN NaN   1\n"", 'len', 'NA', 'x.value_counts().count()', 'len(x.dropna().unique())']"
608;;1;12874054;12867178.0;2;46;;;"<p>I'd do like Vishal but instead of using sum() using size() to get a count of the number of rows allocated to each group of 'start_station_id'. So:</p>

<pre><code>df = male_trips.groupby('start_station_id').size()
</code></pre>
";;"[""df = male_trips.groupby('start_station_id').size()\n""]";"[""df = male_trips.groupby('start_station_id').size()\n""]"
609;;0;12874135;12867178.0;2;18;;;"<p>My answer below works in Pandas 0.7.3. Not sure about the new releases.</p>

<p>This is what the <code>pandas.Series.value_counts</code> method is for:</p>

<pre><code>count_series = male_trips.start_station_id.value_counts()
</code></pre>

<p>It should be straight-forward to then inspect <code>count_series</code> based on the values in <code>stations['id']</code>. However, if you insist on <em>only</em> considering those values, you could do the following:</p>

<pre><code>count_series = (
                male_trips[male_trips.start_station_id.isin(stations.id.values)]
                    .start_station_id
                    .value_counts()
               )
</code></pre>

<p>and this will only give counts for station IDs actually found in <code>stations.id</code>.</p>
";;['count_series = male_trips.start_station_id.value_counts()\n', 'count_series = (\n                male_trips[male_trips.start_station_id.isin(stations.id.values)]\n                    .start_station_id\n                    .value_counts()\n               )\n'];"['pandas.Series.value_counts', 'count_series = male_trips.start_station_id.value_counts()\n', 'count_series', ""stations['id']"", 'count_series = (\n                male_trips[male_trips.start_station_id.isin(stations.id.values)]\n                    .start_station_id\n                    .value_counts()\n               )\n', 'stations.id']"
610;;0;12882439;12877189.0;2;47;;;"<p>As mentioned in the comments, it is a general floating point problem.</p>

<p>However you can use the <code>float_format</code> key word of <code>to_csv</code> to hide it:</p>

<pre><code>df.to_csv('pandasfile.csv', float_format='%.3f')
</code></pre>

<p>or, if you don't want 0.0001 to be rounded to zero:</p>

<pre><code>df.to_csv('pandasfile.csv', float_format='%g')
</code></pre>

<p>will give you:</p>

<pre><code>Bob,0.085
Alice,0.005
</code></pre>

<p>in your output file.</p>

<p>For an explanation of <code>%g</code>, see <a href=""https://docs.python.org/library/string.html#format-specification-mini-language"" rel=""nofollow noreferrer"">Format Specification Mini-Language</a>.</p>
";;"[""df.to_csv('pandasfile.csv', float_format='%.3f')\n"", ""df.to_csv('pandasfile.csv', float_format='%g')\n"", 'Bob,0.085\nAlice,0.005\n']";"['float_format', 'to_csv', ""df.to_csv('pandasfile.csv', float_format='%.3f')\n"", ""df.to_csv('pandasfile.csv', float_format='%g')\n"", 'Bob,0.085\nAlice,0.005\n', '%g']"
611;;2;13003524;13003051.0;2;8;;;"<p>We can potentially make this easier (I created a <a href=""https://github.com/pydata/pandas/issues/2099"" rel=""noreferrer"">github issue</a>), but for now you can select out the columns you want to plot:</p>

<pre><code>df.ix[:, df.columns - to_excl].hist()
</code></pre>
";;['df.ix[:, df.columns - to_excl].hist()\n'];['df.ix[:, df.columns - to_excl].hist()\n']
612;;0;13021797;13021654.0;2;82;;;"<p>Sure, you can use <code>.get_loc()</code>:</p>

<pre><code>In [45]: df = DataFrame({""pear"": [1,2,3], ""apple"": [2,3,4], ""orange"": [3,4,5]})

In [46]: df.columns
Out[46]: Index([apple, orange, pear], dtype=object)

In [47]: df.columns.get_loc(""pear"")
Out[47]: 2
</code></pre>

<p>although to be honest I don't often need this myself.  Usually access by name does what I want it to (<code>df[""pear""]</code>, <code>df[[""apple"", ""orange""]]</code>, or maybe <code>df.columns.isin([""orange"", ""pear""])</code>), although I can definitely see cases where you'd want the index number. </p>
";;"['In [45]: df = DataFrame({""pear"": [1,2,3], ""apple"": [2,3,4], ""orange"": [3,4,5]})\n\nIn [46]: df.columns\nOut[46]: Index([apple, orange, pear], dtype=object)\n\nIn [47]: df.columns.get_loc(""pear"")\nOut[47]: 2\n']";"['.get_loc()', 'In [45]: df = DataFrame({""pear"": [1,2,3], ""apple"": [2,3,4], ""orange"": [3,4,5]})\n\nIn [46]: df.columns\nOut[46]: Index([apple, orange, pear], dtype=object)\n\nIn [47]: df.columns.get_loc(""pear"")\nOut[47]: 2\n', 'df[""pear""]', 'df[[""apple"", ""orange""]]', 'df.columns.isin([""orange"", ""pear""])']"
613;;1;13036844;13021654.0;2;7;;;"<p>DSM's solution works, but if you wanted a direct equivalent to <code>which</code> you could do <code>(df.columns == name).nonzero()</code></p>
";;[];['which', '(df.columns == name).nonzero()']
614;;2;13036848;13035764.0;2;53;;;"<p>Oh my. This is actually so simple!</p>

<pre><code>grouped = df3.groupby(level=0)
df4 = grouped.last()
df4
                      A   B  rownum

2001-01-01 00:00:00   0   0       6
2001-01-01 01:00:00   1   1       7
2001-01-01 02:00:00   2   2       8
2001-01-01 03:00:00   3   3       3
2001-01-01 04:00:00   4   4       4
2001-01-01 05:00:00   5   5       5
</code></pre>

<p><strong>Follow up edit 2013-10-29</strong>
In the case where I have a fairly complex <code>MultiIndex</code>, I think I prefer the <code>groupby</code> approach. Here's simple example for posterity:</p>

<pre><code>import numpy as np
import pandas

# fake index
idx = pandas.MultiIndex.from_tuples([('a', letter) for letter in list('abcde')])

# random data + naming the index levels
df1 = pandas.DataFrame(np.random.normal(size=(5,2)), index=idx, columns=['colA', 'colB'])
df1.index.names = ['iA', 'iB']

# artificially append some duplicate data
df1 = df1.append(df1.select(lambda idx: idx[1] in ['c', 'e']))
df1
#           colA      colB
#iA iB                    
#a  a  -1.297535  0.691787
#   b  -1.688411  0.404430
#   c   0.275806 -0.078871
#   d  -0.509815 -0.220326
#   e  -0.066680  0.607233
#   c   0.275806 -0.078871  # &lt;--- dup 1
#   e  -0.066680  0.607233  # &lt;--- dup 2
</code></pre>

<p><strong>and here's the important part</strong></p>

<pre><code># group the data, using df1.index.names tells pandas to look at the entire index
groups = df1.groupby(level=df1.index.names)  
groups.last() # or .first()
#           colA      colB
#iA iB                    
#a  a  -1.297535  0.691787
#   b  -1.688411  0.404430
#   c   0.275806 -0.078871
#   d  -0.509815 -0.220326
#   e  -0.066680  0.607233
</code></pre>
";;"['grouped = df3.groupby(level=0)\ndf4 = grouped.last()\ndf4\n                      A   B  rownum\n\n2001-01-01 00:00:00   0   0       6\n2001-01-01 01:00:00   1   1       7\n2001-01-01 02:00:00   2   2       8\n2001-01-01 03:00:00   3   3       3\n2001-01-01 04:00:00   4   4       4\n2001-01-01 05:00:00   5   5       5\n', ""import numpy as np\nimport pandas\n\n# fake index\nidx = pandas.MultiIndex.from_tuples([('a', letter) for letter in list('abcde')])\n\n# random data + naming the index levels\ndf1 = pandas.DataFrame(np.random.normal(size=(5,2)), index=idx, columns=['colA', 'colB'])\ndf1.index.names = ['iA', 'iB']\n\n# artificially append some duplicate data\ndf1 = df1.append(df1.select(lambda idx: idx[1] in ['c', 'e']))\ndf1\n#           colA      colB\n#iA iB                    \n#a  a  -1.297535  0.691787\n#   b  -1.688411  0.404430\n#   c   0.275806 -0.078871\n#   d  -0.509815 -0.220326\n#   e  -0.066680  0.607233\n#   c   0.275806 -0.078871  # <--- dup 1\n#   e  -0.066680  0.607233  # <--- dup 2\n"", '# group the data, using df1.index.names tells pandas to look at the entire index\ngroups = df1.groupby(level=df1.index.names)  \ngroups.last() # or .first()\n#           colA      colB\n#iA iB                    \n#a  a  -1.297535  0.691787\n#   b  -1.688411  0.404430\n#   c   0.275806 -0.078871\n#   d  -0.509815 -0.220326\n#   e  -0.066680  0.607233\n']";"['grouped = df3.groupby(level=0)\ndf4 = grouped.last()\ndf4\n                      A   B  rownum\n\n2001-01-01 00:00:00   0   0       6\n2001-01-01 01:00:00   1   1       7\n2001-01-01 02:00:00   2   2       8\n2001-01-01 03:00:00   3   3       3\n2001-01-01 04:00:00   4   4       4\n2001-01-01 05:00:00   5   5       5\n', 'MultiIndex', 'groupby', ""import numpy as np\nimport pandas\n\n# fake index\nidx = pandas.MultiIndex.from_tuples([('a', letter) for letter in list('abcde')])\n\n# random data + naming the index levels\ndf1 = pandas.DataFrame(np.random.normal(size=(5,2)), index=idx, columns=['colA', 'colB'])\ndf1.index.names = ['iA', 'iB']\n\n# artificially append some duplicate data\ndf1 = df1.append(df1.select(lambda idx: idx[1] in ['c', 'e']))\ndf1\n#           colA      colB\n#iA iB                    \n#a  a  -1.297535  0.691787\n#   b  -1.688411  0.404430\n#   c   0.275806 -0.078871\n#   d  -0.509815 -0.220326\n#   e  -0.066680  0.607233\n#   c   0.275806 -0.078871  # <--- dup 1\n#   e  -0.066680  0.607233  # <--- dup 2\n"", '# group the data, using df1.index.names tells pandas to look at the entire index\ngroups = df1.groupby(level=df1.index.names)  \ngroups.last() # or .first()\n#           colA      colB\n#iA iB                    \n#a  a  -1.297535  0.691787\n#   b  -1.688411  0.404430\n#   c   0.275806 -0.078871\n#   d  -0.509815 -0.220326\n#   e  -0.066680  0.607233\n']"
615;;6;13053267;12504976.0;2;41;;;"<p>Do this:</p>

<pre><code>In [43]: temp2.str[-1]
Out[43]: 
0    p500
1    p600
2    p700
Name: ticker
</code></pre>
";;['In [43]: temp2.str[-1]\nOut[43]: \n0    p500\n1    p600\n2    p700\nName: ticker\n'];['In [43]: temp2.str[-1]\nOut[43]: \n0    p500\n1    p600\n2    p700\nName: ticker\n']
616;;6;13059751;12497402.0;2;64;;;"<p>This takes the last. Not the maximum though:</p>

<pre><code>In [10]: df.drop_duplicates(subset='A', keep=""last"")
Out[10]: 
   A   B
1  1  20
3  2  40
4  3  10
</code></pre>

<p>You can do also something like:</p>

<pre><code>In [12]: df.groupby('A', group_keys=False).apply(lambda x: x.ix[x.B.idxmax()])
Out[12]: 
   A   B
A       
1  1  20
2  2  40
3  3  10
</code></pre>
";;"['In [10]: df.drop_duplicates(subset=\'A\', keep=""last"")\nOut[10]: \n   A   B\n1  1  20\n3  2  40\n4  3  10\n', ""In [12]: df.groupby('A', group_keys=False).apply(lambda x: x.ix[x.B.idxmax()])\nOut[12]: \n   A   B\nA       \n1  1  20\n2  2  40\n3  3  10\n""]";"['In [10]: df.drop_duplicates(subset=\'A\', keep=""last"")\nOut[10]: \n   A   B\n1  1  20\n3  2  40\n4  3  10\n', ""In [12]: df.groupby('A', group_keys=False).apply(lambda x: x.ix[x.B.idxmax()])\nOut[12]: \n   A   B\nA       \n1  1  20\n2  2  40\n3  3  10\n""]"
617;;4;13115473;13114512.0;2;50;;;"<p>I think you want to do something like this:</p>

<pre><code>In [26]: data
Out[26]: 
           Date   Close  Adj Close
251  2011-01-03  147.48     143.25
250  2011-01-04  147.64     143.41
249  2011-01-05  147.05     142.83
248  2011-01-06  148.66     144.40
247  2011-01-07  147.93     143.69

In [27]: data.set_index('Date').diff()
Out[27]: 
            Close  Adj Close
Date                        
2011-01-03    NaN        NaN
2011-01-04   0.16       0.16
2011-01-05  -0.59      -0.58
2011-01-06   1.61       1.57
2011-01-07  -0.73      -0.71
</code></pre>
";;"[""In [26]: data\nOut[26]: \n           Date   Close  Adj Close\n251  2011-01-03  147.48     143.25\n250  2011-01-04  147.64     143.41\n249  2011-01-05  147.05     142.83\n248  2011-01-06  148.66     144.40\n247  2011-01-07  147.93     143.69\n\nIn [27]: data.set_index('Date').diff()\nOut[27]: \n            Close  Adj Close\nDate                        \n2011-01-03    NaN        NaN\n2011-01-04   0.16       0.16\n2011-01-05  -0.59      -0.58\n2011-01-06   1.61       1.57\n2011-01-07  -0.73      -0.71\n""]";"[""In [26]: data\nOut[26]: \n           Date   Close  Adj Close\n251  2011-01-03  147.48     143.25\n250  2011-01-04  147.64     143.41\n249  2011-01-05  147.05     142.83\n248  2011-01-06  148.66     144.40\n247  2011-01-07  147.93     143.69\n\nIn [27]: data.set_index('Date').diff()\nOut[27]: \n            Close  Adj Close\nDate                        \n2011-01-03    NaN        NaN\n2011-01-04   0.16       0.16\n2011-01-05  -0.59      -0.58\n2011-01-06   1.61       1.57\n2011-01-07  -0.73      -0.71\n""]"
618;;1;13130357;13129618.0;2;45;;;"<p>You just need to use the histogram function of numpy:</p>

<pre><code>import numpy as np
count, division = np.histogram(series)
</code></pre>

<p>where division is the automatically calculated border for your bins and count is the population inside each bin.</p>

<p>If you need to fix a certain number of bins, you can use the argument bins and specify a number of bins, or give it directly the boundaries between each bin.</p>

<pre><code>count, division = np.histogram(series, bins = [-201,-149,949,1001])
</code></pre>

<p>to plot the results you can use the matplotlib function hist, but if you are working in pandas each Series has its own handle to the hist function, and you can give it the chosen binning:</p>

<pre><code>series.hist(bins=division)
</code></pre>
";;['import numpy as np\ncount, division = np.histogram(series)\n', 'count, division = np.histogram(series, bins = [-201,-149,949,1001])\n', 'series.hist(bins=division)\n'];['import numpy as np\ncount, division = np.histogram(series)\n', 'count, division = np.histogram(series, bins = [-201,-149,949,1001])\n', 'series.hist(bins=division)\n']
619;;6;13148611;13148429.0;2;374;;;"<p>One easy way would be to reassign the dataframe with a list of the columns, rearranged as needed. </p>

<p>This is what you have now: </p>

<pre><code>In [6]: df
Out[6]:
          0         1         2         3         4      mean
0  0.445598  0.173835  0.343415  0.682252  0.582616  0.445543
1  0.881592  0.696942  0.702232  0.696724  0.373551  0.670208
2  0.662527  0.955193  0.131016  0.609548  0.804694  0.632596
3  0.260919  0.783467  0.593433  0.033426  0.512019  0.436653
4  0.131842  0.799367  0.182828  0.683330  0.019485  0.363371
5  0.498784  0.873495  0.383811  0.699289  0.480447  0.587165
6  0.388771  0.395757  0.745237  0.628406  0.784473  0.588529
7  0.147986  0.459451  0.310961  0.706435  0.100914  0.345149
8  0.394947  0.863494  0.585030  0.565944  0.356561  0.553195
9  0.689260  0.865243  0.136481  0.386582  0.730399  0.561593

In [7]: cols = df.columns.tolist()

In [8]: cols
Out[8]: [0L, 1L, 2L, 3L, 4L, 'mean']
</code></pre>

<p>Rearrange <code>cols</code> in any way you want. This is how I moved the last element to the first position: </p>

<pre><code>In [12]: cols = cols[-1:] + cols[:-1]

In [13]: cols
Out[13]: ['mean', 0L, 1L, 2L, 3L, 4L]
</code></pre>

<p>Then reorder the dataframe like this: </p>

<pre><code>In [16]: df = df[cols]  #    OR    df = df.ix[:, cols]

In [17]: df
Out[17]:
       mean         0         1         2         3         4
0  0.445543  0.445598  0.173835  0.343415  0.682252  0.582616
1  0.670208  0.881592  0.696942  0.702232  0.696724  0.373551
2  0.632596  0.662527  0.955193  0.131016  0.609548  0.804694
3  0.436653  0.260919  0.783467  0.593433  0.033426  0.512019
4  0.363371  0.131842  0.799367  0.182828  0.683330  0.019485
5  0.587165  0.498784  0.873495  0.383811  0.699289  0.480447
6  0.588529  0.388771  0.395757  0.745237  0.628406  0.784473
7  0.345149  0.147986  0.459451  0.310961  0.706435  0.100914
8  0.553195  0.394947  0.863494  0.585030  0.565944  0.356561
9  0.561593  0.689260  0.865243  0.136481  0.386582  0.730399
</code></pre>
";;"[""In [6]: df\nOut[6]:\n          0         1         2         3         4      mean\n0  0.445598  0.173835  0.343415  0.682252  0.582616  0.445543\n1  0.881592  0.696942  0.702232  0.696724  0.373551  0.670208\n2  0.662527  0.955193  0.131016  0.609548  0.804694  0.632596\n3  0.260919  0.783467  0.593433  0.033426  0.512019  0.436653\n4  0.131842  0.799367  0.182828  0.683330  0.019485  0.363371\n5  0.498784  0.873495  0.383811  0.699289  0.480447  0.587165\n6  0.388771  0.395757  0.745237  0.628406  0.784473  0.588529\n7  0.147986  0.459451  0.310961  0.706435  0.100914  0.345149\n8  0.394947  0.863494  0.585030  0.565944  0.356561  0.553195\n9  0.689260  0.865243  0.136481  0.386582  0.730399  0.561593\n\nIn [7]: cols = df.columns.tolist()\n\nIn [8]: cols\nOut[8]: [0L, 1L, 2L, 3L, 4L, 'mean']\n"", ""In [12]: cols = cols[-1:] + cols[:-1]\n\nIn [13]: cols\nOut[13]: ['mean', 0L, 1L, 2L, 3L, 4L]\n"", 'In [16]: df = df[cols]  #    OR    df = df.ix[:, cols]\n\nIn [17]: df\nOut[17]:\n       mean         0         1         2         3         4\n0  0.445543  0.445598  0.173835  0.343415  0.682252  0.582616\n1  0.670208  0.881592  0.696942  0.702232  0.696724  0.373551\n2  0.632596  0.662527  0.955193  0.131016  0.609548  0.804694\n3  0.436653  0.260919  0.783467  0.593433  0.033426  0.512019\n4  0.363371  0.131842  0.799367  0.182828  0.683330  0.019485\n5  0.587165  0.498784  0.873495  0.383811  0.699289  0.480447\n6  0.588529  0.388771  0.395757  0.745237  0.628406  0.784473\n7  0.345149  0.147986  0.459451  0.310961  0.706435  0.100914\n8  0.553195  0.394947  0.863494  0.585030  0.565944  0.356561\n9  0.561593  0.689260  0.865243  0.136481  0.386582  0.730399\n']";"[""In [6]: df\nOut[6]:\n          0         1         2         3         4      mean\n0  0.445598  0.173835  0.343415  0.682252  0.582616  0.445543\n1  0.881592  0.696942  0.702232  0.696724  0.373551  0.670208\n2  0.662527  0.955193  0.131016  0.609548  0.804694  0.632596\n3  0.260919  0.783467  0.593433  0.033426  0.512019  0.436653\n4  0.131842  0.799367  0.182828  0.683330  0.019485  0.363371\n5  0.498784  0.873495  0.383811  0.699289  0.480447  0.587165\n6  0.388771  0.395757  0.745237  0.628406  0.784473  0.588529\n7  0.147986  0.459451  0.310961  0.706435  0.100914  0.345149\n8  0.394947  0.863494  0.585030  0.565944  0.356561  0.553195\n9  0.689260  0.865243  0.136481  0.386582  0.730399  0.561593\n\nIn [7]: cols = df.columns.tolist()\n\nIn [8]: cols\nOut[8]: [0L, 1L, 2L, 3L, 4L, 'mean']\n"", 'cols', ""In [12]: cols = cols[-1:] + cols[:-1]\n\nIn [13]: cols\nOut[13]: ['mean', 0L, 1L, 2L, 3L, 4L]\n"", 'In [16]: df = df[cols]  #    OR    df = df.ix[:, cols]\n\nIn [17]: df\nOut[17]:\n       mean         0         1         2         3         4\n0  0.445543  0.445598  0.173835  0.343415  0.682252  0.582616\n1  0.670208  0.881592  0.696942  0.702232  0.696724  0.373551\n2  0.632596  0.662527  0.955193  0.131016  0.609548  0.804694\n3  0.436653  0.260919  0.783467  0.593433  0.033426  0.512019\n4  0.363371  0.131842  0.799367  0.182828  0.683330  0.019485\n5  0.587165  0.498784  0.873495  0.383811  0.699289  0.480447\n6  0.588529  0.388771  0.395757  0.745237  0.628406  0.784473\n7  0.345149  0.147986  0.459451  0.310961  0.706435  0.100914\n8  0.553195  0.394947  0.863494  0.585030  0.565944  0.356561\n9  0.561593  0.689260  0.865243  0.136481  0.386582  0.730399\n']"
620;;6;13165753;11285613.0;2;44;;;"<p>Assuming your column names (<code>df.columns</code>) are <code>['index','a','b','c']</code>, then the data you want is in the 
3rd &amp; 4th columns. If you don't know their names when your script runs, you can do this</p>

<pre><code>newdf = df[df.columns[2:4]] # Remember, Python is 0-offset! The ""3rd"" entry is at slot 2.
</code></pre>

<p>As EMS points out in <a href=""https://stackoverflow.com/a/11287278/623735"">his answer</a>, <code>df.ix</code> slices columns a bit more concisely, but the <code>.columns</code> slicing interface might be more natural because it uses the vanilla 1-D python list indexing/slicing syntax.</p>

<p>WARN: <code>'index'</code> is a bad name for a <code>DataFrame</code> column. That same label is also used for the real <code>df.index</code> attribute, a <code>Index</code> array. So your column is returned by <code>df['index']</code> and the real DataFrame index is returned by <code>df.index</code>. An <code>Index</code> is a special kind of <code>Series</code> optimized for lookup of it's elements' values. For df.index it's for looking up rows by their label. That <code>df.columns</code> attribute is also a <code>pd.Index</code> array, for looking up columns by their labels.</p>
";;"['newdf = df[df.columns[2:4]] # Remember, Python is 0-offset! The ""3rd"" entry is at slot 2.\n']";"['df.columns', ""['index','a','b','c']"", 'newdf = df[df.columns[2:4]] # Remember, Python is 0-offset! The ""3rd"" entry is at slot 2.\n', 'df.ix', '.columns', ""'index'"", 'DataFrame', 'df.index', 'Index', ""df['index']"", 'df.index', 'Index', 'Series', 'df.columns', 'pd.Index']"
621;;2;13181960;13167391.0;2;6;;;"<p>If you still need a workaround:</p>

<pre><code>In [49]: pd.concat([group for _, group in grouped if len(group) &gt; 1])
Out[49]: 
     A  B
0  foo  0
2  foo  2
3  foo  3
</code></pre>
";;['In [49]: pd.concat([group for _, group in grouped if len(group) > 1])\nOut[49]: \n     A  B\n0  foo  0\n2  foo  2\n3  foo  3\n'];['In [49]: pd.concat([group for _, group in grouped if len(group) > 1])\nOut[49]: \n     A  B\n0  foo  0\n2  foo  2\n3  foo  3\n']
622;;3;13193256;13187778.0;2;19;;;"<p>You can use the <code>to_records</code> method, but have to play around a bit with the dtypes if they are not what you want from the get go. In my case, having copied your DF from a string, the index type is string (represented by an <code>object</code> dtype in pandas):</p>

<pre><code>In [102]: df
Out[102]: 
label    A    B    C
ID                  
1      NaN  0.2  NaN
2      NaN  NaN  0.5
3      NaN  0.2  0.5
4      0.1  0.2  NaN
5      0.1  0.2  0.5
6      0.1  NaN  0.5
7      0.1  NaN  NaN

In [103]: df.index.dtype
Out[103]: dtype('object')
In [104]: df.to_records()
Out[104]: 
rec.array([(1, nan, 0.2, nan), (2, nan, nan, 0.5), (3, nan, 0.2, 0.5),
       (4, 0.1, 0.2, nan), (5, 0.1, 0.2, 0.5), (6, 0.1, nan, 0.5),
       (7, 0.1, nan, nan)], 
      dtype=[('index', '|O8'), ('A', '&lt;f8'), ('B', '&lt;f8'), ('C', '&lt;f8')])
In [106]: df.to_records().dtype
Out[106]: dtype([('index', '|O8'), ('A', '&lt;f8'), ('B', '&lt;f8'), ('C', '&lt;f8')])
</code></pre>

<p>Converting the recarray dtype does not work for me, but one can do this in Pandas already:</p>

<pre><code>In [109]: df.index = df.index.astype('i8')
In [111]: df.to_records().view([('ID', '&lt;i8'), ('A', '&lt;f8'), ('B', '&lt;f8'), ('C', '&lt;f8')])
Out[111]:
rec.array([(1, nan, 0.2, nan), (2, nan, nan, 0.5), (3, nan, 0.2, 0.5),
       (4, 0.1, 0.2, nan), (5, 0.1, 0.2, 0.5), (6, 0.1, nan, 0.5),
       (7, 0.1, nan, nan)], 
      dtype=[('ID', '&lt;i8'), ('A', '&lt;f8'), ('B', '&lt;f8'), ('C', '&lt;f8')])
</code></pre>

<p>Note that Pandas does not set the name of the index properly (to <code>ID</code>) in the exported record array (a bug?), so we profit from the type conversion to also correct for that. </p>

<p>At the moment Pandas has only 8-byte integers, <code>i8</code>, and floats, <code>f8</code> (see this <a href=""https://github.com/pydata/pandas/issues/622"">issue</a>).</p>
";;"[""In [102]: df\nOut[102]: \nlabel    A    B    C\nID                  \n1      NaN  0.2  NaN\n2      NaN  NaN  0.5\n3      NaN  0.2  0.5\n4      0.1  0.2  NaN\n5      0.1  0.2  0.5\n6      0.1  NaN  0.5\n7      0.1  NaN  NaN\n\nIn [103]: df.index.dtype\nOut[103]: dtype('object')\nIn [104]: df.to_records()\nOut[104]: \nrec.array([(1, nan, 0.2, nan), (2, nan, nan, 0.5), (3, nan, 0.2, 0.5),\n       (4, 0.1, 0.2, nan), (5, 0.1, 0.2, 0.5), (6, 0.1, nan, 0.5),\n       (7, 0.1, nan, nan)], \n      dtype=[('index', '|O8'), ('A', '<f8'), ('B', '<f8'), ('C', '<f8')])\nIn [106]: df.to_records().dtype\nOut[106]: dtype([('index', '|O8'), ('A', '<f8'), ('B', '<f8'), ('C', '<f8')])\n"", ""In [109]: df.index = df.index.astype('i8')\nIn [111]: df.to_records().view([('ID', '<i8'), ('A', '<f8'), ('B', '<f8'), ('C', '<f8')])\nOut[111]:\nrec.array([(1, nan, 0.2, nan), (2, nan, nan, 0.5), (3, nan, 0.2, 0.5),\n       (4, 0.1, 0.2, nan), (5, 0.1, 0.2, 0.5), (6, 0.1, nan, 0.5),\n       (7, 0.1, nan, nan)], \n      dtype=[('ID', '<i8'), ('A', '<f8'), ('B', '<f8'), ('C', '<f8')])\n""]";"['to_records', 'object', ""In [102]: df\nOut[102]: \nlabel    A    B    C\nID                  \n1      NaN  0.2  NaN\n2      NaN  NaN  0.5\n3      NaN  0.2  0.5\n4      0.1  0.2  NaN\n5      0.1  0.2  0.5\n6      0.1  NaN  0.5\n7      0.1  NaN  NaN\n\nIn [103]: df.index.dtype\nOut[103]: dtype('object')\nIn [104]: df.to_records()\nOut[104]: \nrec.array([(1, nan, 0.2, nan), (2, nan, nan, 0.5), (3, nan, 0.2, 0.5),\n       (4, 0.1, 0.2, nan), (5, 0.1, 0.2, 0.5), (6, 0.1, nan, 0.5),\n       (7, 0.1, nan, nan)], \n      dtype=[('index', '|O8'), ('A', '<f8'), ('B', '<f8'), ('C', '<f8')])\nIn [106]: df.to_records().dtype\nOut[106]: dtype([('index', '|O8'), ('A', '<f8'), ('B', '<f8'), ('C', '<f8')])\n"", ""In [109]: df.index = df.index.astype('i8')\nIn [111]: df.to_records().view([('ID', '<i8'), ('A', '<f8'), ('B', '<f8'), ('C', '<f8')])\nOut[111]:\nrec.array([(1, nan, 0.2, nan), (2, nan, nan, 0.5), (3, nan, 0.2, 0.5),\n       (4, 0.1, 0.2, nan), (5, 0.1, 0.2, 0.5), (6, 0.1, nan, 0.5),\n       (7, 0.1, nan, nan)], \n      dtype=[('ID', '<i8'), ('A', '<f8'), ('B', '<f8'), ('C', '<f8')])\n"", 'ID', 'i8', 'f8']"
623;;2;13226352;13226029.0;2;54;;;"<p>Hierarchical indexing (also referred to as multi-level indexing) was introduced in the pandas 0.4 release. </p>

<p>This opens the door to some quite sophisticated data analysis and manipulation, especially for working with higher dimensional data. In essence, it enables you to effectively store and manipulate arbitrarily high dimension data in a 2-dimensional tabular structure (DataFrame), for example. </p>

<p>Imagine constructing a dataframe using <code>MultiIndex</code> like this:-</p>

<pre><code>import pandas as pd
import numpy as np

np.arrays = [['one','one','one','two','two','two'],[1,2,3,1,2,3]]

df = pd.DataFrame(np.random.randn(6,2),index=pd.MultiIndex.from_tuples(list(zip(*np.arrays))),columns=['A','B'])

df  # This is the dataframe we have generated

          A         B
one 1 -0.732470 -0.313871
    2 -0.031109 -2.068794
    3  1.520652  0.471764
two 1 -0.101713 -1.204458
    2  0.958008 -0.455419
    3 -0.191702 -0.915983
</code></pre>

<p>This <code>df</code> is simply a data structure of two dimensions </p>

<pre><code>df.ndim

2
</code></pre>

<p>But we can imagine it, looking at the output, as a 3 dimensional data structure.</p>

<ul>
<li><code>one</code> with <code>1</code> with data <code>-0.732470 -0.313871</code>. </li>
<li><code>one</code> with <code>2</code> with data <code>-0.031109 -2.068794</code>. </li>
<li><code>one</code> with <code>3</code> with data <code>1.520652  0.471764</code>.</li>
</ul>

<p>A.k.a.: ""effectively store and manipulate arbitrarily high dimension data in a 2-dimensional tabular structure""</p>

<p>This is not just a ""pretty display"". It has the benefit of easy retrieval of data since we now have a hierarchal index.</p>

<p>For example.</p>

<pre><code>In [44]: df.ix[""one""]
Out[44]: 
          A         B
1 -0.732470 -0.313871
2 -0.031109 -2.068794
3  1.520652  0.471764
</code></pre>

<p>will give us a new data frame only for the group of data belonging to ""one"".</p>

<p>And we can narrow down our data selection further by doing this:-</p>

<pre><code>In [45]: df.ix[""one""].ix[1]
Out[45]: 
A   -0.732470
B   -0.313871
Name: 1
</code></pre>

<p>And of course, if we want a specific value, here's an example:-</p>

<pre><code>In [46]: df.ix[""one""].ix[1][""A""]
Out[46]: -0.73247029752040727
</code></pre>

<p>So if we have even more indexes (besides the 2 indexes shown in the example above), we can essentially drill down and select the data set we are really interested in without a need for <code>groupby</code>.</p>

<p>We can even grab a cross-section (either rows or columns) from our dataframe...</p>

<p>By rows:-</p>

<pre><code>In [47]: df.xs('one')
Out[47]: 
          A         B
1 -0.732470 -0.313871
2 -0.031109 -2.068794
3  1.520652  0.471764
</code></pre>

<p>By columns:-</p>

<pre><code>In [48]: df.xs('B', axis=1)
Out[48]: 
one  1   -0.313871
     2   -2.068794
     3    0.471764
two  1   -1.204458
     2   -0.455419
     3   -0.915983
Name: B
</code></pre>
";;"[""import pandas as pd\nimport numpy as np\n\nnp.arrays = [['one','one','one','two','two','two'],[1,2,3,1,2,3]]\n\ndf = pd.DataFrame(np.random.randn(6,2),index=pd.MultiIndex.from_tuples(list(zip(*np.arrays))),columns=['A','B'])\n\ndf  # This is the dataframe we have generated\n\n          A         B\none 1 -0.732470 -0.313871\n    2 -0.031109 -2.068794\n    3  1.520652  0.471764\ntwo 1 -0.101713 -1.204458\n    2  0.958008 -0.455419\n    3 -0.191702 -0.915983\n"", 'df.ndim\n\n2\n', 'In [44]: df.ix[""one""]\nOut[44]: \n          A         B\n1 -0.732470 -0.313871\n2 -0.031109 -2.068794\n3  1.520652  0.471764\n', 'In [45]: df.ix[""one""].ix[1]\nOut[45]: \nA   -0.732470\nB   -0.313871\nName: 1\n', 'In [46]: df.ix[""one""].ix[1][""A""]\nOut[46]: -0.73247029752040727\n', ""In [47]: df.xs('one')\nOut[47]: \n          A         B\n1 -0.732470 -0.313871\n2 -0.031109 -2.068794\n3  1.520652  0.471764\n"", ""In [48]: df.xs('B', axis=1)\nOut[48]: \none  1   -0.313871\n     2   -2.068794\n     3    0.471764\ntwo  1   -1.204458\n     2   -0.455419\n     3   -0.915983\nName: B\n""]";"['MultiIndex', ""import pandas as pd\nimport numpy as np\n\nnp.arrays = [['one','one','one','two','two','two'],[1,2,3,1,2,3]]\n\ndf = pd.DataFrame(np.random.randn(6,2),index=pd.MultiIndex.from_tuples(list(zip(*np.arrays))),columns=['A','B'])\n\ndf  # This is the dataframe we have generated\n\n          A         B\none 1 -0.732470 -0.313871\n    2 -0.031109 -2.068794\n    3  1.520652  0.471764\ntwo 1 -0.101713 -1.204458\n    2  0.958008 -0.455419\n    3 -0.191702 -0.915983\n"", 'df', 'df.ndim\n\n2\n', 'one', '1', '-0.732470 -0.313871', 'one', '2', '-0.031109 -2.068794', 'one', '3', '1.520652  0.471764', 'In [44]: df.ix[""one""]\nOut[44]: \n          A         B\n1 -0.732470 -0.313871\n2 -0.031109 -2.068794\n3  1.520652  0.471764\n', 'In [45]: df.ix[""one""].ix[1]\nOut[45]: \nA   -0.732470\nB   -0.313871\nName: 1\n', 'In [46]: df.ix[""one""].ix[1][""A""]\nOut[46]: -0.73247029752040727\n', 'groupby', ""In [47]: df.xs('one')\nOut[47]: \n          A         B\n1 -0.732470 -0.313871\n2 -0.031109 -2.068794\n3  1.520652  0.471764\n"", ""In [48]: df.xs('B', axis=1)\nOut[48]: \none  1   -0.313871\n     2   -2.068794\n     3    0.471764\ntwo  1   -1.204458\n     2   -0.455419\n     3   -0.915983\nName: B\n""]"
624;;2;13237914;11361985.0;2;125;;;"<p>Use:</p>

<pre><code>pandas.set_option('display.max_columns', 7)
</code></pre>

<p>This will force Pandas to display the 7 columns you have. Or more generally:</p>

<pre><code>pandas.set_option('display.max_columns', None)
</code></pre>

<p>which will force it to display any number of columns.</p>

<p>Explanation: the default for <code>max_columns</code> is <code>0</code>, which tells Pandas to display the table only if all the columns can be squeezed into the width of your console.</p>
";;"[""pandas.set_option('display.max_columns', 7)\n"", ""pandas.set_option('display.max_columns', None)\n""]";"[""pandas.set_option('display.max_columns', 7)\n"", ""pandas.set_option('display.max_columns', None)\n"", 'max_columns', '0']"
625;;1;13270110;13269890.0;2;31;;;"<p>If you have a key that is repeated for each row, then you can produce a cartesian product using merge (like you would in SQL).</p>

<pre><code>from pandas import DataFrame, merge
df1 = DataFrame({'key':[1,1], 'col1':[1,2],'col2':[3,4]})
df2 = DataFrame({'key':[1,1], 'col3':[5,6]})

merge(df1, df2,on='key')[['col1', 'col2', 'col3']]
</code></pre>

<p>See here for the documentation: <a href=""http://pandas.pydata.org/pandas-docs/stable/merging.html#brief-primer-on-merge-methods-relational-algebra"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/merging.html#brief-primer-on-merge-methods-relational-algebra</a></p>
";;"[""from pandas import DataFrame, merge\ndf1 = DataFrame({'key':[1,1], 'col1':[1,2],'col2':[3,4]})\ndf2 = DataFrame({'key':[1,1], 'col3':[5,6]})\n\nmerge(df1, df2,on='key')[['col1', 'col2', 'col3']]\n""]";"[""from pandas import DataFrame, merge\ndf1 = DataFrame({'key':[1,1], 'col1':[1,2],'col2':[3,4]})\ndf2 = DataFrame({'key':[1,1], 'col3':[5,6]})\n\nmerge(df1, df2,on='key')[['col1', 'col2', 'col3']]\n""]"
626;;7;13295801;13295735.0;2;265;;;"<p>I believe <code>DataFrame.fillna()</code> will do this for you.</p>

<p>Link to Docs for <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html"" rel=""noreferrer"">a dataframe</a> and for <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.fillna.html"" rel=""noreferrer"">a Series</a>. </p>

<p>Example: </p>

<pre><code>In [7]: df
Out[7]: 
          0         1
0       NaN       NaN
1 -0.494375  0.570994
2       NaN       NaN
3  1.876360 -0.229738
4       NaN       NaN

In [8]: df.fillna(0)
Out[8]: 
          0         1
0  0.000000  0.000000
1 -0.494375  0.570994
2  0.000000  0.000000
3  1.876360 -0.229738
4  0.000000  0.000000
</code></pre>

<p>To fill the NaNs in only one column, select just that column. in this case I'm using inplace=True to actually change the contents of df. </p>

<pre><code>In [12]: df[1].fillna(0, inplace=True)
Out[12]: 
0    0.000000
1    0.570994
2    0.000000
3   -0.229738
4    0.000000
Name: 1

In [13]: df
Out[13]: 
          0         1
0       NaN  0.000000
1 -0.494375  0.570994
2       NaN  0.000000
3  1.876360 -0.229738
4       NaN  0.000000
</code></pre>
";;['In [7]: df\nOut[7]: \n          0         1\n0       NaN       NaN\n1 -0.494375  0.570994\n2       NaN       NaN\n3  1.876360 -0.229738\n4       NaN       NaN\n\nIn [8]: df.fillna(0)\nOut[8]: \n          0         1\n0  0.000000  0.000000\n1 -0.494375  0.570994\n2  0.000000  0.000000\n3  1.876360 -0.229738\n4  0.000000  0.000000\n', 'In [12]: df[1].fillna(0, inplace=True)\nOut[12]: \n0    0.000000\n1    0.570994\n2    0.000000\n3   -0.229738\n4    0.000000\nName: 1\n\nIn [13]: df\nOut[13]: \n          0         1\n0       NaN  0.000000\n1 -0.494375  0.570994\n2       NaN  0.000000\n3  1.876360 -0.229738\n4       NaN  0.000000\n'];['DataFrame.fillna()', 'In [7]: df\nOut[7]: \n          0         1\n0       NaN       NaN\n1 -0.494375  0.570994\n2       NaN       NaN\n3  1.876360 -0.229738\n4       NaN       NaN\n\nIn [8]: df.fillna(0)\nOut[8]: \n          0         1\n0  0.000000  0.000000\n1 -0.494375  0.570994\n2  0.000000  0.000000\n3  1.876360 -0.229738\n4  0.000000  0.000000\n', 'In [12]: df[1].fillna(0, inplace=True)\nOut[12]: \n0    0.000000\n1    0.570994\n2    0.000000\n3   -0.229738\n4    0.000000\nName: 1\n\nIn [13]: df\nOut[13]: \n          0         1\n0       NaN  0.000000\n1 -0.494375  0.570994\n2       NaN  0.000000\n3  1.876360 -0.229738\n4       NaN  0.000000\n']
627;;1;13316001;13148429.0;2;58;;;"<p>How about:</p>

<pre><code>df.insert(0, 'mean', df.mean(1))
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion</a></p>
";;"[""df.insert(0, 'mean', df.mean(1))\n""]";"[""df.insert(0, 'mean', df.mean(1))\n""]"
628;;4;13332682;13331518.0;2;27;;;"<p>How to add single item. This is not very effective but follows what you are asking for:</p>

<pre><code>x = p.Series()
N = 4
for i in xrange(N):
   x = x.set_value(i, i**2)
</code></pre>

<p>produces x:</p>

<pre><code>0    0
1    1
2    4
3    9
</code></pre>

<p>Obviously there are better ways to generate this series in only one shot.  </p>

<p>For your second question check answer and references of SO question <a href=""https://stackoverflow.com/questions/10715965/add-one-row-in-a-pandas-dataframe"">add one row in a pandas.DataFrame</a>. </p>
";;['x = p.Series()\nN = 4\nfor i in xrange(N):\n   x = x.set_value(i, i**2)\n', '0    0\n1    1\n2    4\n3    9\n'];['x = p.Series()\nN = 4\nfor i in xrange(N):\n   x = x.set_value(i, i**2)\n', '0    0\n1    1\n2    4\n3    9\n']
629;;11;13337376;13331698.0;2;143;;;"<p>Here's an example using <code>apply</code> on the dataframe, which I am calling with <code>axis = 1</code>. </p>

<p>Note the difference is that instead of trying to pass two values to the function <code>f</code>, rewrite the function to accept a pandas Series object, and then index the Series to get the values needed. </p>

<pre><code>In [49]: df
Out[49]: 
          0         1
0  1.000000  0.000000
1 -0.494375  0.570994
2  1.000000  0.000000
3  1.876360 -0.229738
4  1.000000  0.000000

In [50]: def f(x):    
   ....:  return x[0] + x[1]  
   ....:  

In [51]: df.apply(f, axis=1) #passes a Series object, row-wise
Out[51]: 
0    1.000000
1    0.076619
2    1.000000
3    1.646622
4    1.000000
</code></pre>

<p>Depending on your use case, it is sometimes helpful to create a pandas <code>group</code> object, and then use <code>apply</code> on the group. </p>
";;['In [49]: df\nOut[49]: \n          0         1\n0  1.000000  0.000000\n1 -0.494375  0.570994\n2  1.000000  0.000000\n3  1.876360 -0.229738\n4  1.000000  0.000000\n\nIn [50]: def f(x):    \n   ....:  return x[0] + x[1]  \n   ....:  \n\nIn [51]: df.apply(f, axis=1) #passes a Series object, row-wise\nOut[51]: \n0    1.000000\n1    0.076619\n2    1.000000\n3    1.646622\n4    1.000000\n'];['apply', 'axis = 1', 'f', 'In [49]: df\nOut[49]: \n          0         1\n0  1.000000  0.000000\n1 -0.494375  0.570994\n2  1.000000  0.000000\n3  1.876360 -0.229738\n4  1.000000  0.000000\n\nIn [50]: def f(x):    \n   ....:  return x[0] + x[1]  \n   ....:  \n\nIn [51]: df.apply(f, axis=1) #passes a Series object, row-wise\nOut[51]: \n0    1.000000\n1    0.076619\n2    1.000000\n3    1.646622\n4    1.000000\n', 'group', 'apply']
630;;2;13384494;13293810.0;2;51;;;"<p>Just want to reiterate this will work in pandas >= 0.9.1:</p>

<pre><code>In [2]: read_csv('sample.csv', dtype={'ID': object})
Out[2]: 
                           ID
0  00013007854817840016671868
1  00013007854817840016749251
2  00013007854817840016754630
3  00013007854817840016781876
4  00013007854817840017028824
5  00013007854817840017963235
6  00013007854817840018860166
</code></pre>

<p>I'm creating an issue about detecting integer overflows also.</p>

<p>EDIT: See resolution here: <a href=""https://github.com/pydata/pandas/issues/2247"">https://github.com/pydata/pandas/issues/2247</a></p>
";;"[""In [2]: read_csv('sample.csv', dtype={'ID': object})\nOut[2]: \n                           ID\n0  00013007854817840016671868\n1  00013007854817840016749251\n2  00013007854817840016754630\n3  00013007854817840016781876\n4  00013007854817840017028824\n5  00013007854817840017963235\n6  00013007854817840018860166\n""]";"[""In [2]: read_csv('sample.csv', dtype={'ID': object})\nOut[2]: \n                           ID\n0  00013007854817840016671868\n1  00013007854817840016749251\n2  00013007854817840016754630\n3  00013007854817840016781876\n4  00013007854817840017028824\n5  00013007854817840017963235\n6  00013007854817840018860166\n""]"
631;;0;13385921;13385860.0;2;19;;;"<p>Well, the whitespace is in your data, so you can't read in the data without reading in the whitespace.  However, after you've read it in, you could strip out the whitespace by doing, e.g., <code>df[""Make""] = df[""Make""].map(str.strip)</code> (where <code>df</code> is your dataframe).</p>
";;[];"['df[""Make""] = df[""Make""].map(str.strip)', 'df']"
632;;0;13386025;13385860.0;2;30;;;"<p>You could use converters:</p>

<pre><code>import pandas as pd

def strip(text):
    try:
        return text.strip()
    except AttributeError:
        return text

def make_int(text):
    return int(text.strip('"" '))

table = pd.read_table(""data.csv"", sep=r',',
                      names=[""Year"", ""Make"", ""Model"", ""Description""],
                      converters = {'Description' : strip,
                                    'Model' : strip,
                                    'Make' : strip,
                                    'Year' : make_int})
print(table)
</code></pre>

<p>yields</p>

<pre><code>   Year     Make   Model              Description
0  1997     Ford    E350                     None
1  1997     Ford    E350                     None
2  1997     Ford    E350   Super, luxurious truck
3  1997     Ford    E350  Super ""luxurious"" truck
4  1997     Ford    E350    Super luxurious truck
5  1997     Ford    E350                     None
6  1997     Ford    E350                     None
7  2000  Mercury  Cougar                     None
</code></pre>
";;"['import pandas as pd\n\ndef strip(text):\n    try:\n        return text.strip()\n    except AttributeError:\n        return text\n\ndef make_int(text):\n    return int(text.strip(\'"" \'))\n\ntable = pd.read_table(""data.csv"", sep=r\',\',\n                      names=[""Year"", ""Make"", ""Model"", ""Description""],\n                      converters = {\'Description\' : strip,\n                                    \'Model\' : strip,\n                                    \'Make\' : strip,\n                                    \'Year\' : make_int})\nprint(table)\n', '   Year     Make   Model              Description\n0  1997     Ford    E350                     None\n1  1997     Ford    E350                     None\n2  1997     Ford    E350   Super, luxurious truck\n3  1997     Ford    E350  Super ""luxurious"" truck\n4  1997     Ford    E350    Super luxurious truck\n5  1997     Ford    E350                     None\n6  1997     Ford    E350                     None\n7  2000  Mercury  Cougar                     None\n']";"['import pandas as pd\n\ndef strip(text):\n    try:\n        return text.strip()\n    except AttributeError:\n        return text\n\ndef make_int(text):\n    return int(text.strip(\'"" \'))\n\ntable = pd.read_table(""data.csv"", sep=r\',\',\n                      names=[""Year"", ""Make"", ""Model"", ""Description""],\n                      converters = {\'Description\' : strip,\n                                    \'Model\' : strip,\n                                    \'Make\' : strip,\n                                    \'Year\' : make_int})\nprint(table)\n', '   Year     Make   Model              Description\n0  1997     Ford    E350                     None\n1  1997     Ford    E350                     None\n2  1997     Ford    E350   Super, luxurious truck\n3  1997     Ford    E350  Super ""luxurious"" truck\n4  1997     Ford    E350    Super luxurious truck\n5  1997     Ford    E350                     None\n6  1997     Ford    E350                     None\n7  2000  Mercury  Cougar                     None\n']"
633;;0;13413842;13404468.0;2;43;;;"<p>it depends what sort of t-test you want to do (one sided or two sided dependent or independent) but it should be as simple as:</p>

<pre><code>from scipy.stats import ttest_ind

cat1 = my_data[my_data['Category']=='cat1']
cat2 = my_data[my_data['Category']=='cat2']

ttest_ind(cat1['values'], cat2['values'])
&gt;&gt;&gt; (1.4927289925706944, 0.16970867501294376)
</code></pre>

<p>it returns a tuple with the t-statistic &amp; the p-value</p>

<p>see here for other t-tests <a href=""http://docs.scipy.org/doc/scipy/reference/stats.html"" rel=""noreferrer"">http://docs.scipy.org/doc/scipy/reference/stats.html</a></p>
";;"[""from scipy.stats import ttest_ind\n\ncat1 = my_data[my_data['Category']=='cat1']\ncat2 = my_data[my_data['Category']=='cat2']\n\nttest_ind(cat1['values'], cat2['values'])\n>>> (1.4927289925706944, 0.16970867501294376)\n""]";"[""from scipy.stats import ttest_ind\n\ncat1 = my_data[my_data['Category']=='cat1']\ncat2 = my_data[my_data['Category']=='cat2']\n\nttest_ind(cat1['values'], cat2['values'])\n>>> (1.4927289925706944, 0.16970867501294376)\n""]"
634;;4;13413845;13413590.0;2;235;;;"<p>Don't <code>drop</code>. Just take rows where <code>EPS</code> is <strong>finite</strong>:</p>

<pre><code>df = df[np.isfinite(df['EPS'])]
</code></pre>
";;"[""df = df[np.isfinite(df['EPS'])]\n""]";"['drop', 'EPS', ""df = df[np.isfinite(df['EPS'])]\n""]"
635;;0;13415772;13411544.0;2;23;;;"<p>It's good practice to always use the <code>[]</code> notation, one reason is that attribute notation (<code>df.column_name</code>) does not work for numbered indices:</p>

<pre><code>In [1]: df = DataFrame([[1, 2, 3], [4, 5, 6]])

In [2]: df[1]
Out[2]: 
0    2
1    5
Name: 1

In [3]: df.1
  File ""&lt;ipython-input-3-e4803c0d1066&gt;"", line 1
    df.1
       ^
SyntaxError: invalid syntax
</code></pre>
";;"['In [1]: df = DataFrame([[1, 2, 3], [4, 5, 6]])\n\nIn [2]: df[1]\nOut[2]: \n0    2\n1    5\nName: 1\n\nIn [3]: df.1\n  File ""<ipython-input-3-e4803c0d1066>"", line 1\n    df.1\n       ^\nSyntaxError: invalid syntax\n']";"['[]', 'df.column_name', 'In [1]: df = DataFrame([[1, 2, 3], [4, 5, 6]])\n\nIn [2]: df[1]\nOut[2]: \n0    2\n1    5\nName: 1\n\nIn [3]: df.1\n  File ""<ipython-input-3-e4803c0d1066>"", line 1\n    df.1\n       ^\nSyntaxError: invalid syntax\n']"
636;;2;13434501;13413590.0;2;410;;;"<p>This question is already resolved, but... </p>

<p>...also consider the solution suggested by Wouter in <a href=""https://stackoverflow.com/questions/13413590/how-to-drop-rows-of-pandas-dataframe-whose-value-of-certain-column-is-nan/13434501#comment18328797_13413590"">his original comment</a>. The ability to handle missing data, including <code>dropna()</code>, is built into pandas explicitly. Aside from potentially improved performance over doing it manually, these functions also come with a variety of options which may be useful. </p>

<pre><code>In [24]: df = pd.DataFrame(np.random.randn(10,3))

In [25]: df.iloc[::2,0] = np.nan; df.iloc[::4,1] = np.nan; df.iloc[::3,2] = np.nan;

In [26]: df
Out[26]:
          0         1         2
0       NaN       NaN       NaN
1  2.677677 -1.466923 -0.750366
2       NaN  0.798002 -0.906038
3  0.672201  0.964789       NaN
4       NaN       NaN  0.050742
5 -1.250970  0.030561 -2.678622
6       NaN  1.036043       NaN
7  0.049896 -0.308003  0.823295
8       NaN       NaN  0.637482
9 -0.310130  0.078891       NaN
</code></pre>

<hr>

<pre><code>In [27]: df.dropna()     #drop all rows that have any NaN values
Out[27]:
          0         1         2
1  2.677677 -1.466923 -0.750366
5 -1.250970  0.030561 -2.678622
7  0.049896 -0.308003  0.823295
</code></pre>

<hr>

<pre><code>In [28]: df.dropna(how='all')     #drop only if ALL columns are NaN
Out[28]:
          0         1         2
1  2.677677 -1.466923 -0.750366
2       NaN  0.798002 -0.906038
3  0.672201  0.964789       NaN
4       NaN       NaN  0.050742
5 -1.250970  0.030561 -2.678622
6       NaN  1.036043       NaN
7  0.049896 -0.308003  0.823295
8       NaN       NaN  0.637482
9 -0.310130  0.078891       NaN
</code></pre>

<hr>

<pre><code>In [29]: df.dropna(thresh=2)   #Drop row if it does not have at least two values that are **not** NaN
Out[29]:
          0         1         2
1  2.677677 -1.466923 -0.750366
2       NaN  0.798002 -0.906038
3  0.672201  0.964789       NaN
5 -1.250970  0.030561 -2.678622
7  0.049896 -0.308003  0.823295
9 -0.310130  0.078891       NaN
</code></pre>

<hr>

<pre><code>In [30]: df.dropna(subset=[1])   #Drop only if NaN in specific column (as asked in the question)
Out[30]:
          0         1         2
1  2.677677 -1.466923 -0.750366
2       NaN  0.798002 -0.906038
3  0.672201  0.964789       NaN
5 -1.250970  0.030561 -2.678622
6       NaN  1.036043       NaN
7  0.049896 -0.308003  0.823295
9 -0.310130  0.078891       NaN
</code></pre>

<p>There are also other options (See docs at <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html</a>), including dropping columns instead of rows. </p>

<p>Pretty handy! </p>
";;"['In [24]: df = pd.DataFrame(np.random.randn(10,3))\n\nIn [25]: df.iloc[::2,0] = np.nan; df.iloc[::4,1] = np.nan; df.iloc[::3,2] = np.nan;\n\nIn [26]: df\nOut[26]:\n          0         1         2\n0       NaN       NaN       NaN\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n4       NaN       NaN  0.050742\n5 -1.250970  0.030561 -2.678622\n6       NaN  1.036043       NaN\n7  0.049896 -0.308003  0.823295\n8       NaN       NaN  0.637482\n9 -0.310130  0.078891       NaN\n', 'In [27]: df.dropna()     #drop all rows that have any NaN values\nOut[27]:\n          0         1         2\n1  2.677677 -1.466923 -0.750366\n5 -1.250970  0.030561 -2.678622\n7  0.049896 -0.308003  0.823295\n', ""In [28]: df.dropna(how='all')     #drop only if ALL columns are NaN\nOut[28]:\n          0         1         2\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n4       NaN       NaN  0.050742\n5 -1.250970  0.030561 -2.678622\n6       NaN  1.036043       NaN\n7  0.049896 -0.308003  0.823295\n8       NaN       NaN  0.637482\n9 -0.310130  0.078891       NaN\n"", 'In [29]: df.dropna(thresh=2)   #Drop row if it does not have at least two values that are **not** NaN\nOut[29]:\n          0         1         2\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n5 -1.250970  0.030561 -2.678622\n7  0.049896 -0.308003  0.823295\n9 -0.310130  0.078891       NaN\n', 'In [30]: df.dropna(subset=[1])   #Drop only if NaN in specific column (as asked in the question)\nOut[30]:\n          0         1         2\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n5 -1.250970  0.030561 -2.678622\n6       NaN  1.036043       NaN\n7  0.049896 -0.308003  0.823295\n9 -0.310130  0.078891       NaN\n']";"['dropna()', 'In [24]: df = pd.DataFrame(np.random.randn(10,3))\n\nIn [25]: df.iloc[::2,0] = np.nan; df.iloc[::4,1] = np.nan; df.iloc[::3,2] = np.nan;\n\nIn [26]: df\nOut[26]:\n          0         1         2\n0       NaN       NaN       NaN\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n4       NaN       NaN  0.050742\n5 -1.250970  0.030561 -2.678622\n6       NaN  1.036043       NaN\n7  0.049896 -0.308003  0.823295\n8       NaN       NaN  0.637482\n9 -0.310130  0.078891       NaN\n', 'In [27]: df.dropna()     #drop all rows that have any NaN values\nOut[27]:\n          0         1         2\n1  2.677677 -1.466923 -0.750366\n5 -1.250970  0.030561 -2.678622\n7  0.049896 -0.308003  0.823295\n', ""In [28]: df.dropna(how='all')     #drop only if ALL columns are NaN\nOut[28]:\n          0         1         2\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n4       NaN       NaN  0.050742\n5 -1.250970  0.030561 -2.678622\n6       NaN  1.036043       NaN\n7  0.049896 -0.308003  0.823295\n8       NaN       NaN  0.637482\n9 -0.310130  0.078891       NaN\n"", 'In [29]: df.dropna(thresh=2)   #Drop row if it does not have at least two values that are **not** NaN\nOut[29]:\n          0         1         2\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n5 -1.250970  0.030561 -2.678622\n7  0.049896 -0.308003  0.823295\n9 -0.310130  0.078891       NaN\n', 'In [30]: df.dropna(subset=[1])   #Drop only if NaN in specific column (as asked in the question)\nOut[30]:\n          0         1         2\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n5 -1.250970  0.030561 -2.678622\n6       NaN  1.036043       NaN\n7  0.049896 -0.308003  0.823295\n9 -0.310130  0.078891       NaN\n']"
637;;4;13445630;13445241.0;2;22;;;"<p>How about:</p>

<pre><code>d = d.applymap(lambda x: np.nan if isinstance(x, basestring) and x.isspace() else x)
</code></pre>

<p>The <code>applymap</code> function applies a function to every cell of the dataframe.</p>
";;['d = d.applymap(lambda x: np.nan if isinstance(x, basestring) and x.isspace() else x)\n'];['d = d.applymap(lambda x: np.nan if isinstance(x, basestring) and x.isspace() else x)\n', 'applymap']
638;;1;13456432;13331518.0;2;12;;;"<p>You can use the append function to add another element to it. Only, make a series of the new element, before you append it:</p>

<pre><code>test.append(pd.Series(200, index=[101]))
</code></pre>
";;['test.append(pd.Series(200, index=[101]))\n'];['test.append(pd.Series(200, index=[101]))\n']
639;;5;13485766;13411544.0;2;374;;;"<p>It's difficult to make <code>del df.column_name</code> work simply as the result of syntactic limitations in Python. <code>del df[name]</code> gets translated to <code>df.__delitem__(name)</code> under the covers by Python</p>
";;[];['del df.column_name', 'del df[name]', 'df.__delitem__(name)']
640;;3;13581730;13575090.0;2;42;;;"<p>A pandas MultiIndex consists of a list of tuples. So the most natural approach would be to reshape your input dict so that its keys are tuples corresponding to the multi-index values you require. Then you can just construct your dataframe using <code>pd.DataFrame.from_dict</code>, using the option <code>orient='index'</code>: </p>

<pre><code>user_dict = {12: {'Category 1': {'att_1': 1, 'att_2': 'whatever'},
                  'Category 2': {'att_1': 23, 'att_2': 'another'}},
             15: {'Category 1': {'att_1': 10, 'att_2': 'foo'},
                  'Category 2': {'att_1': 30, 'att_2': 'bar'}}}

pd.DataFrame.from_dict({(i,j): user_dict[i][j] 
                           for i in user_dict.keys() 
                           for j in user_dict[i].keys()},
                       orient='index')


               att_1     att_2
12 Category 1      1  whatever
   Category 2     23   another
15 Category 1     10       foo
   Category 2     30       bar
</code></pre>

<p>An alternative approach would be to build your dataframe up by concatenating the component dataframes:</p>

<pre><code>user_ids = []
frames = []

for user_id, d in user_dict.iteritems():
    user_ids.append(user_id)
    frames.append(pd.DataFrame.from_dict(d, orient='index'))

pd.concat(frames, keys=user_ids)

               att_1     att_2
12 Category 1      1  whatever
   Category 2     23   another
15 Category 1     10       foo
   Category 2     30       bar
</code></pre>
";;"[""user_dict = {12: {'Category 1': {'att_1': 1, 'att_2': 'whatever'},\n                  'Category 2': {'att_1': 23, 'att_2': 'another'}},\n             15: {'Category 1': {'att_1': 10, 'att_2': 'foo'},\n                  'Category 2': {'att_1': 30, 'att_2': 'bar'}}}\n\npd.DataFrame.from_dict({(i,j): user_dict[i][j] \n                           for i in user_dict.keys() \n                           for j in user_dict[i].keys()},\n                       orient='index')\n\n\n               att_1     att_2\n12 Category 1      1  whatever\n   Category 2     23   another\n15 Category 1     10       foo\n   Category 2     30       bar\n"", ""user_ids = []\nframes = []\n\nfor user_id, d in user_dict.iteritems():\n    user_ids.append(user_id)\n    frames.append(pd.DataFrame.from_dict(d, orient='index'))\n\npd.concat(frames, keys=user_ids)\n\n               att_1     att_2\n12 Category 1      1  whatever\n   Category 2     23   another\n15 Category 1     10       foo\n   Category 2     30       bar\n""]";"['pd.DataFrame.from_dict', ""orient='index'"", ""user_dict = {12: {'Category 1': {'att_1': 1, 'att_2': 'whatever'},\n                  'Category 2': {'att_1': 23, 'att_2': 'another'}},\n             15: {'Category 1': {'att_1': 10, 'att_2': 'foo'},\n                  'Category 2': {'att_1': 30, 'att_2': 'bar'}}}\n\npd.DataFrame.from_dict({(i,j): user_dict[i][j] \n                           for i in user_dict.keys() \n                           for j in user_dict[i].keys()},\n                       orient='index')\n\n\n               att_1     att_2\n12 Category 1      1  whatever\n   Category 2     23   another\n15 Category 1     10       foo\n   Category 2     30       bar\n"", ""user_ids = []\nframes = []\n\nfor user_id, d in user_dict.iteritems():\n    user_ids.append(user_id)\n    frames.append(pd.DataFrame.from_dict(d, orient='index'))\n\npd.concat(frames, keys=user_ids)\n\n               att_1     att_2\n12 Category 1      1  whatever\n   Category 2     23   another\n15 Category 1     10       foo\n   Category 2     30       bar\n""]"
641;;2;13583024;13582449.0;2;12;;;"<p>The <strong>result</strong> of <code>kl.aggregate(np.sum)</code> is a normal DataFrame, you just have to assign it to a variable to further use it. With some random data:</p>

<pre><code>&gt;&gt;&gt; df = DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',
&gt;&gt;&gt;                         'foo', 'bar', 'foo', 'foo'],
...                  'B' : ['one', 'one', 'two', 'three',
...                         'two', 'two', 'one', 'three'],
...                  'C' : randn(8), 'D' : randn(8)})
&gt;&gt;&gt; grouped = df.groupby('A')
&gt;&gt;&gt; grouped
&lt;pandas.core.groupby.DataFrameGroupBy object at 0x04E2F630&gt;
&gt;&gt;&gt; test = grouped.aggregate(np.sum)
&gt;&gt;&gt; test
            C         D
A                      
bar -1.852376  2.204224
foo -3.398196 -0.045082
</code></pre>
";;"["">>> df = DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n>>>                         'foo', 'bar', 'foo', 'foo'],\n...                  'B' : ['one', 'one', 'two', 'three',\n...                         'two', 'two', 'one', 'three'],\n...                  'C' : randn(8), 'D' : randn(8)})\n>>> grouped = df.groupby('A')\n>>> grouped\n<pandas.core.groupby.DataFrameGroupBy object at 0x04E2F630>\n>>> test = grouped.aggregate(np.sum)\n>>> test\n            C         D\nA                      \nbar -1.852376  2.204224\nfoo -3.398196 -0.045082\n""]";"['kl.aggregate(np.sum)', "">>> df = DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n>>>                         'foo', 'bar', 'foo', 'foo'],\n...                  'B' : ['one', 'one', 'two', 'three',\n...                         'two', 'two', 'one', 'three'],\n...                  'C' : randn(8), 'D' : randn(8)})\n>>> grouped = df.groupby('A')\n>>> grouped\n<pandas.core.groupby.DataFrameGroupBy object at 0x04E2F630>\n>>> test = grouped.aggregate(np.sum)\n>>> test\n            C         D\nA                      \nbar -1.852376  2.204224\nfoo -3.398196 -0.045082\n""]"
642;;6;13592901;12589481.0;2;32;;;"<p>You can simply pass the functions as a list:</p>

<pre><code>In [20]: df.groupby(""dummy"").agg({""returns"": [np.mean, np.sum]})
Out[20]: 
        returns          
            sum      mean

dummy                    
1      0.285833  0.028583
</code></pre>

<p>or as a dictionary:</p>

<pre><code>In [21]: df.groupby('dummy').agg({'returns':
                                  {'Mean': np.mean, 'Sum': np.sum}})
Out[21]: 
        returns          
            Sum      Mean
dummy                    
1      0.285833  0.028583
</code></pre>
";;"['In [20]: df.groupby(""dummy"").agg({""returns"": [np.mean, np.sum]})\nOut[20]: \n        returns          \n            sum      mean\n\ndummy                    \n1      0.285833  0.028583\n', ""In [21]: df.groupby('dummy').agg({'returns':\n                                  {'Mean': np.mean, 'Sum': np.sum}})\nOut[21]: \n        returns          \n            Sum      Mean\ndummy                    \n1      0.285833  0.028583\n""]";"['In [20]: df.groupby(""dummy"").agg({""returns"": [np.mean, np.sum]})\nOut[20]: \n        returns          \n            sum      mean\n\ndummy                    \n1      0.285833  0.028583\n', ""In [21]: df.groupby('dummy').agg({'returns':\n                                  {'Mean': np.mean, 'Sum': np.sum}})\nOut[21]: \n        returns          \n            Sum      Mean\ndummy                    \n1      0.285833  0.028583\n""]"
643;;9;13616382;13611065.0;2;86;;;"<p>Pandas (and numpy) allow for <a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html#boolean-indexing"" rel=""noreferrer"">boolean indexing</a>, which will be much more efficient:</p>

<pre><code>In [11]: df.loc[df['col1'] &gt;= 1, 'col1']
Out[11]: 
1    1
2    2
Name: col1

In [12]: df[df['col1'] &gt;= 1]
Out[12]: 
   col1  col2
1     1    11
2     2    12

In [13]: df[(df['col1'] &gt;= 1) &amp; (df['col1'] &lt;=1 )]
Out[13]: 
   col1  col2
1     1    11
</code></pre>

<p>If you want to write helper functions for this, consider something along these lines:</p>

<pre><code>In [14]: def b(x, col, op, n): 
             return op(x[col],n)

In [15]: def f(x, *b):
             return x[(np.logical_and(*b))]

In [16]: b1 = b(df, 'col1', ge, 1)

In [17]: b2 = b(df, 'col1', le, 1)

In [18]: f(df, b1, b2)
Out[18]: 
   col1  col2
1     1    11
</code></pre>

<p>Update: <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html"" rel=""noreferrer"">pandas 0.13 has a query method</a> for these kind of use cases, assuming column names are valid identifiers the following works (and can be more efficient for large frames as it uses <a href=""https://github.com/pydata/numexpr"" rel=""noreferrer"">numexpr</a> behind the scenes):</p>

<pre><code>In [21]: df.query('col1 &lt;= 1 &amp; 1 &lt;= col1')
Out[21]:
   col1  col2
1     1    11

In [22]: df.query(""col1 &lt;= 1 and 1 &lt;= df['col1']"") # use df[] syntax if not a valid identifier
Out[22]:
   col1  col2
1     1    11
</code></pre>
";;"[""In [11]: df.loc[df['col1'] >= 1, 'col1']\nOut[11]: \n1    1\n2    2\nName: col1\n\nIn [12]: df[df['col1'] >= 1]\nOut[12]: \n   col1  col2\n1     1    11\n2     2    12\n\nIn [13]: df[(df['col1'] >= 1) & (df['col1'] <=1 )]\nOut[13]: \n   col1  col2\n1     1    11\n"", ""In [14]: def b(x, col, op, n): \n             return op(x[col],n)\n\nIn [15]: def f(x, *b):\n             return x[(np.logical_and(*b))]\n\nIn [16]: b1 = b(df, 'col1', ge, 1)\n\nIn [17]: b2 = b(df, 'col1', le, 1)\n\nIn [18]: f(df, b1, b2)\nOut[18]: \n   col1  col2\n1     1    11\n"", 'In [21]: df.query(\'col1 <= 1 & 1 <= col1\')\nOut[21]:\n   col1  col2\n1     1    11\n\nIn [22]: df.query(""col1 <= 1 and 1 <= df[\'col1\']"") \xa0# use df[] syntax if not a valid identifier\nOut[22]:\n   col1  col2\n1     1    11\n']";"[""In [11]: df.loc[df['col1'] >= 1, 'col1']\nOut[11]: \n1    1\n2    2\nName: col1\n\nIn [12]: df[df['col1'] >= 1]\nOut[12]: \n   col1  col2\n1     1    11\n2     2    12\n\nIn [13]: df[(df['col1'] >= 1) & (df['col1'] <=1 )]\nOut[13]: \n   col1  col2\n1     1    11\n"", ""In [14]: def b(x, col, op, n): \n             return op(x[col],n)\n\nIn [15]: def f(x, *b):\n             return x[(np.logical_and(*b))]\n\nIn [16]: b1 = b(df, 'col1', ge, 1)\n\nIn [17]: b2 = b(df, 'col1', le, 1)\n\nIn [18]: f(df, b1, b2)\nOut[18]: \n   col1  col2\n1     1    11\n"", 'In [21]: df.query(\'col1 <= 1 & 1 <= col1\')\nOut[21]:\n   col1  col2\n1     1    11\n\nIn [22]: df.query(""col1 <= 1 and 1 <= df[\'col1\']"") \xa0# use df[] syntax if not a valid identifier\nOut[22]:\n   col1  col2\n1     1    11\n']"
644;;1;13653490;13651117.0;2;69;;;"<p>There isn't an option to filter the rows before the CSV file is loaded into a pandas object. </p>

<p>You can either load the file and then filter using <code>df[df['field'] &gt; constant]</code>, or if you have a very large file and you are worried about memory running out, then use an iterator and apply the filter as you concatenate chunks of your file e.g.:</p>

<pre><code>iter_csv = pandas.read_csv('file.csv', iterator=True, chunksize=1000)
df = pd.concat([chunk[chunk['field'] &gt; constant] for chunk in iter_csv])
</code></pre>

<p>You can vary the <code>chunksize</code> to suit your available memory. See <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#iterating-through-files-chunk-by-chunk"" rel=""noreferrer"">here</a> for more details.</p>
";;"[""iter_csv = pandas.read_csv('file.csv', iterator=True, chunksize=1000)\ndf = pd.concat([chunk[chunk['field'] > constant] for chunk in iter_csv])\n""]";"[""df[df['field'] > constant]"", ""iter_csv = pandas.read_csv('file.csv', iterator=True, chunksize=1000)\ndf = pd.concat([chunk[chunk['field'] > constant] for chunk in iter_csv])\n"", 'chunksize']"
645;;2;13655271;13654699.0;2;41;;;"<p>You could use <code>pd.to_datetime</code>:</p>

<pre><code>In [1]: import pandas as pd

In [2]: pd.to_datetime('2008-02-27')
Out[2]: datetime.datetime(2008, 2, 27, 0, 0)
</code></pre>

<p>This allows you to ""clean"" the index (or similarly a column) by applying it to the Series:</p>

<pre><code>df.index = pd.to_datetime(df.index)
</code></pre>

<p>or</p>

<pre><code>df['date_col'] = df['date_col'].apply(pd.to_datetime)
</code></pre>
";;"[""In [1]: import pandas as pd\n\nIn [2]: pd.to_datetime('2008-02-27')\nOut[2]: datetime.datetime(2008, 2, 27, 0, 0)\n"", 'df.index = pd.to_datetime(df.index)\n', ""df['date_col'] = df['date_col'].apply(pd.to_datetime)\n""]";"['pd.to_datetime', ""In [1]: import pandas as pd\n\nIn [2]: pd.to_datetime('2008-02-27')\nOut[2]: datetime.datetime(2008, 2, 27, 0, 0)\n"", 'df.index = pd.to_datetime(df.index)\n', ""df['date_col'] = df['date_col'].apply(pd.to_datetime)\n""]"
646;;6;13659944;13659881.0;2;31;;;"<p>If you use groupby, you will get what you want. </p>

<pre><code>d.groupby(['ip', 'useragent']).count()
</code></pre>

<p>produces:</p>

<pre><code>ip          useragent               
192.168.0.1 a           2
            b           1
192.168.0.2 b           1
</code></pre>
";;"[""d.groupby(['ip', 'useragent']).count()\n"", 'ip          useragent               \n192.168.0.1 a           2\n            b           1\n192.168.0.2 b           1\n']";"[""d.groupby(['ip', 'useragent']).count()\n"", 'ip          useragent               \n192.168.0.1 a           2\n            b           1\n192.168.0.2 b           1\n']"
647;;1;13674286;12945971.0;2;62;;;"<p>Both <code>pandas</code> and <code>matplotlib.dates</code> use <code>matplotlib.units</code> for locating the ticks. </p>

<p>But while <code>matplotlib.dates</code> has convenient ways to set the ticks manually, pandas seems to have the focus on auto formatting so far (you can have a look at the <a href=""https://github.com/pydata/pandas/blob/master/pandas/tseries/converter.py"" rel=""noreferrer"">code</a> for date conversion and formatting in pandas).</p>

<p>So for the moment it seems more reasonable to use <code>matplotlib.dates</code> (as mentioned by @BrenBarn in his comment).</p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import matplotlib.dates as dates

idx = pd.date_range('2011-05-01', '2011-07-01')
s = pd.Series(np.random.randn(len(idx)), index=idx)

fig, ax = plt.subplots()
ax.plot_date(idx.to_pydatetime(), s, 'v-')
ax.xaxis.set_minor_locator(dates.WeekdayLocator(byweekday=(1),
                                                interval=1))
ax.xaxis.set_minor_formatter(dates.DateFormatter('%d\n%a'))
ax.xaxis.grid(True, which=""minor"")
ax.yaxis.grid()
ax.xaxis.set_major_locator(dates.MonthLocator())
ax.xaxis.set_major_formatter(dates.DateFormatter('\n\n\n%b\n%Y'))
plt.tight_layout()
plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/gSOoY.png"" alt=""pandas_like_date_fomatting""></p>

<p>(my locale is German, so that Tuesday [Tue] becomes Dienstag [Di])  </p>
";;"['import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport matplotlib.dates as dates\n\nidx = pd.date_range(\'2011-05-01\', \'2011-07-01\')\ns = pd.Series(np.random.randn(len(idx)), index=idx)\n\nfig, ax = plt.subplots()\nax.plot_date(idx.to_pydatetime(), s, \'v-\')\nax.xaxis.set_minor_locator(dates.WeekdayLocator(byweekday=(1),\n                                                interval=1))\nax.xaxis.set_minor_formatter(dates.DateFormatter(\'%d\\n%a\'))\nax.xaxis.grid(True, which=""minor"")\nax.yaxis.grid()\nax.xaxis.set_major_locator(dates.MonthLocator())\nax.xaxis.set_major_formatter(dates.DateFormatter(\'\\n\\n\\n%b\\n%Y\'))\nplt.tight_layout()\nplt.show()\n']";"['pandas', 'matplotlib.dates', 'matplotlib.units', 'matplotlib.dates', 'matplotlib.dates', 'import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport matplotlib.dates as dates\n\nidx = pd.date_range(\'2011-05-01\', \'2011-07-01\')\ns = pd.Series(np.random.randn(len(idx)), index=idx)\n\nfig, ax = plt.subplots()\nax.plot_date(idx.to_pydatetime(), s, \'v-\')\nax.xaxis.set_minor_locator(dates.WeekdayLocator(byweekday=(1),\n                                                interval=1))\nax.xaxis.set_minor_formatter(dates.DateFormatter(\'%d\\n%a\'))\nax.xaxis.grid(True, which=""minor"")\nax.yaxis.grid()\nax.xaxis.set_major_locator(dates.MonthLocator())\nax.xaxis.set_major_formatter(dates.DateFormatter(\'\\n\\n\\n%b\\n%Y\'))\nplt.tight_layout()\nplt.show()\n']"
648;;4;13680953;13636848.0;2;31;;;"<p>Similar to @locojay suggestion, you can apply <a href=""http://docs.python.org/2/library/difflib.html""><code>difflib</code></a>'s <a href=""http://docs.python.org/2/library/difflib.html#difflib.get_close_matches""><code>get_closest_matches</code></a> to <code>df2</code>'s index and then apply a <a href=""http://pandas.pydata.org/pandas-docs/dev/merging.html#joining-on-index""><code>join</code></a>:</p>

<pre><code>In [23]: import difflib 

In [24]: difflib.get_close_matches
Out[24]: &lt;function difflib.get_close_matches&gt;

In [25]: df2.index = df2.index.map(lambda x: difflib.get_close_matches(x, df1.index)[0])

In [26]: df2
Out[26]: 
      letter
one        a
two        b
three      c
four       d
five       e

In [31]: df1.join(df2)
Out[31]: 
       number letter
one         1      a
two         2      b
three       3      c
four        4      d
five        5      e
</code></pre>

<p>.</p>

<p>If these were columns, in the same vein you could apply to the column then <a href=""http://pandas.pydata.org/pandas-docs/dev/merging.html#database-style-dataframe-joining-merging""><code>merge</code></a>:</p>

<pre><code>df1 = DataFrame([[1,'one'],[2,'two'],[3,'three'],[4,'four'],[5,'five']], columns=['number', 'name'])
df2 = DataFrame([['a','one'],['b','too'],['c','three'],['d','fours'],['e','five']], columns=['letter', 'name'])

df2['name'] = df2['name'].apply(lambda x: difflib.get_close_matches(x, df1['name'])[0])
df1.merge(df2)
</code></pre>
";;"['In [23]: import difflib \n\nIn [24]: difflib.get_close_matches\nOut[24]: <function difflib.get_close_matches>\n\nIn [25]: df2.index = df2.index.map(lambda x: difflib.get_close_matches(x, df1.index)[0])\n\nIn [26]: df2\nOut[26]: \n      letter\none        a\ntwo        b\nthree      c\nfour       d\nfive       e\n\nIn [31]: df1.join(df2)\nOut[31]: \n       number letter\none         1      a\ntwo         2      b\nthree       3      c\nfour        4      d\nfive        5      e\n', ""df1 = DataFrame([[1,'one'],[2,'two'],[3,'three'],[4,'four'],[5,'five']], columns=['number', 'name'])\ndf2 = DataFrame([['a','one'],['b','too'],['c','three'],['d','fours'],['e','five']], columns=['letter', 'name'])\n\ndf2['name'] = df2['name'].apply(lambda x: difflib.get_close_matches(x, df1['name'])[0])\ndf1.merge(df2)\n""]";"['difflib', 'get_closest_matches', 'df2', 'join', 'In [23]: import difflib \n\nIn [24]: difflib.get_close_matches\nOut[24]: <function difflib.get_close_matches>\n\nIn [25]: df2.index = df2.index.map(lambda x: difflib.get_close_matches(x, df1.index)[0])\n\nIn [26]: df2\nOut[26]: \n      letter\none        a\ntwo        b\nthree      c\nfour       d\nfive       e\n\nIn [31]: df1.join(df2)\nOut[31]: \n       number letter\none         1      a\ntwo         2      b\nthree       3      c\nfour        4      d\nfive        5      e\n', 'merge', ""df1 = DataFrame([[1,'one'],[2,'two'],[3,'three'],[4,'four'],[5,'five']], columns=['number', 'name'])\ndf2 = DataFrame([['a','one'],['b','too'],['c','three'],['d','fours'],['e','five']], columns=['letter', 'name'])\n\ndf2['name'] = df2['name'].apply(lambda x: difflib.get_close_matches(x, df1['name'])[0])\ndf1.merge(df2)\n""]"
649;;4;13682381;13682044.0;2;64;;;"<pre><code>data['result'] = data['result'].map(lambda x: x.lstrip('+-').rstrip('aAbBcC'))
</code></pre>
";;"[""data['result'] = data['result'].map(lambda x: x.lstrip('+-').rstrip('aAbBcC'))\n""]";"[""data['result'] = data['result'].map(lambda x: x.lstrip('+-').rstrip('aAbBcC'))\n""]"
650;;1;13688105;13682044.0;2;10;;;"<p>There's a bug here: currently cannot pass arguments to <code>str.lstrip</code> and <code>str.rstrip</code>:</p>

<p><a href=""http://github.com/pydata/pandas/issues/2411"" rel=""noreferrer"">http://github.com/pydata/pandas/issues/2411</a></p>

<p>EDIT: 2012-12-07 this works now on the dev branch:</p>

<pre><code>In [8]: df['result'].str.lstrip('+-').str.rstrip('aAbBcC')
Out[8]: 
1     52
2     62
3     44
4     30
5    110
Name: result
</code></pre>
";;"[""In [8]: df['result'].str.lstrip('+-').str.rstrip('aAbBcC')\nOut[8]: \n1     52\n2     62\n3     44\n4     30\n5    110\nName: result\n""]";"['str.lstrip', 'str.rstrip', ""In [8]: df['result'].str.lstrip('+-').str.rstrip('aAbBcC')\nOut[8]: \n1     52\n2     62\n3     44\n4     30\n5    110\nName: result\n""]"
651;;2;13703721;13703720.0;2;7;;;"<p>One option is to use <code>str</code>, and then <code>to_datetime</code> (or similar):</p>

<pre><code>In [11]: str(dt64)
Out[11]: '2012-05-01T01:00:00.000000+0100'

In [12]: pd.to_datetime(str(dt64))
Out[12]: datetime.datetime(2012, 5, 1, 1, 0, tzinfo=tzoffset(None, 3600))
</code></pre>

<p><em>Note: it is not equal to <code>dt</code> because it's become <a href=""https://stackoverflow.com/questions/796008/python-cant-subtract-offset-naive-and-offset-aware-datetimes"">""offset-aware""</a>:</em></p>

<pre><code>In [13]: pd.to_datetime(str(dt64)).replace(tzinfo=None)
Out[13]: datetime.datetime(2012, 5, 1, 1, 0)
</code></pre>

<p>This seems inelegant.</p>

<p>.</p>

<p>Update: this can deal with the ""nasty example"":</p>

<pre><code>In [21]: dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100')

In [22]: pd.to_datetime(str(dt64)).replace(tzinfo=None)
Out[22]: datetime.datetime(2002, 6, 28, 1, 0)
</code></pre>
";;"[""In [11]: str(dt64)\nOut[11]: '2012-05-01T01:00:00.000000+0100'\n\nIn [12]: pd.to_datetime(str(dt64))\nOut[12]: datetime.datetime(2012, 5, 1, 1, 0, tzinfo=tzoffset(None, 3600))\n"", 'In [13]: pd.to_datetime(str(dt64)).replace(tzinfo=None)\nOut[13]: datetime.datetime(2012, 5, 1, 1, 0)\n', ""In [21]: dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100')\n\nIn [22]: pd.to_datetime(str(dt64)).replace(tzinfo=None)\nOut[22]: datetime.datetime(2002, 6, 28, 1, 0)\n""]";"['str', 'to_datetime', ""In [11]: str(dt64)\nOut[11]: '2012-05-01T01:00:00.000000+0100'\n\nIn [12]: pd.to_datetime(str(dt64))\nOut[12]: datetime.datetime(2012, 5, 1, 1, 0, tzinfo=tzoffset(None, 3600))\n"", 'dt', 'In [13]: pd.to_datetime(str(dt64)).replace(tzinfo=None)\nOut[13]: datetime.datetime(2012, 5, 1, 1, 0)\n', ""In [21]: dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100')\n\nIn [22]: pd.to_datetime(str(dt64)).replace(tzinfo=None)\nOut[22]: datetime.datetime(2002, 6, 28, 1, 0)\n""]"
652;;5;13703930;13703720.0;2;21;;;"<pre><code>&gt;&gt;&gt; dt64.tolist()
datetime.datetime(2012, 5, 1, 0, 0)
</code></pre>

<p>For <code>DatetimeIndex</code>, the <code>tolist</code> returns a list of <code>datetime</code> objects. For a single <code>datetime64</code> object it returns a single <code>datetime</code> object.</p>
";;['>>> dt64.tolist()\ndatetime.datetime(2012, 5, 1, 0, 0)\n'];['>>> dt64.tolist()\ndatetime.datetime(2012, 5, 1, 0, 0)\n', 'DatetimeIndex', 'tolist', 'datetime', 'datetime64', 'datetime']
653;;9;13704307;13703720.0;2;72;;;"<p>To convert <code>numpy.datetime64</code> to datetime object that represents time in UTC on <code>numpy-1.8</code>:</p>

<pre><code>&gt;&gt;&gt; from datetime import datetime
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; dt = datetime.utcnow()
&gt;&gt;&gt; dt
datetime.datetime(2012, 12, 4, 19, 51, 25, 362455)
&gt;&gt;&gt; dt64 = np.datetime64(dt)
&gt;&gt;&gt; ts = (dt64 - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')
&gt;&gt;&gt; ts
1354650685.3624549
&gt;&gt;&gt; datetime.utcfromtimestamp(ts)
datetime.datetime(2012, 12, 4, 19, 51, 25, 362455)
&gt;&gt;&gt; np.__version__
'1.8.0.dev-7b75899'
</code></pre>

<p>The above example assumes that a naive datetime object is interpreted by <code>np.datetime64</code> as time in UTC.</p>

<hr>

<p>To convert datetime to np.datetime64 and back (<code>numpy-1.6</code>):</p>

<pre><code>&gt;&gt;&gt; np.datetime64(datetime.utcnow()).astype(datetime)
datetime.datetime(2012, 12, 4, 13, 34, 52, 827542)
</code></pre>

<p>It works both on a single np.datetime64 object and a numpy array of np.datetime64.</p>

<p>Think of np.datetime64 the same way you would about np.int8, np.int16, etc and apply the same methods to convert beetween Python objects such as int, datetime and corresponding numpy objects.</p>

<p>Your ""nasty example"" works correctly:</p>

<pre><code>&gt;&gt;&gt; from datetime import datetime
&gt;&gt;&gt; import numpy 
&gt;&gt;&gt; numpy.datetime64('2002-06-28T01:00:00.000000000+0100').astype(datetime)
datetime.datetime(2002, 6, 28, 0, 0)
&gt;&gt;&gt; numpy.__version__
'1.6.2' # current version available via pip install numpy
</code></pre>

<p>I can reproduce the <code>long</code> value on <code>numpy-1.8.0</code> installed as:</p>

<pre><code>pip install git+https://github.com/numpy/numpy.git#egg=numpy-dev
</code></pre>

<p>The same example:</p>

<pre><code>&gt;&gt;&gt; from datetime import datetime
&gt;&gt;&gt; import numpy
&gt;&gt;&gt; numpy.datetime64('2002-06-28T01:00:00.000000000+0100').astype(datetime)
1025222400000000000L
&gt;&gt;&gt; numpy.__version__
'1.8.0.dev-7b75899'
</code></pre>

<p>It returns <code>long</code> because for <code>numpy.datetime64</code> type <code>.astype(datetime)</code> is equivalent to <code>.astype(object)</code> that returns Python integer (<code>long</code>) on <code>numpy-1.8</code>. </p>

<p>To get datetime object you could:</p>

<pre><code>&gt;&gt;&gt; dt64.dtype
dtype('&lt;M8[ns]')
&gt;&gt;&gt; ns = 1e-9 # number of seconds in a nanosecond
&gt;&gt;&gt; datetime.utcfromtimestamp(dt64.astype(int) * ns)
datetime.datetime(2002, 6, 28, 0, 0)
</code></pre>

<p>To get datetime64 that uses seconds directly:</p>

<pre><code>&gt;&gt;&gt; dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100', 's')
&gt;&gt;&gt; dt64.dtype
dtype('&lt;M8[s]')
&gt;&gt;&gt; datetime.utcfromtimestamp(dt64.astype(int))
datetime.datetime(2002, 6, 28, 0, 0)
</code></pre>

<p>The <a href=""http://docs.scipy.org/doc/numpy-dev/reference/arrays.datetime.html"">numpy docs</a> say that the datetime API is experimental and may change in future numpy versions.</p>
";;"["">>> from datetime import datetime\n>>> import numpy as np\n>>> dt = datetime.utcnow()\n>>> dt\ndatetime.datetime(2012, 12, 4, 19, 51, 25, 362455)\n>>> dt64 = np.datetime64(dt)\n>>> ts = (dt64 - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\n>>> ts\n1354650685.3624549\n>>> datetime.utcfromtimestamp(ts)\ndatetime.datetime(2012, 12, 4, 19, 51, 25, 362455)\n>>> np.__version__\n'1.8.0.dev-7b75899'\n"", '>>> np.datetime64(datetime.utcnow()).astype(datetime)\ndatetime.datetime(2012, 12, 4, 13, 34, 52, 827542)\n', "">>> from datetime import datetime\n>>> import numpy \n>>> numpy.datetime64('2002-06-28T01:00:00.000000000+0100').astype(datetime)\ndatetime.datetime(2002, 6, 28, 0, 0)\n>>> numpy.__version__\n'1.6.2' # current version available via pip install numpy\n"", 'pip install git+https://github.com/numpy/numpy.git#egg=numpy-dev\n', "">>> from datetime import datetime\n>>> import numpy\n>>> numpy.datetime64('2002-06-28T01:00:00.000000000+0100').astype(datetime)\n1025222400000000000L\n>>> numpy.__version__\n'1.8.0.dev-7b75899'\n"", "">>> dt64.dtype\ndtype('<M8[ns]')\n>>> ns = 1e-9 # number of seconds in a nanosecond\n>>> datetime.utcfromtimestamp(dt64.astype(int) * ns)\ndatetime.datetime(2002, 6, 28, 0, 0)\n"", "">>> dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100', 's')\n>>> dt64.dtype\ndtype('<M8[s]')\n>>> datetime.utcfromtimestamp(dt64.astype(int))\ndatetime.datetime(2002, 6, 28, 0, 0)\n""]";"['numpy.datetime64', 'numpy-1.8', "">>> from datetime import datetime\n>>> import numpy as np\n>>> dt = datetime.utcnow()\n>>> dt\ndatetime.datetime(2012, 12, 4, 19, 51, 25, 362455)\n>>> dt64 = np.datetime64(dt)\n>>> ts = (dt64 - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\n>>> ts\n1354650685.3624549\n>>> datetime.utcfromtimestamp(ts)\ndatetime.datetime(2012, 12, 4, 19, 51, 25, 362455)\n>>> np.__version__\n'1.8.0.dev-7b75899'\n"", 'np.datetime64', 'numpy-1.6', '>>> np.datetime64(datetime.utcnow()).astype(datetime)\ndatetime.datetime(2012, 12, 4, 13, 34, 52, 827542)\n', "">>> from datetime import datetime\n>>> import numpy \n>>> numpy.datetime64('2002-06-28T01:00:00.000000000+0100').astype(datetime)\ndatetime.datetime(2002, 6, 28, 0, 0)\n>>> numpy.__version__\n'1.6.2' # current version available via pip install numpy\n"", 'long', 'numpy-1.8.0', 'pip install git+https://github.com/numpy/numpy.git#egg=numpy-dev\n', "">>> from datetime import datetime\n>>> import numpy\n>>> numpy.datetime64('2002-06-28T01:00:00.000000000+0100').astype(datetime)\n1025222400000000000L\n>>> numpy.__version__\n'1.8.0.dev-7b75899'\n"", 'long', 'numpy.datetime64', '.astype(datetime)', '.astype(object)', 'long', 'numpy-1.8', "">>> dt64.dtype\ndtype('<M8[ns]')\n>>> ns = 1e-9 # number of seconds in a nanosecond\n>>> datetime.utcfromtimestamp(dt64.astype(int) * ns)\ndatetime.datetime(2002, 6, 28, 0, 0)\n"", "">>> dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100', 's')\n>>> dt64.dtype\ndtype('<M8[s]')\n>>> datetime.utcfromtimestamp(dt64.astype(int))\ndatetime.datetime(2002, 6, 28, 0, 0)\n""]"
654;;0;13731128;9758450.0;2;27;;;"<p>A generic way:</p>

<pre><code>[tuple(x) for x in data_set.to_records(index=False)]
</code></pre>
";;['[tuple(x) for x in data_set.to_records(index=False)]\n'];['[tuple(x) for x in data_set.to_records(index=False)]\n']
655;;2;13753918;13703720.0;2;82;;;"<p>Welcome to hell.</p>

<p>You can just pass a datetime64 object to <code>pandas.Timestamp</code>:</p>

<pre><code>In [16]: Timestamp(numpy.datetime64('2012-05-01T01:00:00.000000'))
Out[16]: &lt;Timestamp: 2012-05-01 01:00:00&gt;
</code></pre>

<p>I noticed that this doesn't work right though in NumPy 1.6.1:</p>

<pre><code>numpy.datetime64('2012-05-01T01:00:00.000000+0100')
</code></pre>

<p>Also, <code>pandas.to_datetime</code> can be used (this is off of the dev version, haven't checked v0.9.1):</p>

<pre><code>In [24]: pandas.to_datetime('2012-05-01T01:00:00.000000+0100')
Out[24]: datetime.datetime(2012, 5, 1, 1, 0, tzinfo=tzoffset(None, 3600))
</code></pre>
";;"[""In [16]: Timestamp(numpy.datetime64('2012-05-01T01:00:00.000000'))\nOut[16]: <Timestamp: 2012-05-01 01:00:00>\n"", ""numpy.datetime64('2012-05-01T01:00:00.000000+0100')\n"", ""In [24]: pandas.to_datetime('2012-05-01T01:00:00.000000+0100')\nOut[24]: datetime.datetime(2012, 5, 1, 1, 0, tzinfo=tzoffset(None, 3600))\n""]";"['pandas.Timestamp', ""In [16]: Timestamp(numpy.datetime64('2012-05-01T01:00:00.000000'))\nOut[16]: <Timestamp: 2012-05-01 01:00:00>\n"", ""numpy.datetime64('2012-05-01T01:00:00.000000+0100')\n"", 'pandas.to_datetime', ""In [24]: pandas.to_datetime('2012-05-01T01:00:00.000000+0100')\nOut[24]: datetime.datetime(2012, 5, 1, 1, 0, tzinfo=tzoffset(None, 3600))\n""]"
656;;7;13786327;13784192.0;2;118;;;"<p>Here's a couple of suggestions:</p>

<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.date_range.html"" rel=""noreferrer""><code>date_range</code></a> for the index:</p>

<pre><code>import datetime
import pandas as pd
import numpy as np

todays_date = datetime.datetime.now().date()
index = pd.date_range(todays_date-datetime.timedelta(10), periods=10, freq='D')

columns = ['A','B', 'C']
</code></pre>

<p><em>Note: we could create an empty DataFrame (with <code>NaN</code>s) simply by writing:</em></p>

<pre><code>df_ = pd.DataFrame(index=index, columns=columns)
df_ = df_.fillna(0) # with 0s rather than NaNs
</code></pre>

<p>To do these type of calculations for the data, use a numpy array:</p>

<pre><code>data = np.array([np.arange(10)]*3).T
</code></pre>

<p>Hence we can create the DataFrame:</p>

<pre><code>In [10]: df = pd.DataFrame(data, index=index, columns=columns)

In [11]: df
Out[11]: 
            A  B  C
2012-11-29  0  0  0
2012-11-30  1  1  1
2012-12-01  2  2  2
2012-12-02  3  3  3
2012-12-03  4  4  4
2012-12-04  5  5  5
2012-12-05  6  6  6
2012-12-06  7  7  7
2012-12-07  8  8  8
2012-12-08  9  9  9
</code></pre>
";;"[""import datetime\nimport pandas as pd\nimport numpy as np\n\ntodays_date = datetime.datetime.now().date()\nindex = pd.date_range(todays_date-datetime.timedelta(10), periods=10, freq='D')\n\ncolumns = ['A','B', 'C']\n"", 'df_ = pd.DataFrame(index=index, columns=columns)\ndf_ = df_.fillna(0) # with 0s rather than NaNs\n', 'data = np.array([np.arange(10)]*3).T\n', 'In [10]: df = pd.DataFrame(data, index=index, columns=columns)\n\nIn [11]: df\nOut[11]: \n            A  B  C\n2012-11-29  0  0  0\n2012-11-30  1  1  1\n2012-12-01  2  2  2\n2012-12-02  3  3  3\n2012-12-03  4  4  4\n2012-12-04  5  5  5\n2012-12-05  6  6  6\n2012-12-06  7  7  7\n2012-12-07  8  8  8\n2012-12-08  9  9  9\n']";"['date_range', ""import datetime\nimport pandas as pd\nimport numpy as np\n\ntodays_date = datetime.datetime.now().date()\nindex = pd.date_range(todays_date-datetime.timedelta(10), periods=10, freq='D')\n\ncolumns = ['A','B', 'C']\n"", 'NaN', 'df_ = pd.DataFrame(index=index, columns=columns)\ndf_ = df_.fillna(0) # with 0s rather than NaNs\n', 'data = np.array([np.arange(10)]*3).T\n', 'In [10]: df = pd.DataFrame(data, index=index, columns=columns)\n\nIn [11]: df\nOut[11]: \n            A  B  C\n2012-11-29  0  0  0\n2012-11-30  1  1  1\n2012-12-01  2  2  2\n2012-12-02  3  3  3\n2012-12-03  4  4  4\n2012-12-04  5  5  5\n2012-12-05  6  6  6\n2012-12-06  7  7  7\n2012-12-07  8  8  8\n2012-12-08  9  9  9\n']"
657;;1;13833239;11232275.0;2;12;;;"<p>Try this,</p>

<pre><code>df.drop_duplicates(['foo','bar'])
df.pivot('foo','bar','baz')
</code></pre>
";;"[""df.drop_duplicates(['foo','bar'])\ndf.pivot('foo','bar','baz')\n""]";"[""df.drop_duplicates(['foo','bar'])\ndf.pivot('foo','bar','baz')\n""]"
658;;8;13839029;13838405.0;2;33;;;"<p>Pandas 0.15 introduced <a href=""http://pandas.pydata.org/pandas-docs/stable/categorical.html"">Categorical Series</a>, which allows a much clearer way to do this:</p>

<p>First make the month column a categorical and specify the ordering to use.</p>

<pre><code>In [21]: df['m'] = pd.Categorical(df['m'], [""March"", ""April"", ""Dec""])

In [22]: df  # looks the same!
Out[22]:
   a  b      m
0  1  2  March
1  5  6    Dec
2  3  4  April
</code></pre>

<p>Now, when you sort the month column it will sort with respect to that list:</p>

<pre><code>In [23]: df.sort(""m"")
Out[23]:
   a  b      m
0  1  2  March
2  3  4  April
1  5  6    Dec
</code></pre>

<p><em>Note: if a value is not in the list it will be converted to NaN.</em></p>

<hr>

<p>An older answer for those interested...</p>

<p>You could create an intermediary series, and <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.set_index.html""><code>set_index</code></a> on that:</p>

<pre><code>df = pd.DataFrame([[1, 2, 'March'],[5, 6, 'Dec'],[3, 4, 'April']], columns=['a','b','m'])
s = df['m'].apply(lambda x: {'March':0, 'April':1, 'Dec':3}[x])
s.sort()

In [4]: df.set_index(s.index).sort()
Out[4]: 
   a  b      m
0  1  2  March
1  3  4  April
2  5  6    Dec
</code></pre>

<hr>

<p>As commented, in newer pandas, Series has a <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.replace.html""><code>replace</code></a> method to do this more elegantly:</p>

<pre><code>s = df['m'].replace({'March':0, 'April':1, 'Dec':3})
</code></pre>

<p><em>The slight difference is that this won't raise if there is a value outside of the dictionary (it'll just stay the same).</em></p>
";;"['In [21]: df[\'m\'] = pd.Categorical(df[\'m\'], [""March"", ""April"", ""Dec""])\n\nIn [22]: df  # looks the same!\nOut[22]:\n   a  b      m\n0  1  2  March\n1  5  6    Dec\n2  3  4  April\n', 'In [23]: df.sort(""m"")\nOut[23]:\n   a  b      m\n0  1  2  March\n2  3  4  April\n1  5  6    Dec\n', ""df = pd.DataFrame([[1, 2, 'March'],[5, 6, 'Dec'],[3, 4, 'April']], columns=['a','b','m'])\ns = df['m'].apply(lambda x: {'March':0, 'April':1, 'Dec':3}[x])\ns.sort()\n\nIn [4]: df.set_index(s.index).sort()\nOut[4]: \n   a  b      m\n0  1  2  March\n1  3  4  April\n2  5  6    Dec\n"", ""s = df['m'].replace({'March':0, 'April':1, 'Dec':3})\n""]";"['In [21]: df[\'m\'] = pd.Categorical(df[\'m\'], [""March"", ""April"", ""Dec""])\n\nIn [22]: df  # looks the same!\nOut[22]:\n   a  b      m\n0  1  2  March\n1  5  6    Dec\n2  3  4  April\n', 'In [23]: df.sort(""m"")\nOut[23]:\n   a  b      m\n0  1  2  March\n2  3  4  April\n1  5  6    Dec\n', 'set_index', ""df = pd.DataFrame([[1, 2, 'March'],[5, 6, 'Dec'],[3, 4, 'April']], columns=['a','b','m'])\ns = df['m'].apply(lambda x: {'March':0, 'April':1, 'Dec':3}[x])\ns.sort()\n\nIn [4]: df.set_index(s.index).sort()\nOut[4]: \n   a  b      m\n0  1  2  March\n1  3  4  April\n2  5  6    Dec\n"", 'replace', ""s = df['m'].replace({'March':0, 'April':1, 'Dec':3})\n""]"
659;;8;13842286;13842088.0;2;168;;;"<p><a href=""https://stackoverflow.com/a/24517695/190597"">RukTech's answer</a>, <code>df.set_value('C', 'x', 10)</code>, is far and away faster than the options I've suggested below.</p>

<hr>

<p><strong>Warning</strong>: It is sometimes difficult to predict if an operation returns a copy or a view. For this reason the <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#returning-a-view-versus-a-copy"" rel=""noreferrer"">docs recommend avoiding using ""chained indexing""</a>. </p>

<hr>

<p><strong>Why <code>df.xs('C')['x']=10</code> does not work:</strong></p>

<p><code>df.xs('C')</code> by default, returns a new dataframe <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.xs.html#pandas.DataFrame.xsY"" rel=""noreferrer"">with a copy</a> of the data, so </p>

<pre><code>df.xs('C')['x']=10
</code></pre>

<p>modifies this new dataframe only.</p>

<p><code>df['x']</code> returns a view of the <code>df</code> dataframe, so </p>

<pre><code>df['x']['C'] = 10
</code></pre>

<p>modifies <code>df</code> itself.</p>

<hr>

<p>Alternatively,</p>

<pre><code>df.xs('C', copy = False)['x']=10
</code></pre>

<p><em>does</em> modify <code>df</code>.</p>

<hr>

<p><code>df.set_value('C', 'x', 10)</code> is the fastest:</p>

<pre><code>In [18]: %timeit df.set_value('C', 'x', 10)
100000 loops, best of 3: 2.9 s per loop

In [20]: %timeit df['x']['C'] = 10
100000 loops, best of 3: 6.31 s per loop

In [19]: %timeit df.ix['C','x'] = 10
10000 loops, best of 3: 104 s per loop

In [32]: %timeit df.xs('C', copy=False)['x'] = 10
10000 loops, best of 3: 89.2 s per loop
</code></pre>
";;"[""df.xs('C')['x']=10\n"", ""df['x']['C'] = 10\n"", ""df.xs('C', copy = False)['x']=10\n"", ""In [18]: %timeit df.set_value('C', 'x', 10)\n100000 loops, best of 3: 2.9 s per loop\n\nIn [20]: %timeit df['x']['C'] = 10\n100000 loops, best of 3: 6.31 s per loop\n\nIn [19]: %timeit df.ix['C','x'] = 10\n10000 loops, best of 3: 104 s per loop\n\nIn [32]: %timeit df.xs('C', copy=False)['x'] = 10\n10000 loops, best of 3: 89.2 s per loop\n""]";"[""df.set_value('C', 'x', 10)"", ""df.xs('C')['x']=10"", ""df.xs('C')"", ""df.xs('C')['x']=10\n"", ""df['x']"", 'df', ""df['x']['C'] = 10\n"", 'df', ""df.xs('C', copy = False)['x']=10\n"", 'df', ""df.set_value('C', 'x', 10)"", ""In [18]: %timeit df.set_value('C', 'x', 10)\n100000 loops, best of 3: 2.9 s per loop\n\nIn [20]: %timeit df['x']['C'] = 10\n100000 loops, best of 3: 6.31 s per loop\n\nIn [19]: %timeit df.ix['C','x'] = 10\n10000 loops, best of 3: 104 s per loop\n\nIn [32]: %timeit df.xs('C', copy=False)['x'] = 10\n10000 loops, best of 3: 89.2 s per loop\n""]"
660;;3;13843741;12555323.0;2;99;;;"<p>This is the simple way of adding a new column: <code>df['e'] = e</code></p>
";;[];"[""df['e'] = e""]"
661;;1;13851602;13851535.0;2;42;;;"<p>When you do <code>len(df['column name'])</code> you are just getting one number, namely the number of rows in the DataFrame (i.e., the length of the column itself).  If you want to apply <code>len</code> to each element in the column, use <code>df['column name'].map(len)</code>.  So try</p>

<pre><code>df[df['column name'].map(len) &lt; 2]
</code></pre>
";;"[""df[df['column name'].map(len) < 2]\n""]";"[""len(df['column name'])"", 'len', ""df['column name'].map(len)"", ""df[df['column name'].map(len) < 2]\n""]"
662;;3;13866073;13636592.0;2;30;;;"<p>On pandas 0.9.1 and higher this should work (this is with 0.10.0b1):</p>

<p>(<strong>Edit:</strong> As of Pandas 0.19, method <code>sort_index</code> is deprecated. Prefer <a href=""http://pandas.pydata.org/pandas-docs/version/0.19.1/generated/pandas.DataFrame.sort_values.html?highlight=sort_values#pandas.DataFrame.sort_values"" rel=""nofollow noreferrer"" title=""sort_values""><code>sort_values</code></a>)</p>

<pre><code>In [23]: songs.sort_index(by=['Peak', 'Weeks'], ascending=[True, False])
Out[23]: 
                                      Song  Peak  Weeks
10                           She Loves You     1     36
118                               Hey Jude     1     27
20                I Want To Hold Your Hand     1     24
22                       Can't Buy Me Love     1     17
56                                   Help!     1     17
76                        Paperback Writer     1     16
109                   All You Need Is Love     1     16
45                             I Feel Fine     1     15
29                      A Hard Day's Night     1     14
48                          Ticket To Ride     1     14
85                           Eleanor Rigby     1     14
87                        Yellow Submarine     1     14
173            The Ballad Of John And Yoko     1     13
60                             Day Tripper     1     12
61                      We Can Work It Out     1     12
117                           Lady Madonna     1      9
8                           From Me To You     1      7
115                          Hello Goodbye     1      7
155                               Get Back     1      6
2                         Please Please Me     2     20
107                   Magical Mystery Tour     2     16
176                              Let It Be     2     14
93                              Penny Lane     2     13
92               Strawberry Fields Forever     2     12
0                               Love Me Do     4     26
166                          Come Together     4     10
157                              Something     4      9
58                               Yesterday     8     21
135                   Back In The U.S.S.R.    19      3
164                     Here Comes The Sun    58     19
96   Sgt. Pepper's Lonely Hearts Club Band    63     12
105     With A Little Help From My Friends    63      7
</code></pre>
";;"[""In [23]: songs.sort_index(by=['Peak', 'Weeks'], ascending=[True, False])\nOut[23]: \n                                      Song  Peak  Weeks\n10                           She Loves You     1     36\n118                               Hey Jude     1     27\n20                I Want To Hold Your Hand     1     24\n22                       Can't Buy Me Love     1     17\n56                                   Help!     1     17\n76                        Paperback Writer     1     16\n109                   All You Need Is Love     1     16\n45                             I Feel Fine     1     15\n29                      A Hard Day's Night     1     14\n48                          Ticket To Ride     1     14\n85                           Eleanor Rigby     1     14\n87                        Yellow Submarine     1     14\n173            The Ballad Of John And Yoko     1     13\n60                             Day Tripper     1     12\n61                      We Can Work It Out     1     12\n117                           Lady Madonna     1      9\n8                           From Me To You     1      7\n115                          Hello Goodbye     1      7\n155                               Get Back     1      6\n2                         Please Please Me     2     20\n107                   Magical Mystery Tour     2     16\n176                              Let It Be     2     14\n93                              Penny Lane     2     13\n92               Strawberry Fields Forever     2     12\n0                               Love Me Do     4     26\n166                          Come Together     4     10\n157                              Something     4      9\n58                               Yesterday     8     21\n135                   Back In The U.S.S.R.    19      3\n164                     Here Comes The Sun    58     19\n96   Sgt. Pepper's Lonely Hearts Club Band    63     12\n105     With A Little Help From My Friends    63      7\n""]";"['sort_index', 'sort_values', ""In [23]: songs.sort_index(by=['Peak', 'Weeks'], ascending=[True, False])\nOut[23]: \n                                      Song  Peak  Weeks\n10                           She Loves You     1     36\n118                               Hey Jude     1     27\n20                I Want To Hold Your Hand     1     24\n22                       Can't Buy Me Love     1     17\n56                                   Help!     1     17\n76                        Paperback Writer     1     16\n109                   All You Need Is Love     1     16\n45                             I Feel Fine     1     15\n29                      A Hard Day's Night     1     14\n48                          Ticket To Ride     1     14\n85                           Eleanor Rigby     1     14\n87                        Yellow Submarine     1     14\n173            The Ballad Of John And Yoko     1     13\n60                             Day Tripper     1     12\n61                      We Can Work It Out     1     12\n117                           Lady Madonna     1      9\n8                           From Me To You     1      7\n115                          Hello Goodbye     1      7\n155                               Get Back     1      6\n2                         Please Please Me     2     20\n107                   Magical Mystery Tour     2     16\n176                              Let It Be     2     14\n93                              Penny Lane     2     13\n92               Strawberry Fields Forever     2     12\n0                               Love Me Do     4     26\n166                          Come Together     4     10\n157                              Something     4      9\n58                               Yesterday     8     21\n135                   Back In The U.S.S.R.    19      3\n164                     Here Comes The Sun    58     19\n96   Sgt. Pepper's Lonely Hearts Club Band    63     12\n105     With A Little Help From My Friends    63      7\n""]"
663;;3;13873014;13872533.0;2;144;;;"<p>Try:</p>

<pre><code>ax = df1.plot()
df2.plot(ax=ax)
</code></pre>
";;['ax = df1.plot()\ndf2.plot(ax=ax)\n'];['ax = df1.plot()\ndf2.plot(ax=ax)\n']
664;;0;13876784;13872533.0;2;15;;;"<p>Although Chang's answer explains how to plot multiple times on the same figure, in this case you might be better off in this case using a <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby"" rel=""noreferrer""><code>groupby</code></a> and <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.unstack.html#pandas.DataFrame.unstack"" rel=""noreferrer""><code>unstack</code></a>ing:</p>

<p><em>(Assuming you have this in dataframe, with datetime index already)</em></p>

<pre><code>In [1]: df
Out[1]:
            value  
datetime                         
2010-01-01      1  
2010-02-01      1  
2009-01-01      1  

# create additional month and year columns for convenience
df['Month'] = map(lambda x: x.month, df.index)
df['Year'] = map(lambda x: x.year, df.index)    

In [5]: df.groupby(['Month','Year']).mean().unstack()
Out[5]:
       value      
Year    2009  2010
Month             
1          1     1
2        NaN     1
</code></pre>

<p>Now it's easy to plot (each year as a separate line):</p>

<pre><code>df.groupby(['Month','Year']).mean().unstack().plot()
</code></pre>
";;"[""In [1]: df\nOut[1]:\n            value  \ndatetime                         \n2010-01-01      1  \n2010-02-01      1  \n2009-01-01      1  \n\n# create additional month and year columns for convenience\ndf['Month'] = map(lambda x: x.month, df.index)\ndf['Year'] = map(lambda x: x.year, df.index)    \n\nIn [5]: df.groupby(['Month','Year']).mean().unstack()\nOut[5]:\n       value      \nYear    2009  2010\nMonth             \n1          1     1\n2        NaN     1\n"", ""df.groupby(['Month','Year']).mean().unstack().plot()\n""]";"['groupby', 'unstack', ""In [1]: df\nOut[1]:\n            value  \ndatetime                         \n2010-01-01      1  \n2010-02-01      1  \n2009-01-01      1  \n\n# create additional month and year columns for convenience\ndf['Month'] = map(lambda x: x.month, df.index)\ndf['Year'] = map(lambda x: x.year, df.index)    \n\nIn [5]: df.groupby(['Month','Year']).mean().unstack()\nOut[5]:\n       value      \nYear    2009  2010\nMonth             \n1          1     1\n2        NaN     1\n"", ""df.groupby(['Month','Year']).mean().unstack().plot()\n""]"
665;;4;13888546;13888468.0;2;24;;;"<p>One way is to use <a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html#some-gory-internal-details"" rel=""noreferrer""><code>index.levels</code></a>:</p>

<pre><code>In [11]: df
Out[11]: 
       C
A B     
0 one  3
1 one  2
2 two  1

In [12]: df.index.levels[1]
Out[12]: Index([one, two], dtype=object)
</code></pre>
";;['In [11]: df\nOut[11]: \n       C\nA B     \n0 one  3\n1 one  2\n2 two  1\n\nIn [12]: df.index.levels[1]\nOut[12]: Index([one, two], dtype=object)\n'];['index.levels', 'In [11]: df\nOut[11]: \n       C\nA B     \n0 one  3\n1 one  2\n2 two  1\n\nIn [12]: df.index.levels[1]\nOut[12]: Index([one, two], dtype=object)\n']
666;;0;13921674;13921647.0;2;74;;;"<p><code>df.shape</code>, where <code>df</code> is your DataFrame.</p>
";;[];['df.shape', 'df']
667;;0;14000420;13999850.0;2;27;;;"<p>You could use <a href=""http://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior""><code>strftime</code></a> to save these as separate columns:</p>

<pre><code>df['date'] = df['datetime'].apply(lambda x: x.strftime('%d%m%Y'))
df['time'] = df['datetime'].apply(lambda x: x.strftime('%H%M%S'))
</code></pre>

<p>and then be specific about which columns to export to csv:</p>

<pre><code>df[['date', 'time', ... ]].to_csv('df.csv')
</code></pre>
";;"[""df['date'] = df['datetime'].apply(lambda x: x.strftime('%d%m%Y'))\ndf['time'] = df['datetime'].apply(lambda x: x.strftime('%H%M%S'))\n"", ""df[['date', 'time', ... ]].to_csv('df.csv')\n""]";"['strftime', ""df['date'] = df['datetime'].apply(lambda x: x.strftime('%d%m%Y'))\ndf['time'] = df['datetime'].apply(lambda x: x.strftime('%H%M%S'))\n"", ""df[['date', 'time', ... ]].to_csv('df.csv')\n""]"
668;;0;14016590;14016247.0;2;24;;;"<p>For DataFrame <code>df</code>:</p>

<pre><code>import numpy as np
index = df['b'].index[df['b'].apply(np.isnan)]
</code></pre>

<p>will give you back the <code>MultiIndex</code> that you can use to index back into <code>df</code>, e.g.:</p>

<pre><code>df['a'].ix[index[0]]
&gt;&gt;&gt; 1.452354
</code></pre>

<p>For the integer index:</p>

<pre><code>df_index = df.index.values.tolist()
[df_index.index(i) for i in index]
&gt;&gt;&gt; [3, 6]
</code></pre>
";;"[""import numpy as np\nindex = df['b'].index[df['b'].apply(np.isnan)]\n"", ""df['a'].ix[index[0]]\n>>> 1.452354\n"", 'df_index = df.index.values.tolist()\n[df_index.index(i) for i in index]\n>>> [3, 6]\n']";"['df', ""import numpy as np\nindex = df['b'].index[df['b'].apply(np.isnan)]\n"", 'MultiIndex', 'df', ""df['a'].ix[index[0]]\n>>> 1.452354\n"", 'df_index = df.index.values.tolist()\n[df_index.index(i) for i in index]\n>>> [3, 6]\n']"
669;;2;14033137;14016247.0;2;95;;;"<p>Here is a simpler solution:</p>

<p><code>inds = pd.isnull(df).any(1).nonzero()[0]</code></p>

<pre><code>In [9]: df
Out[9]: 
          0         1
0  0.450319  0.062595
1 -0.673058  0.156073
2 -0.871179 -0.118575
3  0.594188       NaN
4 -1.017903 -0.484744
5  0.860375  0.239265
6 -0.640070       NaN
7 -0.535802  1.632932
8  0.876523 -0.153634
9 -0.686914  0.131185

In [10]: pd.isnull(df).any(1).nonzero()[0]
Out[10]: array([3, 6])
</code></pre>
";;['In [9]: df\nOut[9]: \n          0         1\n0  0.450319  0.062595\n1 -0.673058  0.156073\n2 -0.871179 -0.118575\n3  0.594188       NaN\n4 -1.017903 -0.484744\n5  0.860375  0.239265\n6 -0.640070       NaN\n7 -0.535802  1.632932\n8  0.876523 -0.153634\n9 -0.686914  0.131185\n\nIn [10]: pd.isnull(df).any(1).nonzero()[0]\nOut[10]: array([3, 6])\n'];['inds = pd.isnull(df).any(1).nonzero()[0]', 'In [9]: df\nOut[9]: \n          0         1\n0  0.450319  0.062595\n1 -0.673058  0.156073\n2 -0.871179 -0.118575\n3  0.594188       NaN\n4 -1.017903 -0.484744\n5  0.860375  0.239265\n6 -0.640070       NaN\n7 -0.535802  1.632932\n8  0.876523 -0.153634\n9 -0.686914  0.131185\n\nIn [10]: pd.isnull(df).any(1).nonzero()[0]\nOut[10]: array([3, 6])\n']
670;;3;14058892;14057007.0;2;39;;;"<p>You can use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.logical_not.html#numpy.logical_not"" rel=""noreferrer""><code>numpy.logical_not</code></a> to invert the boolean array returned by <code>isin</code>:</p>

<pre><code>In [63]: s = pd.Series(np.arange(10.0))

In [64]: x = range(4, 8)

In [65]: mask = np.logical_not(s.isin(x))

In [66]: s[mask]
Out[66]: 
0    0
1    1
2    2
3    3
8    8
9    9
</code></pre>

<p>As given in the comment by Wes McKinney you can also use</p>

<pre><code>s[-s.isin(x)]
</code></pre>
";;['In [63]: s = pd.Series(np.arange(10.0))\n\nIn [64]: x = range(4, 8)\n\nIn [65]: mask = np.logical_not(s.isin(x))\n\nIn [66]: s[mask]\nOut[66]: \n0    0\n1    1\n2    2\n3    3\n8    8\n9    9\n', 's[-s.isin(x)]\n'];['numpy.logical_not', 'isin', 'In [63]: s = pd.Series(np.arange(10.0))\n\nIn [64]: x = range(4, 8)\n\nIn [65]: mask = np.logical_not(s.isin(x))\n\nIn [66]: s[mask]\nOut[66]: \n0    0\n1    1\n2    2\n3    3\n8    8\n9    9\n', 's[-s.isin(x)]\n']
671;;1;14059783;14059094.0;2;15;;;"<p>You can use the DataFrame <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.apply.html#pandas.DataFrame.apply"" rel=""noreferrer""><code>apply</code></a> method:</p>

<pre><code>order_df['Value'] = order_df.apply(lambda row: (row['Prices']*row['Amount']
                                               if row['Action']=='Sell'
                                               else -row['Prices']*row['Amount']),
                                   axis=1)
</code></pre>

<p><em>It is usually faster to use these methods rather than over for loops.</em></p>
";;"[""order_df['Value'] = order_df.apply(lambda row: (row['Prices']*row['Amount']\n                                               if row['Action']=='Sell'\n                                               else -row['Prices']*row['Amount']),\n                                   axis=1)\n""]";"['apply', ""order_df['Value'] = order_df.apply(lambda row: (row['Prices']*row['Amount']\n                                               if row['Action']=='Sell'\n                                               else -row['Prices']*row['Amount']),\n                                   axis=1)\n""]"
672;;0;14060625;14059094.0;2;7;;;"<p>If we're willing to sacrifice the succinctness of Hayden's solution, one could also do something like this:  </p>

<pre><code>In [22]: orders_df['C'] = orders_df.Action.apply(
               lambda x: (1 if x == 'Sell' else -1))

In [23]: orders_df   # New column C represents the sign of the transaction
Out[23]:
   Prices  Amount Action  C
0       3      57   Sell  1
1      89      42   Sell  1
2      45      70    Buy -1
3       6      43   Sell  1
4      60      47   Sell  1
5      19      16    Buy -1
6      56      89   Sell  1
7       3      28    Buy -1
8      56      69   Sell  1
9      90      49    Buy -1
</code></pre>

<p>Now we have eliminated the need for the <code>if</code> statement. Using <code>DataFrame.apply()</code>, we also do away with the <code>for</code> loop. As Hayden noted, vectorized operations are always faster. </p>

<pre><code>In [24]: orders_df['Value'] = orders_df.Prices * orders_df.Amount * orders_df.C

In [25]: orders_df   # The resulting dataframe
Out[25]:
   Prices  Amount Action  C  Value
0       3      57   Sell  1    171
1      89      42   Sell  1   3738
2      45      70    Buy -1  -3150
3       6      43   Sell  1    258
4      60      47   Sell  1   2820
5      19      16    Buy -1   -304
6      56      89   Sell  1   4984
7       3      28    Buy -1    -84
8      56      69   Sell  1   3864
9      90      49    Buy -1  -4410
</code></pre>

<p>This solution takes two lines of code instead of one, but is a bit easier to read. I suspect that the computational costs are similar as well. </p>
";;"[""In [22]: orders_df['C'] = orders_df.Action.apply(\n               lambda x: (1 if x == 'Sell' else -1))\n\nIn [23]: orders_df   # New column C represents the sign of the transaction\nOut[23]:\n   Prices  Amount Action  C\n0       3      57   Sell  1\n1      89      42   Sell  1\n2      45      70    Buy -1\n3       6      43   Sell  1\n4      60      47   Sell  1\n5      19      16    Buy -1\n6      56      89   Sell  1\n7       3      28    Buy -1\n8      56      69   Sell  1\n9      90      49    Buy -1\n"", ""In [24]: orders_df['Value'] = orders_df.Prices * orders_df.Amount * orders_df.C\n\nIn [25]: orders_df   # The resulting dataframe\nOut[25]:\n   Prices  Amount Action  C  Value\n0       3      57   Sell  1    171\n1      89      42   Sell  1   3738\n2      45      70    Buy -1  -3150\n3       6      43   Sell  1    258\n4      60      47   Sell  1   2820\n5      19      16    Buy -1   -304\n6      56      89   Sell  1   4984\n7       3      28    Buy -1    -84\n8      56      69   Sell  1   3864\n9      90      49    Buy -1  -4410\n""]";"[""In [22]: orders_df['C'] = orders_df.Action.apply(\n               lambda x: (1 if x == 'Sell' else -1))\n\nIn [23]: orders_df   # New column C represents the sign of the transaction\nOut[23]:\n   Prices  Amount Action  C\n0       3      57   Sell  1\n1      89      42   Sell  1\n2      45      70    Buy -1\n3       6      43   Sell  1\n4      60      47   Sell  1\n5      19      16    Buy -1\n6      56      89   Sell  1\n7       3      28    Buy -1\n8      56      69   Sell  1\n9      90      49    Buy -1\n"", 'if', 'DataFrame.apply()', 'for', ""In [24]: orders_df['Value'] = orders_df.Prices * orders_df.Amount * orders_df.C\n\nIn [25]: orders_df   # The resulting dataframe\nOut[25]:\n   Prices  Amount Action  C  Value\n0       3      57   Sell  1    171\n1      89      42   Sell  1   3738\n2      45      70    Buy -1  -3150\n3       6      43   Sell  1    258\n4      60      47   Sell  1   2820\n5      19      16    Buy -1   -304\n6      56      89   Sell  1   4984\n7       3      28    Buy -1    -84\n8      56      69   Sell  1   3864\n9      90      49    Buy -1  -4410\n""]"
673;;4;14071265;14059094.0;2;50;;;"<p>I think an elegant solution is to use the <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#the-where-method-and-masking""><code>where</code></a> method (also see the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.where.html""><code>API docs</code></a>):</p>

<pre><code>In [37]: values = df.Prices * df.Amount

In [38]: df['Values'] = values.where(df.Action == 'Sell', other=-values)

In [39]: df
Out[39]: 
   Prices  Amount Action  Values
0       3      57   Sell     171
1      89      42   Sell    3738
2      45      70    Buy   -3150
3       6      43   Sell     258
4      60      47   Sell    2820
5      19      16    Buy    -304
6      56      89   Sell    4984
7       3      28    Buy     -84
8      56      69   Sell    3864
9      90      49    Buy   -4410
</code></pre>

<p>Further more this should be the fastest solution.</p>
";;"[""In [37]: values = df.Prices * df.Amount\n\nIn [38]: df['Values'] = values.where(df.Action == 'Sell', other=-values)\n\nIn [39]: df\nOut[39]: \n   Prices  Amount Action  Values\n0       3      57   Sell     171\n1      89      42   Sell    3738\n2      45      70    Buy   -3150\n3       6      43   Sell     258\n4      60      47   Sell    2820\n5      19      16    Buy    -304\n6      56      89   Sell    4984\n7       3      28    Buy     -84\n8      56      69   Sell    3864\n9      90      49    Buy   -4410\n""]";"['where', 'API docs', ""In [37]: values = df.Prices * df.Amount\n\nIn [38]: df['Values'] = values.where(df.Action == 'Sell', other=-values)\n\nIn [39]: df\nOut[39]: \n   Prices  Amount Action  Values\n0       3      57   Sell     171\n1      89      42   Sell    3738\n2      45      70    Buy   -3150\n3       6      43   Sell     258\n4      60      47   Sell    2820\n5      19      16    Buy    -304\n6      56      89   Sell    4984\n7       3      28    Buy     -84\n8      56      69   Sell    3864\n9      90      49    Buy   -4410\n""]"
674;;1;14163174;14162723.0;2;8;;;"<p>You can replace <code>nan</code> with <code>None</code> in your numpy array:</p>

<pre><code>&gt;&gt;&gt; x = np.array([1, np.nan, 3])
&gt;&gt;&gt; y = np.where(np.isnan(x), None, x)
&gt;&gt;&gt; print y
[1.0 None 3.0]
&gt;&gt;&gt; print type(y[1])
&lt;type 'NoneType'&gt;
</code></pre>
";;"["">>> x = np.array([1, np.nan, 3])\n>>> y = np.where(np.isnan(x), None, x)\n>>> print y\n[1.0 None 3.0]\n>>> print type(y[1])\n<type 'NoneType'>\n""]";"['nan', 'None', "">>> x = np.array([1, np.nan, 3])\n>>> y = np.where(np.isnan(x), None, x)\n>>> print y\n[1.0 None 3.0]\n>>> print type(y[1])\n<type 'NoneType'>\n""]"
675;;5;14163209;14162723.0;2;56;;;"<p>@bogatron has it right, you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#where-and-masking"" rel=""noreferrer""><code>where</code></a>, it's worth noting that you can do this natively in pandas:</p>

<pre><code>df1 = df.where((pd.notnull(df)), None)
</code></pre>

<p>Note: this changes the dtype of <strong>all columns</strong> to <code>object</code>.</p>

<p>Example:</p>

<pre><code>In [1]: df = pd.DataFrame([1, np.nan])

In [2]: df
Out[2]: 
    0
0   1
1 NaN

In [3]: df1 = df.where((pd.notnull(df)), None)

In [4]: df1
Out[4]: 
      0
0     1
1  None
</code></pre>

<hr>

<p>Note: what you cannot do recast the DataFrames <code>dtype</code> to allow all datatypes types, using <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.astype.html#pandas.DataFrame.astype"" rel=""noreferrer""><code>astype</code></a>, and then the DataFrame <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.fillna.html"" rel=""noreferrer""><code>fillna</code></a> method:</p>

<pre><code>df1 = df.astype(object).replace(np.nan, 'None')
</code></pre>

<p><em>Unfortunately neither this, nor using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html#pandas.DataFrame.replace"" rel=""noreferrer""><code>replace</code></a>, works with <code>None</code> see <a href=""https://github.com/pydata/pandas/issues/1972"" rel=""noreferrer"">this (closed) issue</a>.</em></p>

<hr>

<p>As an aside, it's worth noting that for most use cases you don't need to replace NaN with None, see this question about <a href=""https://stackoverflow.com/q/17534106/1240268""><strong>the difference between NaN and None in pandas</strong></a>.</p>

<p>However, in this specific case it seems you do (at least at the time of this answer).</p>
";;"['df1 = df.where((pd.notnull(df)), None)\n', 'In [1]: df = pd.DataFrame([1, np.nan])\n\nIn [2]: df\nOut[2]: \n    0\n0   1\n1 NaN\n\nIn [3]: df1 = df.where((pd.notnull(df)), None)\n\nIn [4]: df1\nOut[4]: \n      0\n0     1\n1  None\n', ""df1 = df.astype(object).replace(np.nan, 'None')\n""]";"['where', 'df1 = df.where((pd.notnull(df)), None)\n', 'object', 'In [1]: df = pd.DataFrame([1, np.nan])\n\nIn [2]: df\nOut[2]: \n    0\n0   1\n1 NaN\n\nIn [3]: df1 = df.where((pd.notnull(df)), None)\n\nIn [4]: df1\nOut[4]: \n      0\n0     1\n1  None\n', 'dtype', 'astype', 'fillna', ""df1 = df.astype(object).replace(np.nan, 'None')\n"", 'replace', 'None']"
676;;5;14225838;14225676.0;2;67;;;"<p>You should be using pandas own <code>ExcelWriter</code> class:</p>

<pre><code>from pandas import ExcelWriter
# from pandas.io.parsers import ExcelWriter
</code></pre>

<p>Then the <code>save_xls</code> function works as expected:</p>

<pre><code>def save_xls(list_dfs, xls_path):
    writer = ExcelWriter(xls_path)
    for n, df in enumerate(list_dfs):
        df.to_excel(writer,'sheet%s' % n)
    writer.save()
</code></pre>
";;"['from pandas import ExcelWriter\n# from pandas.io.parsers import ExcelWriter\n', ""def save_xls(list_dfs, xls_path):\n    writer = ExcelWriter(xls_path)\n    for n, df in enumerate(list_dfs):\n        df.to_excel(writer,'sheet%s' % n)\n    writer.save()\n""]";"['ExcelWriter', 'from pandas import ExcelWriter\n# from pandas.io.parsers import ExcelWriter\n', 'save_xls', ""def save_xls(list_dfs, xls_path):\n    writer = ExcelWriter(xls_path)\n    for n, df in enumerate(list_dfs):\n        df.to_excel(writer,'sheet%s' % n)\n    writer.save()\n""]"
677;;0;14247708;14247586.0;2;119;;;"<p>[Updated to adapt to modern <code>pandas</code>, which has <code>isnull</code> as a method of <code>DataFrame</code>s..]</p>

<p>You can use <code>isnull</code> and <code>any</code> to build a boolean Series and use that to index into your frame:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([range(3), [0, np.NaN, 0], [0, 0, np.NaN], range(3), range(3)])
&gt;&gt;&gt; df.isnull()
       0      1      2
0  False  False  False
1  False   True  False
2  False  False   True
3  False  False  False
4  False  False  False
&gt;&gt;&gt; df.isnull().any(axis=1)
0    False
1     True
2     True
3    False
4    False
dtype: bool
&gt;&gt;&gt; df[df.isnull().any(axis=1)]
   0   1   2
1  0 NaN   0
2  0   0 NaN
</code></pre>

<hr>

<p>[For older <code>pandas</code>:]</p>

<p>You could use the function <code>isnull</code> instead of the method:</p>

<pre><code>In [56]: df = pd.DataFrame([range(3), [0, np.NaN, 0], [0, 0, np.NaN], range(3), range(3)])

In [57]: df
Out[57]: 
   0   1   2
0  0   1   2
1  0 NaN   0
2  0   0 NaN
3  0   1   2
4  0   1   2

In [58]: pd.isnull(df)
Out[58]: 
       0      1      2
0  False  False  False
1  False   True  False
2  False  False   True
3  False  False  False
4  False  False  False

In [59]: pd.isnull(df).any(axis=1)
Out[59]: 
0    False
1     True
2     True
3    False
4    False
</code></pre>

<p>leading to the rather compact:</p>

<pre><code>In [60]: df[pd.isnull(df).any(axis=1)]
Out[60]: 
   0   1   2
1  0 NaN   0
2  0   0 NaN
</code></pre>
";;['>>> df = pd.DataFrame([range(3), [0, np.NaN, 0], [0, 0, np.NaN], range(3), range(3)])\n>>> df.isnull()\n       0      1      2\n0  False  False  False\n1  False   True  False\n2  False  False   True\n3  False  False  False\n4  False  False  False\n>>> df.isnull().any(axis=1)\n0    False\n1     True\n2     True\n3    False\n4    False\ndtype: bool\n>>> df[df.isnull().any(axis=1)]\n   0   1   2\n1  0 NaN   0\n2  0   0 NaN\n', 'In [56]: df = pd.DataFrame([range(3), [0, np.NaN, 0], [0, 0, np.NaN], range(3), range(3)])\n\nIn [57]: df\nOut[57]: \n   0   1   2\n0  0   1   2\n1  0 NaN   0\n2  0   0 NaN\n3  0   1   2\n4  0   1   2\n\nIn [58]: pd.isnull(df)\nOut[58]: \n       0      1      2\n0  False  False  False\n1  False   True  False\n2  False  False   True\n3  False  False  False\n4  False  False  False\n\nIn [59]: pd.isnull(df).any(axis=1)\nOut[59]: \n0    False\n1     True\n2     True\n3    False\n4    False\n', 'In [60]: df[pd.isnull(df).any(axis=1)]\nOut[60]: \n   0   1   2\n1  0 NaN   0\n2  0   0 NaN\n'];['pandas', 'isnull', 'DataFrame', 'isnull', 'any', '>>> df = pd.DataFrame([range(3), [0, np.NaN, 0], [0, 0, np.NaN], range(3), range(3)])\n>>> df.isnull()\n       0      1      2\n0  False  False  False\n1  False   True  False\n2  False  False   True\n3  False  False  False\n4  False  False  False\n>>> df.isnull().any(axis=1)\n0    False\n1     True\n2     True\n3    False\n4    False\ndtype: bool\n>>> df[df.isnull().any(axis=1)]\n   0   1   2\n1  0 NaN   0\n2  0   0 NaN\n', 'pandas', 'isnull', 'In [56]: df = pd.DataFrame([range(3), [0, np.NaN, 0], [0, 0, np.NaN], range(3), range(3)])\n\nIn [57]: df\nOut[57]: \n   0   1   2\n0  0   1   2\n1  0 NaN   0\n2  0   0 NaN\n3  0   1   2\n4  0   1   2\n\nIn [58]: pd.isnull(df)\nOut[58]: \n       0      1      2\n0  False  False  False\n1  False   True  False\n2  False  False   True\n3  False  False  False\n4  False  False  False\n\nIn [59]: pd.isnull(df).any(axis=1)\nOut[59]: \n0    False\n1     True\n2     True\n3    False\n4    False\n', 'In [60]: df[pd.isnull(df).any(axis=1)]\nOut[60]: \n   0   1   2\n1  0 NaN   0\n2  0   0 NaN\n']
678;;19;14268804;14262433.0;2;364;;;"<p>I routinely use tens of gigabytes of data in just this fashion
e.g. I have tables on disk that I read via queries, create data and append back.</p>

<p>It's worth reading <a href=""http://pandas-docs.github.io/pandas-docs-travis/io.html#hdf5-pytables"">the docs</a> and <a href=""https://groups.google.com/forum/m/?fromgroups#!topic/pydata/cmw1F3OFJSc"">late in this thread</a> for several suggestions for how to store your data.</p>

<p>Details which will affect how you store your data, like:<br>
<em>Give as much detail as you can; and I can help you develop a structure.</em></p>

<ol>
<li>Size of data, # of rows, columns, types of columns; are you appending
rows, or just columns? </li>
<li>What will typical operations look like. E.g. do a query on columns to select a bunch of rows and specific columns, then do an operation (in-memory), create new columns, save these.<br>
(Giving a toy example could enable us to offer more specific recommendations.)</li>
<li>After that processing, then what do you do? Is step 2 ad hoc, or repeatable?</li>
<li>Input flat files: how many, rough total size in Gb. How are these organized e.g. by records? Does each one contains different fields, or do they have some records per file with all of the fields in each file?</li>
<li>Do you ever select subsets of rows (records) based on criteria (e.g. select the rows with field A > 5)? and then do something, or do you just select fields A, B, C with all of the records (and then do something)?</li>
<li>Do you 'work on' all of your columns (in groups), or are there a good proportion that you may only use for reports (e.g. you want to keep the data around, but don't need to pull in that column explicity until final results time)?</li>
</ol>

<h2>Solution</h2>

<p><em>Ensure you have <a href=""http://pandas.pydata.org/getpandas.html"">pandas at least <code>0.10.1</code></a> installed.</em></p>

<p>Read <a href=""http://pandas-docs.github.io/pandas-docs-travis/io.html#iterating-through-files-chunk-by-chunk"">iterating files chunk-by-chunk</a> and <a href=""http://pandas-docs.github.io/pandas-docs-travis/io.html#multiple-table-queries"">multiple table queries</a>.</p>

<p>Since pytables is optimized to operate on row-wise (which is what you query on), we will create a table for each group of fields. This way it's easy to select a small group of fields (which will work with a big table, but it's more efficient to do it this way... I think I may be able to fix this limitation in the future... this is more intuitive anyhow):<br>
(The following is pseudocode.)</p>

<pre><code>import numpy as np
import pandas as pd

# create a store
store = pd.HDFStore('mystore.h5')

# this is the key to your storage:
#    this maps your fields to a specific group, and defines 
#    what you want to have as data_columns.
#    you might want to create a nice class wrapping this
#    (as you will want to have this map and its inversion)  
group_map = dict(
    A = dict(fields = ['field_1','field_2',.....], dc = ['field_1',....,'field_5']),
    B = dict(fields = ['field_10',......        ], dc = ['field_10']),
    .....
    REPORTING_ONLY = dict(fields = ['field_1000','field_1001',...], dc = []),

)

group_map_inverted = dict()
for g, v in group_map.items():
    group_map_inverted.update(dict([ (f,g) for f in v['fields'] ]))
</code></pre>

<p>Reading in the files and creating the storage (essentially doing what <code>append_to_multiple</code> does):</p>

<pre><code>for f in files:
   # read in the file, additional options hmay be necessary here
   # the chunksize is not strictly necessary, you may be able to slurp each 
   # file into memory in which case just eliminate this part of the loop 
   # (you can also change chunksize if necessary)
   for chunk in pd.read_table(f, chunksize=50000):
       # we are going to append to each table by group
       # we are not going to create indexes at this time
       # but we *ARE* going to create (some) data_columns

       # figure out the field groupings
       for g, v in group_map.items():
             # create the frame for this group
             frame = chunk.reindex(columns = v['fields'], copy = False)    

             # append it
             store.append(g, frame, index=False, data_columns = v['dc'])
</code></pre>

<p>Now you have all of the tables in the file (actually you could store them in separate files if you wish, you would prob have to add the filename to the group_map, but probably this isn't necessary).</p>

<p>This is how you get columns and create new ones:</p>

<pre><code>frame = store.select(group_that_I_want)
# you can optionally specify:
# columns = a list of the columns IN THAT GROUP (if you wanted to
#     select only say 3 out of the 20 columns in this sub-table)
# and a where clause if you want a subset of the rows

# do calculations on this frame
new_frame = cool_function_on_frame(frame)

# to 'add columns', create a new group (you probably want to
# limit the columns in this new_group to be only NEW ones
# (e.g. so you don't overlap from the other tables)
# add this info to the group_map
store.append(new_group, new_frame.reindex(columns = new_columns_created, copy = False), data_columns = new_columns_created)
</code></pre>

<p>When you are ready for post_processing:</p>

<pre><code># This may be a bit tricky; and depends what you are actually doing.
# I may need to modify this function to be a bit more general:
report_data = store.select_as_multiple([groups_1,groups_2,.....], where =['field_1&gt;0', 'field_1000=foo'], selector = group_1)
</code></pre>

<p>About data_columns, you don't actually need to define <strong>ANY</strong> data_columns; they allow you to sub-select rows based on the column. E.g. something like:</p>

<pre><code>store.select(group, where = ['field_1000=foo', 'field_1001&gt;0'])
</code></pre>

<p>They may be most interesting to you in the final report generation stage (essentially a data column is segregated from other columns, which might impact efficiency somewhat if you define a lot).</p>

<p>You also might want to:</p>

<ul>
<li>create a function which takes a list of fields, looks up the groups in the groups_map, then selects these and concatenates the results so you get the resulting frame (this is essentially what select_as_multiple does). <em>This way the structure would be pretty transparent to you.</em></li>
<li>indexes on certain data columns (makes row-subsetting much faster).</li>
<li>enable compression.</li>
</ul>

<p>Let me know when you have questions!</p>
";;"[""import numpy as np\nimport pandas as pd\n\n# create a store\nstore = pd.HDFStore('mystore.h5')\n\n# this is the key to your storage:\n#    this maps your fields to a specific group, and defines \n#    what you want to have as data_columns.\n#    you might want to create a nice class wrapping this\n#    (as you will want to have this map and its inversion)  \ngroup_map = dict(\n    A = dict(fields = ['field_1','field_2',.....], dc = ['field_1',....,'field_5']),\n    B = dict(fields = ['field_10',......        ], dc = ['field_10']),\n    .....\n    REPORTING_ONLY = dict(fields = ['field_1000','field_1001',...], dc = []),\n\n)\n\ngroup_map_inverted = dict()\nfor g, v in group_map.items():\n    group_map_inverted.update(dict([ (f,g) for f in v['fields'] ]))\n"", ""for f in files:\n   # read in the file, additional options hmay be necessary here\n   # the chunksize is not strictly necessary, you may be able to slurp each \n   # file into memory in which case just eliminate this part of the loop \n   # (you can also change chunksize if necessary)\n   for chunk in pd.read_table(f, chunksize=50000):\n       # we are going to append to each table by group\n       # we are not going to create indexes at this time\n       # but we *ARE* going to create (some) data_columns\n\n       # figure out the field groupings\n       for g, v in group_map.items():\n             # create the frame for this group\n             frame = chunk.reindex(columns = v['fields'], copy = False)    \n\n             # append it\n             store.append(g, frame, index=False, data_columns = v['dc'])\n"", ""frame = store.select(group_that_I_want)\n# you can optionally specify:\n# columns = a list of the columns IN THAT GROUP (if you wanted to\n#     select only say 3 out of the 20 columns in this sub-table)\n# and a where clause if you want a subset of the rows\n\n# do calculations on this frame\nnew_frame = cool_function_on_frame(frame)\n\n# to 'add columns', create a new group (you probably want to\n# limit the columns in this new_group to be only NEW ones\n# (e.g. so you don't overlap from the other tables)\n# add this info to the group_map\nstore.append(new_group, new_frame.reindex(columns = new_columns_created, copy = False), data_columns = new_columns_created)\n"", ""# This may be a bit tricky; and depends what you are actually doing.\n# I may need to modify this function to be a bit more general:\nreport_data = store.select_as_multiple([groups_1,groups_2,.....], where =['field_1>0', 'field_1000=foo'], selector = group_1)\n"", ""store.select(group, where = ['field_1000=foo', 'field_1001>0'])\n""]";"['0.10.1', ""import numpy as np\nimport pandas as pd\n\n# create a store\nstore = pd.HDFStore('mystore.h5')\n\n# this is the key to your storage:\n#    this maps your fields to a specific group, and defines \n#    what you want to have as data_columns.\n#    you might want to create a nice class wrapping this\n#    (as you will want to have this map and its inversion)  \ngroup_map = dict(\n    A = dict(fields = ['field_1','field_2',.....], dc = ['field_1',....,'field_5']),\n    B = dict(fields = ['field_10',......        ], dc = ['field_10']),\n    .....\n    REPORTING_ONLY = dict(fields = ['field_1000','field_1001',...], dc = []),\n\n)\n\ngroup_map_inverted = dict()\nfor g, v in group_map.items():\n    group_map_inverted.update(dict([ (f,g) for f in v['fields'] ]))\n"", 'append_to_multiple', ""for f in files:\n   # read in the file, additional options hmay be necessary here\n   # the chunksize is not strictly necessary, you may be able to slurp each \n   # file into memory in which case just eliminate this part of the loop \n   # (you can also change chunksize if necessary)\n   for chunk in pd.read_table(f, chunksize=50000):\n       # we are going to append to each table by group\n       # we are not going to create indexes at this time\n       # but we *ARE* going to create (some) data_columns\n\n       # figure out the field groupings\n       for g, v in group_map.items():\n             # create the frame for this group\n             frame = chunk.reindex(columns = v['fields'], copy = False)    \n\n             # append it\n             store.append(g, frame, index=False, data_columns = v['dc'])\n"", ""frame = store.select(group_that_I_want)\n# you can optionally specify:\n# columns = a list of the columns IN THAT GROUP (if you wanted to\n#     select only say 3 out of the 20 columns in this sub-table)\n# and a where clause if you want a subset of the rows\n\n# do calculations on this frame\nnew_frame = cool_function_on_frame(frame)\n\n# to 'add columns', create a new group (you probably want to\n# limit the columns in this new_group to be only NEW ones\n# (e.g. so you don't overlap from the other tables)\n# add this info to the group_map\nstore.append(new_group, new_frame.reindex(columns = new_columns_created, copy = False), data_columns = new_columns_created)\n"", ""# This may be a bit tricky; and depends what you are actually doing.\n# I may need to modify this function to be a bit more general:\nreport_data = store.select_as_multiple([groups_1,groups_2,.....], where =['field_1>0', 'field_1000=foo'], selector = group_1)\n"", ""store.select(group, where = ['field_1000=foo', 'field_1001>0'])\n""]"
679;;2;14287518;14262433.0;2;41;;;"<p>This is the case for pymongo.  I have also prototyped using sql server, sqlite, HDF, ORM (SQLAlchemy) in python.  First and foremost pymongo is a document based DB, so each person would be a document (<code>dict</code> of attributes).  Many people form a collection and you can have many collections (people, stock market, income).</p>

<p>pd.dateframe -> pymongo Note: I use the <code>chunksize</code> in <code>read_csv</code> to keep it to 5 to 10k records(pymongo drops the socket if larger)</p>

<pre><code>aCollection.insert((a[1].to_dict() for a in df.iterrows()))
</code></pre>

<p>querying: gt = greater than...</p>

<pre><code>pd.DataFrame(list(mongoCollection.find({'anAttribute':{'$gt':2887000, '$lt':2889000}})))
</code></pre>

<p><code>.find()</code> returns an iterator so I commonly use <code>ichunked</code> to chop into smaller iterators.  </p>

<p>How about a join since I normally get 10 data sources to paste together:</p>

<pre><code>aJoinDF = pandas.DataFrame(list(mongoCollection.find({'anAttribute':{'$in':Att_Keys}})))
</code></pre>

<p>then (in my case sometimes I have to agg on <code>aJoinDF</code> first before its ""mergeable"".)</p>

<pre><code>df = pandas.merge(df, aJoinDF, on=aKey, how='left')
</code></pre>

<p>And you can then write the new info to your main collection via the update method below. (logical collection vs physical datasources).</p>

<pre><code>collection.update({primarykey:foo},{key:change})
</code></pre>

<p>On smaller lookups, just denormalize.  For example, you have code in the document and you just add the field code text and do a <code>dict</code> lookup as you create documents.</p>

<p>Now you have a nice dataset based around a person, you can unleash your logic on each case and make more attributes. Finally you can read into pandas your 3 to memory max key indicators and do pivots/agg/data exploration.  This works for me for 3 million records with numbers/big text/categories/codes/floats/...</p>

<p>You can also use the two methods built into MongoDB (MapReduce and aggregate framework). <a href=""http://docs.mongodb.org/manual/tutorial/aggregation-examples/"">See here for more info about the aggregate framework</a>, as it seems to be easier than MapReduce and looks handy for quick aggregate work.  Notice I didn't need to define my fields or relations, and I can add items to a document.  At the current state of the rapidly changing numpy, pandas, python toolset, MongoDB helps me just get to work :)</p>
";;"['aCollection.insert((a[1].to_dict() for a in df.iterrows()))\n', ""pd.DataFrame(list(mongoCollection.find({'anAttribute':{'$gt':2887000, '$lt':2889000}})))\n"", ""aJoinDF = pandas.DataFrame(list(mongoCollection.find({'anAttribute':{'$in':Att_Keys}})))\n"", ""df = pandas.merge(df, aJoinDF, on=aKey, how='left')\n"", 'collection.update({primarykey:foo},{key:change})\n']";"['dict', 'chunksize', 'read_csv', 'aCollection.insert((a[1].to_dict() for a in df.iterrows()))\n', ""pd.DataFrame(list(mongoCollection.find({'anAttribute':{'$gt':2887000, '$lt':2889000}})))\n"", '.find()', 'ichunked', ""aJoinDF = pandas.DataFrame(list(mongoCollection.find({'anAttribute':{'$in':Att_Keys}})))\n"", 'aJoinDF', ""df = pandas.merge(df, aJoinDF, on=aKey, how='left')\n"", 'collection.update({primarykey:foo},{key:change})\n', 'dict']"
680;;6;14306902;14300137.0;2;76;;;"<p>Try passing columns of the <code>DataFrame</code> directly to matplotlib, as in the examples below, instead of extracting them as numpy arrays.</p>

<pre><code>df = pd.DataFrame(np.random.randn(10,2), columns=['col1','col2'])
df['col3'] = np.arange(len(df))**2 * 100 + 100

In [5]: df
Out[5]: 
       col1      col2  col3
0 -1.000075 -0.759910   100
1  0.510382  0.972615   200
2  1.872067 -0.731010   500
3  0.131612  1.075142  1000
4  1.497820  0.237024  1700
</code></pre>

<h3>Vary scatter point size based on another column</h3>

<pre><code>plt.scatter(df.col1, df.col2, s=df.col3)
# OR (with pandas 0.13 and up)
df.plot(kind='scatter', x='col1', y='col2', s=df.col3)
</code></pre>

<p><img src=""https://i.stack.imgur.com/FA5KP.png"" alt=""enter image description here""></p>

<h3>Vary scatter point color based on another column</h3>

<pre><code>colors = np.where(df.col3 &gt; 300, 'r', 'k')
plt.scatter(df.col1, df.col2, s=120, c=colors)
# OR (with pandas 0.13 and up)
df.plot(kind='scatter', x='col1', y='col2', s=120, c=colors)
</code></pre>

<p><img src=""https://i.stack.imgur.com/rghSv.png"" alt=""enter image description here""></p>

<h3>Scatter plot with legend</h3>

<p>However, the easiest way I've found to create a scatter plot with legend is to call <code>plt.scatter</code> once for each point type.</p>

<pre><code>cond = df.col3 &gt; 300
subset_a = df[cond].dropna()
subset_b = df[~cond].dropna()
plt.scatter(subset_a.col1, subset_a.col2, s=120, c='b', label='col3 &gt; 300')
plt.scatter(subset_b.col1, subset_b.col2, s=60, c='r', label='col3 &lt;= 300') 
plt.legend()
</code></pre>

<p><img src=""https://i.stack.imgur.com/tlibK.png"" alt=""enter image description here""></p>

<h3><em>Update</em></h3>

<p>From what I can tell, matplotlib simply skips points with NA x/y coordinates or NA style settings (e.g., color/size).  To find points skipped due to NA, try the <code>isnull</code> method: <code>df[df.col3.isnull()]</code></p>

<p>To split a list of points into many types, take a look at <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.select.html"" rel=""noreferrer"">numpy <code>select</code></a>, which is a vectorized if-then-else implementation and accepts an optional default value.  For example:</p>

<pre><code>df['subset'] = np.select([df.col3 &lt; 150, df.col3 &lt; 400, df.col3 &lt; 600],
                         [0, 1, 2], -1)
for color, label in zip('bgrm', [0, 1, 2, -1]):
    subset = df[df.subset == label]
    plt.scatter(subset.col1, subset.col2, s=120, c=color, label=str(label))
plt.legend()
</code></pre>

<p><img src=""https://i.stack.imgur.com/CtA9s.png"" alt=""enter image description here""></p>
";;"[""df = pd.DataFrame(np.random.randn(10,2), columns=['col1','col2'])\ndf['col3'] = np.arange(len(df))**2 * 100 + 100\n\nIn [5]: df\nOut[5]: \n       col1      col2  col3\n0 -1.000075 -0.759910   100\n1  0.510382  0.972615   200\n2  1.872067 -0.731010   500\n3  0.131612  1.075142  1000\n4  1.497820  0.237024  1700\n"", ""plt.scatter(df.col1, df.col2, s=df.col3)\n# OR (with pandas 0.13 and up)\ndf.plot(kind='scatter', x='col1', y='col2', s=df.col3)\n"", ""colors = np.where(df.col3 > 300, 'r', 'k')\nplt.scatter(df.col1, df.col2, s=120, c=colors)\n# OR (with pandas 0.13 and up)\ndf.plot(kind='scatter', x='col1', y='col2', s=120, c=colors)\n"", ""cond = df.col3 > 300\nsubset_a = df[cond].dropna()\nsubset_b = df[~cond].dropna()\nplt.scatter(subset_a.col1, subset_a.col2, s=120, c='b', label='col3 > 300')\nplt.scatter(subset_b.col1, subset_b.col2, s=60, c='r', label='col3 <= 300') \nplt.legend()\n"", ""df['subset'] = np.select([df.col3 < 150, df.col3 < 400, df.col3 < 600],\n                         [0, 1, 2], -1)\nfor color, label in zip('bgrm', [0, 1, 2, -1]):\n    subset = df[df.subset == label]\n    plt.scatter(subset.col1, subset.col2, s=120, c=color, label=str(label))\nplt.legend()\n""]";"['DataFrame', ""df = pd.DataFrame(np.random.randn(10,2), columns=['col1','col2'])\ndf['col3'] = np.arange(len(df))**2 * 100 + 100\n\nIn [5]: df\nOut[5]: \n       col1      col2  col3\n0 -1.000075 -0.759910   100\n1  0.510382  0.972615   200\n2  1.872067 -0.731010   500\n3  0.131612  1.075142  1000\n4  1.497820  0.237024  1700\n"", ""plt.scatter(df.col1, df.col2, s=df.col3)\n# OR (with pandas 0.13 and up)\ndf.plot(kind='scatter', x='col1', y='col2', s=df.col3)\n"", ""colors = np.where(df.col3 > 300, 'r', 'k')\nplt.scatter(df.col1, df.col2, s=120, c=colors)\n# OR (with pandas 0.13 and up)\ndf.plot(kind='scatter', x='col1', y='col2', s=120, c=colors)\n"", 'plt.scatter', ""cond = df.col3 > 300\nsubset_a = df[cond].dropna()\nsubset_b = df[~cond].dropna()\nplt.scatter(subset_a.col1, subset_a.col2, s=120, c='b', label='col3 > 300')\nplt.scatter(subset_b.col1, subset_b.col2, s=60, c='r', label='col3 <= 300') \nplt.legend()\n"", 'isnull', 'df[df.col3.isnull()]', 'select', ""df['subset'] = np.select([df.col3 < 150, df.col3 < 400, df.col3 < 600],\n                         [0, 1, 2], -1)\nfor color, label in zip('bgrm', [0, 1, 2, -1]):\n    subset = df[df.subset == label]\n    plt.scatter(subset.col1, subset.col2, s=120, c=color, label=str(label))\nplt.legend()\n""]"
681;;2;14349645;14349055.0;2;10;;;"<p>I would check out <a href=""https://github.com/ContinuumIO/Bokeh/"" rel=""nofollow"">Bokeh</a> which aims to ""provide a compelling Python equivalent of ggplot in R"". Example <a href=""http://htmlpreview.github.com/?https://github.com/ContinuumIO/Bokeh/blob/master/tests/cdx/bokeh-facets.html"" rel=""nofollow"">here</a></p>

<p>EDIT: Also check out <a href=""http://stanford.edu/~mwaskom/software/seaborn/"" rel=""nofollow"">Seaborn</a>, which attempts to reproduce the visual style and syntax of ggplot2.</p>
";;[];[]
682;;9;14349766;14349055.0;2;41;;;"<h3>Edit 1 year later:</h3>

<p>With <code>seaborn</code>, the example below becomes:</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
import seaborn
seaborn.set(style='ticks')
# Data to be represented
X = np.random.randn(256)

# Actual plotting
fig = plt.figure(figsize=(8,6), dpi=72, facecolor=""white"")
axes = plt.subplot(111)
heights, positions, patches = axes.hist(X, color='white')
seaborn.despine(ax=axes, offset=10, trim=True)
fig.tight_layout()
plt.show()
</code></pre>

<p>Pretty dang easy.</p>

<h3>Original post:</h3>

<p>This blog post is the best I've seen so far.
<a href=""http://messymind.net/making-matplotlib-look-like-ggplot/"" rel=""noreferrer"">http://messymind.net/making-matplotlib-look-like-ggplot/</a></p>

<p>It doesn't focus on your standard R plots like you see in most of the ""getting started""-type examples. Instead it tries to emulate the style of ggplot2, which seems to be nearly universally heralded as stylish and well-designed.  </p>

<p>To get the axis spines like you see the in bar plot, try to follow one of the first few examples here: <a href=""http://www.loria.fr/~rougier/coding/gallery/"" rel=""noreferrer"">http://www.loria.fr/~rougier/coding/gallery/</a></p>

<p>Lastly, to get the axis tick marks pointing outward, you can edit your <code>matplotlibrc</code> files to say <code>xtick.direction : out</code> and <code>ytick.direction : out</code>.</p>

<p>Combining these concepts together we get something like this:</p>

<pre><code>import numpy as np
import matplotlib
import matplotlib.pyplot as plt
# Data to be represented
X = np.random.randn(256)

# Actual plotting
fig = plt.figure(figsize=(8,6), dpi=72, facecolor=""white"")
axes = plt.subplot(111)
heights, positions, patches = axes.hist(X, color='white')

axes.spines['right'].set_color('none')
axes.spines['top'].set_color('none')
axes.xaxis.set_ticks_position('bottom')

# was: axes.spines['bottom'].set_position(('data',1.1*X.min()))
axes.spines['bottom'].set_position(('axes', -0.05))
axes.yaxis.set_ticks_position('left')
axes.spines['left'].set_position(('axes', -0.05))

axes.set_xlim([np.floor(positions.min()), np.ceil(positions.max())])
axes.set_ylim([0,70])
axes.xaxis.grid(False)
axes.yaxis.grid(False)
fig.tight_layout()
plt.show()
</code></pre>

<p>The position of the spines can be specified a number of ways. If you run the code above in IPython, you can then do <code>axes.spines['bottom'].set_position?</code> to see all of your options.</p>

<p><img src=""https://i.stack.imgur.com/f2zrz.png"" alt=""R-style bar plot in python""></p>

<p>So yeah. It's not exactly trivial, but you can get close.</p>
";;"['import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn\nseaborn.set(style=\'ticks\')\n# Data to be represented\nX = np.random.randn(256)\n\n# Actual plotting\nfig = plt.figure(figsize=(8,6), dpi=72, facecolor=""white"")\naxes = plt.subplot(111)\nheights, positions, patches = axes.hist(X, color=\'white\')\nseaborn.despine(ax=axes, offset=10, trim=True)\nfig.tight_layout()\nplt.show()\n', 'import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n# Data to be represented\nX = np.random.randn(256)\n\n# Actual plotting\nfig = plt.figure(figsize=(8,6), dpi=72, facecolor=""white"")\naxes = plt.subplot(111)\nheights, positions, patches = axes.hist(X, color=\'white\')\n\naxes.spines[\'right\'].set_color(\'none\')\naxes.spines[\'top\'].set_color(\'none\')\naxes.xaxis.set_ticks_position(\'bottom\')\n\n# was: axes.spines[\'bottom\'].set_position((\'data\',1.1*X.min()))\naxes.spines[\'bottom\'].set_position((\'axes\', -0.05))\naxes.yaxis.set_ticks_position(\'left\')\naxes.spines[\'left\'].set_position((\'axes\', -0.05))\n\naxes.set_xlim([np.floor(positions.min()), np.ceil(positions.max())])\naxes.set_ylim([0,70])\naxes.xaxis.grid(False)\naxes.yaxis.grid(False)\nfig.tight_layout()\nplt.show()\n']";"['seaborn', 'import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn\nseaborn.set(style=\'ticks\')\n# Data to be represented\nX = np.random.randn(256)\n\n# Actual plotting\nfig = plt.figure(figsize=(8,6), dpi=72, facecolor=""white"")\naxes = plt.subplot(111)\nheights, positions, patches = axes.hist(X, color=\'white\')\nseaborn.despine(ax=axes, offset=10, trim=True)\nfig.tight_layout()\nplt.show()\n', 'matplotlibrc', 'xtick.direction : out', 'ytick.direction : out', 'import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n# Data to be represented\nX = np.random.randn(256)\n\n# Actual plotting\nfig = plt.figure(figsize=(8,6), dpi=72, facecolor=""white"")\naxes = plt.subplot(111)\nheights, positions, patches = axes.hist(X, color=\'white\')\n\naxes.spines[\'right\'].set_color(\'none\')\naxes.spines[\'top\'].set_color(\'none\')\naxes.xaxis.set_ticks_position(\'bottom\')\n\n# was: axes.spines[\'bottom\'].set_position((\'data\',1.1*X.min()))\naxes.spines[\'bottom\'].set_position((\'axes\', -0.05))\naxes.yaxis.set_ticks_position(\'left\')\naxes.spines[\'left\'].set_position((\'axes\', -0.05))\n\naxes.set_xlim([np.floor(positions.min()), np.ceil(positions.max())])\naxes.set_ylim([0,70])\naxes.xaxis.grid(False)\naxes.yaxis.grid(False)\nfig.tight_layout()\nplt.show()\n', ""axes.spines['bottom'].set_position?""]"
683;;3;14351567;14349055.0;2;27;;;"<p>#
#
#
#
#
#</p>

<p>EDIT 10/14/2013:
For information, ggplot has now been implemented for python (built on matplotlib). </p>

<p>See this <a href=""http://blog.yhathq.com/posts/ggplot-for-python.html"" rel=""nofollow noreferrer"">blog</a> or go directly to the <a href=""https://github.com/yhat/ggplot"" rel=""nofollow noreferrer"">github page</a> of the project for more information and examples.</p>

<p>#
#
#
#
#
#</p>

<p>To my knowledge, there is no built-in solution in matplotlib that will directly give to your figures a similar look than the ones made with R. </p>

<p>Some packages, like <a href=""http://tonysyu.github.com/mpltools/index.html"" rel=""nofollow noreferrer"">mpltools</a>, adds support for stylesheets using Matplotlibs rc-parameters, and can help you to obtain a ggplot look (see the <a href=""http://tonysyu.github.com/mpltools/auto_examples/style/plot_ggplot.html"" rel=""nofollow noreferrer"">ggplot style</a> for an example).</p>

<p>However, since everything can be tweaked in matplotlib, it might be easier for you to directly develop your own functions to achieve exactly what you want.  As an example, below is a snippet that will allow you to easily customize the axes of any matplotlib plot.</p>

<pre><code>def customaxis(ax, c_left='k', c_bottom='k', c_right='none', c_top='none',
               lw=3, size=20, pad=8):

    for c_spine, spine in zip([c_left, c_bottom, c_right, c_top],
                              ['left', 'bottom', 'right', 'top']):
        if c_spine != 'none':
            ax.spines[spine].set_color(c_spine)
            ax.spines[spine].set_linewidth(lw)
        else:
            ax.spines[spine].set_color('none')
    if (c_bottom == 'none') &amp; (c_top == 'none'): # no bottom and no top
        ax.xaxis.set_ticks_position('none')
    elif (c_bottom != 'none') &amp; (c_top != 'none'): # bottom and top
        ax.tick_params(axis='x', direction='out', width=lw, length=7,
                      color=c_bottom, labelsize=size, pad=pad)
    elif (c_bottom != 'none') &amp; (c_top == 'none'): # bottom but not top
        ax.xaxis.set_ticks_position('bottom')
        ax.tick_params(axis='x', direction='out', width=lw, length=7,
                       color=c_bottom, labelsize=size, pad=pad)
    elif (c_bottom == 'none') &amp; (c_top != 'none'): # no bottom but top
        ax.xaxis.set_ticks_position('top')
        ax.tick_params(axis='x', direction='out', width=lw, length=7,
                       color=c_top, labelsize=size, pad=pad)
    if (c_left == 'none') &amp; (c_right == 'none'): # no left and no right
        ax.yaxis.set_ticks_position('none')
    elif (c_left != 'none') &amp; (c_right != 'none'): # left and right
        ax.tick_params(axis='y', direction='out', width=lw, length=7,
                       color=c_left, labelsize=size, pad=pad)
    elif (c_left != 'none') &amp; (c_right == 'none'): # left but not right
        ax.yaxis.set_ticks_position('left')
        ax.tick_params(axis='y', direction='out', width=lw, length=7,
                       color=c_left, labelsize=size, pad=pad)
    elif (c_left == 'none') &amp; (c_right != 'none'): # no left but right
        ax.yaxis.set_ticks_position('right')
        ax.tick_params(axis='y', direction='out', width=lw, length=7,
                       color=c_right, labelsize=size, pad=pad)
</code></pre>

<p><strong>EDIT:</strong> for non touching spines, see the function below which induces a 10 pts displacement of the spines (taken from <a href=""http://matplotlib.org/examples/pylab_examples/spine_placement_demo.html"" rel=""nofollow noreferrer"">this example</a> on the matplotlib website).</p>

<pre><code>def adjust_spines(ax,spines):
    for loc, spine in ax.spines.items():
        if loc in spines:
            spine.set_position(('outward',10)) # outward by 10 points
            spine.set_smart_bounds(True)
        else:
            spine.set_color('none') # don't draw spine
</code></pre>

<p>For example, the code and the two plots below show you the default output from matplotib (on the left), and the output when the functions are called (on the right):</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

fig,(ax1,ax2) = plt.subplots(figsize=(8,5), ncols=2)
ax1.plot(np.random.rand(20), np.random.rand(20), 'ok')
ax2.plot(np.random.rand(20), np.random.rand(20), 'ok')

customaxis(ax2) # remove top and right spines, ticks out
adjust_spines(ax2, ['left', 'bottom']) # non touching spines

plt.show()
</code></pre>

<p><img src=""https://i.imgur.com/X97AT.png"" alt=""image""></p>

<p>Of course, it will take time for you to figure out which parameters have to be tweaked in matplotlib to make your plots look exactly like the R ones, but I am not sure there are other options right now. </p>
";;"[""def customaxis(ax, c_left='k', c_bottom='k', c_right='none', c_top='none',\n               lw=3, size=20, pad=8):\n\n    for c_spine, spine in zip([c_left, c_bottom, c_right, c_top],\n                              ['left', 'bottom', 'right', 'top']):\n        if c_spine != 'none':\n            ax.spines[spine].set_color(c_spine)\n            ax.spines[spine].set_linewidth(lw)\n        else:\n            ax.spines[spine].set_color('none')\n    if (c_bottom == 'none') & (c_top == 'none'): # no bottom and no top\n        ax.xaxis.set_ticks_position('none')\n    elif (c_bottom != 'none') & (c_top != 'none'): # bottom and top\n        ax.tick_params(axis='x', direction='out', width=lw, length=7,\n                      color=c_bottom, labelsize=size, pad=pad)\n    elif (c_bottom != 'none') & (c_top == 'none'): # bottom but not top\n        ax.xaxis.set_ticks_position('bottom')\n        ax.tick_params(axis='x', direction='out', width=lw, length=7,\n                       color=c_bottom, labelsize=size, pad=pad)\n    elif (c_bottom == 'none') & (c_top != 'none'): # no bottom but top\n        ax.xaxis.set_ticks_position('top')\n        ax.tick_params(axis='x', direction='out', width=lw, length=7,\n                       color=c_top, labelsize=size, pad=pad)\n    if (c_left == 'none') & (c_right == 'none'): # no left and no right\n        ax.yaxis.set_ticks_position('none')\n    elif (c_left != 'none') & (c_right != 'none'): # left and right\n        ax.tick_params(axis='y', direction='out', width=lw, length=7,\n                       color=c_left, labelsize=size, pad=pad)\n    elif (c_left != 'none') & (c_right == 'none'): # left but not right\n        ax.yaxis.set_ticks_position('left')\n        ax.tick_params(axis='y', direction='out', width=lw, length=7,\n                       color=c_left, labelsize=size, pad=pad)\n    elif (c_left == 'none') & (c_right != 'none'): # no left but right\n        ax.yaxis.set_ticks_position('right')\n        ax.tick_params(axis='y', direction='out', width=lw, length=7,\n                       color=c_right, labelsize=size, pad=pad)\n"", ""def adjust_spines(ax,spines):\n    for loc, spine in ax.spines.items():\n        if loc in spines:\n            spine.set_position(('outward',10)) # outward by 10 points\n            spine.set_smart_bounds(True)\n        else:\n            spine.set_color('none') # don't draw spine\n"", ""import numpy as np\nimport matplotlib.pyplot as plt\n\nfig,(ax1,ax2) = plt.subplots(figsize=(8,5), ncols=2)\nax1.plot(np.random.rand(20), np.random.rand(20), 'ok')\nax2.plot(np.random.rand(20), np.random.rand(20), 'ok')\n\ncustomaxis(ax2) # remove top and right spines, ticks out\nadjust_spines(ax2, ['left', 'bottom']) # non touching spines\n\nplt.show()\n""]";"[""def customaxis(ax, c_left='k', c_bottom='k', c_right='none', c_top='none',\n               lw=3, size=20, pad=8):\n\n    for c_spine, spine in zip([c_left, c_bottom, c_right, c_top],\n                              ['left', 'bottom', 'right', 'top']):\n        if c_spine != 'none':\n            ax.spines[spine].set_color(c_spine)\n            ax.spines[spine].set_linewidth(lw)\n        else:\n            ax.spines[spine].set_color('none')\n    if (c_bottom == 'none') & (c_top == 'none'): # no bottom and no top\n        ax.xaxis.set_ticks_position('none')\n    elif (c_bottom != 'none') & (c_top != 'none'): # bottom and top\n        ax.tick_params(axis='x', direction='out', width=lw, length=7,\n                      color=c_bottom, labelsize=size, pad=pad)\n    elif (c_bottom != 'none') & (c_top == 'none'): # bottom but not top\n        ax.xaxis.set_ticks_position('bottom')\n        ax.tick_params(axis='x', direction='out', width=lw, length=7,\n                       color=c_bottom, labelsize=size, pad=pad)\n    elif (c_bottom == 'none') & (c_top != 'none'): # no bottom but top\n        ax.xaxis.set_ticks_position('top')\n        ax.tick_params(axis='x', direction='out', width=lw, length=7,\n                       color=c_top, labelsize=size, pad=pad)\n    if (c_left == 'none') & (c_right == 'none'): # no left and no right\n        ax.yaxis.set_ticks_position('none')\n    elif (c_left != 'none') & (c_right != 'none'): # left and right\n        ax.tick_params(axis='y', direction='out', width=lw, length=7,\n                       color=c_left, labelsize=size, pad=pad)\n    elif (c_left != 'none') & (c_right == 'none'): # left but not right\n        ax.yaxis.set_ticks_position('left')\n        ax.tick_params(axis='y', direction='out', width=lw, length=7,\n                       color=c_left, labelsize=size, pad=pad)\n    elif (c_left == 'none') & (c_right != 'none'): # no left but right\n        ax.yaxis.set_ticks_position('right')\n        ax.tick_params(axis='y', direction='out', width=lw, length=7,\n                       color=c_right, labelsize=size, pad=pad)\n"", ""def adjust_spines(ax,spines):\n    for loc, spine in ax.spines.items():\n        if loc in spines:\n            spine.set_position(('outward',10)) # outward by 10 points\n            spine.set_smart_bounds(True)\n        else:\n            spine.set_color('none') # don't draw spine\n"", ""import numpy as np\nimport matplotlib.pyplot as plt\n\nfig,(ax1,ax2) = plt.subplots(figsize=(8,5), ncols=2)\nax1.plot(np.random.rand(20), np.random.rand(20), 'ok')\nax2.plot(np.random.rand(20), np.random.rand(20), 'ok')\n\ncustomaxis(ax2) # remove top and right spines, ticks out\nadjust_spines(ax2, ['left', 'bottom']) # non touching spines\n\nplt.show()\n""]"
684;;5;14365647;14365542.0;2;70;;;"<p><a href=""http://pandas.pydata.org/"" rel=""noreferrer"">pandas</a> to the rescue:</p>

<pre><code>import pandas as pd
print pd.read_csv('value.txt')

        Date    price  factor_1  factor_2
0  2012-06-11  1600.20     1.255     1.548
1  2012-06-12  1610.02     1.258     1.554
2  2012-06-13  1618.07     1.249     1.552
3  2012-06-14  1624.40     1.253     1.556
4  2012-06-15  1626.15     1.258     1.552
5  2012-06-16  1626.15     1.263     1.558
6  2012-06-17  1626.15     1.264     1.572
</code></pre>

<p>This returns pandas <a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe"" rel=""noreferrer"">DataFrame</a> that is similar to <code>R's</code>.</p>
";;"[""import pandas as pd\nprint pd.read_csv('value.txt')\n\n        Date    price  factor_1  factor_2\n0  2012-06-11  1600.20     1.255     1.548\n1  2012-06-12  1610.02     1.258     1.554\n2  2012-06-13  1618.07     1.249     1.552\n3  2012-06-14  1624.40     1.253     1.556\n4  2012-06-15  1626.15     1.258     1.552\n5  2012-06-16  1626.15     1.263     1.558\n6  2012-06-17  1626.15     1.264     1.572\n""]";"[""import pandas as pd\nprint pd.read_csv('value.txt')\n\n        Date    price  factor_1  factor_2\n0  2012-06-11  1600.20     1.255     1.548\n1  2012-06-12  1610.02     1.258     1.554\n2  2012-06-13  1618.07     1.249     1.552\n3  2012-06-14  1624.40     1.253     1.556\n4  2012-06-15  1626.15     1.258     1.552\n5  2012-06-16  1626.15     1.263     1.558\n6  2012-06-17  1626.15     1.264     1.572\n"", ""R's""]"
685;;0;14366084;14365542.0;2;6;;;"<p>Here's an alternative to pandas library using Python's built-in <a href=""http://docs.python.org/2/library/csv.html"" rel=""noreferrer"">csv module</a>.</p>

<pre><code>import csv
from pprint import pprint
with open('foo.csv', 'rb') as f:
    reader = csv.reader(f)
    headers = reader.next()
    column = {h:[] for h in headers}
    for row in reader:
        for h, v in zip(headers, row):
            column[h].append(v)
    pprint(column)    # Pretty printer
</code></pre>

<p>will print</p>

<pre><code>{'Date': ['2012-06-11',
          '2012-06-12',
          '2012-06-13',
          '2012-06-14',
          '2012-06-15',
          '2012-06-16',
          '2012-06-17'],
 'factor_1': ['1.255', '1.258', '1.249', '1.253', '1.258', '1.263', '1.264'],
 'factor_2': ['1.548', '1.554', '1.552', '1.556', '1.552', '1.558', '1.572'],
 'price': ['1600.20',
           '1610.02',
           '1618.07',
           '1624.40',
           '1626.15',
           '1626.15',
           '1626.15']}
</code></pre>
";;"[""import csv\nfrom pprint import pprint\nwith open('foo.csv', 'rb') as f:\n    reader = csv.reader(f)\n    headers = reader.next()\n    column = {h:[] for h in headers}\n    for row in reader:\n        for h, v in zip(headers, row):\n            column[h].append(v)\n    pprint(column)    # Pretty printer\n"", ""{'Date': ['2012-06-11',\n          '2012-06-12',\n          '2012-06-13',\n          '2012-06-14',\n          '2012-06-15',\n          '2012-06-16',\n          '2012-06-17'],\n 'factor_1': ['1.255', '1.258', '1.249', '1.253', '1.258', '1.263', '1.264'],\n 'factor_2': ['1.548', '1.554', '1.552', '1.556', '1.552', '1.558', '1.572'],\n 'price': ['1600.20',\n           '1610.02',\n           '1618.07',\n           '1624.40',\n           '1626.15',\n           '1626.15',\n           '1626.15']}\n""]";"[""import csv\nfrom pprint import pprint\nwith open('foo.csv', 'rb') as f:\n    reader = csv.reader(f)\n    headers = reader.next()\n    column = {h:[] for h in headers}\n    for row in reader:\n        for h, v in zip(headers, row):\n            column[h].append(v)\n    pprint(column)    # Pretty printer\n"", ""{'Date': ['2012-06-11',\n          '2012-06-12',\n          '2012-06-13',\n          '2012-06-14',\n          '2012-06-15',\n          '2012-06-16',\n          '2012-06-17'],\n 'factor_1': ['1.255', '1.258', '1.249', '1.253', '1.258', '1.263', '1.264'],\n 'factor_2': ['1.548', '1.554', '1.552', '1.556', '1.552', '1.558', '1.572'],\n 'price': ['1600.20',\n           '1610.02',\n           '1618.07',\n           '1624.40',\n           '1626.15',\n           '1626.15',\n           '1626.15']}\n""]"
686;;5;14383654;14380371.0;2;50;;;"<p>DataFrames have a <code>to_latex</code> method:</p>

<pre><code>In [42]: df = pd.DataFrame(np.random.random((5, 5)))

In [43]: df
Out[43]: 
          0         1         2         3         4
0  0.886864  0.518538  0.359964  0.167291  0.940414
1  0.834130  0.022920  0.265131  0.059002  0.530584
2  0.648019  0.953043  0.263551  0.595798  0.153969
3  0.207003  0.015721  0.931170  0.045044  0.432870
4  0.039886  0.898780  0.728195  0.112069  0.468485

In [44]: print df.to_latex()
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
{} &amp;         0 &amp;         1 &amp;         2 &amp;         3 &amp;         4 \\
\hline
0 &amp;  0.886864 &amp;  0.518538 &amp;  0.359964 &amp;  0.167291 &amp;  0.940414 \\
1 &amp;  0.834130 &amp;  0.022920 &amp;  0.265131 &amp;  0.059002 &amp;  0.530584 \\
2 &amp;  0.648019 &amp;  0.953043 &amp;  0.263551 &amp;  0.595798 &amp;  0.153969 \\
3 &amp;  0.207003 &amp;  0.015721 &amp;  0.931170 &amp;  0.045044 &amp;  0.432870 \\
4 &amp;  0.039886 &amp;  0.898780 &amp;  0.728195 &amp;  0.112069 &amp;  0.468485 \\
\hline
\end{tabular}
</code></pre>

<p>You can simply write this to a tex file.</p>

<p>By default latex will render this as:</p>

<p><img src=""https://i.stack.imgur.com/W4Mqu.png"" alt=""as it would appear in latex""></p>

<p><em>Note: the <code>to_latex</code> method offers several configuration options.</em></p>
";;['In [42]: df = pd.DataFrame(np.random.random((5, 5)))\n\nIn [43]: df\nOut[43]: \n          0         1         2         3         4\n0  0.886864  0.518538  0.359964  0.167291  0.940414\n1  0.834130  0.022920  0.265131  0.059002  0.530584\n2  0.648019  0.953043  0.263551  0.595798  0.153969\n3  0.207003  0.015721  0.931170  0.045044  0.432870\n4  0.039886  0.898780  0.728195  0.112069  0.468485\n\nIn [44]: print df.to_latex()\n\\begin{tabular}{|l|c|c|c|c|c|c|}\n\\hline\n{} &         0 &         1 &         2 &         3 &         4 \\\\\n\\hline\n0 &  0.886864 &  0.518538 &  0.359964 &  0.167291 &  0.940414 \\\\\n1 &  0.834130 &  0.022920 &  0.265131 &  0.059002 &  0.530584 \\\\\n2 &  0.648019 &  0.953043 &  0.263551 &  0.595798 &  0.153969 \\\\\n3 &  0.207003 &  0.015721 &  0.931170 &  0.045044 &  0.432870 \\\\\n4 &  0.039886 &  0.898780 &  0.728195 &  0.112069 &  0.468485 \\\\\n\\hline\n\\end{tabular}\n'];['to_latex', 'In [42]: df = pd.DataFrame(np.random.random((5, 5)))\n\nIn [43]: df\nOut[43]: \n          0         1         2         3         4\n0  0.886864  0.518538  0.359964  0.167291  0.940414\n1  0.834130  0.022920  0.265131  0.059002  0.530584\n2  0.648019  0.953043  0.263551  0.595798  0.153969\n3  0.207003  0.015721  0.931170  0.045044  0.432870\n4  0.039886  0.898780  0.728195  0.112069  0.468485\n\nIn [44]: print df.to_latex()\n\\begin{tabular}{|l|c|c|c|c|c|c|}\n\\hline\n{} &         0 &         1 &         2 &         3 &         4 \\\\\n\\hline\n0 &  0.886864 &  0.518538 &  0.359964 &  0.167291 &  0.940414 \\\\\n1 &  0.834130 &  0.022920 &  0.265131 &  0.059002 &  0.530584 \\\\\n2 &  0.648019 &  0.953043 &  0.263551 &  0.595798 &  0.153969 \\\\\n3 &  0.207003 &  0.015721 &  0.931170 &  0.045044 &  0.432870 \\\\\n4 &  0.039886 &  0.898780 &  0.728195 &  0.112069 &  0.468485 \\\\\n\\hline\n\\end{tabular}\n', 'to_latex']
687;;4;14487936;12047193.0;2;60;;;"<p><strong>Edit: Mar. 2015</strong></p>

<p>As noted below, pandas now uses <a href=""http://www.sqlalchemy.org/"" rel=""noreferrer"">SQLAlchemy</a> to both read from (<a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.read_sql.html"" rel=""noreferrer"">read_sql</a>) and insert into (<a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.to_sql.html"" rel=""noreferrer"">to_sql</a>) a database. The following should work</p>

<pre><code>import pandas as pd

df = pd.read_sql(sql, cnxn)
</code></pre>

<p><strong>Previous answer:</strong>
Via mikebmassey from a <a href=""https://stackoverflow.com/a/13570851/386279"">similar question</a></p>

<pre><code>import pyodbc
import pandas.io.sql as psql

cnxn = pyodbc.connect(connection_info) 
cursor = cnxn.cursor()
sql = ""SELECT * FROM TABLE""

df = psql.frame_query(sql, cnxn)
cnxn.close()
</code></pre>
";;"['import pandas as pd\n\ndf = pd.read_sql(sql, cnxn)\n', 'import pyodbc\nimport pandas.io.sql as psql\n\ncnxn = pyodbc.connect(connection_info) \ncursor = cnxn.cursor()\nsql = ""SELECT * FROM TABLE""\n\ndf = psql.frame_query(sql, cnxn)\ncnxn.close()\n']";"['import pandas as pd\n\ndf = pd.read_sql(sql, cnxn)\n', 'import pyodbc\nimport pandas.io.sql as psql\n\ncnxn = pyodbc.connect(connection_info) \ncursor = cnxn.cursor()\nsql = ""SELECT * FROM TABLE""\n\ndf = psql.frame_query(sql, cnxn)\ncnxn.close()\n']"
688;;3;14508355;14507794.0;2;151;;;"<p>I think the easiest way to do this would be to set the columns to the top level:</p>

<pre><code>df.columns = df.columns.get_level_values(0)
</code></pre>

<p><em>Note: if the to level has a name you can also access it by this, rather than 0.</em></p>

<p>.</p>

<p>If you want to combine/<a href=""http://docs.python.org/2/library/stdtypes.html#str.join""><code>join</code></a> your MultiIndex into one Index <em>(assuming you have just string entries in your columns)</em> you could:</p>

<pre><code>df.columns = [' '.join(col).strip() for col in df.columns.values]
</code></pre>

<p><em>Note: we must <a href=""http://docs.python.org/2/library/stdtypes.html#str.strip""><code>strip</code></a> the whitespace for when there is no second index.</em></p>

<pre><code>In [11]: [' '.join(col).strip() for col in df.columns.values]
Out[11]: 
['USAF',
 'WBAN',
 'day',
 'month',
 's_CD sum',
 's_CL sum',
 's_CNT sum',
 's_PC sum',
 'tempf amax',
 'tempf amin',
 'year']
</code></pre>
";;"['df.columns = df.columns.get_level_values(0)\n', ""df.columns = [' '.join(col).strip() for col in df.columns.values]\n"", ""In [11]: [' '.join(col).strip() for col in df.columns.values]\nOut[11]: \n['USAF',\n 'WBAN',\n 'day',\n 'month',\n 's_CD sum',\n 's_CL sum',\n 's_CNT sum',\n 's_PC sum',\n 'tempf amax',\n 'tempf amin',\n 'year']\n""]";"['df.columns = df.columns.get_level_values(0)\n', 'join', ""df.columns = [' '.join(col).strip() for col in df.columns.values]\n"", 'strip', ""In [11]: [' '.join(col).strip() for col in df.columns.values]\nOut[11]: \n['USAF',\n 'WBAN',\n 'day',\n 'month',\n 's_CD sum',\n 's_CL sum',\n 's_CNT sum',\n 's_PC sum',\n 'tempf amax',\n 'tempf amin',\n 'year']\n""]"
689;;1;14508639;14507794.0;2;13;;;"<p>Andy Hayden's answer is certainly the easiest way -- if you want to avoid duplicate column labels you need to tweak a bit</p>

<pre><code>In [34]: df
Out[34]: 
     USAF   WBAN  day  month  s_CD  s_CL  s_CNT  s_PC  tempf         year
                               sum   sum    sum   sum   amax   amin      
0  702730  26451    1      1    12     0     13     1  30.92  24.98  1993
1  702730  26451    2      1    13     0     13     0  32.00  24.98  1993
2  702730  26451    3      1     2    10     13     1  23.00   6.98  1993
3  702730  26451    4      1    12     0     13     1  10.04   3.92  1993
4  702730  26451    5      1    10     0     13     3  19.94  10.94  1993


In [35]: mi = df.columns

In [36]: mi
Out[36]: 
MultiIndex
[(USAF, ), (WBAN, ), (day, ), (month, ), (s_CD, sum), (s_CL, sum), (s_CNT, sum), (s_PC, sum), (tempf, amax), (tempf, amin), (year, )]


In [37]: mi.tolist()
Out[37]: 
[('USAF', ''),
 ('WBAN', ''),
 ('day', ''),
 ('month', ''),
 ('s_CD', 'sum'),
 ('s_CL', 'sum'),
 ('s_CNT', 'sum'),
 ('s_PC', 'sum'),
 ('tempf', 'amax'),
 ('tempf', 'amin'),
 ('year', '')]

In [38]: ind = pd.Index([e[0] + e[1] for e in mi.tolist()])

In [39]: ind
Out[39]: Index([USAF, WBAN, day, month, s_CDsum, s_CLsum, s_CNTsum, s_PCsum, tempfamax, tempfamin, year], dtype=object)

In [40]: df.columns = ind




In [46]: df
Out[46]:
  USAF  WBAN day month s_CDsum s_CLsum s_CNTsum s_PCsum tempfamax tempfamin \
0 702730 26451  1   1    12    0    13    1   30.92   24.98 
1 702730 26451  2   1    13    0    13    0   32.00   24.98 
2 702730 26451  3   1    2    10    13    1   23.00    6.98 
3 702730 26451  4   1    12    0    13    1   10.04    3.92 
4 702730 26451  5   1    10    0    13    3   19.94   10.94 




 year 
0 1993 
1 1993 
2 1993 
3 1993 
4 1993
</code></pre>
";;"[""In [34]: df\nOut[34]: \n     USAF   WBAN  day  month  s_CD  s_CL  s_CNT  s_PC  tempf         year\n                               sum   sum    sum   sum   amax   amin      \n0  702730  26451    1      1    12     0     13     1  30.92  24.98  1993\n1  702730  26451    2      1    13     0     13     0  32.00  24.98  1993\n2  702730  26451    3      1     2    10     13     1  23.00   6.98  1993\n3  702730  26451    4      1    12     0     13     1  10.04   3.92  1993\n4  702730  26451    5      1    10     0     13     3  19.94  10.94  1993\n\n\nIn [35]: mi = df.columns\n\nIn [36]: mi\nOut[36]: \nMultiIndex\n[(USAF, ), (WBAN, ), (day, ), (month, ), (s_CD, sum), (s_CL, sum), (s_CNT, sum), (s_PC, sum), (tempf, amax), (tempf, amin), (year, )]\n\n\nIn [37]: mi.tolist()\nOut[37]: \n[('USAF', ''),\n ('WBAN', ''),\n ('day', ''),\n ('month', ''),\n ('s_CD', 'sum'),\n ('s_CL', 'sum'),\n ('s_CNT', 'sum'),\n ('s_PC', 'sum'),\n ('tempf', 'amax'),\n ('tempf', 'amin'),\n ('year', '')]\n\nIn [38]: ind = pd.Index([e[0] + e[1] for e in mi.tolist()])\n\nIn [39]: ind\nOut[39]: Index([USAF, WBAN, day, month, s_CDsum, s_CLsum, s_CNTsum, s_PCsum, tempfamax, tempfamin, year], dtype=object)\n\nIn [40]: df.columns = ind\n\n\n\n\nIn [46]: df\nOut[46]:\xa0\n\xa0 \xa0 \xa0USAF \xa0 WBAN \xa0day \xa0month \xa0s_CDsum \xa0s_CLsum \xa0s_CNTsum \xa0s_PCsum \xa0tempfamax \xa0tempfamin \xa0\\\n0 \xa0702730 \xa026451 \xa0 \xa01 \xa0 \xa0 \xa01 \xa0 \xa0 \xa0 12 \xa0 \xa0 \xa0 \xa00 \xa0 \xa0 \xa0 \xa013 \xa0 \xa0 \xa0 \xa01 \xa0 \xa0 \xa030.92 \xa0 \xa0 \xa024.98 \xa0\xa0\n1 \xa0702730 \xa026451 \xa0 \xa02 \xa0 \xa0 \xa01 \xa0 \xa0 \xa0 13 \xa0 \xa0 \xa0 \xa00 \xa0 \xa0 \xa0 \xa013 \xa0 \xa0 \xa0 \xa00 \xa0 \xa0 \xa032.00 \xa0 \xa0 \xa024.98 \xa0\xa0\n2 \xa0702730 \xa026451 \xa0 \xa03 \xa0 \xa0 \xa01 \xa0 \xa0 \xa0 \xa02 \xa0 \xa0 \xa0 10 \xa0 \xa0 \xa0 \xa013 \xa0 \xa0 \xa0 \xa01 \xa0 \xa0 \xa023.00 \xa0 \xa0 \xa0 6.98 \xa0\xa0\n3 \xa0702730 \xa026451 \xa0 \xa04 \xa0 \xa0 \xa01 \xa0 \xa0 \xa0 12 \xa0 \xa0 \xa0 \xa00 \xa0 \xa0 \xa0 \xa013 \xa0 \xa0 \xa0 \xa01 \xa0 \xa0 \xa010.04 \xa0 \xa0 \xa0 3.92 \xa0\xa0\n4 \xa0702730 \xa026451 \xa0 \xa05 \xa0 \xa0 \xa01 \xa0 \xa0 \xa0 10 \xa0 \xa0 \xa0 \xa00 \xa0 \xa0 \xa0 \xa013 \xa0 \xa0 \xa0 \xa03 \xa0 \xa0 \xa019.94 \xa0 \xa0 \xa010.94 \xa0\xa0\n\n\n\n\n\xa0 \xa0year \xa0\n0 \xa01993 \xa0\n1 \xa01993 \xa0\n2 \xa01993 \xa0\n3 \xa01993 \xa0\n4 \xa01993\n""]";"[""In [34]: df\nOut[34]: \n     USAF   WBAN  day  month  s_CD  s_CL  s_CNT  s_PC  tempf         year\n                               sum   sum    sum   sum   amax   amin      \n0  702730  26451    1      1    12     0     13     1  30.92  24.98  1993\n1  702730  26451    2      1    13     0     13     0  32.00  24.98  1993\n2  702730  26451    3      1     2    10     13     1  23.00   6.98  1993\n3  702730  26451    4      1    12     0     13     1  10.04   3.92  1993\n4  702730  26451    5      1    10     0     13     3  19.94  10.94  1993\n\n\nIn [35]: mi = df.columns\n\nIn [36]: mi\nOut[36]: \nMultiIndex\n[(USAF, ), (WBAN, ), (day, ), (month, ), (s_CD, sum), (s_CL, sum), (s_CNT, sum), (s_PC, sum), (tempf, amax), (tempf, amin), (year, )]\n\n\nIn [37]: mi.tolist()\nOut[37]: \n[('USAF', ''),\n ('WBAN', ''),\n ('day', ''),\n ('month', ''),\n ('s_CD', 'sum'),\n ('s_CL', 'sum'),\n ('s_CNT', 'sum'),\n ('s_PC', 'sum'),\n ('tempf', 'amax'),\n ('tempf', 'amin'),\n ('year', '')]\n\nIn [38]: ind = pd.Index([e[0] + e[1] for e in mi.tolist()])\n\nIn [39]: ind\nOut[39]: Index([USAF, WBAN, day, month, s_CDsum, s_CLsum, s_CNTsum, s_PCsum, tempfamax, tempfamin, year], dtype=object)\n\nIn [40]: df.columns = ind\n\n\n\n\nIn [46]: df\nOut[46]:\xa0\n\xa0 \xa0 \xa0USAF \xa0 WBAN \xa0day \xa0month \xa0s_CDsum \xa0s_CLsum \xa0s_CNTsum \xa0s_PCsum \xa0tempfamax \xa0tempfamin \xa0\\\n0 \xa0702730 \xa026451 \xa0 \xa01 \xa0 \xa0 \xa01 \xa0 \xa0 \xa0 12 \xa0 \xa0 \xa0 \xa00 \xa0 \xa0 \xa0 \xa013 \xa0 \xa0 \xa0 \xa01 \xa0 \xa0 \xa030.92 \xa0 \xa0 \xa024.98 \xa0\xa0\n1 \xa0702730 \xa026451 \xa0 \xa02 \xa0 \xa0 \xa01 \xa0 \xa0 \xa0 13 \xa0 \xa0 \xa0 \xa00 \xa0 \xa0 \xa0 \xa013 \xa0 \xa0 \xa0 \xa00 \xa0 \xa0 \xa032.00 \xa0 \xa0 \xa024.98 \xa0\xa0\n2 \xa0702730 \xa026451 \xa0 \xa03 \xa0 \xa0 \xa01 \xa0 \xa0 \xa0 \xa02 \xa0 \xa0 \xa0 10 \xa0 \xa0 \xa0 \xa013 \xa0 \xa0 \xa0 \xa01 \xa0 \xa0 \xa023.00 \xa0 \xa0 \xa0 6.98 \xa0\xa0\n3 \xa0702730 \xa026451 \xa0 \xa04 \xa0 \xa0 \xa01 \xa0 \xa0 \xa0 12 \xa0 \xa0 \xa0 \xa00 \xa0 \xa0 \xa0 \xa013 \xa0 \xa0 \xa0 \xa01 \xa0 \xa0 \xa010.04 \xa0 \xa0 \xa0 3.92 \xa0\xa0\n4 \xa0702730 \xa026451 \xa0 \xa05 \xa0 \xa0 \xa01 \xa0 \xa0 \xa0 10 \xa0 \xa0 \xa0 \xa00 \xa0 \xa0 \xa0 \xa013 \xa0 \xa0 \xa0 \xa03 \xa0 \xa0 \xa019.94 \xa0 \xa0 \xa010.94 \xa0\xa0\n\n\n\n\n\xa0 \xa0year \xa0\n0 \xa01993 \xa0\n1 \xa01993 \xa0\n2 \xa01993 \xa0\n3 \xa01993 \xa0\n4 \xa01993\n""]"
690;;6;14530027;14529838.0;2;78;;;"<p>For the first part you can pass a dict of column names for keys and a list of functions for the values:</p>

<pre><code>In [28]: df
Out[28]:
          A         B         C         D         E  GRP
0  0.395670  0.219560  0.600644  0.613445  0.242893    0
1  0.323911  0.464584  0.107215  0.204072  0.927325    0
2  0.321358  0.076037  0.166946  0.439661  0.914612    1
3  0.133466  0.447946  0.014815  0.130781  0.268290    1

In [26]: f = {'A':['sum','mean'], 'B':['prod']}

In [27]: df.groupby('GRP').agg(f)
Out[27]:
            A                   B
          sum      mean      prod
GRP
0    0.719580  0.359790  0.102004
1    0.454824  0.227412  0.034060
</code></pre>

<p>UPDATE 1:</p>

<p>Because the aggregate function works on Series, references to the other column names are lost.  To get around this, you can reference the full dataframe and index it using the group indices within the lambda function.</p>

<p>Here's a hacky workaround:</p>

<pre><code>In [67]: f = {'A':['sum','mean'], 'B':['prod'], 'D': lambda g: df.ix[g.index].E.sum()}

In [69]: df.groupby('GRP').agg(f)
Out[69]:
            A                   B         D
          sum      mean      prod  &lt;lambda&gt;
GRP
0    0.719580  0.359790  0.102004  1.170219
1    0.454824  0.227412  0.034060  1.182901
</code></pre>

<p>Here, the resultant 'D' column is made up of the summed 'E' values.</p>

<p>UPDATE 2:</p>

<p>Here's a method that I think will do everything you ask.  First make a custom lambda function.  Below, g references the group.  When aggregating, g will be a Series.  Passing <code>g.index</code> to <code>df.ix[]</code> selects the current group from df.  I then test if column C is less than 0.5.  The returned boolean series is passed to <code>g[]</code> which selects only those rows meeting the criteria.</p>

<pre><code>In [95]: cust = lambda g: g[df.ix[g.index]['C'] &lt; 0.5].sum()

In [96]: f = {'A':['sum','mean'], 'B':['prod'], 'D': {'my name': cust}}

In [97]: df.groupby('GRP').agg(f)
Out[97]:
            A                   B         D
          sum      mean      prod   my name
GRP
0    0.719580  0.359790  0.102004  0.204072
1    0.454824  0.227412  0.034060  0.570441
</code></pre>
";;"[""In [28]: df\nOut[28]:\n          A         B         C         D         E  GRP\n0  0.395670  0.219560  0.600644  0.613445  0.242893    0\n1  0.323911  0.464584  0.107215  0.204072  0.927325    0\n2  0.321358  0.076037  0.166946  0.439661  0.914612    1\n3  0.133466  0.447946  0.014815  0.130781  0.268290    1\n\nIn [26]: f = {'A':['sum','mean'], 'B':['prod']}\n\nIn [27]: df.groupby('GRP').agg(f)\nOut[27]:\n            A                   B\n          sum      mean      prod\nGRP\n0    0.719580  0.359790  0.102004\n1    0.454824  0.227412  0.034060\n"", ""In [67]: f = {'A':['sum','mean'], 'B':['prod'], 'D': lambda g: df.ix[g.index].E.sum()}\n\nIn [69]: df.groupby('GRP').agg(f)\nOut[69]:\n            A                   B         D\n          sum      mean      prod  <lambda>\nGRP\n0    0.719580  0.359790  0.102004  1.170219\n1    0.454824  0.227412  0.034060  1.182901\n"", ""In [95]: cust = lambda g: g[df.ix[g.index]['C'] < 0.5].sum()\n\nIn [96]: f = {'A':['sum','mean'], 'B':['prod'], 'D': {'my name': cust}}\n\nIn [97]: df.groupby('GRP').agg(f)\nOut[97]:\n            A                   B         D\n          sum      mean      prod   my name\nGRP\n0    0.719580  0.359790  0.102004  0.204072\n1    0.454824  0.227412  0.034060  0.570441\n""]";"[""In [28]: df\nOut[28]:\n          A         B         C         D         E  GRP\n0  0.395670  0.219560  0.600644  0.613445  0.242893    0\n1  0.323911  0.464584  0.107215  0.204072  0.927325    0\n2  0.321358  0.076037  0.166946  0.439661  0.914612    1\n3  0.133466  0.447946  0.014815  0.130781  0.268290    1\n\nIn [26]: f = {'A':['sum','mean'], 'B':['prod']}\n\nIn [27]: df.groupby('GRP').agg(f)\nOut[27]:\n            A                   B\n          sum      mean      prod\nGRP\n0    0.719580  0.359790  0.102004\n1    0.454824  0.227412  0.034060\n"", ""In [67]: f = {'A':['sum','mean'], 'B':['prod'], 'D': lambda g: df.ix[g.index].E.sum()}\n\nIn [69]: df.groupby('GRP').agg(f)\nOut[69]:\n            A                   B         D\n          sum      mean      prod  <lambda>\nGRP\n0    0.719580  0.359790  0.102004  1.170219\n1    0.454824  0.227412  0.034060  1.182901\n"", 'g.index', 'df.ix[]', 'g[]', ""In [95]: cust = lambda g: g[df.ix[g.index]['C'] < 0.5].sum()\n\nIn [96]: f = {'A':['sum','mean'], 'B':['prod'], 'D': {'my name': cust}}\n\nIn [97]: df.groupby('GRP').agg(f)\nOut[97]:\n            A                   B         D\n          sum      mean      prod   my name\nGRP\n0    0.719580  0.359790  0.102004  0.204072\n1    0.454824  0.227412  0.034060  0.570441\n""]"
691;;4;14630250;14627380.0;2;18;;;"<p>You can use the DataFrame <a href=""http://pandas-docs.github.io/pandas-docs-travis/generated/pandas.DataFrame.to_html.html?highlight=to_html#pandas.DataFrame.to_html"" rel=""nofollow noreferrer""><code>to_html</code></a> method, which comes with <code>formatters</code> argument.</p>

<p><em>An easier solution would be to surround by <code>&lt;span class=""significant""&gt;</code> and <code>&lt;/span&gt;</code>, (rather than <code>*</code>). Note: by default this will be escaped (i.e. <code>&lt;</code> becomes <code>&amp;lt;</code>) so you will need to use the <code>escape=False</code> argument.</em></p>
";;[];"['to_html', 'formatters', '<span class=""significant"">', '</span>', '*', '<', '&lt;', 'escape=False']"
692;;5;14661768;14661701.0;2;201;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"">DataFrame.drop</a> and pass it a Series of index labels:</p>

<pre><code>In [65]: df
Out[65]: 
       one  two
one      1    4
two      2    3
three    3    2
four     4    1


In [66]: df.drop(df.index[[1,3]])
Out[66]: 
       one  two
one      1    4
three    3    2
</code></pre>
";;['In [65]: df\nOut[65]: \n       one  two\none      1    4\ntwo      2    3\nthree    3    2\nfour     4    1\n\n\nIn [66]: df.drop(df.index[[1,3]])\nOut[66]: \n       one  two\none      1    4\nthree    3    2\n'];['In [65]: df\nOut[65]: \n       one  two\none      1    4\ntwo      2    3\nthree    3    2\nfour     4    1\n\n\nIn [66]: df.drop(df.index[[1,3]])\nOut[66]: \n       one  two\none      1    4\nthree    3    2\n']
693;;1;14669654;14663004.0;2;22;;;"<p>This is because of using integer indices (<code>ix</code> selects those by <em>label</em> over -3 rather than <em>position</em>, and this is by design: see <a href=""http://pandas.pydata.org/pandas-docs/dev/gotchas.html#integer-indexing"" rel=""nofollow noreferrer"">integer indexing in pandas ""gotchas""</a>*).</p>

<p>*In newer versions of pandas prefer loc or iloc to remove the ambiguity of ix as position or label:</p>

<pre><code>df.iloc[-3:]
</code></pre>

<p><em>see the <a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html#different-choices-for-indexing-loc-iloc-and-ix"" rel=""nofollow noreferrer"">docs</a></em>.</p>

<p>As Wes points out, in this specific case you should just use tail!</p>

<p>It should also be noted that in Pandas pre-0.14 <code>iloc</code> will raise an <code>IndexError</code> on an out-of-bounds access, while <code>.head()</code> and <code>.tail()</code> will not:</p>

<pre><code>&gt;&gt;&gt; pd.__version__
'0.12.0'
&gt;&gt;&gt; df = pd.DataFrame([{""a"": 1}, {""a"": 2}])
&gt;&gt;&gt; df.iloc[-5:]
...
IndexError: out-of-bounds on slice (end)
&gt;&gt;&gt; df.tail(5)
   a
0  1
1  2
</code></pre>

<hr>

<p>Old answer (depreciated method):</p>

<p>You can use the <code>irows</code> DataFrame method to overcome this ambiguity:</p>

<pre><code>In [11]: df1.irow(slice(-3, None))
Out[11]: 
    STK_ID  RPT_Date  TClose   sales  discount
8      568  20080331   38.75  12.668       NaN
9      568  20080630   30.09  21.102       NaN
10     568  20080930   26.00  30.769       NaN
</code></pre>

<p><em>Note: Series has <a href=""https://stackoverflow.com/a/14466665/1240268"">a similar <code>iget</code> method</a>.</em></p>
";;"['df.iloc[-3:]\n', '>>> pd.__version__\n\'0.12.0\'\n>>> df = pd.DataFrame([{""a"": 1}, {""a"": 2}])\n>>> df.iloc[-5:]\n...\nIndexError: out-of-bounds on slice (end)\n>>> df.tail(5)\n   a\n0  1\n1  2\n', 'In [11]: df1.irow(slice(-3, None))\nOut[11]: \n    STK_ID  RPT_Date  TClose   sales  discount\n8      568  20080331   38.75  12.668       NaN\n9      568  20080630   30.09  21.102       NaN\n10     568  20080930   26.00  30.769       NaN\n']";"['ix', 'df.iloc[-3:]\n', 'iloc', 'IndexError', '.head()', '.tail()', '>>> pd.__version__\n\'0.12.0\'\n>>> df = pd.DataFrame([{""a"": 1}, {""a"": 2}])\n>>> df.iloc[-5:]\n...\nIndexError: out-of-bounds on slice (end)\n>>> df.tail(5)\n   a\n0  1\n1  2\n', 'irows', 'In [11]: df1.irow(slice(-3, None))\nOut[11]: \n    STK_ID  RPT_Date  TClose   sales  discount\n8      568  20080331   38.75  12.668       NaN\n9      568  20080630   30.09  21.102       NaN\n10     568  20080930   26.00  30.769       NaN\n', 'iget']"
694;;5;14688398;14688306.0;2;40;;;"<p>Sure, like most Python objects, you can attach new attributes to a <code>pandas.DataFrame</code>:</p>

<pre><code>import pandas as pd
df = pd.DataFrame([])
df.instrument_name = 'Binky'
</code></pre>

<p>Note, however, that while you can attach attributes to a DataFrame, operations performed on the DataFrame (such as <code>groupby</code>, <code>pivot</code>, <code>join</code> or <code>loc</code> to name just a few) may return a new DataFrame <em>without</em> the metadata attached. Pandas does not yet have a robust method of <em>propagating</em> <a href=""https://github.com/pydata/pandas/issues/2485"" rel=""noreferrer"">metadata attached to DataFrames</a>.</p>

<p>Preserving the metadata <em>in a file</em> is possible. You can find an example of how to store metadata in an HDF5 file <a href=""https://stackoverflow.com/a/29130146/190597"">here</a>.</p>
";;"[""import pandas as pd\ndf = pd.DataFrame([])\ndf.instrument_name = 'Binky'\n""]";"['pandas.DataFrame', ""import pandas as pd\ndf = pd.DataFrame([])\ndf.instrument_name = 'Binky'\n"", 'groupby', 'pivot', 'join', 'loc']"
695;;0;14688529;14688306.0;2;9;;;"<p>Not really. Although you could add attributes containing metadata to the DataFrame class as @unutbu mentions, many DataFrame methods return a new DataFrame, so your meta data would be lost. If you need to manipulate your dataframe, then the best option would be to wrap your metadata and DataFrame in another class. See this discussion on GitHub: <a href=""https://github.com/pydata/pandas/issues/2485"">https://github.com/pydata/pandas/issues/2485</a></p>

<p>There is currently an open <a href=""https://github.com/pydata/pandas/pull/2695"">pull request</a> to add a MetaDataFrame object, which would support metadata better. </p>
";;[];[]
696;;6;14734148;14733871.0;2;53;;;"<p>When sorting by a MultiIndex you need to contain the tuple describing the column inside a list*:</p>

<pre><code>In [11]: df.sort([('Group1', 'C')], ascending=False)
Out[11]: 
  Group1       Group2      
       A  B  C      A  B  C
2      5  6  9      1  0  0
1      1  0  3      2  5  7
3      7  0  2      0  3  5
</code></pre>

<p>* <em>so as not to confuse pandas into thinking you want to sort first by Group1 then by C.</em></p>
";;"[""In [11]: df.sort([('Group1', 'C')], ascending=False)\nOut[11]: \n  Group1       Group2      \n       A  B  C      A  B  C\n2      5  6  9      1  0  0\n1      1  0  3      2  5  7\n3      7  0  2      0  3  5\n""]";"[""In [11]: df.sort([('Group1', 'C')], ascending=False)\nOut[11]: \n  Group1       Group2      \n       A  B  C      A  B  C\n2      5  6  9      1  0  0\n1      1  0  3      2  5  7\n3      7  0  2      0  3  5\n""]"
697;;2;14734627;14734533.0;2;94;;;"<p>You can use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.get_group.html""><code>get_group</code></a> method:</p>

<pre><code>In [21]: gb.get_group('foo')
Out[21]: 
     A         B   C
0  foo  1.624345   5
2  foo -0.528172  11
4  foo  0.865408  14
</code></pre>

<p><em>Note: This doesn't require creating an intermediary dictionary / copy of every subdataframe for every group, so will be much more memory-efficient that creating the naive dictionary with <code>dict(iter(gb))</code>. This is because it uses data-structures already available in the groupby object.</em></p>

<hr>

<p>You can select different columns using the groupby slicing:</p>

<pre><code>In [22]: gb[[""A"", ""B""]].get_group(""foo"")
Out[22]:
     A         B
0  foo  1.624345
2  foo -0.528172
4  foo  0.865408

In [23]: gb[""C""].get_group(""foo"")
Out[23]:
0     5
2    11
4    14
Name: C, dtype: int64
</code></pre>
";;"[""In [21]: gb.get_group('foo')\nOut[21]: \n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n"", 'In [22]: gb[[""A"", ""B""]].get_group(""foo"")\nOut[22]:\n     A         B\n0  foo  1.624345\n2  foo -0.528172\n4  foo  0.865408\n\nIn [23]: gb[""C""].get_group(""foo"")\nOut[23]:\n0     5\n2    11\n4    14\nName: C, dtype: int64\n']";"['get_group', ""In [21]: gb.get_group('foo')\nOut[21]: \n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n"", 'dict(iter(gb))', 'In [22]: gb[[""A"", ""B""]].get_group(""foo"")\nOut[22]:\n     A         B\n0  foo  1.624345\n2  foo -0.528172\n4  foo  0.865408\n\nIn [23]: gb[""C""].get_group(""foo"")\nOut[23]:\n0     5\n2    11\n4    14\nName: C, dtype: int64\n']"
698;;3;14745484;14745022.0;2;34;;;"<p>There might be a better way, but this here's one approach:</p>

<pre><code>In [34]: import pandas as pd

In [35]: df
Out[35]: 
                        row
0       00000 UNITED STATES
1             01000 ALABAMA
2  01001 Autauga County, AL
3  01003 Baldwin County, AL
4  01005 Barbour County, AL

In [36]: df = pd.DataFrame(df.row.str.split(' ',1).tolist(),
                                   columns = ['flips','row'])

In [37]: df
Out[37]: 
   flips                 row
0  00000       UNITED STATES
1  01000             ALABAMA
2  01001  Autauga County, AL
3  01003  Baldwin County, AL
4  01005  Barbour County, AL
</code></pre>
";;"[""In [34]: import pandas as pd\n\nIn [35]: df\nOut[35]: \n                        row\n0       00000 UNITED STATES\n1             01000 ALABAMA\n2  01001 Autauga County, AL\n3  01003 Baldwin County, AL\n4  01005 Barbour County, AL\n\nIn [36]: df = pd.DataFrame(df.row.str.split(' ',1).tolist(),\n                                   columns = ['flips','row'])\n\nIn [37]: df\nOut[37]: \n   flips                 row\n0  00000       UNITED STATES\n1  01000             ALABAMA\n2  01001  Autauga County, AL\n3  01003  Baldwin County, AL\n4  01005  Barbour County, AL\n""]";"[""In [34]: import pandas as pd\n\nIn [35]: df\nOut[35]: \n                        row\n0       00000 UNITED STATES\n1             01000 ALABAMA\n2  01001 Autauga County, AL\n3  01003 Baldwin County, AL\n4  01005 Barbour County, AL\n\nIn [36]: df = pd.DataFrame(df.row.str.split(' ',1).tolist(),\n                                   columns = ['flips','row'])\n\nIn [37]: df\nOut[37]: \n   flips                 row\n0  00000       UNITED STATES\n1  01000             ALABAMA\n2  01001  Autauga County, AL\n3  01003  Baldwin County, AL\n4  01005  Barbour County, AL\n""]"
699;;1;14746845;14744068.0;2;51;;;"<p>You can first add it as a normal column and then append it to the current index, so:</p>

<pre><code>df['Firstlevel'] = 'Foo'
df.set_index('Firstlevel', append=True, inplace=True)
</code></pre>

<p>And change the order if needed with:</p>

<pre><code>df.reorder_levels(['Firstlevel', 'A', 'B'])
</code></pre>

<p>Which results in:</p>

<pre><code>                      Vals
Firstlevel A  B           
Foo        a1 b1  0.871563
              b2  0.494001
           a2 b3 -0.167811
           a3 b4 -1.353409
</code></pre>
";;"[""df['Firstlevel'] = 'Foo'\ndf.set_index('Firstlevel', append=True, inplace=True)\n"", ""df.reorder_levels(['Firstlevel', 'A', 'B'])\n"", '                      Vals\nFirstlevel A  B           \nFoo        a1 b1  0.871563\n              b2  0.494001\n           a2 b3 -0.167811\n           a3 b4 -1.353409\n']";"[""df['Firstlevel'] = 'Foo'\ndf.set_index('Firstlevel', append=True, inplace=True)\n"", ""df.reorder_levels(['Firstlevel', 'A', 'B'])\n"", '                      Vals\nFirstlevel A  B           \nFoo        a1 b1  0.871563\n              b2  0.494001\n           a2 b3 -0.167811\n           a3 b4 -1.353409\n']"
700;;0;14760930;14663004.0;2;155;;;"<p>Don't forget <code>DataFrame.tail</code>! e.g. <code>df1.tail(10)</code></p>
";;[];['DataFrame.tail', 'df1.tail(10)']
701;;1;14809026;14808945.0;2;59;;;"<p>Use the built-in isinstance() function.</p>

<pre><code>import pandas as pd

def f(var):
    if isinstance(var, pd.DataFrame):
        print ""do stuff""
</code></pre>
";;"['import pandas as pd\n\ndef f(var):\n    if isinstance(var, pd.DataFrame):\n        print ""do stuff""\n']";"['import pandas as pd\n\ndef f(var):\n    if isinstance(var, pd.DataFrame):\n        print ""do stuff""\n']"
702;;0;14809149;14808945.0;2;27;;;"<p><a href=""http://docs.python.org/2/library/functions.html#isinstance"" rel=""nofollow noreferrer"">isinstance</a>, nothing else.</p>

<p><a href=""http://www.python.org/dev/peps/pep-0008/"" rel=""nofollow noreferrer"">PEP8</a> says explicitly that <code>isinstance</code> is the preferred way to check types</p>

<pre><code>Yes: if isinstance(obj, int):
No:  if type(obj) is type(1):
</code></pre>

<p>And don't even think about</p>

<pre><code>if obj.__class__.__name__ = ""MyInheritedClass"":
    expect_problems_some_day()
</code></pre>

<p><code>isinstance</code> handles inheritance (see <a href=""https://stackoverflow.com/questions/1549801/differences-between-isinstance-and-type-in-python"">Differences between isinstance() and type() in python</a>). For example, it will tell you if a variable is a string (either <code>str</code> or <code>unicode</code>), because they derive from <code>basestring</code>)</p>

<pre><code>if isinstance(obj, basestring):
    i_am_string(obj)
</code></pre>
";;"['Yes: if isinstance(obj, int):\nNo:  if type(obj) is type(1):\n', 'if obj.__class__.__name__ = ""MyInheritedClass"":\n    expect_problems_some_day()\n', 'if isinstance(obj, basestring):\n    i_am_string(obj)\n']";"['isinstance', 'Yes: if isinstance(obj, int):\nNo:  if type(obj) is type(1):\n', 'if obj.__class__.__name__ = ""MyInheritedClass"":\n    expect_problems_some_day()\n', 'isinstance', 'str', 'unicode', 'basestring', 'if isinstance(obj, basestring):\n    i_am_string(obj)\n']"
703;;4;14900065;13035764.0;2;82;;;"<p>A simple solution is to use <code>drop_duplicates</code></p>

<pre><code>df4 = df3.drop_duplicates(subset='rownum', keep='last')
</code></pre>

<p>For me, this operated quickly on large data sets.</p>

<p>This requires that 'rownum' be the column with duplicates.  In the modified example, 'rownum' has no duplicates, therefore nothing gets eliminated.  What we really want is to have the 'cols' be set to the index.  I've not found a way to tell drop_duplicates to only consider the index.</p>

<p>Here is a solution that adds the index as a dataframe column, drops duplicates on that, then removes the new column:</p>

<pre><code>df3 = df3.reset_index().drop_duplicates(subset='index', keep='last').set_index('index')
</code></pre>

<p>And if you want things back in the proper order, just call <code>sort</code> on the dataframe.</p>

<pre><code>df3 = df3.sort()
</code></pre>

<h2>Edit: Better answer below</h2>

<p>Take a look at n8yoder's answer using 'duplicated'.  I don't believe this existed in older versions of Pandas, where this answer might still apply.</p>
";;"[""df4 = df3.drop_duplicates(subset='rownum', keep='last')\n"", ""df3 = df3.reset_index().drop_duplicates(subset='index', keep='last').set_index('index')\n"", 'df3 = df3.sort()\n']";"['drop_duplicates', ""df4 = df3.drop_duplicates(subset='rownum', keep='last')\n"", ""df3 = df3.reset_index().drop_duplicates(subset='index', keep='last').set_index('index')\n"", 'sort', 'df3 = df3.sort()\n']"
704;;5;14942625;14941366.0;2;8;;;"<p>One way to do this is to insert a dummy column with the sums in order to sort:</p>

<pre><code>In [10]: sum_B_over_A = df.groupby('A').sum().B

In [11]: sum_B_over_A
Out[11]: 
A
bar    0.253652
baz   -2.829711
foo    0.551376
Name: B

in [12]: df['sum_B_over_A'] = df.A.apply(sum_B_over_A.get_value)

In [13]: df
Out[13]: 
     A         B      C  sum_B_over_A
0  foo  1.624345  False      0.551376
1  bar -0.611756   True      0.253652
2  baz -0.528172  False     -2.829711
3  foo -1.072969   True      0.551376
4  bar  0.865408  False      0.253652
5  baz -2.301539   True     -2.829711

In [14]: df.sort(['sum_B_over_A', 'A', 'B'])
Out[14]: 
     A         B      C   sum_B_over_A
5  baz -2.301539   True      -2.829711
2  baz -0.528172  False      -2.829711
1  bar -0.611756   True       0.253652
4  bar  0.865408  False       0.253652
3  foo -1.072969   True       0.551376
0  foo  1.624345  False       0.551376
</code></pre>

<p><em>and maybe you would drop the dummy row:</em></p>

<pre><code>In [15]: df.sort(['sum_B_over_A', 'A', 'B']).drop('sum_B_over_A', axis=1)
Out[15]: 
     A         B      C
5  baz -2.301539   True
2  baz -0.528172  False
1  bar -0.611756   True
4  bar  0.865408  False
3  foo -1.072969   True
0  foo  1.624345  False
</code></pre>
";;"[""In [10]: sum_B_over_A = df.groupby('A').sum().B\n\nIn [11]: sum_B_over_A\nOut[11]: \nA\nbar    0.253652\nbaz   -2.829711\nfoo    0.551376\nName: B\n\nin [12]: df['sum_B_over_A'] = df.A.apply(sum_B_over_A.get_value)\n\nIn [13]: df\nOut[13]: \n     A         B      C  sum_B_over_A\n0  foo  1.624345  False      0.551376\n1  bar -0.611756   True      0.253652\n2  baz -0.528172  False     -2.829711\n3  foo -1.072969   True      0.551376\n4  bar  0.865408  False      0.253652\n5  baz -2.301539   True     -2.829711\n\nIn [14]: df.sort(['sum_B_over_A', 'A', 'B'])\nOut[14]: \n     A         B      C   sum_B_over_A\n5  baz -2.301539   True      -2.829711\n2  baz -0.528172  False      -2.829711\n1  bar -0.611756   True       0.253652\n4  bar  0.865408  False       0.253652\n3  foo -1.072969   True       0.551376\n0  foo  1.624345  False       0.551376\n"", ""In [15]: df.sort(['sum_B_over_A', 'A', 'B']).drop('sum_B_over_A', axis=1)\nOut[15]: \n     A         B      C\n5  baz -2.301539   True\n2  baz -0.528172  False\n1  bar -0.611756   True\n4  bar  0.865408  False\n3  foo -1.072969   True\n0  foo  1.624345  False\n""]";"[""In [10]: sum_B_over_A = df.groupby('A').sum().B\n\nIn [11]: sum_B_over_A\nOut[11]: \nA\nbar    0.253652\nbaz   -2.829711\nfoo    0.551376\nName: B\n\nin [12]: df['sum_B_over_A'] = df.A.apply(sum_B_over_A.get_value)\n\nIn [13]: df\nOut[13]: \n     A         B      C  sum_B_over_A\n0  foo  1.624345  False      0.551376\n1  bar -0.611756   True      0.253652\n2  baz -0.528172  False     -2.829711\n3  foo -1.072969   True      0.551376\n4  bar  0.865408  False      0.253652\n5  baz -2.301539   True     -2.829711\n\nIn [14]: df.sort(['sum_B_over_A', 'A', 'B'])\nOut[14]: \n     A         B      C   sum_B_over_A\n5  baz -2.301539   True      -2.829711\n2  baz -0.528172  False      -2.829711\n1  bar -0.611756   True       0.253652\n4  bar  0.865408  False       0.253652\n3  foo -1.072969   True       0.551376\n0  foo  1.624345  False       0.551376\n"", ""In [15]: df.sort(['sum_B_over_A', 'A', 'B']).drop('sum_B_over_A', axis=1)\nOut[15]: \n     A         B      C\n5  baz -2.301539   True\n2  baz -0.528172  False\n1  bar -0.611756   True\n4  bar  0.865408  False\n3  foo -1.072969   True\n0  foo  1.624345  False\n""]"
705;;4;14946246;14941366.0;2;43;;;"<p>Groupby A:</p>

<pre><code>In [0]: grp = df.groupby('A')
</code></pre>

<p>Within each group, sum over B and broadcast the values using transform.  Then sort by B:</p>

<pre><code>In [1]: grp[['B']].transform(sum).sort('B')
Out[1]:
          B
2 -2.829710
5 -2.829710
1  0.253651
4  0.253651
0  0.551377
3  0.551377
</code></pre>

<p>Index the original df by passing the index from above.  This will re-order the A values by the aggregate sum of the B values:</p>

<pre><code>In [2]: sort1 = df.ix[grp[['B']].transform(sum).sort('B').index]

In [3]: sort1
Out[3]:
     A         B      C
2  baz -0.528172  False
5  baz -2.301539   True
1  bar -0.611756   True
4  bar  0.865408  False
0  foo  1.624345  False
3  foo -1.072969   True
</code></pre>

<p>Finally, sort the 'C' values within groups of 'A' using the <code>sort=False</code> option to preserve the A sort order from step 1:</p>

<pre><code>In [4]: f = lambda x: x.sort('C', ascending=False)

In [5]: sort2 = sort1.groupby('A', sort=False).apply(f)

In [6]: sort2
Out[6]:
         A         B      C
A
baz 5  baz -2.301539   True
    2  baz -0.528172  False
bar 1  bar -0.611756   True
    4  bar  0.865408  False
foo 3  foo -1.072969   True
    0  foo  1.624345  False
</code></pre>

<p>Clean up the df index by using <code>reset_index</code> with <code>drop=True</code>:</p>

<pre><code>In [7]: sort2.reset_index(0, drop=True)
Out[7]:
     A         B      C
5  baz -2.301539   True
2  baz -0.528172  False
1  bar -0.611756   True
4  bar  0.865408  False
3  foo -1.072969   True
0  foo  1.624345  False
</code></pre>
";;"[""In [0]: grp = df.groupby('A')\n"", ""In [1]: grp[['B']].transform(sum).sort('B')\nOut[1]:\n          B\n2 -2.829710\n5 -2.829710\n1  0.253651\n4  0.253651\n0  0.551377\n3  0.551377\n"", ""In [2]: sort1 = df.ix[grp[['B']].transform(sum).sort('B').index]\n\nIn [3]: sort1\nOut[3]:\n     A         B      C\n2  baz -0.528172  False\n5  baz -2.301539   True\n1  bar -0.611756   True\n4  bar  0.865408  False\n0  foo  1.624345  False\n3  foo -1.072969   True\n"", ""In [4]: f = lambda x: x.sort('C', ascending=False)\n\nIn [5]: sort2 = sort1.groupby('A', sort=False).apply(f)\n\nIn [6]: sort2\nOut[6]:\n         A         B      C\nA\nbaz 5  baz -2.301539   True\n    2  baz -0.528172  False\nbar 1  bar -0.611756   True\n    4  bar  0.865408  False\nfoo 3  foo -1.072969   True\n    0  foo  1.624345  False\n"", 'In [7]: sort2.reset_index(0, drop=True)\nOut[7]:\n     A         B      C\n5  baz -2.301539   True\n2  baz -0.528172  False\n1  bar -0.611756   True\n4  bar  0.865408  False\n3  foo -1.072969   True\n0  foo  1.624345  False\n']";"[""In [0]: grp = df.groupby('A')\n"", ""In [1]: grp[['B']].transform(sum).sort('B')\nOut[1]:\n          B\n2 -2.829710\n5 -2.829710\n1  0.253651\n4  0.253651\n0  0.551377\n3  0.551377\n"", ""In [2]: sort1 = df.ix[grp[['B']].transform(sum).sort('B').index]\n\nIn [3]: sort1\nOut[3]:\n     A         B      C\n2  baz -0.528172  False\n5  baz -2.301539   True\n1  bar -0.611756   True\n4  bar  0.865408  False\n0  foo  1.624345  False\n3  foo -1.072969   True\n"", 'sort=False', ""In [4]: f = lambda x: x.sort('C', ascending=False)\n\nIn [5]: sort2 = sort1.groupby('A', sort=False).apply(f)\n\nIn [6]: sort2\nOut[6]:\n         A         B      C\nA\nbaz 5  baz -2.301539   True\n    2  baz -0.528172  False\nbar 1  bar -0.611756   True\n    4  bar  0.865408  False\nfoo 3  foo -1.072969   True\n    0  foo  1.624345  False\n"", 'reset_index', 'drop=True', 'In [7]: sort2.reset_index(0, drop=True)\nOut[7]:\n     A         B      C\n5  baz -2.301539   True\n2  baz -0.528172  False\n1  bar -0.611756   True\n4  bar  0.865408  False\n3  foo -1.072969   True\n0  foo  1.624345  False\n']"
706;;1;14964637;14964493.0;2;26;;;"<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.xs.html"">xs</a> may be what you want. Here's a few examples:</p>

<pre><code>In [63]: df.xs(('B',), level='Alpha')
Out[63]:
                  I        II       III        IV         V        VI       VII
Int Bool                                                                       
0   True  -0.430563  0.139969 -0.356883 -0.574463 -0.107693 -1.030063  0.271250
    False  0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951
1   True  -0.450375  1.237018  0.398290  0.246182 -0.237919  1.372239 -0.805403
    False -0.064493  0.967132 -0.674451  0.666691 -0.350378  1.721682 -0.791897
2   True   0.143154 -0.061543 -1.157361  0.864847 -0.379616 -0.762626  0.645582
    False -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494

In [64]: df.xs(('B', False), level=('Alpha', 'Bool'))
Out[64]:
            I        II       III        IV         V        VI       VII
Int                                                                      
0    0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951
1   -0.064493  0.967132 -0.674451  0.666691 -0.350378  1.721682 -0.791897
2   -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494 
</code></pre>

<p><strong>Edit</strong>:</p>

<p>For the last requirement you can chain <code>get_level_values</code> and <code>isin</code>:</p>

<p>Get the even values in the index (other ways to do this too)</p>

<pre><code>In [87]: ix_vals = set(i for _, i, _ in df.index if i % 2 == 0)
         ix_vals

Out[87]: set([0L, 2L])
</code></pre>

<p>Use these with <code>isin</code></p>

<pre><code>In [89]: ix = df.index.get_level_values('Int').isin(ix_vals)
In [90]: df[ix]
Out[90]:                I        II       III        IV         V        VI       VII
Alpha Int Bool                                                                       
A     0   True  -1.315409  1.203800  0.330372 -0.295718 -0.679039  1.402114  0.778572
          False  0.008189 -0.104372  0.419110  0.302978 -0.880262 -1.037645 -0.264265
      2   True  -2.414290  0.896990  0.986167 -0.527074  0.550753 -0.302920  0.228165
          False  1.275831  0.448089 -0.635874 -0.733855 -0.747774 -1.108976  0.151474
B     0   True  -0.430563  0.139969 -0.356883 -0.574463 -0.107693 -1.030063  0.271250
          False  0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951
      2   True   0.143154 -0.061543 -1.157361  0.864847 -0.379616 -0.762626  0.645582
          False -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494 
</code></pre>
";;"[""In [63]: df.xs(('B',), level='Alpha')\nOut[63]:\n                  I        II       III        IV         V        VI       VII\nInt Bool                                                                       \n0   True  -0.430563  0.139969 -0.356883 -0.574463 -0.107693 -1.030063  0.271250\n    False  0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951\n1   True  -0.450375  1.237018  0.398290  0.246182 -0.237919  1.372239 -0.805403\n    False -0.064493  0.967132 -0.674451  0.666691 -0.350378  1.721682 -0.791897\n2   True   0.143154 -0.061543 -1.157361  0.864847 -0.379616 -0.762626  0.645582\n    False -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494\n\nIn [64]: df.xs(('B', False), level=('Alpha', 'Bool'))\nOut[64]:\n            I        II       III        IV         V        VI       VII\nInt                                                                      \n0    0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951\n1   -0.064493  0.967132 -0.674451  0.666691 -0.350378  1.721682 -0.791897\n2   -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494 \n"", 'In [87]: ix_vals = set(i for _, i, _ in df.index if i % 2 == 0)\n         ix_vals\n\nOut[87]: set([0L, 2L])\n', ""In [89]: ix = df.index.get_level_values('Int').isin(ix_vals)\nIn [90]: df[ix]\nOut[90]:                I        II       III        IV         V        VI       VII\nAlpha Int Bool                                                                       \nA     0   True  -1.315409  1.203800  0.330372 -0.295718 -0.679039  1.402114  0.778572\n          False  0.008189 -0.104372  0.419110  0.302978 -0.880262 -1.037645 -0.264265\n      2   True  -2.414290  0.896990  0.986167 -0.527074  0.550753 -0.302920  0.228165\n          False  1.275831  0.448089 -0.635874 -0.733855 -0.747774 -1.108976  0.151474\nB     0   True  -0.430563  0.139969 -0.356883 -0.574463 -0.107693 -1.030063  0.271250\n          False  0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951\n      2   True   0.143154 -0.061543 -1.157361  0.864847 -0.379616 -0.762626  0.645582\n          False -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494 \n""]";"[""In [63]: df.xs(('B',), level='Alpha')\nOut[63]:\n                  I        II       III        IV         V        VI       VII\nInt Bool                                                                       \n0   True  -0.430563  0.139969 -0.356883 -0.574463 -0.107693 -1.030063  0.271250\n    False  0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951\n1   True  -0.450375  1.237018  0.398290  0.246182 -0.237919  1.372239 -0.805403\n    False -0.064493  0.967132 -0.674451  0.666691 -0.350378  1.721682 -0.791897\n2   True   0.143154 -0.061543 -1.157361  0.864847 -0.379616 -0.762626  0.645582\n    False -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494\n\nIn [64]: df.xs(('B', False), level=('Alpha', 'Bool'))\nOut[64]:\n            I        II       III        IV         V        VI       VII\nInt                                                                      \n0    0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951\n1   -0.064493  0.967132 -0.674451  0.666691 -0.350378  1.721682 -0.791897\n2   -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494 \n"", 'get_level_values', 'isin', 'In [87]: ix_vals = set(i for _, i, _ in df.index if i % 2 == 0)\n         ix_vals\n\nOut[87]: set([0L, 2L])\n', 'isin', ""In [89]: ix = df.index.get_level_values('Int').isin(ix_vals)\nIn [90]: df[ix]\nOut[90]:                I        II       III        IV         V        VI       VII\nAlpha Int Bool                                                                       \nA     0   True  -1.315409  1.203800  0.330372 -0.295718 -0.679039  1.402114  0.778572\n          False  0.008189 -0.104372  0.419110  0.302978 -0.880262 -1.037645 -0.264265\n      2   True  -2.414290  0.896990  0.986167 -0.527074  0.550753 -0.302920  0.228165\n          False  1.275831  0.448089 -0.635874 -0.733855 -0.747774 -1.108976  0.151474\nB     0   True  -0.430563  0.139969 -0.356883 -0.574463 -0.107693 -1.030063  0.271250\n          False  0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951\n      2   True   0.143154 -0.061543 -1.157361  0.864847 -0.379616 -0.762626  0.645582\n          False -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494 \n""]"
707;;2;14985695;14984119.0;2;20;;;"<p>It sounds like you already know the unique column names. If that's the case, then <code>df = df['Time', 'Time Relative', 'N2']</code> would work.</p>

<p>If not, your solution should work:</p>

<pre><code>In [101]: vals = np.random.randint(0,20, (4,3))
          vals
Out[101]:
array([[ 3, 13,  0],
       [ 1, 15, 14],
       [14, 19, 14],
       [19,  5,  1]])

In [106]: df = pd.DataFrame(np.hstack([vals, vals]), columns=['Time', 'H1', 'N2', 'Time Relative', 'N2', 'Time'] )
          df
Out[106]:
   Time  H1  N2  Time Relative  N2  Time
0     3  13   0              3  13     0
1     1  15  14              1  15    14
2    14  19  14             14  19    14
3    19   5   1             19   5     1

In [107]: df.T.drop_duplicates().T
Out[107]:
   Time  H1  N2
0     3  13   0
1     1  15  14
2    14  19  14
3    19   5   1
</code></pre>

<p>You probably have something specific to your data that's messing it up. We could give more help if there's more details you could give us about the data. </p>

<p><strong>Edit:</strong>
Like Andy said, the problem is probably with the duplicate column titles.</p>

<p>For a sample table file 'dummy.csv' I made up:</p>

<pre><code>Time    H1  N2  Time    N2  Time Relative
3   13  13  3   13  0
1   15  15  1   15  14
14  19  19  14  19  14
19  5   5   19  5   1
</code></pre>

<p>using <code>read_table</code> gives unique columns and works properly:</p>

<pre><code>In [151]: df2 = pd.read_table('dummy.csv')
          df2
Out[151]:
         Time  H1  N2  Time.1  N2.1  Time Relative
      0     3  13  13       3    13              0
      1     1  15  15       1    15             14
      2    14  19  19      14    19             14
      3    19   5   5      19     5              1
In [152]: df2.T.drop_duplicates().T
Out[152]:
             Time  H1  Time Relative
          0     3  13              0
          1     1  15             14
          2    14  19             14
          3    19   5              1  
</code></pre>

<p>If your version doesn't let your, you can hack together a solution to make them unique:</p>

<pre><code>In [169]: df2 = pd.read_table('dummy.csv', header=None)
          df2
Out[169]:
              0   1   2     3   4              5
        0  Time  H1  N2  Time  N2  Time Relative
        1     3  13  13     3  13              0
        2     1  15  15     1  15             14
        3    14  19  19    14  19             14
        4    19   5   5    19   5              1
In [171]: from collections import defaultdict
          col_counts = defaultdict(int)
          col_ix = df2.first_valid_index()
In [172]: cols = []
          for col in df2.ix[col_ix]:
              cnt = col_counts[col]
              col_counts[col] += 1
              suf = '_' + str(cnt) if cnt else ''
              cols.append(col + suf)
          cols
Out[172]:
          ['Time', 'H1', 'N2', 'Time_1', 'N2_1', 'Time Relative']
In [174]: df2.columns = cols
          df2 = df2.drop([col_ix])
In [177]: df2
Out[177]:
          Time  H1  N2 Time_1 N2_1 Time Relative
        1    3  13  13      3   13             0
        2    1  15  15      1   15            14
        3   14  19  19     14   19            14
        4   19   5   5     19    5             1
In [178]: df2.T.drop_duplicates().T
Out[178]:
          Time  H1 Time Relative
        1    3  13             0
        2    1  15            14
        3   14  19            14
        4   19   5             1 
</code></pre>
";;"[""In [101]: vals = np.random.randint(0,20, (4,3))\n          vals\nOut[101]:\narray([[ 3, 13,  0],\n       [ 1, 15, 14],\n       [14, 19, 14],\n       [19,  5,  1]])\n\nIn [106]: df = pd.DataFrame(np.hstack([vals, vals]), columns=['Time', 'H1', 'N2', 'Time Relative', 'N2', 'Time'] )\n          df\nOut[106]:\n   Time  H1  N2  Time Relative  N2  Time\n0     3  13   0              3  13     0\n1     1  15  14              1  15    14\n2    14  19  14             14  19    14\n3    19   5   1             19   5     1\n\nIn [107]: df.T.drop_duplicates().T\nOut[107]:\n   Time  H1  N2\n0     3  13   0\n1     1  15  14\n2    14  19  14\n3    19   5   1\n"", 'Time    H1  N2  Time    N2  Time Relative\n3   13  13  3   13  0\n1   15  15  1   15  14\n14  19  19  14  19  14\n19  5   5   19  5   1\n', ""In [151]: df2 = pd.read_table('dummy.csv')\n          df2\nOut[151]:\n         Time  H1  N2  Time.1  N2.1  Time Relative\n      0     3  13  13       3    13              0\n      1     1  15  15       1    15             14\n      2    14  19  19      14    19             14\n      3    19   5   5      19     5              1\nIn [152]: df2.T.drop_duplicates().T\nOut[152]:\n             Time  H1  Time Relative\n          0     3  13              0\n          1     1  15             14\n          2    14  19             14\n          3    19   5              1  \n"", ""In [169]: df2 = pd.read_table('dummy.csv', header=None)\n          df2\nOut[169]:\n              0   1   2     3   4              5\n        0  Time  H1  N2  Time  N2  Time Relative\n        1     3  13  13     3  13              0\n        2     1  15  15     1  15             14\n        3    14  19  19    14  19             14\n        4    19   5   5    19   5              1\nIn [171]: from collections import defaultdict\n          col_counts = defaultdict(int)\n          col_ix = df2.first_valid_index()\nIn [172]: cols = []\n          for col in df2.ix[col_ix]:\n              cnt = col_counts[col]\n              col_counts[col] += 1\n              suf = '_' + str(cnt) if cnt else ''\n              cols.append(col + suf)\n          cols\nOut[172]:\n          ['Time', 'H1', 'N2', 'Time_1', 'N2_1', 'Time Relative']\nIn [174]: df2.columns = cols\n          df2 = df2.drop([col_ix])\nIn [177]: df2\nOut[177]:\n          Time  H1  N2 Time_1 N2_1 Time Relative\n        1    3  13  13      3   13             0\n        2    1  15  15      1   15            14\n        3   14  19  19     14   19            14\n        4   19   5   5     19    5             1\nIn [178]: df2.T.drop_duplicates().T\nOut[178]:\n          Time  H1 Time Relative\n        1    3  13             0\n        2    1  15            14\n        3   14  19            14\n        4   19   5             1 \n""]";"[""df = df['Time', 'Time Relative', 'N2']"", ""In [101]: vals = np.random.randint(0,20, (4,3))\n          vals\nOut[101]:\narray([[ 3, 13,  0],\n       [ 1, 15, 14],\n       [14, 19, 14],\n       [19,  5,  1]])\n\nIn [106]: df = pd.DataFrame(np.hstack([vals, vals]), columns=['Time', 'H1', 'N2', 'Time Relative', 'N2', 'Time'] )\n          df\nOut[106]:\n   Time  H1  N2  Time Relative  N2  Time\n0     3  13   0              3  13     0\n1     1  15  14              1  15    14\n2    14  19  14             14  19    14\n3    19   5   1             19   5     1\n\nIn [107]: df.T.drop_duplicates().T\nOut[107]:\n   Time  H1  N2\n0     3  13   0\n1     1  15  14\n2    14  19  14\n3    19   5   1\n"", 'Time    H1  N2  Time    N2  Time Relative\n3   13  13  3   13  0\n1   15  15  1   15  14\n14  19  19  14  19  14\n19  5   5   19  5   1\n', 'read_table', ""In [151]: df2 = pd.read_table('dummy.csv')\n          df2\nOut[151]:\n         Time  H1  N2  Time.1  N2.1  Time Relative\n      0     3  13  13       3    13              0\n      1     1  15  15       1    15             14\n      2    14  19  19      14    19             14\n      3    19   5   5      19     5              1\nIn [152]: df2.T.drop_duplicates().T\nOut[152]:\n             Time  H1  Time Relative\n          0     3  13              0\n          1     1  15             14\n          2    14  19             14\n          3    19   5              1  \n"", ""In [169]: df2 = pd.read_table('dummy.csv', header=None)\n          df2\nOut[169]:\n              0   1   2     3   4              5\n        0  Time  H1  N2  Time  N2  Time Relative\n        1     3  13  13     3  13              0\n        2     1  15  15     1  15             14\n        3    14  19  19    14  19             14\n        4    19   5   5    19   5              1\nIn [171]: from collections import defaultdict\n          col_counts = defaultdict(int)\n          col_ix = df2.first_valid_index()\nIn [172]: cols = []\n          for col in df2.ix[col_ix]:\n              cnt = col_counts[col]\n              col_counts[col] += 1\n              suf = '_' + str(cnt) if cnt else ''\n              cols.append(col + suf)\n          cols\nOut[172]:\n          ['Time', 'H1', 'N2', 'Time_1', 'N2_1', 'Time Relative']\nIn [174]: df2.columns = cols\n          df2 = df2.drop([col_ix])\nIn [177]: df2\nOut[177]:\n          Time  H1  N2 Time_1 N2_1 Time Relative\n        1    3  13  13      3   13             0\n        2    1  15  15      1   15            14\n        3   14  19  19     14   19            14\n        4   19   5   5     19    5             1\nIn [178]: df2.T.drop_duplicates().T\nOut[178]:\n          Time  H1 Time Relative\n        1    3  13             0\n        2    1  15            14\n        3   14  19            14\n        4   19   5             1 \n""]"
708;;0;14988913;14988480.0;2;18;;;"<p>Ah, this is to do with how I created the DataFrame, not with how I was combining them. The long and the short of it is, if you are creating a frame using a loop and a statement that looks like this:</p>

<pre><code>Frame = Frame.append(pandas.DataFrame(data = SomeNewLineOfData))
</code></pre>

<p><strong>You must ignore the index</strong></p>

<pre><code>Frame = Frame.append(pandas.DataFrame(data = SomeNewLineOfData), ignore_index=True)
</code></pre>

<p>Or you will have issues later when combining data.</p>
";;['Frame = Frame.append(pandas.DataFrame(data = SomeNewLineOfData))\n', 'Frame = Frame.append(pandas.DataFrame(data = SomeNewLineOfData), ignore_index=True)\n'];['Frame = Frame.append(pandas.DataFrame(data = SomeNewLineOfData))\n', 'Frame = Frame.append(pandas.DataFrame(data = SomeNewLineOfData), ignore_index=True)\n']
709;;2;14989047;14988480.0;2;10;;;"<p>This worked for me:</p>

<pre><code>import numpy as np
import pandas as pd

dates = np.asarray(pd.date_range('1/1/2000', periods=8))
df1 = pd.DataFrame(np.random.randn(8, 4), index=dates, columns=['A', 'B', 'C', 'D'])
df2 = df1.copy()
df = df1.append(df2)
</code></pre>

<p>Yields:</p>

<pre><code>                   A         B         C         D
2000-01-01 -0.327208  0.552500  0.862529  0.493109
2000-01-02  1.039844 -2.141089 -0.781609  1.307600
2000-01-03 -0.462831  0.066505 -1.698346  1.123174
2000-01-04 -0.321971 -0.544599 -0.486099 -0.283791
2000-01-05  0.693749  0.544329 -1.606851  0.527733
2000-01-06 -2.461177 -0.339378 -0.236275  0.155569
2000-01-07 -0.597156  0.904511  0.369865  0.862504
2000-01-08 -0.958300 -0.583621 -2.068273  0.539434
2000-01-01 -0.327208  0.552500  0.862529  0.493109
2000-01-02  1.039844 -2.141089 -0.781609  1.307600
2000-01-03 -0.462831  0.066505 -1.698346  1.123174
2000-01-04 -0.321971 -0.544599 -0.486099 -0.283791
2000-01-05  0.693749  0.544329 -1.606851  0.527733
2000-01-06 -2.461177 -0.339378 -0.236275  0.155569
2000-01-07 -0.597156  0.904511  0.369865  0.862504
2000-01-08 -0.958300 -0.583621 -2.068273  0.539434
</code></pre>

<p>If you don't already use the latest version of <code>pandas</code> I highly recommend upgrading. It is now possible to operate with DataFrames which contain duplicate indices.</p>
";;"[""import numpy as np\nimport pandas as pd\n\ndates = np.asarray(pd.date_range('1/1/2000', periods=8))\ndf1 = pd.DataFrame(np.random.randn(8, 4), index=dates, columns=['A', 'B', 'C', 'D'])\ndf2 = df1.copy()\ndf = df1.append(df2)\n"", '                   A         B         C         D\n2000-01-01 -0.327208  0.552500  0.862529  0.493109\n2000-01-02  1.039844 -2.141089 -0.781609  1.307600\n2000-01-03 -0.462831  0.066505 -1.698346  1.123174\n2000-01-04 -0.321971 -0.544599 -0.486099 -0.283791\n2000-01-05  0.693749  0.544329 -1.606851  0.527733\n2000-01-06 -2.461177 -0.339378 -0.236275  0.155569\n2000-01-07 -0.597156  0.904511  0.369865  0.862504\n2000-01-08 -0.958300 -0.583621 -2.068273  0.539434\n2000-01-01 -0.327208  0.552500  0.862529  0.493109\n2000-01-02  1.039844 -2.141089 -0.781609  1.307600\n2000-01-03 -0.462831  0.066505 -1.698346  1.123174\n2000-01-04 -0.321971 -0.544599 -0.486099 -0.283791\n2000-01-05  0.693749  0.544329 -1.606851  0.527733\n2000-01-06 -2.461177 -0.339378 -0.236275  0.155569\n2000-01-07 -0.597156  0.904511  0.369865  0.862504\n2000-01-08 -0.958300 -0.583621 -2.068273  0.539434\n']";"[""import numpy as np\nimport pandas as pd\n\ndates = np.asarray(pd.date_range('1/1/2000', periods=8))\ndf1 = pd.DataFrame(np.random.randn(8, 4), index=dates, columns=['A', 'B', 'C', 'D'])\ndf2 = df1.copy()\ndf = df1.append(df2)\n"", '                   A         B         C         D\n2000-01-01 -0.327208  0.552500  0.862529  0.493109\n2000-01-02  1.039844 -2.141089 -0.781609  1.307600\n2000-01-03 -0.462831  0.066505 -1.698346  1.123174\n2000-01-04 -0.321971 -0.544599 -0.486099 -0.283791\n2000-01-05  0.693749  0.544329 -1.606851  0.527733\n2000-01-06 -2.461177 -0.339378 -0.236275  0.155569\n2000-01-07 -0.597156  0.904511  0.369865  0.862504\n2000-01-08 -0.958300 -0.583621 -2.068273  0.539434\n2000-01-01 -0.327208  0.552500  0.862529  0.493109\n2000-01-02  1.039844 -2.141089 -0.781609  1.307600\n2000-01-03 -0.462831  0.066505 -1.698346  1.123174\n2000-01-04 -0.321971 -0.544599 -0.486099 -0.283791\n2000-01-05  0.693749  0.544329 -1.606851  0.527733\n2000-01-06 -2.461177 -0.339378 -0.236275  0.155569\n2000-01-07 -0.597156  0.904511  0.369865  0.862504\n2000-01-08 -0.958300 -0.583621 -2.068273  0.539434\n', 'pandas']"
710;;2;14992237;14991195.0;2;53;;;"<p>You can call <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.dropna.html"" rel=""noreferrer""><code>dropna</code></a> with arguments <code>subset</code> and <code>how</code>:</p>

<pre><code>df2.dropna(subset=['three', 'four', 'five'], how='all')
</code></pre>

<p>As the names suggests:</p>

<ul>
<li><code>how='all'</code> requires every column (of <code>subset</code>) in the row to be <code>NaN</code> in order to be dropped, as opposed to the default <code>'any'</code>.</li>
<li><code>subset</code> is those columns to inspect for <code>NaN</code>s.</li>
</ul>

<p>As <a href=""https://stackoverflow.com/users/1552748/paul-h"">@PaulH</a> points out, we can generalise to drop the last <code>k</code> columns with:</p>

<pre><code>subset=df2.columns[-k:]
</code></pre>

<p>Indeed, we could even do something more complicated if desired:</p>

<pre><code>subset=filter(lambda x: len(x) &gt; 3, df2.columns)
</code></pre>
";;"[""df2.dropna(subset=['three', 'four', 'five'], how='all')\n"", 'subset=df2.columns[-k:]\n', 'subset=filter(lambda x: len(x) > 3, df2.columns)\n']";"['dropna', 'subset', 'how', ""df2.dropna(subset=['three', 'four', 'five'], how='all')\n"", ""how='all'"", 'subset', 'NaN', ""'any'"", 'subset', 'NaN', 'k', 'subset=df2.columns[-k:]\n', 'subset=filter(lambda x: len(x) > 3, df2.columns)\n']"
711;;1;15006495;15006298.0;2;27;;;"<p>In this case, where the <code>DataFrame</code> is long but not too wide, you can simply slice it:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({""A"": range(1000), ""B"": range(1000)})
&gt;&gt;&gt; df
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 1000 entries, 0 to 999
Data columns:
A    1000  non-null values
B    1000  non-null values
dtypes: int64(2)
&gt;&gt;&gt; df[:5]
   A  B
0  0  0
1  1  1
2  2  2
3  3  3
4  4  4
</code></pre>

<p>If it's both wide and long, I tend to use <code>.ix</code>:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({i: range(1000) for i in range(100)})
&gt;&gt;&gt; df.ix[:5, :10]
   0   1   2   3   4   5   6   7   8   9   10
0   0   0   0   0   0   0   0   0   0   0   0
1   1   1   1   1   1   1   1   1   1   1   1
2   2   2   2   2   2   2   2   2   2   2   2
3   3   3   3   3   3   3   3   3   3   3   3
4   4   4   4   4   4   4   4   4   4   4   4
5   5   5   5   5   5   5   5   5   5   5   5
</code></pre>
";;"['>>> df = pd.DataFrame({""A"": range(1000), ""B"": range(1000)})\n>>> df\n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 1000 entries, 0 to 999\nData columns:\nA    1000  non-null values\nB    1000  non-null values\ndtypes: int64(2)\n>>> df[:5]\n   A  B\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n4  4  4\n', '>>> df = pd.DataFrame({i: range(1000) for i in range(100)})\n>>> df.ix[:5, :10]\n   0   1   2   3   4   5   6   7   8   9   10\n0   0   0   0   0   0   0   0   0   0   0   0\n1   1   1   1   1   1   1   1   1   1   1   1\n2   2   2   2   2   2   2   2   2   2   2   2\n3   3   3   3   3   3   3   3   3   3   3   3\n4   4   4   4   4   4   4   4   4   4   4   4\n5   5   5   5   5   5   5   5   5   5   5   5\n']";"['DataFrame', '>>> df = pd.DataFrame({""A"": range(1000), ""B"": range(1000)})\n>>> df\n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 1000 entries, 0 to 999\nData columns:\nA    1000  non-null values\nB    1000  non-null values\ndtypes: int64(2)\n>>> df[:5]\n   A  B\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n4  4  4\n', '.ix', '>>> df = pd.DataFrame({i: range(1000) for i in range(100)})\n>>> df.ix[:5, :10]\n   0   1   2   3   4   5   6   7   8   9   10\n0   0   0   0   0   0   0   0   0   0   0   0\n1   1   1   1   1   1   1   1   1   1   1   1\n2   2   2   2   2   2   2   2   2   2   2   2\n3   3   3   3   3   3   3   3   3   3   3   3\n4   4   4   4   4   4   4   4   4   4   4   4\n5   5   5   5   5   5   5   5   5   5   5   5\n']"
712;;1;15009160;15008970.0;2;48;;;"<p>I think you can use the <code>nrows</code> parameter.  From <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""noreferrer"">the docs</a>:</p>

<pre><code>nrows : int, default None

    Number of rows of file to read. Useful for reading pieces of large files
</code></pre>

<p>which seems to work.  Using one of the standard large test files (988504479 bytes, 5344499 lines):</p>

<pre><code>In [1]: import pandas as pd

In [2]: time z = pd.read_csv(""P00000001-ALL.csv"", nrows=20)
CPU times: user 0.00 s, sys: 0.00 s, total: 0.00 s
Wall time: 0.00 s

In [3]: len(z)
Out[3]: 20

In [4]: time z = pd.read_csv(""P00000001-ALL.csv"")
CPU times: user 27.63 s, sys: 1.92 s, total: 29.55 s
Wall time: 30.23 s
</code></pre>
";;"['nrows : int, default None\n\n    Number of rows of file to read. Useful for reading pieces of large files\n', 'In [1]: import pandas as pd\n\nIn [2]: time z = pd.read_csv(""P00000001-ALL.csv"", nrows=20)\nCPU times: user 0.00 s, sys: 0.00 s, total: 0.00 s\nWall time: 0.00 s\n\nIn [3]: len(z)\nOut[3]: 20\n\nIn [4]: time z = pd.read_csv(""P00000001-ALL.csv"")\nCPU times: user 27.63 s, sys: 1.92 s, total: 29.55 s\nWall time: 30.23 s\n']";"['nrows', 'nrows : int, default None\n\n    Number of rows of file to read. Useful for reading pieces of large files\n', 'In [1]: import pandas as pd\n\nIn [2]: time z = pd.read_csv(""P00000001-ALL.csv"", nrows=20)\nCPU times: user 0.00 s, sys: 0.00 s, total: 0.00 s\nWall time: 0.00 s\n\nIn [3]: len(z)\nOut[3]: 20\n\nIn [4]: time z = pd.read_csv(""P00000001-ALL.csv"")\nCPU times: user 27.63 s, sys: 1.92 s, total: 29.55 s\nWall time: 30.23 s\n']"
713;;1;15026839;15026698.0;2;52;;;"<p>From the <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html"">documentation</a>, you can use either a regex or <code>delim_whitespace</code>:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; for line in open(""whitespace.csv""):
...     print repr(line)
...     
'a\t  b\tc 1 2\n'
'd\t  e\tf 3 4\n'
&gt;&gt;&gt; pd.read_csv(""whitespace.csv"", header=None, delimiter=r""\s+"")
   0  1  2  3  4
0  a  b  c  1  2
1  d  e  f  3  4
&gt;&gt;&gt; pd.read_csv(""whitespace.csv"", header=None, delim_whitespace=True)
   0  1  2  3  4
0  a  b  c  1  2
1  d  e  f  3  4
</code></pre>
";;"['>>> import pandas as pd\n>>> for line in open(""whitespace.csv""):\n...     print repr(line)\n...     \n\'a\\t  b\\tc 1 2\\n\'\n\'d\\t  e\\tf 3 4\\n\'\n>>> pd.read_csv(""whitespace.csv"", header=None, delimiter=r""\\s+"")\n   0  1  2  3  4\n0  a  b  c  1  2\n1  d  e  f  3  4\n>>> pd.read_csv(""whitespace.csv"", header=None, delim_whitespace=True)\n   0  1  2  3  4\n0  a  b  c  1  2\n1  d  e  f  3  4\n']";"['delim_whitespace', '>>> import pandas as pd\n>>> for line in open(""whitespace.csv""):\n...     print repr(line)\n...     \n\'a\\t  b\\tc 1 2\\n\'\n\'d\\t  e\\tf 3 4\\n\'\n>>> pd.read_csv(""whitespace.csv"", header=None, delimiter=r""\\s+"")\n   0  1  2  3  4\n0  a  b  c  1  2\n1  d  e  f  3  4\n>>> pd.read_csv(""whitespace.csv"", header=None, delim_whitespace=True)\n   0  1  2  3  4\n0  a  b  c  1  2\n1  d  e  f  3  4\n']"
714;;1;15030455;15017072.0;2;7;;;"<p>This code achieves what you want --- also its weird and certainly buggy:</p>

<p>I observed that it works when:</p>

<p>a) you specify the <code>index_col</code> rel. to the number of columns you really use -- so its three columns in this example, not four (you drop <code>dummy</code> and start counting from then onwards)</p>

<p>b) same for <code>parse_dates</code></p>

<p>c) not so for <code>usecols</code> ;) for obvious reasons</p>

<p>d) here I adapted the <code>names</code> to mirror this behaviour</p>

<pre><code>import pandas as pd
from StringIO import StringIO

csv = """"""dummy,date,loc,x
bar,20090101,a,1
bar,20090102,a,3
bar,20090103,a,5
bar,20090101,b,1
bar,20090102,b,3
bar,20090103,b,5
""""""

df = pd.read_csv(StringIO(csv),
        index_col=[0,1],
        usecols=[1,2,3], 
        parse_dates=[0],
        header=0,
        names=[""date"", ""loc"", """", ""x""])

print df
</code></pre>

<p>which prints</p>

<pre><code>                x
date       loc   
2009-01-01 a    1
2009-01-02 a    3
2009-01-03 a    5
2009-01-01 b    1
2009-01-02 b    3
2009-01-03 b    5
</code></pre>
";;"['import pandas as pd\nfrom StringIO import StringIO\n\ncsv = """"""dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nbar,20090103,b,5\n""""""\n\ndf = pd.read_csv(StringIO(csv),\n        index_col=[0,1],\n        usecols=[1,2,3], \n        parse_dates=[0],\n        header=0,\n        names=[""date"", ""loc"", """", ""x""])\n\nprint df\n', '                x\ndate       loc   \n2009-01-01 a    1\n2009-01-02 a    3\n2009-01-03 a    5\n2009-01-01 b    1\n2009-01-02 b    3\n2009-01-03 b    5\n']";"['index_col', 'dummy', 'parse_dates', 'usecols', 'names', 'import pandas as pd\nfrom StringIO import StringIO\n\ncsv = """"""dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nbar,20090103,b,5\n""""""\n\ndf = pd.read_csv(StringIO(csv),\n        index_col=[0,1],\n        usecols=[1,2,3], \n        parse_dates=[0],\n        header=0,\n        names=[""date"", ""loc"", """", ""x""])\n\nprint df\n', '                x\ndate       loc   \n2009-01-01 a    1\n2009-01-02 a    3\n2009-01-03 a    5\n2009-01-01 b    1\n2009-01-02 b    3\n2009-01-03 b    5\n']"
715;;2;15097125;10065051.0;2;22;;;"<p>For the record, here is an example using a sqlite database:</p>

<pre><code>import pandas as pd
import sqlite3

with sqlite3.connect(""whatever.sqlite"") as con:
    sql = ""SELECT * FROM table_name""
    df = pd.read_sql_query(sql, con)
    print df.shape
</code></pre>
";;"['import pandas as pd\nimport sqlite3\n\nwith sqlite3.connect(""whatever.sqlite"") as con:\n    sql = ""SELECT * FROM table_name""\n    df = pd.read_sql_query(sql, con)\n    print df.shape\n']";"['import pandas as pd\nimport sqlite3\n\nwith sqlite3.connect(""whatever.sqlite"") as con:\n    sql = ""SELECT * FROM table_name""\n    df = pd.read_sql_query(sql, con)\n    print df.shape\n']"
716;;0;15100193;15017072.0;2;8;;;"<p>If your csv file contains extra data, columns can be <a href=""http://pandas.pydata.org/pandas-docs/dev/dsintro.html#column-selection-addition-deletion"">deleted</a> from the DataFrame after import.   </p>

<pre><code>import pandas as pd
from StringIO import StringIO

csv = r""""""dummy,date,loc,x
bar,20090101,a,1
bar,20090102,a,3
bar,20090103,a,5
bar,20090101,b,1
bar,20090102,b,3
bar,20090103,b,5""""""

df = pd.read_csv(StringIO(csv),
        index_col=[""date"", ""loc""], 
        usecols=[""dummy"", ""date"", ""loc"", ""x""],
        parse_dates=[""date""],
        header=0,
        names=[""dummy"", ""date"", ""loc"", ""x""])
del df['dummy']
</code></pre>

<p>Which gives us:</p>

<pre><code>                x
date       loc
2009-01-01 a    1
2009-01-02 a    3
2009-01-03 a    5
2009-01-01 b    1
2009-01-02 b    3
2009-01-03 b    5
</code></pre>
";;"['import pandas as pd\nfrom StringIO import StringIO\n\ncsv = r""""""dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nbar,20090103,b,5""""""\n\ndf = pd.read_csv(StringIO(csv),\n        index_col=[""date"", ""loc""], \n        usecols=[""dummy"", ""date"", ""loc"", ""x""],\n        parse_dates=[""date""],\n        header=0,\n        names=[""dummy"", ""date"", ""loc"", ""x""])\ndel df[\'dummy\']\n', '                x\ndate       loc\n2009-01-01 a    1\n2009-01-02 a    3\n2009-01-03 a    5\n2009-01-01 b    1\n2009-01-02 b    3\n2009-01-03 b    5\n']";"['import pandas as pd\nfrom StringIO import StringIO\n\ncsv = r""""""dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nbar,20090103,b,5""""""\n\ndf = pd.read_csv(StringIO(csv),\n        index_col=[""date"", ""loc""], \n        usecols=[""dummy"", ""date"", ""loc"", ""x""],\n        parse_dates=[""date""],\n        header=0,\n        names=[""dummy"", ""date"", ""loc"", ""x""])\ndel df[\'dummy\']\n', '                x\ndate       loc\n2009-01-01 a    1\n2009-01-02 a    3\n2009-01-03 a    5\n2009-01-01 b    1\n2009-01-02 b    3\n2009-01-03 b    5\n']"
717;;2;15125793;15118111.0;2;18;;;"<p>To make the first approach work, try returning a Series instead of a tuple (apply is throwing an exception because it doesn't know how to glue the rows back together as the number of columns doesn't match the original frame).</p>

<pre><code>def calculate(s):
    a = s['path'] + 2*s['row'] # Simple calc for example
    b = s['path'] * 0.153
    return pd.Series(dict(col1=a, col2=b))
</code></pre>

<p>The second approach should work if you replace:</p>

<pre><code>st.ix[i]['a'] = a
</code></pre>

<p>with:</p>

<pre><code>st.ix[i, 'a'] = a
</code></pre>
";;"[""def calculate(s):\n    a = s['path'] + 2*s['row'] # Simple calc for example\n    b = s['path'] * 0.153\n    return pd.Series(dict(col1=a, col2=b))\n"", ""st.ix[i]['a'] = a\n"", ""st.ix[i, 'a'] = a\n""]";"[""def calculate(s):\n    a = s['path'] + 2*s['row'] # Simple calc for example\n    b = s['path'] * 0.153\n    return pd.Series(dict(col1=a, col2=b))\n"", ""st.ix[i]['a'] = a\n"", ""st.ix[i, 'a'] = a\n""]"
718;;2;15203886;15203623.0;2;23;;;"<p>Note: Timestamp is just unix time with nanoseconds (so divide it by 10**9):</p>

<pre class=""lang-py prettyprint-override""><code>[t.value // 10 ** 9 for t in tsframe.index]
</code></pre>

<p>For example:</p>

<pre><code>In [1]: t = pd.Timestamp('2000-02-11 00:00:00')

In [2]: t
Out[2]: &lt;Timestamp: 2000-02-11 00:00:00&gt;

In [3]: t.value
Out[3]: 950227200000000000L

In [4]: time.mktime(t.timetuple())
Out[4]: 950227200.0
</code></pre>

<p>As @root points out it's faster to extract the array of values directly:</p>

<pre class=""lang-py prettyprint-override""><code>tsframe.index.astype(np.int64) // 10 ** 9
</code></pre>
";;"['[t.value // 10 ** 9 for t in tsframe.index]\n', ""In [1]: t = pd.Timestamp('2000-02-11 00:00:00')\n\nIn [2]: t\nOut[2]: <Timestamp: 2000-02-11 00:00:00>\n\nIn [3]: t.value\nOut[3]: 950227200000000000L\n\nIn [4]: time.mktime(t.timetuple())\nOut[4]: 950227200.0\n"", 'tsframe.index.astype(np.int64) // 10 ** 9\n']";"['[t.value // 10 ** 9 for t in tsframe.index]\n', ""In [1]: t = pd.Timestamp('2000-02-11 00:00:00')\n\nIn [2]: t\nOut[2]: <Timestamp: 2000-02-11 00:00:00>\n\nIn [3]: t.value\nOut[3]: 950227200000000000L\n\nIn [4]: time.mktime(t.timetuple())\nOut[4]: 950227200.0\n"", 'tsframe.index.astype(np.int64) // 10 ** 9\n']"
719;;2;15204235;15203623.0;2;50;;;"<p>As <code>DatetimeIndex</code> is <code>ndarray</code> under the hood, you can do the conversion without a comprehension (much faster).</p>

<pre class=""lang-py prettyprint-override""><code>In [1]: import numpy as np

In [2]: import pandas as pd

In [3]: from datetime import datetime

In [4]: dates = [datetime(2012, 5, 1), datetime(2012, 5, 2), datetime(2012, 5, 3)]
   ...: index = pd.DatetimeIndex(dates)
   ...: 
In [5]: index.astype(np.int64)
Out[5]: array([1335830400000000000, 1335916800000000000, 1336003200000000000], 
        dtype=int64)

In [6]: index.astype(np.int64) // 10**9
Out[6]: array([1335830400, 1335916800, 1336003200], dtype=int64)

%timeit [t.value // 10 ** 9 for t in index]
10000 loops, best of 3: 119 us per loop

%timeit index.astype(np.int64) // 10**9
100000 loops, best of 3: 18.4 us per loop
</code></pre>
";;['In [1]: import numpy as np\n\nIn [2]: import pandas as pd\n\nIn [3]: from datetime import datetime\n\nIn [4]: dates = [datetime(2012, 5, 1), datetime(2012, 5, 2), datetime(2012, 5, 3)]\n   ...: index = pd.DatetimeIndex(dates)\n   ...: \nIn [5]: index.astype(np.int64)\nOut[5]: array([1335830400000000000, 1335916800000000000, 1336003200000000000], \n        dtype=int64)\n\nIn [6]: index.astype(np.int64) // 10**9\nOut[6]: array([1335830400, 1335916800, 1336003200], dtype=int64)\n\n%timeit [t.value // 10 ** 9 for t in index]\n10000 loops, best of 3: 119 us per loop\n\n%timeit index.astype(np.int64) // 10**9\n100000 loops, best of 3: 18.4 us per loop\n'];['DatetimeIndex', 'ndarray', 'In [1]: import numpy as np\n\nIn [2]: import pandas as pd\n\nIn [3]: from datetime import datetime\n\nIn [4]: dates = [datetime(2012, 5, 1), datetime(2012, 5, 2), datetime(2012, 5, 3)]\n   ...: index = pd.DatetimeIndex(dates)\n   ...: \nIn [5]: index.astype(np.int64)\nOut[5]: array([1335830400000000000, 1335916800000000000, 1336003200000000000], \n        dtype=int64)\n\nIn [6]: index.astype(np.int64) // 10**9\nOut[6]: array([1335830400, 1335916800, 1336003200], dtype=int64)\n\n%timeit [t.value // 10 ** 9 for t in index]\n10000 loops, best of 3: 119 us per loop\n\n%timeit index.astype(np.int64) // 10**9\n100000 loops, best of 3: 18.4 us per loop\n']
720;;5;15213171;15210962.0;2;18;;;"<p>0.10.1 doesn't really support float32 very much</p>

<p>see this <a href=""http://pandas.pydata.org/pandas-docs/dev/whatsnew.html#dtype-specification"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/dev/whatsnew.html#dtype-specification</a></p>

<p>you can do this in 0.11 like this:</p>

<pre><code># dont' use dtype converters explicity for the columns you care about
# they will be converted to float64 if possible, or object if they cannot
df = pd.read_csv('test.csv'.....)

#### this is optional and related to the issue you posted ####
# force anything that is not a numeric to nan
# columns are the list of columns that you are interesetd in
df[columns] = df[columns].convert_objects(convert_numeric=True)


    # astype
    df[columns] = df[columns].astype('float32')

see http://pandas.pydata.org/pandas-docs/dev/basics.html#object-conversion

Its not as efficient as doing it directly in read_csv (but that requires
</code></pre>

<p>I have confirmed that with 0.11-dev, this DOES work (on 32-bit and 64-bit, results are the same)</p>

<pre><code>In [5]: x = pd.read_csv(StringIO.StringIO(data), dtype={'a': np.float32}, delim_whitespace=True)

In [6]: x
Out[6]: 
         a        b
0  0.76398  0.81394
1  0.32136  0.91063

In [7]: x.dtypes
Out[7]: 
a    float32
b    float64
dtype: object

In [8]: pd.__version__
Out[8]: '0.11.0.dev-385ff82'

In [9]: quit()
vagrant@precise32:~/pandas$ uname -a
Linux precise32 3.2.0-23-generic-pae #36-Ubuntu SMP Tue Apr 10 22:19:09 UTC 2012 i686 i686 i386 GNU/Linux

 some low-level changes)
</code></pre>
";;"[""# dont' use dtype converters explicity for the columns you care about\n# they will be converted to float64 if possible, or object if they cannot\ndf = pd.read_csv('test.csv'.....)\n\n#### this is optional and related to the issue you posted ####\n# force anything that is not a numeric to nan\n# columns are the list of columns that you are interesetd in\ndf[columns] = df[columns].convert_objects(convert_numeric=True)\n\n\n    # astype\n    df[columns] = df[columns].astype('float32')\n\nsee http://pandas.pydata.org/pandas-docs/dev/basics.html#object-conversion\n\nIts not as efficient as doing it directly in read_csv (but that requires\n"", ""In [5]: x = pd.read_csv(StringIO.StringIO(data), dtype={'a': np.float32}, delim_whitespace=True)\n\nIn [6]: x\nOut[6]: \n         a        b\n0  0.76398  0.81394\n1  0.32136  0.91063\n\nIn [7]: x.dtypes\nOut[7]: \na    float32\nb    float64\ndtype: object\n\nIn [8]: pd.__version__\nOut[8]: '0.11.0.dev-385ff82'\n\nIn [9]: quit()\nvagrant@precise32:~/pandas$ uname -a\nLinux precise32 3.2.0-23-generic-pae #36-Ubuntu SMP Tue Apr 10 22:19:09 UTC 2012 i686 i686 i386 GNU/Linux\n\n some low-level changes)\n""]";"[""# dont' use dtype converters explicity for the columns you care about\n# they will be converted to float64 if possible, or object if they cannot\ndf = pd.read_csv('test.csv'.....)\n\n#### this is optional and related to the issue you posted ####\n# force anything that is not a numeric to nan\n# columns are the list of columns that you are interesetd in\ndf[columns] = df[columns].convert_objects(convert_numeric=True)\n\n\n    # astype\n    df[columns] = df[columns].astype('float32')\n\nsee http://pandas.pydata.org/pandas-docs/dev/basics.html#object-conversion\n\nIts not as efficient as doing it directly in read_csv (but that requires\n"", ""In [5]: x = pd.read_csv(StringIO.StringIO(data), dtype={'a': np.float32}, delim_whitespace=True)\n\nIn [6]: x\nOut[6]: \n         a        b\n0  0.76398  0.81394\n1  0.32136  0.91063\n\nIn [7]: x.dtypes\nOut[7]: \na    float32\nb    float64\ndtype: object\n\nIn [8]: pd.__version__\nOut[8]: '0.11.0.dev-385ff82'\n\nIn [9]: quit()\nvagrant@precise32:~/pandas$ uname -a\nLinux precise32 3.2.0-23-generic-pae #36-Ubuntu SMP Tue Apr 10 22:19:09 UTC 2012 i686 i686 i386 GNU/Linux\n\n some low-level changes)\n""]"
721;;5;15220374;15210962.0;2;6;;;"<pre><code>In [22]: df.a.dtype = pd.np.float32

In [23]: df.a.dtype
Out[23]: dtype('float32')
</code></pre>

<p>the above works fine for me under pandas 0.10.1</p>
";;"[""In [22]: df.a.dtype = pd.np.float32\n\nIn [23]: df.a.dtype\nOut[23]: dtype('float32')\n""]";"[""In [22]: df.a.dtype = pd.np.float32\n\nIn [23]: df.a.dtype\nOut[23]: dtype('float32')\n""]"
722;;3;15222976;15222754.0;2;7;;;"<p>For <code>agg</code>, the lambba function gets a <code>Series</code>, which does not have a <code>'Short name'</code> attribute.</p>

<p><code>stats.mode</code> returns a tuple of two arrays, so you have to take the first element of the first array in this tuple.</p>

<p>With these two simple changements:</p>

<pre><code>source.groupby(['Country','City']).agg(lambda x: stats.mode(x)[0][0])
</code></pre>

<p>returns</p>

<pre><code>                         Short name
Country City                       
Russia  Sankt-Petersburg        Spb
USA     New-York                 NY
</code></pre>
";;"[""source.groupby(['Country','City']).agg(lambda x: stats.mode(x)[0][0])\n"", '                         Short name\nCountry City                       \nRussia  Sankt-Petersburg        Spb\nUSA     New-York                 NY\n']";"['agg', 'Series', ""'Short name'"", 'stats.mode', ""source.groupby(['Country','City']).agg(lambda x: stats.mode(x)[0][0])\n"", '                         Short name\nCountry City                       \nRussia  Sankt-Petersburg        Spb\nUSA     New-York                 NY\n']"
723;;3;15223034;15222754.0;2;52;;;"<p>You can use <code>value_counts()</code> to get a count series, and get the first row:</p>

<pre><code>import pandas as pd

source = pd.DataFrame({'Country' : ['USA', 'USA', 'Russia','USA'], 
                  'City' : ['New-York', 'New-York', 'Sankt-Petersburg', 'New-York'],
                  'Short name' : ['NY','New','Spb','NY']})

source.groupby(['Country','City']).agg(lambda x:x.value_counts().index[0])
</code></pre>
";;"[""import pandas as pd\n\nsource = pd.DataFrame({'Country' : ['USA', 'USA', 'Russia','USA'], \n                  'City' : ['New-York', 'New-York', 'Sankt-Petersburg', 'New-York'],\n                  'Short name' : ['NY','New','Spb','NY']})\n\nsource.groupby(['Country','City']).agg(lambda x:x.value_counts().index[0])\n""]";"['value_counts()', ""import pandas as pd\n\nsource = pd.DataFrame({'Country' : ['USA', 'USA', 'Russia','USA'], \n                  'City' : ['New-York', 'New-York', 'Sankt-Petersburg', 'New-York'],\n                  'Short name' : ['NY','New','Spb','NY']})\n\nsource.groupby(['Country','City']).agg(lambda x:x.value_counts().index[0])\n""]"
724;;0;15244074;15242746.0;2;7;;;"<p>I'd also be interested to know if this is possible, from the doc it doesn't seem to be the case. What you could probably do is read the file line by line, and concatenate each reading to a DataFrame:</p>

<pre><code>import pandas as pd

df = pd.DataFrame()

with open(filepath, 'r') as f:
    for line in f:
        df = pd.concat( [df, pd.DataFrame([tuple(line.strip().split(','))])], ignore_index=True )
</code></pre>

<p>It works but not in the most elegant way, I guess...</p>
";;"[""import pandas as pd\n\ndf = pd.DataFrame()\n\nwith open(filepath, 'r') as f:\n    for line in f:\n        df = pd.concat( [df, pd.DataFrame([tuple(line.strip().split(','))])], ignore_index=True )\n""]";"[""import pandas as pd\n\ndf = pd.DataFrame()\n\nwith open(filepath, 'r') as f:\n    for line in f:\n        df = pd.concat( [df, pd.DataFrame([tuple(line.strip().split(','))])], ignore_index=True )\n""]"
725;;4;15252012;15242746.0;2;42;;;"<p>One way which seems to work (at least in 0.10.1 and 0.11.0.dev-fc8de6d):</p>

<pre><code>&gt;&gt;&gt; !cat ragged.csv
1,2,3
1,2,3,4
1,2,3,4,5
1,2
1,2,3,4
&gt;&gt;&gt; my_cols = [""A"", ""B"", ""C"", ""D"", ""E""]
&gt;&gt;&gt; pd.read_csv(""ragged.csv"", names=my_cols, engine='python')
   A  B   C   D   E
0  1  2   3 NaN NaN
1  1  2   3   4 NaN
2  1  2   3   4   5
3  1  2 NaN NaN NaN
4  1  2   3   4 NaN
</code></pre>

<p>Note that this approach requires that you give names to the columns you want, though.  Not as general as some other ways, but works well enough when it applies.</p>
";;"['>>> !cat ragged.csv\n1,2,3\n1,2,3,4\n1,2,3,4,5\n1,2\n1,2,3,4\n>>> my_cols = [""A"", ""B"", ""C"", ""D"", ""E""]\n>>> pd.read_csv(""ragged.csv"", names=my_cols, engine=\'python\')\n   A  B   C   D   E\n0  1  2   3 NaN NaN\n1  1  2   3   4 NaN\n2  1  2   3   4   5\n3  1  2 NaN NaN NaN\n4  1  2   3   4 NaN\n']";"['>>> !cat ragged.csv\n1,2,3\n1,2,3,4\n1,2,3,4,5\n1,2\n1,2,3,4\n>>> my_cols = [""A"", ""B"", ""C"", ""D"", ""E""]\n>>> pd.read_csv(""ragged.csv"", names=my_cols, engine=\'python\')\n   A  B   C   D   E\n0  1  2   3 NaN NaN\n1  1  2   3   4 NaN\n2  1  2   3   4   5\n3  1  2 NaN NaN NaN\n4  1  2   3   4 NaN\n']"
726;;5;15315507;15315452.0;2;156;;;"<p>Sure!  Setup:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; from random import randint
&gt;&gt;&gt; df = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],
                   'B': [randint(1, 9)*10 for x in xrange(10)],
                   'C': [randint(1, 9)*100 for x in xrange(10)]})
&gt;&gt;&gt; df
   A   B    C
0  9  40  300
1  9  70  700
2  5  70  900
3  8  80  900
4  7  50  200
5  9  30  900
6  2  80  700
7  2  80  400
8  5  80  300
9  7  70  800
</code></pre>

<p>We can apply column operations and get boolean Series objects:</p>

<pre><code>&gt;&gt;&gt; df[""B""] &gt; 50
0    False
1     True
2     True
3     True
4    False
5    False
6     True
7     True
8     True
9     True
Name: B
&gt;&gt;&gt; (df[""B""] &gt; 50) &amp; (df[""C""] == 900)
0    False
1    False
2     True
3     True
4    False
5    False
6    False
7    False
8    False
9    False
</code></pre>

<p>[Update, to switch to new-style <code>.loc</code>]:</p>

<p>And then we can use these to index into the object.  For read access, you can chain indices:</p>

<pre><code>&gt;&gt;&gt; df[""A""][(df[""B""] &gt; 50) &amp; (df[""C""] == 900)]
2    5
3    8
Name: A, dtype: int64
</code></pre>

<p>but you can get yourself into trouble because of the difference between a view and a copy doing this for write access.  You can use <code>.loc</code> instead:</p>

<pre><code>&gt;&gt;&gt; df.loc[(df[""B""] &gt; 50) &amp; (df[""C""] == 900), ""A""]
2    5
3    8
Name: A, dtype: int64
&gt;&gt;&gt; df.loc[(df[""B""] &gt; 50) &amp; (df[""C""] == 900), ""A""].values
array([5, 8], dtype=int64)
&gt;&gt;&gt; df.loc[(df[""B""] &gt; 50) &amp; (df[""C""] == 900), ""A""] *= 1000
&gt;&gt;&gt; df
      A   B    C
0     9  40  300
1     9  70  700
2  5000  70  900
3  8000  80  900
4     7  50  200
5     9  30  900
6     2  80  700
7     2  80  400
8     5  80  300
9     7  70  800
</code></pre>

<p>Note that I accidentally did <code>== 900</code> and not <code>!= 900</code>, or <code>~(df[""C""] == 900)</code>, but I'm too lazy to fix it. Exercise for the reader. :^)</p>
";;"["">>> import pandas as pd\n>>> from random import randint\n>>> df = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],\n                   'B': [randint(1, 9)*10 for x in xrange(10)],\n                   'C': [randint(1, 9)*100 for x in xrange(10)]})\n>>> df\n   A   B    C\n0  9  40  300\n1  9  70  700\n2  5  70  900\n3  8  80  900\n4  7  50  200\n5  9  30  900\n6  2  80  700\n7  2  80  400\n8  5  80  300\n9  7  70  800\n"", '>>> df[""B""] > 50\n0    False\n1     True\n2     True\n3     True\n4    False\n5    False\n6     True\n7     True\n8     True\n9     True\nName: B\n>>> (df[""B""] > 50) & (df[""C""] == 900)\n0    False\n1    False\n2     True\n3     True\n4    False\n5    False\n6    False\n7    False\n8    False\n9    False\n', '>>> df[""A""][(df[""B""] > 50) & (df[""C""] == 900)]\n2    5\n3    8\nName: A, dtype: int64\n', '>>> df.loc[(df[""B""] > 50) & (df[""C""] == 900), ""A""]\n2    5\n3    8\nName: A, dtype: int64\n>>> df.loc[(df[""B""] > 50) & (df[""C""] == 900), ""A""].values\narray([5, 8], dtype=int64)\n>>> df.loc[(df[""B""] > 50) & (df[""C""] == 900), ""A""] *= 1000\n>>> df\n      A   B    C\n0     9  40  300\n1     9  70  700\n2  5000  70  900\n3  8000  80  900\n4     7  50  200\n5     9  30  900\n6     2  80  700\n7     2  80  400\n8     5  80  300\n9     7  70  800\n']";"["">>> import pandas as pd\n>>> from random import randint\n>>> df = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],\n                   'B': [randint(1, 9)*10 for x in xrange(10)],\n                   'C': [randint(1, 9)*100 for x in xrange(10)]})\n>>> df\n   A   B    C\n0  9  40  300\n1  9  70  700\n2  5  70  900\n3  8  80  900\n4  7  50  200\n5  9  30  900\n6  2  80  700\n7  2  80  400\n8  5  80  300\n9  7  70  800\n"", '>>> df[""B""] > 50\n0    False\n1     True\n2     True\n3     True\n4    False\n5    False\n6     True\n7     True\n8     True\n9     True\nName: B\n>>> (df[""B""] > 50) & (df[""C""] == 900)\n0    False\n1    False\n2     True\n3     True\n4    False\n5    False\n6    False\n7    False\n8    False\n9    False\n', '.loc', '>>> df[""A""][(df[""B""] > 50) & (df[""C""] == 900)]\n2    5\n3    8\nName: A, dtype: int64\n', '.loc', '>>> df.loc[(df[""B""] > 50) & (df[""C""] == 900), ""A""]\n2    5\n3    8\nName: A, dtype: int64\n>>> df.loc[(df[""B""] > 50) & (df[""C""] == 900), ""A""].values\narray([5, 8], dtype=int64)\n>>> df.loc[(df[""B""] > 50) & (df[""C""] == 900), ""A""] *= 1000\n>>> df\n      A   B    C\n0     9  40  300\n1     9  70  700\n2  5000  70  900\n3  8000  80  900\n4     7  50  200\n5     9  30  900\n6     2  80  700\n7     2  80  400\n8     5  80  300\n9     7  70  800\n', '== 900', '!= 900', '~(df[""C""] == 900)']"
727;;2;15322715;15322632.0;2;63;;;"<p><code>agg</code> is the same as <code>aggregate</code>. It's callable is passed the columns (<code>Series</code> objects) of the <code>DataFrame</code>, one at a time.</p>

<hr>

<p>You could use <code>idxmax</code> to collect the index labels of the rows with the maximum
count:</p>

<pre><code>idx = df.groupby('word')['count'].idxmax()
print(idx)
</code></pre>

<p>yields</p>

<pre><code>word
a       2
an      3
the     1
Name: count
</code></pre>

<p>and then use <code>loc</code> to select those rows in the <code>word</code> and <code>tag</code> columns:</p>

<pre><code>print(df.loc[idx, ['word', 'tag']])
</code></pre>

<p>yields</p>

<pre><code>  word tag
2    a   T
3   an   T
1  the   S
</code></pre>

<p>Note that <code>idxmax</code> returns index <em>labels</em>. <code>df.loc</code> can be used to select rows
by label. But if the index is not unique -- that is, if there are rows with duplicate index labels -- then <code>df.loc</code> will select <em>all rows</em> with the labels listed in <code>idx</code>. So be careful that <code>df.index.is_unique</code> is <code>True</code> if you use <code>idxmax</code> with <code>df.loc</code></p>

<hr>

<p>Alternative, you could use <code>apply</code>. <code>apply</code>'s callable is passed a sub-DataFrame which gives you access to all the columns:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'word':'a the a an the'.split(),
                   'tag': list('SSTTT'),
                   'count': [30, 20, 60, 5, 10]})

print(df.groupby('word').apply(lambda subf: subf['tag'][subf['count'].idxmax()]))
</code></pre>

<p>yields</p>

<pre><code>word
a       T
an      T
the     S
</code></pre>

<hr>

<p>Using <code>idxmax</code> and <code>loc</code> is typically faster than <code>apply</code>, especially for large DataFrames. Using IPython's %timeit:</p>

<pre><code>N = 10000
df = pd.DataFrame({'word':'a the a an the'.split()*N,
                   'tag': list('SSTTT')*N,
                   'count': [30, 20, 60, 5, 10]*N})
def using_apply(df):
    return (df.groupby('word').apply(lambda subf: subf['tag'][subf['count'].idxmax()]))

def using_idxmax_loc(df):
    idx = df.groupby('word')['count'].idxmax()
    return df.loc[idx, ['word', 'tag']]

In [22]: %timeit using_apply(df)
100 loops, best of 3: 7.68 ms per loop

In [23]: %timeit using_idxmax_loc(df)
100 loops, best of 3: 5.43 ms per loop
</code></pre>

<hr>

<p>If you want a dictionary mapping words to tags, then you could use <code>set_index</code>
and <code>to_dict</code> like this:</p>

<pre><code>In [36]: df2 = df.loc[idx, ['word', 'tag']].set_index('word')

In [37]: df2
Out[37]: 
     tag
word    
a      T
an     T
the    S

In [38]: df2.to_dict()['tag']
Out[38]: {'a': 'T', 'an': 'T', 'the': 'S'}
</code></pre>
";;"[""idx = df.groupby('word')['count'].idxmax()\nprint(idx)\n"", 'word\na       2\nan      3\nthe     1\nName: count\n', ""print(df.loc[idx, ['word', 'tag']])\n"", '  word tag\n2    a   T\n3   an   T\n1  the   S\n', ""import pandas as pd\ndf = pd.DataFrame({'word':'a the a an the'.split(),\n                   'tag': list('SSTTT'),\n                   'count': [30, 20, 60, 5, 10]})\n\nprint(df.groupby('word').apply(lambda subf: subf['tag'][subf['count'].idxmax()]))\n"", 'word\na       T\nan      T\nthe     S\n', ""N = 10000\ndf = pd.DataFrame({'word':'a the a an the'.split()*N,\n                   'tag': list('SSTTT')*N,\n                   'count': [30, 20, 60, 5, 10]*N})\ndef using_apply(df):\n    return (df.groupby('word').apply(lambda subf: subf['tag'][subf['count'].idxmax()]))\n\ndef using_idxmax_loc(df):\n    idx = df.groupby('word')['count'].idxmax()\n    return df.loc[idx, ['word', 'tag']]\n\nIn [22]: %timeit using_apply(df)\n100 loops, best of 3: 7.68 ms per loop\n\nIn [23]: %timeit using_idxmax_loc(df)\n100 loops, best of 3: 5.43 ms per loop\n"", ""In [36]: df2 = df.loc[idx, ['word', 'tag']].set_index('word')\n\nIn [37]: df2\nOut[37]: \n     tag\nword    \na      T\nan     T\nthe    S\n\nIn [38]: df2.to_dict()['tag']\nOut[38]: {'a': 'T', 'an': 'T', 'the': 'S'}\n""]";"['agg', 'aggregate', 'Series', 'DataFrame', 'idxmax', ""idx = df.groupby('word')['count'].idxmax()\nprint(idx)\n"", 'word\na       2\nan      3\nthe     1\nName: count\n', 'loc', 'word', 'tag', ""print(df.loc[idx, ['word', 'tag']])\n"", '  word tag\n2    a   T\n3   an   T\n1  the   S\n', 'idxmax', 'df.loc', 'df.loc', 'idx', 'df.index.is_unique', 'True', 'idxmax', 'df.loc', 'apply', 'apply', ""import pandas as pd\ndf = pd.DataFrame({'word':'a the a an the'.split(),\n                   'tag': list('SSTTT'),\n                   'count': [30, 20, 60, 5, 10]})\n\nprint(df.groupby('word').apply(lambda subf: subf['tag'][subf['count'].idxmax()]))\n"", 'word\na       T\nan      T\nthe     S\n', 'idxmax', 'loc', 'apply', ""N = 10000\ndf = pd.DataFrame({'word':'a the a an the'.split()*N,\n                   'tag': list('SSTTT')*N,\n                   'count': [30, 20, 60, 5, 10]*N})\ndef using_apply(df):\n    return (df.groupby('word').apply(lambda subf: subf['tag'][subf['count'].idxmax()]))\n\ndef using_idxmax_loc(df):\n    idx = df.groupby('word')['count'].idxmax()\n    return df.loc[idx, ['word', 'tag']]\n\nIn [22]: %timeit using_apply(df)\n100 loops, best of 3: 7.68 ms per loop\n\nIn [23]: %timeit using_idxmax_loc(df)\n100 loops, best of 3: 5.43 ms per loop\n"", 'set_index', 'to_dict', ""In [36]: df2 = df.loc[idx, ['word', 'tag']].set_index('word')\n\nIn [37]: df2\nOut[37]: \n     tag\nword    \na      T\nan     T\nthe    S\n\nIn [38]: df2.to_dict()['tag']\nOut[38]: {'a': 'T', 'an': 'T', 'the': 'S'}\n""]"
728;;1;15322920;15322632.0;2;17;;;"<p>Here's a simple way to figure out what is being passed (the unutbu) solution then 'applies'!</p>

<pre><code>In [33]: def f(x):
....:     print type(x)
....:     print x
....:     

In [34]: df.groupby('word').apply(f)
&lt;class 'pandas.core.frame.DataFrame'&gt;
  word tag  count
0    a   S     30
2    a   T     60
&lt;class 'pandas.core.frame.DataFrame'&gt;
  word tag  count
0    a   S     30
2    a   T     60
&lt;class 'pandas.core.frame.DataFrame'&gt;
  word tag  count
3   an   T      5
&lt;class 'pandas.core.frame.DataFrame'&gt;
  word tag  count
1  the   S     20
4  the   T     10
</code></pre>

<p>your function just operates (in this case) on a sub-section of the frame with the grouped variable all having the same value (in this cas 'word'), if you are passing a function, then you have to deal with the aggregation of potentially non-string columns; standard functions, like 'sum' do this for you</p>

<p>Automatically does NOT aggregate on the string columns</p>

<pre><code>In [41]: df.groupby('word').sum()
Out[41]: 
      count
word       
a        90
an        5
the      30
</code></pre>

<p>You ARE aggregating on all columns</p>

<pre><code>In [42]: df.groupby('word').apply(lambda x: x.sum())
Out[42]: 
        word tag count
word                  
a         aa  ST    90
an        an   T     5
the   thethe  ST    30
</code></pre>

<p>You can do pretty much anything within the function</p>

<pre><code>In [43]: df.groupby('word').apply(lambda x: x['count'].sum())
Out[43]: 
word
a       90
an       5
the     30
</code></pre>
";;"[""In [33]: def f(x):\n....:     print type(x)\n....:     print x\n....:     \n\nIn [34]: df.groupby('word').apply(f)\n<class 'pandas.core.frame.DataFrame'>\n  word tag  count\n0    a   S     30\n2    a   T     60\n<class 'pandas.core.frame.DataFrame'>\n  word tag  count\n0    a   S     30\n2    a   T     60\n<class 'pandas.core.frame.DataFrame'>\n  word tag  count\n3   an   T      5\n<class 'pandas.core.frame.DataFrame'>\n  word tag  count\n1  the   S     20\n4  the   T     10\n"", ""In [41]: df.groupby('word').sum()\nOut[41]: \n      count\nword       \na        90\nan        5\nthe      30\n"", ""In [42]: df.groupby('word').apply(lambda x: x.sum())\nOut[42]: \n        word tag count\nword                  \na         aa  ST    90\nan        an   T     5\nthe   thethe  ST    30\n"", ""In [43]: df.groupby('word').apply(lambda x: x['count'].sum())\nOut[43]: \nword\na       90\nan       5\nthe     30\n""]";"[""In [33]: def f(x):\n....:     print type(x)\n....:     print x\n....:     \n\nIn [34]: df.groupby('word').apply(f)\n<class 'pandas.core.frame.DataFrame'>\n  word tag  count\n0    a   S     30\n2    a   T     60\n<class 'pandas.core.frame.DataFrame'>\n  word tag  count\n0    a   S     30\n2    a   T     60\n<class 'pandas.core.frame.DataFrame'>\n  word tag  count\n3   an   T      5\n<class 'pandas.core.frame.DataFrame'>\n  word tag  count\n1  the   S     20\n4  the   T     10\n"", ""In [41]: df.groupby('word').sum()\nOut[41]: \n      count\nword       \na        90\nan        5\nthe      30\n"", ""In [42]: df.groupby('word').apply(lambda x: x.sum())\nOut[42]: \n        word tag count\nword                  \na         aa  ST    90\nan        an   T     5\nthe   thethe  ST    30\n"", ""In [43]: df.groupby('word').apply(lambda x: x['count'].sum())\nOut[43]: \nword\na       90\nan       5\nthe     30\n""]"
729;;1;15333283;15325182.0;2;72;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#vectorized-string-methods"">contains</a> instead:</p>

<pre><code>In [10]: df.b.str.contains('^f')
Out[10]: 
0    False
1     True
2     True
3    False
Name: b, dtype: bool
</code></pre>
";;"[""In [10]: df.b.str.contains('^f')\nOut[10]: \n0    False\n1     True\n2     True\n3    False\nName: b, dtype: bool\n""]";"[""In [10]: df.b.str.contains('^f')\nOut[10]: \n0    False\n1     True\n2     True\n3    False\nName: b, dtype: bool\n""]"
730;;4;15361537;15360925.0;2;50;;;"<p>You can get the first column as a Series by following code:</p>

<pre><code>x[x.columns[0]]
</code></pre>
";;['x[x.columns[0]]\n'];['x[x.columns[0]]\n']
731;;5;15362700;15360925.0;2;66;;;"<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame({'x' : [1, 2, 3, 4], 'y' : [4, 5, 6, 7]})
&gt;&gt;&gt; df
   x  y
0  1  4
1  2  5
2  3  6
3  4  7
&gt;&gt;&gt; s = df.ix[:,0]
&gt;&gt;&gt; type(s)
&lt;class 'pandas.core.series.Series'&gt;
&gt;&gt;&gt;
</code></pre>
";;"["">>> import pandas as pd\n>>> df = pd.DataFrame({'x' : [1, 2, 3, 4], 'y' : [4, 5, 6, 7]})\n>>> df\n   x  y\n0  1  4\n1  2  5\n2  3  6\n3  4  7\n>>> s = df.ix[:,0]\n>>> type(s)\n<class 'pandas.core.series.Series'>\n>>>\n""]";"["">>> import pandas as pd\n>>> df = pd.DataFrame({'x' : [1, 2, 3, 4], 'y' : [4, 5, 6, 7]})\n>>> df\n   x  y\n0  1  4\n1  2  5\n2  3  6\n3  4  7\n>>> s = df.ix[:,0]\n>>> type(s)\n<class 'pandas.core.series.Series'>\n>>>\n""]"
732;;0;15364468;15360925.0;2;24;;;"<pre><code>in 0.11

In [7]: df.iloc[:,0]
Out[7]: 
0    1
1    2
2    3
3    4
Name: x, dtype: int64
</code></pre>
";;['in 0.11\n\nIn [7]: df.iloc[:,0]\nOut[7]: \n0    1\n1    2\n2    3\n3    4\nName: x, dtype: int64\n'];['in 0.11\n\nIn [7]: df.iloc[:,0]\nOut[7]: \n0    1\n1    2\n2    3\n3    4\nName: x, dtype: int64\n']
733;;5;15411596;15411158.0;2;147;;;"<p>I believe this is what you want:</p>

<pre><code>table.groupby('YEARMONTH').CLIENTCODE.nunique()
</code></pre>

<p>Example:</p>

<pre><code>In [2]: table
Out[2]: 
   CLIENTCODE  YEARMONTH
0           1     201301
1           1     201301
2           2     201301
3           1     201302
4           2     201302
5           2     201302
6           3     201302

In [3]: table.groupby('YEARMONTH').CLIENTCODE.nunique()
Out[3]: 
YEARMONTH
201301       2
201302       3
</code></pre>
";;"[""table.groupby('YEARMONTH').CLIENTCODE.nunique()\n"", ""In [2]: table\nOut[2]: \n   CLIENTCODE  YEARMONTH\n0           1     201301\n1           1     201301\n2           2     201301\n3           1     201302\n4           2     201302\n5           2     201302\n6           3     201302\n\nIn [3]: table.groupby('YEARMONTH').CLIENTCODE.nunique()\nOut[3]: \nYEARMONTH\n201301       2\n201302       3\n""]";"[""table.groupby('YEARMONTH').CLIENTCODE.nunique()\n"", ""In [2]: table\nOut[2]: \n   CLIENTCODE  YEARMONTH\n0           1     201301\n1           1     201301\n2           2     201301\n3           1     201302\n4           2     201302\n5           2     201302\n6           3     201302\n\nIn [3]: table.groupby('YEARMONTH').CLIENTCODE.nunique()\nOut[3]: \nYEARMONTH\n201301       2\n201302       3\n""]"
734;;3;15466103;15465645.0;2;38;;;"<p>I think @herrfz hit all the high points. I'll just flesh out the details:</p>

<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

sin = np.sin
cos = np.cos
pi = np.pi
N = 100

x = np.linspace(0, pi, N)
a = sin(x)
b = cos(x)

df = pd.DataFrame({
    'A': [True]*N + [False]*N,
    'B': np.hstack((a,b))
    })

for key, grp in df.groupby(['A']):
    plt.plot(grp['B'], label=key)
    grp['D'] = pd.rolling_mean(grp['B'], window=5)    
    plt.plot(grp['D'], label='rolling ({k})'.format(k=key))
plt.legend(loc='best')    
plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/HeH3b.png"" alt=""enter image description here""></p>
";;"[""import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsin = np.sin\ncos = np.cos\npi = np.pi\nN = 100\n\nx = np.linspace(0, pi, N)\na = sin(x)\nb = cos(x)\n\ndf = pd.DataFrame({\n    'A': [True]*N + [False]*N,\n    'B': np.hstack((a,b))\n    })\n\nfor key, grp in df.groupby(['A']):\n    plt.plot(grp['B'], label=key)\n    grp['D'] = pd.rolling_mean(grp['B'], window=5)    \n    plt.plot(grp['D'], label='rolling ({k})'.format(k=key))\nplt.legend(loc='best')    \nplt.show()\n""]";"[""import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsin = np.sin\ncos = np.cos\npi = np.pi\nN = 100\n\nx = np.linspace(0, pi, N)\na = sin(x)\nb = cos(x)\n\ndf = pd.DataFrame({\n    'A': [True]*N + [False]*N,\n    'B': np.hstack((a,b))\n    })\n\nfor key, grp in df.groupby(['A']):\n    plt.plot(grp['B'], label=key)\n    grp['D'] = pd.rolling_mean(grp['B'], window=5)    \n    plt.plot(grp['D'], label='rolling ({k})'.format(k=key))\nplt.legend(loc='best')    \nplt.show()\n""]"
735;;4;15558350;14262433.0;2;34;;;"<p>I spotted this a little late, but I work with a similar problem (mortgage prepayment models). My solution has been to skip the pandas HDFStore layer and use straight pytables. I save each column as an individual HDF5 array in my final file.</p>

<p>My basic workflow is to first get a CSV file from the database. I gzip it, so it's not as huge. Then I convert that to a row-oriented HDF5 file, by iterating over it in python, converting each row to a real data type, and writing it to a HDF5 file. That takes some tens of minutes, but it doesn't use any memory, since it's only operating row-by-row. Then I ""transpose"" the row-oriented HDF5 file into a column-oriented HDF5 file.</p>

<p>The table transpose looks like:</p>

<pre><code>def transpose_table(h_in, table_path, h_out, group_name=""data"", group_path=""/""):
    # Get a reference to the input data.
    tb = h_in.getNode(table_path)
    # Create the output group to hold the columns.
    grp = h_out.createGroup(group_path, group_name, filters=tables.Filters(complevel=1))
    for col_name in tb.colnames:
        logger.debug(""Processing %s"", col_name)
        # Get the data.
        col_data = tb.col(col_name)
        # Create the output array.
        arr = h_out.createCArray(grp,
                                 col_name,
                                 tables.Atom.from_dtype(col_data.dtype),
                                 col_data.shape)
        # Store the data.
        arr[:] = col_data
    h_out.flush()
</code></pre>

<p>Reading it back in then looks like:</p>

<pre><code>def read_hdf5(hdf5_path, group_path=""/data"", columns=None):
    """"""Read a transposed data set from a HDF5 file.""""""
    if isinstance(hdf5_path, tables.file.File):
        hf = hdf5_path
    else:
        hf = tables.openFile(hdf5_path)

    grp = hf.getNode(group_path)
    if columns is None:
        data = [(child.name, child[:]) for child in grp]
    else:
        data = [(child.name, child[:]) for child in grp if child.name in columns]

    # Convert any float32 columns to float64 for processing.
    for i in range(len(data)):
        name, vec = data[i]
        if vec.dtype == np.float32:
            data[i] = (name, vec.astype(np.float64))

    if not isinstance(hdf5_path, tables.file.File):
        hf.close()
    return pd.DataFrame.from_items(data)
</code></pre>

<p>Now, I generally run this on a machine with a ton of memory, so I may not be careful enough with my memory usage. For example, by default the load operation reads the whole data set.</p>

<p>This generally works for me, but it's a bit clunky, and I can't use the fancy pytables magic.</p>

<p>Edit: The real advantage of this approach, over the array-of-records pytables default, is that I can then load the data into R using h5r, which can't handle tables. Or, at least, I've been unable to get it to load heterogeneous tables.</p>
";;"['def transpose_table(h_in, table_path, h_out, group_name=""data"", group_path=""/""):\n    # Get a reference to the input data.\n    tb = h_in.getNode(table_path)\n    # Create the output group to hold the columns.\n    grp = h_out.createGroup(group_path, group_name, filters=tables.Filters(complevel=1))\n    for col_name in tb.colnames:\n        logger.debug(""Processing %s"", col_name)\n        # Get the data.\n        col_data = tb.col(col_name)\n        # Create the output array.\n        arr = h_out.createCArray(grp,\n                                 col_name,\n                                 tables.Atom.from_dtype(col_data.dtype),\n                                 col_data.shape)\n        # Store the data.\n        arr[:] = col_data\n    h_out.flush()\n', 'def read_hdf5(hdf5_path, group_path=""/data"", columns=None):\n    """"""Read a transposed data set from a HDF5 file.""""""\n    if isinstance(hdf5_path, tables.file.File):\n        hf = hdf5_path\n    else:\n        hf = tables.openFile(hdf5_path)\n\n    grp = hf.getNode(group_path)\n    if columns is None:\n        data = [(child.name, child[:]) for child in grp]\n    else:\n        data = [(child.name, child[:]) for child in grp if child.name in columns]\n\n    # Convert any float32 columns to float64 for processing.\n    for i in range(len(data)):\n        name, vec = data[i]\n        if vec.dtype == np.float32:\n            data[i] = (name, vec.astype(np.float64))\n\n    if not isinstance(hdf5_path, tables.file.File):\n        hf.close()\n    return pd.DataFrame.from_items(data)\n']";"['def transpose_table(h_in, table_path, h_out, group_name=""data"", group_path=""/""):\n    # Get a reference to the input data.\n    tb = h_in.getNode(table_path)\n    # Create the output group to hold the columns.\n    grp = h_out.createGroup(group_path, group_name, filters=tables.Filters(complevel=1))\n    for col_name in tb.colnames:\n        logger.debug(""Processing %s"", col_name)\n        # Get the data.\n        col_data = tb.col(col_name)\n        # Create the output array.\n        arr = h_out.createCArray(grp,\n                                 col_name,\n                                 tables.Atom.from_dtype(col_data.dtype),\n                                 col_data.shape)\n        # Store the data.\n        arr[:] = col_data\n    h_out.flush()\n', 'def read_hdf5(hdf5_path, group_path=""/data"", columns=None):\n    """"""Read a transposed data set from a HDF5 file.""""""\n    if isinstance(hdf5_path, tables.file.File):\n        hf = hdf5_path\n    else:\n        hf = tables.openFile(hdf5_path)\n\n    grp = hf.getNode(group_path)\n    if columns is None:\n        data = [(child.name, child[:]) for child in grp]\n    else:\n        data = [(child.name, child[:]) for child in grp if child.name in columns]\n\n    # Convert any float32 columns to float64 for processing.\n    for i in range(len(data)):\n        name, vec = data[i]\n        if vec.dtype == np.float32:\n            data[i] = (name, vec.astype(np.float64))\n\n    if not isinstance(hdf5_path, tables.file.File):\n        hf.close()\n    return pd.DataFrame.from_items(data)\n']"
736;;0;15570546;15570099.0;2;14;;;"<p>You can get the summarized values by using groupby() on the State column.</p>

<p>Lets make some sample data first:</p>

<pre><code>import pandas as pd
import StringIO

incsv = StringIO.StringIO(""""""Date,State,City,SalesToday,SalesMTD,SalesYTD
20130320,stA,ctA,20,400,1000
20130320,stA,ctB,30,500,1100
20130320,stB,ctC,10,500,900
20130320,stB,ctD,40,200,1300
20130320,stC,ctF,30,300,800"""""")

df = pd.read_csv(incsv, index_col=['Date'], parse_dates=True)
</code></pre>

<p>Then apply the groupby function and add a column City:</p>

<pre><code>dfsum = df.groupby('State', as_index=False).sum()
dfsum['City'] = 'All'

print dfsum

  State  SalesToday  SalesMTD  SalesYTD City
0   stA          50       900      2100  All
1   stB          50       700      2200  All
2   stC          30       300       800  All
</code></pre>

<p>We can append the original data to the summed df by using append:</p>

<pre><code>dfsum.append(df).set_index(['State','City']).sort_index()

print dfsum

            SalesMTD  SalesToday  SalesYTD
State City                                
stA   All        900          50      2100
      ctA        400          20      1000
      ctB        500          30      1100
stB   All        700          50      2200
      ctC        500          10       900
      ctD        200          40      1300
stC   All        300          30       800
      ctF        300          30       800
</code></pre>

<p>I added the set_index and sort_index to make it look more like your example output, its not strictly necessary to get the results.</p>
";;"['import pandas as pd\nimport StringIO\n\nincsv = StringIO.StringIO(""""""Date,State,City,SalesToday,SalesMTD,SalesYTD\n20130320,stA,ctA,20,400,1000\n20130320,stA,ctB,30,500,1100\n20130320,stB,ctC,10,500,900\n20130320,stB,ctD,40,200,1300\n20130320,stC,ctF,30,300,800"""""")\n\ndf = pd.read_csv(incsv, index_col=[\'Date\'], parse_dates=True)\n', ""dfsum = df.groupby('State', as_index=False).sum()\ndfsum['City'] = 'All'\n\nprint dfsum\n\n  State  SalesToday  SalesMTD  SalesYTD City\n0   stA          50       900      2100  All\n1   stB          50       700      2200  All\n2   stC          30       300       800  All\n"", ""dfsum.append(df).set_index(['State','City']).sort_index()\n\nprint dfsum\n\n            SalesMTD  SalesToday  SalesYTD\nState City                                \nstA   All        900          50      2100\n      ctA        400          20      1000\n      ctB        500          30      1100\nstB   All        700          50      2200\n      ctC        500          10       900\n      ctD        200          40      1300\nstC   All        300          30       800\n      ctF        300          30       800\n""]";"['import pandas as pd\nimport StringIO\n\nincsv = StringIO.StringIO(""""""Date,State,City,SalesToday,SalesMTD,SalesYTD\n20130320,stA,ctA,20,400,1000\n20130320,stA,ctB,30,500,1100\n20130320,stB,ctC,10,500,900\n20130320,stB,ctD,40,200,1300\n20130320,stC,ctF,30,300,800"""""")\n\ndf = pd.read_csv(incsv, index_col=[\'Date\'], parse_dates=True)\n', ""dfsum = df.groupby('State', as_index=False).sum()\ndfsum['City'] = 'All'\n\nprint dfsum\n\n  State  SalesToday  SalesMTD  SalesYTD City\n0   stA          50       900      2100  All\n1   stB          50       700      2200  All\n2   stC          30       300       800  All\n"", ""dfsum.append(df).set_index(['State','City']).sort_index()\n\nprint dfsum\n\n            SalesMTD  SalesToday  SalesYTD\nState City                                \nstA   All        900          50      2100\n      ctA        400          20      1000\n      ctB        500          30      1100\nstB   All        700          50      2200\n      ctC        500          10       900\n      ctD        200          40      1300\nstC   All        300          30       800\n      ctF        300          30       800\n""]"
737;;1;15574875;15570099.0;2;33;;;"<p>If you put State and City not both in the rows, you'll get separate margins. Reshape and you get the table you're after:</p>

<pre><code>In [10]: table = pivot_table(df, values=['SalesToday', 'SalesMTD','SalesYTD'],\
                     rows=['State'], cols=['City'], aggfunc=np.sum, margins=True)


In [11]: table.stack('City')
Out[11]: 
            SalesMTD  SalesToday  SalesYTD
State City                                
stA   All        900          50      2100
      ctA        400          20      1000
      ctB        500          30      1100
stB   All        700          50      2200
      ctC        500          10       900
      ctD        200          40      1300
stC   All        300          30       800
      ctF        300          30       800
All   All       1900         130      5100
      ctA        400          20      1000
      ctB        500          30      1100
      ctC        500          10       900
      ctD        200          40      1300
      ctF        300          30       800
</code></pre>

<p>I admit this isn't totally obvious. </p>
";;"[""In [10]: table = pivot_table(df, values=['SalesToday', 'SalesMTD','SalesYTD'],\\\n                     rows=['State'], cols=['City'], aggfunc=np.sum, margins=True)\n\n\nIn [11]: table.stack('City')\nOut[11]: \n            SalesMTD  SalesToday  SalesYTD\nState City                                \nstA   All        900          50      2100\n      ctA        400          20      1000\n      ctB        500          30      1100\nstB   All        700          50      2200\n      ctC        500          10       900\n      ctD        200          40      1300\nstC   All        300          30       800\n      ctF        300          30       800\nAll   All       1900         130      5100\n      ctA        400          20      1000\n      ctB        500          30      1100\n      ctC        500          10       900\n      ctD        200          40      1300\n      ctF        300          30       800\n""]";"[""In [10]: table = pivot_table(df, values=['SalesToday', 'SalesMTD','SalesYTD'],\\\n                     rows=['State'], cols=['City'], aggfunc=np.sum, margins=True)\n\n\nIn [11]: table.stack('City')\nOut[11]: \n            SalesMTD  SalesToday  SalesYTD\nState City                                \nstA   All        900          50      2100\n      ctA        400          20      1000\n      ctB        500          30      1100\nstB   All        700          50      2200\n      ctC        500          10       900\n      ctD        200          40      1300\nstC   All        300          30       800\n      ctF        300          30       800\nAll   All       1900         130      5100\n      ctA        400          20      1000\n      ctB        500          30      1100\n      ctC        500          10       900\n      ctD        200          40      1300\n      ctF        300          30       800\n""]"
738;;2;15705958;15705630.0;2;78;;;"<pre><code>In [1]: df
Out[1]:
    Sp  Mt Value  count
0  MM1  S1     a      3
1  MM1  S1     n      2
2  MM1  S3    cb      5
3  MM2  S3    mk      8
4  MM2  S4    bg     10
5  MM2  S4   dgd      1
6  MM4  S2    rd      2
7  MM4  S2    cb      2
8  MM4  S2   uyi      7

In [2]: df.groupby(['Mt'], sort=False)['count'].max()
Out[2]:
Mt
S1     3
S3     8
S4    10
S2     7
Name: count
</code></pre>

<p>To get the indices of the original DF you can do:</p>

<pre><code>In [3]: idx = df.groupby(['Mt'])['count'].transform(max) == df['count']

In [4]: df[idx]
Out[4]:
    Sp  Mt Value  count
0  MM1  S1     a      3
3  MM2  S3    mk      8
4  MM2  S4    bg     10
8  MM4  S2   uyi      7
</code></pre>

<p>Note that if you have multiple max values per group, all will be returned.</p>

<p><strong>Update</strong></p>

<p>On a hail mary chance that this is what the OP is requesting:</p>

<pre><code>In [5]: df['count_max'] = df.groupby(['Mt'])['count'].transform(max)

In [6]: df
Out[6]:
    Sp  Mt Value  count  count_max
0  MM1  S1     a      3          3
1  MM1  S1     n      2          3
2  MM1  S3    cb      5          8
3  MM2  S3    mk      8          8
4  MM2  S4    bg     10         10
5  MM2  S4   dgd      1         10
6  MM4  S2    rd      2          7
7  MM4  S2    cb      2          7
8  MM4  S2   uyi      7          7
</code></pre>
";;"[""In [1]: df\nOut[1]:\n    Sp  Mt Value  count\n0  MM1  S1     a      3\n1  MM1  S1     n      2\n2  MM1  S3    cb      5\n3  MM2  S3    mk      8\n4  MM2  S4    bg     10\n5  MM2  S4   dgd      1\n6  MM4  S2    rd      2\n7  MM4  S2    cb      2\n8  MM4  S2   uyi      7\n\nIn [2]: df.groupby(['Mt'], sort=False)['count'].max()\nOut[2]:\nMt\nS1     3\nS3     8\nS4    10\nS2     7\nName: count\n"", ""In [3]: idx = df.groupby(['Mt'])['count'].transform(max) == df['count']\n\nIn [4]: df[idx]\nOut[4]:\n    Sp  Mt Value  count\n0  MM1  S1     a      3\n3  MM2  S3    mk      8\n4  MM2  S4    bg     10\n8  MM4  S2   uyi      7\n"", ""In [5]: df['count_max'] = df.groupby(['Mt'])['count'].transform(max)\n\nIn [6]: df\nOut[6]:\n    Sp  Mt Value  count  count_max\n0  MM1  S1     a      3          3\n1  MM1  S1     n      2          3\n2  MM1  S3    cb      5          8\n3  MM2  S3    mk      8          8\n4  MM2  S4    bg     10         10\n5  MM2  S4   dgd      1         10\n6  MM4  S2    rd      2          7\n7  MM4  S2    cb      2          7\n8  MM4  S2   uyi      7          7\n""]";"[""In [1]: df\nOut[1]:\n    Sp  Mt Value  count\n0  MM1  S1     a      3\n1  MM1  S1     n      2\n2  MM1  S3    cb      5\n3  MM2  S3    mk      8\n4  MM2  S4    bg     10\n5  MM2  S4   dgd      1\n6  MM4  S2    rd      2\n7  MM4  S2    cb      2\n8  MM4  S2   uyi      7\n\nIn [2]: df.groupby(['Mt'], sort=False)['count'].max()\nOut[2]:\nMt\nS1     3\nS3     8\nS4    10\nS2     7\nName: count\n"", ""In [3]: idx = df.groupby(['Mt'])['count'].transform(max) == df['count']\n\nIn [4]: df[idx]\nOut[4]:\n    Sp  Mt Value  count\n0  MM1  S1     a      3\n3  MM2  S3    mk      8\n4  MM2  S4    bg     10\n8  MM4  S2   uyi      7\n"", ""In [5]: df['count_max'] = df.groupby(['Mt'])['count'].transform(max)\n\nIn [6]: df\nOut[6]:\n    Sp  Mt Value  count  count_max\n0  MM1  S1     a      3          3\n1  MM1  S1     n      2          3\n2  MM1  S3    cb      5          8\n3  MM2  S3    mk      8          8\n4  MM2  S4    bg     10         10\n5  MM2  S4   dgd      1         10\n6  MM4  S2    rd      2          7\n7  MM4  S2    cb      2          7\n8  MM4  S2   uyi      7          7\n""]"
739;;4;15723905;15723628.0;2;43;;;"<p>You can use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html"" rel=""noreferrer""><code>astype</code></a> method to cast a Series (one column):</p>

<pre><code>df['col_name'] = df['col_name'].astype(object)
</code></pre>

<p>Or the entire DataFrame:</p>

<pre><code>df = df.astype(object)
</code></pre>

<hr>

<h3>Update</h3>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/categorical.html"" rel=""noreferrer"">Since version 0.15, you can use the category datatype</a> in a Series/column:</p>

<pre><code>df['col_name'] = df['col_name'].astype('category')
</code></pre>

<p><em>Note: <code>pd.Factor</code> was been deprecated and has been removed in favor of <code>pd.Categorical</code>.</em></p>
";;"[""df['col_name'] = df['col_name'].astype(object)\n"", 'df = df.astype(object)\n', ""df['col_name'] = df['col_name'].astype('category')\n""]";"['astype', ""df['col_name'] = df['col_name'].astype(object)\n"", 'df = df.astype(object)\n', ""df['col_name'] = df['col_name'].astype('category')\n"", 'pd.Factor', 'pd.Categorical']"
740;;0;15723994;15723628.0;2;12;;;"<p><code>Factor</code> and <code>Categorical</code> are the same, as far as I know. I think it was initially called Factor, and then changed to Categorical. To convert to Categorical maybe you can use <code>pandas.Categorical.from_array</code>, something like this:</p>

<pre><code>In [27]: df = pd.DataFrame({'a' : [1, 2, 3, 4, 5], 'b' : ['yes', 'no', 'yes', 'no', 'absent']})

In [28]: df
Out[28]: 
   a       b
0  1     yes
1  2      no
2  3     yes
3  4      no
4  5  absent

In [29]: df['c'] = pd.Categorical.from_array(df.b).labels

In [30]: df
Out[30]: 
   a       b  c
0  1     yes  2
1  2      no  1
2  3     yes  2
3  4      no  1
4  5  absent  0
</code></pre>
";;"[""In [27]: df = pd.DataFrame({'a' : [1, 2, 3, 4, 5], 'b' : ['yes', 'no', 'yes', 'no', 'absent']})\n\nIn [28]: df\nOut[28]: \n   a       b\n0  1     yes\n1  2      no\n2  3     yes\n3  4      no\n4  5  absent\n\nIn [29]: df['c'] = pd.Categorical.from_array(df.b).labels\n\nIn [30]: df\nOut[30]: \n   a       b  c\n0  1     yes  2\n1  2      no  1\n2  3     yes  2\n3  4      no  1\n4  5  absent  0\n""]";"['Factor', 'Categorical', 'pandas.Categorical.from_array', ""In [27]: df = pd.DataFrame({'a' : [1, 2, 3, 4, 5], 'b' : ['yes', 'no', 'yes', 'no', 'absent']})\n\nIn [28]: df\nOut[28]: \n   a       b\n0  1     yes\n1  2      no\n2  3     yes\n3  4      no\n4  5  absent\n\nIn [29]: df['c'] = pd.Categorical.from_array(df.b).labels\n\nIn [30]: df\nOut[30]: \n   a       b  c\n0  1     yes  2\n1  2      no  1\n2  3     yes  2\n3  4      no  1\n4  5  absent  0\n""]"
741;;1;15742147;15741759.0;2;36;;;"<p>Assuming <code>df</code> has a unique index, this gives the row with the maximum value:</p>

<pre><code>In [34]: df.loc[df['Value'].idxmax()]
Out[34]: 
Country        US
Place      Kansas
Value         894
Name: 7
</code></pre>

<p>Note that <code>idxmax</code> returns index <em>labels</em>. So if the DataFrame as duplicates in the index, the label may not uniquely identify the row, so <code>df.loc</code> may return more than one row.</p>

<p>Therefore, if <code>df</code> does not have a unique index, you must make the index unique before proceeding as above. Depending on the DataFrame, sometimes you can use <code>stack</code> or <code>set_index</code> to make the index unique. Or, you can simply reset the index (so the rows become renumbered, starting at 0):</p>

<pre><code>df = df.reset_index()
</code></pre>
";;"[""In [34]: df.loc[df['Value'].idxmax()]\nOut[34]: \nCountry        US\nPlace      Kansas\nValue         894\nName: 7\n"", 'df = df.reset_index()\n']";"['df', ""In [34]: df.loc[df['Value'].idxmax()]\nOut[34]: \nCountry        US\nPlace      Kansas\nValue         894\nName: 7\n"", 'idxmax', 'df.loc', 'df', 'stack', 'set_index', 'df = df.reset_index()\n']"
742;;5;15772263;15771472.0;2;32;;;"<p>What about something like this:</p>

<p>First resample the data frame into 1D intervals.  This takes the mean of the values for all duplicate days.  Use the <code>fill_method</code> option to fill in missing date values.  Next, pass the resampled frame into <code>pd.rolling_mean</code> with a window of 3 and min_periods=1 :</p>

<pre><code>pd.rolling_mean(df.resample(""1D"", fill_method=""ffill""), window=3, min_periods=1)

            favorable  unfavorable     other
enddate
2012-10-25   0.495000     0.485000  0.025000
2012-10-26   0.527500     0.442500  0.032500
2012-10-27   0.521667     0.451667  0.028333
2012-10-28   0.515833     0.450000  0.035833
2012-10-29   0.488333     0.476667  0.038333
2012-10-30   0.495000     0.470000  0.038333
2012-10-31   0.512500     0.460000  0.029167
2012-11-01   0.516667     0.456667  0.026667
2012-11-02   0.503333     0.463333  0.033333
2012-11-03   0.490000     0.463333  0.046667
2012-11-04   0.494000     0.456000  0.043333
2012-11-05   0.500667     0.452667  0.036667
2012-11-06   0.507333     0.456000  0.023333
2012-11-07   0.510000     0.443333  0.013333
</code></pre>

<p><strong>UPDATE</strong>: As Ben points out in the comments, <a href=""http://pandas.pydata.org/pandas-docs/stable/computation.html#window-functions"">with pandas 0.18.0 the syntax has changed</a>.  With the new syntax this would be:</p>

<pre class=""lang-py prettyprint-override""><code>df.resample(""1d"").sum().fillna(0).rolling(window=3, min_periods=1).mean()
</code></pre>
";;"['pd.rolling_mean(df.resample(""1D"", fill_method=""ffill""), window=3, min_periods=1)\n\n            favorable  unfavorable     other\nenddate\n2012-10-25   0.495000     0.485000  0.025000\n2012-10-26   0.527500     0.442500  0.032500\n2012-10-27   0.521667     0.451667  0.028333\n2012-10-28   0.515833     0.450000  0.035833\n2012-10-29   0.488333     0.476667  0.038333\n2012-10-30   0.495000     0.470000  0.038333\n2012-10-31   0.512500     0.460000  0.029167\n2012-11-01   0.516667     0.456667  0.026667\n2012-11-02   0.503333     0.463333  0.033333\n2012-11-03   0.490000     0.463333  0.046667\n2012-11-04   0.494000     0.456000  0.043333\n2012-11-05   0.500667     0.452667  0.036667\n2012-11-06   0.507333     0.456000  0.023333\n2012-11-07   0.510000     0.443333  0.013333\n', 'df.resample(""1d"").sum().fillna(0).rolling(window=3, min_periods=1).mean()\n']";"['fill_method', 'pd.rolling_mean', 'pd.rolling_mean(df.resample(""1D"", fill_method=""ffill""), window=3, min_periods=1)\n\n            favorable  unfavorable     other\nenddate\n2012-10-25   0.495000     0.485000  0.025000\n2012-10-26   0.527500     0.442500  0.032500\n2012-10-27   0.521667     0.451667  0.028333\n2012-10-28   0.515833     0.450000  0.035833\n2012-10-29   0.488333     0.476667  0.038333\n2012-10-30   0.495000     0.470000  0.038333\n2012-10-31   0.512500     0.460000  0.029167\n2012-11-01   0.516667     0.456667  0.026667\n2012-11-02   0.503333     0.463333  0.033333\n2012-11-03   0.490000     0.463333  0.046667\n2012-11-04   0.494000     0.456000  0.043333\n2012-11-05   0.500667     0.452667  0.036667\n2012-11-06   0.507333     0.456000  0.023333\n2012-11-07   0.510000     0.443333  0.013333\n', 'df.resample(""1d"").sum().fillna(0).rolling(window=3, min_periods=1).mean()\n']"
743;;4;15772330;15772009.0;2;162;;;"<p>Use numpy's random.permuation function:</p>

<pre><code>In [1]: df = pd.DataFrame({'A':range(10), 'B':range(10)})

In [2]: df
Out[2]:
   A  B
0  0  0
1  1  1
2  2  2
3  3  3
4  4  4
5  5  5
6  6  6
7  7  7
8  8  8
9  9  9


In [3]: df.reindex(np.random.permutation(df.index))
Out[3]:
   A  B
0  0  0
5  5  5
6  6  6
3  3  3
8  8  8
7  7  7
9  9  9
1  1  1
2  2  2
4  4  4
</code></pre>
";;"[""In [1]: df = pd.DataFrame({'A':range(10), 'B':range(10)})\n\nIn [2]: df\nOut[2]:\n   A  B\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n4  4  4\n5  5  5\n6  6  6\n7  7  7\n8  8  8\n9  9  9\n\n\nIn [3]: df.reindex(np.random.permutation(df.index))\nOut[3]:\n   A  B\n0  0  0\n5  5  5\n6  6  6\n3  3  3\n8  8  8\n7  7  7\n9  9  9\n1  1  1\n2  2  2\n4  4  4\n""]";"[""In [1]: df = pd.DataFrame({'A':range(10), 'B':range(10)})\n\nIn [2]: df\nOut[2]:\n   A  B\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n4  4  4\n5  5  5\n6  6  6\n7  7  7\n8  8  8\n9  9  9\n\n\nIn [3]: df.reindex(np.random.permutation(df.index))\nOut[3]:\n   A  B\n0  0  0\n5  5  5\n6  6  6\n3  3  3\n8  8  8\n7  7  7\n9  9  9\n1  1  1\n2  2  2\n4  4  4\n""]"
744;;6;15772356;15772009.0;2;21;;;"<pre><code>In [16]: def shuffle(df, n=1, axis=0):     
    ...:     df = df.copy()
    ...:     for _ in range(n):
    ...:         df.apply(np.random.shuffle, axis=axis)
    ...:     return df
    ...:     

In [17]: df = pd.DataFrame({'A':range(10), 'B':range(10)})

In [18]: shuffle(df)

In [19]: df
Out[19]: 
   A  B
0  8  5
1  1  7
2  7  3
3  6  2
4  3  4
5  0  1
6  9  0
7  4  6
8  2  8
9  5  9
</code></pre>
";;"[""In [16]: def shuffle(df, n=1, axis=0):     \n    ...:     df = df.copy()\n    ...:     for _ in range(n):\n    ...:         df.apply(np.random.shuffle, axis=axis)\n    ...:     return df\n    ...:     \n\nIn [17]: df = pd.DataFrame({'A':range(10), 'B':range(10)})\n\nIn [18]: shuffle(df)\n\nIn [19]: df\nOut[19]: \n   A  B\n0  8  5\n1  1  7\n2  7  3\n3  6  2\n4  3  4\n5  0  1\n6  9  0\n7  4  6\n8  2  8\n9  5  9\n""]";"[""In [16]: def shuffle(df, n=1, axis=0):     \n    ...:     df = df.copy()\n    ...:     for _ in range(n):\n    ...:         df.apply(np.random.shuffle, axis=axis)\n    ...:     return df\n    ...:     \n\nIn [17]: df = pd.DataFrame({'A':range(10), 'B':range(10)})\n\nIn [18]: shuffle(df)\n\nIn [19]: df\nOut[19]: \n   A  B\n0  8  5\n1  1  7\n2  7  3\n3  6  2\n4  3  4\n5  0  1\n6  9  0\n7  4  6\n8  2  8\n9  5  9\n""]"
745;;2;15778297;15777951.0;2;47;;;"<p>Found this on <a href=""https://github.com/pydata/pandas/issues/2841"" rel=""noreferrer"">github</a>...</p>

<pre><code>import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
</code></pre>
";;"[""import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n""]";"[""import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n""]"
746;;1;15786557;15006298.0;2;13;;;"<p>I write a method to show the four corners of the data and monkey-patch to dataframe to do so:</p>

<pre><code>def _sw(df, up_rows=10, down_rows=5, left_cols=4, right_cols=3, return_df=False):
    ''' display df data at four corners
        A,B (up_pt)
        C,D (down_pt)
        parameters : up_rows=10, down_rows=5, left_cols=4, right_cols=3
        usage:
            df = pd.DataFrame(np.random.randn(20,10), columns=list('ABCDEFGHIJKLMN')[0:10])
            df.sw(5,2,3,2)
            df1 = df.set_index(['A','B'], drop=True, inplace=False)
            df1.sw(5,2,3,2)
    '''
    #pd.set_printoptions(max_columns = 80, max_rows = 40)
    ncol, nrow = len(df.columns), len(df)

    # handle columns
    if ncol &lt;= (left_cols + right_cols) :
        up_pt = df.ix[0:up_rows, :]         # screen width can contain all columns
        down_pt = df.ix[-down_rows:, :]
    else:                                   # screen width can not contain all columns
        pt_a = df.ix[0:up_rows,  0:left_cols]
        pt_b = df.ix[0:up_rows,  -right_cols:]
        pt_c = df[-down_rows:].ix[:,0:left_cols]
        pt_d = df[-down_rows:].ix[:,-right_cols:]

        up_pt   = pt_a.join(pt_b, how='inner')
        down_pt = pt_c.join(pt_d, how='inner')
        up_pt.insert(left_cols, '..', '..')
        down_pt.insert(left_cols, '..', '..')

    overlap_qty = len(up_pt) + len(down_pt) - len(df)
    down_pt = down_pt.drop(down_pt.index[range(overlap_qty)]) # remove overlap rows

    dt_str_list = down_pt.to_string().split('\n') # transfer down_pt to string list

    # Display up part data
    print up_pt

    start_row = (1 if df.index.names[0] is None else 2) # start from 1 if without index

    # Display omit line if screen height is not enought to display all rows
    if overlap_qty &lt; 0:
        print ""."" * len(dt_str_list[start_row])

    # Display down part data row by row
    for line in dt_str_list[start_row:]:
        print line

    # Display foot note
    print ""\n""
    print ""Index :"",df.index.names
    print ""Column:"","","".join(list(df.columns.values))
    print ""row: %d    col: %d""%(len(df), len(df.columns))
    print ""\n""

    return (df if return_df else None)
DataFrame.sw = _sw  #add a method to DataFrame class
</code></pre>

<p>Here is the sample:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(20,10), columns=list('ABCDEFGHIJKLMN')[0:10])

&gt;&gt;&gt; df.sw()
         A       B       C       D  ..       H       I       J
0  -0.8166  0.0102  0.0215 -0.0307  .. -0.0820  1.2727  0.6395
1   1.0659 -1.0102 -1.3960  0.4700  ..  1.0999  1.1222 -1.2476
2   0.4347  1.5423  0.5710 -0.5439  ..  0.2491 -0.0725  2.0645
3  -1.5952 -1.4959  2.2697 -1.1004  .. -1.9614  0.6488 -0.6190
4  -1.4426 -0.8622  0.0942 -0.1977  .. -0.7802 -1.1774  1.9682
5   1.2526 -0.2694  0.4841 -0.7568  ..  0.2481  0.3608 -0.7342
6   0.2108  2.5181  1.3631  0.4375  .. -0.1266  1.0572  0.3654
7  -1.0617 -0.4743 -1.7399 -1.4123  .. -1.0398 -1.4703 -0.9466
8  -0.5682 -1.3323 -0.6992  1.7737  ..  0.6152  0.9269  2.1854
9   0.2361  0.4873 -1.1278 -0.2251  ..  1.4232  2.1212  2.9180
10  2.0034  0.5454 -2.6337  0.1556  ..  0.0016 -1.6128 -0.8093
..............................................................
15  1.4091  0.3540 -1.3498 -1.0490  ..  0.9328  0.3668  1.3948
16  0.4528 -0.3183  0.4308 -0.1818  ..  0.1295  1.2268  0.1365
17 -0.7093  1.3991  0.9501  2.1227  .. -1.5296  1.1908  0.0318
18  1.7101  0.5962  0.8948  1.5606  .. -0.6862  0.9558 -0.5514
19  1.0329 -1.2308 -0.6896 -0.5112  ..  0.2719  1.1478 -0.1459


Index : [None]
Column: A,B,C,D,E,F,G,H,I,J
row: 20    col: 10


&gt;&gt;&gt; df.sw(4,2,3,4)
        A       B       C  ..       G       H       I       J
0 -0.8166  0.0102  0.0215  ..  0.3671 -0.0820  1.2727  0.6395
1  1.0659 -1.0102 -1.3960  ..  1.0984  1.0999  1.1222 -1.2476
2  0.4347  1.5423  0.5710  ..  1.6675  0.2491 -0.0725  2.0645
3 -1.5952 -1.4959  2.2697  ..  0.4856 -1.9614  0.6488 -0.6190
4 -1.4426 -0.8622  0.0942  .. -0.0947 -0.7802 -1.1774  1.9682
..............................................................
18  1.7101  0.5962  0.8948  .. -0.8592 -0.6862  0.9558 -0.5514
19  1.0329 -1.2308 -0.6896  .. -0.3954  0.2719  1.1478 -0.1459


Index : [None]
Column: A,B,C,D,E,F,G,H,I,J
row: 20    col: 10
</code></pre>
";;"['def _sw(df, up_rows=10, down_rows=5, left_cols=4, right_cols=3, return_df=False):\n    \'\'\' display df data at four corners\n        A,B (up_pt)\n        C,D (down_pt)\n        parameters : up_rows=10, down_rows=5, left_cols=4, right_cols=3\n        usage:\n            df = pd.DataFrame(np.random.randn(20,10), columns=list(\'ABCDEFGHIJKLMN\')[0:10])\n            df.sw(5,2,3,2)\n            df1 = df.set_index([\'A\',\'B\'], drop=True, inplace=False)\n            df1.sw(5,2,3,2)\n    \'\'\'\n    #pd.set_printoptions(max_columns = 80, max_rows = 40)\n    ncol, nrow = len(df.columns), len(df)\n\n    # handle columns\n    if ncol <= (left_cols + right_cols) :\n        up_pt = df.ix[0:up_rows, :]         # screen width can contain all columns\n        down_pt = df.ix[-down_rows:, :]\n    else:                                   # screen width can not contain all columns\n        pt_a = df.ix[0:up_rows,  0:left_cols]\n        pt_b = df.ix[0:up_rows,  -right_cols:]\n        pt_c = df[-down_rows:].ix[:,0:left_cols]\n        pt_d = df[-down_rows:].ix[:,-right_cols:]\n\n        up_pt   = pt_a.join(pt_b, how=\'inner\')\n        down_pt = pt_c.join(pt_d, how=\'inner\')\n        up_pt.insert(left_cols, \'..\', \'..\')\n        down_pt.insert(left_cols, \'..\', \'..\')\n\n    overlap_qty = len(up_pt) + len(down_pt) - len(df)\n    down_pt = down_pt.drop(down_pt.index[range(overlap_qty)]) # remove overlap rows\n\n    dt_str_list = down_pt.to_string().split(\'\\n\') # transfer down_pt to string list\n\n    # Display up part data\n    print up_pt\n\n    start_row = (1 if df.index.names[0] is None else 2) # start from 1 if without index\n\n    # Display omit line if screen height is not enought to display all rows\n    if overlap_qty < 0:\n        print ""."" * len(dt_str_list[start_row])\n\n    # Display down part data row by row\n    for line in dt_str_list[start_row:]:\n        print line\n\n    # Display foot note\n    print ""\\n""\n    print ""Index :"",df.index.names\n    print ""Column:"","","".join(list(df.columns.values))\n    print ""row: %d    col: %d""%(len(df), len(df.columns))\n    print ""\\n""\n\n    return (df if return_df else None)\nDataFrame.sw = _sw  #add a method to DataFrame class\n', "">>> df = pd.DataFrame(np.random.randn(20,10), columns=list('ABCDEFGHIJKLMN')[0:10])\n\n>>> df.sw()\n         A       B       C       D  ..       H       I       J\n0  -0.8166  0.0102  0.0215 -0.0307  .. -0.0820  1.2727  0.6395\n1   1.0659 -1.0102 -1.3960  0.4700  ..  1.0999  1.1222 -1.2476\n2   0.4347  1.5423  0.5710 -0.5439  ..  0.2491 -0.0725  2.0645\n3  -1.5952 -1.4959  2.2697 -1.1004  .. -1.9614  0.6488 -0.6190\n4  -1.4426 -0.8622  0.0942 -0.1977  .. -0.7802 -1.1774  1.9682\n5   1.2526 -0.2694  0.4841 -0.7568  ..  0.2481  0.3608 -0.7342\n6   0.2108  2.5181  1.3631  0.4375  .. -0.1266  1.0572  0.3654\n7  -1.0617 -0.4743 -1.7399 -1.4123  .. -1.0398 -1.4703 -0.9466\n8  -0.5682 -1.3323 -0.6992  1.7737  ..  0.6152  0.9269  2.1854\n9   0.2361  0.4873 -1.1278 -0.2251  ..  1.4232  2.1212  2.9180\n10  2.0034  0.5454 -2.6337  0.1556  ..  0.0016 -1.6128 -0.8093\n..............................................................\n15  1.4091  0.3540 -1.3498 -1.0490  ..  0.9328  0.3668  1.3948\n16  0.4528 -0.3183  0.4308 -0.1818  ..  0.1295  1.2268  0.1365\n17 -0.7093  1.3991  0.9501  2.1227  .. -1.5296  1.1908  0.0318\n18  1.7101  0.5962  0.8948  1.5606  .. -0.6862  0.9558 -0.5514\n19  1.0329 -1.2308 -0.6896 -0.5112  ..  0.2719  1.1478 -0.1459\n\n\nIndex : [None]\nColumn: A,B,C,D,E,F,G,H,I,J\nrow: 20    col: 10\n\n\n>>> df.sw(4,2,3,4)\n        A       B       C  ..       G       H       I       J\n0 -0.8166  0.0102  0.0215  ..  0.3671 -0.0820  1.2727  0.6395\n1  1.0659 -1.0102 -1.3960  ..  1.0984  1.0999  1.1222 -1.2476\n2  0.4347  1.5423  0.5710  ..  1.6675  0.2491 -0.0725  2.0645\n3 -1.5952 -1.4959  2.2697  ..  0.4856 -1.9614  0.6488 -0.6190\n4 -1.4426 -0.8622  0.0942  .. -0.0947 -0.7802 -1.1774  1.9682\n..............................................................\n18  1.7101  0.5962  0.8948  .. -0.8592 -0.6862  0.9558 -0.5514\n19  1.0329 -1.2308 -0.6896  .. -0.3954  0.2719  1.1478 -0.1459\n\n\nIndex : [None]\nColumn: A,B,C,D,E,F,G,H,I,J\nrow: 20    col: 10\n""]";"['def _sw(df, up_rows=10, down_rows=5, left_cols=4, right_cols=3, return_df=False):\n    \'\'\' display df data at four corners\n        A,B (up_pt)\n        C,D (down_pt)\n        parameters : up_rows=10, down_rows=5, left_cols=4, right_cols=3\n        usage:\n            df = pd.DataFrame(np.random.randn(20,10), columns=list(\'ABCDEFGHIJKLMN\')[0:10])\n            df.sw(5,2,3,2)\n            df1 = df.set_index([\'A\',\'B\'], drop=True, inplace=False)\n            df1.sw(5,2,3,2)\n    \'\'\'\n    #pd.set_printoptions(max_columns = 80, max_rows = 40)\n    ncol, nrow = len(df.columns), len(df)\n\n    # handle columns\n    if ncol <= (left_cols + right_cols) :\n        up_pt = df.ix[0:up_rows, :]         # screen width can contain all columns\n        down_pt = df.ix[-down_rows:, :]\n    else:                                   # screen width can not contain all columns\n        pt_a = df.ix[0:up_rows,  0:left_cols]\n        pt_b = df.ix[0:up_rows,  -right_cols:]\n        pt_c = df[-down_rows:].ix[:,0:left_cols]\n        pt_d = df[-down_rows:].ix[:,-right_cols:]\n\n        up_pt   = pt_a.join(pt_b, how=\'inner\')\n        down_pt = pt_c.join(pt_d, how=\'inner\')\n        up_pt.insert(left_cols, \'..\', \'..\')\n        down_pt.insert(left_cols, \'..\', \'..\')\n\n    overlap_qty = len(up_pt) + len(down_pt) - len(df)\n    down_pt = down_pt.drop(down_pt.index[range(overlap_qty)]) # remove overlap rows\n\n    dt_str_list = down_pt.to_string().split(\'\\n\') # transfer down_pt to string list\n\n    # Display up part data\n    print up_pt\n\n    start_row = (1 if df.index.names[0] is None else 2) # start from 1 if without index\n\n    # Display omit line if screen height is not enought to display all rows\n    if overlap_qty < 0:\n        print ""."" * len(dt_str_list[start_row])\n\n    # Display down part data row by row\n    for line in dt_str_list[start_row:]:\n        print line\n\n    # Display foot note\n    print ""\\n""\n    print ""Index :"",df.index.names\n    print ""Column:"","","".join(list(df.columns.values))\n    print ""row: %d    col: %d""%(len(df), len(df.columns))\n    print ""\\n""\n\n    return (df if return_df else None)\nDataFrame.sw = _sw  #add a method to DataFrame class\n', "">>> df = pd.DataFrame(np.random.randn(20,10), columns=list('ABCDEFGHIJKLMN')[0:10])\n\n>>> df.sw()\n         A       B       C       D  ..       H       I       J\n0  -0.8166  0.0102  0.0215 -0.0307  .. -0.0820  1.2727  0.6395\n1   1.0659 -1.0102 -1.3960  0.4700  ..  1.0999  1.1222 -1.2476\n2   0.4347  1.5423  0.5710 -0.5439  ..  0.2491 -0.0725  2.0645\n3  -1.5952 -1.4959  2.2697 -1.1004  .. -1.9614  0.6488 -0.6190\n4  -1.4426 -0.8622  0.0942 -0.1977  .. -0.7802 -1.1774  1.9682\n5   1.2526 -0.2694  0.4841 -0.7568  ..  0.2481  0.3608 -0.7342\n6   0.2108  2.5181  1.3631  0.4375  .. -0.1266  1.0572  0.3654\n7  -1.0617 -0.4743 -1.7399 -1.4123  .. -1.0398 -1.4703 -0.9466\n8  -0.5682 -1.3323 -0.6992  1.7737  ..  0.6152  0.9269  2.1854\n9   0.2361  0.4873 -1.1278 -0.2251  ..  1.4232  2.1212  2.9180\n10  2.0034  0.5454 -2.6337  0.1556  ..  0.0016 -1.6128 -0.8093\n..............................................................\n15  1.4091  0.3540 -1.3498 -1.0490  ..  0.9328  0.3668  1.3948\n16  0.4528 -0.3183  0.4308 -0.1818  ..  0.1295  1.2268  0.1365\n17 -0.7093  1.3991  0.9501  2.1227  .. -1.5296  1.1908  0.0318\n18  1.7101  0.5962  0.8948  1.5606  .. -0.6862  0.9558 -0.5514\n19  1.0329 -1.2308 -0.6896 -0.5112  ..  0.2719  1.1478 -0.1459\n\n\nIndex : [None]\nColumn: A,B,C,D,E,F,G,H,I,J\nrow: 20    col: 10\n\n\n>>> df.sw(4,2,3,4)\n        A       B       C  ..       G       H       I       J\n0 -0.8166  0.0102  0.0215  ..  0.3671 -0.0820  1.2727  0.6395\n1  1.0659 -1.0102 -1.3960  ..  1.0984  1.0999  1.1222 -1.2476\n2  0.4347  1.5423  0.5710  ..  1.6675  0.2491 -0.0725  2.0645\n3 -1.5952 -1.4959  2.2697  ..  0.4856 -1.9614  0.6488 -0.6190\n4 -1.4426 -0.8622  0.0942  .. -0.0947 -0.7802 -1.1774  1.9682\n..............................................................\n18  1.7101  0.5962  0.8948  .. -0.8592 -0.6862  0.9558 -0.5514\n19  1.0329 -1.2308 -0.6896  .. -0.3954  0.2719  1.1478 -0.1459\n\n\nIndex : [None]\nColumn: A,B,C,D,E,F,G,H,I,J\nrow: 20    col: 10\n""]"
747;;3;15822811;15819050.0;2;23;;;"<p>So what are you doing is with append and concat is <em>almost</em> equivalent. The difference is the empty DataFrame. For some reason this causes a big slowdown, not sure exactly why, will have to look at some point. Below is a recreation of basically what you did.</p>

<p>I almost always use concat (though in this case they are equivalent, except for the empty frame);
if you don't use the empty frame they will be the same speed.</p>

<pre><code>In [17]: df1 = pd.DataFrame(dict(A = range(10000)),index=pd.date_range('20130101',periods=10000,freq='s'))

In [18]: df1
Out[18]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 10000 entries, 2013-01-01 00:00:00 to 2013-01-01 02:46:39
Freq: S
Data columns (total 1 columns):
A    10000  non-null values
dtypes: int64(1)

In [19]: df4 = pd.DataFrame()

The concat

In [20]: %timeit pd.concat([df1,df2,df3])
1000 loops, best of 3: 270 us per loop

This is equavalent of your append

In [21]: %timeit pd.concat([df4,df1,df2,df3])
10 loops, best of 

 3: 56.8 ms per loop
</code></pre>
";;"[""In [17]: df1 = pd.DataFrame(dict(A = range(10000)),index=pd.date_range('20130101',periods=10000,freq='s'))\n\nIn [18]: df1\nOut[18]: \n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 10000 entries, 2013-01-01 00:00:00 to 2013-01-01 02:46:39\nFreq: S\nData columns (total 1 columns):\nA    10000  non-null values\ndtypes: int64(1)\n\nIn [19]: df4 = pd.DataFrame()\n\nThe concat\n\nIn [20]: %timeit pd.concat([df1,df2,df3])\n1000 loops, best of 3: 270 us per loop\n\nThis is equavalent of your append\n\nIn [21]: %timeit pd.concat([df4,df1,df2,df3])\n10 loops, best of \n\n 3: 56.8 ms per loop\n""]";"[""In [17]: df1 = pd.DataFrame(dict(A = range(10000)),index=pd.date_range('20130101',periods=10000,freq='s'))\n\nIn [18]: df1\nOut[18]: \n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 10000 entries, 2013-01-01 00:00:00 to 2013-01-01 02:46:39\nFreq: S\nData columns (total 1 columns):\nA    10000  non-null values\ndtypes: int64(1)\n\nIn [19]: df4 = pd.DataFrame()\n\nThe concat\n\nIn [20]: %timeit pd.concat([df1,df2,df3])\n1000 loops, best of 3: 270 us per loop\n\nThis is equavalent of your append\n\nIn [21]: %timeit pd.concat([df4,df1,df2,df3])\n10 loops, best of \n\n 3: 56.8 ms per loop\n""]"
748;;1;15855998;15854878.0;2;60;;;"<p><a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.correlate.html"">np.correlate</a> calculates the (unnormalized) <a href=""http://en.wikipedia.org/wiki/Cross-correlation"">cross-correlation</a> between two 1-dimensional sequences:</p>

<pre><code>z[k] = sum_n a[n] * conj(v[n+k])
</code></pre>

<p>while <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.corr.html"">df.corr</a> (by default) calculates the <a href=""http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient#For_a_sample"">Pearson correlation coefficient</a>. </p>

<p>The correlation coefficient (if it exists) is always between -1 and 1 inclusive.
The cross-correlation is not bounded.</p>

<p>The formulas are somewhat related, but notice that in the cross-correlation formula (above) there is no subtraction of the means, and no division by the standard deviations which is part of the formula for Pearson correlation coefficient.</p>

<p>The fact that the standard deviation of <code>df['a']</code> and <code>df['b']</code> is zero is what causes <code>df.corr</code> to be NaN everywhere. </p>

<hr>

<p>From the comment below, it sounds like you are looking for <a href=""http://en.wikipedia.org/wiki/Beta_%28finance%29"">Beta</a>. It is related to Pearson's correlation coefficient, but instead of dividing by the product of standard deviations:</p>

<p><img src=""https://i.stack.imgur.com/ft4zy.png"" alt=""enter image description here""></p>

<p>you divide by a variance:</p>

<p><img src=""https://i.stack.imgur.com/fHUQO.png"" alt=""enter image description here""></p>

<hr>

<p>You can compute <code>Beta</code> using <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html"">np.cov</a></p>

<pre><code>cov = np.cov(a, b)
beta = cov[1, 0] / cov[0, 0]
</code></pre>

<hr>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
np.random.seed(100)


def geometric_brownian_motion(T=1, N=100, mu=0.1, sigma=0.01, S0=20):
    """"""
    http://stackoverflow.com/a/13203189/190597 (unutbu)
    """"""
    dt = float(T) / N
    t = np.linspace(0, T, N)
    W = np.random.standard_normal(size=N)
    W = np.cumsum(W) * np.sqrt(dt)  # standard brownian motion ###
    X = (mu - 0.5 * sigma ** 2) * t + sigma * W
    S = S0 * np.exp(X)  # geometric brownian motion ###
    return S

N = 10 ** 6
a = geometric_brownian_motion(T=1, mu=0.1, sigma=0.01, N=N)
b = geometric_brownian_motion(T=1, mu=0.2, sigma=0.01, N=N)

cov = np.cov(a, b)
print(cov)
# [[ 0.38234755  0.80525967]
#  [ 0.80525967  1.73517501]]
beta = cov[1, 0] / cov[0, 0]
print(beta)
# 2.10609347015

plt.plot(a)
plt.plot(b)
plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/xLLow.png"" alt=""enter image description here""></p>

<p>The ratio of <code>mu</code>s is 2, and <code>beta</code> is ~2.1.</p>

<hr>

<p>And you could also compute it with <code>df.corr</code>, though this is a much more round-about way of doing it (but it is nice to see there is consistency):</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'a': a, 'b': b})
beta2 = (df.corr() * df['b'].std() * df['a'].std() / df['a'].var()).ix[0, 1]
print(beta2)
# 2.10609347015
assert np.allclose(beta, beta2)
</code></pre>
";;"['z[k] = sum_n a[n] * conj(v[n+k])\n', 'cov = np.cov(a, b)\nbeta = cov[1, 0] / cov[0, 0]\n', 'import numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(100)\n\n\ndef geometric_brownian_motion(T=1, N=100, mu=0.1, sigma=0.01, S0=20):\n    """"""\n    http://stackoverflow.com/a/13203189/190597 (unutbu)\n    """"""\n    dt = float(T) / N\n    t = np.linspace(0, T, N)\n    W = np.random.standard_normal(size=N)\n    W = np.cumsum(W) * np.sqrt(dt)  # standard brownian motion ###\n    X = (mu - 0.5 * sigma ** 2) * t + sigma * W\n    S = S0 * np.exp(X)  # geometric brownian motion ###\n    return S\n\nN = 10 ** 6\na = geometric_brownian_motion(T=1, mu=0.1, sigma=0.01, N=N)\nb = geometric_brownian_motion(T=1, mu=0.2, sigma=0.01, N=N)\n\ncov = np.cov(a, b)\nprint(cov)\n# [[ 0.38234755  0.80525967]\n#  [ 0.80525967  1.73517501]]\nbeta = cov[1, 0] / cov[0, 0]\nprint(beta)\n# 2.10609347015\n\nplt.plot(a)\nplt.plot(b)\nplt.show()\n', ""import pandas as pd\ndf = pd.DataFrame({'a': a, 'b': b})\nbeta2 = (df.corr() * df['b'].std() * df['a'].std() / df['a'].var()).ix[0, 1]\nprint(beta2)\n# 2.10609347015\nassert np.allclose(beta, beta2)\n""]";"['z[k] = sum_n a[n] * conj(v[n+k])\n', ""df['a']"", ""df['b']"", 'df.corr', 'Beta', 'cov = np.cov(a, b)\nbeta = cov[1, 0] / cov[0, 0]\n', 'import numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(100)\n\n\ndef geometric_brownian_motion(T=1, N=100, mu=0.1, sigma=0.01, S0=20):\n    """"""\n    http://stackoverflow.com/a/13203189/190597 (unutbu)\n    """"""\n    dt = float(T) / N\n    t = np.linspace(0, T, N)\n    W = np.random.standard_normal(size=N)\n    W = np.cumsum(W) * np.sqrt(dt)  # standard brownian motion ###\n    X = (mu - 0.5 * sigma ** 2) * t + sigma * W\n    S = S0 * np.exp(X)  # geometric brownian motion ###\n    return S\n\nN = 10 ** 6\na = geometric_brownian_motion(T=1, mu=0.1, sigma=0.01, N=N)\nb = geometric_brownian_motion(T=1, mu=0.2, sigma=0.01, N=N)\n\ncov = np.cov(a, b)\nprint(cov)\n# [[ 0.38234755  0.80525967]\n#  [ 0.80525967  1.73517501]]\nbeta = cov[1, 0] / cov[0, 0]\nprint(beta)\n# 2.10609347015\n\nplt.plot(a)\nplt.plot(b)\nplt.show()\n', 'mu', 'beta', 'df.corr', ""import pandas as pd\ndf = pd.DataFrame({'a': a, 'b': b})\nbeta2 = (df.corr() * df['b'].std() * df['a'].std() / df['a'].var()).ix[0, 1]\nprint(beta2)\n# 2.10609347015\nassert np.allclose(beta, beta2)\n""]"
749;;2;15863028;15862034.0;2;26;;;"<p>The former answer is now superseded by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.iloc.html"" rel=""noreferrer""><code>.iloc</code></a>:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({""date"": range(10, 64, 8)})
&gt;&gt;&gt; df.index += 17
&gt;&gt;&gt; df
    date
17    10
18    18
19    26
20    34
21    42
22    50
23    58
&gt;&gt;&gt; df[""date""].iloc[0]
10
&gt;&gt;&gt; df[""date""].iloc[-1]
58
</code></pre>

<hr>

<p>The shortest way I can think of uses <code>.iget()</code>:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({""date"": range(10, 64, 8)})
&gt;&gt;&gt; df.index += 17
&gt;&gt;&gt; df
    date
17    10
18    18
19    26
20    34
21    42
22    50
23    58
&gt;&gt;&gt; df['date'].iget(0)
10
&gt;&gt;&gt; df['date'].iget(-1)
58
</code></pre>

<p>Alternatively:</p>

<pre><code>&gt;&gt;&gt; df['date'][df.index[0]]
10
&gt;&gt;&gt; df['date'][df.index[-1]]
58
</code></pre>

<p>There's also <code>.first_valid_index()</code> and <code>.last_valid_index()</code>, but depending on whether or not you want to rule out <code>NaN</code>s they might not be what you want.</p>

<p>Remember that <code>df.ix[0]</code> doesn't give you the first, but the one indexed by 0.  For example, in the above case, <code>df.ix[0]</code> would produce</p>

<pre><code>&gt;&gt;&gt; df.ix[0]
Traceback (most recent call last):
  File ""&lt;ipython-input-489-494245247e87&gt;"", line 1, in &lt;module&gt;
    df.ix[0]
[...]
KeyError: 0
</code></pre>
";;"['>>> df = pd.DataFrame({""date"": range(10, 64, 8)})\n>>> df.index += 17\n>>> df\n    date\n17    10\n18    18\n19    26\n20    34\n21    42\n22    50\n23    58\n>>> df[""date""].iloc[0]\n10\n>>> df[""date""].iloc[-1]\n58\n', '>>> df = pd.DataFrame({""date"": range(10, 64, 8)})\n>>> df.index += 17\n>>> df\n    date\n17    10\n18    18\n19    26\n20    34\n21    42\n22    50\n23    58\n>>> df[\'date\'].iget(0)\n10\n>>> df[\'date\'].iget(-1)\n58\n', "">>> df['date'][df.index[0]]\n10\n>>> df['date'][df.index[-1]]\n58\n"", '>>> df.ix[0]\nTraceback (most recent call last):\n  File ""<ipython-input-489-494245247e87>"", line 1, in <module>\n    df.ix[0]\n[...]\nKeyError: 0\n']";"['.iloc', '>>> df = pd.DataFrame({""date"": range(10, 64, 8)})\n>>> df.index += 17\n>>> df\n    date\n17    10\n18    18\n19    26\n20    34\n21    42\n22    50\n23    58\n>>> df[""date""].iloc[0]\n10\n>>> df[""date""].iloc[-1]\n58\n', '.iget()', '>>> df = pd.DataFrame({""date"": range(10, 64, 8)})\n>>> df.index += 17\n>>> df\n    date\n17    10\n18    18\n19    26\n20    34\n21    42\n22    50\n23    58\n>>> df[\'date\'].iget(0)\n10\n>>> df[\'date\'].iget(-1)\n58\n', "">>> df['date'][df.index[0]]\n10\n>>> df['date'][df.index[-1]]\n58\n"", '.first_valid_index()', '.last_valid_index()', 'NaN', 'df.ix[0]', 'df.ix[0]', '>>> df.ix[0]\nTraceback (most recent call last):\n  File ""<ipython-input-489-494245247e87>"", line 1, in <module>\n    df.ix[0]\n[...]\nKeyError: 0\n']"
750;;4;15923878;15923826.0;2;28;;;"<p>Something like this?</p>

<pre><code>import random

def some(x, n):
    return x.ix[random.sample(x.index, n)]
</code></pre>
";;['import random\n\ndef some(x, n):\n    return x.ix[random.sample(x.index, n)]\n'];['import random\n\ndef some(x, n):\n    return x.ix[random.sample(x.index, n)]\n']
751;;7;15943975;15943769.0;2;331;;;"<p>You can use the <code>.shape</code> property or just <code>len(DataFrame.index)</code>. However, there are notable performance differences ( the <code>.shape</code> property is faster):</p>

<pre><code>In [1]: import numpy as np

In [2]: import pandas as pd

In [3]: df = pd.DataFrame(np.arange(9).reshape(3,3))

In [4]: df
Out[4]: 
   0  1  2
0  0  1  2
1  3  4  5
2  6  7  8

In [5]: df.shape
Out[5]: (3, 3)

In [6]: timeit df.shape
1000000 loops, best of 3: 1.17 us per loop

In [7]: timeit df[0].count()
10000 loops, best of 3: 56 us per loop

In [8]: len(df.index)
Out[8]: 3

In [9]: timeit len(df.index)
1000000 loops, best of 3: 381 ns per loop
</code></pre>

<hr>

<p>EDIT: As @Dan Allen noted in the comments <code>len(df.index)</code> and <code>df[0].count()</code> are not interchangeable as <code>count</code> excludes <code>NaN</code>s,</p>
";;['In [1]: import numpy as np\n\nIn [2]: import pandas as pd\n\nIn [3]: df = pd.DataFrame(np.arange(9).reshape(3,3))\n\nIn [4]: df\nOut[4]: \n   0  1  2\n0  0  1  2\n1  3  4  5\n2  6  7  8\n\nIn [5]: df.shape\nOut[5]: (3, 3)\n\nIn [6]: timeit df.shape\n1000000 loops, best of 3: 1.17 us per loop\n\nIn [7]: timeit df[0].count()\n10000 loops, best of 3: 56 us per loop\n\nIn [8]: len(df.index)\nOut[8]: 3\n\nIn [9]: timeit len(df.index)\n1000000 loops, best of 3: 381 ns per loop\n'];['.shape', 'len(DataFrame.index)', '.shape', 'In [1]: import numpy as np\n\nIn [2]: import pandas as pd\n\nIn [3]: df = pd.DataFrame(np.arange(9).reshape(3,3))\n\nIn [4]: df\nOut[4]: \n   0  1  2\n0  0  1  2\n1  3  4  5\n2  6  7  8\n\nIn [5]: df.shape\nOut[5]: (3, 3)\n\nIn [6]: timeit df.shape\n1000000 loops, best of 3: 1.17 us per loop\n\nIn [7]: timeit df[0].count()\n10000 loops, best of 3: 56 us per loop\n\nIn [8]: len(df.index)\nOut[8]: 3\n\nIn [9]: timeit len(df.index)\n1000000 loops, best of 3: 381 ns per loop\n', 'len(df.index)', 'df[0].count()', 'count', 'NaN']
752;;1;15998251;15998188.0;2;8;;;"<p>I just give it a shot:</p>

<pre><code>In [9]: s = Series([True, True, True, False])

In [10]: s
Out[10]: 
0     True
1     True
2     True
3    False

In [11]: -s
Out[11]: 
0    False
1    False
2    False
3     True
</code></pre>
";;['In [9]: s = Series([True, True, True, False])\n\nIn [10]: s\nOut[10]: \n0     True\n1     True\n2     True\n3    False\n\nIn [11]: -s\nOut[11]: \n0    False\n1    False\n2    False\n3     True\n'];['In [9]: s = Series([True, True, True, False])\n\nIn [10]: s\nOut[10]: \n0     True\n1     True\n2     True\n3    False\n\nIn [11]: -s\nOut[11]: \n0    False\n1    False\n2    False\n3     True\n']
753;;8;15998993;15998188.0;2;95;;;"<p>To invert a boolean Series, <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"">use <code>~s</code></a>:</p>

<pre><code>In [7]: s = pd.Series([True, True, False, True])

In [8]: ~s
Out[8]: 
0    False
1    False
2     True
3    False
dtype: bool
</code></pre>

<p>Using Python2.7, NumPy 1.8.0, Pandas 0.13.1:</p>

<pre><code>In [119]: s = pd.Series([True, True, False, True]*10000)

In [10]:  %timeit np.invert(s)
10000 loops, best of 3: 91.8 s per loop

In [11]: %timeit ~s
10000 loops, best of 3: 73.5 s per loop

In [12]: %timeit (-s)
10000 loops, best of 3: 73.5 s per loop
</code></pre>

<p>As of Pandas 0.13.0, Series are no longer subclasses of <code>numpy.ndarray</code>; they are now subclasses of <code>pd.NDFrame</code>. This might have something to do with why <code>np.invert(s)</code> is no longer as fast as <code>~s</code> or <code>-s</code>.</p>

<p>Caveat: <code>timeit</code> results may vary depending on many factors including hardware, compiler, OS, Python, NumPy and Pandas versions. </p>
";;['In [7]: s = pd.Series([True, True, False, True])\n\nIn [8]: ~s\nOut[8]: \n0    False\n1    False\n2     True\n3    False\ndtype: bool\n', 'In [119]: s = pd.Series([True, True, False, True]*10000)\n\nIn [10]:  %timeit np.invert(s)\n10000 loops, best of 3: 91.8 s per loop\n\nIn [11]: %timeit ~s\n10000 loops, best of 3: 73.5 s per loop\n\nIn [12]: %timeit (-s)\n10000 loops, best of 3: 73.5 s per loop\n'];['~s', 'In [7]: s = pd.Series([True, True, False, True])\n\nIn [8]: ~s\nOut[8]: \n0    False\n1    False\n2     True\n3    False\ndtype: bool\n', 'In [119]: s = pd.Series([True, True, False, True]*10000)\n\nIn [10]:  %timeit np.invert(s)\n10000 loops, best of 3: 91.8 s per loop\n\nIn [11]: %timeit ~s\n10000 loops, best of 3: 73.5 s per loop\n\nIn [12]: %timeit (-s)\n10000 loops, best of 3: 73.5 s per loop\n', 'numpy.ndarray', 'pd.NDFrame', 'np.invert(s)', '~s', '-s', 'timeit']
754;;9;16033048;16031056.0;2;30;;;"<pre><code>In [10]: df
Out[10]:
          A         B       lat      long
0  1.428987  0.614405  0.484370 -0.628298
1 -0.485747  0.275096  0.497116  1.047605
2  0.822527  0.340689  2.120676 -2.436831
3  0.384719 -0.042070  1.426703 -0.634355
4 -0.937442  2.520756 -1.662615 -1.377490
5 -0.154816  0.617671 -0.090484 -0.191906
6 -0.705177 -1.086138 -0.629708  1.332853
7  0.637496 -0.643773 -0.492668 -0.777344
8  1.109497 -0.610165  0.260325  2.533383
9 -1.224584  0.117668  1.304369 -0.152561

In [11]: df['lat_long'] = df[['lat', 'long']].apply(tuple, axis=1)

In [12]: df
Out[12]:
          A         B       lat      long                             lat_long
0  1.428987  0.614405  0.484370 -0.628298      (0.484370195967, -0.6282975278)
1 -0.485747  0.275096  0.497116  1.047605      (0.497115615839, 1.04760475074)
2  0.822527  0.340689  2.120676 -2.436831      (2.12067574274, -2.43683074367)
3  0.384719 -0.042070  1.426703 -0.634355      (1.42670326172, -0.63435462504)
4 -0.937442  2.520756 -1.662615 -1.377490     (-1.66261469102, -1.37749004179)
5 -0.154816  0.617671 -0.090484 -0.191906  (-0.0904840623396, -0.191905582481)
6 -0.705177 -1.086138 -0.629708  1.332853     (-0.629707821728, 1.33285348929)
7  0.637496 -0.643773 -0.492668 -0.777344   (-0.492667604075, -0.777344111021)
8  1.109497 -0.610165  0.260325  2.533383        (0.26032456699, 2.5333825651)
9 -1.224584  0.117668  1.304369 -0.152561     (1.30436900612, -0.152560909725)
</code></pre>
";;"[""In [10]: df\nOut[10]:\n          A         B       lat      long\n0  1.428987  0.614405  0.484370 -0.628298\n1 -0.485747  0.275096  0.497116  1.047605\n2  0.822527  0.340689  2.120676 -2.436831\n3  0.384719 -0.042070  1.426703 -0.634355\n4 -0.937442  2.520756 -1.662615 -1.377490\n5 -0.154816  0.617671 -0.090484 -0.191906\n6 -0.705177 -1.086138 -0.629708  1.332853\n7  0.637496 -0.643773 -0.492668 -0.777344\n8  1.109497 -0.610165  0.260325  2.533383\n9 -1.224584  0.117668  1.304369 -0.152561\n\nIn [11]: df['lat_long'] = df[['lat', 'long']].apply(tuple, axis=1)\n\nIn [12]: df\nOut[12]:\n          A         B       lat      long                             lat_long\n0  1.428987  0.614405  0.484370 -0.628298      (0.484370195967, -0.6282975278)\n1 -0.485747  0.275096  0.497116  1.047605      (0.497115615839, 1.04760475074)\n2  0.822527  0.340689  2.120676 -2.436831      (2.12067574274, -2.43683074367)\n3  0.384719 -0.042070  1.426703 -0.634355      (1.42670326172, -0.63435462504)\n4 -0.937442  2.520756 -1.662615 -1.377490     (-1.66261469102, -1.37749004179)\n5 -0.154816  0.617671 -0.090484 -0.191906  (-0.0904840623396, -0.191905582481)\n6 -0.705177 -1.086138 -0.629708  1.332853     (-0.629707821728, 1.33285348929)\n7  0.637496 -0.643773 -0.492668 -0.777344   (-0.492667604075, -0.777344111021)\n8  1.109497 -0.610165  0.260325  2.533383        (0.26032456699, 2.5333825651)\n9 -1.224584  0.117668  1.304369 -0.152561     (1.30436900612, -0.152560909725)\n""]";"[""In [10]: df\nOut[10]:\n          A         B       lat      long\n0  1.428987  0.614405  0.484370 -0.628298\n1 -0.485747  0.275096  0.497116  1.047605\n2  0.822527  0.340689  2.120676 -2.436831\n3  0.384719 -0.042070  1.426703 -0.634355\n4 -0.937442  2.520756 -1.662615 -1.377490\n5 -0.154816  0.617671 -0.090484 -0.191906\n6 -0.705177 -1.086138 -0.629708  1.332853\n7  0.637496 -0.643773 -0.492668 -0.777344\n8  1.109497 -0.610165  0.260325  2.533383\n9 -1.224584  0.117668  1.304369 -0.152561\n\nIn [11]: df['lat_long'] = df[['lat', 'long']].apply(tuple, axis=1)\n\nIn [12]: df\nOut[12]:\n          A         B       lat      long                             lat_long\n0  1.428987  0.614405  0.484370 -0.628298      (0.484370195967, -0.6282975278)\n1 -0.485747  0.275096  0.497116  1.047605      (0.497115615839, 1.04760475074)\n2  0.822527  0.340689  2.120676 -2.436831      (2.12067574274, -2.43683074367)\n3  0.384719 -0.042070  1.426703 -0.634355      (1.42670326172, -0.63435462504)\n4 -0.937442  2.520756 -1.662615 -1.377490     (-1.66261469102, -1.37749004179)\n5 -0.154816  0.617671 -0.090484 -0.191906  (-0.0904840623396, -0.191905582481)\n6 -0.705177 -1.086138 -0.629708  1.332853     (-0.629707821728, 1.33285348929)\n7  0.637496 -0.643773 -0.492668 -0.777344   (-0.492667604075, -0.777344111021)\n8  1.109497 -0.610165  0.260325  2.533383        (0.26032456699, 2.5333825651)\n9 -1.224584  0.117668  1.304369 -0.152561     (1.30436900612, -0.152560909725)\n""]"
755;;4;16068497;16031056.0;2;57;;;"<p>Get comfortable with <code>zip</code>. It comes in handy when dealing with column data. </p>

<pre><code>df['new_col'] = list(zip(df.lat, df.long))
</code></pre>

<p>It's less complicated and faster than using <code>apply</code> or <code>map</code>. Something like <code>np.dstack</code> is twice as fast as <code>zip</code>, but wouldn't give you tuples.</p>
";;"[""df['new_col'] = list(zip(df.lat, df.long))\n""]";"['zip', ""df['new_col'] = list(zip(df.lat, df.long))\n"", 'apply', 'map', 'np.dstack', 'zip']"
756;;7;16074407;16074392.0;2;47;;;"<p>You may need to give boolean arg in your calls, e.g. use <code>ax.yaxis.grid(True)</code> instead of <code>ax.yaxis.grid()</code>.  Additionally, since you are using both of them you can combine into <code>ax.grid</code>, which works on both, rather than doing it once for each dimension.</p>

<pre><code>ax = plt.gca()
ax.grid(True)
</code></pre>

<p>That should sort you out.</p>
";;['ax = plt.gca()\nax.grid(True)\n'];['ax.yaxis.grid(True)', 'ax.yaxis.grid()', 'ax.grid', 'ax = plt.gca()\nax.grid(True)\n']
757;;3;16089219;16088741.0;2;37;;;"<p>It's actually pretty simple (FWIW, I originally thought to do it your way):</p>

<pre><code>df['bar', 'three'] = [0, 1, 2]
df = df.sort_index(axis=1)
print(df)

        bar                        baz          
        one       two  three       one       two
A -0.212901  0.503615      0 -1.660945  0.446778
B -0.803926 -0.417570      1 -0.336827  0.989343
C  3.400885 -0.214245      2  0.895745  1.011671
</code></pre>
";;"[""df['bar', 'three'] = [0, 1, 2]\ndf = df.sort_index(axis=1)\nprint(df)\n\n        bar                        baz          \n        one       two  three       one       two\nA -0.212901  0.503615      0 -1.660945  0.446778\nB -0.803926 -0.417570      1 -0.336827  0.989343\nC  3.400885 -0.214245      2  0.895745  1.011671\n""]";"[""df['bar', 'three'] = [0, 1, 2]\ndf = df.sort_index(axis=1)\nprint(df)\n\n        bar                        baz          \n        one       two  three       one       two\nA -0.212901  0.503615      0 -1.660945  0.446778\nB -0.803926 -0.417570      1 -0.336827  0.989343\nC  3.400885 -0.214245      2  0.895745  1.011671\n""]"
758;;0;16099579;16096627.0;2;12;;;"<p>You can think DataFrame as a dict of Series. <code>df[key]</code> try to select the column index by <code>key</code> and returns a Series object.</p>

<p>However slicing inside of [] slices the rows, because it's a very common operation.</p>

<p>You can read the document for detail:</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#basics"">http://pandas.pydata.org/pandas-docs/stable/indexing.html#basics</a></p>
";;[];['df[key]', 'key']
759;;3;16104482;16096627.0;2;247;;;"<p>echoing @HYRY, see the new docs in 0.11</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/indexing.html</a></p>

<p>Here we have new operators, <code>.iloc</code> to explicity support only integer indexing, and <code>.loc</code> to explicity support only label indexing</p>

<p>e.g. imagine this scenario</p>

<pre><code>In [1]: df = DataFrame(randn(5,2),index=range(0,10,2),columns=list('AB'))

In [2]: df
Out[2]: 
          A         B
0  1.068932 -0.794307
2 -0.470056  1.192211
4 -0.284561  0.756029
6  1.037563 -0.267820
8 -0.538478 -0.800654

In [5]: df.iloc[[2]]
Out[5]: 
          A         B
4 -0.284561  0.756029

In [6]: df.loc[[2]]
Out[6]: 
          A         B
2 -0.470056  1.192211
</code></pre>

<p><code>[]</code> slices the rows (by label location) only</p>
";;"[""In [1]: df = DataFrame(randn(5,2),index=range(0,10,2),columns=list('AB'))\n\nIn [2]: df\nOut[2]: \n          A         B\n0  1.068932 -0.794307\n2 -0.470056  1.192211\n4 -0.284561  0.756029\n6  1.037563 -0.267820\n8 -0.538478 -0.800654\n\nIn [5]: df.iloc[[2]]\nOut[5]: \n          A         B\n4 -0.284561  0.756029\n\nIn [6]: df.loc[[2]]\nOut[6]: \n          A         B\n2 -0.470056  1.192211\n""]";"['.iloc', '.loc', ""In [1]: df = DataFrame(randn(5,2),index=range(0,10,2),columns=list('AB'))\n\nIn [2]: df\nOut[2]: \n          A         B\n0  1.068932 -0.794307\n2 -0.470056  1.192211\n4 -0.284561  0.756029\n6  1.037563 -0.267820\n8 -0.538478 -0.800654\n\nIn [5]: df.iloc[[2]]\nOut[5]: \n          A         B\n4 -0.284561  0.756029\n\nIn [6]: df.loc[[2]]\nOut[6]: \n          A         B\n2 -0.470056  1.192211\n"", '[]']"
760;;9;16134561;15891038.0;2;300;;;"<p>How about this? </p>

<pre class=""lang-py prettyprint-override""><code>a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]
df = pd.DataFrame(a, columns=['one', 'two', 'three'])
df
Out[16]: 
  one  two three
0   a  1.2   4.2
1   b   70  0.03
2   x    5     0

df.dtypes
Out[17]: 
one      object
two      object
three    object

df[['two', 'three']] = df[['two', 'three']].astype(float)

df.dtypes
Out[19]: 
one       object
two      float64
three    float64
</code></pre>
";;"[""a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\ndf = pd.DataFrame(a, columns=['one', 'two', 'three'])\ndf\nOut[16]: \n  one  two three\n0   a  1.2   4.2\n1   b   70  0.03\n2   x    5     0\n\ndf.dtypes\nOut[17]: \none      object\ntwo      object\nthree    object\n\ndf[['two', 'three']] = df[['two', 'three']].astype(float)\n\ndf.dtypes\nOut[19]: \none       object\ntwo      float64\nthree    float64\n""]";"[""a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\ndf = pd.DataFrame(a, columns=['one', 'two', 'three'])\ndf\nOut[16]: \n  one  two three\n0   a  1.2   4.2\n1   b   70  0.03\n2   x    5     0\n\ndf.dtypes\nOut[17]: \none      object\ntwo      object\nthree    object\n\ndf[['two', 'three']] = df[['two', 'three']].astype(float)\n\ndf.dtypes\nOut[19]: \none       object\ntwo      float64\nthree    float64\n""]"
761;;4;16176457;16175874.0;2;37;;;"<p>Use <code>searchsorted</code> to find the nearest times first, and then use it to slice.</p>

<pre><code>In [15]: df = pd.DataFrame([1, 2, 3], index=[dt.datetime(2013, 1, 1), dt.datetime(2013, 1, 3), dt.datetime(2013, 1, 5)])

In [16]: df
Out[16]: 
            0
2013-01-01  1
2013-01-03  2
2013-01-05  3

In [22]: start = df.index.searchsorted(dt.datetime(2013, 1, 2))

In [23]: end = df.index.searchsorted(dt.datetime(2013, 1, 4))

In [24]: df.ix[start:end]
Out[24]: 
            0
2013-01-03  2
</code></pre>
";;['In [15]: df = pd.DataFrame([1, 2, 3], index=[dt.datetime(2013, 1, 1), dt.datetime(2013, 1, 3), dt.datetime(2013, 1, 5)])\n\nIn [16]: df\nOut[16]: \n            0\n2013-01-01  1\n2013-01-03  2\n2013-01-05  3\n\nIn [22]: start = df.index.searchsorted(dt.datetime(2013, 1, 2))\n\nIn [23]: end = df.index.searchsorted(dt.datetime(2013, 1, 4))\n\nIn [24]: df.ix[start:end]\nOut[24]: \n            0\n2013-01-03  2\n'];['searchsorted', 'In [15]: df = pd.DataFrame([1, 2, 3], index=[dt.datetime(2013, 1, 1), dt.datetime(2013, 1, 3), dt.datetime(2013, 1, 5)])\n\nIn [16]: df\nOut[16]: \n            0\n2013-01-01  1\n2013-01-03  2\n2013-01-05  3\n\nIn [22]: start = df.index.searchsorted(dt.datetime(2013, 1, 2))\n\nIn [23]: end = df.index.searchsorted(dt.datetime(2013, 1, 4))\n\nIn [24]: df.ix[start:end]\nOut[24]: \n            0\n2013-01-03  2\n']
762;;5;16179190;16175874.0;2;20;;;"<p>Short answer: Sort your data (<code>data.sort()</code>) and then I think everything will work the way you are expecting.</p>

<p>Yes, you can slice using datetimes not present in the DataFrame. For example:</p>

<pre><code>In [12]: df
Out[12]: 
                   0
2013-04-20  1.120024
2013-04-21 -0.721101
2013-04-22  0.379392
2013-04-23  0.924535
2013-04-24  0.531902
2013-04-25 -0.957936

In [13]: df['20130419':'20130422']
Out[13]: 
                   0
2013-04-20  1.120024
2013-04-21 -0.721101
2013-04-22  0.379392
</code></pre>

<p>As you can see, you don't even have to build datetime objects; strings work.</p>

<p>Because the datetimes in your index are not sequential, the behavior is weird. If we shuffle the index of my example here...</p>

<pre><code>In [17]: df
Out[17]: 
                   0
2013-04-22  1.120024
2013-04-20 -0.721101
2013-04-24  0.379392
2013-04-23  0.924535
2013-04-21  0.531902
2013-04-25 -0.957936
</code></pre>

<p>...and take the same slice, we get a different result. It returns the first element inside the range and stops at the first element outside the range.</p>

<pre><code>In [18]: df['20130419':'20130422']
Out[18]: 
                   0
2013-04-22  1.120024
2013-04-20 -0.721101
2013-04-24  0.379392
</code></pre>

<p>This is probably not useful behavior. If you want to select ranges of dates, would it make sense to sort it by date first?</p>

<pre><code>df.sort_index()
</code></pre>
";;"[""In [12]: df\nOut[12]: \n                   0\n2013-04-20  1.120024\n2013-04-21 -0.721101\n2013-04-22  0.379392\n2013-04-23  0.924535\n2013-04-24  0.531902\n2013-04-25 -0.957936\n\nIn [13]: df['20130419':'20130422']\nOut[13]: \n                   0\n2013-04-20  1.120024\n2013-04-21 -0.721101\n2013-04-22  0.379392\n"", 'In [17]: df\nOut[17]: \n                   0\n2013-04-22  1.120024\n2013-04-20 -0.721101\n2013-04-24  0.379392\n2013-04-23  0.924535\n2013-04-21  0.531902\n2013-04-25 -0.957936\n', ""In [18]: df['20130419':'20130422']\nOut[18]: \n                   0\n2013-04-22  1.120024\n2013-04-20 -0.721101\n2013-04-24  0.379392\n"", 'df.sort_index()\n']";"['data.sort()', ""In [12]: df\nOut[12]: \n                   0\n2013-04-20  1.120024\n2013-04-21 -0.721101\n2013-04-22  0.379392\n2013-04-23  0.924535\n2013-04-24  0.531902\n2013-04-25 -0.957936\n\nIn [13]: df['20130419':'20130422']\nOut[13]: \n                   0\n2013-04-20  1.120024\n2013-04-21 -0.721101\n2013-04-22  0.379392\n"", 'In [17]: df\nOut[17]: \n                   0\n2013-04-22  1.120024\n2013-04-20 -0.721101\n2013-04-24  0.379392\n2013-04-23  0.924535\n2013-04-21  0.531902\n2013-04-25 -0.957936\n', ""In [18]: df['20130419':'20130422']\nOut[18]: \n                   0\n2013-04-22  1.120024\n2013-04-20 -0.721101\n2013-04-24  0.379392\n"", 'df.sort_index()\n']"
763;;0;16242202;16236684.0;2;48;;;"<p>This is what I've done in the past</p>

<pre><code>df = pd.DataFrame({'textcol' : np.random.rand(5)})

df
    textcol
0  0.626524
1  0.119967
2  0.803650
3  0.100880
4  0.017859

df.textcol.apply(lambda s: pd.Series({'feature1':s+1, 'feature2':s-1}))
   feature1  feature2
0  1.626524 -0.373476
1  1.119967 -0.880033
2  1.803650 -0.196350
3  1.100880 -0.899120
4  1.017859 -0.982141
</code></pre>

<p>Editing for completeness</p>

<pre><code>pd.concat([df, df.textcol.apply(lambda s: pd.Series({'feature1':s+1, 'feature2':s-1}))], axis=1)
    textcol feature1  feature2
0  0.626524 1.626524 -0.373476
1  0.119967 1.119967 -0.880033
2  0.803650 1.803650 -0.196350
3  0.100880 1.100880 -0.899120
4  0.017859 1.017859 -0.982141
</code></pre>
";;"[""df = pd.DataFrame({'textcol' : np.random.rand(5)})\n\ndf\n    textcol\n0  0.626524\n1  0.119967\n2  0.803650\n3  0.100880\n4  0.017859\n\ndf.textcol.apply(lambda s: pd.Series({'feature1':s+1, 'feature2':s-1}))\n   feature1  feature2\n0  1.626524 -0.373476\n1  1.119967 -0.880033\n2  1.803650 -0.196350\n3  1.100880 -0.899120\n4  1.017859 -0.982141\n"", ""pd.concat([df, df.textcol.apply(lambda s: pd.Series({'feature1':s+1, 'feature2':s-1}))], axis=1)\n    textcol feature1  feature2\n0  0.626524 1.626524 -0.373476\n1  0.119967 1.119967 -0.880033\n2  0.803650 1.803650 -0.196350\n3  0.100880 1.100880 -0.899120\n4  0.017859 1.017859 -0.982141\n""]";"[""df = pd.DataFrame({'textcol' : np.random.rand(5)})\n\ndf\n    textcol\n0  0.626524\n1  0.119967\n2  0.803650\n3  0.100880\n4  0.017859\n\ndf.textcol.apply(lambda s: pd.Series({'feature1':s+1, 'feature2':s-1}))\n   feature1  feature2\n0  1.626524 -0.373476\n1  1.119967 -0.880033\n2  1.803650 -0.196350\n3  1.100880 -0.899120\n4  1.017859 -0.982141\n"", ""pd.concat([df, df.textcol.apply(lambda s: pd.Series({'feature1':s+1, 'feature2':s-1}))], axis=1)\n    textcol feature1  feature2\n0  0.626524 1.626524 -0.373476\n1  0.119967 1.119967 -0.880033\n2  0.803650 1.803650 -0.196350\n3  0.100880 1.100880 -0.899120\n4  0.017859 1.017859 -0.982141\n""]"
764;;1;16245109;16236684.0;2;44;;;"<p>Building off of user1827356 's answer, you can do the assignment in one pass using <code>df.merge</code>:</p>

<pre><code>df.merge(df.textcol.apply(lambda s: pd.Series({'feature1':s+1, 'feature2':s-1})), 
    left_index=True, right_index=True)

    textcol  feature1  feature2
0  0.772692  1.772692 -0.227308
1  0.857210  1.857210 -0.142790
2  0.065639  1.065639 -0.934361
3  0.819160  1.819160 -0.180840
4  0.088212  1.088212 -0.911788
</code></pre>
";;"[""df.merge(df.textcol.apply(lambda s: pd.Series({'feature1':s+1, 'feature2':s-1})), \n    left_index=True, right_index=True)\n\n    textcol  feature1  feature2\n0  0.772692  1.772692 -0.227308\n1  0.857210  1.857210 -0.142790\n2  0.065639  1.065639 -0.934361\n3  0.819160  1.819160 -0.180840\n4  0.088212  1.088212 -0.911788\n""]";"['df.merge', ""df.merge(df.textcol.apply(lambda s: pd.Series({'feature1':s+1, 'feature2':s-1})), \n    left_index=True, right_index=True)\n\n    textcol  feature1  feature2\n0  0.772692  1.772692 -0.227308\n1  0.857210  1.857210 -0.142790\n2  0.065639  1.065639 -0.934361\n3  0.819160  1.819160 -0.180840\n4  0.088212  1.088212 -0.911788\n""]"
765;;4;16255680;16249736.0;2;52;;;"<p><code>pymongo</code> might give you a hand, followings are some codes I'm using:</p>

<pre><code>import pandas as pd
from pymongo import MongoClient


def _connect_mongo(host, port, username, password, db):
    """""" A util for making a connection to mongo """"""

    if username and password:
        mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % (username, password, host, port, db)
        conn = MongoClient(mongo_uri)
    else:
        conn = MongoClient(host, port)


    return conn[db]


def read_mongo(db, collection, query={}, host='localhost', port=27017, username=None, password=None, no_id=True):
    """""" Read from Mongo and Store into DataFrame """"""

    # Connect to MongoDB
    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)

    # Make a query to the specific DB and Collection
    cursor = db[collection].find(query)

    # Expand the cursor and construct the DataFrame
    df =  pd.DataFrame(list(cursor))

    # Delete the _id
    if no_id:
        del df['_id']

    return df
</code></pre>
";;"['import pandas as pd\nfrom pymongo import MongoClient\n\n\ndef _connect_mongo(host, port, username, password, db):\n    """""" A util for making a connection to mongo """"""\n\n    if username and password:\n        mongo_uri = \'mongodb://%s:%s@%s:%s/%s\' % (username, password, host, port, db)\n        conn = MongoClient(mongo_uri)\n    else:\n        conn = MongoClient(host, port)\n\n\n    return conn[db]\n\n\ndef read_mongo(db, collection, query={}, host=\'localhost\', port=27017, username=None, password=None, no_id=True):\n    """""" Read from Mongo and Store into DataFrame """"""\n\n    # Connect to MongoDB\n    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\n\n    # Make a query to the specific DB and Collection\n    cursor = db[collection].find(query)\n\n    # Expand the cursor and construct the DataFrame\n    df =  pd.DataFrame(list(cursor))\n\n    # Delete the _id\n    if no_id:\n        del df[\'_id\']\n\n    return df\n']";"['pymongo', 'import pandas as pd\nfrom pymongo import MongoClient\n\n\ndef _connect_mongo(host, port, username, password, db):\n    """""" A util for making a connection to mongo """"""\n\n    if username and password:\n        mongo_uri = \'mongodb://%s:%s@%s:%s/%s\' % (username, password, host, port, db)\n        conn = MongoClient(mongo_uri)\n    else:\n        conn = MongoClient(host, port)\n\n\n    return conn[db]\n\n\ndef read_mongo(db, collection, query={}, host=\'localhost\', port=27017, username=None, password=None, no_id=True):\n    """""" Read from Mongo and Store into DataFrame """"""\n\n    # Connect to MongoDB\n    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\n\n    # Make a query to the specific DB and Collection\n    cursor = db[collection].find(query)\n\n    # Expand the cursor and construct the DataFrame\n    df =  pd.DataFrame(list(cursor))\n\n    # Delete the _id\n    if no_id:\n        del df[\'_id\']\n\n    return df\n']"
766;;2;16266318;16266019.0;2;25;;;"<p>Can't you do, where <code>df</code> is your DataFrame:</p>

<pre><code>times = pd.to_datetime(df.timestamp_col)
df.groupby([times.hour, times.minute]).value_col.sum()
</code></pre>
";;['times = pd.to_datetime(df.timestamp_col)\ndf.groupby([times.hour, times.minute]).value_col.sum()\n'];['df', 'times = pd.to_datetime(df.timestamp_col)\ndf.groupby([times.hour, times.minute]).value_col.sum()\n']
767;;2;16271849;11697887.0;2;32;;;"<pre><code>import pandas as pd
import datetime
from myapp.models import BlogPost

df = pd.DataFrame(list(BlogPost.objects.all().values()))
df = pd.DataFrame(list(BlogPost.objects.filter(date__gte=datetime.datetime(2012, 5, 1)).values()))

# limit which fields
df = pd.DataFrame(list(BlogPost.objects.all().values('author', 'date', 'slug')))
</code></pre>

<p>The above is how I do the same thing. The most useful addition is specifying which fields you are interested in. If it's only a subset of the available fields you are interested in, then this would give a performance boost I imagine.</p>
";;"[""import pandas as pd\nimport datetime\nfrom myapp.models import BlogPost\n\ndf = pd.DataFrame(list(BlogPost.objects.all().values()))\ndf = pd.DataFrame(list(BlogPost.objects.filter(date__gte=datetime.datetime(2012, 5, 1)).values()))\n\n# limit which fields\ndf = pd.DataFrame(list(BlogPost.objects.all().values('author', 'date', 'slug')))\n""]";"[""import pandas as pd\nimport datetime\nfrom myapp.models import BlogPost\n\ndf = pd.DataFrame(list(BlogPost.objects.all().values()))\ndf = pd.DataFrame(list(BlogPost.objects.filter(date__gte=datetime.datetime(2012, 5, 1)).values()))\n\n# limit which fields\ndf = pd.DataFrame(list(BlogPost.objects.all().values('author', 'date', 'slug')))\n""]"
768;;2;16327135;16327055.0;2;117;;;"<p>IIUC, assignment should fill:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({""A"": [1,2,3], ""B"": [2,3,4]})
&gt;&gt;&gt; df
   A  B
0  1  2
1  2  3
2  3  4
&gt;&gt;&gt; df[""C""] = """"
&gt;&gt;&gt; df[""D""] = np.nan
&gt;&gt;&gt; df
   A  B C   D
0  1  2   NaN
1  2  3   NaN
2  3  4   NaN
</code></pre>
";;"['>>> df = pd.DataFrame({""A"": [1,2,3], ""B"": [2,3,4]})\n>>> df\n   A  B\n0  1  2\n1  2  3\n2  3  4\n>>> df[""C""] = """"\n>>> df[""D""] = np.nan\n>>> df\n   A  B C   D\n0  1  2   NaN\n1  2  3   NaN\n2  3  4   NaN\n']";"['>>> df = pd.DataFrame({""A"": [1,2,3], ""B"": [2,3,4]})\n>>> df\n   A  B\n0  1  2\n1  2  3\n2  3  4\n>>> df[""C""] = """"\n>>> df[""D""] = np.nan\n>>> df\n   A  B C   D\n0  1  2   NaN\n1  2  3   NaN\n2  3  4   NaN\n']"
769;;1;16354103;16353729.0;2;12;;;"<p>If you just want to compute (column a) % (column b), you don't need <code>apply</code>, just do it directly:</p>

<pre><code>In [7]: df['a'] % df['c']                                                                                                                                                        
Out[7]: 
0   -1.132022                                                                                                                                                                    
1   -0.939493                                                                                                                                                                    
2    0.201931                                                                                                                                                                    
3    0.511374                                                                                                                                                                    
4   -0.694647                                                                                                                                                                    
5   -0.023486                                                                                                                                                                    
Name: a
</code></pre>
";;"[""In [7]: df['a'] % df['c']                                                                                                                                                        \nOut[7]: \n0   -1.132022                                                                                                                                                                    \n1   -0.939493                                                                                                                                                                    \n2    0.201931                                                                                                                                                                    \n3    0.511374                                                                                                                                                                    \n4   -0.694647                                                                                                                                                                    \n5   -0.023486                                                                                                                                                                    \nName: a\n""]";"['apply', ""In [7]: df['a'] % df['c']                                                                                                                                                        \nOut[7]: \n0   -1.132022                                                                                                                                                                    \n1   -0.939493                                                                                                                                                                    \n2    0.201931                                                                                                                                                                    \n3    0.511374                                                                                                                                                                    \n4   -0.694647                                                                                                                                                                    \n5   -0.023486                                                                                                                                                                    \nName: a\n""]"
770;;4;16354730;16353729.0;2;133;;;"<p>Seems you forgot the <code>''</code> of your string.</p>

<pre><code>In [43]: df['Value'] = df.apply(lambda row: my_test(row['a'], row['c']), axis=1)

In [44]: df
Out[44]:
                    a    b         c     Value
          0 -1.674308  foo  0.343801  0.044698
          1 -2.163236  bar -2.046438 -0.116798
          2 -0.199115  foo -0.458050 -0.199115
          3  0.918646  bar -0.007185 -0.001006
          4  1.336830  foo  0.534292  0.268245
          5  0.976844  bar -0.773630 -0.570417
</code></pre>

<p>BTW, in my opinion, following way is more elegant:</p>

<pre><code>In [53]: def my_test2(row):
....:     return row['a'] % row['c']
....:     

In [54]: df['Value'] = df.apply(my_test2, axis=1)
</code></pre>
";;"[""In [43]: df['Value'] = df.apply(lambda row: my_test(row['a'], row['c']), axis=1)\n\nIn [44]: df\nOut[44]:\n                    a    b         c     Value\n          0 -1.674308  foo  0.343801  0.044698\n          1 -2.163236  bar -2.046438 -0.116798\n          2 -0.199115  foo -0.458050 -0.199115\n          3  0.918646  bar -0.007185 -0.001006\n          4  1.336830  foo  0.534292  0.268245\n          5  0.976844  bar -0.773630 -0.570417\n"", ""In [53]: def my_test2(row):\n....:     return row['a'] % row['c']\n....:     \n\nIn [54]: df['Value'] = df.apply(my_test2, axis=1)\n""]";"[""''"", ""In [43]: df['Value'] = df.apply(lambda row: my_test(row['a'], row['c']), axis=1)\n\nIn [44]: df\nOut[44]:\n                    a    b         c     Value\n          0 -1.674308  foo  0.343801  0.044698\n          1 -2.163236  bar -2.046438 -0.116798\n          2 -0.199115  foo -0.458050 -0.199115\n          3  0.918646  bar -0.007185 -0.001006\n          4  1.336830  foo  0.534292  0.268245\n          5  0.976844  bar -0.773630 -0.570417\n"", ""In [53]: def my_test2(row):\n....:     return row['a'] % row['c']\n....:     \n\nIn [54]: df['Value'] = df.apply(my_test2, axis=1)\n""]"
771;;5;16393023;16392921.0;2;48;;;"<p>Make the multiple axes first and pass them to the Pandas plot function, like:</p>

<pre><code>fig, axs = plt.subplots(1,2)

df['korisnika'].plot(ax=axs[0])
df['osiguranika'].plot(ax=axs[1])
</code></pre>

<p>It still gives you 1 figure, but with two different plots next to each other.</p>
";;"[""fig, axs = plt.subplots(1,2)\n\ndf['korisnika'].plot(ax=axs[0])\ndf['osiguranika'].plot(ax=axs[1])\n""]";"[""fig, axs = plt.subplots(1,2)\n\ndf['korisnika'].plot(ax=axs[0])\ndf['osiguranika'].plot(ax=axs[1])\n""]"
772;;4;16398361;16396903.0;2;63;;;"<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/indexing.html"" rel=""nofollow noreferrer""><code>iloc</code></a> like you were using <code>ix</code>, but apply a different slice...</p>

<pre><code>df2 = df1.iloc[3:] #edited since .ix is now deprecated.
</code></pre>

<p>will give you a new df without the first three rows.</p>
";;['df2 = df1.iloc[3:] #edited since .ix is now deprecated.\n'];['iloc', 'ix', 'df2 = df1.iloc[3:] #edited since .ix is now deprecated.\n']
773;;2;16433953;16424493.0;2;79;;;"<p>For version 0.11.0 you need to change both <code>display.height</code> and <code>display.max_rows</code>.</p>

<pre><code>pd.set_option('display.height', 500)
pd.set_option('display.max_rows', 500)
</code></pre>

<p>See also <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.describe_option.html"" rel=""noreferrer""><code>pd.describe_option('display')</code></a>.</p>
";;"[""pd.set_option('display.height', 500)\npd.set_option('display.max_rows', 500)\n""]";"['display.height', 'display.max_rows', ""pd.set_option('display.height', 500)\npd.set_option('display.max_rows', 500)\n"", ""pd.describe_option('display')""]"
774;;1;16476974;16476924.0;2;556;;;"<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html#pandas.DataFrame.iterrows"">iterrows</a> is a generator which yield both index and row</p>

<pre><code>In [18]: for index, row in df.iterrows():
   ....:     print row['c1'], row['c2']
   ....:     
10 100
11 110
12 120
</code></pre>
";;"[""In [18]: for index, row in df.iterrows():\n   ....:     print row['c1'], row['c2']\n   ....:     \n10 100\n11 110\n12 120\n""]";"[""In [18]: for index, row in df.iterrows():\n   ....:     print row['c1'], row['c2']\n   ....:     \n10 100\n11 110\n12 120\n""]"
775;;4;16522626;16522380.0;2;134;;;"<p>Put</p>

<pre><code>import matplotlib.pyplot as plt
</code></pre>

<p>at the top, and</p>

<pre><code>plt.show()
</code></pre>

<p>at the end.</p>
";;['import matplotlib.pyplot as plt\n', 'plt.show()\n'];['import matplotlib.pyplot as plt\n', 'plt.show()\n']
776;;0;16545324;14941366.0;2;19;;;"<p>Here's a more concise approach...</p>

<pre><code>df['a_bsum'] = df.groupby('A')['B'].transform(sum)
df.sort(['a_bsum','C'], ascending=[True, False]).drop('a_bsum', axis=1)
</code></pre>

<p>The first line adds a column to the data frame with the groupwise sum. The second line performs the sort and then removes the extra column.</p>

<p>Result:</p>

<pre><code>    A       B           C
5   baz     -2.301539   True
2   baz     -0.528172   False
1   bar     -0.611756   True
4   bar      0.865408   False
3   foo     -1.072969   True
0   foo      1.624345   False
</code></pre>

<p>NOTE: <code>sort</code> is deprecated, use <code>sort_values</code> instead</p>
";;"[""df['a_bsum'] = df.groupby('A')['B'].transform(sum)\ndf.sort(['a_bsum','C'], ascending=[True, False]).drop('a_bsum', axis=1)\n"", '    A       B           C\n5   baz     -2.301539   True\n2   baz     -0.528172   False\n1   bar     -0.611756   True\n4   bar      0.865408   False\n3   foo     -1.072969   True\n0   foo      1.624345   False\n']";"[""df['a_bsum'] = df.groupby('A')['B'].transform(sum)\ndf.sort(['a_bsum','C'], ascending=[True, False]).drop('a_bsum', axis=1)\n"", '    A       B           C\n5   baz     -2.301539   True\n2   baz     -0.528172   False\n1   bar     -0.611756   True\n4   bar      0.865408   False\n3   foo     -1.072969   True\n0   foo      1.624345   False\n', 'sort', 'sort_values']"
777;;4;16597375;16597265.0;2;156;;;"<p>That should work:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame()
&gt;&gt;&gt; data = pd.DataFrame({""A"": range(3)})
&gt;&gt;&gt; df.append(data)
   A
0  0
1  1
2  2
</code></pre>

<p>but the <code>append</code> doesn't happen in-place, so you'll have to store the output if you want it:</p>

<pre><code>&gt;&gt;&gt; df
Empty DataFrame
Columns: []
Index: []
&gt;&gt;&gt; df = df.append(data)
&gt;&gt;&gt; df
   A
0  0
1  1
2  2
</code></pre>
";;"['>>> df = pd.DataFrame()\n>>> data = pd.DataFrame({""A"": range(3)})\n>>> df.append(data)\n   A\n0  0\n1  1\n2  2\n', '>>> df\nEmpty DataFrame\nColumns: []\nIndex: []\n>>> df = df.append(data)\n>>> df\n   A\n0  0\n1  1\n2  2\n']";"['>>> df = pd.DataFrame()\n>>> data = pd.DataFrame({""A"": range(3)})\n>>> df.append(data)\n   A\n0  0\n1  1\n2  2\n', 'append', '>>> df\nEmpty DataFrame\nColumns: []\nIndex: []\n>>> df = df.append(data)\n>>> df\n   A\n0  0\n1  1\n2  2\n']"
778;;3;16629243;16628329.0;2;58;;;"<p>Updated to use pandas 0.13.1</p>

<p>1) No. <a href=""http://pandas.pydata.org/pandas-docs/dev/io.html#notes-caveats"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/dev/io.html#notes-caveats</a>. There are various ways to <em>do</em> this, e.g. have your different threads/processes write out the computation results, then have a single process combine.</p>

<p>2) depending the type of data you store, how you do it, and how you want to retrieve, HDF5 can offer vastly better performance. Storing in an <code>HDFStore</code> as a single array, float data, compressed (in other words, not storing it in a format that allows for querying), will be stored/read amazing fast. Even storing in the table format (which slows down the write performance), will offer quite good write performance. You can look at this for some detailed comparsions (which is what <code>HDFStore</code> uses under the hood). <a href=""http://www.pytables.org/"" rel=""noreferrer"">http://www.pytables.org/</a>, here's a nice picture: <img src=""https://i.stack.imgur.com/YNhCa.png"" alt=""""></p>

<p>(and since PyTables 2.3 the queries are now indexed), so perf actually is MUCH better than this
So to answer your question, if you want any kind of performance, HDF5 is the way to go.</p>

<p>Writing:</p>

<pre><code>In [14]: %timeit test_sql_write(df)
1 loops, best of 3: 6.24 s per loop

In [15]: %timeit test_hdf_fixed_write(df)
1 loops, best of 3: 237 ms per loop

In [16]: %timeit test_hdf_table_write(df)
1 loops, best of 3: 901 ms per loop

In [17]: %timeit test_csv_write(df)
1 loops, best of 3: 3.44 s per loop
</code></pre>

<p>Reading</p>

<pre><code>In [18]: %timeit test_sql_read()
1 loops, best of 3: 766 ms per loop

In [19]: %timeit test_hdf_fixed_read()
10 loops, best of 3: 19.1 ms per loop

In [20]: %timeit test_hdf_table_read()
10 loops, best of 3: 39 ms per loop

In [22]: %timeit test_csv_read()
1 loops, best of 3: 620 ms per loop
</code></pre>

<p>And here's the code</p>

<pre><code>import sqlite3
import os
from pandas.io import sql

In [3]: df = DataFrame(randn(1000000,2),columns=list('AB'))
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 1000000 entries, 0 to 999999
Data columns (total 2 columns):
A    1000000  non-null values
B    1000000  non-null values
dtypes: float64(2)

def test_sql_write(df):
    if os.path.exists('test.sql'):
        os.remove('test.sql')
    sql_db = sqlite3.connect('test.sql')
    sql.write_frame(df, name='test_table', con=sql_db)
    sql_db.close()

def test_sql_read():
    sql_db = sqlite3.connect('test.sql')
    sql.read_frame(""select * from test_table"", sql_db)
    sql_db.close()

def test_hdf_fixed_write(df):
    df.to_hdf('test_fixed.hdf','test',mode='w')

def test_csv_read():
    pd.read_csv('test.csv',index_col=0)

def test_csv_write(df):
    df.to_csv('test.csv',mode='w')    

def test_hdf_fixed_read():
    pd.read_hdf('test_fixed.hdf','test')

def test_hdf_table_write(df):
    df.to_hdf('test_table.hdf','test',format='table',mode='w')

def test_hdf_table_read():
    pd.read_hdf('test_table.hdf','test')
</code></pre>

<p>Of course YMMV.</p>
";;"['In [14]: %timeit test_sql_write(df)\n1 loops, best of 3: 6.24 s per loop\n\nIn [15]: %timeit test_hdf_fixed_write(df)\n1 loops, best of 3: 237 ms per loop\n\nIn [16]: %timeit test_hdf_table_write(df)\n1 loops, best of 3: 901 ms per loop\n\nIn [17]: %timeit test_csv_write(df)\n1 loops, best of 3: 3.44 s per loop\n', 'In [18]: %timeit test_sql_read()\n1 loops, best of 3: 766 ms per loop\n\nIn [19]: %timeit test_hdf_fixed_read()\n10 loops, best of 3: 19.1 ms per loop\n\nIn [20]: %timeit test_hdf_table_read()\n10 loops, best of 3: 39 ms per loop\n\nIn [22]: %timeit test_csv_read()\n1 loops, best of 3: 620 ms per loop\n', 'import sqlite3\nimport os\nfrom pandas.io import sql\n\nIn [3]: df = DataFrame(randn(1000000,2),columns=list(\'AB\'))\n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 1000000 entries, 0 to 999999\nData columns (total 2 columns):\nA    1000000  non-null values\nB    1000000  non-null values\ndtypes: float64(2)\n\ndef test_sql_write(df):\n    if os.path.exists(\'test.sql\'):\n        os.remove(\'test.sql\')\n    sql_db = sqlite3.connect(\'test.sql\')\n    sql.write_frame(df, name=\'test_table\', con=sql_db)\n    sql_db.close()\n\ndef test_sql_read():\n    sql_db = sqlite3.connect(\'test.sql\')\n    sql.read_frame(""select * from test_table"", sql_db)\n    sql_db.close()\n\ndef test_hdf_fixed_write(df):\n    df.to_hdf(\'test_fixed.hdf\',\'test\',mode=\'w\')\n\ndef test_csv_read():\n    pd.read_csv(\'test.csv\',index_col=0)\n\ndef test_csv_write(df):\n    df.to_csv(\'test.csv\',mode=\'w\')    \n\ndef test_hdf_fixed_read():\n    pd.read_hdf(\'test_fixed.hdf\',\'test\')\n\ndef test_hdf_table_write(df):\n    df.to_hdf(\'test_table.hdf\',\'test\',format=\'table\',mode=\'w\')\n\ndef test_hdf_table_read():\n    pd.read_hdf(\'test_table.hdf\',\'test\')\n']";"['HDFStore', 'HDFStore', 'In [14]: %timeit test_sql_write(df)\n1 loops, best of 3: 6.24 s per loop\n\nIn [15]: %timeit test_hdf_fixed_write(df)\n1 loops, best of 3: 237 ms per loop\n\nIn [16]: %timeit test_hdf_table_write(df)\n1 loops, best of 3: 901 ms per loop\n\nIn [17]: %timeit test_csv_write(df)\n1 loops, best of 3: 3.44 s per loop\n', 'In [18]: %timeit test_sql_read()\n1 loops, best of 3: 766 ms per loop\n\nIn [19]: %timeit test_hdf_fixed_read()\n10 loops, best of 3: 19.1 ms per loop\n\nIn [20]: %timeit test_hdf_table_read()\n10 loops, best of 3: 39 ms per loop\n\nIn [22]: %timeit test_csv_read()\n1 loops, best of 3: 620 ms per loop\n', 'import sqlite3\nimport os\nfrom pandas.io import sql\n\nIn [3]: df = DataFrame(randn(1000000,2),columns=list(\'AB\'))\n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 1000000 entries, 0 to 999999\nData columns (total 2 columns):\nA    1000000  non-null values\nB    1000000  non-null values\ndtypes: float64(2)\n\ndef test_sql_write(df):\n    if os.path.exists(\'test.sql\'):\n        os.remove(\'test.sql\')\n    sql_db = sqlite3.connect(\'test.sql\')\n    sql.write_frame(df, name=\'test_table\', con=sql_db)\n    sql_db.close()\n\ndef test_sql_read():\n    sql_db = sqlite3.connect(\'test.sql\')\n    sql.read_frame(""select * from test_table"", sql_db)\n    sql_db.close()\n\ndef test_hdf_fixed_write(df):\n    df.to_hdf(\'test_fixed.hdf\',\'test\',mode=\'w\')\n\ndef test_csv_read():\n    pd.read_csv(\'test.csv\',index_col=0)\n\ndef test_csv_write(df):\n    df.to_csv(\'test.csv\',mode=\'w\')    \n\ndef test_hdf_fixed_read():\n    pd.read_hdf(\'test_fixed.hdf\',\'test\')\n\ndef test_hdf_table_write(df):\n    df.to_hdf(\'test_table.hdf\',\'test\',format=\'table\',mode=\'w\')\n\ndef test_hdf_table_read():\n    pd.read_hdf(\'test_table.hdf\',\'test\')\n']"
779;;3;16667215;11346283.0;2;216;;;"<p>The <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html""><code>rename</code></a> method can take a function, for example:</p>

<pre><code>In [11]: df.columns
Out[11]: Index([u'$a', u'$b', u'$c', u'$d', u'$e'], dtype=object)

In [12]: df.rename(columns=lambda x: x[1:], inplace=True)

In [13]: df.columns
Out[13]: Index([u'a', u'b', u'c', u'd', u'e'], dtype=object)
</code></pre>
";;"[""In [11]: df.columns\nOut[11]: Index([u'$a', u'$b', u'$c', u'$d', u'$e'], dtype=object)\n\nIn [12]: df.rename(columns=lambda x: x[1:], inplace=True)\n\nIn [13]: df.columns\nOut[13]: Index([u'a', u'b', u'c', u'd', u'e'], dtype=object)\n""]";"['rename', ""In [11]: df.columns\nOut[11]: Index([u'$a', u'$b', u'$c', u'$d', u'$e'], dtype=object)\n\nIn [12]: df.rename(columns=lambda x: x[1:], inplace=True)\n\nIn [13]: df.columns\nOut[13]: Index([u'a', u'b', u'c', u'd', u'e'], dtype=object)\n""]"
780;;3;16689573;16689514.0;2;48;;;"<p>Simply using <code>df.mean()</code> will Do The Right Thing(tm) with respect to NaNs:</p>

<pre><code>&gt;&gt;&gt; df
                 A      B
DATE                     
2013-05-01  473077  71333
2013-05-02   35131  62441
2013-05-03     727  27381
2013-05-04     481   1206
2013-05-05     226   1733
2013-05-06     NaN   4064
2013-05-07     NaN  41151
2013-05-08     NaN   8144
2013-05-09     NaN     23
2013-05-10     NaN     10
&gt;&gt;&gt; df.mean(axis=1)
DATE
2013-05-01    272205.0
2013-05-02     48786.0
2013-05-03     14054.0
2013-05-04       843.5
2013-05-05       979.5
2013-05-06      4064.0
2013-05-07     41151.0
2013-05-08      8144.0
2013-05-09        23.0
2013-05-10        10.0
dtype: float64
</code></pre>

<p>You can use <code>df[[""A"", ""B""]].mean(axis=1)</code> if there are other columns to ignore.</p>
";;['>>> df\n                 A      B\nDATE                     \n2013-05-01  473077  71333\n2013-05-02   35131  62441\n2013-05-03     727  27381\n2013-05-04     481   1206\n2013-05-05     226   1733\n2013-05-06     NaN   4064\n2013-05-07     NaN  41151\n2013-05-08     NaN   8144\n2013-05-09     NaN     23\n2013-05-10     NaN     10\n>>> df.mean(axis=1)\nDATE\n2013-05-01    272205.0\n2013-05-02     48786.0\n2013-05-03     14054.0\n2013-05-04       843.5\n2013-05-05       979.5\n2013-05-06      4064.0\n2013-05-07     41151.0\n2013-05-08      8144.0\n2013-05-09        23.0\n2013-05-10        10.0\ndtype: float64\n'];"['df.mean()', '>>> df\n                 A      B\nDATE                     \n2013-05-01  473077  71333\n2013-05-02   35131  62441\n2013-05-03     727  27381\n2013-05-04     481   1206\n2013-05-05     226   1733\n2013-05-06     NaN   4064\n2013-05-07     NaN  41151\n2013-05-08     NaN   8144\n2013-05-09     NaN     23\n2013-05-10     NaN     10\n>>> df.mean(axis=1)\nDATE\n2013-05-01    272205.0\n2013-05-02     48786.0\n2013-05-03     14054.0\n2013-05-04       843.5\n2013-05-05       979.5\n2013-05-06      4064.0\n2013-05-07     41151.0\n2013-05-08      8144.0\n2013-05-09        23.0\n2013-05-10        10.0\ndtype: float64\n', 'df[[""A"", ""B""]].mean(axis=1)']"
781;;0;16729635;16729483.0;2;21;;;"<p>You can try <code>df.column_name = df.column_name.astype(float)</code>. As for the <code>NaN</code> values, you need to specify how they should be converted, but you can use the <code>.fillna</code> method to do it.</p>

<p><em>Example:</em></p>

<pre><code>In [12]: df
Out[12]: 
     a    b
0  0.1  0.2
1  NaN  0.3
2  0.4  0.5

In [13]: df.a.values
Out[13]: array(['0.1', nan, '0.4'], dtype=object)

In [14]: df.a = df.a.astype(float).fillna(0.0)

In [15]: df
Out[15]: 
     a    b
0  0.1  0.2
1  0.0  0.3
2  0.4  0.5

In [16]: df.a.values
Out[16]: array([ 0.1,  0. ,  0.4])
</code></pre>
";;"[""In [12]: df\nOut[12]: \n     a    b\n0  0.1  0.2\n1  NaN  0.3\n2  0.4  0.5\n\nIn [13]: df.a.values\nOut[13]: array(['0.1', nan, '0.4'], dtype=object)\n\nIn [14]: df.a = df.a.astype(float).fillna(0.0)\n\nIn [15]: df\nOut[15]: \n     a    b\n0  0.1  0.2\n1  0.0  0.3\n2  0.4  0.5\n\nIn [16]: df.a.values\nOut[16]: array([ 0.1,  0. ,  0.4])\n""]";"['df.column_name = df.column_name.astype(float)', 'NaN', '.fillna', ""In [12]: df\nOut[12]: \n     a    b\n0  0.1  0.2\n1  NaN  0.3\n2  0.4  0.5\n\nIn [13]: df.a.values\nOut[13]: array(['0.1', nan, '0.4'], dtype=object)\n\nIn [14]: df.a = df.a.astype(float).fillna(0.0)\n\nIn [15]: df\nOut[15]: \n     a    b\n0  0.1  0.2\n1  0.0  0.3\n2  0.4  0.5\n\nIn [16]: df.a.values\nOut[16]: array([ 0.1,  0. ,  0.4])\n""]"
782;;5;16729808;16729574.0;2;100;;;"<p>If you have a DataFrame with only one row, then access the first (only) row as a Series using <code>iloc</code>, and then the value using the column name:</p>

<pre><code>In [3]: sub_df
Out[3]:
          A         B
2 -0.133653 -0.030854

In [4]: sub_df.iloc[0]
Out[4]:
A   -0.133653
B   -0.030854
Name: 2, dtype: float64

In [5]: sub_df.iloc[0]['A']
Out[5]: -0.13365288513107493
</code></pre>
";;"[""In [3]: sub_df\nOut[3]:\n          A         B\n2 -0.133653 -0.030854\n\nIn [4]: sub_df.iloc[0]\nOut[4]:\nA   -0.133653\nB   -0.030854\nName: 2, dtype: float64\n\nIn [5]: sub_df.iloc[0]['A']\nOut[5]: -0.13365288513107493\n""]";"['iloc', ""In [3]: sub_df\nOut[3]:\n          A         B\n2 -0.133653 -0.030854\n\nIn [4]: sub_df.iloc[0]\nOut[4]:\nA   -0.133653\nB   -0.030854\nName: 2, dtype: float64\n\nIn [5]: sub_df.iloc[0]['A']\nOut[5]: -0.13365288513107493\n""]"
783;;6;16735476;16729483.0;2;43;;;"<p>This is available in 0.11. Forces conversion (or set's to nan)
This will work even when <code>astype</code> will fail; its also series by series
so it won't convert say a complete string column</p>

<pre><code>In [10]: df = DataFrame(dict(A = Series(['1.0','1']), B = Series(['1.0','foo'])))

In [11]: df
Out[11]: 
     A    B
0  1.0  1.0
1    1  foo

In [12]: df.dtypes
Out[12]: 
A    object
B    object
dtype: object

In [13]: df.convert_objects(convert_numeric=True)
Out[13]: 
   A   B
0  1   1
1  1 NaN

In [14]: df.convert_objects(convert_numeric=True).dtypes
Out[14]: 
A    float64
B    float64
dtype: object
</code></pre>
";;"[""In [10]: df = DataFrame(dict(A = Series(['1.0','1']), B = Series(['1.0','foo'])))\n\nIn [11]: df\nOut[11]: \n     A    B\n0  1.0  1.0\n1    1  foo\n\nIn [12]: df.dtypes\nOut[12]: \nA    object\nB    object\ndtype: object\n\nIn [13]: df.convert_objects(convert_numeric=True)\nOut[13]: \n   A   B\n0  1   1\n1  1 NaN\n\nIn [14]: df.convert_objects(convert_numeric=True).dtypes\nOut[14]: \nA    float64\nB    float64\ndtype: object\n""]";"['astype', ""In [10]: df = DataFrame(dict(A = Series(['1.0','1']), B = Series(['1.0','foo'])))\n\nIn [11]: df\nOut[11]: \n     A    B\n0  1.0  1.0\n1    1  foo\n\nIn [12]: df.dtypes\nOut[12]: \nA    object\nB    object\ndtype: object\n\nIn [13]: df.convert_objects(convert_numeric=True)\nOut[13]: \n   A   B\n0  1   1\n1  1 NaN\n\nIn [14]: df.convert_objects(convert_numeric=True).dtypes\nOut[14]: \nA    float64\nB    float64\ndtype: object\n""]"
784;;0;16735536;16729574.0;2;51;;;"<p>These are fast access for scalars</p>

<pre><code>In [15]: df = DataFrame(randn(5,3),columns=list('ABC'))

In [16]: df
Out[16]: 
          A         B         C
0 -0.074172 -0.090626  0.038272
1 -0.128545  0.762088 -0.714816
2  0.201498 -0.734963  0.558397
3  1.563307 -1.186415  0.848246
4  0.205171  0.962514  0.037709

In [17]: df.iat[0,0]
Out[17]: -0.074171888537611502

In [18]: df.at[0,'A']
Out[18]: -0.074171888537611502
</code></pre>
";;"[""In [15]: df = DataFrame(randn(5,3),columns=list('ABC'))\n\nIn [16]: df\nOut[16]: \n          A         B         C\n0 -0.074172 -0.090626  0.038272\n1 -0.128545  0.762088 -0.714816\n2  0.201498 -0.734963  0.558397\n3  1.563307 -1.186415  0.848246\n4  0.205171  0.962514  0.037709\n\nIn [17]: df.iat[0,0]\nOut[17]: -0.074171888537611502\n\nIn [18]: df.at[0,'A']\nOut[18]: -0.074171888537611502\n""]";"[""In [15]: df = DataFrame(randn(5,3),columns=list('ABC'))\n\nIn [16]: df\nOut[16]: \n          A         B         C\n0 -0.074172 -0.090626  0.038272\n1 -0.128545  0.762088 -0.714816\n2  0.201498 -0.734963  0.558397\n3  1.563307 -1.186415  0.848246\n4  0.205171  0.962514  0.037709\n\nIn [17]: df.iat[0,0]\nOut[17]: -0.074171888537611502\n\nIn [18]: df.at[0,'A']\nOut[18]: -0.074171888537611502\n""]"
785;;0;16780413;16777570.0;2;39;;;"<p>Note this is using numpy >= 1.7, for numpy &lt; 1.7, see the conversion here: <a href=""http://pandas.pydata.org/pandas-docs/dev/timeseries.html#time-deltas"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/dev/timeseries.html#time-deltas</a></p>

<p>Your original frame, with a datetime index</p>

<pre><code>In [196]: df
Out[196]: 
                     value
2012-03-16 23:50:00      1
2012-03-16 23:56:00      2
2012-03-17 00:08:00      3
2012-03-17 00:10:00      4
2012-03-17 00:12:00      5
2012-03-17 00:20:00      6
2012-03-20 00:43:00      7

In [199]: df.index
Out[199]: 
&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
[2012-03-16 23:50:00, ..., 2012-03-20 00:43:00]
Length: 7, Freq: None, Timezone: None
</code></pre>

<p>Here is the timedelta64 of what you want</p>

<pre><code>In [200]: df['tvalue'] = df.index

In [201]: df['delta'] = (df['tvalue']-df['tvalue'].shift()).fillna(0)

In [202]: df
Out[202]: 
                     value              tvalue            delta
2012-03-16 23:50:00      1 2012-03-16 23:50:00         00:00:00
2012-03-16 23:56:00      2 2012-03-16 23:56:00         00:06:00
2012-03-17 00:08:00      3 2012-03-17 00:08:00         00:12:00
2012-03-17 00:10:00      4 2012-03-17 00:10:00         00:02:00
2012-03-17 00:12:00      5 2012-03-17 00:12:00         00:02:00
2012-03-17 00:20:00      6 2012-03-17 00:20:00         00:08:00
2012-03-20 00:43:00      7 2012-03-20 00:43:00 3 days, 00:23:00
</code></pre>

<p>Getting out the answer while disregarding the day difference (your last day is 3/20, prior is 3/17), actually is tricky</p>

<pre><code>In [204]: df['ans'] = df['delta'].apply(lambda x: x  / np.timedelta64(1,'m')).astype('int64') % (24*60)

In [205]: df
Out[205]: 
                     value              tvalue            delta  ans
2012-03-16 23:50:00      1 2012-03-16 23:50:00         00:00:00    0
2012-03-16 23:56:00      2 2012-03-16 23:56:00         00:06:00    6
2012-03-17 00:08:00      3 2012-03-17 00:08:00         00:12:00   12
2012-03-17 00:10:00      4 2012-03-17 00:10:00         00:02:00    2
2012-03-17 00:12:00      5 2012-03-17 00:12:00         00:02:00    2
2012-03-17 00:20:00      6 2012-03-17 00:20:00         00:08:00    8
2012-03-20 00:43:00      7 2012-03-20 00:43:00 3 days, 00:23:00   23
</code></pre>
";;"[""In [196]: df\nOut[196]: \n                     value\n2012-03-16 23:50:00      1\n2012-03-16 23:56:00      2\n2012-03-17 00:08:00      3\n2012-03-17 00:10:00      4\n2012-03-17 00:12:00      5\n2012-03-17 00:20:00      6\n2012-03-20 00:43:00      7\n\nIn [199]: df.index\nOut[199]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-03-16 23:50:00, ..., 2012-03-20 00:43:00]\nLength: 7, Freq: None, Timezone: None\n"", ""In [200]: df['tvalue'] = df.index\n\nIn [201]: df['delta'] = (df['tvalue']-df['tvalue'].shift()).fillna(0)\n\nIn [202]: df\nOut[202]: \n                     value              tvalue            delta\n2012-03-16 23:50:00      1 2012-03-16 23:50:00         00:00:00\n2012-03-16 23:56:00      2 2012-03-16 23:56:00         00:06:00\n2012-03-17 00:08:00      3 2012-03-17 00:08:00         00:12:00\n2012-03-17 00:10:00      4 2012-03-17 00:10:00         00:02:00\n2012-03-17 00:12:00      5 2012-03-17 00:12:00         00:02:00\n2012-03-17 00:20:00      6 2012-03-17 00:20:00         00:08:00\n2012-03-20 00:43:00      7 2012-03-20 00:43:00 3 days, 00:23:00\n"", ""In [204]: df['ans'] = df['delta'].apply(lambda x: x  / np.timedelta64(1,'m')).astype('int64') % (24*60)\n\nIn [205]: df\nOut[205]: \n                     value              tvalue            delta  ans\n2012-03-16 23:50:00      1 2012-03-16 23:50:00         00:00:00    0\n2012-03-16 23:56:00      2 2012-03-16 23:56:00         00:06:00    6\n2012-03-17 00:08:00      3 2012-03-17 00:08:00         00:12:00   12\n2012-03-17 00:10:00      4 2012-03-17 00:10:00         00:02:00    2\n2012-03-17 00:12:00      5 2012-03-17 00:12:00         00:02:00    2\n2012-03-17 00:20:00      6 2012-03-17 00:20:00         00:08:00    8\n2012-03-20 00:43:00      7 2012-03-20 00:43:00 3 days, 00:23:00   23\n""]";"[""In [196]: df\nOut[196]: \n                     value\n2012-03-16 23:50:00      1\n2012-03-16 23:56:00      2\n2012-03-17 00:08:00      3\n2012-03-17 00:10:00      4\n2012-03-17 00:12:00      5\n2012-03-17 00:20:00      6\n2012-03-20 00:43:00      7\n\nIn [199]: df.index\nOut[199]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-03-16 23:50:00, ..., 2012-03-20 00:43:00]\nLength: 7, Freq: None, Timezone: None\n"", ""In [200]: df['tvalue'] = df.index\n\nIn [201]: df['delta'] = (df['tvalue']-df['tvalue'].shift()).fillna(0)\n\nIn [202]: df\nOut[202]: \n                     value              tvalue            delta\n2012-03-16 23:50:00      1 2012-03-16 23:50:00         00:00:00\n2012-03-16 23:56:00      2 2012-03-16 23:56:00         00:06:00\n2012-03-17 00:08:00      3 2012-03-17 00:08:00         00:12:00\n2012-03-17 00:10:00      4 2012-03-17 00:10:00         00:02:00\n2012-03-17 00:12:00      5 2012-03-17 00:12:00         00:02:00\n2012-03-17 00:20:00      6 2012-03-17 00:20:00         00:08:00\n2012-03-20 00:43:00      7 2012-03-20 00:43:00 3 days, 00:23:00\n"", ""In [204]: df['ans'] = df['delta'].apply(lambda x: x  / np.timedelta64(1,'m')).astype('int64') % (24*60)\n\nIn [205]: df\nOut[205]: \n                     value              tvalue            delta  ans\n2012-03-16 23:50:00      1 2012-03-16 23:50:00         00:00:00    0\n2012-03-16 23:56:00      2 2012-03-16 23:56:00         00:06:00    6\n2012-03-17 00:08:00      3 2012-03-17 00:08:00         00:12:00   12\n2012-03-17 00:10:00      4 2012-03-17 00:10:00         00:02:00    2\n2012-03-17 00:12:00      5 2012-03-17 00:12:00         00:02:00    2\n2012-03-17 00:20:00      6 2012-03-17 00:20:00         00:08:00    8\n2012-03-20 00:43:00      7 2012-03-20 00:43:00 3 days, 00:23:00   23\n""]"
786;;3;16789254;16782323.0;2;40;;;"<p>As @Jeff mentions there are a few ways to do this, but I recommend using loc/iloc to be more explicit (and raise errors early if your trying something ambiguous):</p>

<pre><code>In [10]: df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])

In [11]: df
Out[11]:
   A  B
0  1  2
1  3  4

In [12]: df[['A']]

In [13]: df[[0]]

In [14]: df.loc[:, ['A']]

In [15]: df.iloc[:, [0]]

Out[12-15]:  # they all return the same thing:
   A
0  1
1  3
</code></pre>

<p>The latter two choices remove ambiguity in the case of integer column names (precisely why loc/iloc were created). For example:</p>

<pre><code>In [16]: df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 0])

In [17]: df
Out[17]:
   A  0
0  1  2
1  3  4

In [18]: df[[0]]  # ambiguous
Out[18]:
   A
0  1
1  3
</code></pre>
";;"[""In [10]: df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])\n\nIn [11]: df\nOut[11]:\n   A  B\n0  1  2\n1  3  4\n\nIn [12]: df[['A']]\n\nIn [13]: df[[0]]\n\nIn [14]: df.loc[:, ['A']]\n\nIn [15]: df.iloc[:, [0]]\n\nOut[12-15]:  # they all return the same thing:\n   A\n0  1\n1  3\n"", ""In [16]: df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 0])\n\nIn [17]: df\nOut[17]:\n   A  0\n0  1  2\n1  3  4\n\nIn [18]: df[[0]]  # ambiguous\nOut[18]:\n   A\n0  1\n1  3\n""]";"[""In [10]: df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])\n\nIn [11]: df\nOut[11]:\n   A  B\n0  1  2\n1  3  4\n\nIn [12]: df[['A']]\n\nIn [13]: df[[0]]\n\nIn [14]: df.loc[:, ['A']]\n\nIn [15]: df.iloc[:, [0]]\n\nOut[12-15]:  # they all return the same thing:\n   A\n0  1\n1  3\n"", ""In [16]: df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 0])\n\nIn [17]: df\nOut[17]:\n   A  0\n0  1  2\n1  3  4\n\nIn [18]: df[[0]]  # ambiguous\nOut[18]:\n   A\n0  1\n1  3\n""]"
787;;0;16789834;11361985.0;2;20;;;"<p>I know this is an old question, but I have just had a similar problem and I think what I did would work for you too.</p>

<p>I used the to_csv() method and wrote to stdout:</p>

<pre><code>import sys

paramdata.to_csv(sys.stdout)
</code></pre>

<p>This should dump the whole dataframe whether it's nicely-printable or not, and you can use the to_csv parameters to configure column separators, whether the index is printed, etc.</p>
";;['import sys\n\nparamdata.to_csv(sys.stdout)\n'];['import sys\n\nparamdata.to_csv(sys.stdout)\n']
788;;0;16826250;13331698.0;2;7;;;"<p>The way you have written f it needs two inputs. If you look at the error message it says you are not providing two inputs to f, just one. The error message is correct.<br>
The mismatch is because df[['col1','col2']] returns a single dataframe with two columns, not two separate columns.</p>

<p>You need to change your f so that it takes a single input, keep the above data frame as input, then break it up into x,y <em>inside</em> the function body. Then do whatever you need and return a single value.</p>

<p>You need this function signature because the syntax is .apply(f)
So f needs to take the single thing = dataframe and not two things which is what your current f expects.  </p>

<p>Since you haven't provided the body of f I can't help in anymore detail - but this should provide the way out without fundamentally changing your code or using some other methods rather than apply</p>
";;[];[]
789;;6;16853161;16852911.0;2;42;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html"">astype</a></p>

<pre><code>In [31]: df
Out[31]: 
   a        time
0  1  2013-01-01
1  2  2013-01-02
2  3  2013-01-03

In [32]: df['time'] = df['time'].astype('datetime64[ns]')

In [33]: df
Out[33]: 
   a                time
0  1 2013-01-01 00:00:00
1  2 2013-01-02 00:00:00
2  3 2013-01-03 00:00:00
</code></pre>
";;"[""In [31]: df\nOut[31]: \n   a        time\n0  1  2013-01-01\n1  2  2013-01-02\n2  3  2013-01-03\n\nIn [32]: df['time'] = df['time'].astype('datetime64[ns]')\n\nIn [33]: df\nOut[33]: \n   a                time\n0  1 2013-01-01 00:00:00\n1  2 2013-01-02 00:00:00\n2  3 2013-01-03 00:00:00\n""]";"[""In [31]: df\nOut[31]: \n   a        time\n0  1  2013-01-01\n1  2  2013-01-02\n2  3  2013-01-03\n\nIn [32]: df['time'] = df['time'].astype('datetime64[ns]')\n\nIn [33]: df\nOut[33]: \n   a                time\n0  1 2013-01-01 00:00:00\n1  2 2013-01-02 00:00:00\n2  3 2013-01-03 00:00:00\n""]"
790;;9;16854430;16852911.0;2;58;;;"<p>Essentially equivalent to @waitingkuo, but I would use <code>to_datetime</code> here (it seems a little cleaner, and offers some additional functionality e.g. <code>dayfirst</code>):</p>

<pre><code>In [11]: df
Out[11]:
   a        time
0  1  2013-01-01
1  2  2013-01-02
2  3  2013-01-03

In [12]: pd.to_datetime(df['time'])
Out[12]:
0   2013-01-01 00:00:00
1   2013-01-02 00:00:00
2   2013-01-03 00:00:00
Name: time, dtype: datetime64[ns]

In [13]: df['time'] = pd.to_datetime(df['time'])

In [14]: df
Out[14]:
   a                time
0  1 2013-01-01 00:00:00
1  2 2013-01-02 00:00:00
2  3 2013-01-03 00:00:00
</code></pre>
";;"[""In [11]: df\nOut[11]:\n   a        time\n0  1  2013-01-01\n1  2  2013-01-02\n2  3  2013-01-03\n\nIn [12]: pd.to_datetime(df['time'])\nOut[12]:\n0   2013-01-01 00:00:00\n1   2013-01-02 00:00:00\n2   2013-01-03 00:00:00\nName: time, dtype: datetime64[ns]\n\nIn [13]: df['time'] = pd.to_datetime(df['time'])\n\nIn [14]: df\nOut[14]:\n   a                time\n0  1 2013-01-01 00:00:00\n1  2 2013-01-02 00:00:00\n2  3 2013-01-03 00:00:00\n""]";"['to_datetime', 'dayfirst', ""In [11]: df\nOut[11]:\n   a        time\n0  1  2013-01-01\n1  2  2013-01-02\n2  3  2013-01-03\n\nIn [12]: pd.to_datetime(df['time'])\nOut[12]:\n0   2013-01-01 00:00:00\n1   2013-01-02 00:00:00\n2   2013-01-03 00:00:00\nName: time, dtype: datetime64[ns]\n\nIn [13]: df['time'] = pd.to_datetime(df['time'])\n\nIn [14]: df\nOut[14]:\n   a                time\n0  1 2013-01-01 00:00:00\n1  2 2013-01-02 00:00:00\n2  3 2013-01-03 00:00:00\n""]"
791;;0;16884805;10636024.0;2;7;;;"<p>You could use the to_html() dataframe method to convert the dataframe to html and display it in your browser. Here is an example assuming you have a dataframe called df. You should check the documentation to see what other options are available in the to_html() method.</p>

<pre><code># Format floating point numbers with 2 decimal places.
data_table = df.to_html(float_format=lambda x: '%6.2f' % x,
    classes=""table display"")
# The to_html() method forces a html table border of 1 pixel.
# I use 0  in my table so I  change the html, since there is no 
# border argument in the to_html() method.
data_table = data_table.replace('border=""1""','border=""0""')
# I alson like to display blanks instead on nan.
data_table = data_table.replace('nan', '')
</code></pre>

<p>If you want to get the table to be nicely formatted and scrollable then you can use the datatables plug-in for jQuery <a href=""http://www.datatables.net/"" rel=""noreferrer"">www.datatables.net</a>. Here is the javascript I use to display a table the scrolls in both x and y directiions.</p>

<pre><code>$('.table').dataTable({
    ""bPaginate"": true,
    ""bLengthChange"": true,
    ""bSort"": false,
    ""bStateSave"": true,
    ""sScrollY"": 900,
    ""sScrollX"": 1000,
    ""aLengthMenu"": [[50, 100, 250, 500, 1000, -1], [50, 100, 250, 500, 1000, ""All""]],
    ""iDisplayLength"": 100,
});
</code></pre>
";;"['# Format floating point numbers with 2 decimal places.\ndata_table = df.to_html(float_format=lambda x: \'%6.2f\' % x,\n    classes=""table display"")\n# The to_html() method forces a html table border of 1 pixel.\n# I use 0  in my table so I  change the html, since there is no \n# border argument in the to_html() method.\ndata_table = data_table.replace(\'border=""1""\',\'border=""0""\')\n# I alson like to display blanks instead on nan.\ndata_table = data_table.replace(\'nan\', \'\')\n', '$(\'.table\').dataTable({\n    ""bPaginate"": true,\n    ""bLengthChange"": true,\n    ""bSort"": false,\n    ""bStateSave"": true,\n    ""sScrollY"": 900,\n    ""sScrollX"": 1000,\n    ""aLengthMenu"": [[50, 100, 250, 500, 1000, -1], [50, 100, 250, 500, 1000, ""All""]],\n    ""iDisplayLength"": 100,\n});\n']";"['# Format floating point numbers with 2 decimal places.\ndata_table = df.to_html(float_format=lambda x: \'%6.2f\' % x,\n    classes=""table display"")\n# The to_html() method forces a html table border of 1 pixel.\n# I use 0  in my table so I  change the html, since there is no \n# border argument in the to_html() method.\ndata_table = data_table.replace(\'border=""1""\',\'border=""0""\')\n# I alson like to display blanks instead on nan.\ndata_table = data_table.replace(\'nan\', \'\')\n', '$(\'.table\').dataTable({\n    ""bPaginate"": true,\n    ""bLengthChange"": true,\n    ""bSort"": false,\n    ""bStateSave"": true,\n    ""sScrollY"": 900,\n    ""sScrollX"": 1000,\n    ""aLengthMenu"": [[50, 100, 250, 500, 1000, -1], [50, 100, 250, 500, 1000, ""All""]],\n    ""iDisplayLength"": 100,\n});\n']"
792;;6;16896091;16888888.0;2;40;;;"<p>I usually create a dictionary containing a <code>DataFrame</code> for every sheet:</p>

<pre><code>xl_file = pd.ExcelFile(file_name)

dfs = {sheet_name: xl_file.parse(sheet_name) 
          for sheet_name in xl_file.sheet_names}
</code></pre>

<hr>

<p>Update: In pandas version 0.20.0+ (edit: perhaps 0.19.2 as well) you will get this behavior more cleanly by passing <a href=""https://github.com/pandas-dev/pandas/issues/9930"" rel=""nofollow noreferrer""><code>sheetname=None</code></a> to <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html"" rel=""nofollow noreferrer""><code>read_excel</code></a>:</p>

<pre><code>dfs = pd.read_excel(file_name, sheetname=None)
</code></pre>
";;['xl_file = pd.ExcelFile(file_name)\n\ndfs = {sheet_name: xl_file.parse(sheet_name) \n          for sheet_name in xl_file.sheet_names}\n', 'dfs = pd.read_excel(file_name, sheetname=None)\n'];['DataFrame', 'xl_file = pd.ExcelFile(file_name)\n\ndfs = {sheet_name: xl_file.parse(sheet_name) \n          for sheet_name in xl_file.sheet_names}\n', 'sheetname=None', 'read_excel', 'dfs = pd.read_excel(file_name, sheetname=None)\n']
793;;6;16923367;16923281.0;2;353;;;"<p>To delimit by a tab you can use the <code>sep</code> argument of <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html""><code>to_csv</code></a>:</p>

<pre><code>df.to_csv(file_name, sep='\t')
</code></pre>

<p>To use a specific encoding (e.g. 'utf-8') use the <code>encoding</code> argument:</p>

<pre><code>df.to_csv(file_name, sep='\t', encoding='utf-8')
</code></pre>
";;"[""df.to_csv(file_name, sep='\\t')\n"", ""df.to_csv(file_name, sep='\\t', encoding='utf-8')\n""]";"['sep', 'to_csv', ""df.to_csv(file_name, sep='\\t')\n"", 'encoding', ""df.to_csv(file_name, sep='\\t', encoding='utf-8')\n""]"
794;;4;16949498;16947336.0;2;41;;;"<p>There may be a more efficient way (I have a feeling <code>pandas.crosstab</code> would be useful here), but here's how I'd do it:</p>

<pre><code>import numpy as np
import pandas

df = pandas.DataFrame({""a"": np.random.random(100),
                       ""b"": np.random.random(100),
                       ""id"": np.arange(100)})

# Bin the data frame by ""a"" with 10 bins...
bins = np.linspace(df.a.min(), df.a.max(), 10)
groups = df.groupby(np.digitize(df.a, bins))

# Get the mean of each bin:
print groups.mean() # Also could do ""groups.aggregate(np.mean)""

# Similarly, the median:
print groups.median()

# Apply some arbitrary function to aggregate binned data
print groups.aggregate(lambda x: np.mean(x[x &gt; 0.5]))
</code></pre>

<hr>

<p>Edit: As the OP was asking specifically for just the means of <code>b</code> binned by the values in <code>a</code>, just do </p>

<pre><code>groups.mean().b
</code></pre>

<p>Also if you wanted the index to look nicer (e.g. display intervals as the index), as they do in @bdiamante's example, use <code>pandas.cut</code> instead of <code>numpy.digitize</code>.  (Kudos to bidamante. I didn't realize <code>pandas.cut</code> existed.)</p>

<pre><code>import numpy as np
import pandas

df = pandas.DataFrame({""a"": np.random.random(100), 
                       ""b"": np.random.random(100) + 10})

# Bin the data frame by ""a"" with 10 bins...
bins = np.linspace(df.a.min(), df.a.max(), 10)
groups = df.groupby(pandas.cut(df.a, bins))

# Get the mean of b, binned by the values in a
print groups.mean().b
</code></pre>

<p>This results in:</p>

<pre><code>a
(0.00186, 0.111]    10.421839
(0.111, 0.22]       10.427540
(0.22, 0.33]        10.538932
(0.33, 0.439]       10.445085
(0.439, 0.548]      10.313612
(0.548, 0.658]      10.319387
(0.658, 0.767]      10.367444
(0.767, 0.876]      10.469655
(0.876, 0.986]      10.571008
Name: b
</code></pre>
";;"['import numpy as np\nimport pandas\n\ndf = pandas.DataFrame({""a"": np.random.random(100),\n                       ""b"": np.random.random(100),\n                       ""id"": np.arange(100)})\n\n# Bin the data frame by ""a"" with 10 bins...\nbins = np.linspace(df.a.min(), df.a.max(), 10)\ngroups = df.groupby(np.digitize(df.a, bins))\n\n# Get the mean of each bin:\nprint groups.mean() # Also could do ""groups.aggregate(np.mean)""\n\n# Similarly, the median:\nprint groups.median()\n\n# Apply some arbitrary function to aggregate binned data\nprint groups.aggregate(lambda x: np.mean(x[x > 0.5]))\n', 'groups.mean().b\n', 'import numpy as np\nimport pandas\n\ndf = pandas.DataFrame({""a"": np.random.random(100), \n                       ""b"": np.random.random(100) + 10})\n\n# Bin the data frame by ""a"" with 10 bins...\nbins = np.linspace(df.a.min(), df.a.max(), 10)\ngroups = df.groupby(pandas.cut(df.a, bins))\n\n# Get the mean of b, binned by the values in a\nprint groups.mean().b\n', 'a\n(0.00186, 0.111]    10.421839\n(0.111, 0.22]       10.427540\n(0.22, 0.33]        10.538932\n(0.33, 0.439]       10.445085\n(0.439, 0.548]      10.313612\n(0.548, 0.658]      10.319387\n(0.658, 0.767]      10.367444\n(0.767, 0.876]      10.469655\n(0.876, 0.986]      10.571008\nName: b\n']";"['pandas.crosstab', 'import numpy as np\nimport pandas\n\ndf = pandas.DataFrame({""a"": np.random.random(100),\n                       ""b"": np.random.random(100),\n                       ""id"": np.arange(100)})\n\n# Bin the data frame by ""a"" with 10 bins...\nbins = np.linspace(df.a.min(), df.a.max(), 10)\ngroups = df.groupby(np.digitize(df.a, bins))\n\n# Get the mean of each bin:\nprint groups.mean() # Also could do ""groups.aggregate(np.mean)""\n\n# Similarly, the median:\nprint groups.median()\n\n# Apply some arbitrary function to aggregate binned data\nprint groups.aggregate(lambda x: np.mean(x[x > 0.5]))\n', 'b', 'a', 'groups.mean().b\n', 'pandas.cut', 'numpy.digitize', 'pandas.cut', 'import numpy as np\nimport pandas\n\ndf = pandas.DataFrame({""a"": np.random.random(100), \n                       ""b"": np.random.random(100) + 10})\n\n# Bin the data frame by ""a"" with 10 bins...\nbins = np.linspace(df.a.min(), df.a.max(), 10)\ngroups = df.groupby(pandas.cut(df.a, bins))\n\n# Get the mean of b, binned by the values in a\nprint groups.mean().b\n', 'a\n(0.00186, 0.111]    10.421839\n(0.111, 0.22]       10.427540\n(0.22, 0.33]        10.538932\n(0.33, 0.439]       10.445085\n(0.439, 0.548]      10.313612\n(0.548, 0.658]      10.319387\n(0.658, 0.767]      10.367444\n(0.767, 0.876]      10.469655\n(0.876, 0.986]      10.571008\nName: b\n']"
795;;2;16949500;16947336.0;2;19;;;"<p>Not 100% sure if this is what you're looking for, but here's what I think you're getting at:</p>

<pre><code>In [144]: df = DataFrame({""a"": np.random.random(100), ""b"": np.random.random(100), ""id"":   np.arange(100)})

In [145]: bins = [0, .25, .5, .75, 1]

In [146]: a_bins = df.a.groupby(cut(df.a,bins))

In [147]: b_bins = df.b.groupby(cut(df.b,bins))

In [148]: a_bins.agg([mean,median])
Out[148]:
                 mean    median
a
(0, 0.25]    0.124173  0.114613
(0.25, 0.5]  0.367703  0.358866
(0.5, 0.75]  0.624251  0.626730
(0.75, 1]    0.875395  0.869843

In [149]: b_bins.agg([mean,median])
Out[149]:
                 mean    median
b
(0, 0.25]    0.147936  0.166900
(0.25, 0.5]  0.394918  0.386729
(0.5, 0.75]  0.636111  0.655247
(0.75, 1]    0.851227  0.838805
</code></pre>

<p>Of course, I don't know what bins you had in mind, so you'll have to swap mine out for your circumstance.</p>
";;"['In [144]: df = DataFrame({""a"": np.random.random(100), ""b"": np.random.random(100), ""id"":   np.arange(100)})\n\nIn [145]: bins = [0, .25, .5, .75, 1]\n\nIn [146]: a_bins = df.a.groupby(cut(df.a,bins))\n\nIn [147]: b_bins = df.b.groupby(cut(df.b,bins))\n\nIn [148]: a_bins.agg([mean,median])\nOut[148]:\n                 mean    median\na\n(0, 0.25]    0.124173  0.114613\n(0.25, 0.5]  0.367703  0.358866\n(0.5, 0.75]  0.624251  0.626730\n(0.75, 1]    0.875395  0.869843\n\nIn [149]: b_bins.agg([mean,median])\nOut[149]:\n                 mean    median\nb\n(0, 0.25]    0.147936  0.166900\n(0.25, 0.5]  0.394918  0.386729\n(0.5, 0.75]  0.636111  0.655247\n(0.75, 1]    0.851227  0.838805\n']";"['In [144]: df = DataFrame({""a"": np.random.random(100), ""b"": np.random.random(100), ""id"":   np.arange(100)})\n\nIn [145]: bins = [0, .25, .5, .75, 1]\n\nIn [146]: a_bins = df.a.groupby(cut(df.a,bins))\n\nIn [147]: b_bins = df.b.groupby(cut(df.b,bins))\n\nIn [148]: a_bins.agg([mean,median])\nOut[148]:\n                 mean    median\na\n(0, 0.25]    0.124173  0.114613\n(0.25, 0.5]  0.367703  0.358866\n(0.5, 0.75]  0.624251  0.626730\n(0.75, 1]    0.875395  0.869843\n\nIn [149]: b_bins.agg([mean,median])\nOut[149]:\n                 mean    median\nb\n(0, 0.25]    0.147936  0.166900\n(0.25, 0.5]  0.394918  0.386729\n(0.5, 0.75]  0.636111  0.655247\n(0.75, 1]    0.851227  0.838805\n']"
796;;3;17001474;17001389.0;2;175;;;"<pre><code>B       business day frequency
C       custom business day frequency (experimental)
D       calendar day frequency
W       weekly frequency
M       month end frequency
SM      semi-month end frequency (15th and end of month)
BM      business month end frequency
CBM     custom business month end frequency
MS      month start frequency
SMS     semi-month start frequency (1st and 15th)
BMS     business month start frequency
CBMS    custom business month start frequency
Q       quarter end frequency
BQ      business quarter endfrequency
QS      quarter start frequency
BQS     business quarter start frequency
A       year end frequency
BA      business year end frequency
AS      year start frequency
BAS     business year start frequency
BH      business hour frequency
H       hourly frequency
T       minutely frequency
S       secondly frequency
L       milliseonds
U       microseconds
N       nanoseconds
</code></pre>

<p>See the <a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html"">timeseries documentation</a>. It includes a list of <a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases"">offsets</a> (and <a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html#anchored-offsets"">'anchored' offsets</a>), and a section about <a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html#up-and-downsampling"">resampling</a>. </p>

<p>Note that there isn't a list of all the different <code>how</code> options, because it can be any NumPy array function and any function that is available via <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#groupby-dispatch"">groupby dispatching</a> can be passed to <code>how</code> by name.</p>
";;['B       business day frequency\nC       custom business day frequency (experimental)\nD       calendar day frequency\nW       weekly frequency\nM       month end frequency\nSM      semi-month end frequency (15th and end of month)\nBM      business month end frequency\nCBM     custom business month end frequency\nMS      month start frequency\nSMS     semi-month start frequency (1st and 15th)\nBMS     business month start frequency\nCBMS    custom business month start frequency\nQ       quarter end frequency\nBQ      business quarter endfrequency\nQS      quarter start frequency\nBQS     business quarter start frequency\nA       year end frequency\nBA      business year end frequency\nAS      year start frequency\nBAS     business year start frequency\nBH      business hour frequency\nH       hourly frequency\nT       minutely frequency\nS       secondly frequency\nL       milliseonds\nU       microseconds\nN       nanoseconds\n'];['B       business day frequency\nC       custom business day frequency (experimental)\nD       calendar day frequency\nW       weekly frequency\nM       month end frequency\nSM      semi-month end frequency (15th and end of month)\nBM      business month end frequency\nCBM     custom business month end frequency\nMS      month start frequency\nSMS     semi-month start frequency (1st and 15th)\nBMS     business month start frequency\nCBMS    custom business month start frequency\nQ       quarter end frequency\nBQ      business quarter endfrequency\nQS      quarter start frequency\nBQS     business quarter start frequency\nA       year end frequency\nBA      business year end frequency\nAS      year start frequency\nBAS     business year start frequency\nBH      business hour frequency\nH       hourly frequency\nT       minutely frequency\nS       secondly frequency\nL       milliseonds\nU       microseconds\nN       nanoseconds\n', 'how', 'how']
797;;1;17027507;16628819.0;2;10;;;"<p>I think you can't achieve what you want in a more efficient manner than you proposed.</p>

<p>The underlying problem is that the timestamps (as you seem aware) are made up of two parts.  The data that represents the UTC time, and the timezone, tz_info.  The timezone information is used only for display purposes when printing the timezone to the screen.  At display time, the data is offset appropriately and +01:00 (or similar) is added to the string.  Stripping off the tz_info value (using tz_convert(tz=None)) doesn't doesn't actually change the data that represents the naive part of the timestamp.  </p>

<p>So, the only way to do what you want is to modify the underlying data (pandas doesn't allow this... DatetimeIndex are immutable -- see the help on DatetimeIndex), or to create a new set of timestamp objects and wrap them in a new DatetimeIndex.  Your solution does the latter:</p>

<pre><code>pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])
</code></pre>

<p>For reference, here is the <code>replace</code> method of <code>Timestamp</code> (see tslib.pyx):</p>

<pre><code>def replace(self, **kwds):
    return Timestamp(datetime.replace(self, **kwds),
                     offset=self.offset)
</code></pre>

<p>You can refer to the docs on <code>datetime.datetime</code> to see that <code>datetime.datetime.replace</code> also creates a new object.   </p>

<p>If you can, your best bet for efficiency is to modify the source of the data so that it (incorrectly) reports the timestamps without their timezone.  You mentioned:</p>

<blockquote>
  <p>I want to work with timezone naive timeseries (to avoid the extra hassle with timezones, and I do not need them for the case I am working on)</p>
</blockquote>

<p>I'd be curious what extra hassle you are referring to.  I recommend as a general rule for all software development, keep your timestamp 'naive values' in UTC.  There is little worse than looking at two different int64 values wondering which timezone they belong to.  If you always, always, always use UTC for the internal storage, then you will avoid countless headaches.  My mantra is <strong>Timezones are for human I/O only</strong>.</p>
";;['pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])\n', 'def replace(self, **kwds):\n    return Timestamp(datetime.replace(self, **kwds),\n                     offset=self.offset)\n'];['pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])\n', 'replace', 'Timestamp', 'def replace(self, **kwds):\n    return Timestamp(datetime.replace(self, **kwds),\n                     offset=self.offset)\n', 'datetime.datetime', 'datetime.datetime.replace']
798;;3;17056022;16740887.0;2;9;;;"<p>You are actually trying to solve two problems: capturing real-time data and analyzing that data.  The first problem can be solved with <a href=""http://docs.python.org/2/library/logging.html"" rel=""noreferrer"">Python logging</a>, which is designed for this purpose.  Then the other problem can be solved by reading that same log file.</p>
";;[];[]
799;;4;17063653;17063458.0;2;104;;;"<p>Close: first you call <code>ExcelFile</code>, but then you call the <code>.parse</code> method and pass it the sheet name.</p>

<pre><code>&gt;&gt;&gt; xl = pd.ExcelFile(""dummydata.xlsx"")
&gt;&gt;&gt; xl.sheet_names
[u'Sheet1', u'Sheet2', u'Sheet3']
&gt;&gt;&gt; df = xl.parse(""Sheet1"")
&gt;&gt;&gt; df.head()
                  Tid  dummy1    dummy2    dummy3    dummy4    dummy5  \
0 2006-09-01 00:00:00       0  5.894611  0.605211  3.842871  8.265307   
1 2006-09-01 01:00:00       0  5.712107  0.605211  3.416617  8.301360   
2 2006-09-01 02:00:00       0  5.105300  0.605211  3.090865  8.335395   
3 2006-09-01 03:00:00       0  4.098209  0.605211  3.198452  8.170187   
4 2006-09-01 04:00:00       0  3.338196  0.605211  2.970015  7.765058   

     dummy6  dummy7    dummy8    dummy9  
0  0.623354       0  2.579108  2.681728  
1  0.554211       0  7.210000  3.028614  
2  0.567841       0  6.940000  3.644147  
3  0.581470       0  6.630000  4.016155  
4  0.595100       0  6.350000  3.974442  
</code></pre>

<p>What you're doing is calling the method which lives on the class itself, rather than the instance, which is okay (although not very idiomatic), but if you're doing that you would also need to pass the sheet name:</p>

<pre><code>&gt;&gt;&gt; parsed = pd.io.parsers.ExcelFile.parse(xl, ""Sheet1"")
&gt;&gt;&gt; parsed.columns
Index([u'Tid', u'dummy1', u'dummy2', u'dummy3', u'dummy4', u'dummy5', u'dummy6', u'dummy7', u'dummy8', u'dummy9'], dtype=object)
</code></pre>
";;"['>>> xl = pd.ExcelFile(""dummydata.xlsx"")\n>>> xl.sheet_names\n[u\'Sheet1\', u\'Sheet2\', u\'Sheet3\']\n>>> df = xl.parse(""Sheet1"")\n>>> df.head()\n                  Tid  dummy1    dummy2    dummy3    dummy4    dummy5  \\\n0 2006-09-01 00:00:00       0  5.894611  0.605211  3.842871  8.265307   \n1 2006-09-01 01:00:00       0  5.712107  0.605211  3.416617  8.301360   \n2 2006-09-01 02:00:00       0  5.105300  0.605211  3.090865  8.335395   \n3 2006-09-01 03:00:00       0  4.098209  0.605211  3.198452  8.170187   \n4 2006-09-01 04:00:00       0  3.338196  0.605211  2.970015  7.765058   \n\n     dummy6  dummy7    dummy8    dummy9  \n0  0.623354       0  2.579108  2.681728  \n1  0.554211       0  7.210000  3.028614  \n2  0.567841       0  6.940000  3.644147  \n3  0.581470       0  6.630000  4.016155  \n4  0.595100       0  6.350000  3.974442  \n', '>>> parsed = pd.io.parsers.ExcelFile.parse(xl, ""Sheet1"")\n>>> parsed.columns\nIndex([u\'Tid\', u\'dummy1\', u\'dummy2\', u\'dummy3\', u\'dummy4\', u\'dummy5\', u\'dummy6\', u\'dummy7\', u\'dummy8\', u\'dummy9\'], dtype=object)\n']";"['ExcelFile', '.parse', '>>> xl = pd.ExcelFile(""dummydata.xlsx"")\n>>> xl.sheet_names\n[u\'Sheet1\', u\'Sheet2\', u\'Sheet3\']\n>>> df = xl.parse(""Sheet1"")\n>>> df.head()\n                  Tid  dummy1    dummy2    dummy3    dummy4    dummy5  \\\n0 2006-09-01 00:00:00       0  5.894611  0.605211  3.842871  8.265307   \n1 2006-09-01 01:00:00       0  5.712107  0.605211  3.416617  8.301360   \n2 2006-09-01 02:00:00       0  5.105300  0.605211  3.090865  8.335395   \n3 2006-09-01 03:00:00       0  4.098209  0.605211  3.198452  8.170187   \n4 2006-09-01 04:00:00       0  3.338196  0.605211  2.970015  7.765058   \n\n     dummy6  dummy7    dummy8    dummy9  \n0  0.623354       0  2.579108  2.681728  \n1  0.554211       0  7.210000  3.028614  \n2  0.567841       0  6.940000  3.644147  \n3  0.581470       0  6.630000  4.016155  \n4  0.595100       0  6.350000  3.974442  \n', '>>> parsed = pd.io.parsers.ExcelFile.parse(xl, ""Sheet1"")\n>>> parsed.columns\nIndex([u\'Tid\', u\'dummy1\', u\'dummy2\', u\'dummy3\', u\'dummy4\', u\'dummy5\', u\'dummy6\', u\'dummy7\', u\'dummy8\', u\'dummy9\'], dtype=object)\n']"
800;;14;17071908;17071871.0;2;755;;;"<p>To select rows whose column value equals a scalar, <code>some_value</code>, use <code>==</code>:</p>

<pre><code>df.loc[df['column_name'] == some_value]
</code></pre>

<p>To select rows whose column value is in an iterable, <code>some_values</code>, use <code>isin</code>:</p>

<pre><code>df.loc[df['column_name'].isin(some_values)]
</code></pre>

<p>Combine multiple conditions with <code>&amp;</code>: </p>

<pre><code>df.loc[(df['column_name'] == some_value) &amp; df['other_column'].isin(some_values)]
</code></pre>

<hr>

<p>To select rows whose column value <em>does not equal</em> <code>some_value</code>, use <code>!=</code>:</p>

<pre><code>df.loc[df['column_name'] != some_value]
</code></pre>

<p><code>isin</code> returns a boolean Series, so to select rows whose value is <em>not</em> in <code>some_values</code>, negate the boolean Series using <code>~</code>:</p>

<pre><code>df.loc[~df['column_name'].isin(some_values)]
</code></pre>

<hr>

<p>For example,</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),
                   'B': 'one one two three two two one three'.split(),
                   'C': np.arange(8), 'D': np.arange(8) * 2})
print(df)
#      A      B  C   D
# 0  foo    one  0   0
# 1  bar    one  1   2
# 2  foo    two  2   4
# 3  bar  three  3   6
# 4  foo    two  4   8
# 5  bar    two  5  10
# 6  foo    one  6  12
# 7  foo  three  7  14

print(df.loc[df['A'] == 'foo'])
</code></pre>

<p>yields</p>

<pre><code>     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14
</code></pre>

<hr>

<p>If you have multiple values you want to include, put them in a
list (or more generally, any iterable) and use <code>isin</code>:</p>

<pre><code>print(df.loc[df['B'].isin(['one','three'])])
</code></pre>

<p>yields</p>

<pre><code>     A      B  C   D
0  foo    one  0   0
1  bar    one  1   2
3  bar  three  3   6
6  foo    one  6  12
7  foo  three  7  14
</code></pre>

<hr>

<p>Note, however, that if you wish to do this many times, it is more efficient to
make an index first, and then use <code>df.loc</code>:</p>

<pre><code>df = df.set_index(['B'])
print(df.loc['one'])
</code></pre>

<p>yields</p>

<pre><code>       A  C   D
B              
one  foo  0   0
one  bar  1   2
one  foo  6  12
</code></pre>

<p>or, to include multiple values from the index use <code>df.index.isin</code>:</p>

<pre><code>df.loc[df.index.isin(['one','two'])]
</code></pre>

<p>yields</p>

<pre><code>       A  C   D
B              
one  foo  0   0
one  bar  1   2
two  foo  2   4
two  foo  4   8
two  bar  5  10
one  foo  6  12
</code></pre>
";;"[""df.loc[df['column_name'] == some_value]\n"", ""df.loc[df['column_name'].isin(some_values)]\n"", ""df.loc[(df['column_name'] == some_value) & df['other_column'].isin(some_values)]\n"", ""df.loc[df['column_name'] != some_value]\n"", ""df.loc[~df['column_name'].isin(some_values)]\n"", ""import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n                   'B': 'one one two three two two one three'.split(),\n                   'C': np.arange(8), 'D': np.arange(8) * 2})\nprint(df)\n#      A      B  C   D\n# 0  foo    one  0   0\n# 1  bar    one  1   2\n# 2  foo    two  2   4\n# 3  bar  three  3   6\n# 4  foo    two  4   8\n# 5  bar    two  5  10\n# 6  foo    one  6  12\n# 7  foo  three  7  14\n\nprint(df.loc[df['A'] == 'foo'])\n"", '     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n', ""print(df.loc[df['B'].isin(['one','three'])])\n"", '     A      B  C   D\n0  foo    one  0   0\n1  bar    one  1   2\n3  bar  three  3   6\n6  foo    one  6  12\n7  foo  three  7  14\n', ""df = df.set_index(['B'])\nprint(df.loc['one'])\n"", '       A  C   D\nB              \none  foo  0   0\none  bar  1   2\none  foo  6  12\n', ""df.loc[df.index.isin(['one','two'])]\n"", '       A  C   D\nB              \none  foo  0   0\none  bar  1   2\ntwo  foo  2   4\ntwo  foo  4   8\ntwo  bar  5  10\none  foo  6  12\n']";"['some_value', '==', ""df.loc[df['column_name'] == some_value]\n"", 'some_values', 'isin', ""df.loc[df['column_name'].isin(some_values)]\n"", '&', ""df.loc[(df['column_name'] == some_value) & df['other_column'].isin(some_values)]\n"", 'some_value', '!=', ""df.loc[df['column_name'] != some_value]\n"", 'isin', 'some_values', '~', ""df.loc[~df['column_name'].isin(some_values)]\n"", ""import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n                   'B': 'one one two three two two one three'.split(),\n                   'C': np.arange(8), 'D': np.arange(8) * 2})\nprint(df)\n#      A      B  C   D\n# 0  foo    one  0   0\n# 1  bar    one  1   2\n# 2  foo    two  2   4\n# 3  bar  three  3   6\n# 4  foo    two  4   8\n# 5  bar    two  5  10\n# 6  foo    one  6  12\n# 7  foo  three  7  14\n\nprint(df.loc[df['A'] == 'foo'])\n"", '     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n', 'isin', ""print(df.loc[df['B'].isin(['one','three'])])\n"", '     A      B  C   D\n0  foo    one  0   0\n1  bar    one  1   2\n3  bar  three  3   6\n6  foo    one  6  12\n7  foo  three  7  14\n', 'df.loc', ""df = df.set_index(['B'])\nprint(df.loc['one'])\n"", '       A  C   D\nB              \none  foo  0   0\none  bar  1   2\none  foo  6  12\n', 'df.index.isin', ""df.loc[df.index.isin(['one','two'])]\n"", '       A  C   D\nB              \none  foo  0   0\none  bar  1   2\ntwo  foo  2   4\ntwo  foo  4   8\ntwo  bar  5  10\none  foo  6  12\n']"
801;;0;17086321;17071871.0;2;9;;;"<p>Here is a simple example  </p>

<pre><code>from pandas import DataFrame

# Create data set
d = {'Revenue':[100,111,222], 
     'Cost':[333,444,555]}
df = DataFrame(d)


# mask = Return True when the value in column ""Revenue"" is equal to 111
mask = df['Revenue'] == 111

print mask

# Result:
# 0    False
# 1     True
# 2    False
# Name: Revenue, dtype: bool


# Select * FROM df WHERE Revenue = 111
df[mask]

# Result:
#    Cost    Revenue
# 1  444     111
</code></pre>
";;"['from pandas import DataFrame\n\n# Create data set\nd = {\'Revenue\':[100,111,222], \n     \'Cost\':[333,444,555]}\ndf = DataFrame(d)\n\n\n# mask = Return True when the value in column ""Revenue"" is equal to 111\nmask = df[\'Revenue\'] == 111\n\nprint mask\n\n# Result:\n# 0    False\n# 1     True\n# 2    False\n# Name: Revenue, dtype: bool\n\n\n# Select * FROM df WHERE Revenue = 111\ndf[mask]\n\n# Result:\n#    Cost    Revenue\n# 1  444     111\n']";"['from pandas import DataFrame\n\n# Create data set\nd = {\'Revenue\':[100,111,222], \n     \'Cost\':[333,444,555]}\ndf = DataFrame(d)\n\n\n# mask = Return True when the value in column ""Revenue"" is equal to 111\nmask = df[\'Revenue\'] == 111\n\nprint mask\n\n# Result:\n# 0    False\n# 1     True\n# 2    False\n# Name: Revenue, dtype: bool\n\n\n# Select * FROM df WHERE Revenue = 111\ndf[mask]\n\n# Result:\n#    Cost    Revenue\n# 1  444     111\n']"
802;;11;17092113;17091769.0;2;47;;;"<p><code>df['y']</code> will set a column</p>

<p>since you want to set a row, use <code>.loc</code></p>

<p>Note that <code>.ix</code> is equivalent here, yours failed because you tried to assign a dictionary
to each element of the row <code>y</code> probably not what you want; converting to a Series tells pandas
that you want to align the input (for example you then don't have to to specify all of the elements)</p>

<pre><code>In [7]: df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'])

In [8]: df.loc['y'] = pandas.Series({'a':1, 'b':5, 'c':2, 'd':3})

In [9]: df
Out[9]: 
     a    b    c    d
x  NaN  NaN  NaN  NaN
y    1    5    2    3
z  NaN  NaN  NaN  NaN
</code></pre>
";;"[""In [7]: df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'])\n\nIn [8]: df.loc['y'] = pandas.Series({'a':1, 'b':5, 'c':2, 'd':3})\n\nIn [9]: df\nOut[9]: \n     a    b    c    d\nx  NaN  NaN  NaN  NaN\ny    1    5    2    3\nz  NaN  NaN  NaN  NaN\n""]";"[""df['y']"", '.loc', '.ix', 'y', ""In [7]: df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'])\n\nIn [8]: df.loc['y'] = pandas.Series({'a':1, 'b':5, 'c':2, 'd':3})\n\nIn [9]: df\nOut[9]: \n     a    b    c    d\nx  NaN  NaN  NaN  NaN\ny    1    5    2    3\nz  NaN  NaN  NaN  NaN\n""]"
803;;7;17095620;17095101.0;2;56;;;"<p>The first part is similar to Constantine, you can get the boolean of which rows are empty*:</p>

<pre><code>In [21]: ne = (df1 != df2).any(1)

In [22]: ne
Out[22]:
0    False
1     True
2     True
dtype: bool
</code></pre>

<p>Then we can see which entries have changed:</p>

<pre><code>In [23]: ne_stacked = (df1 != df2).stack()

In [24]: changed = ne_stacked[ne_stacked]

In [25]: changed.index.names = ['id', 'col']

In [26]: changed
Out[26]:
id  col
1   score         True
2   isEnrolled    True
    Comment       True
dtype: bool
</code></pre>

<p><em>Here the first entry is the index and the second the columns which has been changed.</em></p>

<pre><code>In [27]: difference_locations = np.where(df1 != df2)

In [28]: changed_from = df1.values[difference_locations]

In [29]: changed_to = df2.values[difference_locations]

In [30]: pd.DataFrame({'from': changed_from, 'to': changed_to}, index=changed.index)
Out[30]:
               from           to
id col
1  score       1.11         1.21
2  isEnrolled  True        False
   Comment     None  On vacation
</code></pre>

<p>* Note: it's important that <code>df1</code> and <code>df2</code> share the same index here. To overcome this ambiguity, you can ensure you only look at the shared labels using <code>df1.index &amp; df2.index</code>, but I think I'll leave that as an exercise.</p>
";;"['In [21]: ne = (df1 != df2).any(1)\n\nIn [22]: ne\nOut[22]:\n0    False\n1     True\n2     True\ndtype: bool\n', ""In [23]: ne_stacked = (df1 != df2).stack()\n\nIn [24]: changed = ne_stacked[ne_stacked]\n\nIn [25]: changed.index.names = ['id', 'col']\n\nIn [26]: changed\nOut[26]:\nid  col\n1   score         True\n2   isEnrolled    True\n    Comment       True\ndtype: bool\n"", ""In [27]: difference_locations = np.where(df1 != df2)\n\nIn [28]: changed_from = df1.values[difference_locations]\n\nIn [29]: changed_to = df2.values[difference_locations]\n\nIn [30]: pd.DataFrame({'from': changed_from, 'to': changed_to}, index=changed.index)\nOut[30]:\n               from           to\nid col\n1  score       1.11         1.21\n2  isEnrolled  True        False\n   Comment     None  On vacation\n""]";"['In [21]: ne = (df1 != df2).any(1)\n\nIn [22]: ne\nOut[22]:\n0    False\n1     True\n2     True\ndtype: bool\n', ""In [23]: ne_stacked = (df1 != df2).stack()\n\nIn [24]: changed = ne_stacked[ne_stacked]\n\nIn [25]: changed.index.names = ['id', 'col']\n\nIn [26]: changed\nOut[26]:\nid  col\n1   score         True\n2   isEnrolled    True\n    Comment       True\ndtype: bool\n"", ""In [27]: difference_locations = np.where(df1 != df2)\n\nIn [28]: changed_from = df1.values[difference_locations]\n\nIn [29]: changed_to = df2.values[difference_locations]\n\nIn [30]: pd.DataFrame({'from': changed_from, 'to': changed_to}, index=changed.index)\nOut[30]:\n               from           to\nid col\n1  score       1.11         1.21\n2  isEnrolled  True        False\n   Comment     None  On vacation\n"", 'df1', 'df2', 'df1.index & df2.index']"
804;;3;17096675;17095101.0;2;9;;;"<pre><code>import pandas as pd
import io

texts = ['''\
id   Name   score                    isEnrolled                        Comment
111  Jack   2.17                     True                 He was late to class
112  Nick   1.11                     False                           Graduated
113  Zoe    4.12                     True       ''',

         '''\
id   Name   score                    isEnrolled                        Comment
111  Jack   2.17                     True                 He was late to class
112  Nick   1.21                     False                           Graduated
113  Zoe    4.12                     False                         On vacation''']


df1 = pd.read_fwf(io.BytesIO(texts[0]), widths=[5,7,25,21,20])
df2 = pd.read_fwf(io.BytesIO(texts[1]), widths=[5,7,25,21,20])
df = pd.concat([df1,df2]) 

print(df)
#     id  Name  score isEnrolled               Comment
# 0  111  Jack   2.17       True  He was late to class
# 1  112  Nick   1.11      False             Graduated
# 2  113   Zoe   4.12       True                   NaN
# 0  111  Jack   2.17       True  He was late to class
# 1  112  Nick   1.21      False             Graduated
# 2  113   Zoe   4.12      False           On vacation

df.set_index(['id', 'Name'], inplace=True)
print(df)
#           score isEnrolled               Comment
# id  Name                                        
# 111 Jack   2.17       True  He was late to class
# 112 Nick   1.11      False             Graduated
# 113 Zoe    4.12       True                   NaN
# 111 Jack   2.17       True  He was late to class
# 112 Nick   1.21      False             Graduated
# 113 Zoe    4.12      False           On vacation

def report_diff(x):
    return x[0] if x[0] == x[1] else '{} | {}'.format(*x)

changes = df.groupby(level=['id', 'Name']).agg(report_diff)
print(changes)
</code></pre>

<p>prints</p>

<pre><code>                score    isEnrolled               Comment
id  Name                                                 
111 Jack         2.17          True  He was late to class
112 Nick  1.11 | 1.21         False             Graduated
113 Zoe          4.12  True | False     nan | On vacation
</code></pre>
";;"[""import pandas as pd\nimport io\n\ntexts = ['''\\\nid   Name   score                    isEnrolled                        Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.11                     False                           Graduated\n113  Zoe    4.12                     True       ''',\n\n         '''\\\nid   Name   score                    isEnrolled                        Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.21                     False                           Graduated\n113  Zoe    4.12                     False                         On vacation''']\n\n\ndf1 = pd.read_fwf(io.BytesIO(texts[0]), widths=[5,7,25,21,20])\ndf2 = pd.read_fwf(io.BytesIO(texts[1]), widths=[5,7,25,21,20])\ndf = pd.concat([df1,df2]) \n\nprint(df)\n#     id  Name  score isEnrolled               Comment\n# 0  111  Jack   2.17       True  He was late to class\n# 1  112  Nick   1.11      False             Graduated\n# 2  113   Zoe   4.12       True                   NaN\n# 0  111  Jack   2.17       True  He was late to class\n# 1  112  Nick   1.21      False             Graduated\n# 2  113   Zoe   4.12      False           On vacation\n\ndf.set_index(['id', 'Name'], inplace=True)\nprint(df)\n#           score isEnrolled               Comment\n# id  Name                                        \n# 111 Jack   2.17       True  He was late to class\n# 112 Nick   1.11      False             Graduated\n# 113 Zoe    4.12       True                   NaN\n# 111 Jack   2.17       True  He was late to class\n# 112 Nick   1.21      False             Graduated\n# 113 Zoe    4.12      False           On vacation\n\ndef report_diff(x):\n    return x[0] if x[0] == x[1] else '{} | {}'.format(*x)\n\nchanges = df.groupby(level=['id', 'Name']).agg(report_diff)\nprint(changes)\n"", '                score    isEnrolled               Comment\nid  Name                                                 \n111 Jack         2.17          True  He was late to class\n112 Nick  1.11 | 1.21         False             Graduated\n113 Zoe          4.12  True | False     nan | On vacation\n']";"[""import pandas as pd\nimport io\n\ntexts = ['''\\\nid   Name   score                    isEnrolled                        Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.11                     False                           Graduated\n113  Zoe    4.12                     True       ''',\n\n         '''\\\nid   Name   score                    isEnrolled                        Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.21                     False                           Graduated\n113  Zoe    4.12                     False                         On vacation''']\n\n\ndf1 = pd.read_fwf(io.BytesIO(texts[0]), widths=[5,7,25,21,20])\ndf2 = pd.read_fwf(io.BytesIO(texts[1]), widths=[5,7,25,21,20])\ndf = pd.concat([df1,df2]) \n\nprint(df)\n#     id  Name  score isEnrolled               Comment\n# 0  111  Jack   2.17       True  He was late to class\n# 1  112  Nick   1.11      False             Graduated\n# 2  113   Zoe   4.12       True                   NaN\n# 0  111  Jack   2.17       True  He was late to class\n# 1  112  Nick   1.21      False             Graduated\n# 2  113   Zoe   4.12      False           On vacation\n\ndf.set_index(['id', 'Name'], inplace=True)\nprint(df)\n#           score isEnrolled               Comment\n# id  Name                                        \n# 111 Jack   2.17       True  He was late to class\n# 112 Nick   1.11      False             Graduated\n# 113 Zoe    4.12       True                   NaN\n# 111 Jack   2.17       True  He was late to class\n# 112 Nick   1.21      False             Graduated\n# 113 Zoe    4.12      False           On vacation\n\ndef report_diff(x):\n    return x[0] if x[0] == x[1] else '{} | {}'.format(*x)\n\nchanges = df.groupby(level=['id', 'Name']).agg(report_diff)\nprint(changes)\n"", '                score    isEnrolled               Comment\nid  Name                                                 \n111 Jack         2.17          True  He was late to class\n112 Nick  1.11 | 1.21         False             Graduated\n113 Zoe          4.12  True | False     nan | On vacation\n']"
805;;8;17097397;17097236.0;2;36;;;"<p>Actually in later versions of pandas this will give a TypeError:</p>

<pre><code>df.replace('-', None)
TypeError: If ""to_replace"" and ""value"" are both None then regex must be a mapping
</code></pre>

<p>You can do it by passing either a list or a dictionary:</p>

<pre><code>In [11]: df.replace('-', df.replace(['-'], [None]) # or .replace('-', {0: None})
Out[11]:
      0
0  None
1     3
2     2
3     5
4     1
5    -5
6    -1
7  None
8     9
</code></pre>

<p>But I recommend using NaNs rather than None:</p>

<pre><code>In [12]: df.replace('-', np.nan)
Out[12]:
     0
0  NaN
1    3
2    2
3    5
4    1
5   -5
6   -1
7  NaN
8    9
</code></pre>
";;"['df.replace(\'-\', None)\nTypeError: If ""to_replace"" and ""value"" are both None then regex must be a mapping\n', ""In [11]: df.replace('-', df.replace(['-'], [None]) # or .replace('-', {0: None})\nOut[11]:\n      0\n0  None\n1     3\n2     2\n3     5\n4     1\n5    -5\n6    -1\n7  None\n8     9\n"", ""In [12]: df.replace('-', np.nan)\nOut[12]:\n     0\n0  NaN\n1    3\n2    2\n3    5\n4    1\n5   -5\n6   -1\n7  NaN\n8    9\n""]";"['df.replace(\'-\', None)\nTypeError: If ""to_replace"" and ""value"" are both None then regex must be a mapping\n', ""In [11]: df.replace('-', df.replace(['-'], [None]) # or .replace('-', {0: None})\nOut[11]:\n      0\n0  None\n1     3\n2     2\n3     5\n4     1\n5    -5\n6    -1\n7  None\n8     9\n"", ""In [12]: df.replace('-', np.nan)\nOut[12]:\n     0\n0  NaN\n1    3\n2    2\n3    5\n4    1\n5   -5\n6   -1\n7  NaN\n8    9\n""]"
806;;2;17097777;17097643.0;2;31;;;"<p>You can use the invert (~) operator (which acts like a not for boolean data):</p>

<pre><code>~df[""col""].str.contains(word)
</code></pre>

<p><em>contains also accepts a regular expression...</em></p>
";;"['~df[""col""].str.contains(word)\n']";"['~df[""col""].str.contains(word)\n']"
807;;2;17098736;17098654.0;2;170;;;"<p>The easiest way is to <a href=""http://docs.python.org/2/library/pickle.html"" rel=""noreferrer"">pickle</a> it using <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#pickling"" rel=""noreferrer""><code>to_pickle</code></a>:</p>

<pre><code>df.to_pickle(file_name)  # where to save it, usually as a .pkl
</code></pre>

<p>Then you can load it back using:</p>

<pre><code>df = pd.read_pickle(file_name)
</code></pre>

<p><em>Note: before 0.11.1 <code>save</code> and <code>load</code> were the only way to do this (they are now deprecated in favor of <code>to_pickle</code> and <code>read_pickle</code> respectively).</em></p>

<hr>

<p>Another popular choice is to use <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#hdf5-pytables"" rel=""noreferrer"">HDF5</a> (<a href=""http://www.pytables.org"" rel=""noreferrer"">pytables</a>) which offers <a href=""https://stackoverflow.com/questions/16628329/hdf5-and-sqlite-concurrency-compression-i-o-performance"">very fast</a> access times for large datasets:</p>

<pre><code>store = HDFStore('store.h5')

store['df'] = df  # save it
store['df']  # load it
</code></pre>

<p><em>More advanced strategies are discussed in the <a href=""http://pandas-docs.github.io/pandas-docs-travis/#pandas-powerful-python-data-analysis-toolkit"" rel=""noreferrer"">cookbook</a>.</em></p>

<hr>

<p>Since 0.13 there's also <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#msgpack-experimental"" rel=""noreferrer"">msgpack</a> which may be be better for interoperability, as a faster alternative to JSON, or if you have python object/text-heavy data (see <a href=""https://stackoverflow.com/q/30651724/1240268"">this question</a>).</p>
";;"['df.to_pickle(file_name)  # where to save it, usually as a .pkl\n', 'df = pd.read_pickle(file_name)\n', ""store = HDFStore('store.h5')\n\nstore['df'] = df  # save it\nstore['df']  # load it\n""]";"['to_pickle', 'df.to_pickle(file_name)  # where to save it, usually as a .pkl\n', 'df = pd.read_pickle(file_name)\n', 'save', 'load', 'to_pickle', 'read_pickle', ""store = HDFStore('store.h5')\n\nstore['df'] = df  # save it\nstore['df']  # load it\n""]"
808;;0;17098885;17098654.0;2;22;;;"<p>If I understand correctly, you're already using <code>pandas.read_csv()</code> but would like to speed up the development process so that you don't have to load the file in every time you edit your script, is that right? I have a few recommendations:</p>

<ol>
<li><p>you could load in only part of the CSV file using <code>pandas.read_csv(..., nrows=1000)</code> to only load the top bit of the table, while you're doing the development</p></li>
<li><p>use <a href=""http://ipython.org/"" rel=""nofollow noreferrer"">ipython</a> for an interactive session, such that you keep the pandas table in memory as you edit and reload your script.</p></li>
<li><p>convert the csv to an <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#hdf5-pytables"" rel=""nofollow noreferrer"">HDF5 table</a></p></li>
<li><p><strong>updated</strong> use <code>DataFrame.to_feather()</code> and <code>pd.read_feather()</code> to store data in the R-compatible <a href=""https://github.com/wesm/feather"" rel=""nofollow noreferrer"">feather</a> binary format that is super fast (in my hands, slightly faster than <code>pandas.to_pickle()</code> on numeric data and much faster on string data).</p></li>
</ol>

<p>You might also be interested in <a href=""https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas"">this answer</a> on stackoverflow.</p>
";;[];['pandas.read_csv()', 'pandas.read_csv(..., nrows=1000)', 'DataFrame.to_feather()', 'pd.read_feather()', 'pandas.to_pickle()']
809;;6;17116976;17116814.0;2;145;;;"<p>This splits the Seatblocks by space and gives each its own row.</p>

<pre><code>In [43]: df
Out[43]: 
   CustNum     CustomerName  ItemQty Item                 Seatblocks  ItemExt
0    32363  McCartney, Paul        3  F04               2:218:10:4,6       60
1    31316     Lennon, John       25  F01  1:13:36:1,12 1:13:37:1,13      300

In [44]: s = df['Seatblocks'].str.split(' ').apply(Series, 1).stack()

In [45]: s.index = s.index.droplevel(-1) # to line up with df's index

In [46]: s.name = 'Seatblocks' # needs a name to join

In [47]: s
Out[47]: 
0    2:218:10:4,6
1    1:13:36:1,12
1    1:13:37:1,13
Name: Seatblocks, dtype: object

In [48]: del df['Seatblocks']

In [49]: df.join(s)
Out[49]: 
   CustNum     CustomerName  ItemQty Item  ItemExt    Seatblocks
0    32363  McCartney, Paul        3  F04       60  2:218:10:4,6
1    31316     Lennon, John       25  F01      300  1:13:36:1,12
1    31316     Lennon, John       25  F01      300  1:13:37:1,13
</code></pre>

<p>Or, to give each colon-separated string in its own column:</p>

<pre><code>In [50]: df.join(s.apply(lambda x: Series(x.split(':'))))
Out[50]: 
   CustNum     CustomerName  ItemQty Item  ItemExt  0    1   2     3
0    32363  McCartney, Paul        3  F04       60  2  218  10   4,6
1    31316     Lennon, John       25  F01      300  1   13  36  1,12
1    31316     Lennon, John       25  F01      300  1   13  37  1,13
</code></pre>

<p>This is a little ugly, but maybe someone will chime in with a prettier solution.</p>
";;"[""In [43]: df\nOut[43]: \n   CustNum     CustomerName  ItemQty Item                 Seatblocks  ItemExt\n0    32363  McCartney, Paul        3  F04               2:218:10:4,6       60\n1    31316     Lennon, John       25  F01  1:13:36:1,12 1:13:37:1,13      300\n\nIn [44]: s = df['Seatblocks'].str.split(' ').apply(Series, 1).stack()\n\nIn [45]: s.index = s.index.droplevel(-1) # to line up with df's index\n\nIn [46]: s.name = 'Seatblocks' # needs a name to join\n\nIn [47]: s\nOut[47]: \n0    2:218:10:4,6\n1    1:13:36:1,12\n1    1:13:37:1,13\nName: Seatblocks, dtype: object\n\nIn [48]: del df['Seatblocks']\n\nIn [49]: df.join(s)\nOut[49]: \n   CustNum     CustomerName  ItemQty Item  ItemExt    Seatblocks\n0    32363  McCartney, Paul        3  F04       60  2:218:10:4,6\n1    31316     Lennon, John       25  F01      300  1:13:36:1,12\n1    31316     Lennon, John       25  F01      300  1:13:37:1,13\n"", ""In [50]: df.join(s.apply(lambda x: Series(x.split(':'))))\nOut[50]: \n   CustNum     CustomerName  ItemQty Item  ItemExt  0    1   2     3\n0    32363  McCartney, Paul        3  F04       60  2  218  10   4,6\n1    31316     Lennon, John       25  F01      300  1   13  36  1,12\n1    31316     Lennon, John       25  F01      300  1   13  37  1,13\n""]";"[""In [43]: df\nOut[43]: \n   CustNum     CustomerName  ItemQty Item                 Seatblocks  ItemExt\n0    32363  McCartney, Paul        3  F04               2:218:10:4,6       60\n1    31316     Lennon, John       25  F01  1:13:36:1,12 1:13:37:1,13      300\n\nIn [44]: s = df['Seatblocks'].str.split(' ').apply(Series, 1).stack()\n\nIn [45]: s.index = s.index.droplevel(-1) # to line up with df's index\n\nIn [46]: s.name = 'Seatblocks' # needs a name to join\n\nIn [47]: s\nOut[47]: \n0    2:218:10:4,6\n1    1:13:36:1,12\n1    1:13:37:1,13\nName: Seatblocks, dtype: object\n\nIn [48]: del df['Seatblocks']\n\nIn [49]: df.join(s)\nOut[49]: \n   CustNum     CustomerName  ItemQty Item  ItemExt    Seatblocks\n0    32363  McCartney, Paul        3  F04       60  2:218:10:4,6\n1    31316     Lennon, John       25  F01      300  1:13:36:1,12\n1    31316     Lennon, John       25  F01      300  1:13:37:1,13\n"", ""In [50]: df.join(s.apply(lambda x: Series(x.split(':'))))\nOut[50]: \n   CustNum     CustomerName  ItemQty Item  ItemExt  0    1   2     3\n0    32363  McCartney, Paul        3  F04       60  2  218  10   4,6\n1    31316     Lennon, John       25  F01      300  1   13  36  1,12\n1    31316     Lennon, John       25  F01      300  1   13  37  1,13\n""]"
810;;12;17134750;17134716.0;2;159;;;"<p>The easiest way is to use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html""><code>to_datetime</code></a>:</p>

<pre><code>df['col'] = pd.to_datetime(df['col'])
</code></pre>

<p>It also offers a <code>dayfirst</code> argument for European times (but beware <a href=""https://github.com/pydata/pandas/issues/3341"">this isn't strict</a>).</p>

<p>Here it is in action:</p>

<pre><code>In [11]: pd.to_datetime(pd.Series(['05/23/2005']))
Out[11]:
0   2005-05-23 00:00:00
dtype: datetime64[ns]
</code></pre>

<p>You can pass a specific <a href=""https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior"">format</a>:</p>

<pre><code>In [12]: pd.to_datetime(pd.Series(['05/23/2005']), format=""%m/%d/%Y"")
Out[12]:
0   2005-05-23
dtype: datetime64[ns]
</code></pre>
";;"[""df['col'] = pd.to_datetime(df['col'])\n"", ""In [11]: pd.to_datetime(pd.Series(['05/23/2005']))\nOut[11]:\n0   2005-05-23 00:00:00\ndtype: datetime64[ns]\n"", 'In [12]: pd.to_datetime(pd.Series([\'05/23/2005\']), format=""%m/%d/%Y"")\nOut[12]:\n0   2005-05-23\ndtype: datetime64[ns]\n']";"['to_datetime', ""df['col'] = pd.to_datetime(df['col'])\n"", 'dayfirst', ""In [11]: pd.to_datetime(pd.Series(['05/23/2005']))\nOut[11]:\n0   2005-05-23 00:00:00\ndtype: datetime64[ns]\n"", 'In [12]: pd.to_datetime(pd.Series([\'05/23/2005\']), format=""%m/%d/%Y"")\nOut[12]:\n0   2005-05-23\ndtype: datetime64[ns]\n']"
811;;6;17141755;17141558.0;2;126;;;"<p>As of the 0.17.0 release, the <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.sort.html"" rel=""noreferrer""><code>sort</code></a> method was deprecated in favor of <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html"" rel=""noreferrer""><code>sort_values</code></a>.  <code>sort</code> was completely removed in the 0.20.0 release. The arguments (and results) remain the same:</p>

<pre><code>df.sort_values(['a', 'b'], ascending=[True, False])
</code></pre>

<hr>

<p>You can use the ascending argument of <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort.html"" rel=""noreferrer""><code>sort</code></a>:</p>

<pre><code>df.sort(['a', 'b'], ascending=[True, False])
</code></pre>

<p>For example:</p>

<pre><code>In [11]: df1 = pd.DataFrame(np.random.randint(1, 5, (10,2)), columns=['a','b'])

In [12]: df1.sort(['a', 'b'], ascending=[True, False])
Out[12]:
   a  b
2  1  4
7  1  3
1  1  2
3  1  2
4  3  2
6  4  4
0  4  3
9  4  3
5  4  1
8  4  1
</code></pre>

<hr>

<p>As commented by @renadeen</p>

<blockquote>
  <p>Sort isn't in place by default! So you should assign result of the sort method to a variable or add inplace=True to method call.</p>
</blockquote>

<p>that is, if you want to reuse df1 as a sorted DataFrame:</p>

<pre><code>df1 = df1.sort(['a', 'b'], ascending=[True, False])
</code></pre>

<p>or</p>

<pre><code>df1.sort(['a', 'b'], ascending=[True, False], inplace=True)
</code></pre>
";;"[""df.sort_values(['a', 'b'], ascending=[True, False])\n"", ""df.sort(['a', 'b'], ascending=[True, False])\n"", ""In [11]: df1 = pd.DataFrame(np.random.randint(1, 5, (10,2)), columns=['a','b'])\n\nIn [12]: df1.sort(['a', 'b'], ascending=[True, False])\nOut[12]:\n   a  b\n2  1  4\n7  1  3\n1  1  2\n3  1  2\n4  3  2\n6  4  4\n0  4  3\n9  4  3\n5  4  1\n8  4  1\n"", ""df1 = df1.sort(['a', 'b'], ascending=[True, False])\n"", ""df1.sort(['a', 'b'], ascending=[True, False], inplace=True)\n""]";"['sort', 'sort_values', 'sort', ""df.sort_values(['a', 'b'], ascending=[True, False])\n"", 'sort', ""df.sort(['a', 'b'], ascending=[True, False])\n"", ""In [11]: df1 = pd.DataFrame(np.random.randint(1, 5, (10,2)), columns=['a','b'])\n\nIn [12]: df1.sort(['a', 'b'], ascending=[True, False])\nOut[12]:\n   a  b\n2  1  4\n7  1  3\n1  1  2\n3  1  2\n4  3  2\n6  4  4\n0  4  3\n9  4  3\n5  4  1\n8  4  1\n"", ""df1 = df1.sort(['a', 'b'], ascending=[True, False])\n"", ""df1.sort(['a', 'b'], ascending=[True, False], inplace=True)\n""]"
812;;2;17142595;17142304.0;2;49;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.replace.html"" rel=""noreferrer"">replace</a></p>

<pre><code>In [126]: df.replace(['very bad', 'bad', 'poor', 'good', 'very good'], 
                     [1, 2, 3, 4, 5]) 
Out[126]: 
      resp  A  B  C
   0     1  3  3  4
   1     2  4  3  4
   2     3  5  5  5
   3     4  2  3  2
   4     5  1  1  1
   5     6  3  4  1
   6     7  4  4  4
   7     8  5  5  5
   8     9  2  2  1
   9    10  1  1  1
</code></pre>
";;"[""In [126]: df.replace(['very bad', 'bad', 'poor', 'good', 'very good'], \n                     [1, 2, 3, 4, 5]) \nOut[126]: \n      resp  A  B  C\n   0     1  3  3  4\n   1     2  4  3  4\n   2     3  5  5  5\n   3     4  2  3  2\n   4     5  1  1  1\n   5     6  3  4  1\n   6     7  4  4  4\n   7     8  5  5  5\n   8     9  2  2  1\n   9    10  1  1  1\n""]";"[""In [126]: df.replace(['very bad', 'bad', 'poor', 'good', 'very good'], \n                     [1, 2, 3, 4, 5]) \nOut[126]: \n      resp  A  B  C\n   0     1  3  3  4\n   1     2  4  3  4\n   2     3  5  5  5\n   3     4  2  3  2\n   4     5  1  1  1\n   5     6  3  4  1\n   6     7  4  4  4\n   7     8  5  5  5\n   8     9  2  2  1\n   9    10  1  1  1\n""]"
813;;5;17171819;12190874.0;2;75;;;"<p>I have found that <code>np.random.choice()</code> new in NumPy 1.7.0 works quite well for this.</p>

<p>For example you can pass the index values from a DataFrame and and the integer 10 to select 10 random uniformly sampled rows.</p>

<pre><code>rows = np.random.choice(df.index.values, 10)
sampled_df = df.ix[rows]
</code></pre>
";;['rows = np.random.choice(df.index.values, 10)\nsampled_df = df.ix[rows]\n'];['np.random.choice()', 'rows = np.random.choice(df.index.values, 10)\nsampled_df = df.ix[rows]\n']
814;;4;17241104;17241004.0;2;33;;;"<p>You can use <code>df.index</code> to access the index object and then get the values in a list using <code>df.index.tolist()</code>.</p>
";;[];['df.index', 'df.index.tolist()']
815;;2;17242374;17241004.0;2;132;;;"<p>You should use the <code>values</code> attribute:</p>

<pre><code>In [1]: df = pd.DataFrame(index=['a', 'b'])

In [2]: df.index.values
Out[2]: array(['a', 'b'], dtype=object)
</code></pre>

<p><em>This accesses how the data is already stored, so there's no need for a conversion.<br>
Note: This attribute is also available for many other pandas' objects.</em></p>
";;"[""In [1]: df = pd.DataFrame(index=['a', 'b'])\n\nIn [2]: df.index.values\nOut[2]: array(['a', 'b'], dtype=object)\n""]";"['values', ""In [1]: df = pd.DataFrame(index=['a', 'b'])\n\nIn [2]: df.index.values\nOut[2]: array(['a', 'b'], dtype=object)\n""]"
816;;4;17302673;14734533.0;2;43;;;"<p>Wes McKinney (pandas' author) in Python for Data Analysis provides the following recipe:</p>

<pre><code>groups = dict(list(gb))
</code></pre>

<p>which returns a dictionary whose keys are your group labels and whose values are DataFrames, i.e.</p>

<pre><code>groups['foo']
</code></pre>

<p>will yield what you are looking for:</p>

<pre><code>     A         B   C
0  foo  1.624345   5
2  foo -0.528172  11
4  foo  0.865408  14
</code></pre>
";;"['groups = dict(list(gb))\n', ""groups['foo']\n"", '     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n']";"['groups = dict(list(gb))\n', ""groups['foo']\n"", '     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n']"
817;;3;17383140;17383094.0;2;13;;;"<p><code>True</code> is <code>1</code> in Python, and likewise <code>False</code> is <code>0</code><sup>*</sup>:</p>

<pre><code>&gt;&gt;&gt; True == 1
True
&gt;&gt;&gt; False == 0
True
</code></pre>

<p>You should be able to perform any operations you want on them by just treating them as though they were numbers, as they <em>are</em> numbers:</p>

<pre><code>&gt;&gt;&gt; issubclass(bool, int)
True
&gt;&gt;&gt; True * 5
5
</code></pre>

<p>So to answer your question, no work necessary - you already have what you are looking for.</p>

<p><sup>* Note I use <em>is</em> as an English word, not the Python keyword <code>is</code> - <code>True</code> will not be the same object as any random <code>1</code>.</sup></p>
";;['>>> True == 1\nTrue\n>>> False == 0\nTrue\n', '>>> issubclass(bool, int)\nTrue\n>>> True * 5\n5\n'];['True', '1', 'False', '0', '>>> True == 1\nTrue\n>>> False == 0\nTrue\n', '>>> issubclass(bool, int)\nTrue\n>>> True * 5\n5\n', 'is', 'True', '1']
818;;0;17383325;17383094.0;2;12;;;"<p>You also can do this directly on Frames</p>

<pre><code>In [104]: df = DataFrame(dict(A = True, B = False),index=range(3))

In [105]: df
Out[105]: 
      A      B
0  True  False
1  True  False
2  True  False

In [106]: df.dtypes
Out[106]: 
A    bool
B    bool
dtype: object

In [107]: df.astype(int)
Out[107]: 
   A  B
0  1  0
1  1  0
2  1  0

In [108]: df.astype(int).dtypes
Out[108]: 
A    int64
B    int64
dtype: object
</code></pre>
";;['In [104]: df = DataFrame(dict(A = True, B = False),index=range(3))\n\nIn [105]: df\nOut[105]: \n      A      B\n0  True  False\n1  True  False\n2  True  False\n\nIn [106]: df.dtypes\nOut[106]: \nA    bool\nB    bool\ndtype: object\n\nIn [107]: df.astype(int)\nOut[107]: \n   A  B\n0  1  0\n1  1  0\n2  1  0\n\nIn [108]: df.astype(int).dtypes\nOut[108]: \nA    int64\nB    int64\ndtype: object\n'];['In [104]: df = DataFrame(dict(A = True, B = False),index=range(3))\n\nIn [105]: df\nOut[105]: \n      A      B\n0  True  False\n1  True  False\n2  True  False\n\nIn [106]: df.dtypes\nOut[106]: \nA    bool\nB    bool\ndtype: object\n\nIn [107]: df.astype(int)\nOut[107]: \n   A  B\n0  1  0\n1  1  0\n2  1  0\n\nIn [108]: df.astype(int).dtypes\nOut[108]: \nA    int64\nB    int64\ndtype: object\n']
819;;0;17439693;17438906.0;2;39;;;"<p>Starting from</p>

<pre><code>&gt;&gt;&gt; df
              val1  val2  val3
city_id                       
houston,tx       1     2     0
houston,tx       0     0     1
houston,tx       2     1     1
somewhere,ew     4     3     7
</code></pre>

<p>I might do</p>

<pre><code>&gt;&gt;&gt; df.groupby(df.index).sum()
              val1  val2  val3
city_id                       
houston,tx       3     3     2
somewhere,ew     4     3     7
</code></pre>

<p>or</p>

<pre><code>&gt;&gt;&gt; df.reset_index().groupby(""city_id"").sum()
              val1  val2  val3
city_id                       
houston,tx       3     3     2
somewhere,ew     4     3     7
</code></pre>

<p>The first approach passes the index values (in this case, the <code>city_id</code> values) to <code>groupby</code> and tells it to use those as the group keys, and the second resets the index and then selects the <code>city_id</code> column.  See <a href=""http://pandas.pydata.org/pandas-docs/dev/groupby.html#aggregation"">this section</a> of the docs for more examples.  Note that there are lots of other methods in the <code>DataFrameGroupBy</code> objects, too:</p>

<pre><code>&gt;&gt;&gt; df.groupby(df.index)
&lt;pandas.core.groupby.DataFrameGroupBy object at 0x1045a1790&gt;
&gt;&gt;&gt; df.groupby(df.index).max()
              val1  val2  val3
city_id                       
houston,tx       2     2     1
somewhere,ew     4     3     7
&gt;&gt;&gt; df.groupby(df.index).mean()
              val1  val2      val3
city_id                           
houston,tx       1     1  0.666667
somewhere,ew     4     3  7.000000
</code></pre>
";;"['>>> df\n              val1  val2  val3\ncity_id                       \nhouston,tx       1     2     0\nhouston,tx       0     0     1\nhouston,tx       2     1     1\nsomewhere,ew     4     3     7\n', '>>> df.groupby(df.index).sum()\n              val1  val2  val3\ncity_id                       \nhouston,tx       3     3     2\nsomewhere,ew     4     3     7\n', '>>> df.reset_index().groupby(""city_id"").sum()\n              val1  val2  val3\ncity_id                       \nhouston,tx       3     3     2\nsomewhere,ew     4     3     7\n', '>>> df.groupby(df.index)\n<pandas.core.groupby.DataFrameGroupBy object at 0x1045a1790>\n>>> df.groupby(df.index).max()\n              val1  val2  val3\ncity_id                       \nhouston,tx       2     2     1\nsomewhere,ew     4     3     7\n>>> df.groupby(df.index).mean()\n              val1  val2      val3\ncity_id                           \nhouston,tx       1     1  0.666667\nsomewhere,ew     4     3  7.000000\n']";"['>>> df\n              val1  val2  val3\ncity_id                       \nhouston,tx       1     2     0\nhouston,tx       0     0     1\nhouston,tx       2     1     1\nsomewhere,ew     4     3     7\n', '>>> df.groupby(df.index).sum()\n              val1  val2  val3\ncity_id                       \nhouston,tx       3     3     2\nsomewhere,ew     4     3     7\n', '>>> df.reset_index().groupby(""city_id"").sum()\n              val1  val2  val3\ncity_id                       \nhouston,tx       3     3     2\nsomewhere,ew     4     3     7\n', 'city_id', 'groupby', 'city_id', 'DataFrameGroupBy', '>>> df.groupby(df.index)\n<pandas.core.groupby.DataFrameGroupBy object at 0x1045a1790>\n>>> df.groupby(df.index).max()\n              val1  val2  val3\ncity_id                       \nhouston,tx       2     2     1\nsomewhere,ew     4     3     7\n>>> df.groupby(df.index).mean()\n              val1  val2      val3\ncity_id                           \nhouston,tx       1     1  0.666667\nsomewhere,ew     4     3  7.000000\n']"
820;;2;17468012;17465045.0;2;110;;;"<p>You should add <code>parse_dates=True</code>, or <code>parse_dates=['column name']</code> when reading, thats usually enough to magically parse it. But there are always weird formats which need to be defined manually. In such a case you can also add a date parser function, which is the most flexible way possible.</p>

<p>Suppose you have a column 'datetime' with your string, then:</p>

<pre><code>dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')

df = pd.read_csv(infile, parse_dates=['datetime'], date_parser=dateparse)
</code></pre>

<p>This way you can even combine multiple columns into a single datetime column, this merges a 'date' and a 'time' column into a single 'datetime' column:</p>

<pre><code>dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')

df = pd.read_csv(infile, parse_dates={'datetime': ['date', 'time']}, date_parser=dateparse)
</code></pre>
";;"[""dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n\ndf = pd.read_csv(infile, parse_dates=['datetime'], date_parser=dateparse)\n"", ""dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n\ndf = pd.read_csv(infile, parse_dates={'datetime': ['date', 'time']}, date_parser=dateparse)\n""]";"['parse_dates=True', ""parse_dates=['column name']"", ""dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n\ndf = pd.read_csv(infile, parse_dates=['datetime'], date_parser=dateparse)\n"", ""dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n\ndf = pd.read_csv(infile, parse_dates={'datetime': ['date', 'time']}, date_parser=dateparse)\n""]"
821;;3;17468154;17465045.0;2;10;;;"<p>pandas read_csv method is great for parsing dates.  Complete documentation at <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html</a></p>

<p>you can even have the different date parts in different columns and pass the parameter:</p>

<pre><code>parse_dates : boolean, list of ints or names, list of lists, or dict
If True -&gt; try parsing the index. If [1, 2, 3] -&gt; try parsing columns 1, 2, 3 each as a
separate date column. If [[1, 3]] -&gt; combine columns 1 and 3 and parse as a single date
column. {foo : [1, 3]} -&gt; parse columns 1, 3 as date and call result foo
</code></pre>

<p>The default sensing of dates works great, but it seems to be biased towards north american Date formats.   If you live elsewhere you might occasionally be caught by the results.   As far as I can remember 1/6/2000  means 6 January in the USA as opposed to 1 Jun where I live.  It is smart enough to swing them around if dates like 23/6/2000 are used.  Probably safer to stay with YYYYMMDD variations of date though.   Apologies to pandas developers,here but i have not tested it with local dates recently.</p>

<p>you can use the date_parser parameter to pass a function to convert your format.</p>

<pre><code>date_parser : function
Function to use for converting a sequence of string columns to an array of datetime
instances. The default uses dateutil.parser.parser to do the conversion.
</code></pre>
";;['parse_dates : boolean, list of ints or names, list of lists, or dict\nIf True -> try parsing the index. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a\nseparate date column. If [[1, 3]] -> combine columns 1 and 3 and parse as a single date\ncolumn. {\x91foo\x92 : [1, 3]} -> parse columns 1, 3 as date and call result \x91foo\x92\n', 'date_parser : function\nFunction to use for converting a sequence of string columns to an array of datetime\ninstances. The default uses dateutil.parser.parser to do the conversion.\n'];['parse_dates : boolean, list of ints or names, list of lists, or dict\nIf True -> try parsing the index. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a\nseparate date column. If [[1, 3]] -> combine columns 1 and 3 and parse as a single date\ncolumn. {\x91foo\x92 : [1, 3]} -> parse columns 1, 3 as date and call result \x91foo\x92\n', 'date_parser : function\nFunction to use for converting a sequence of string columns to an array of datetime\ninstances. The default uses dateutil.parser.parser to do the conversion.\n']
822;;0;17478495;17477979.0;2;116;;;"<p>The simplest way would be to first <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html""><code>replace</code></a> infs to NaN:</p>

<pre><code>df.replace([np.inf, -np.inf], np.nan)
</code></pre>

<p>and then use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html""><code>dropna</code></a>:</p>

<pre><code>df.replace([np.inf, -np.inf], np.nan).dropna(subset=[""col1"", ""col2""], how=""all"")
</code></pre>

<p>For example:</p>

<pre><code>In [11]: df = pd.DataFrame([1, 2, np.inf, -np.inf])

In [12]: df.replace([np.inf, -np.inf], np.nan)
Out[12]:
    0
0   1
1   2
2 NaN
3 NaN
</code></pre>

<p><em>The same method would work for a Series.</em></p>
";;"['df.replace([np.inf, -np.inf], np.nan)\n', 'df.replace([np.inf, -np.inf], np.nan).dropna(subset=[""col1"", ""col2""], how=""all"")\n', 'In [11]: df = pd.DataFrame([1, 2, np.inf, -np.inf])\n\nIn [12]: df.replace([np.inf, -np.inf], np.nan)\nOut[12]:\n    0\n0   1\n1   2\n2 NaN\n3 NaN\n']";"['replace', 'df.replace([np.inf, -np.inf], np.nan)\n', 'dropna', 'df.replace([np.inf, -np.inf], np.nan).dropna(subset=[""col1"", ""col2""], how=""all"")\n', 'In [11]: df = pd.DataFrame([1, 2, np.inf, -np.inf])\n\nIn [12]: df.replace([np.inf, -np.inf], np.nan)\nOut[12]:\n    0\n0   1\n1   2\n2 NaN\n3 NaN\n']"
823;;6;17496530;10715965.0;2;139;;;"<p>You could create a list of dictionaries, where each dictionary corresponds to an input data row. Once the list is complete, then create a data frame. This is a much faster approach. </p>

<p>I had a similar problem where if I created a data frame for each row and appended it to the main data frame it took 30 mins. On the other hand, if I used the below methodology, it was successful within seconds.</p>

<pre><code>rows_list = []
for row in input_rows:

        dict1 = {}
        # get input row in dictionary format
        # key = col_name
        dict1.update(blah..) 

        rows_list.append(dict1)

df = pd.DataFrame(rows_list)               
</code></pre>
";;['rows_list = []\nfor row in input_rows:\n\n        dict1 = {}\n        # get input row in dictionary format\n        # key = col_name\n        dict1.update(blah..) \n\n        rows_list.append(dict1)\n\ndf = pd.DataFrame(rows_list)               \n'];['rows_list = []\nfor row in input_rows:\n\n        dict1 = {}\n        # get input row in dictionary format\n        # key = col_name\n        dict1.update(blah..) \n\n        rows_list.append(dict1)\n\ndf = pd.DataFrame(rows_list)               \n']
824;;5;17531025;17530542.0;2;89;;;"<p>You can <em>append</em> to a csv by <a href=""http://docs.python.org/2/library/functions.html#open"">opening the file</a> in append mode:</p>

<pre><code>with open('my_csv.csv', 'a') as f:
    df.to_csv(f, header=False)
</code></pre>

<p>If this was your csv, <code>foo.csv</code>:</p>

<pre><code>,A,B,C
0,1,2,3
1,4,5,6
</code></pre>

<p>If you read that and then append, for example, <code>df + 6</code>:</p>

<pre><code>In [1]: df = pd.read_csv('foo.csv', index_col=0)

In [2]: df
Out[2]:
   A  B  C
0  1  2  3
1  4  5  6

In [3]: df + 6
Out[3]:
    A   B   C
0   7   8   9
1  10  11  12

In [4]: with open('foo.csv', 'a') as f:
             (df + 6).to_csv(f, header=False)
</code></pre>

<p><code>foo.csv</code> becomes:</p>

<pre><code>,A,B,C
0,1,2,3
1,4,5,6
0,7,8,9
1,10,11,12
</code></pre>
";;"[""with open('my_csv.csv', 'a') as f:\n    df.to_csv(f, header=False)\n"", ',A,B,C\n0,1,2,3\n1,4,5,6\n', ""In [1]: df = pd.read_csv('foo.csv', index_col=0)\n\nIn [2]: df\nOut[2]:\n   A  B  C\n0  1  2  3\n1  4  5  6\n\nIn [3]: df + 6\nOut[3]:\n    A   B   C\n0   7   8   9\n1  10  11  12\n\nIn [4]: with open('foo.csv', 'a') as f:\n             (df + 6).to_csv(f, header=False)\n"", ',A,B,C\n0,1,2,3\n1,4,5,6\n0,7,8,9\n1,10,11,12\n']";"[""with open('my_csv.csv', 'a') as f:\n    df.to_csv(f, header=False)\n"", 'foo.csv', ',A,B,C\n0,1,2,3\n1,4,5,6\n', 'df + 6', ""In [1]: df = pd.read_csv('foo.csv', index_col=0)\n\nIn [2]: df\nOut[2]:\n   A  B  C\n0  1  2  3\n1  4  5  6\n\nIn [3]: df + 6\nOut[3]:\n    A   B   C\n0   7   8   9\n1  10  11  12\n\nIn [4]: with open('foo.csv', 'a') as f:\n             (df + 6).to_csv(f, header=False)\n"", 'foo.csv', ',A,B,C\n0,1,2,3\n1,4,5,6\n0,7,8,9\n1,10,11,12\n']"
825;;7;17534256;17534106.0;2;7;;;"<p><code>NaN</code> can be used as a numerical value on mathematical operations, while <code>None</code> cannot (or at least shouldn't).</p>

<p><code>NaN</code> is a numeric value, as defined in <a href=""http://en.wikipedia.org/wiki/IEEE_floating_point"" rel=""noreferrer"">IEEE 754 floating-point standard</a>.
<code>None</code> is an internal Python tipe (<code>NoneType</code>) and would be more like ""inexistent"" or ""empty"" than ""numerically invalid"" in this context.</p>

<p>The main ""symptom"" of that is that, if you perform, say, an average or a sum on an array containing NaN, even a single one, you get NaN as a result...</p>

<p>In the other hand, you cannot perform mathematical operations using <code>None</code> as operand.</p>

<p>So, depending on the case, you could use <code>None</code> as a way to tell your algorithm not to consider invalid or inexistent values on computations. That would mean the algorithm should test each value to see if it is <code>None</code>.</p>

<p>Numpy has some functions to avoid NaN values to contaminate your results, such as <code>nansum</code> and <code>nan_to_num</code> for example.</p>
";;[];['NaN', 'None', 'NaN', 'None', 'NoneType', 'None', 'None', 'None', 'nansum', 'nan_to_num']
826;;2;17534682;17534106.0;2;41;;;"<p>NaN is used as a placeholder for <a href=""http://pandas.pydata.org/pandas-docs/dev/gotchas.html#choice-of-na-representation"" rel=""noreferrer"">missing data <em>consistently</em> in pandas</a>, consistency is good. I usually read/translate NaN as <strong>""missing""</strong>. <em>Also see the <a href=""http://pandas.pydata.org/pandas-docs/dev/missing_data.html"" rel=""noreferrer"">'working with missing data'</a> section in the docs.</em></p>

<p>Wes writes in the docs <a href=""http://pandas.pydata.org/pandas-docs/dev/gotchas.html#choice-of-na-representation"" rel=""noreferrer"">'choice of NA-representation'</a>:</p>

<blockquote>
  <p>After years of production use [NaN] has proven, at least in my opinion, to be the best decision given the state of affairs in NumPy and Python in general. The special value NaN (Not-A-Number) is used <em>everywhere</em> as the NA value, and there are API functions <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.core.common.isnull.html"" rel=""noreferrer""><code>isnull</code></a> and <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.core.common.notnull.html"" rel=""noreferrer""><code>notnull</code></a> which can be used across the dtypes to detect NA values.<br>
  ...<br>
  Thus, I have chosen the Pythonic practicality beats purity approach and traded integer NA capability for a much simpler approach of using a special value in float and object arrays to denote NA, and promoting integer arrays to floating when NAs must be introduced.</p>
</blockquote>

<p><em>Note: the <a href=""http://pandas.pydata.org/pandas-docs/dev/gotchas.html#support-for-integer-na"" rel=""noreferrer"">""gotcha"" that integer Series containing missing data are upcast to floats</a>.</em></p>

<p>In my opinion the main reason to use NaN (over None) is that it can be stored with numpy's float64 dtype, rather than the less efficient object dtype, <em>see <a href=""http://pandas.pydata.org/pandas-docs/dev/gotchas.html#na-type-promotions"" rel=""noreferrer"">NA type promotions</a></em>.</p>

<pre><code>#  without forcing dtype it changes None to NaN!
s_bad = pd.Series([1, None], dtype=object)
s_good = pd.Series([1, np.nan])

In [13]: s_bad.dtype
Out[13]: dtype('O')

In [14]: s_good.dtype
Out[14]: dtype('float64')
</code></pre>

<p>Jeff comments (below) on this:</p>

<blockquote>
  <p><code>np.nan</code> allows for vectorized operations; its a float value, while <code>None</code>, by definition, forces object type, which basically disables all efficiency in numpy.  </p>
  
  <blockquote>
    <p><strong>So repeat 3 times fast: object==bad, float==good</strong></p>
  </blockquote>
</blockquote>

<p>Saying that, many operations may still work just as well with None vs NaN (but perhaps are not supported i.e. they may sometimes give <a href=""https://stackoverflow.com/a/19866269/1240268"">surprising results</a>):</p>

<pre><code>In [15]: s_bad.sum()
Out[15]: 1

In [16]: s_good.sum()
Out[16]: 1.0
</code></pre>

<p>To answer the second question:<br>
You should be using <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.core.common.isnull.html"" rel=""noreferrer""><code>pd.isnull</code></a> and <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.core.common.notnull.html"" rel=""noreferrer""><code>pd.notnull</code></a> to test for missing data (NaN).</p>
";;"[""#  without forcing dtype it changes None to NaN!\ns_bad = pd.Series([1, None], dtype=object)\ns_good = pd.Series([1, np.nan])\n\nIn [13]: s_bad.dtype\nOut[13]: dtype('O')\n\nIn [14]: s_good.dtype\nOut[14]: dtype('float64')\n"", 'In [15]: s_bad.sum()\nOut[15]: 1\n\nIn [16]: s_good.sum()\nOut[16]: 1.0\n']";"['isnull', 'notnull', ""#  without forcing dtype it changes None to NaN!\ns_bad = pd.Series([1, None], dtype=object)\ns_good = pd.Series([1, np.nan])\n\nIn [13]: s_bad.dtype\nOut[13]: dtype('O')\n\nIn [14]: s_good.dtype\nOut[14]: dtype('float64')\n"", 'np.nan', 'None', 'In [15]: s_bad.sum()\nOut[15]: 1\n\nIn [16]: s_good.sum()\nOut[16]: 1.0\n', 'pd.isnull', 'pd.notnull']"
827;;0;17619032;17618981.0;2;27;;;"<p>Your code works for me.</p>

<pre><code>&gt;&gt;&gt; import pandas
&gt;&gt;&gt; df = pandas.DataFrame([{'c1':3,'c2':10},{'c1':2, 'c2':30},{'c1':1,'c2':20},{'c1':2,'c2':15},{'c1':2,'c2':100}])
&gt;&gt;&gt; df.sort(['c1','c2'], ascending=[False,True])
   c1   c2
0   3   10
3   2   15
1   2   30
4   2  100
2   1   20
</code></pre>

<hr>

<p>Did you paste as is?</p>

<pre><code>&gt;&gt;&gt; df.sort(['c1','c2'], ascending=[True,True])
   c1   c2
2   1   20
3   2   15
1   2   30
4   2  100
0   3   10
</code></pre>

<hr>

<p><strong>UPDATE</strong> <code>DataFrame.sort</code> is deprecated; use <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html"" rel=""nofollow noreferrer""><code>DataFrame.sort_values</code></a>.</p>

<pre><code>&gt;&gt;&gt; df.sort(['c1','c2'], ascending=[False,True])
__main__:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
   c1   c2
0   3   10
3   2   15
1   2   30
4   2  100
2   1   20
&gt;&gt;&gt; df.sort_values(['c1','c2'], ascending=[False,True])
   c1   c2
0   3   10
3   2   15
1   2   30
4   2  100
2   1   20
</code></pre>
";;"["">>> import pandas\n>>> df = pandas.DataFrame([{'c1':3,'c2':10},{'c1':2, 'c2':30},{'c1':1,'c2':20},{'c1':2,'c2':15},{'c1':2,'c2':100}])\n>>> df.sort(['c1','c2'], ascending=[False,True])\n   c1   c2\n0   3   10\n3   2   15\n1   2   30\n4   2  100\n2   1   20\n"", "">>> df.sort(['c1','c2'], ascending=[True,True])\n   c1   c2\n2   1   20\n3   2   15\n1   2   30\n4   2  100\n0   3   10\n"", "">>> df.sort(['c1','c2'], ascending=[False,True])\n__main__:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n   c1   c2\n0   3   10\n3   2   15\n1   2   30\n4   2  100\n2   1   20\n>>> df.sort_values(['c1','c2'], ascending=[False,True])\n   c1   c2\n0   3   10\n3   2   15\n1   2   30\n4   2  100\n2   1   20\n""]";"["">>> import pandas\n>>> df = pandas.DataFrame([{'c1':3,'c2':10},{'c1':2, 'c2':30},{'c1':1,'c2':20},{'c1':2,'c2':15},{'c1':2,'c2':100}])\n>>> df.sort(['c1','c2'], ascending=[False,True])\n   c1   c2\n0   3   10\n3   2   15\n1   2   30\n4   2  100\n2   1   20\n"", "">>> df.sort(['c1','c2'], ascending=[True,True])\n   c1   c2\n2   1   20\n3   2   15\n1   2   30\n4   2  100\n0   3   10\n"", 'DataFrame.sort', 'DataFrame.sort_values', "">>> df.sort(['c1','c2'], ascending=[False,True])\n__main__:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n   c1   c2\n0   3   10\n3   2   15\n1   2   30\n4   2  100\n2   1   20\n>>> df.sort_values(['c1','c2'], ascending=[False,True])\n   c1   c2\n0   3   10\n3   2   15\n1   2   30\n4   2  100\n2   1   20\n""]"
828;;1;17679517;17679089.0;2;43;;;"<p>You are looking for <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#aggregation""><code>size</code></a>:</p>

<pre><code>In [11]: df.groupby(['col5', 'col2']).size()
Out[11]:
col5  col2
1     A       1
      D       3
2     B       2
3     A       3
      C       1
4     B       1
5     B       2
6     B       1
dtype: int64
</code></pre>

<hr>

<p>To get the same answer as waitingkuo (the ""second question""), but slightly cleaner, is to groupby the level:</p>

<pre><code>In [12]: df.groupby(['col5', 'col2']).size().groupby(level=1).max()
Out[12]:
col2
A       3
B       2
C       1
D       3
dtype: int64
</code></pre>
";;"[""In [11]: df.groupby(['col5', 'col2']).size()\nOut[11]:\ncol5  col2\n1     A       1\n      D       3\n2     B       2\n3     A       3\n      C       1\n4     B       1\n5     B       2\n6     B       1\ndtype: int64\n"", ""In [12]: df.groupby(['col5', 'col2']).size().groupby(level=1).max()\nOut[12]:\ncol2\nA       3\nB       2\nC       1\nD       3\ndtype: int64\n""]";"['size', ""In [11]: df.groupby(['col5', 'col2']).size()\nOut[11]:\ncol5  col2\n1     A       1\n      D       3\n2     B       2\n3     A       3\n      C       1\n4     B       1\n5     B       2\n6     B       1\ndtype: int64\n"", ""In [12]: df.groupby(['col5', 'col2']).size().groupby(level=1).max()\nOut[12]:\ncol2\nA       3\nB       2\nC       1\nD       3\ndtype: int64\n""]"
829;;1;17679980;17679089.0;2;29;;;"<p>Followed by @Andy's answer, you can do following to solve your second question:</p>

<pre><code>In [56]: df.groupby(['col5','col2']).size().reset_index().groupby('col2')[[0]].max()
Out[56]: 
      0
col2   
A     3
B     2
C     1
D     3
</code></pre>
";;"[""In [56]: df.groupby(['col5','col2']).size().reset_index().groupby('col2')[[0]].max()\nOut[56]: \n      0\ncol2   \nA     3\nB     2\nC     1\nD     3\n""]";"[""In [56]: df.groupby(['col5','col2']).size().reset_index().groupby('col2')[[0]].max()\nOut[56]: \n      0\ncol2   \nA     3\nB     2\nC     1\nD     3\n""]"
830;;4;17682662;17682613.0;2;70;;;"<p>Use its value directly:</p>

<pre><code>In [79]: df[df.c &gt; 0.5][['b', 'e']].values
Out[79]: 
array([[ 0.98836259,  0.82403141],
       [ 0.337358  ,  0.02054435],
       [ 0.29271728,  0.37813099],
       [ 0.70033513,  0.69919695]])
</code></pre>
";;"[""In [79]: df[df.c > 0.5][['b', 'e']].values\nOut[79]: \narray([[ 0.98836259,  0.82403141],\n       [ 0.337358  ,  0.02054435],\n       [ 0.29271728,  0.37813099],\n       [ 0.70033513,  0.69919695]])\n""]";"[""In [79]: df[df.c > 0.5][['b', 'e']].values\nOut[79]: \narray([[ 0.98836259,  0.82403141],\n       [ 0.337358  ,  0.02054435],\n       [ 0.29271728,  0.37813099],\n       [ 0.70033513,  0.69919695]])\n""]"
831;;0;17682665;17682613.0;2;16;;;"<p>Perhaps something like this for the first problem, you can simply access the columns by their names:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))
&gt;&gt;&gt; df[df['c']&gt;.5][['b','e']]
          b         e
1  0.071146  0.132145
2  0.495152  0.420219
</code></pre>

<p>For the second problem:</p>

<pre><code>&gt;&gt;&gt; df[df['c']&gt;.5][['b','e']].values
array([[ 0.07114556,  0.13214495],
       [ 0.49515157,  0.42021946]])
</code></pre>
";;"["">>> df = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\n>>> df[df['c']>.5][['b','e']]\n          b         e\n1  0.071146  0.132145\n2  0.495152  0.420219\n"", "">>> df[df['c']>.5][['b','e']].values\narray([[ 0.07114556,  0.13214495],\n       [ 0.49515157,  0.42021946]])\n""]";"["">>> df = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\n>>> df[df['c']>.5][['b','e']]\n          b         e\n1  0.071146  0.132145\n2  0.495152  0.420219\n"", "">>> df[df['c']>.5][['b','e']].values\narray([[ 0.07114556,  0.13214495],\n       [ 0.49515157,  0.42021946]])\n""]"
832;;2;17682726;17682613.0;2;7;;;"<p><code>.loc</code> accept row and column selectors simultaneously (as do <code>.ix/.iloc</code> FYI)
This is done in a single pass as well.</p>

<pre><code>In [1]: df = DataFrame(np.random.rand(4,5), columns = list('abcde'))

In [2]: df
Out[2]: 
          a         b         c         d         e
0  0.669701  0.780497  0.955690  0.451573  0.232194
1  0.952762  0.585579  0.890801  0.643251  0.556220
2  0.900713  0.790938  0.952628  0.505775  0.582365
3  0.994205  0.330560  0.286694  0.125061  0.575153

In [5]: df.loc[df['c']&gt;0.5,['a','d']]
Out[5]: 
          a         d
0  0.669701  0.451573
1  0.952762  0.643251
2  0.900713  0.505775
</code></pre>

<p>And if you want the values (though this should pass directly to sklearn as is); frames support the array interface</p>

<pre><code>In [6]: df.loc[df['c']&gt;0.5,['a','d']].values
Out[6]: 
array([[ 0.66970138,  0.45157274],
       [ 0.95276167,  0.64325143],
       [ 0.90071271,  0.50577509]])
</code></pre>
";;"[""In [1]: df = DataFrame(np.random.rand(4,5), columns = list('abcde'))\n\nIn [2]: df\nOut[2]: \n          a         b         c         d         e\n0  0.669701  0.780497  0.955690  0.451573  0.232194\n1  0.952762  0.585579  0.890801  0.643251  0.556220\n2  0.900713  0.790938  0.952628  0.505775  0.582365\n3  0.994205  0.330560  0.286694  0.125061  0.575153\n\nIn [5]: df.loc[df['c']>0.5,['a','d']]\nOut[5]: \n          a         d\n0  0.669701  0.451573\n1  0.952762  0.643251\n2  0.900713  0.505775\n"", ""In [6]: df.loc[df['c']>0.5,['a','d']].values\nOut[6]: \narray([[ 0.66970138,  0.45157274],\n       [ 0.95276167,  0.64325143],\n       [ 0.90071271,  0.50577509]])\n""]";"['.loc', '.ix/.iloc', ""In [1]: df = DataFrame(np.random.rand(4,5), columns = list('abcde'))\n\nIn [2]: df\nOut[2]: \n          a         b         c         d         e\n0  0.669701  0.780497  0.955690  0.451573  0.232194\n1  0.952762  0.585579  0.890801  0.643251  0.556220\n2  0.900713  0.790938  0.952628  0.505775  0.582365\n3  0.994205  0.330560  0.286694  0.125061  0.575153\n\nIn [5]: df.loc[df['c']>0.5,['a','d']]\nOut[5]: \n          a         d\n0  0.669701  0.451573\n1  0.952762  0.643251\n2  0.900713  0.505775\n"", ""In [6]: df.loc[df['c']>0.5,['a','d']].values\nOut[6]: \narray([[ 0.66970138,  0.45157274],\n       [ 0.95276167,  0.64325143],\n       [ 0.90071271,  0.50577509]])\n""]"
833;;13;17690795;17690738.0;2;41;;;"<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; date_stngs = ('2008-12-20','2008-12-21','2008-12-22','2008-12-23')
&gt;&gt;&gt; a = pd.Series([pd.to_datetime(date) for date in date_stngs])
&gt;&gt;&gt; a
0    2008-12-20 00:00:00
1    2008-12-21 00:00:00
2    2008-12-22 00:00:00
3    2008-12-23 00:00:00
</code></pre>

<p><strong>UPDATE</strong></p>

<p>Use pandas.to_datetime(pd.Series(..)). It's concise and much faster than above code.</p>

<pre><code>&gt;&gt;&gt; pd.to_datetime(pd.Series(date_stngs))
0   2008-12-20 00:00:00
1   2008-12-21 00:00:00
2   2008-12-22 00:00:00
3   2008-12-23 00:00:00
</code></pre>
";;"["">>> import pandas as pd\n>>> date_stngs = ('2008-12-20','2008-12-21','2008-12-22','2008-12-23')\n>>> a = pd.Series([pd.to_datetime(date) for date in date_stngs])\n>>> a\n0    2008-12-20 00:00:00\n1    2008-12-21 00:00:00\n2    2008-12-22 00:00:00\n3    2008-12-23 00:00:00\n"", '>>> pd.to_datetime(pd.Series(date_stngs))\n0   2008-12-20 00:00:00\n1   2008-12-21 00:00:00\n2   2008-12-22 00:00:00\n3   2008-12-23 00:00:00\n']";"["">>> import pandas as pd\n>>> date_stngs = ('2008-12-20','2008-12-21','2008-12-22','2008-12-23')\n>>> a = pd.Series([pd.to_datetime(date) for date in date_stngs])\n>>> a\n0    2008-12-20 00:00:00\n1    2008-12-21 00:00:00\n2    2008-12-22 00:00:00\n3    2008-12-23 00:00:00\n"", '>>> pd.to_datetime(pd.Series(date_stngs))\n0   2008-12-20 00:00:00\n1   2008-12-21 00:00:00\n2   2008-12-22 00:00:00\n3   2008-12-23 00:00:00\n']"
834;;3;17690868;17690738.0;2;33;;;"<pre><code>In [46]: pd.to_datetime(pd.Series(date_stngs))
Out[46]: 
0   2008-12-20 00:00:00
1   2008-12-21 00:00:00
2   2008-12-22 00:00:00
3   2008-12-23 00:00:00
dtype: datetime64[ns]
</code></pre>

<h2>Update: benchmark</h2>

<pre><code>In [43]: dates = [(dt.datetime(1960, 1, 1)+dt.timedelta(days=i)).date().isoformat() for i in range(20000)]

In [44]: timeit pd.Series([pd.to_datetime(date) for date in dates])
1 loops, best of 3: 1.71 s per loop

In [45]: timeit pd.to_datetime(pd.Series(dates))
100 loops, best of 3: 5.71 ms per loop
</code></pre>
";;['In [46]: pd.to_datetime(pd.Series(date_stngs))\nOut[46]: \n0   2008-12-20 00:00:00\n1   2008-12-21 00:00:00\n2   2008-12-22 00:00:00\n3   2008-12-23 00:00:00\ndtype: datetime64[ns]\n', 'In [43]: dates = [(dt.datetime(1960, 1, 1)+dt.timedelta(days=i)).date().isoformat() for i in range(20000)]\n\nIn [44]: timeit pd.Series([pd.to_datetime(date) for date in dates])\n1 loops, best of 3: 1.71 s per loop\n\nIn [45]: timeit pd.to_datetime(pd.Series(dates))\n100 loops, best of 3: 5.71 ms per loop\n'];['In [46]: pd.to_datetime(pd.Series(date_stngs))\nOut[46]: \n0   2008-12-20 00:00:00\n1   2008-12-21 00:00:00\n2   2008-12-22 00:00:00\n3   2008-12-23 00:00:00\ndtype: datetime64[ns]\n', 'In [43]: dates = [(dt.datetime(1960, 1, 1)+dt.timedelta(days=i)).date().isoformat() for i in range(20000)]\n\nIn [44]: timeit pd.Series([pd.to_datetime(date) for date in dates])\n1 loops, best of 3: 1.71 s per loop\n\nIn [45]: timeit pd.to_datetime(pd.Series(dates))\n100 loops, best of 3: 5.71 ms per loop\n']
835;;2;17692156;17691447.0;2;31;;;"<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.stack.html"">stack</a> it first and then use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html"">value_counts</a>:</p>

<pre><code>In [14]: df.stack().value_counts()
Out[14]: 
192.248.8.183    3   
192.168.2.85     3   
66.249.74.52     2   
192.168.2.161    2   
124.43.113.22    1   
dtype: int64
</code></pre>
";;['In [14]: df.stack().value_counts()\nOut[14]: \n192.248.8.183    3   \n192.168.2.85     3   \n66.249.74.52     2   \n192.168.2.161    2   \n124.43.113.22    1   \ndtype: int64\n'];['In [14]: df.stack().value_counts()\nOut[14]: \n192.248.8.183    3   \n192.168.2.85     3   \n66.249.74.52     2   \n192.168.2.161    2   \n124.43.113.22    1   \ndtype: int64\n']
836;;0;17811984;17326973.0;2;13;;;"<p>There is probably no automatic way to do it right now, but as you use openpyxl, the following line (adapted from another answer by user <a href=""https://stackoverflow.com/users/443457/bufke"">Bufke</a> on <a href=""https://stackoverflow.com/a/14450572/2375855"">how to do in manually</a>) allows you to specify a sane value (in character widths):</p>

<pre><code>writer.sheets['Summary'].column_dimensions['A'].width = 15
</code></pre>
";;"[""writer.sheets['Summary'].column_dimensions['A'].width = 15\n""]";"[""writer.sheets['Summary'].column_dimensions['A'].width = 15\n""]"
837;;0;17813222;17812978.0;2;35;;;"<p>You can specify the <code>style</code> of the plotted line when calling <a href=""http://pandas.pydata.org/pandas-docs/version/0.15.0/generated/pandas.DataFrame.plot.html?highlight=plot#pandas-dataframe-plot"" rel=""noreferrer""><code>df.plot</code></a>:</p>

<pre><code>df.plot(x='col_name_1', y='col_name_2', style='o')
</code></pre>

<p>The <code>style</code> argument can also be a <code>dict</code> or <code>list</code>, e.g.:</p>

<pre><code>import numpy as np
import pandas as pd

d = {'one' : np.random.rand(10),
     'two' : np.random.rand(10)}

df = pd.DataFrame(d)

df.plot(style=['o','rx'])
</code></pre>

<p>All the accepted style formats are listed in the documentation of <a href=""http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot"" rel=""noreferrer""><code>matplotlib.pyplot.plot</code></a>.</p>

<p><img src=""https://i.stack.imgur.com/3FXAE.png"" alt=""Output""></p>
";;"[""df.plot(x='col_name_1', y='col_name_2', style='o')\n"", ""import numpy as np\nimport pandas as pd\n\nd = {'one' : np.random.rand(10),\n     'two' : np.random.rand(10)}\n\ndf = pd.DataFrame(d)\n\ndf.plot(style=['o','rx'])\n""]";"['style', 'df.plot', ""df.plot(x='col_name_1', y='col_name_2', style='o')\n"", 'style', 'dict', 'list', ""import numpy as np\nimport pandas as pd\n\nd = {'one' : np.random.rand(10),\n     'two' : np.random.rand(10)}\n\ndf = pd.DataFrame(d)\n\ndf.plot(style=['o','rx'])\n"", 'matplotlib.pyplot.plot']"
838;;1;17813277;17812978.0;2;29;;;"<p>For this (and most plotting) I would not rely on the Pandas wrappers to matplotlib. Instead, just use matplotlib directly:</p>

<pre><code>import matplotlib.pyplot as plt
plt.scatter(df['col_name_1'], df['col_name_2'])
plt.show() # Depending on whether you use IPython or interactive mode, etc.
</code></pre>

<p>and remember that you can access a NumPy array of the column's values with <code>df.col_name_1.values</code> for example.</p>

<p>I ran into trouble using this with Pandas default plotting in the case of a column of Timestamp values with millisecond precision. In trying to convert the objects to <code>datetime64</code> type, I also discovered a nasty issue: &lt; <a href=""https://stackoverflow.com/questions/26350364/pandas-gives-incorrect-result-when-asking-if-timestamp-column-values-have-attr-a"">Pandas gives incorrect result when asking if Timestamp column values have attr astype</a> >.</p>
";;"[""import matplotlib.pyplot as plt\nplt.scatter(df['col_name_1'], df['col_name_2'])\nplt.show() # Depending on whether you use IPython or interactive mode, etc.\n""]";"[""import matplotlib.pyplot as plt\nplt.scatter(df['col_name_1'], df['col_name_2'])\nplt.show() # Depending on whether you use IPython or interactive mode, etc.\n"", 'df.col_name_1.values', 'datetime64']"
839;;5;17819427;17818783.0;2;22;;;"<p>A direct conversion is not supported ATM. Contributions are welcome!</p>

<p>Try this, should be ok on memory as the SpareSeries is much like a csc_matrix (for 1 column)
and pretty space efficient</p>

<pre><code>In [37]: col = np.array([0,0,1,2,2,2])

In [38]: data = np.array([1,2,3,4,5,6],dtype='float64')

In [39]: m = csc_matrix( (data,(row,col)), shape=(3,3) )

In [40]: m
Out[40]: 
&lt;3x3 sparse matrix of type '&lt;type 'numpy.float64'&gt;'
        with 6 stored elements in Compressed Sparse Column format&gt;

In [46]: pd.SparseDataFrame([ pd.SparseSeries(m[i].toarray().ravel()) 
                              for i in np.arange(m.shape[0]) ])
Out[46]: 
   0  1  2
0  1  0  4
1  0  0  5
2  2  3  6

In [47]: df = pd.SparseDataFrame([ pd.SparseSeries(m[i].toarray().ravel()) 
                                   for i in np.arange(m.shape[0]) ])

In [48]: type(df)
Out[48]: pandas.sparse.frame.SparseDataFrame
</code></pre>
";;"[""In [37]: col = np.array([0,0,1,2,2,2])\n\nIn [38]: data = np.array([1,2,3,4,5,6],dtype='float64')\n\nIn [39]: m = csc_matrix( (data,(row,col)), shape=(3,3) )\n\nIn [40]: m\nOut[40]: \n<3x3 sparse matrix of type '<type 'numpy.float64'>'\n        with 6 stored elements in Compressed Sparse Column format>\n\nIn [46]: pd.SparseDataFrame([ pd.SparseSeries(m[i].toarray().ravel()) \n                              for i in np.arange(m.shape[0]) ])\nOut[46]: \n   0  1  2\n0  1  0  4\n1  0  0  5\n2  2  3  6\n\nIn [47]: df = pd.SparseDataFrame([ pd.SparseSeries(m[i].toarray().ravel()) \n                                   for i in np.arange(m.shape[0]) ])\n\nIn [48]: type(df)\nOut[48]: pandas.sparse.frame.SparseDataFrame\n""]";"[""In [37]: col = np.array([0,0,1,2,2,2])\n\nIn [38]: data = np.array([1,2,3,4,5,6],dtype='float64')\n\nIn [39]: m = csc_matrix( (data,(row,col)), shape=(3,3) )\n\nIn [40]: m\nOut[40]: \n<3x3 sparse matrix of type '<type 'numpy.float64'>'\n        with 6 stored elements in Compressed Sparse Column format>\n\nIn [46]: pd.SparseDataFrame([ pd.SparseSeries(m[i].toarray().ravel()) \n                              for i in np.arange(m.shape[0]) ])\nOut[46]: \n   0  1  2\n0  1  0  4\n1  0  0  5\n2  2  3  6\n\nIn [47]: df = pd.SparseDataFrame([ pd.SparseSeries(m[i].toarray().ravel()) \n                                   for i in np.arange(m.shape[0]) ])\n\nIn [48]: type(df)\nOut[48]: pandas.sparse.frame.SparseDataFrame\n""]"
840;;3;17840195;17839973.0;2;111;;;"<p>The error message says that if you're passing scalar values, you have to pass an index.  So you can either not use scalar values for the columns -- e.g. use a list:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'A': [a], 'B': [b]})
&gt;&gt;&gt; df
   A  B
0  2  3
</code></pre>

<p>or use scalar values and pass an index:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'A': a, 'B': b}, index=[0])
&gt;&gt;&gt; df
   A  B
0  2  3
</code></pre>
";;"["">>> df = pd.DataFrame({'A': [a], 'B': [b]})\n>>> df\n   A  B\n0  2  3\n"", "">>> df = pd.DataFrame({'A': a, 'B': b}, index=[0])\n>>> df\n   A  B\n0  2  3\n""]";"["">>> df = pd.DataFrame({'A': [a], 'B': [b]})\n>>> df\n   A  B\n0  2  3\n"", "">>> df = pd.DataFrame({'A': a, 'B': b}, index=[0])\n>>> df\n   A  B\n0  2  3\n""]"
841;;0;17840197;17839973.0;2;6;;;"<p>You need to provide iterables as the values for the Pandas DataFrame columns:</p>

<pre><code>df2 = pd.DataFrame({'A':[a],'B':[b]})
</code></pre>
";;"[""df2 = pd.DataFrame({'A':[a],'B':[b]})\n""]";"[""df2 = pd.DataFrame({'A':[a],'B':[b]})\n""]"
842;;10;17841294;17841149.0;2;70;;;"<pre><code>In [4]: df = read_csv(StringIO(data),sep='\s+')

In [5]: df
Out[5]: 
   A         B       C
0  1  0.749065    This
1  2  0.301084      is
2  3  0.463468       a
3  4  0.643961  random
4  1  0.866521  string
5  2  0.120737       !

In [6]: df.dtypes
Out[6]: 
A      int64
B    float64
C     object
dtype: object
</code></pre>

<p>When you apply your own function, there is not automatic exclusions of non-numeric columns. This is slower, though (that the applicatino of <code>.sum()</code> to the groupby</p>

<pre><code>In [8]: df.groupby('A').apply(lambda x: x.sum())
Out[8]: 
   A         B           C
A                         
1  2  1.615586  Thisstring
2  4  0.421821         is!
3  3  0.463468           a
4  4  0.643961      random
</code></pre>

<p>Sum by default concatenates</p>

<pre><code>In [9]: df.groupby('A')['C'].apply(lambda x: x.sum())
Out[9]: 
A
1    Thisstring
2           is!
3             a
4        random
dtype: object
</code></pre>

<p>You can do pretty much what you want</p>

<pre><code>In [11]: df.groupby('A')['C'].apply(lambda x: ""{%s}"" % ', '.join(x))
Out[11]: 
A
1    {This, string}
2           {is, !}
3               {a}
4          {random}
dtype: object
</code></pre>

<p>Doing this a whole frame group at a time. Key is to return a Series</p>

<pre><code>def f(x):
     return Series(dict(A = x['A'].sum(), 
                        B = x['B'].sum(), 
                        C = ""{%s}"" % ', '.join(x['C'])))

In [14]: df.groupby('A').apply(f)
Out[14]: 
   A         B               C
A                             
1  2  1.615586  {This, string}
2  4  0.421821         {is, !}
3  3  0.463468             {a}
4  4  0.643961        {random}
</code></pre>
";;"[""In [4]: df = read_csv(StringIO(data),sep='\\s+')\n\nIn [5]: df\nOut[5]: \n   A         B       C\n0  1  0.749065    This\n1  2  0.301084      is\n2  3  0.463468       a\n3  4  0.643961  random\n4  1  0.866521  string\n5  2  0.120737       !\n\nIn [6]: df.dtypes\nOut[6]: \nA      int64\nB    float64\nC     object\ndtype: object\n"", ""In [8]: df.groupby('A').apply(lambda x: x.sum())\nOut[8]: \n   A         B           C\nA                         \n1  2  1.615586  Thisstring\n2  4  0.421821         is!\n3  3  0.463468           a\n4  4  0.643961      random\n"", ""In [9]: df.groupby('A')['C'].apply(lambda x: x.sum())\nOut[9]: \nA\n1    Thisstring\n2           is!\n3             a\n4        random\ndtype: object\n"", 'In [11]: df.groupby(\'A\')[\'C\'].apply(lambda x: ""{%s}"" % \', \'.join(x))\nOut[11]: \nA\n1    {This, string}\n2           {is, !}\n3               {a}\n4          {random}\ndtype: object\n', 'def f(x):\n     return Series(dict(A = x[\'A\'].sum(), \n                        B = x[\'B\'].sum(), \n                        C = ""{%s}"" % \', \'.join(x[\'C\'])))\n\nIn [14]: df.groupby(\'A\').apply(f)\nOut[14]: \n   A         B               C\nA                             \n1  2  1.615586  {This, string}\n2  4  0.421821         {is, !}\n3  3  0.463468             {a}\n4  4  0.643961        {random}\n']";"[""In [4]: df = read_csv(StringIO(data),sep='\\s+')\n\nIn [5]: df\nOut[5]: \n   A         B       C\n0  1  0.749065    This\n1  2  0.301084      is\n2  3  0.463468       a\n3  4  0.643961  random\n4  1  0.866521  string\n5  2  0.120737       !\n\nIn [6]: df.dtypes\nOut[6]: \nA      int64\nB    float64\nC     object\ndtype: object\n"", '.sum()', ""In [8]: df.groupby('A').apply(lambda x: x.sum())\nOut[8]: \n   A         B           C\nA                         \n1  2  1.615586  Thisstring\n2  4  0.421821         is!\n3  3  0.463468           a\n4  4  0.643961      random\n"", ""In [9]: df.groupby('A')['C'].apply(lambda x: x.sum())\nOut[9]: \nA\n1    Thisstring\n2           is!\n3             a\n4        random\ndtype: object\n"", 'In [11]: df.groupby(\'A\')[\'C\'].apply(lambda x: ""{%s}"" % \', \'.join(x))\nOut[11]: \nA\n1    {This, string}\n2           {is, !}\n3               {a}\n4          {random}\ndtype: object\n', 'def f(x):\n     return Series(dict(A = x[\'A\'].sum(), \n                        B = x[\'B\'].sum(), \n                        C = ""{%s}"" % \', \'.join(x[\'C\'])))\n\nIn [14]: df.groupby(\'A\').apply(f)\nOut[14]: \n   A         B               C\nA                             \n1  2  1.615586  {This, string}\n2  4  0.421821         {is, !}\n3  3  0.463468             {a}\n4  4  0.643961        {random}\n']"
843;;1;17841308;17841149.0;2;21;;;"<p>You can use the <code>apply</code> method to apply an arbitrary function to the grouped data.  So if you want a set, apply <code>set</code>.  If you want a list, apply <code>list</code>.</p>

<pre><code>&gt;&gt;&gt; d
   A       B
0  1    This
1  2      is
2  3       a
3  4  random
4  1  string
5  2       !
&gt;&gt;&gt; d.groupby('A')['B'].apply(list)
A
1    [This, string]
2           [is, !]
3               [a]
4          [random]
dtype: object
</code></pre>

<p>If you want something else, just write a function that does what you want and then <code>apply</code> that.</p>
";;"["">>> d\n   A       B\n0  1    This\n1  2      is\n2  3       a\n3  4  random\n4  1  string\n5  2       !\n>>> d.groupby('A')['B'].apply(list)\nA\n1    [This, string]\n2           [is, !]\n3               [a]\n4          [random]\ndtype: object\n""]";"['apply', 'set', 'list', "">>> d\n   A       B\n0  1    This\n1  2      is\n2  3       a\n3  4  random\n4  1  string\n5  2       !\n>>> d.groupby('A')['B'].apply(list)\nA\n1    [This, string]\n2           [is, !]\n3               [a]\n4          [random]\ndtype: object\n"", 'apply']"
844;;3;17877159;17874063.0;2;25;;;"<p>The <code>normed=True</code> returns a histogram for which <code>np.sum(pdf * np.diff(bins))</code> equals 1. If you want the sum of the histogram to be 1 you can use Numpy's histogram() and normalize the results yourself.</p>

<pre><code>x = np.random.randn(30)

fig, ax = plt.subplots(1,2, figsize=(10,4))

ax[0].hist(x, normed=True, color='grey')

hist, bins = np.histogram(x)
ax[1].bar(bins[:-1], hist.astype(np.float32) / hist.sum(), width=(bins[1]-bins[0]), color='grey')

ax[0].set_title('normed=True')
ax[1].set_title('hist = hist / hist.sum()')
</code></pre>

<p><img src=""https://i.stack.imgur.com/geSyT.png"" alt=""enter image description here""></p>

<p>Btw: Strange plotting glitch at the first bin of the left plot.</p>
";;"[""x = np.random.randn(30)\n\nfig, ax = plt.subplots(1,2, figsize=(10,4))\n\nax[0].hist(x, normed=True, color='grey')\n\nhist, bins = np.histogram(x)\nax[1].bar(bins[:-1], hist.astype(np.float32) / hist.sum(), width=(bins[1]-bins[0]), color='grey')\n\nax[0].set_title('normed=True')\nax[1].set_title('hist = hist / hist.sum()')\n""]";"['normed=True', 'np.sum(pdf * np.diff(bins))', ""x = np.random.randn(30)\n\nfig, ax = plt.subplots(1,2, figsize=(10,4))\n\nax[0].hist(x, normed=True, color='grey')\n\nhist, bins = np.histogram(x)\nax[1].bar(bins[:-1], hist.astype(np.float32) / hist.sum(), width=(bins[1]-bins[0]), color='grey')\n\nax[0].set_title('normed=True')\nax[1].set_title('hist = hist / hist.sum()')\n""]"
845;;6;17950081;16396903.0;2;69;;;"<p>I think a more explicit way of doing this is to use drop.</p>

<p>The syntax is:</p>

<pre><code>df.drop(label)
</code></pre>

<p>And as pointed out by @tim and @ChaimG, this can be done in-place:</p>

<pre><code>df.drop(label, inplace=True)
</code></pre>

<p>One way of implementing this could be:</p>

<pre><code>df.drop(df.index[:3], inplace=True)
</code></pre>

<p>And another ""in place"" use:</p>

<pre><code>df.drop(df.head(3).index, inplace=True)
</code></pre>
";;['df.drop(label)\n', 'df.drop(label, inplace=True)\n', 'df.drop(df.index[:3], inplace=True)\n', 'df.drop(df.head(3).index, inplace=True)\n'];['df.drop(label)\n', 'df.drop(label, inplace=True)\n', 'df.drop(df.index[:3], inplace=True)\n', 'df.drop(df.head(3).index, inplace=True)\n']
846;;4;17950531;17950374.0;2;46;;;"<pre><code>In [16]: df = DataFrame(np.arange(10).reshape(5,2),columns=list('AB'))

In [17]: df
Out[17]: 
   A  B
0  0  1
1  2  3
2  4  5
3  6  7
4  8  9

In [18]: df.dtypes
Out[18]: 
A    int64
B    int64
dtype: object
</code></pre>

<p>Convert a series</p>

<pre><code>In [19]: df['A'].apply(str)
Out[19]: 
0    0
1    2
2    4
3    6
4    8
Name: A, dtype: object

In [20]: df['A'].apply(str)[0]
Out[20]: '0'
</code></pre>

<p>Convert the whole frame</p>

<pre><code>In [21]: df.applymap(str)
Out[21]: 
   A  B
0  0  1
1  2  3
2  4  5
3  6  7
4  8  9

In [22]: df.applymap(str).iloc[0,0]
Out[22]: '0'
</code></pre>
";;"[""In [16]: df = DataFrame(np.arange(10).reshape(5,2),columns=list('AB'))\n\nIn [17]: df\nOut[17]: \n   A  B\n0  0  1\n1  2  3\n2  4  5\n3  6  7\n4  8  9\n\nIn [18]: df.dtypes\nOut[18]: \nA    int64\nB    int64\ndtype: object\n"", ""In [19]: df['A'].apply(str)\nOut[19]: \n0    0\n1    2\n2    4\n3    6\n4    8\nName: A, dtype: object\n\nIn [20]: df['A'].apply(str)[0]\nOut[20]: '0'\n"", ""In [21]: df.applymap(str)\nOut[21]: \n   A  B\n0  0  1\n1  2  3\n2  4  5\n3  6  7\n4  8  9\n\nIn [22]: df.applymap(str).iloc[0,0]\nOut[22]: '0'\n""]";"[""In [16]: df = DataFrame(np.arange(10).reshape(5,2),columns=list('AB'))\n\nIn [17]: df\nOut[17]: \n   A  B\n0  0  1\n1  2  3\n2  4  5\n3  6  7\n4  8  9\n\nIn [18]: df.dtypes\nOut[18]: \nA    int64\nB    int64\ndtype: object\n"", ""In [19]: df['A'].apply(str)\nOut[19]: \n0    0\n1    2\n2    4\n3    6\n4    8\nName: A, dtype: object\n\nIn [20]: df['A'].apply(str)[0]\nOut[20]: '0'\n"", ""In [21]: df.applymap(str)\nOut[21]: \n   A  B\n0  0  1\n1  2  3\n2  4  5\n3  6  7\n4  8  9\n\nIn [22]: df.applymap(str).iloc[0,0]\nOut[22]: '0'\n""]"
847;;2;17973255;17972938.0;2;39;;;"<p>The <a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#vectorized-string-methods""><code>str.contains</code></a> method accepts a regular expression pattern:</p>

<pre><code>In [11]: pattern = '|'.join(mylist)

In [12]: pattern
Out[12]: 'dog|cat|fish'

In [13]: frame.a.str.contains(pattern)
Out[13]:
0     True
1    False
2     True
Name: a, dtype: bool
</code></pre>
";;"[""In [11]: pattern = '|'.join(mylist)\n\nIn [12]: pattern\nOut[12]: 'dog|cat|fish'\n\nIn [13]: frame.a.str.contains(pattern)\nOut[13]:\n0     True\n1    False\n2     True\nName: a, dtype: bool\n""]";"['str.contains', ""In [11]: pattern = '|'.join(mylist)\n\nIn [12]: pattern\nOut[12]: 'dog|cat|fish'\n\nIn [13]: frame.a.str.contains(pattern)\nOut[13]:\n0     True\n1    False\n2     True\nName: a, dtype: bool\n""]"
848;;0;17975690;17530542.0;2;130;;;"<p>You can specify a python write mode in the pandas to_csv() function. For append it is 'a'.</p>

<p>In your case:</p>

<blockquote>
  <p>df.to_csv('my_csv.csv', mode='a', header=False)</p>
</blockquote>

<p>The default mode is 'w'.</p>
";;[];[]
849;;7;17977609;17977540.0;2;60;;;"<p>You can still use the <a href=""http://pandas.pydata.org/pandas-docs/dev/io.html#excel-files"">ExcelFile</a> class (and the <code>sheet_names</code> attribute):</p>

<pre><code>xl = pd.ExcelFile('foo.xls')

xl.sheet_names  # see all sheet names

xl.parse(sheet_name)  # read a specific sheet to DataFrame
</code></pre>

<p><em>see <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.ExcelFile.parse.html"">docs for parse</a> for more options...</em></p>
";;"[""xl = pd.ExcelFile('foo.xls')\n\nxl.sheet_names  # see all sheet names\n\nxl.parse(sheet_name)  # read a specific sheet to DataFrame\n""]";"['sheet_names', ""xl = pd.ExcelFile('foo.xls')\n\nxl.sheet_names  # see all sheet names\n\nxl.parse(sheet_name)  # read a specific sheet to DataFrame\n""]"
850;;3;17978188;17978092.0;2;49;;;"<p>It's worth mentioning that you may have been able to read this in <strong>directly</strong> e.g. if you were using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html"" rel=""noreferrer""><code>read_csv</code></a> using <code>parse_dates=[['Date', 'Time']]</code>.</p>

<p>Assuming these are just strings you could simply add them together (with a space), allowing you to apply <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.tseries.tools.to_datetime.html"" rel=""noreferrer""><code>to_datetime</code></a>:</p>

<pre><code>In [11]: df['Date'] + ' ' + df['Time']
Out[11]:
0    01-06-2013 23:00:00
1    02-06-2013 01:00:00
2    02-06-2013 21:00:00
3    02-06-2013 22:00:00
4    02-06-2013 23:00:00
5    03-06-2013 01:00:00
6    03-06-2013 21:00:00
7    03-06-2013 22:00:00
8    03-06-2013 23:00:00
9    04-06-2013 01:00:00
dtype: object

In [12]: pd.to_datetime(df['Date'] + ' ' + df['Time'])
Out[12]:
0   2013-01-06 23:00:00
1   2013-02-06 01:00:00
2   2013-02-06 21:00:00
3   2013-02-06 22:00:00
4   2013-02-06 23:00:00
5   2013-03-06 01:00:00
6   2013-03-06 21:00:00
7   2013-03-06 22:00:00
8   2013-03-06 23:00:00
9   2013-04-06 01:00:00
dtype: datetime64[ns]
</code></pre>

<p><em>Note: surprisingly (for me), this works fine with NaNs being converted to NaT, but it is worth worrying that the conversion (perhaps using the <code>raise</code> argument).</em></p>
";;"[""In [11]: df['Date'] + ' ' + df['Time']\nOut[11]:\n0    01-06-2013 23:00:00\n1    02-06-2013 01:00:00\n2    02-06-2013 21:00:00\n3    02-06-2013 22:00:00\n4    02-06-2013 23:00:00\n5    03-06-2013 01:00:00\n6    03-06-2013 21:00:00\n7    03-06-2013 22:00:00\n8    03-06-2013 23:00:00\n9    04-06-2013 01:00:00\ndtype: object\n\nIn [12]: pd.to_datetime(df['Date'] + ' ' + df['Time'])\nOut[12]:\n0   2013-01-06 23:00:00\n1   2013-02-06 01:00:00\n2   2013-02-06 21:00:00\n3   2013-02-06 22:00:00\n4   2013-02-06 23:00:00\n5   2013-03-06 01:00:00\n6   2013-03-06 21:00:00\n7   2013-03-06 22:00:00\n8   2013-03-06 23:00:00\n9   2013-04-06 01:00:00\ndtype: datetime64[ns]\n""]";"['read_csv', ""parse_dates=[['Date', 'Time']]"", 'to_datetime', ""In [11]: df['Date'] + ' ' + df['Time']\nOut[11]:\n0    01-06-2013 23:00:00\n1    02-06-2013 01:00:00\n2    02-06-2013 21:00:00\n3    02-06-2013 22:00:00\n4    02-06-2013 23:00:00\n5    03-06-2013 01:00:00\n6    03-06-2013 21:00:00\n7    03-06-2013 22:00:00\n8    03-06-2013 23:00:00\n9    04-06-2013 01:00:00\ndtype: object\n\nIn [12]: pd.to_datetime(df['Date'] + ' ' + df['Time'])\nOut[12]:\n0   2013-01-06 23:00:00\n1   2013-02-06 01:00:00\n2   2013-02-06 21:00:00\n3   2013-02-06 22:00:00\n4   2013-02-06 23:00:00\n5   2013-03-06 01:00:00\n6   2013-03-06 21:00:00\n7   2013-03-06 22:00:00\n8   2013-03-06 23:00:00\n9   2013-04-06 01:00:00\ndtype: datetime64[ns]\n"", 'raise']"
851;;1;17978414;17978133.0;2;26;;;"<p>You could merge the sub-DataFrame (with just those columns):</p>

<pre><code>df2[list('xab')]  # df2 but only with columns x, a, and b

df1.merge(df2[list('xab')])
</code></pre>
";;"[""df2[list('xab')]  # df2 but only with columns x, a, and b\n\ndf1.merge(df2[list('xab')])\n""]";"[""df2[list('xab')]  # df2 but only with columns x, a, and b\n\ndf1.merge(df2[list('xab')])\n""]"
852;;4;18023468;18022845.0;2;125;;;"<p>You can just get/set the index via its <code>name</code> property</p>

<pre><code>In [7]: df.index.name
Out[7]: 'Index Title'

In [8]: df.index.name = 'foo'

In [9]: df.index.name
Out[9]: 'foo'

In [10]: df
Out[10]: 
         Column 1
foo              
Apples          1
Oranges         2
Puppies         3
Ducks           4
</code></pre>
";;"[""In [7]: df.index.name\nOut[7]: 'Index Title'\n\nIn [8]: df.index.name = 'foo'\n\nIn [9]: df.index.name\nOut[9]: 'foo'\n\nIn [10]: df\nOut[10]: \n         Column 1\nfoo              \nApples          1\nOranges         2\nPuppies         3\nDucks           4\n""]";"['name', ""In [7]: df.index.name\nOut[7]: 'Index Title'\n\nIn [8]: df.index.name = 'foo'\n\nIn [9]: df.index.name\nOut[9]: 'foo'\n\nIn [10]: df\nOut[10]: \n         Column 1\nfoo              \nApples          1\nOranges         2\nPuppies         3\nDucks           4\n""]"
853;;1;18023485;18022845.0;2;22;;;"<p><code>df.index.name</code> should do the trick.</p>

<p>Python has a <code>dir</code> function that let's you query object attributes. <code>dir(df.index)</code> was helpful here.</p>
";;[];['df.index.name', 'dir', 'dir(df.index)']
854;;1;18039175;18039057.0;2;17;;;"<p>The parser is getting confused by the header of the file.  It reads the first row and infers the number of columns from that row.  But the first two rows aren't representative of the actual data in the file.</p>

<p>Try it with <code>data = pd.read_csv(path, skiprows=2)</code></p>
";;[];['data = pd.read_csv(path, skiprows=2)']
855;;1;18046682;16392921.0;2;42;;;"<p>You can also call the show() function after each plot. 
e.g</p>

<pre><code>   plot(a)
   show()
   plot(b)
   show()
</code></pre>

<p>see example at - <a href=""http://nbviewer.ipython.org/6151560"" rel=""noreferrer"">http://nbviewer.ipython.org/6151560</a></p>
";;['   plot(a)\n   show()\n   plot(b)\n   show()\n'];['   plot(a)\n   show()\n   plot(b)\n   show()\n']
856;;0;18062430;18062135.0;2;23;;;"<p>Pandas will automatically align these passed in series and create the joint index
They happen to be the same here. <code>reset_index</code> moves the index to a column.</p>

<pre><code>In [2]: s1 = Series(randn(5),index=[1,2,4,5,6])

In [4]: s2 = Series(randn(5),index=[1,2,4,5,6])

In [8]: DataFrame(dict(s1 = s1, s2 = s2)).reset_index()
Out[8]: 
   index        s1        s2
0      1 -0.176143  0.128635
1      2 -1.286470  0.908497
2      4 -0.995881  0.528050
3      5  0.402241  0.458870
4      6  0.380457  0.072251
</code></pre>
";;['In [2]: s1 = Series(randn(5),index=[1,2,4,5,6])\n\nIn [4]: s2 = Series(randn(5),index=[1,2,4,5,6])\n\nIn [8]: DataFrame(dict(s1 = s1, s2 = s2)).reset_index()\nOut[8]: \n   index        s1        s2\n0      1 -0.176143  0.128635\n1      2 -1.286470  0.908497\n2      4 -0.995881  0.528050\n3      5  0.402241  0.458870\n4      6  0.380457  0.072251\n'];['reset_index', 'In [2]: s1 = Series(randn(5),index=[1,2,4,5,6])\n\nIn [4]: s2 = Series(randn(5),index=[1,2,4,5,6])\n\nIn [8]: DataFrame(dict(s1 = s1, s2 = s2)).reset_index()\nOut[8]: \n   index        s1        s2\n0      1 -0.176143  0.128635\n1      2 -1.286470  0.908497\n2      4 -0.995881  0.528050\n3      5  0.402241  0.458870\n4      6  0.380457  0.072251\n']
857;;7;18062521;18062135.0;2;172;;;"<p>I think <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.tools.merge.concat.html""><code>concat</code></a> is a nice way to do this. If they are present it uses the name attributes of the Series as the columns (otherwise it simply numbers them):</p>

<pre><code>In [1]: s1 = pd.Series([1, 2], index=['A', 'B'], name='s1')

In [2]: s2 = pd.Series([3, 4], index=['A', 'B'], name='s2')

In [3]: pd.concat([s1, s2], axis=1)
Out[3]:
   s1  s2
A   1   3
B   2   4

In [4]: pd.concat([s1, s2], axis=1).reset_index()
Out[4]:
  index  s1  s2
0     A   1   3
1     B   2   4
</code></pre>

<p><em>Note: This extends to more than 2 Series.</em></p>
";;"[""In [1]: s1 = pd.Series([1, 2], index=['A', 'B'], name='s1')\n\nIn [2]: s2 = pd.Series([3, 4], index=['A', 'B'], name='s2')\n\nIn [3]: pd.concat([s1, s2], axis=1)\nOut[3]:\n   s1  s2\nA   1   3\nB   2   4\n\nIn [4]: pd.concat([s1, s2], axis=1).reset_index()\nOut[4]:\n  index  s1  s2\n0     A   1   3\n1     B   2   4\n""]";"['concat', ""In [1]: s1 = pd.Series([1, 2], index=['A', 'B'], name='s1')\n\nIn [2]: s2 = pd.Series([3, 4], index=['A', 'B'], name='s2')\n\nIn [3]: pd.concat([s1, s2], axis=1)\nOut[3]:\n   s1  s2\nA   1   3\nB   2   4\n\nIn [4]: pd.concat([s1, s2], axis=1).reset_index()\nOut[4]:\n  index  s1  s2\n0     A   1   3\n1     B   2   4\n""]"
858;;10;18079695;18079563.0;2;24;;;"<p>Place both series in the python set container.  See documentation:  <a href=""http://docs.python.org/2/library/sets.html"">http://docs.python.org/2/library/sets.html</a>  </p>

<p>then use the set intersection method.</p>

<p>s1.intersection(s2)   and then transform back to list if needed.</p>

<p>Just noticed pandas in the tag.   Can translate back to that.</p>

<pre><code>pd.Series(list(set(s1).intersection(set(s2))))
</code></pre>

<p>from comments I have changed this to a more pythonic expression, which is shorter and easier to read:</p>

<pre><code>Series(list(set(s1) &amp; set(s2)))
</code></pre>

<p>should to the trick, except if the index data is also important to you</p>

<p>Have added the list(....)  to translate the set before going to pd.Series   Pandas does not accept a set as direct input for a Series.</p>
";;['pd.Series(list(set(s1).intersection(set(s2))))\n', 'Series(list(set(s1) & set(s2)))\n'];['pd.Series(list(set(s1).intersection(set(s2))))\n', 'Series(list(set(s1) & set(s2)))\n']
859;;5;18080142;18079563.0;2;8;;;"<p>If you are using Panda's, I assume you are also using NumPy. Numpy has a function <code>intersect1d</code> that will work with a Pandas' series. </p>

<p>Example:</p>

<pre><code>pd.Series(np.intersect1d(pd.Series([1,2,3,5,42]), pd.Series([4,5,6,20,42])))
</code></pre>

<p>will return a Series with the values 5 and 42.</p>
";;['pd.Series(np.intersect1d(pd.Series([1,2,3,5,42]), pd.Series([4,5,6,20,42])))\n'];['intersect1d', 'pd.Series(np.intersect1d(pd.Series([1,2,3,5,42]), pd.Series([4,5,6,20,42])))\n']
860;;4;18090009;18089667.0;2;8;;;"<p>If you know the <code>dtype</code>s of your array then you can directly compute the number of bytes that it will take to store your data + some for the Python objects themselves. A useful attribute of <code>numpy</code> arrays is <code>nbytes</code>. You can get the number of bytes from the arrays in a pandas <code>DataFrame</code> by doing</p>

<pre><code>nbytes = sum(block.values.nbytes for block in df.blocks.values())
</code></pre>

<p><code>object</code> dtype arrays store 8 bytes per object (object dtype arrays store a pointer to an opaque <code>PyObject</code>), so if you have strings in your csv you need to take into account that <code>read_csv</code> will turn those into <code>object</code> dtype arrays and adjust your calculations accordingly.</p>

<p>EDIT:</p>

<p>See the <a href=""http://docs.scipy.org/doc/numpy/reference/arrays.scalars.html#built-in-scalar-types"" rel=""nofollow""><code>numpy</code> scalar types page</a> for more details on the <code>object</code> <code>dtype</code>. Since only a reference is stored you need to take into account the size of the object in the array as well. As that page says, object arrays are somewhat similar to Python <code>list</code> objects.</p>
";;['nbytes = sum(block.values.nbytes for block in df.blocks.values())\n'];['dtype', 'numpy', 'nbytes', 'DataFrame', 'nbytes = sum(block.values.nbytes for block in df.blocks.values())\n', 'object', 'PyObject', 'read_csv', 'object', 'numpy', 'object', 'dtype', 'list']
861;;7;18090393;18089667.0;2;23;;;"<p>You have to do this in reverse.</p>

<pre><code>In [4]: DataFrame(randn(1000000,20)).to_csv('test.csv')

In [5]: !ls -ltr test.csv
-rw-rw-r-- 1 users 399508276 Aug  6 16:55 test.csv
</code></pre>

<p>Technically memory is about this (which includes the indexes)</p>

<pre><code>In [16]: df.values.nbytes + df.index.nbytes + df.columns.nbytes
Out[16]: 168000160
</code></pre>

<p>So 168MB in memory with a 400MB file, 1M rows of 20 float columns</p>

<pre><code>DataFrame(randn(1000000,20)).to_hdf('test.h5','df')

!ls -ltr test.h5
-rw-rw-r-- 1 users 168073944 Aug  6 16:57 test.h5
</code></pre>

<p>MUCH more compact when written as a binary HDF5 file</p>

<pre><code>In [12]: DataFrame(randn(1000000,20)).to_hdf('test.h5','df',complevel=9,complib='blosc')

In [13]: !ls -ltr test.h5
-rw-rw-r-- 1 users 154727012 Aug  6 16:58 test.h5
</code></pre>

<p>The data was random, so compression doesn't help too much</p>
";;"[""In [4]: DataFrame(randn(1000000,20)).to_csv('test.csv')\n\nIn [5]: !ls -ltr test.csv\n-rw-rw-r-- 1 users 399508276 Aug  6 16:55 test.csv\n"", 'In [16]: df.values.nbytes + df.index.nbytes + df.columns.nbytes\nOut[16]: 168000160\n', ""DataFrame(randn(1000000,20)).to_hdf('test.h5','df')\n\n!ls -ltr test.h5\n-rw-rw-r-- 1 users 168073944 Aug  6 16:57 test.h5\n"", ""In [12]: DataFrame(randn(1000000,20)).to_hdf('test.h5','df',complevel=9,complib='blosc')\n\nIn [13]: !ls -ltr test.h5\n-rw-rw-r-- 1 users 154727012 Aug  6 16:58 test.h5\n""]";"[""In [4]: DataFrame(randn(1000000,20)).to_csv('test.csv')\n\nIn [5]: !ls -ltr test.csv\n-rw-rw-r-- 1 users 399508276 Aug  6 16:55 test.csv\n"", 'In [16]: df.values.nbytes + df.index.nbytes + df.columns.nbytes\nOut[16]: 168000160\n', ""DataFrame(randn(1000000,20)).to_hdf('test.h5','df')\n\n!ls -ltr test.h5\n-rw-rw-r-- 1 users 168073944 Aug  6 16:57 test.h5\n"", ""In [12]: DataFrame(randn(1000000,20)).to_hdf('test.h5','df',complevel=9,complib='blosc')\n\nIn [13]: !ls -ltr test.h5\n-rw-rw-r-- 1 users 154727012 Aug  6 16:58 test.h5\n""]"
862;;5;18129082;18039057.0;2;127;;;"<p>you could also try;</p>

<pre><code>data = pd.read_csv('file1.csv', error_bad_lines=False)
</code></pre>
";;"[""data = pd.read_csv('file1.csv', error_bad_lines=False)\n""]";"[""data = pd.read_csv('file1.csv', error_bad_lines=False)\n""]"
863;;14;18145399;13411544.0;2;1202;;;"<p>The best way to do this in pandas is to use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"">drop</a>:</p>

<pre><code>df = df.drop('column_name', 1)
</code></pre>

<p>where <code>1</code> is the <em>axis</em> number (<code>0</code> for rows and <code>1</code> for columns.)</p>

<p>To delete the column without having to reassign <code>df</code> you can do:</p>

<pre><code>df.drop('column_name', axis=1, inplace=True)
</code></pre>

<p>Finally, to drop by column <em>number</em> instead of by column <em>label</em>, try this to delete, e.g. the 1st, 2nd and 4th columns:</p>

<pre><code>df.drop(df.columns[[0, 1, 3]], axis=1)  # df.columns is zero-based pd.Index 
</code></pre>
";;"[""df = df.drop('column_name', 1)\n"", ""df.drop('column_name', axis=1, inplace=True)\n"", 'df.drop(df.columns[[0, 1, 3]], axis=1)  # df.columns is zero-based pd.Index \n']";"[""df = df.drop('column_name', 1)\n"", '1', '0', '1', 'df', ""df.drop('column_name', axis=1, inplace=True)\n"", 'df.drop(df.columns[[0, 1, 3]], axis=1)  # df.columns is zero-based pd.Index \n']"
864;;3;18172249;18171739.0;2;155;;;"<p><code>read_csv</code> takes an <code>encoding</code> option to deal with files in different formats. I mostly use <code>read_csv('file', encoding = ""ISO-8859-1"")</code>, or alternatively <code>encoding = utf8</code> for reading, and generally <code>utf-8</code> for <code>to_csv</code>.</p>

<p>You can also use the <a href=""https://en.wikipedia.org/wiki/ISO/IEC_8859-1"" rel=""noreferrer"">alias</a> <code>'latin1'</code> instead of <code>'ISO-8859-1'</code>.</p>

<p>See <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html,"" rel=""noreferrer"">relevant Pandas documentation</a>,
<a href=""http://docs.python.org/2/library/csv.html#examples"" rel=""noreferrer"">python docs examples on csv files</a>, and plenty of related questions here on SO.</p>
";;[];"['read_csv', 'encoding', 'read_csv(\'file\', encoding = ""ISO-8859-1"")', 'encoding = utf8', 'utf-8', 'to_csv', ""'latin1'"", ""'ISO-8859-1'""]"
865;;9;18173074;18172851.0;2;352;;;"<p>If I'm understanding correctly, it should be as simple as:</p>

<pre><code>df = df[df.line_race != 0]
</code></pre>
";;['df = df[df.line_race != 0]\n'];['df = df[df.line_race != 0]\n']
866;;5;18173088;18172851.0;2;28;;;"<p>The best way to do this is with boolean masking:</p>

<pre><code>In [56]: df
Out[56]:
     line_date  daysago  line_race  rating    raw  wrating
0   2007-03-31       62         11      56  1.000   56.000
1   2007-03-10       83         11      67  1.000   67.000
2   2007-02-10      111          9      66  1.000   66.000
3   2007-01-13      139         10      83  0.881   73.096
4   2006-12-23      160         10      88  0.793   69.787
5   2006-11-09      204          9      52  0.637   33.106
6   2006-10-22      222          8      66  0.582   38.408
7   2006-09-29      245          9      70  0.519   36.318
8   2006-09-16      258         11      68  0.486   33.063
9   2006-08-30      275          8      72  0.447   32.160
10  2006-02-11      475          5      65  0.165   10.698
11  2006-01-13      504          0      70  0.142    9.969
12  2006-01-02      515          0      64  0.135    8.627
13  2005-12-06      542          0      70  0.118    8.246
14  2005-11-29      549          0      70  0.114    7.963
15  2005-11-22      556          0      -1  0.110   -0.110
16  2005-11-01      577          0      -1  0.099   -0.099
17  2005-10-20      589          0      -1  0.093   -0.093
18  2005-09-27      612          0      -1  0.083   -0.083
19  2005-09-07      632          0      -1  0.075   -0.075
20  2005-06-12      719          0      69  0.049    3.360
21  2005-05-29      733          0      -1  0.045   -0.045
22  2005-05-02      760          0      -1  0.040   -0.040
23  2005-04-02      790          0      -1  0.034   -0.034
24  2005-03-13      810          0      -1  0.031   -0.031
25  2004-11-09      934          0      -1  0.017   -0.017

In [57]: df[df.line_race != 0]
Out[57]:
     line_date  daysago  line_race  rating    raw  wrating
0   2007-03-31       62         11      56  1.000   56.000
1   2007-03-10       83         11      67  1.000   67.000
2   2007-02-10      111          9      66  1.000   66.000
3   2007-01-13      139         10      83  0.881   73.096
4   2006-12-23      160         10      88  0.793   69.787
5   2006-11-09      204          9      52  0.637   33.106
6   2006-10-22      222          8      66  0.582   38.408
7   2006-09-29      245          9      70  0.519   36.318
8   2006-09-16      258         11      68  0.486   33.063
9   2006-08-30      275          8      72  0.447   32.160
10  2006-02-11      475          5      65  0.165   10.698
</code></pre>

<p><strong>UPDATE:</strong> Now that pandas 0.13 is out, another way to do this is <code>df.query('line_race != 0')</code>.</p>
";;['In [56]: df\nOut[56]:\n     line_date  daysago  line_race  rating    raw  wrating\n0   2007-03-31       62         11      56  1.000   56.000\n1   2007-03-10       83         11      67  1.000   67.000\n2   2007-02-10      111          9      66  1.000   66.000\n3   2007-01-13      139         10      83  0.881   73.096\n4   2006-12-23      160         10      88  0.793   69.787\n5   2006-11-09      204          9      52  0.637   33.106\n6   2006-10-22      222          8      66  0.582   38.408\n7   2006-09-29      245          9      70  0.519   36.318\n8   2006-09-16      258         11      68  0.486   33.063\n9   2006-08-30      275          8      72  0.447   32.160\n10  2006-02-11      475          5      65  0.165   10.698\n11  2006-01-13      504          0      70  0.142    9.969\n12  2006-01-02      515          0      64  0.135    8.627\n13  2005-12-06      542          0      70  0.118    8.246\n14  2005-11-29      549          0      70  0.114    7.963\n15  2005-11-22      556          0      -1  0.110   -0.110\n16  2005-11-01      577          0      -1  0.099   -0.099\n17  2005-10-20      589          0      -1  0.093   -0.093\n18  2005-09-27      612          0      -1  0.083   -0.083\n19  2005-09-07      632          0      -1  0.075   -0.075\n20  2005-06-12      719          0      69  0.049    3.360\n21  2005-05-29      733          0      -1  0.045   -0.045\n22  2005-05-02      760          0      -1  0.040   -0.040\n23  2005-04-02      790          0      -1  0.034   -0.034\n24  2005-03-13      810          0      -1  0.031   -0.031\n25  2004-11-09      934          0      -1  0.017   -0.017\n\nIn [57]: df[df.line_race != 0]\nOut[57]:\n     line_date  daysago  line_race  rating    raw  wrating\n0   2007-03-31       62         11      56  1.000   56.000\n1   2007-03-10       83         11      67  1.000   67.000\n2   2007-02-10      111          9      66  1.000   66.000\n3   2007-01-13      139         10      83  0.881   73.096\n4   2006-12-23      160         10      88  0.793   69.787\n5   2006-11-09      204          9      52  0.637   33.106\n6   2006-10-22      222          8      66  0.582   38.408\n7   2006-09-29      245          9      70  0.519   36.318\n8   2006-09-16      258         11      68  0.486   33.063\n9   2006-08-30      275          8      72  0.447   32.160\n10  2006-02-11      475          5      65  0.165   10.698\n'];"['In [56]: df\nOut[56]:\n     line_date  daysago  line_race  rating    raw  wrating\n0   2007-03-31       62         11      56  1.000   56.000\n1   2007-03-10       83         11      67  1.000   67.000\n2   2007-02-10      111          9      66  1.000   66.000\n3   2007-01-13      139         10      83  0.881   73.096\n4   2006-12-23      160         10      88  0.793   69.787\n5   2006-11-09      204          9      52  0.637   33.106\n6   2006-10-22      222          8      66  0.582   38.408\n7   2006-09-29      245          9      70  0.519   36.318\n8   2006-09-16      258         11      68  0.486   33.063\n9   2006-08-30      275          8      72  0.447   32.160\n10  2006-02-11      475          5      65  0.165   10.698\n11  2006-01-13      504          0      70  0.142    9.969\n12  2006-01-02      515          0      64  0.135    8.627\n13  2005-12-06      542          0      70  0.118    8.246\n14  2005-11-29      549          0      70  0.114    7.963\n15  2005-11-22      556          0      -1  0.110   -0.110\n16  2005-11-01      577          0      -1  0.099   -0.099\n17  2005-10-20      589          0      -1  0.093   -0.093\n18  2005-09-27      612          0      -1  0.083   -0.083\n19  2005-09-07      632          0      -1  0.075   -0.075\n20  2005-06-12      719          0      69  0.049    3.360\n21  2005-05-29      733          0      -1  0.045   -0.045\n22  2005-05-02      760          0      -1  0.040   -0.040\n23  2005-04-02      790          0      -1  0.034   -0.034\n24  2005-03-13      810          0      -1  0.031   -0.031\n25  2004-11-09      934          0      -1  0.017   -0.017\n\nIn [57]: df[df.line_race != 0]\nOut[57]:\n     line_date  daysago  line_race  rating    raw  wrating\n0   2007-03-31       62         11      56  1.000   56.000\n1   2007-03-10       83         11      67  1.000   67.000\n2   2007-02-10      111          9      66  1.000   66.000\n3   2007-01-13      139         10      83  0.881   73.096\n4   2006-12-23      160         10      88  0.793   69.787\n5   2006-11-09      204          9      52  0.637   33.106\n6   2006-10-22      222          8      66  0.582   38.408\n7   2006-09-29      245          9      70  0.519   36.318\n8   2006-09-16      258         11      68  0.486   33.063\n9   2006-08-30      275          8      72  0.447   32.160\n10  2006-02-11      475          5      65  0.165   10.698\n', ""df.query('line_race != 0')""]"
867;;0;18196299;18196203.0;2;47;;;"<pre><code>df.loc[df['line_race'] == 0, 'rating'] = 0
</code></pre>
";;"[""df.loc[df['line_race'] == 0, 'rating'] = 0\n""]";"[""df.loc[df['line_race'] == 0, 'rating'] = 0\n""]"
868;;4;18215499;18215317.0;2;67;;;"<p>You can convert it to a timedelta with a day precision. To extract the integer value of days you divide it with a timedelta of one day.</p>

<pre><code>&gt;&gt;&gt; x = np.timedelta64(2069211000000000, 'ns')
&gt;&gt;&gt; days = x.astype('timedelta64[D]')
&gt;&gt;&gt; days / np.timedelta64(1, 'D')
23
</code></pre>

<p>Or, as @PhillipCloud suggested, just <code>days.astype(int)</code> since the <code>timedelta</code> is just a 64bit integer that is interpreted in various ways depending on the second parameter you passed in (<code>'D'</code>, <code>'ns'</code>, ...).</p>

<p>You can find more about it <a href=""http://docs.scipy.org/doc/numpy-dev/reference/arrays.datetime.html"">here</a>.</p>
";;"["">>> x = np.timedelta64(2069211000000000, 'ns')\n>>> days = x.astype('timedelta64[D]')\n>>> days / np.timedelta64(1, 'D')\n23\n""]";"["">>> x = np.timedelta64(2069211000000000, 'ns')\n>>> days = x.astype('timedelta64[D]')\n>>> days / np.timedelta64(1, 'D')\n23\n"", 'days.astype(int)', 'timedelta', ""'D'"", ""'ns'""]"
869;;3;18261958;13167391.0;2;28;;;"<p>As of pandas 0.12 you can do:</p>

<pre><code>&gt;&gt;&gt; grouped.filter(lambda x: len(x) &gt; 1)

     A  B
0  foo  0
2  foo  2
3  foo  3
</code></pre>
";;['>>> grouped.filter(lambda x: len(x) > 1)\n\n     A  B\n0  foo  0\n2  foo  2\n3  foo  3\n'];['>>> grouped.filter(lambda x: len(x) > 1)\n\n     A  B\n0  foo  0\n2  foo  2\n3  foo  3\n']
870;;4;18316830;18316211.0;2;27;;;"<p>I don't believe <code>apply</code> has access to the index; it treats each row as a numpy object, not a Series, as you can see:</p>

<pre><code>In [27]: s.apply(lambda x: type(x))
Out[27]: 
a  b
1  2    &lt;type 'numpy.float64'&gt;
3  6    &lt;type 'numpy.float64'&gt;
4  4    &lt;type 'numpy.float64'&gt;
</code></pre>

<p>To get around this limitation, promote the indexes to columns, apply your function, and recreate a Series with the original index.</p>

<pre><code>Series(s.reset_index().apply(f, axis=1).values, index=s.index)
</code></pre>

<p>Other approaches might use <code>s.get_level_values</code>, which often gets a little ugly in my opinion, or <code>s.iterrows()</code>, which is likely to be slower -- perhaps depending on exactly what <code>f</code> does.</p>
";;"[""In [27]: s.apply(lambda x: type(x))\nOut[27]: \na  b\n1  2    <type 'numpy.float64'>\n3  6    <type 'numpy.float64'>\n4  4    <type 'numpy.float64'>\n"", 'Series(s.reset_index().apply(f, axis=1).values, index=s.index)\n']";"['apply', ""In [27]: s.apply(lambda x: type(x))\nOut[27]: \na  b\n1  2    <type 'numpy.float64'>\n3  6    <type 'numpy.float64'>\n4  4    <type 'numpy.float64'>\n"", 'Series(s.reset_index().apply(f, axis=1).values, index=s.index)\n', 's.get_level_values', 's.iterrows()', 'f']"
871;;0;18317067;15943769.0;2;56;;;"<p>Use <code>len(df)</code>. This works as of pandas 0.11 or maybe even earlier.</p>

<p><code>__len__()</code> is currently (0.12) documented with <code>Returns length of index</code>. Timing info, set up the same way as in root's answer:</p>

<pre><code>In [7]: timeit len(df.index)
1000000 loops, best of 3: 248 ns per loop

In [8]: timeit len(df)
1000000 loops, best of 3: 573 ns per loop
</code></pre>

<p>Due to one additional function call it is a bit slower than calling <code>len(df.index)</code> directly, but this should not play any role in most use cases.</p>
";;['In [7]: timeit len(df.index)\n1000000 loops, best of 3: 248 ns per loop\n\nIn [8]: timeit len(df)\n1000000 loops, best of 3: 573 ns per loop\n'];['len(df)', '__len__()', 'Returns length of index', 'In [7]: timeit len(df.index)\n1000000 loops, best of 3: 248 ns per loop\n\nIn [8]: timeit len(df)\n1000000 loops, best of 3: 573 ns per loop\n', 'len(df.index)']
872;;1;18327852;18327624.0;2;57;;;"<pre><code>&gt;&gt;&gt; myseries[myseries == 7]
3    7
dtype: int64
&gt;&gt;&gt; myseries[myseries == 7].index[0]
3
</code></pre>

<p>Though I admit that there should be a better way to do that, but this at least avoids iterating and looping through the object and moves it to the C level.</p>
";;['>>> myseries[myseries == 7]\n3    7\ndtype: int64\n>>> myseries[myseries == 7].index[0]\n3\n'];['>>> myseries[myseries == 7]\n3    7\ndtype: int64\n>>> myseries[myseries == 7].index[0]\n3\n']
873;;3;18334025;18327624.0;2;16;;;"<p>Converting to an Index, you can use <code>get_loc</code></p>

<pre><code>In [1]: myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])

In [3]: Index(myseries).get_loc(7)
Out[3]: 3

In [4]: Index(myseries).get_loc(10)
KeyError: 10
</code></pre>

<p>Duplicate handling</p>

<pre><code>In [5]: Index([1,1,2,2,3,4]).get_loc(2)
Out[5]: slice(2, 4, None)
</code></pre>

<p>Will return a boolean array if non-contiguous returns</p>

<pre><code>In [6]: Index([1,1,2,1,3,2,4]).get_loc(2)
Out[6]: array([False, False,  True, False, False,  True, False], dtype=bool)
</code></pre>

<p>Uses a hashtable internally, so fast</p>

<pre><code>In [7]: s = Series(randint(0,10,10000))

In [9]: %timeit s[s == 5]
1000 loops, best of 3: 203 s per loop

In [12]: i = Index(s)

In [13]: %timeit i.get_loc(5)
1000 loops, best of 3: 226 s per loop
</code></pre>

<p>As Viktor points out, there is a one-time creation overhead to creating an index (its incurred when you actually DO something with the index, e.g. the <code>is_unique</code>)</p>

<pre><code>In [2]: s = Series(randint(0,10,10000))

In [3]: %timeit Index(s)
100000 loops, best of 3: 9.6 s per loop

In [4]: %timeit Index(s).is_unique
10000 loops, best of 3: 140 s per loop
</code></pre>
";;['In [1]: myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])\n\nIn [3]: Index(myseries).get_loc(7)\nOut[3]: 3\n\nIn [4]: Index(myseries).get_loc(10)\nKeyError: 10\n', 'In [5]: Index([1,1,2,2,3,4]).get_loc(2)\nOut[5]: slice(2, 4, None)\n', 'In [6]: Index([1,1,2,1,3,2,4]).get_loc(2)\nOut[6]: array([False, False,  True, False, False,  True, False], dtype=bool)\n', 'In [7]: s = Series(randint(0,10,10000))\n\nIn [9]: %timeit s[s == 5]\n1000 loops, best of 3: 203 s per loop\n\nIn [12]: i = Index(s)\n\nIn [13]: %timeit i.get_loc(5)\n1000 loops, best of 3: 226 s per loop\n', 'In [2]: s = Series(randint(0,10,10000))\n\nIn [3]: %timeit Index(s)\n100000 loops, best of 3: 9.6 s per loop\n\nIn [4]: %timeit Index(s).is_unique\n10000 loops, best of 3: 140 s per loop\n'];['get_loc', 'In [1]: myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])\n\nIn [3]: Index(myseries).get_loc(7)\nOut[3]: 3\n\nIn [4]: Index(myseries).get_loc(10)\nKeyError: 10\n', 'In [5]: Index([1,1,2,2,3,4]).get_loc(2)\nOut[5]: slice(2, 4, None)\n', 'In [6]: Index([1,1,2,1,3,2,4]).get_loc(2)\nOut[6]: array([False, False,  True, False, False,  True, False], dtype=bool)\n', 'In [7]: s = Series(randint(0,10,10000))\n\nIn [9]: %timeit s[s == 5]\n1000 loops, best of 3: 203 s per loop\n\nIn [12]: i = Index(s)\n\nIn [13]: %timeit i.get_loc(5)\n1000 loops, best of 3: 226 s per loop\n', 'is_unique', 'In [2]: s = Series(randint(0,10,10000))\n\nIn [3]: %timeit Index(s)\n100000 loops, best of 3: 9.6 s per loop\n\nIn [4]: %timeit Index(s).is_unique\n10000 loops, best of 3: 140 s per loop\n']
874;;1;18360223;18358938.0;2;53;;;"<p>To get the <code>index</code> values as a <code>list</code>/<code>list</code> of <code>tuple</code>s for <code>Index</code>/<code>MultiIndex</code> do:</p>

<pre><code>df.index.values.tolist()  # an ndarray method, you probably shouldn't depend on this
</code></pre>

<p>or</p>

<pre><code>list(df.index.values)  # this will always work in pandas
</code></pre>
";;"[""df.index.values.tolist()  # an ndarray method, you probably shouldn't depend on this\n"", 'list(df.index.values)  # this will always work in pandas\n']";"['index', 'list', 'list', 'tuple', 'Index', 'MultiIndex', ""df.index.values.tolist()  # an ndarray method, you probably shouldn't depend on this\n"", 'list(df.index.values)  # this will always work in pandas\n']"
875;;0;18369312;17709641.0;2;57;;;"<p>(to expand a bit on my comment)</p>

<p>Numpy developers follow in general a policy of keeping a backward compatible binary interface (ABI). However, the ABI is not forward compatible.</p>

<p>What that means:</p>

<p>A package, that uses numpy in a compiled extension, is compiled against a specific version of numpy. Future version of numpy will be compatible with the compiled extension of the package (for exception see below).
Distributers of those other packages do not need to recompile their package against a newer  versions of numpy and users do not need to update these other packages, when users update to a newer version of numpy.</p>

<p>However, this does not go in the other direction. If a package is compiled against a specific numpy version, say 1.7, then there is no guarantee that the binaries of that package will work with older numpy versions, say 1.6, and very often or most of the time they will not.</p>

<p>The binary distribution of packages like pandas and statsmodels, that are compiled against a recent version of numpy, will not work when an older version of numpy is installed.
Some packages, for example matplotlib, if I remember correctly, compile their extensions against the oldest numpy version that they support. In this case, users with the same old or any more recent version of numpy can use those binaries.</p>

<p>The error message in the question is a typical result of binary incompatibilities.</p>

<p>The solution is to get a binary compatible version, either by updating numpy to at least the version against which pandas or statsmodels were compiled, or to recompile pandas and statsmodels against the older version of numpy that is already installed.</p>

<p>Breaking the ABI backward compatibility:</p>

<p>Sometimes improvements or refactorings in numpy break ABI backward compatibility. This happened (unintentionally) with numpy 1.4.0.
As a consequence, users that updated numpy to 1.4.0, had binary incompatibilities with all other compiled packages, that were compiled against a previous version of numpy. This requires that all packages with binary extensions that use numpy have to be recompiled to work with the ABI incompatible version. </p>
";;[];[]
876;;10;18431417;18429491.0;2;43;;;"<p>This is <a href=""http://pandas.pydata.org/pandas-docs/stable/missing_data.html#na-values-in-groupby"" rel=""noreferrer"">mentioned in the Missing Data section of the docs</a>:</p>

<blockquote>
  <p>NA groups in GroupBy are automatically excluded. This behavior is consistent with R, for example.</p>
</blockquote>

<p>One workaround is to use a placeholder before doing the groupby (e.g. -1):</p>

<pre><code>In [11]: df.fillna(-1)
Out[11]: 
   a   b
0  1   4
1  2  -1
2  3   6

In [12]: df.fillna(-1).groupby('b').sum()
Out[12]: 
    a
b    
-1  2
4   1
6   3
</code></pre>

<p><em>That said, this feels pretty awful hack... perhaps there should be an option to include NaN in groupby (see <a href=""https://github.com/pydata/pandas/issues/3729"" rel=""noreferrer"">this github issue</a> - which uses the same placeholder hack).</em></p>
";;"[""In [11]: df.fillna(-1)\nOut[11]: \n   a   b\n0  1   4\n1  2  -1\n2  3   6\n\nIn [12]: df.fillna(-1).groupby('b').sum()\nOut[12]: \n    a\nb    \n-1  2\n4   1\n6   3\n""]";"[""In [11]: df.fillna(-1)\nOut[11]: \n   a   b\n0  1   4\n1  2  -1\n2  3   6\n\nIn [12]: df.fillna(-1).groupby('b').sum()\nOut[12]: \n    a\nb    \n-1  2\n4   1\n6   3\n""]"
877;;5;18467097;15771472.0;2;30;;;"<p>I just had the same question but with irregularly spaced datapoints. Resample is not really an option here. So I created my own function. Maybe it will be useful for others too:</p>

<pre><code>from pandas import Series, DataFrame
import pandas as pd
from datetime import datetime, timedelta
import numpy as np

def rolling_mean(data, window, min_periods=1, center=False):
    ''' Function that computes a rolling mean

    Parameters
    ----------
    data : DataFrame or Series
           If a DataFrame is passed, the rolling_mean is computed for all columns.
    window : int or string
             If int is passed, window is the number of observations used for calculating 
             the statistic, as defined by the function pd.rolling_mean()
             If a string is passed, it must be a frequency string, e.g. '90S'. This is
             internally converted into a DateOffset object, representing the window size.
    min_periods : int
                  Minimum number of observations in window required to have a value.

    Returns
    -------
    Series or DataFrame, if more than one column    
    '''
    def f(x):
        '''Function to apply that actually computes the rolling mean'''
        if center == False:
            dslice = col[x-pd.datetools.to_offset(window).delta+timedelta(0,0,1):x]
                # adding a microsecond because when slicing with labels start and endpoint
                # are inclusive
        else:
            dslice = col[x-pd.datetools.to_offset(window).delta/2+timedelta(0,0,1):
                         x+pd.datetools.to_offset(window).delta/2]
        if dslice.size &lt; min_periods:
            return np.nan
        else:
            return dslice.mean()

    data = DataFrame(data.copy())
    dfout = DataFrame()
    if isinstance(window, int):
        dfout = pd.rolling_mean(data, window, min_periods=min_periods, center=center)
    elif isinstance(window, basestring):
        idx = Series(data.index.to_pydatetime(), index=data.index)
        for colname, col in data.iterkv():
            result = idx.apply(f)
            result.name = colname
            dfout = dfout.join(result, how='outer')
    if dfout.columns.size == 1:
        dfout = dfout.ix[:,0]
    return dfout


# Example
idx = [datetime(2011, 2, 7, 0, 0),
       datetime(2011, 2, 7, 0, 1),
       datetime(2011, 2, 7, 0, 1, 30),
       datetime(2011, 2, 7, 0, 2),
       datetime(2011, 2, 7, 0, 4),
       datetime(2011, 2, 7, 0, 5),
       datetime(2011, 2, 7, 0, 5, 10),
       datetime(2011, 2, 7, 0, 6),
       datetime(2011, 2, 7, 0, 8),
       datetime(2011, 2, 7, 0, 9)]
idx = pd.Index(idx)
vals = np.arange(len(idx)).astype(float)
s = Series(vals, index=idx)
rm = rolling_mean(s, window='2min')
</code></pre>
";;"[""from pandas import Series, DataFrame\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport numpy as np\n\ndef rolling_mean(data, window, min_periods=1, center=False):\n    ''' Function that computes a rolling mean\n\n    Parameters\n    ----------\n    data : DataFrame or Series\n           If a DataFrame is passed, the rolling_mean is computed for all columns.\n    window : int or string\n             If int is passed, window is the number of observations used for calculating \n             the statistic, as defined by the function pd.rolling_mean()\n             If a string is passed, it must be a frequency string, e.g. '90S'. This is\n             internally converted into a DateOffset object, representing the window size.\n    min_periods : int\n                  Minimum number of observations in window required to have a value.\n\n    Returns\n    -------\n    Series or DataFrame, if more than one column    \n    '''\n    def f(x):\n        '''Function to apply that actually computes the rolling mean'''\n        if center == False:\n            dslice = col[x-pd.datetools.to_offset(window).delta+timedelta(0,0,1):x]\n                # adding a microsecond because when slicing with labels start and endpoint\n                # are inclusive\n        else:\n            dslice = col[x-pd.datetools.to_offset(window).delta/2+timedelta(0,0,1):\n                         x+pd.datetools.to_offset(window).delta/2]\n        if dslice.size < min_periods:\n            return np.nan\n        else:\n            return dslice.mean()\n\n    data = DataFrame(data.copy())\n    dfout = DataFrame()\n    if isinstance(window, int):\n        dfout = pd.rolling_mean(data, window, min_periods=min_periods, center=center)\n    elif isinstance(window, basestring):\n        idx = Series(data.index.to_pydatetime(), index=data.index)\n        for colname, col in data.iterkv():\n            result = idx.apply(f)\n            result.name = colname\n            dfout = dfout.join(result, how='outer')\n    if dfout.columns.size == 1:\n        dfout = dfout.ix[:,0]\n    return dfout\n\n\n# Example\nidx = [datetime(2011, 2, 7, 0, 0),\n       datetime(2011, 2, 7, 0, 1),\n       datetime(2011, 2, 7, 0, 1, 30),\n       datetime(2011, 2, 7, 0, 2),\n       datetime(2011, 2, 7, 0, 4),\n       datetime(2011, 2, 7, 0, 5),\n       datetime(2011, 2, 7, 0, 5, 10),\n       datetime(2011, 2, 7, 0, 6),\n       datetime(2011, 2, 7, 0, 8),\n       datetime(2011, 2, 7, 0, 9)]\nidx = pd.Index(idx)\nvals = np.arange(len(idx)).astype(float)\ns = Series(vals, index=idx)\nrm = rolling_mean(s, window='2min')\n""]";"[""from pandas import Series, DataFrame\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport numpy as np\n\ndef rolling_mean(data, window, min_periods=1, center=False):\n    ''' Function that computes a rolling mean\n\n    Parameters\n    ----------\n    data : DataFrame or Series\n           If a DataFrame is passed, the rolling_mean is computed for all columns.\n    window : int or string\n             If int is passed, window is the number of observations used for calculating \n             the statistic, as defined by the function pd.rolling_mean()\n             If a string is passed, it must be a frequency string, e.g. '90S'. This is\n             internally converted into a DateOffset object, representing the window size.\n    min_periods : int\n                  Minimum number of observations in window required to have a value.\n\n    Returns\n    -------\n    Series or DataFrame, if more than one column    \n    '''\n    def f(x):\n        '''Function to apply that actually computes the rolling mean'''\n        if center == False:\n            dslice = col[x-pd.datetools.to_offset(window).delta+timedelta(0,0,1):x]\n                # adding a microsecond because when slicing with labels start and endpoint\n                # are inclusive\n        else:\n            dslice = col[x-pd.datetools.to_offset(window).delta/2+timedelta(0,0,1):\n                         x+pd.datetools.to_offset(window).delta/2]\n        if dslice.size < min_periods:\n            return np.nan\n        else:\n            return dslice.mean()\n\n    data = DataFrame(data.copy())\n    dfout = DataFrame()\n    if isinstance(window, int):\n        dfout = pd.rolling_mean(data, window, min_periods=min_periods, center=center)\n    elif isinstance(window, basestring):\n        idx = Series(data.index.to_pydatetime(), index=data.index)\n        for colname, col in data.iterkv():\n            result = idx.apply(f)\n            result.name = colname\n            dfout = dfout.join(result, how='outer')\n    if dfout.columns.size == 1:\n        dfout = dfout.ix[:,0]\n    return dfout\n\n\n# Example\nidx = [datetime(2011, 2, 7, 0, 0),\n       datetime(2011, 2, 7, 0, 1),\n       datetime(2011, 2, 7, 0, 1, 30),\n       datetime(2011, 2, 7, 0, 2),\n       datetime(2011, 2, 7, 0, 4),\n       datetime(2011, 2, 7, 0, 5),\n       datetime(2011, 2, 7, 0, 5, 10),\n       datetime(2011, 2, 7, 0, 6),\n       datetime(2011, 2, 7, 0, 8),\n       datetime(2011, 2, 7, 0, 9)]\nidx = pd.Index(idx)\nvals = np.arange(len(idx)).astype(float)\ns = Series(vals, index=idx)\nrm = rolling_mean(s, window='2min')\n""]"
878;;2;18500854;10591000.0;2;22;;;"<p>Since Pandas 0.11.0 you can use dtype argument to explicitly specify data type for each column:</p>

<pre><code>d = pandas.read_csv('foo.csv', dtype={'BAR': 'S10'})
</code></pre>
";;"[""d = pandas.read_csv('foo.csv', dtype={'BAR': 'S10'})\n""]";"[""d = pandas.read_csv('foo.csv', dtype={'BAR': 'S10'})\n""]"
879;;3;18505101;18504967.0;2;32;;;"<p>You can do this easily manually for each column like this:</p>

<pre><code>df['A_perc'] = df['A']/df['sum']
</code></pre>

<hr>

<p>If you want to do this in one step for all columns, you can use the <code>div</code> method (<a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#matching-broadcasting-behavior"">http://pandas.pydata.org/pandas-docs/stable/basics.html#matching-broadcasting-behavior</a>):</p>

<pre><code>ds.div(ds['sum'], axis=0)
</code></pre>

<p>And if you want this in one step added to the same dataframe:</p>

<pre><code>&gt;&gt;&gt; ds.join(ds.div(ds['sum'], axis=0), rsuffix='_perc')
          A         B         C         D       sum    A_perc    B_perc  \
1  0.151722  0.935917  1.033526  0.941962  3.063127  0.049532  0.305543   
2  0.033761  1.087302  1.110695  1.401260  3.633017  0.009293  0.299283   
3  0.761368  0.484268  0.026837  1.276130  2.548603  0.298739  0.190013   

     C_perc    D_perc  sum_perc  
1  0.337409  0.307517         1  
2  0.305722  0.385701         1  
3  0.010530  0.500718         1  
</code></pre>
";;"[""df['A_perc'] = df['A']/df['sum']\n"", ""ds.div(ds['sum'], axis=0)\n"", "">>> ds.join(ds.div(ds['sum'], axis=0), rsuffix='_perc')\n          A         B         C         D       sum    A_perc    B_perc  \\\n1  0.151722  0.935917  1.033526  0.941962  3.063127  0.049532  0.305543   \n2  0.033761  1.087302  1.110695  1.401260  3.633017  0.009293  0.299283   \n3  0.761368  0.484268  0.026837  1.276130  2.548603  0.298739  0.190013   \n\n     C_perc    D_perc  sum_perc  \n1  0.337409  0.307517         1  \n2  0.305722  0.385701         1  \n3  0.010530  0.500718         1  \n""]";"[""df['A_perc'] = df['A']/df['sum']\n"", 'div', ""ds.div(ds['sum'], axis=0)\n"", "">>> ds.join(ds.div(ds['sum'], axis=0), rsuffix='_perc')\n          A         B         C         D       sum    A_perc    B_perc  \\\n1  0.151722  0.935917  1.033526  0.941962  3.063127  0.049532  0.305543   \n2  0.033761  1.087302  1.110695  1.401260  3.633017  0.009293  0.299283   \n3  0.761368  0.484268  0.026837  1.276130  2.548603  0.298739  0.190013   \n\n     C_perc    D_perc  sum_perc  \n1  0.337409  0.307517         1  \n2  0.305722  0.385701         1  \n3  0.010530  0.500718         1  \n""]"
880;;3;18527067;11615504.0;2;9;;;"<p>I am doing this all the time, so I tested different ways for speed.
The fastest I found is the following, approx. 3 times faster than Chang She's solution, at least in my case, when taking the total time of file parsing and date parsing into account:</p>

<p>First, parse the data file using pd.read_csv withOUT parsing dates. I find that it is slowing down the file-reading quite a lot. Make sure that the columns of the CSV file are now columns in the dataframe df. Then:</p>

<pre><code>format = ""%Y%m%d %H""
times = pd.to_datetime(df.YYYYMMDD + ' ' + df.HH, format=format)
df.set_index(times, inplace=True)
# and maybe for cleanup
df = df.drop(['YYYYMMDD','HH'], axis=1)
</code></pre>
";;"['format = ""%Y%m%d %H""\ntimes = pd.to_datetime(df.YYYYMMDD + \' \' + df.HH, format=format)\ndf.set_index(times, inplace=True)\n# and maybe for cleanup\ndf = df.drop([\'YYYYMMDD\',\'HH\'], axis=1)\n']";"['format = ""%Y%m%d %H""\ntimes = pd.to_datetime(df.YYYYMMDD + \' \' + df.HH, format=format)\ndf.set_index(times, inplace=True)\n# and maybe for cleanup\ndf = df.drop([\'YYYYMMDD\',\'HH\'], axis=1)\n']"
881;;1;18554949;18554920.0;2;69;;;"<p>How about either of:</p>

<pre><code>&gt;&gt;&gt; df
         date  duration user_id
0  2013-04-01        30    0001
1  2013-04-01        15    0001
2  2013-04-01        20    0002
3  2013-04-02        15    0002
4  2013-04-02        30    0002
&gt;&gt;&gt; df.groupby(""date"").agg({""duration"": np.sum, ""user_id"": pd.Series.nunique})
            duration  user_id
date                         
2013-04-01        65        2
2013-04-02        45        1
&gt;&gt;&gt; df.groupby(""date"").agg({""duration"": np.sum, ""user_id"": lambda x: x.nunique()})
            duration  user_id
date                         
2013-04-01        65        2
2013-04-02        45        1
</code></pre>
";;"['>>> df\n         date  duration user_id\n0  2013-04-01        30    0001\n1  2013-04-01        15    0001\n2  2013-04-01        20    0002\n3  2013-04-02        15    0002\n4  2013-04-02        30    0002\n>>> df.groupby(""date"").agg({""duration"": np.sum, ""user_id"": pd.Series.nunique})\n            duration  user_id\ndate                         \n2013-04-01        65        2\n2013-04-02        45        1\n>>> df.groupby(""date"").agg({""duration"": np.sum, ""user_id"": lambda x: x.nunique()})\n            duration  user_id\ndate                         \n2013-04-01        65        2\n2013-04-02        45        1\n']";"['>>> df\n         date  duration user_id\n0  2013-04-01        30    0001\n1  2013-04-01        15    0001\n2  2013-04-01        20    0002\n3  2013-04-02        15    0002\n4  2013-04-02        30    0002\n>>> df.groupby(""date"").agg({""duration"": np.sum, ""user_id"": pd.Series.nunique})\n            duration  user_id\ndate                         \n2013-04-01        65        2\n2013-04-02        45        1\n>>> df.groupby(""date"").agg({""duration"": np.sum, ""user_id"": lambda x: x.nunique()})\n            duration  user_id\ndate                         \n2013-04-01        65        2\n2013-04-02        45        1\n']"
882;;0;18594595;18594469.0;2;60;;;"<p>To overcome the broadcasting issue, you can use the <code>div</code> method:</p>

<pre><code>df.div(df.sum(axis=1), axis=0)
</code></pre>

<p>See <a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#matching-broadcasting-behavior"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/basics.html#matching-broadcasting-behavior</a></p>
";;['df.div(df.sum(axis=1), axis=0)\n'];['div', 'df.div(df.sum(axis=1), axis=0)\n']
883;;1;18674915;18674064.0;2;117;;;"<p>see docs: <a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion"">http://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion</a></p>

<p>using idx = 0 will insert at the beginning</p>

<pre><code>df.insert(idx, col_name, value)
</code></pre>
";;['df.insert(idx, col_name, value)\n'];['df.insert(idx, col_name, value)\n']
884;;4;18689589;18689512.0;2;10;;;"<p>Is your type really arbitrary?  If you know it is just going to be a int float or string you could just do</p>

<pre><code> if val.dtype == float and np.isnan(val):
</code></pre>

<p>assuming it is wrapped in numpy , it will always have a dtype and only float and complex can be NaN</p>
";;[' if val.dtype == float and np.isnan(val):\n'];[' if val.dtype == float and np.isnan(val):\n']
885;;0;18689712;18689512.0;2;100;;;"<p><code>pandas.isnull()</code> checks for missing values in both numeric and string/object arrays. From the documentation, it checks for:</p>

<blockquote>
  <p>NaN in numeric arrays, None/NaN in object arrays</p>
</blockquote>

<p>Quick example:</p>

<pre><code>import pandas as pd
import numpy as np
s = pd.Series(['apple', np.nan, 'banana'])
pd.isnull(s)
Out[9]: 
0    False
1     True
2    False
dtype: bool
</code></pre>

<p>The idea of using <code>numpy.nan</code> to represent missing values is something that <code>pandas</code> introduced, which is why <code>pandas</code> has the tools to deal with it.</p>

<p>Datetimes too (if you use <code>pd.NaT</code> you won't need to specify the dtype)</p>

<pre><code>In [24]: s = Series([Timestamp('20130101'),np.nan,Timestamp('20130102 9:30')],dtype='M8[ns]')

In [25]: s
Out[25]: 
0   2013-01-01 00:00:00
1                   NaT
2   2013-01-02 09:30:00
dtype: datetime64[ns]``

In [26]: pd.isnull(s)
Out[26]: 
0    False
1     True
2    False
dtype: bool
</code></pre>
";;"[""import pandas as pd\nimport numpy as np\ns = pd.Series(['apple', np.nan, 'banana'])\npd.isnull(s)\nOut[9]: \n0    False\n1     True\n2    False\ndtype: bool\n"", ""In [24]: s = Series([Timestamp('20130101'),np.nan,Timestamp('20130102 9:30')],dtype='M8[ns]')\n\nIn [25]: s\nOut[25]: \n0   2013-01-01 00:00:00\n1                   NaT\n2   2013-01-02 09:30:00\ndtype: datetime64[ns]``\n\nIn [26]: pd.isnull(s)\nOut[26]: \n0    False\n1     True\n2    False\ndtype: bool\n""]";"['pandas.isnull()', ""import pandas as pd\nimport numpy as np\ns = pd.Series(['apple', np.nan, 'banana'])\npd.isnull(s)\nOut[9]: \n0    False\n1     True\n2    False\ndtype: bool\n"", 'numpy.nan', 'pandas', 'pandas', 'pd.NaT', ""In [24]: s = Series([Timestamp('20130101'),np.nan,Timestamp('20130102 9:30')],dtype='M8[ns]')\n\nIn [25]: s\nOut[25]: \n0   2013-01-01 00:00:00\n1                   NaT\n2   2013-01-02 09:30:00\ndtype: datetime64[ns]``\n\nIn [26]: pd.isnull(s)\nOut[26]: \n0    False\n1     True\n2    False\ndtype: bool\n""]"
886;;0;18691949;18689823.0;2;58;;;"<p>You can simply use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html#pandas.DataFrame.fillna"" rel=""noreferrer""><code>DataFrame.fillna</code></a> to fill the <code>nan</code>'s directly:</p>

<pre><code>In [27]: df 
Out[27]: 
          A         B         C
0 -0.166919  0.979728 -0.632955
1 -0.297953 -0.912674 -1.365463
2 -0.120211 -0.540679 -0.680481
3       NaN -2.027325  1.533582
4       NaN       NaN  0.461821
5 -0.788073       NaN       NaN
6 -0.916080 -0.612343       NaN
7 -0.887858  1.033826       NaN
8  1.948430  1.025011 -2.982224
9  0.019698 -0.795876 -0.046431

In [28]: df.mean()
Out[28]: 
A   -0.151121
B   -0.231291
C   -0.530307
dtype: float64

In [29]: df.fillna(df.mean())
Out[29]: 
          A         B         C
0 -0.166919  0.979728 -0.632955
1 -0.297953 -0.912674 -1.365463
2 -0.120211 -0.540679 -0.680481
3 -0.151121 -2.027325  1.533582
4 -0.151121 -0.231291  0.461821
5 -0.788073 -0.231291 -0.530307
6 -0.916080 -0.612343 -0.530307
7 -0.887858  1.033826 -0.530307
8  1.948430  1.025011 -2.982224
9  0.019698 -0.795876 -0.046431
</code></pre>

<p>The docstring of <code>fillna</code> says, that <code>value</code> should be a scalar or a dict, however it seems to work with a <code>Series</code> to. If you want to pass a dict you could use <code>df.mean().to_dict()</code>.</p>
";;['In [27]: df \nOut[27]: \n          A         B         C\n0 -0.166919  0.979728 -0.632955\n1 -0.297953 -0.912674 -1.365463\n2 -0.120211 -0.540679 -0.680481\n3       NaN -2.027325  1.533582\n4       NaN       NaN  0.461821\n5 -0.788073       NaN       NaN\n6 -0.916080 -0.612343       NaN\n7 -0.887858  1.033826       NaN\n8  1.948430  1.025011 -2.982224\n9  0.019698 -0.795876 -0.046431\n\nIn [28]: df.mean()\nOut[28]: \nA   -0.151121\nB   -0.231291\nC   -0.530307\ndtype: float64\n\nIn [29]: df.fillna(df.mean())\nOut[29]: \n          A         B         C\n0 -0.166919  0.979728 -0.632955\n1 -0.297953 -0.912674 -1.365463\n2 -0.120211 -0.540679 -0.680481\n3 -0.151121 -2.027325  1.533582\n4 -0.151121 -0.231291  0.461821\n5 -0.788073 -0.231291 -0.530307\n6 -0.916080 -0.612343 -0.530307\n7 -0.887858  1.033826 -0.530307\n8  1.948430  1.025011 -2.982224\n9  0.019698 -0.795876 -0.046431\n'];['DataFrame.fillna', 'nan', 'In [27]: df \nOut[27]: \n          A         B         C\n0 -0.166919  0.979728 -0.632955\n1 -0.297953 -0.912674 -1.365463\n2 -0.120211 -0.540679 -0.680481\n3       NaN -2.027325  1.533582\n4       NaN       NaN  0.461821\n5 -0.788073       NaN       NaN\n6 -0.916080 -0.612343       NaN\n7 -0.887858  1.033826       NaN\n8  1.948430  1.025011 -2.982224\n9  0.019698 -0.795876 -0.046431\n\nIn [28]: df.mean()\nOut[28]: \nA   -0.151121\nB   -0.231291\nC   -0.530307\ndtype: float64\n\nIn [29]: df.fillna(df.mean())\nOut[29]: \n          A         B         C\n0 -0.166919  0.979728 -0.632955\n1 -0.297953 -0.912674 -1.365463\n2 -0.120211 -0.540679 -0.680481\n3 -0.151121 -2.027325  1.533582\n4 -0.151121 -0.231291  0.461821\n5 -0.788073 -0.231291 -0.530307\n6 -0.916080 -0.612343 -0.530307\n7 -0.887858  1.033826 -0.530307\n8  1.948430  1.025011 -2.982224\n9  0.019698 -0.795876 -0.046431\n', 'fillna', 'value', 'Series', 'df.mean().to_dict()']
887;;2;18695700;18695605.0;2;74;;;"<p>See the docs for <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_dict.html"" rel=""noreferrer""><code>to_dict</code></a>. You can use it like this:</p>

<pre><code>df.set_index('id').to_dict()
</code></pre>

<p>And if you have only one column, to avoid the column name is also a level in the dict (actually, in this case you use the <code>Series.to_dict()</code>):</p>

<pre><code>df.set_index('id')['value'].to_dict()
</code></pre>
";;"[""df.set_index('id').to_dict()\n"", ""df.set_index('id')['value'].to_dict()\n""]";"['to_dict', ""df.set_index('id').to_dict()\n"", 'Series.to_dict()', ""df.set_index('id')['value'].to_dict()\n""]"
888;;3;18793067;18792918.0;2;15;;;"<p>Joining fails if the DataFrames have some column names in common. The simplest way around it is to include an <code>lsuffix</code> or <code>rsuffix</code> keyword like so:</p>

<pre><code>restaurant_review_frame.join(restaurant_ids_dataframe, on='business_id', how='left', lsuffix=""_review"")
</code></pre>

<p>This way, the columns have distinct names. The documentation <a href=""http://pandas.pydata.org/pandas-docs/dev/merging.html#overlapping-value-columns"">addresses this very problem</a>.</p>

<p>Or, you could get around this by simply deleting the offending columns before you join. If, for example, the stars in <code>restaurant_ids_dataframe</code> are redundant to the stars in <code>restaurant_review_frame</code>, you could <code>del restaurant_ids_dataframe['stars']</code>.</p>
";;"['restaurant_review_frame.join(restaurant_ids_dataframe, on=\'business_id\', how=\'left\', lsuffix=""_review"")\n']";"['lsuffix', 'rsuffix', 'restaurant_review_frame.join(restaurant_ids_dataframe, on=\'business_id\', how=\'left\', lsuffix=""_review"")\n', 'restaurant_ids_dataframe', 'restaurant_review_frame', ""del restaurant_ids_dataframe['stars']""]"
889;;2;18799713;18792918.0;2;50;;;"<p>You can use <strong>merge</strong> to combine two dataframes into one:</p>

<pre><code>import pandas as pd
pd.merge(restaurant_ids_dataframe, restaurant_review_frame, on='business_id', how='outer')
</code></pre>

<p>where <strong>on</strong> specifies field name that exists in both dataframes to join on, and <strong>how</strong>
defines whether its inner/outer/left/right join, with outer using 'union of keys from both frames (SQL: full outer join).' Since you have 'star' column in both dataframes, this by default will create two columns star_x and star_y in the combined dataframe. As @DanAllan mentioned for the join method, you can modify the suffixes for merge by passing it as a kwarg. Default is <code>suffixes=('_x', '_y')</code>. if you wanted to do something like <code>star_restaurant_id</code> and <code>star_restaurant_review</code>, you can do:</p>

<pre><code> pd.merge(restaurant_ids_dataframe, restaurant_review_frame, on='business_id', how='outer', suffixes=('_restaurant_id', '_restaurant_review'))
</code></pre>

<p>The parameters are explained in detail in this <a href=""http://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging"">link</a>. </p>
";;"[""import pandas as pd\npd.merge(restaurant_ids_dataframe, restaurant_review_frame, on='business_id', how='outer')\n"", "" pd.merge(restaurant_ids_dataframe, restaurant_review_frame, on='business_id', how='outer', suffixes=('_restaurant_id', '_restaurant_review'))\n""]";"[""import pandas as pd\npd.merge(restaurant_ids_dataframe, restaurant_review_frame, on='business_id', how='outer')\n"", ""suffixes=('_x', '_y')"", 'star_restaurant_id', 'star_restaurant_review', "" pd.merge(restaurant_ids_dataframe, restaurant_review_frame, on='business_id', how='outer', suffixes=('_restaurant_id', '_restaurant_review'))\n""]"
890;;0;18835121;18835077.0;2;43;;;"<p>One way is to use the <code>get_level_values</code> Index method:</p>

<pre><code>In [11]: df
Out[11]:
     0
A B
1 4  1
2 5  2
3 6  3

In [12]: df.iloc[df.index.get_level_values('A') == 1]
Out[12]:
     0
A B
1 4  1
</code></pre>

<p>In 0.13 you'll be able to use <a href=""https://github.com/pydata/pandas/pull/4180""><code>xs</code> with <code>drop_level</code> argument</a>:</p>

<pre><code>df.xs(1, level='A', drop_level=False) # axis=1 if columns
</code></pre>

<p>Note: if this were column MultiIndex rather than index, you could use the same technique:</p>

<pre><code>In [21]: df1 = df.T

In [22]: df1.iloc[:, df1.columns.get_level_values('A') == 1]
Out[22]:
A  1
B  4
0  1
</code></pre>
";;"[""In [11]: df\nOut[11]:\n     0\nA B\n1 4  1\n2 5  2\n3 6  3\n\nIn [12]: df.iloc[df.index.get_level_values('A') == 1]\nOut[12]:\n     0\nA B\n1 4  1\n"", ""df.xs(1, level='A', drop_level=False) # axis=1 if columns\n"", ""In [21]: df1 = df.T\n\nIn [22]: df1.iloc[:, df1.columns.get_level_values('A') == 1]\nOut[22]:\nA  1\nB  4\n0  1\n""]";"['get_level_values', ""In [11]: df\nOut[11]:\n     0\nA B\n1 4  1\n2 5  2\n3 6  3\n\nIn [12]: df.iloc[df.index.get_level_values('A') == 1]\nOut[12]:\n     0\nA B\n1 4  1\n"", 'xs', 'drop_level', ""df.xs(1, level='A', drop_level=False) # axis=1 if columns\n"", ""In [21]: df1 = df.T\n\nIn [22]: df1.iloc[:, df1.columns.get_level_values('A') == 1]\nOut[22]:\nA  1\nB  4\n0  1\n""]"
891;;3;18835174;18835077.0;2;15;;;"<p>You can use <code>DataFrame.xs()</code>:</p>

<pre><code>In [36]: df = DataFrame(np.random.randn(10, 4))

In [37]: df.columns = [np.random.choice(['a', 'b'], size=4).tolist(), np.random.choice(['c', 'd'], size=4)]

In [38]: df.columns.names = ['A', 'B']

In [39]: df
Out[39]:
A      b             a
B      d      d      d      d
0 -1.406  0.548 -0.635  0.576
1 -0.212 -0.583  1.012 -1.377
2  0.951 -0.349 -0.477 -1.230
3  0.451 -0.168  0.949  0.545
4 -0.362 -0.855  1.676 -2.881
5  1.283  1.027  0.085 -1.282
6  0.583 -1.406  0.327 -0.146
7 -0.518 -0.480  0.139  0.851
8 -0.030 -0.630 -1.534  0.534
9  0.246 -1.558 -1.885 -1.543

In [40]: df.xs('a', level='A', axis=1)
Out[40]:
B      d      d
0 -0.635  0.576
1  1.012 -1.377
2 -0.477 -1.230
3  0.949  0.545
4  1.676 -2.881
5  0.085 -1.282
6  0.327 -0.146
7  0.139  0.851
8 -1.534  0.534
9 -1.885 -1.543
</code></pre>

<p>If you want to keep the <code>A</code> level (the <code>drop_level</code> keyword argument is only available in the to-be-released v0.13.0):</p>

<pre><code>In [42]: df.xs('a', level='A', axis=1, drop_level=False)
Out[42]:
A      a
B      d      d
0 -0.635  0.576
1  1.012 -1.377
2 -0.477 -1.230
3  0.949  0.545
4  1.676 -2.881
5  0.085 -1.282
6  0.327 -0.146
7  0.139  0.851
8 -1.534  0.534
9 -1.885 -1.543
</code></pre>
";;"[""In [36]: df = DataFrame(np.random.randn(10, 4))\n\nIn [37]: df.columns = [np.random.choice(['a', 'b'], size=4).tolist(), np.random.choice(['c', 'd'], size=4)]\n\nIn [38]: df.columns.names = ['A', 'B']\n\nIn [39]: df\nOut[39]:\nA      b             a\nB      d      d      d      d\n0 -1.406  0.548 -0.635  0.576\n1 -0.212 -0.583  1.012 -1.377\n2  0.951 -0.349 -0.477 -1.230\n3  0.451 -0.168  0.949  0.545\n4 -0.362 -0.855  1.676 -2.881\n5  1.283  1.027  0.085 -1.282\n6  0.583 -1.406  0.327 -0.146\n7 -0.518 -0.480  0.139  0.851\n8 -0.030 -0.630 -1.534  0.534\n9  0.246 -1.558 -1.885 -1.543\n\nIn [40]: df.xs('a', level='A', axis=1)\nOut[40]:\nB      d      d\n0 -0.635  0.576\n1  1.012 -1.377\n2 -0.477 -1.230\n3  0.949  0.545\n4  1.676 -2.881\n5  0.085 -1.282\n6  0.327 -0.146\n7  0.139  0.851\n8 -1.534  0.534\n9 -1.885 -1.543\n"", ""In [42]: df.xs('a', level='A', axis=1, drop_level=False)\nOut[42]:\nA      a\nB      d      d\n0 -0.635  0.576\n1  1.012 -1.377\n2 -0.477 -1.230\n3  0.949  0.545\n4  1.676 -2.881\n5  0.085 -1.282\n6  0.327 -0.146\n7  0.139  0.851\n8 -1.534  0.534\n9 -1.885 -1.543\n""]";"['DataFrame.xs()', ""In [36]: df = DataFrame(np.random.randn(10, 4))\n\nIn [37]: df.columns = [np.random.choice(['a', 'b'], size=4).tolist(), np.random.choice(['c', 'd'], size=4)]\n\nIn [38]: df.columns.names = ['A', 'B']\n\nIn [39]: df\nOut[39]:\nA      b             a\nB      d      d      d      d\n0 -1.406  0.548 -0.635  0.576\n1 -0.212 -0.583  1.012 -1.377\n2  0.951 -0.349 -0.477 -1.230\n3  0.451 -0.168  0.949  0.545\n4 -0.362 -0.855  1.676 -2.881\n5  1.283  1.027  0.085 -1.282\n6  0.583 -1.406  0.327 -0.146\n7 -0.518 -0.480  0.139  0.851\n8 -0.030 -0.630 -1.534  0.534\n9  0.246 -1.558 -1.885 -1.543\n\nIn [40]: df.xs('a', level='A', axis=1)\nOut[40]:\nB      d      d\n0 -0.635  0.576\n1  1.012 -1.377\n2 -0.477 -1.230\n3  0.949  0.545\n4  1.676 -2.881\n5  0.085 -1.282\n6  0.327 -0.146\n7  0.139  0.851\n8 -1.534  0.534\n9 -1.885 -1.543\n"", 'A', 'drop_level', ""In [42]: df.xs('a', level='A', axis=1, drop_level=False)\nOut[42]:\nA      a\nB      d      d\n0 -0.635  0.576\n1  1.012 -1.377\n2 -0.477 -1.230\n3  0.949  0.545\n4  1.676 -2.881\n5  0.085 -1.282\n6  0.327 -0.146\n7  0.139  0.851\n8 -1.534  0.534\n9 -1.885 -1.543\n""]"
892;;2;18837378;18837262.0;2;16;;;"<p>Pass the items of the dictionary to the DataFrame constructor, and give the column names. After that parse the <code>Date</code> column to get <code>Timestamp</code> values.</p>

<pre><code>df = pd.DataFrame(data.items(), columns=['Date', 'DateValue'])
df['Date'] = pd.to_datetime(df['Date'])
</code></pre>
";;"[""df = pd.DataFrame(data.items(), columns=['Date', 'DateValue'])\ndf['Date'] = pd.to_datetime(df['Date'])\n""]";"['Date', 'Timestamp', ""df = pd.DataFrame(data.items(), columns=['Date', 'DateValue'])\ndf['Date'] = pd.to_datetime(df['Date'])\n""]"
893;;5;18837389;18837262.0;2;145;;;"<p>The error here, is since calling the DataFrame constructor with scalar values (where it expects values to be a list/dict/... i.e. have multiple columns):</p>

<pre><code>pd.DataFrame(d)
ValueError: If using all scalar values, you must must pass an index
</code></pre>

<p>You could take the items from the dictionary (i.e. the key-value pairs):</p>

<pre><code>In [11]: pd.DataFrame(d.items())  # or list(d.items()) in python 3
Out[11]:
             0    1
0   2012-07-02  392
1   2012-07-06  392
2   2012-06-29  391
3   2012-06-28  391
...

In [12]: pd.DataFrame(d.items(), columns=['Date', 'DateValue'])
Out[12]:
          Date  DateValue
0   2012-07-02        392
1   2012-07-06        392
2   2012-06-29        391
</code></pre>

<p>But I think it makes more sense to pass the Series constructor:</p>

<pre><code>In [21]: s = pd.Series(d, name='DateValue')
Out[21]:
2012-06-08    388
2012-06-09    388
2012-06-10    388

In [22]: s.index.name = 'Date'

In [23]: s.reset_index()
Out[23]:
          Date  DateValue
0   2012-06-08        388
1   2012-06-09        388
2   2012-06-10        388
</code></pre>
";;"['pd.DataFrame(d)\nValueError: If using all scalar values, you must must pass an index\n', ""In [11]: pd.DataFrame(d.items())  # or list(d.items()) in python 3\nOut[11]:\n             0    1\n0   2012-07-02  392\n1   2012-07-06  392\n2   2012-06-29  391\n3   2012-06-28  391\n...\n\nIn [12]: pd.DataFrame(d.items(), columns=['Date', 'DateValue'])\nOut[12]:\n          Date  DateValue\n0   2012-07-02        392\n1   2012-07-06        392\n2   2012-06-29        391\n"", ""In [21]: s = pd.Series(d, name='DateValue')\nOut[21]:\n2012-06-08    388\n2012-06-09    388\n2012-06-10    388\n\nIn [22]: s.index.name = 'Date'\n\nIn [23]: s.reset_index()\nOut[23]:\n          Date  DateValue\n0   2012-06-08        388\n1   2012-06-09        388\n2   2012-06-10        388\n""]";"['pd.DataFrame(d)\nValueError: If using all scalar values, you must must pass an index\n', ""In [11]: pd.DataFrame(d.items())  # or list(d.items()) in python 3\nOut[11]:\n             0    1\n0   2012-07-02  392\n1   2012-07-06  392\n2   2012-06-29  391\n3   2012-06-28  391\n...\n\nIn [12]: pd.DataFrame(d.items(), columns=['Date', 'DateValue'])\nOut[12]:\n          Date  DateValue\n0   2012-07-02        392\n1   2012-07-06        392\n2   2012-06-29        391\n"", ""In [21]: s = pd.Series(d, name='DateValue')\nOut[21]:\n2012-06-08    388\n2012-06-09    388\n2012-06-10    388\n\nIn [22]: s.index.name = 'Date'\n\nIn [23]: s.reset_index()\nOut[23]:\n          Date  DateValue\n0   2012-06-08        388\n1   2012-06-09        388\n2   2012-06-10        388\n""]"
894;;3;18878267;18876022.0;2;23;;;"<p>HTML receives a custom string of html data. Nobody forbids you to pass in a style tag with the custom CSS style for the <code>.dataframe</code> class (which the <code>to_html</code> method adds to the table).</p>

<p>So the simplest solution would be to just add a style and concatenate it with the output of the <code>df.to_html</code>:</p>

<pre><code>style = '&lt;style&gt;.dataframe td { text-align: right; }&lt;/style&gt;'
HTML( style + df.to_html( formatters=frmt ) )
</code></pre>

<p>But I would suggest to define a custom class for a DataFrame since this will change the style of all the tables in your notebook (style is ""global"").</p>

<pre><code>style = '&lt;style&gt;.right_aligned_df td { text-align: right; }&lt;/style&gt;'
HTML(style + df.to_html(formatters=frmt, classes='right_aligned_df'))
</code></pre>

<p>You can also define the style in one of the previous cells, and then just set the <code>classes</code> parameter of the <code>to_html</code> method:</p>

<pre><code># Some cell at the begining of the notebook
In [2]: HTML('''&lt;style&gt;
                    .right_aligned_df td { text-align: right; }
                    .left_aligned_df td { text-align: right; }
                    .pink_df { background-color: pink; }
                &lt;/style&gt;''')

...

# Much later in your notebook
In [66]: HTML(df.to_html(classes='pink_df'))
</code></pre>
";;"[""style = '<style>.dataframe td { text-align: right; }</style>'\nHTML( style + df.to_html( formatters=frmt ) )\n"", ""style = '<style>.right_aligned_df td { text-align: right; }</style>'\nHTML(style + df.to_html(formatters=frmt, classes='right_aligned_df'))\n"", ""# Some cell at the begining of the notebook\nIn [2]: HTML('''<style>\n                    .right_aligned_df td { text-align: right; }\n                    .left_aligned_df td { text-align: right; }\n                    .pink_df { background-color: pink; }\n                </style>''')\n\n...\n\n# Much later in your notebook\nIn [66]: HTML(df.to_html(classes='pink_df'))\n""]";"['.dataframe', 'to_html', 'df.to_html', ""style = '<style>.dataframe td { text-align: right; }</style>'\nHTML( style + df.to_html( formatters=frmt ) )\n"", ""style = '<style>.right_aligned_df td { text-align: right; }</style>'\nHTML(style + df.to_html(formatters=frmt, classes='right_aligned_df'))\n"", 'classes', 'to_html', ""# Some cell at the begining of the notebook\nIn [2]: HTML('''<style>\n                    .right_aligned_df td { text-align: right; }\n                    .left_aligned_df td { text-align: right; }\n                    .pink_df { background-color: pink; }\n                </style>''')\n\n...\n\n# Much later in your notebook\nIn [66]: HTML(df.to_html(classes='pink_df'))\n""]"
895;;1;18885319;18885175.0;2;22;;;"<p>I think you want to <a href=""http://docs.python.org/2/library/zipfile#zipfile.ZipFile.open""><code>open</code></a> the ZipFile, which returns a file-like object,  rather than <a href=""http://docs.python.org/2/library/zipfile#zipfile.ZipFile.read""><code>read</code></a>:</p>

<pre><code>In [11]: crime2013 = pd.read_csv(z.open('crime_incidents_2013_CSV.csv'))

In [12]: crime2013
Out[12]:
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 24567 entries, 0 to 24566
Data columns (total 15 columns):
CCN                            24567  non-null values
REPORTDATETIME                 24567  non-null values
SHIFT                          24567  non-null values
OFFENSE                        24567  non-null values
METHOD                         24567  non-null values
LASTMODIFIEDDATE               24567  non-null values
BLOCKSITEADDRESS               24567  non-null values
BLOCKXCOORD                    24567  non-null values
BLOCKYCOORD                    24567  non-null values
WARD                           24563  non-null values
ANC                            24567  non-null values
DISTRICT                       24567  non-null values
PSA                            24567  non-null values
NEIGHBORHOODCLUSTER            24263  non-null values
BUSINESSIMPROVEMENTDISTRICT    3613  non-null values
dtypes: float64(4), int64(1), object(10)
</code></pre>
";;"[""In [11]: crime2013 = pd.read_csv(z.open('crime_incidents_2013_CSV.csv'))\n\nIn [12]: crime2013\nOut[12]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 24567 entries, 0 to 24566\nData columns (total 15 columns):\nCCN                            24567  non-null values\nREPORTDATETIME                 24567  non-null values\nSHIFT                          24567  non-null values\nOFFENSE                        24567  non-null values\nMETHOD                         24567  non-null values\nLASTMODIFIEDDATE               24567  non-null values\nBLOCKSITEADDRESS               24567  non-null values\nBLOCKXCOORD                    24567  non-null values\nBLOCKYCOORD                    24567  non-null values\nWARD                           24563  non-null values\nANC                            24567  non-null values\nDISTRICT                       24567  non-null values\nPSA                            24567  non-null values\nNEIGHBORHOODCLUSTER            24263  non-null values\nBUSINESSIMPROVEMENTDISTRICT    3613  non-null values\ndtypes: float64(4), int64(1), object(10)\n""]";"['open', 'read', ""In [11]: crime2013 = pd.read_csv(z.open('crime_incidents_2013_CSV.csv'))\n\nIn [12]: crime2013\nOut[12]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 24567 entries, 0 to 24566\nData columns (total 15 columns):\nCCN                            24567  non-null values\nREPORTDATETIME                 24567  non-null values\nSHIFT                          24567  non-null values\nOFFENSE                        24567  non-null values\nMETHOD                         24567  non-null values\nLASTMODIFIEDDATE               24567  non-null values\nBLOCKSITEADDRESS               24567  non-null values\nBLOCKXCOORD                    24567  non-null values\nBLOCKYCOORD                    24567  non-null values\nWARD                           24563  non-null values\nANC                            24567  non-null values\nDISTRICT                       24567  non-null values\nPSA                            24567  non-null values\nNEIGHBORHOODCLUSTER            24263  non-null values\nBUSINESSIMPROVEMENTDISTRICT    3613  non-null values\ndtypes: float64(4), int64(1), object(10)\n""]"
896;;4;18942558;18942506.0;2;55;;;"<p>You just do an opposite comparison. <code>if Col2 &lt;= 1</code>. This will return a boolean Series with <code>False</code> values for those greater than 1 and <code>True</code> values for the other. If you convert it to an <code>int64</code> dtype, <code>True</code> becomes 1 and <code>False</code> become <code>0</code>,</p>

<pre><code>df['Col3'] = (df['Col2'] &lt;= 1).astype(int)
</code></pre>

<p>If you want a more general solution, where you can assign any number to the <code>Col3</code> depending on the value of the <code>Col2</code> you should do something like:</p>

<pre><code>df['Col3'] = df['Col2'].map(lambda x: 42 if x &gt; 1 else 55)
</code></pre>

<p>Or:</p>

<pre><code>df['Col3'] = 0
condition = df['Col2'] &gt; 1
df.loc[condition, 'Col3'] = 42
df.loc[~condition, 'Col3'] = 55
</code></pre>
";;"[""df['Col3'] = (df['Col2'] <= 1).astype(int)\n"", ""df['Col3'] = df['Col2'].map(lambda x: 42 if x > 1 else 55)\n"", ""df['Col3'] = 0\ncondition = df['Col2'] > 1\ndf.loc[condition, 'Col3'] = 42\ndf.loc[~condition, 'Col3'] = 55\n""]";"['if Col2 <= 1', 'False', 'True', 'int64', 'True', 'False', '0', ""df['Col3'] = (df['Col2'] <= 1).astype(int)\n"", 'Col3', 'Col2', ""df['Col3'] = df['Col2'].map(lambda x: 42 if x > 1 else 55)\n"", ""df['Col3'] = 0\ncondition = df['Col2'] > 1\ndf.loc[condition, 'Col3'] = 42\ndf.loc[~condition, 'Col3'] = 55\n""]"
897;;6;18973430;18973404.0;2;49;;;"<p>Simple, just use <code>.set_color</code></p>

<pre><code>&gt;&gt;&gt; barlist=plt.bar([1,2,3,4], [1,2,3,4])
&gt;&gt;&gt; barlist[0].set_color('r')
&gt;&gt;&gt; plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/SYCFW.png"" alt=""enter image description here""></p>

<p>For your new question, not much harder either, just need to find the bar from your axis, an example:</p>

<pre><code>&gt;&gt;&gt; f=plt.figure()
&gt;&gt;&gt; ax=f.add_subplot(1,1,1)
&gt;&gt;&gt; ax.bar([1,2,3,4], [1,2,3,4])
&lt;Container object of 4 artists&gt;
&gt;&gt;&gt; ax.get_children()
[&lt;matplotlib.axis.XAxis object at 0x6529850&gt;, 
 &lt;matplotlib.axis.YAxis object at 0x78460d0&gt;,  
 &lt;matplotlib.patches.Rectangle object at 0x733cc50&gt;, 
 &lt;matplotlib.patches.Rectangle object at 0x733cdd0&gt;, 
 &lt;matplotlib.patches.Rectangle object at 0x777f290&gt;, 
 &lt;matplotlib.patches.Rectangle object at 0x777f710&gt;, 
 &lt;matplotlib.text.Text object at 0x7836450&gt;, 
 &lt;matplotlib.patches.Rectangle object at 0x7836390&gt;, 
 &lt;matplotlib.spines.Spine object at 0x6529950&gt;, 
 &lt;matplotlib.spines.Spine object at 0x69aef50&gt;,
 &lt;matplotlib.spines.Spine object at 0x69ae310&gt;, 
 &lt;matplotlib.spines.Spine object at 0x69aea50&gt;]
&gt;&gt;&gt; ax.get_children()[2].set_color('r') 
 #You can also try to locate the first patches.Rectangle object 
 #instead of direct calling the index.
</code></pre>

<p>If you have a complex plot and want to identify the bars first, add those:</p>

<pre><code>&gt;&gt;&gt; import matplotlib
&gt;&gt;&gt; childrenLS=ax.get_children()
&gt;&gt;&gt; barlist=filter(lambda x: isinstance(x, matplotlib.patches.Rectangle), childrenLS)
[&lt;matplotlib.patches.Rectangle object at 0x3103650&gt;, 
 &lt;matplotlib.patches.Rectangle object at 0x3103810&gt;, 
 &lt;matplotlib.patches.Rectangle object at 0x3129850&gt;, 
 &lt;matplotlib.patches.Rectangle object at 0x3129cd0&gt;, 
 &lt;matplotlib.patches.Rectangle object at 0x3112ad0&gt;]
</code></pre>
";;"["">>> barlist=plt.bar([1,2,3,4], [1,2,3,4])\n>>> barlist[0].set_color('r')\n>>> plt.show()\n"", "">>> f=plt.figure()\n>>> ax=f.add_subplot(1,1,1)\n>>> ax.bar([1,2,3,4], [1,2,3,4])\n<Container object of 4 artists>\n>>> ax.get_children()\n[<matplotlib.axis.XAxis object at 0x6529850>, \n <matplotlib.axis.YAxis object at 0x78460d0>,  \n <matplotlib.patches.Rectangle object at 0x733cc50>, \n <matplotlib.patches.Rectangle object at 0x733cdd0>, \n <matplotlib.patches.Rectangle object at 0x777f290>, \n <matplotlib.patches.Rectangle object at 0x777f710>, \n <matplotlib.text.Text object at 0x7836450>, \n <matplotlib.patches.Rectangle object at 0x7836390>, \n <matplotlib.spines.Spine object at 0x6529950>, \n <matplotlib.spines.Spine object at 0x69aef50>,\n <matplotlib.spines.Spine object at 0x69ae310>, \n <matplotlib.spines.Spine object at 0x69aea50>]\n>>> ax.get_children()[2].set_color('r') \n #You can also try to locate the first patches.Rectangle object \n #instead of direct calling the index.\n"", '>>> import matplotlib\n>>> childrenLS=ax.get_children()\n>>> barlist=filter(lambda x: isinstance(x, matplotlib.patches.Rectangle), childrenLS)\n[<matplotlib.patches.Rectangle object at 0x3103650>, \n <matplotlib.patches.Rectangle object at 0x3103810>, \n <matplotlib.patches.Rectangle object at 0x3129850>, \n <matplotlib.patches.Rectangle object at 0x3129cd0>, \n <matplotlib.patches.Rectangle object at 0x3112ad0>]\n']";"['.set_color', "">>> barlist=plt.bar([1,2,3,4], [1,2,3,4])\n>>> barlist[0].set_color('r')\n>>> plt.show()\n"", "">>> f=plt.figure()\n>>> ax=f.add_subplot(1,1,1)\n>>> ax.bar([1,2,3,4], [1,2,3,4])\n<Container object of 4 artists>\n>>> ax.get_children()\n[<matplotlib.axis.XAxis object at 0x6529850>, \n <matplotlib.axis.YAxis object at 0x78460d0>,  \n <matplotlib.patches.Rectangle object at 0x733cc50>, \n <matplotlib.patches.Rectangle object at 0x733cdd0>, \n <matplotlib.patches.Rectangle object at 0x777f290>, \n <matplotlib.patches.Rectangle object at 0x777f710>, \n <matplotlib.text.Text object at 0x7836450>, \n <matplotlib.patches.Rectangle object at 0x7836390>, \n <matplotlib.spines.Spine object at 0x6529950>, \n <matplotlib.spines.Spine object at 0x69aef50>,\n <matplotlib.spines.Spine object at 0x69ae310>, \n <matplotlib.spines.Spine object at 0x69aea50>]\n>>> ax.get_children()[2].set_color('r') \n #You can also try to locate the first patches.Rectangle object \n #instead of direct calling the index.\n"", '>>> import matplotlib\n>>> childrenLS=ax.get_children()\n>>> barlist=filter(lambda x: isinstance(x, matplotlib.patches.Rectangle), childrenLS)\n[<matplotlib.patches.Rectangle object at 0x3103650>, \n <matplotlib.patches.Rectangle object at 0x3103810>, \n <matplotlib.patches.Rectangle object at 0x3129850>, \n <matplotlib.patches.Rectangle object at 0x3129cd0>, \n <matplotlib.patches.Rectangle object at 0x3112ad0>]\n']"
898;;0;18975065;18973404.0;2;15;;;"<p>I assume you are using Series.plot() to plot your data.  If you look at the docs for Series.plot() here:</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.Series.plot.html"" rel=""nofollow noreferrer"">http://pandas.pydata.org/pandas-docs/dev/generated/pandas.Series.plot.html</a></p>

<p>there is no <em>color</em> parameter listed where you might be able to set the colors for your bar graph.</p>

<p>However, the Series.plot() docs state the following at the end of the parameter list: </p>

<pre><code>kwds : keywords
Options to pass to matplotlib plotting method
</code></pre>

<p>What that means is that when you specify the <em>kind</em> argument for Series.plot() as <em>bar</em>, Series.plot() will actually call matplotlib.pyplot.bar(), and matplotlib.pyplot.bar() will be sent all the extra keyword arguments that you specify at the end of the argument list for Series.plot().  </p>

<p>If you examine the docs for the matplotlib.pyplot.bar() method here:</p>

<p><a href=""http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.bar"" rel=""nofollow noreferrer"">http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.bar</a></p>

<p>..it also accepts keyword arguments at the end of it's parameter list, and if you peruse the list of recognized parameter names, one of them is <em>color</em>, which can be a sequence specifying the different colors for your bar graph.  </p>

<p>Putting it all together, if you specify the <em>color</em> keyword argument at the end of your Series.plot() argument list, the keyword argument will be relayed to the matplotlib.pyplot.bar() method.  Here is the proof:  </p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt

s = pd.Series(
    [5, 4, 4, 1, 12],
    index = [""AK"", ""AX"", ""GA"", ""SQ"", ""WN""]
)

#Set descriptions:
plt.title(""Total Delay Incident Caused by Carrier"")
plt.ylabel('Delay Incident')
plt.xlabel('Carrier')

#Set tick colors:
ax = plt.gca()
ax.tick_params(axis='x', colors='blue')
ax.tick_params(axis='y', colors='red')

#Plot the data:
my_colors = 'rgbkymc'  #red, green, blue, black, etc.

pd.Series.plot(
    s, 
    kind='bar', 
    color=my_colors,
)

plt.show()
</code></pre>

<hr>

<p><img src=""https://i.stack.imgur.com/aE3V0.png"" alt=""enter image description here""></p>

<p>Note that if there are more bars than colors in your sequence, the colors will repeat.</p>
";;"['kwds : keywords\nOptions to pass to matplotlib plotting method\n', 'import pandas as pd\nimport matplotlib.pyplot as plt\n\ns = pd.Series(\n    [5, 4, 4, 1, 12],\n    index = [""AK"", ""AX"", ""GA"", ""SQ"", ""WN""]\n)\n\n#Set descriptions:\nplt.title(""Total Delay Incident Caused by Carrier"")\nplt.ylabel(\'Delay Incident\')\nplt.xlabel(\'Carrier\')\n\n#Set tick colors:\nax = plt.gca()\nax.tick_params(axis=\'x\', colors=\'blue\')\nax.tick_params(axis=\'y\', colors=\'red\')\n\n#Plot the data:\nmy_colors = \'rgbkymc\'  #red, green, blue, black, etc.\n\npd.Series.plot(\n    s, \n    kind=\'bar\', \n    color=my_colors,\n)\n\nplt.show()\n']";"['kwds : keywords\nOptions to pass to matplotlib plotting method\n', 'import pandas as pd\nimport matplotlib.pyplot as plt\n\ns = pd.Series(\n    [5, 4, 4, 1, 12],\n    index = [""AK"", ""AX"", ""GA"", ""SQ"", ""WN""]\n)\n\n#Set descriptions:\nplt.title(""Total Delay Incident Caused by Carrier"")\nplt.ylabel(\'Delay Incident\')\nplt.xlabel(\'Carrier\')\n\n#Set tick colors:\nax = plt.gca()\nax.tick_params(axis=\'x\', colors=\'blue\')\nax.tick_params(axis=\'y\', colors=\'red\')\n\n#Plot the data:\nmy_colors = \'rgbkymc\'  #red, green, blue, black, etc.\n\npd.Series.plot(\n    s, \n    kind=\'bar\', \n    color=my_colors,\n)\n\nplt.show()\n']"
899;;1;18992172;18992086.0;2;69;;;"<p>Use the <code>Figure.savefig()</code> method, like so:</p>

<pre><code>ax = s.hist()  # s is an instance of Series
fig = ax.get_figure()
fig.savefig('/path/to/figure.pdf')
</code></pre>

<p>It doesn't have to end in <code>pdf</code>, there are many options. Check out <a href=""http://matplotlib.org/api/figure_api.html?highlight=savefig#matplotlib.figure.Figure.savefig"" rel=""noreferrer"">the documentation</a>.</p>

<p>Alternatively, you can use the <code>pyplot</code> interface and just call the <code>savefig</code> as a function to save the most recently created figure:</p>

<pre><code>s.hist()
savefig('path/to/figure.pdf')  # saves the current figure
</code></pre>
";;"[""ax = s.hist()  # s is an instance of Series\nfig = ax.get_figure()\nfig.savefig('/path/to/figure.pdf')\n"", ""s.hist()\nsavefig('path/to/figure.pdf')  # saves the current figure\n""]";"['Figure.savefig()', ""ax = s.hist()  # s is an instance of Series\nfig = ax.get_figure()\nfig.savefig('/path/to/figure.pdf')\n"", 'pdf', 'pyplot', 'savefig', ""s.hist()\nsavefig('path/to/figure.pdf')  # saves the current figure\n""]"
900;;1;19031661;12047193.0;2;25;;;"<p>If you are using SQLAlchemy's ORM rather than the expression language, you might find yourself wanting to convert an object of type <code>sqlalchemy.orm.query.Query</code> to a Pandas data frame. </p>

<p>Here is one way to do it, starting with a Query object called 'query':</p>

<pre><code>data_records = [rec.__dict__ for rec in query.all()]
df = pandas.DataFrame.from_records(data_records)
</code></pre>

<p>I'm curious to know if there's a better approach, but this did the trick for me in two lines.</p>
";;['data_records = [rec.__dict__ for rec in query.all()]\ndf = pandas.DataFrame.from_records(data_records)\n'];['sqlalchemy.orm.query.Query', 'data_records = [rec.__dict__ for rec in query.all()]\ndf = pandas.DataFrame.from_records(data_records)\n']
901;;3;19078773;19078325.0;2;39;;;"<p>This will drop the outermost level from the hierarchical column index:</p>

<pre><code>df = data.groupby(...).agg(...)
df.columns = df.columns.droplevel(0)
</code></pre>

<p>If you'd like to keep the outermost level, you can use the ravel() function on the multi-level column to form new labels:</p>

<pre><code>df.columns = [""_"".join(x) for x in df.columns.ravel()]
</code></pre>

<hr>

<p>For example:</p>

<pre><code>import pandas as pd
import pandas.rpy.common as com
import numpy as np

data = com.load_data('Loblolly')
print(data.head())
#     height  age Seed
# 1     4.51    3  301
# 15   10.89    5  301
# 29   28.72   10  301
# 43   41.74   15  301
# 57   52.70   20  301

df = data.groupby('Seed').agg(
    {'age':['sum'],
     'height':['mean', 'std']})
print(df.head())
#       age     height           
#       sum        std       mean
# Seed                           
# 301    78  22.638417  33.246667
# 303    78  23.499706  34.106667
# 305    78  23.927090  35.115000
# 307    78  22.222266  31.328333
# 309    78  23.132574  33.781667

df.columns = df.columns.droplevel(0)
print(df.head())
</code></pre>

<p>yields</p>

<pre><code>      sum        std       mean
Seed                           
301    78  22.638417  33.246667
303    78  23.499706  34.106667
305    78  23.927090  35.115000
307    78  22.222266  31.328333
309    78  23.132574  33.781667
</code></pre>

<p>Alternatively, to keep the first level of the index:</p>

<pre><code>df = data.groupby('Seed').agg(
    {'age':['sum'],
     'height':['mean', 'std']})
df.columns = [""_"".join(x) for x in df.columns.ravel()]
</code></pre>

<p>yields</p>

<pre><code>      age_sum   height_std  height_mean
Seed                           
301        78    22.638417    33.246667
303        78    23.499706    34.106667
305        78    23.927090    35.115000
307        78    22.222266    31.328333
309        78    23.132574    33.781667
</code></pre>
";;"['df = data.groupby(...).agg(...)\ndf.columns = df.columns.droplevel(0)\n', 'df.columns = [""_"".join(x) for x in df.columns.ravel()]\n', ""import pandas as pd\nimport pandas.rpy.common as com\nimport numpy as np\n\ndata = com.load_data('Loblolly')\nprint(data.head())\n#     height  age Seed\n# 1     4.51    3  301\n# 15   10.89    5  301\n# 29   28.72   10  301\n# 43   41.74   15  301\n# 57   52.70   20  301\n\ndf = data.groupby('Seed').agg(\n    {'age':['sum'],\n     'height':['mean', 'std']})\nprint(df.head())\n#       age     height           \n#       sum        std       mean\n# Seed                           \n# 301    78  22.638417  33.246667\n# 303    78  23.499706  34.106667\n# 305    78  23.927090  35.115000\n# 307    78  22.222266  31.328333\n# 309    78  23.132574  33.781667\n\ndf.columns = df.columns.droplevel(0)\nprint(df.head())\n"", '      sum        std       mean\nSeed                           \n301    78  22.638417  33.246667\n303    78  23.499706  34.106667\n305    78  23.927090  35.115000\n307    78  22.222266  31.328333\n309    78  23.132574  33.781667\n', 'df = data.groupby(\'Seed\').agg(\n    {\'age\':[\'sum\'],\n     \'height\':[\'mean\', \'std\']})\ndf.columns = [""_"".join(x) for x in df.columns.ravel()]\n', '      age_sum   height_std  height_mean\nSeed                           \n301        78    22.638417    33.246667\n303        78    23.499706    34.106667\n305        78    23.927090    35.115000\n307        78    22.222266    31.328333\n309        78    23.132574    33.781667\n']";"['df = data.groupby(...).agg(...)\ndf.columns = df.columns.droplevel(0)\n', 'df.columns = [""_"".join(x) for x in df.columns.ravel()]\n', ""import pandas as pd\nimport pandas.rpy.common as com\nimport numpy as np\n\ndata = com.load_data('Loblolly')\nprint(data.head())\n#     height  age Seed\n# 1     4.51    3  301\n# 15   10.89    5  301\n# 29   28.72   10  301\n# 43   41.74   15  301\n# 57   52.70   20  301\n\ndf = data.groupby('Seed').agg(\n    {'age':['sum'],\n     'height':['mean', 'std']})\nprint(df.head())\n#       age     height           \n#       sum        std       mean\n# Seed                           \n# 301    78  22.638417  33.246667\n# 303    78  23.499706  34.106667\n# 305    78  23.927090  35.115000\n# 307    78  22.222266  31.328333\n# 309    78  23.132574  33.781667\n\ndf.columns = df.columns.droplevel(0)\nprint(df.head())\n"", '      sum        std       mean\nSeed                           \n301    78  22.638417  33.246667\n303    78  23.499706  34.106667\n305    78  23.927090  35.115000\n307    78  22.222266  31.328333\n309    78  23.132574  33.781667\n', 'df = data.groupby(\'Seed\').agg(\n    {\'age\':[\'sum\'],\n     \'height\':[\'mean\', \'std\']})\ndf.columns = [""_"".join(x) for x in df.columns.ravel()]\n', '      age_sum   height_std  height_mean\nSeed                           \n301        78    22.638417    33.246667\n303        78    23.499706    34.106667\n305        78    23.927090    35.115000\n307        78    22.222266    31.328333\n309        78    23.132574    33.781667\n']"
902;;0;19112890;19112398.0;2;113;;;"<p>The following gives what you want:</p>

<pre><code>df = DataFrame(table, columns=headers)
df
</code></pre>

<p>outputs</p>

<pre><code>Out[7]:
   Heading1  Heading2
0         1         2
1         3         4
</code></pre>
";;['df = DataFrame(table, columns=headers)\ndf\n', 'Out[7]:\n   Heading1  Heading2\n0         1         2\n1         3         4\n'];['df = DataFrame(table, columns=headers)\ndf\n', 'Out[7]:\n   Heading1  Heading2\n0         1         2\n1         3         4\n']
903;;7;19125531;19125091.0;2;33;;;"<p>You can work out the columns that are only in one dataframe and use this to select a subset of columns in the merge</p>

<pre><code>cols_to_use = df2.columns - df.columns
</code></pre>

<p>then perform the merge using this (note this is an index object but it has a handy <code>tolist()</code> method)</p>

<pre><code>dfNew = merge(df, df2[cols_to_use], left_index=True, right_index=True, how='outer')
</code></pre>

<p>This will avoid any columns clashing in the merge</p>

<p><strong>For version 0.15 and above, the new preferred syntax is:</strong></p>

<pre><code>cols_to_use = df2.columns.difference(df.columns)
</code></pre>

<p>thanks @odedbd</p>
";;"['cols_to_use = df2.columns - df.columns\n', ""dfNew = merge(df, df2[cols_to_use], left_index=True, right_index=True, how='outer')\n"", 'cols_to_use = df2.columns.difference(df.columns)\n']";"['cols_to_use = df2.columns - df.columns\n', 'tolist()', ""dfNew = merge(df, df2[cols_to_use], left_index=True, right_index=True, how='outer')\n"", 'cols_to_use = df2.columns.difference(df.columns)\n']"
904;;2;19126566;19124601.0;2;127;;;"<p>Sure, if this comes up a lot, make a function like this one. You can even configure it to load every time you start IPython: <a href=""https://ipython.org/ipython-doc/1/config/overview.html"" rel=""noreferrer"">https://ipython.org/ipython-doc/1/config/overview.html</a></p>

<pre><code>def print_full(x):
    pd.set_option('display.max_rows', len(x))
    print(x)
    pd.reset_option('display.max_rows')
</code></pre>

<p>As for coloring, getting too elaborate with colors sounds counterproductive to me, but I agree something like <a href=""http://getbootstrap.com/2.3.2/base-css.html#tables"" rel=""noreferrer"">bootstrap's <code>.table-striped</code></a> would be nice. You could always <a href=""https://github.com/pydata/pandas/issues"" rel=""noreferrer"">create an issue</a> to suggest this feature.</p>
";;"[""def print_full(x):\n    pd.set_option('display.max_rows', len(x))\n    print(x)\n    pd.reset_option('display.max_rows')\n""]";"[""def print_full(x):\n    pd.set_option('display.max_rows', len(x))\n    print(x)\n    pd.reset_option('display.max_rows')\n"", '.table-striped']"
905;;0;19155860;19155718.0;2;52;;;"<pre><code>    List = [1, 3]
    df.ix[List]
</code></pre>

<p>should do the trick!
Whe I index with data frames I always use the .ix() method. Its so much easier and more flexible...</p>

<p><strong>UPDATE</strong>
This is no longer the accepted method for indexing. While the <code>ix</code> method is not deprecated its use can potentially lead to unintended consequences. Use <code>.iloc</code> for integer based indexing and <code>.loc</code> for label based indexing. </p>
";;['    List = [1, 3]\n    df.ix[List]\n'];['    List = [1, 3]\n    df.ix[List]\n', 'ix', '.iloc', '.loc']
906;;1;19213836;19213789.0;2;75;;;"<pre><code>plt.axvline(x_position)
</code></pre>

<p>It takes the standard plot formatting options (<code>linestlye</code>, <code>color</code>, ect)</p>

<p><a href=""http://matplotlib.org/api/pyplot_api.html"" rel=""noreferrer"">(doc)</a></p>

<p>If you have a reference to your <code>axes</code> object:</p>

<pre><code>ax.axvline(x, color='k', linestyle='--')
</code></pre>
";;"['plt.axvline(x_position)\n', ""ax.axvline(x, color='k', linestyle='--')\n""]";"['plt.axvline(x_position)\n', 'linestlye', 'color', 'axes', ""ax.axvline(x, color='k', linestyle='--')\n""]"
907;;9;19231939;19231871.0;2;68;;;"<p>These appear to be seconds since epoch.</p>

<pre><code>In [20]: df = DataFrame(data['values'])

In [21]: df.columns = [""date"",""price""]

In [22]: df
Out[22]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 358 entries, 0 to 357
Data columns (total 2 columns):
date     358  non-null values
price    358  non-null values
dtypes: float64(1), int64(1)

In [23]: df.head()
Out[23]: 
         date  price
0  1349720105  12.08
1  1349806505  12.35
2  1349892905  12.15
3  1349979305  12.19
4  1350065705  12.15
In [25]: df['date'] = pd.to_datetime(df['date'],unit='s')

In [26]: df.head()
Out[26]: 
                 date  price
0 2012-10-08 18:15:05  12.08
1 2012-10-09 18:15:05  12.35
2 2012-10-10 18:15:05  12.15
3 2012-10-11 18:15:05  12.19
4 2012-10-12 18:15:05  12.15

In [27]: df.dtypes
Out[27]: 
date     datetime64[ns]
price           float64
dtype: object
</code></pre>
";;"['In [20]: df = DataFrame(data[\'values\'])\n\nIn [21]: df.columns = [""date"",""price""]\n\nIn [22]: df\nOut[22]: \n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 358 entries, 0 to 357\nData columns (total 2 columns):\ndate     358  non-null values\nprice    358  non-null values\ndtypes: float64(1), int64(1)\n\nIn [23]: df.head()\nOut[23]: \n         date  price\n0  1349720105  12.08\n1  1349806505  12.35\n2  1349892905  12.15\n3  1349979305  12.19\n4  1350065705  12.15\nIn [25]: df[\'date\'] = pd.to_datetime(df[\'date\'],unit=\'s\')\n\nIn [26]: df.head()\nOut[26]: \n                 date  price\n0 2012-10-08 18:15:05  12.08\n1 2012-10-09 18:15:05  12.35\n2 2012-10-10 18:15:05  12.15\n3 2012-10-11 18:15:05  12.19\n4 2012-10-12 18:15:05  12.15\n\nIn [27]: df.dtypes\nOut[27]: \ndate     datetime64[ns]\nprice           float64\ndtype: object\n']";"['In [20]: df = DataFrame(data[\'values\'])\n\nIn [21]: df.columns = [""date"",""price""]\n\nIn [22]: df\nOut[22]: \n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 358 entries, 0 to 357\nData columns (total 2 columns):\ndate     358  non-null values\nprice    358  non-null values\ndtypes: float64(1), int64(1)\n\nIn [23]: df.head()\nOut[23]: \n         date  price\n0  1349720105  12.08\n1  1349806505  12.35\n2  1349892905  12.15\n3  1349979305  12.19\n4  1350065705  12.15\nIn [25]: df[\'date\'] = pd.to_datetime(df[\'date\'],unit=\'s\')\n\nIn [26]: df.head()\nOut[26]: \n                 date  price\n0 2012-10-08 18:15:05  12.08\n1 2012-10-09 18:15:05  12.35\n2 2012-10-10 18:15:05  12.15\n3 2012-10-11 18:15:05  12.19\n4 2012-10-12 18:15:05  12.15\n\nIn [27]: df.dtypes\nOut[27]: \ndate     datetime64[ns]\nprice           float64\ndtype: object\n']"
908;;5;19237920;19237878.0;2;54;;;"<p>I'll assume that <code>Time</code> and <code>Product</code> are columns in a <code>DataFrame</code>,  <code>df</code> is an instance of <code>DataFrame</code>, and that other variables are scalar values:</p>

<p>For now, you'll have to reference the <code>DataFrame</code> instance:</p>

<pre><code>k1 = df.loc[(df.Product == p_id) &amp; (df.Time &gt;= start_time) &amp; (df.Time &lt; end_time), ['Time', 'Product']]
</code></pre>

<p>The parentheses are also necessary, because of the precedence of the <code>&amp;</code> operator vs. the comparison operators. The <code>&amp;</code> operator is actually an overloaded bitwise operator which has the same precedence as arithmetic operators which in turn have a higher precedence than comparison operators.</p>

<p>In <code>pandas</code> 0.13 a new experimental <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html"" rel=""nofollow noreferrer""><code>DataFrame.query()</code></a> method will be available. It's extremely similar to subset modulo the <code>select</code> argument:</p>

<p>With <code>query()</code> you'd do it like this:</p>

<pre><code>df[['Time', 'Product']].query('Product == p_id and Month &lt; mn and Year == yr')
</code></pre>

<p>Here's a simple example:</p>

<pre><code>In [9]: df = DataFrame({'gender': np.random.choice(['m', 'f'], size=10), 'price': poisson(100, size=10)})

In [10]: df
Out[10]:
  gender  price
0      m     89
1      f    123
2      f    100
3      m    104
4      m     98
5      m    103
6      f    100
7      f    109
8      f     95
9      m     87

In [11]: df.query('gender == ""m"" and price &lt; 100')
Out[11]:
  gender  price
0      m     89
4      m     98
9      m     87
</code></pre>

<p>The final query that you're interested will even be able to take advantage of chained comparisons, like this:</p>

<pre><code>k1 = df[['Time', 'Product']].query('Product == p_id and start_time &lt;= Time &lt; end_time')
</code></pre>
";;"[""k1 = df.loc[(df.Product == p_id) & (df.Time >= start_time) & (df.Time < end_time), ['Time', 'Product']]\n"", ""df[['Time', 'Product']].query('Product == p_id and Month < mn and Year == yr')\n"", 'In [9]: df = DataFrame({\'gender\': np.random.choice([\'m\', \'f\'], size=10), \'price\': poisson(100, size=10)})\n\nIn [10]: df\nOut[10]:\n  gender  price\n0      m     89\n1      f    123\n2      f    100\n3      m    104\n4      m     98\n5      m    103\n6      f    100\n7      f    109\n8      f     95\n9      m     87\n\nIn [11]: df.query(\'gender == ""m"" and price < 100\')\nOut[11]:\n  gender  price\n0      m     89\n4      m     98\n9      m     87\n', ""k1 = df[['Time', 'Product']].query('Product == p_id and start_time <= Time < end_time')\n""]";"['Time', 'Product', 'DataFrame', 'df', 'DataFrame', 'DataFrame', ""k1 = df.loc[(df.Product == p_id) & (df.Time >= start_time) & (df.Time < end_time), ['Time', 'Product']]\n"", '&', '&', 'pandas', 'DataFrame.query()', 'select', 'query()', ""df[['Time', 'Product']].query('Product == p_id and Month < mn and Year == yr')\n"", 'In [9]: df = DataFrame({\'gender\': np.random.choice([\'m\', \'f\'], size=10), \'price\': poisson(100, size=10)})\n\nIn [10]: df\nOut[10]:\n  gender  price\n0      m     89\n1      f    123\n2      f    100\n3      m    104\n4      m     98\n5      m    103\n6      f    100\n7      f    109\n8      f     95\n9      m     87\n\nIn [11]: df.query(\'gender == ""m"" and price < 100\')\nOut[11]:\n  gender  price\n0      m     89\n4      m     98\n9      m     87\n', ""k1 = df[['Time', 'Product']].query('Product == p_id and start_time <= Time < end_time')\n""]"
909;;0;19238029;11067027.0;2;12;;;"<p><a href=""https://stackoverflow.com/a/11067079/841830"">Tweet's answer</a> can be passed to BrenBarn's answer above with </p>

<pre><code>data.reindex_axis(sorted(data.columns, key=lambda x: float(x[1:])), axis=1)
</code></pre>

<p>So for your example, say:</p>

<pre><code>vals = randint(low=16, high=80, size=25).reshape(5,5)
cols = ['Q1.3', 'Q6.1', 'Q1.2', 'Q9.1', 'Q10.2']
data = DataFrame(vals, columns = cols)
</code></pre>

<p>You get: </p>

<pre><code>data

    Q1.3    Q6.1    Q1.2    Q9.1    Q10.2
0   73      29      63      51      72
1   61      29      32      68      57
2   36      49      76      18      37
3   63      61      51      30      31
4   36      66      71      24      77
</code></pre>

<p>Then do:</p>

<pre><code>data.reindex_axis(sorted(data.columns, key=lambda x: float(x[1:])), axis=1)
</code></pre>

<p>resulting in:</p>

<pre><code>data


     Q1.2    Q1.3    Q6.1    Q9.1    Q10.2
0    2       0       1       3       4
1    7       5       6       8       9
2    2       0       1       3       4
3    2       0       1       3       4
4    2       0       1       3       4
</code></pre>
";;"['data.reindex_axis(sorted(data.columns, key=lambda x: float(x[1:])), axis=1)\n', ""vals = randint(low=16, high=80, size=25).reshape(5,5)\ncols = ['Q1.3', 'Q6.1', 'Q1.2', 'Q9.1', 'Q10.2']\ndata = DataFrame(vals, columns = cols)\n"", 'data\n\n    Q1.3    Q6.1    Q1.2    Q9.1    Q10.2\n0   73      29      63      51      72\n1   61      29      32      68      57\n2   36      49      76      18      37\n3   63      61      51      30      31\n4   36      66      71      24      77\n', 'data.reindex_axis(sorted(data.columns, key=lambda x: float(x[1:])), axis=1)\n', 'data\n\n\n     Q1.2    Q1.3    Q6.1    Q9.1    Q10.2\n0    2       0       1       3       4\n1    7       5       6       8       9\n2    2       0       1       3       4\n3    2       0       1       3       4\n4    2       0       1       3       4\n']";"['data.reindex_axis(sorted(data.columns, key=lambda x: float(x[1:])), axis=1)\n', ""vals = randint(low=16, high=80, size=25).reshape(5,5)\ncols = ['Q1.3', 'Q6.1', 'Q1.2', 'Q9.1', 'Q10.2']\ndata = DataFrame(vals, columns = cols)\n"", 'data\n\n    Q1.3    Q6.1    Q1.2    Q9.1    Q10.2\n0   73      29      63      51      72\n1   61      29      32      68      57\n2   36      49      76      18      37\n3   63      61      51      30      31\n4   36      66      71      24      77\n', 'data.reindex_axis(sorted(data.columns, key=lambda x: float(x[1:])), axis=1)\n', 'data\n\n\n     Q1.2    Q1.3    Q6.1    Q9.1    Q10.2\n0    2       0       1       3       4\n1    7       5       6       8       9\n2    2       0       1       3       4\n3    2       0       1       3       4\n4    2       0       1       3       4\n']"
910;;0;19295539;19155718.0;2;14;;;"<p>you can also use iloc:</p>

<pre><code>df.iloc[[1,3],:]
</code></pre>
";;['df.iloc[[1,3],:]\n'];['df.iloc[[1,3],:]\n']
911;;0;19295726;12065885.0;2;34;;;"<p>you can also use ranges by using:</p>

<pre><code>b = df[(df['a'] &gt; 1) &amp; (df['a'] &lt; 5)]
</code></pre>
";;"[""b = df[(df['a'] > 1) & (df['a'] < 5)]\n""]";"[""b = df[(df['a'] > 1) & (df['a'] < 5)]\n""]"
912;;4;19324591;19324453.0;2;83;;;"<p>You could use <code>Series.reindex</code>:</p>

<pre><code>import pandas as pd

idx = pd.date_range('09-01-2013', '09-30-2013')

s = pd.Series({'09-02-2013': 2,
               '09-03-2013': 10,
               '09-06-2013': 5,
               '09-07-2013': 1})
s.index = pd.DatetimeIndex(s.index)

s = s.reindex(idx, fill_value=0)
print(s)
</code></pre>

<p>yields</p>

<pre><code>2013-09-01     0
2013-09-02     2
2013-09-03    10
2013-09-04     0
2013-09-05     0
2013-09-06     5
2013-09-07     1
2013-09-08     0
...
</code></pre>
";;"[""import pandas as pd\n\nidx = pd.date_range('09-01-2013', '09-30-2013')\n\ns = pd.Series({'09-02-2013': 2,\n               '09-03-2013': 10,\n               '09-06-2013': 5,\n               '09-07-2013': 1})\ns.index = pd.DatetimeIndex(s.index)\n\ns = s.reindex(idx, fill_value=0)\nprint(s)\n"", '2013-09-01     0\n2013-09-02     2\n2013-09-03    10\n2013-09-04     0\n2013-09-05     0\n2013-09-06     5\n2013-09-07     1\n2013-09-08     0\n...\n']";"['Series.reindex', ""import pandas as pd\n\nidx = pd.date_range('09-01-2013', '09-30-2013')\n\ns = pd.Series({'09-02-2013': 2,\n               '09-03-2013': 10,\n               '09-06-2013': 5,\n               '09-07-2013': 1})\ns.index = pd.DatetimeIndex(s.index)\n\ns = s.reindex(idx, fill_value=0)\nprint(s)\n"", '2013-09-01     0\n2013-09-02     2\n2013-09-03    10\n2013-09-04     0\n2013-09-05     0\n2013-09-06     5\n2013-09-07     1\n2013-09-08     0\n...\n']"
913;;10;19368360;19365513.0;2;30;;;"<p>Upcoming pandas 0.13 version will allow to add rows through <code>loc</code> on non existing index data.</p>

<p>Description is <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#setting-with-enlargement"">here</a> and this new feature is called <em>Setting With Enlargement</em>.</p>
";;[];['loc']
914;;6;19378497;19377969.0;2;110;;;"<pre><code>dataframe[""period""] = dataframe[""Year""].map(str) + dataframe[""quarter""]
</code></pre>
";;"['dataframe[""period""] = dataframe[""Year""].map(str) + dataframe[""quarter""]\n']";"['dataframe[""period""] = dataframe[""Year""].map(str) + dataframe[""quarter""]\n']"
915;;3;19385591;19384532.0;2;116;;;"<p>On <code>groupby</code> object, the <code>agg</code> function can take a list to <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#applying-multiple-functions-at-once"">apply several aggregation methods</a> at once. This should give you the result you need:</p>

<pre><code>df[['col1', 'col2', 'col3', 'col4']].groupby(['col1', 'col2']).agg(['mean', 'count'])
</code></pre>
";;"[""df[['col1', 'col2', 'col3', 'col4']].groupby(['col1', 'col2']).agg(['mean', 'count'])\n""]";"['groupby', 'agg', ""df[['col1', 'col2', 'col3', 'col4']].groupby(['col1', 'col2']).agg(['mean', 'count'])\n""]"
916;;7;19415186;7837722.0;2;13;;;"<p>Just as a small addition, you can also do an apply if you have a complex function that you apply to a single column:</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.apply.html"">http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.apply.html</a></p>

<pre><code>df[b] = df[a].apply(lambda col: do stuff with col here)
</code></pre>
";;['df[b] = df[a].apply(lambda col: do stuff with col here)\n'];['df[b] = df[a].apply(lambda col: do stuff with col here)\n']
917;;0;19473752;13636592.0;2;18;;;"<pre><code>df.sort(['Peak', 'Weeks'], ascending=[True, False], inplace=True)
</code></pre>

<p>If you want the sorted result for future use, <code>inplace=True</code> is required.</p>
";;"[""df.sort(['Peak', 'Weeks'], ascending=[True, False], inplace=True)\n""]";"[""df.sort(['Peak', 'Weeks'], ascending=[True, False], inplace=True)\n"", 'inplace=True']"
918;;0;19482988;19482970.0;2;11;;;"<p>That's available as <code>my_dataframe.columns</code>.</p>
";;[];['my_dataframe.columns']
919;;5;19483025;19482970.0;2;548;;;"<p>You can get the values as a list by doing:</p>

<pre><code>list(my_dataframe.columns.values)
</code></pre>

<p>Also you can simply use:</p>

<pre><code>list(my_dataframe)
</code></pre>
";;['list(my_dataframe.columns.values)\n', 'list(my_dataframe)\n'];['list(my_dataframe.columns.values)\n', 'list(my_dataframe)\n']
920;;1;19483602;19482970.0;2;202;;;"<p>There is a built in method which is the most performant:</p>

<pre><code>my_dataframe.columns.values.tolist()
</code></pre>

<p><code>.columns</code> returns an <code>Index</code>, <code>.columns.values</code> returns an <code>array</code> and this has a helper function to return a <code>list</code>.</p>

<p><strong>EDIT</strong></p>

<p>For those who hate typing this is probably the shortest method:</p>

<pre><code>list(df)
</code></pre>
";;['my_dataframe.columns.values.tolist()\n', 'list(df)\n'];['my_dataframe.columns.values.tolist()\n', '.columns', 'Index', '.columns.values', 'array', 'list', 'list(df)\n']
921;;2;19555675;19555525.0;2;7;;;"<p>So I'm not entirely sure why this works, but it saves an image with my plot:</p>

<pre><code>dtf = pd.DataFrame.from_records(d,columns=h)
dtf2.plot()
fig = plt.gcf()
fig.savefig('output.png')
</code></pre>

<p>I'm guessing that the last snippet from my original post saved blank because the figure was never getting the axes generated by pandas.  With the above code, the figure object is returned from some magic global state by the gcf() call (get current figure), which automagically bakes in axes plotted in the line above.</p>
";;"[""dtf = pd.DataFrame.from_records(d,columns=h)\ndtf2.plot()\nfig = plt.gcf()\nfig.savefig('output.png')\n""]";"[""dtf = pd.DataFrame.from_records(d,columns=h)\ndtf2.plot()\nfig = plt.gcf()\nfig.savefig('output.png')\n""]"
922;;3;19585378;19585280.0;2;31;;;"<p>You can achieve this using <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.iterrows.html""><code>iterrows</code></a>:</p>

<pre><code>temp=[]

for row in df.iterrows():
    index, data = row
    temp.append(data.tolist())
</code></pre>

<p>Alternatively you can also use <code>apply</code>:</p>

<pre><code>df.apply(lambda x: x.tolist(), axis=1)
</code></pre>

<p><strong>Update</strong></p>

<p>After looking at this again there is a built in method which would be the fastest method also, calling <code>tolist</code> on the <code>.values</code> np array:</p>

<pre><code>In [62]:
df.values.tolist()

Out[62]:
[[0.0, 3.61, 380.0, 3.0],
 [1.0, 3.67, 660.0, 3.0],
 [1.0, 3.19, 640.0, 4.0],
 [0.0, 2.93, 520.0, 4.0]]
</code></pre>
";;['temp=[]\n\nfor row in df.iterrows():\n    index, data = row\n    temp.append(data.tolist())\n', 'df.apply(lambda x: x.tolist(), axis=1)\n', 'In [62]:\ndf.values.tolist()\n\nOut[62]:\n[[0.0, 3.61, 380.0, 3.0],\n [1.0, 3.67, 660.0, 3.0],\n [1.0, 3.19, 640.0, 4.0],\n [0.0, 2.93, 520.0, 4.0]]\n'];['iterrows', 'temp=[]\n\nfor row in df.iterrows():\n    index, data = row\n    temp.append(data.tolist())\n', 'apply', 'df.apply(lambda x: x.tolist(), axis=1)\n', 'tolist', '.values', 'In [62]:\ndf.values.tolist()\n\nOut[62]:\n[[0.0, 3.61, 380.0, 3.0],\n [1.0, 3.67, 660.0, 3.0],\n [1.0, 3.19, 640.0, 4.0],\n [0.0, 2.93, 520.0, 4.0]]\n']
923;;0;19585413;19585280.0;2;19;;;"<p>you can do it like this:</p>

<pre><code>map(list, df.values)
</code></pre>
";;['map(list, df.values)\n'];['map(list, df.values)\n']
924;;3;19592693;19584029.0;2;7;;;"<p>Your function is failing because the groupby dataframe you end up with has a hierarchical index and two columns (Letter and N) so when you do <code>.hist()</code> it's trying to make a histogram of both columns hence the str error.</p>

<p>This is the default behavior of pandas plotting functions (one plot per column) so if you reshape your data frame so that each letter is a column you will get exactly what you want.</p>

<pre><code>df.reset_index().pivot('index','Letter','N').hist()
</code></pre>

<p>The <code>reset_index()</code> is just to shove the current index into a column called <code>index</code>.  Then <code>pivot</code> will take your data frame, collect all of the values <code>N</code> for each <code>Letter</code> and make them a column.  The resulting data frame as 400 rows (fills missing values with <code>NaN</code>) and three columns (<code>A, B, C</code>).  <code>hist()</code> will then produce one histogram per column and you get format the plots as needed.</p>
";;"[""df.reset_index().pivot('index','Letter','N').hist()\n""]";"['.hist()', ""df.reset_index().pivot('index','Letter','N').hist()\n"", 'reset_index()', 'index', 'pivot', 'N', 'Letter', 'NaN', 'A, B, C', 'hist()']"
925;;4;19600533;11232275.0;2;55;;;"<p>As far as I can tell with updates to pandas, you have to use pivot_table() instead of pivot().</p>

<pre><code>pandas.pivot_table(df,values='count',index='site_id',columns='week')
</code></pre>
";;"[""pandas.pivot_table(df,values='count',index='site_id',columns='week')\n""]";"[""pandas.pivot_table(df,values='count',index='site_id',columns='week')\n""]"
926;;4;19603918;19584029.0;2;86;;;"<p>I'm on a roll, just found an even simpler way to do it using the <em>by</em> keyword in the hist method:</p>

<pre><code>df['N'].hist(by=df['Letter'])
</code></pre>

<p>That's a very handy little shortcut for quickly scanning your grouped data!</p>

<p>For future visitors, the product of this call is the following chart:
<img src=""https://i.stack.imgur.com/oy4NN.png"" alt=""enter image description here""></p>
";;"[""df['N'].hist(by=df['Letter'])\n""]";"[""df['N'].hist(by=df['Letter'])\n""]"
927;;7;19611857;19611729.0;2;28;;;"<p>You can use <code>read_csv()</code> on a <code>StringIO</code> object:</p>

<pre><code>from StringIO import StringIO  # got moved to io in python3.

import requests
r = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&amp;output=csv')
data = r.content

In [10]: df = pd.read_csv(StringIO(data), index_col=0,parse_dates=['Quradate'])

In [11]: df.head()
Out[11]: 
          City                                            region     Res_Comm  \
0       Dothan  South_Central-Montgomery-Auburn-Wiregrass-Dothan  Residential   
10       Foley                              South_Mobile-Baldwin  Residential   
12  Birmingham      North_Central-Birmingham-Tuscaloosa-Anniston   Commercial   
38       Brent      North_Central-Birmingham-Tuscaloosa-Anniston  Residential   
44      Athens                 North_Huntsville-Decatur-Florence  Residential   

          mkt_type            Quradate  National_exp  Alabama_exp  Sales_exp  \
0            Rural 2010-01-15 00:00:00             2            2          3   
10  Suburban_Urban 2010-01-15 00:00:00             4            4          4   
12  Suburban_Urban 2010-01-15 00:00:00             2            2          3   
38           Rural 2010-01-15 00:00:00             3            3          3   
44  Suburban_Urban 2010-01-15 00:00:00             4            5          4   

    Inventory_exp  Price_exp  Credit_exp  
0               2          3           3  
10              4          4           3  
12              2          2           3  
38              3          3           2  
44              4          4           4  
</code></pre>
";;"[""from StringIO import StringIO  # got moved to io in python3.\n\nimport requests\nr = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&output=csv')\ndata = r.content\n\nIn [10]: df = pd.read_csv(StringIO(data), index_col=0,parse_dates=['Quradate'])\n\nIn [11]: df.head()\nOut[11]: \n          City                                            region     Res_Comm  \\\n0       Dothan  South_Central-Montgomery-Auburn-Wiregrass-Dothan  Residential   \n10       Foley                              South_Mobile-Baldwin  Residential   \n12  Birmingham      North_Central-Birmingham-Tuscaloosa-Anniston   Commercial   \n38       Brent      North_Central-Birmingham-Tuscaloosa-Anniston  Residential   \n44      Athens                 North_Huntsville-Decatur-Florence  Residential   \n\n          mkt_type            Quradate  National_exp  Alabama_exp  Sales_exp  \\\n0            Rural 2010-01-15 00:00:00             2            2          3   \n10  Suburban_Urban 2010-01-15 00:00:00             4            4          4   \n12  Suburban_Urban 2010-01-15 00:00:00             2            2          3   \n38           Rural 2010-01-15 00:00:00             3            3          3   \n44  Suburban_Urban 2010-01-15 00:00:00             4            5          4   \n\n    Inventory_exp  Price_exp  Credit_exp  \n0               2          3           3  \n10              4          4           3  \n12              2          2           3  \n38              3          3           2  \n44              4          4           4  \n""]";"['read_csv()', 'StringIO', ""from StringIO import StringIO  # got moved to io in python3.\n\nimport requests\nr = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&output=csv')\ndata = r.content\n\nIn [10]: df = pd.read_csv(StringIO(data), index_col=0,parse_dates=['Quradate'])\n\nIn [11]: df.head()\nOut[11]: \n          City                                            region     Res_Comm  \\\n0       Dothan  South_Central-Montgomery-Auburn-Wiregrass-Dothan  Residential   \n10       Foley                              South_Mobile-Baldwin  Residential   \n12  Birmingham      North_Central-Birmingham-Tuscaloosa-Anniston   Commercial   \n38       Brent      North_Central-Birmingham-Tuscaloosa-Anniston  Residential   \n44      Athens                 North_Huntsville-Decatur-Florence  Residential   \n\n          mkt_type            Quradate  National_exp  Alabama_exp  Sales_exp  \\\n0            Rural 2010-01-15 00:00:00             2            2          3   \n10  Suburban_Urban 2010-01-15 00:00:00             4            4          4   \n12  Suburban_Urban 2010-01-15 00:00:00             2            2          3   \n38           Rural 2010-01-15 00:00:00             3            3          3   \n44  Suburban_Urban 2010-01-15 00:00:00             4            5          4   \n\n    Inventory_exp  Price_exp  Credit_exp  \n0               2          3           3  \n10              4          4           3  \n12              2          2           3  \n38              3          3           2  \n44              4          4           4  \n""]"
928;;4;19619020;19618912.0;2;6;;;"<p>If I understand you correctly, you can use a combination of <code>Series.isin()</code> and <code>DataFrame.append()</code>:</p>

<pre><code>In [80]: df1
Out[80]:
   rating  user_id
0       2  0x21abL
1       1  0x21abL
2       1   0xdafL
3       0  0x21abL
4       4  0x1d14L
5       2  0x21abL
6       1  0x21abL
7       0   0xdafL
8       4  0x1d14L
9       1  0x21abL

In [81]: df2
Out[81]:
   rating      user_id
0       2      0x1d14L
1       1    0xdbdcad7
2       1      0x21abL
3       3      0x21abL
4       3      0x21abL
5       1  0x5734a81e2
6       2      0x1d14L
7       0       0xdafL
8       0      0x1d14L
9       4  0x5734a81e2

In [82]: ind = df2.user_id.isin(df1.user_id) &amp; df1.user_id.isin(df2.user_id)

In [83]: ind
Out[83]:
0     True
1    False
2     True
3     True
4     True
5    False
6     True
7     True
8     True
9    False
Name: user_id, dtype: bool

In [84]: df1[ind].append(df2[ind])
Out[84]:
   rating  user_id
0       2  0x21abL
2       1   0xdafL
3       0  0x21abL
4       4  0x1d14L
6       1  0x21abL
7       0   0xdafL
8       4  0x1d14L
0       2  0x1d14L
2       1  0x21abL
3       3  0x21abL
4       3  0x21abL
6       2  0x1d14L
7       0   0xdafL
8       0  0x1d14L
</code></pre>

<p>This is essentially the algorithm you described as ""clunky"", using idiomatic <code>pandas</code> methods. Note the duplicate row indices. Also, note that this won't give you the expected output if <code>df1</code> and <code>df2</code> have no overlapping row indices, i.e., if</p>

<pre><code>In [93]: df1.index &amp; df2.index
Out[93]: Int64Index([], dtype='int64')
</code></pre>

<p>In fact, it won't give the expected output if their row indices are not equal.</p>
";;"['In [80]: df1\nOut[80]:\n   rating  user_id\n0       2  0x21abL\n1       1  0x21abL\n2       1   0xdafL\n3       0  0x21abL\n4       4  0x1d14L\n5       2  0x21abL\n6       1  0x21abL\n7       0   0xdafL\n8       4  0x1d14L\n9       1  0x21abL\n\nIn [81]: df2\nOut[81]:\n   rating      user_id\n0       2      0x1d14L\n1       1    0xdbdcad7\n2       1      0x21abL\n3       3      0x21abL\n4       3      0x21abL\n5       1  0x5734a81e2\n6       2      0x1d14L\n7       0       0xdafL\n8       0      0x1d14L\n9       4  0x5734a81e2\n\nIn [82]: ind = df2.user_id.isin(df1.user_id) & df1.user_id.isin(df2.user_id)\n\nIn [83]: ind\nOut[83]:\n0     True\n1    False\n2     True\n3     True\n4     True\n5    False\n6     True\n7     True\n8     True\n9    False\nName: user_id, dtype: bool\n\nIn [84]: df1[ind].append(df2[ind])\nOut[84]:\n   rating  user_id\n0       2  0x21abL\n2       1   0xdafL\n3       0  0x21abL\n4       4  0x1d14L\n6       1  0x21abL\n7       0   0xdafL\n8       4  0x1d14L\n0       2  0x1d14L\n2       1  0x21abL\n3       3  0x21abL\n4       3  0x21abL\n6       2  0x1d14L\n7       0   0xdafL\n8       0  0x1d14L\n', ""In [93]: df1.index & df2.index\nOut[93]: Int64Index([], dtype='int64')\n""]";"['Series.isin()', 'DataFrame.append()', 'In [80]: df1\nOut[80]:\n   rating  user_id\n0       2  0x21abL\n1       1  0x21abL\n2       1   0xdafL\n3       0  0x21abL\n4       4  0x1d14L\n5       2  0x21abL\n6       1  0x21abL\n7       0   0xdafL\n8       4  0x1d14L\n9       1  0x21abL\n\nIn [81]: df2\nOut[81]:\n   rating      user_id\n0       2      0x1d14L\n1       1    0xdbdcad7\n2       1      0x21abL\n3       3      0x21abL\n4       3      0x21abL\n5       1  0x5734a81e2\n6       2      0x1d14L\n7       0       0xdafL\n8       0      0x1d14L\n9       4  0x5734a81e2\n\nIn [82]: ind = df2.user_id.isin(df1.user_id) & df1.user_id.isin(df2.user_id)\n\nIn [83]: ind\nOut[83]:\n0     True\n1    False\n2     True\n3     True\n4     True\n5    False\n6     True\n7     True\n8     True\n9    False\nName: user_id, dtype: bool\n\nIn [84]: df1[ind].append(df2[ind])\nOut[84]:\n   rating  user_id\n0       2  0x21abL\n2       1   0xdafL\n3       0  0x21abL\n4       4  0x1d14L\n6       1  0x21abL\n7       0   0xdafL\n8       4  0x1d14L\n0       2  0x1d14L\n2       1  0x21abL\n3       3  0x21abL\n4       3  0x21abL\n6       2  0x1d14L\n7       0   0xdafL\n8       0  0x1d14L\n', 'pandas', 'df1', 'df2', ""In [93]: df1.index & df2.index\nOut[93]: Int64Index([], dtype='int64')\n""]"
929;;0;19632099;19632075.0;2;11;;;"<p>you can use regex as the delimiter:</p>

<pre><code>pd.read_csv(""whitespace.csv"", header=None, delimiter=r""\s+"")
</code></pre>
";;"['pd.read_csv(""whitespace.csv"", header=None, delimiter=r""\\s+"")\n']";"['pd.read_csv(""whitespace.csv"", header=None, delimiter=r""\\s+"")\n']"
930;;0;19633103;19632075.0;2;43;;;"<p>add <code>delim_whitespace=True</code> argument, it's faster than regex.</p>
";;[];['delim_whitespace=True']
931;;9;19736406;19736080.0;2;36;;;"<p><strong>In Python 3.x:</strong></p>

<pre><code>In [6]: d = dict( A = np.array([1,2]), B = np.array([1,2,3,4]) )

In [7]: DataFrame(dict([ (k,Series(v)) for k,v in d.items() ]))
Out[7]: 
    A  B
0   1  1
1   2  2
2 NaN  3
3 NaN  4
</code></pre>

<p><strong>In Python 2.x:</strong></p>

<p>replace <code>d.items()</code> with <code>d.iteritems()</code>.</p>
";;['In [6]: d = dict( A = np.array([1,2]), B = np.array([1,2,3,4]) )\n\nIn [7]: DataFrame(dict([ (k,Series(v)) for k,v in d.items() ]))\nOut[7]: \n    A  B\n0   1  1\n1   2  2\n2 NaN  3\n3 NaN  4\n'];['In [6]: d = dict( A = np.array([1,2]), B = np.array([1,2,3,4]) )\n\nIn [7]: DataFrame(dict([ (k,Series(v)) for k,v in d.items() ]))\nOut[7]: \n    A  B\n0   1  1\n1   2  2\n2 NaN  3\n3 NaN  4\n', 'd.items()', 'd.iteritems()']
932;;2;19739768;14262433.0;2;38;;;"<p>If your datasets are between 1 and 20GB, you should get a workstation with 48GB of RAM. Then Pandas can hold the entire dataset in RAM. I know its not the answer you're looking for here, but doing scientific computing on a notebook with 4GB of RAM isn't reasonable.</p>
";;[];[]
933;;8;19758398;19758364.0;2;145;;;"<pre><code>data.rename(columns={'gdp':'log(gdp)'}, inplace=True)
</code></pre>

<p>The <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html#pandas.DataFrame.rename"" rel=""noreferrer""><code>rename</code></a> show that it accepts a dict as a param for <code>columns</code> so you just pass a dict with a single entry.</p>

<p>Also see <a href=""https://stackoverflow.com/questions/11346283/renaming-columns-in-pandas"">related</a></p>
";;"[""data.rename(columns={'gdp':'log(gdp)'}, inplace=True)\n""]";"[""data.rename(columns={'gdp':'log(gdp)'}, inplace=True)\n"", 'rename', 'columns']"
934;;5;19791302;19790790.0;2;28;;;"<p>Can I ask why not just do it by slicing the data frame. Something like</p>

<pre><code>#create some data with Names column
data = pd.DataFrame({'Names': ['Joe', 'John', 'Jasper', 'Jez'] *4, 'Ob1' : np.random.rand(16), 'Ob2' : np.random.rand(16)})

#create unique list of names
UniqueNames = data.Names.unique()

#create a data frame dictionary to store your data frames
DataFrameDict = {elem : pd.DataFrame for elem in UniqueNames}

for key in DataFrameDict.keys():
    DataFrameDict[key] = data[:][data.Names == key]
</code></pre>

<p>Hey presto you have a dictionary of data frames just as (I think) you want them. Need to access one? Just enter</p>

<pre><code>DataFrameDict['Joe']
</code></pre>

<p>Hope that helps</p>
";;"[""#create some data with Names column\ndata = pd.DataFrame({'Names': ['Joe', 'John', 'Jasper', 'Jez'] *4, 'Ob1' : np.random.rand(16), 'Ob2' : np.random.rand(16)})\n\n#create unique list of names\nUniqueNames = data.Names.unique()\n\n#create a data frame dictionary to store your data frames\nDataFrameDict = {elem : pd.DataFrame for elem in UniqueNames}\n\nfor key in DataFrameDict.keys():\n    DataFrameDict[key] = data[:][data.Names == key]\n"", ""DataFrameDict['Joe']\n""]";"[""#create some data with Names column\ndata = pd.DataFrame({'Names': ['Joe', 'John', 'Jasper', 'Jez'] *4, 'Ob1' : np.random.rand(16), 'Ob2' : np.random.rand(16)})\n\n#create unique list of names\nUniqueNames = data.Names.unique()\n\n#create a data frame dictionary to store your data frames\nDataFrameDict = {elem : pd.DataFrame for elem in UniqueNames}\n\nfor key in DataFrameDict.keys():\n    DataFrameDict[key] = data[:][data.Names == key]\n"", ""DataFrameDict['Joe']\n""]"
935;;4;19798528;19798153.0;2;221;;;"<p>Straight from Wes McKinney's <a href=""http://shop.oreilly.com/product/0636920023784.do"" rel=""noreferrer"">Python for Data Analysis</a> book, pg. 132 (I highly recommended this book):</p>

<blockquote>
  <p>Another frequent operation is applying a function on 1D arrays to each column or row. DataFrames apply method does exactly this:</p>
</blockquote>

<pre><code>In [116]: frame = DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])

In [117]: frame
Out[117]: 
               b         d         e
Utah   -0.029638  1.081563  1.280300
Ohio    0.647747  0.831136 -1.549481
Texas   0.513416 -0.884417  0.195343
Oregon -0.485454 -0.477388 -0.309548

In [118]: f = lambda x: x.max() - x.min()

In [119]: frame.apply(f)
Out[119]: 
b    1.133201
d    1.965980
e    2.829781
dtype: float64
</code></pre>

<blockquote>
  <p>Many of the most common array statistics (like sum and mean) are DataFrame methods,
      so using apply is not necessary.</p>
  
  <p>Element-wise Python functions can be used, too. Suppose you wanted to compute a formatted string from each floating point value in frame. You can do this with applymap:</p>
</blockquote>

<pre><code>In [120]: format = lambda x: '%.2f' % x

In [121]: frame.applymap(format)
Out[121]: 
            b      d      e
Utah    -0.03   1.08   1.28
Ohio     0.65   0.83  -1.55
Texas    0.51  -0.88   0.20
Oregon  -0.49  -0.48  -0.31
</code></pre>

<blockquote>
  <p>The reason for the name applymap is that Series has a map method for applying an element-wise function:</p>
</blockquote>

<pre><code>In [122]: frame['e'].map(format)
Out[122]: 
Utah       1.28
Ohio      -1.55
Texas      0.20
Oregon    -0.31
Name: e, dtype: object
</code></pre>

<p>Summing up, <code>apply</code> works on a row / column basis of a DataFrame, <code>applymap</code> works element-wise on a DataFrame, and <code>map</code> works element-wise on a Series.</p>
";;"[""In [116]: frame = DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n\nIn [117]: frame\nOut[117]: \n               b         d         e\nUtah   -0.029638  1.081563  1.280300\nOhio    0.647747  0.831136 -1.549481\nTexas   0.513416 -0.884417  0.195343\nOregon -0.485454 -0.477388 -0.309548\n\nIn [118]: f = lambda x: x.max() - x.min()\n\nIn [119]: frame.apply(f)\nOut[119]: \nb    1.133201\nd    1.965980\ne    2.829781\ndtype: float64\n"", ""In [120]: format = lambda x: '%.2f' % x\n\nIn [121]: frame.applymap(format)\nOut[121]: \n            b      d      e\nUtah    -0.03   1.08   1.28\nOhio     0.65   0.83  -1.55\nTexas    0.51  -0.88   0.20\nOregon  -0.49  -0.48  -0.31\n"", ""In [122]: frame['e'].map(format)\nOut[122]: \nUtah       1.28\nOhio      -1.55\nTexas      0.20\nOregon    -0.31\nName: e, dtype: object\n""]";"[""In [116]: frame = DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n\nIn [117]: frame\nOut[117]: \n               b         d         e\nUtah   -0.029638  1.081563  1.280300\nOhio    0.647747  0.831136 -1.549481\nTexas   0.513416 -0.884417  0.195343\nOregon -0.485454 -0.477388 -0.309548\n\nIn [118]: f = lambda x: x.max() - x.min()\n\nIn [119]: frame.apply(f)\nOut[119]: \nb    1.133201\nd    1.965980\ne    2.829781\ndtype: float64\n"", ""In [120]: format = lambda x: '%.2f' % x\n\nIn [121]: frame.applymap(format)\nOut[121]: \n            b      d      e\nUtah    -0.03   1.08   1.28\nOhio     0.65   0.83  -1.55\nTexas    0.51  -0.88   0.20\nOregon  -0.49  -0.48  -0.31\n"", ""In [122]: frame['e'].map(format)\nOut[122]: \nUtah       1.28\nOhio      -1.55\nTexas      0.20\nOregon    -0.31\nName: e, dtype: object\n"", 'apply', 'applymap', 'map']"
936;;8;19809616;19790790.0;2;26;;;"<p>Firstly your approach is inefficient because the appending to the list on a row by basis will be slow as it has to periodically grow the list when there is insufficient space for the new entry, list comprehensions are better in this respect as the size is determined up front and allocated once.</p>

<p>However, I think fundamentally your approach is a little wasteful as you have a dataframe already so why create a new one for each of these users?</p>

<p>I would sort the dataframe by column <code>'name'</code>, set the index to be this and if required not drop the column.</p>

<p>Then generate a list of all the unique entries and then you can perform a lookup using these entries and crucially if you only querying the data, use the selection critieria to return a view on the dataframe without incurring a costly data copy.</p>

<p>So:</p>

<pre><code># sort the dataframe
df.sort(columns=['name'], inplace=True)
# set the index to be this and don't drop
df.set_index(keys=['name'], drop=False,inplace=True)
# get a list of names
names=df['name'].unique().tolist()
# now we can perform a lookup on a 'view' of the dataframe
joe = df.loc[df.name=='joe']
# now you can query all 'joes'
</code></pre>
";;"[""# sort the dataframe\ndf.sort(columns=['name'], inplace=True)\n# set the index to be this and don't drop\ndf.set_index(keys=['name'], drop=False,inplace=True)\n# get a list of names\nnames=df['name'].unique().tolist()\n# now we can perform a lookup on a 'view' of the dataframe\njoe = df.loc[df.name=='joe']\n# now you can query all 'joes'\n""]";"[""'name'"", ""# sort the dataframe\ndf.sort(columns=['name'], inplace=True)\n# set the index to be this and don't drop\ndf.set_index(keys=['name'], drop=False,inplace=True)\n# get a list of names\nnames=df['name'].unique().tolist()\n# now we can perform a lookup on a 'view' of the dataframe\njoe = df.loc[df.name=='joe']\n# now you can query all 'joes'\n""]"
937;;1;19821311;17001389.0;2;62;;;"<p>There's more to it than this, but you're probably looking for this list:</p>

<pre><code>B   business day frequency
C   custom business day frequency (experimental)
D   calendar day frequency
W   weekly frequency
M   month end frequency
BM  business month end frequency
MS  month start frequency
BMS business month start frequency
Q   quarter end frequency
BQ  business quarter endfrequency
QS  quarter start frequency
BQS business quarter start frequency
A   year end frequency
BA  business year end frequency
AS  year start frequency
BAS business year start frequency
H   hourly frequency
T   minutely frequency
S   secondly frequency
L   milliseconds
U   microseconds
</code></pre>

<p>Source: <a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases"">http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases</a></p>
";;['B   business day frequency\nC   custom business day frequency (experimental)\nD   calendar day frequency\nW   weekly frequency\nM   month end frequency\nBM  business month end frequency\nMS  month start frequency\nBMS business month start frequency\nQ   quarter end frequency\nBQ  business quarter endfrequency\nQS  quarter start frequency\nBQS business quarter start frequency\nA   year end frequency\nBA  business year end frequency\nAS  year start frequency\nBAS business year start frequency\nH   hourly frequency\nT   minutely frequency\nS   secondly frequency\nL   milliseconds\nU   microseconds\n'];['B   business day frequency\nC   custom business day frequency (experimental)\nD   calendar day frequency\nW   weekly frequency\nM   month end frequency\nBM  business month end frequency\nMS  month start frequency\nBMS business month start frequency\nQ   quarter end frequency\nBQ  business quarter endfrequency\nQS  quarter start frequency\nBQS business quarter start frequency\nA   year end frequency\nBA  business year end frequency\nAS  year start frequency\nBAS business year start frequency\nH   hourly frequency\nT   minutely frequency\nS   secondly frequency\nL   milliseconds\nU   microseconds\n']
938;;2;19828967;19828822.0;2;171;;;"<p>You can use the attribute <code>df.empty</code> to check whether it's empty or not:</p>

<pre><code>if df.empty:
    print('DataFrame is empty!')
</code></pre>

<p>Source: <a href=""http://pandas.pydata.org/pandas-docs/dev/basics.html#boolean-reductions"">Pandas Documentation</a></p>
";;"[""if df.empty:\n    print('DataFrame is empty!')\n""]";"['df.empty', ""if df.empty:\n    print('DataFrame is empty!')\n""]"
939;;0;19851521;19851005.0;2;95;;;"<p>The <code>rename</code> method takes a dictionary for the index which applies to index <em>values</em>.<br>
You want to rename to index level's name:</p>

<pre><code>df.index.names = ['Date']
</code></pre>

<p><em>A good way to think about this is that columns and index are the same type of object (<code>Index</code> or <code>MultiIndex</code>), and you can interchange the two via transpose.</em></p>

<p>This is a little bit confusing since the index names have a similar meaning to columns, so here are some more examples:</p>

<pre><code>In [1]: df = pd.DataFrame([[1, 2, 3], [4, 5 ,6]], columns=list('ABC'))

In [2]: df
Out[2]: 
   A  B  C
0  1  2  3
1  4  5  6

In [3]: df1 = df.set_index('A')

In [4]: df1
Out[4]: 
   B  C
A      
1  2  3
4  5  6
</code></pre>

<p>You can see the rename on the index, which can change the <em>value</em> 1:</p>

<pre><code>In [5]: df1.rename(index={1: 'a'})
Out[5]: 
   B  C
A      
a  2  3
4  5  6

In [6]: df1.rename(columns={'B': 'BB'})
Out[6]: 
   BB  C
A       
1   2  3
4   5  6
</code></pre>

<p>Whilst renaming the level names:</p>

<pre><code>In [7]: df1.index.names = ['index']
        df1.columns.names = ['column']
</code></pre>

<p>Note: this attribute is just a list, and you could do the renaming as a list comprehension/map.</p>

<pre><code>In [8]: df1
Out[8]: 
column  B  C
index       
1       2  3
4       5  6
</code></pre>
";;"[""df.index.names = ['Date']\n"", ""In [1]: df = pd.DataFrame([[1, 2, 3], [4, 5 ,6]], columns=list('ABC'))\n\nIn [2]: df\nOut[2]: \n   A  B  C\n0  1  2  3\n1  4  5  6\n\nIn [3]: df1 = df.set_index('A')\n\nIn [4]: df1\nOut[4]: \n   B  C\nA      \n1  2  3\n4  5  6\n"", ""In [5]: df1.rename(index={1: 'a'})\nOut[5]: \n   B  C\nA      \na  2  3\n4  5  6\n\nIn [6]: df1.rename(columns={'B': 'BB'})\nOut[6]: \n   BB  C\nA       \n1   2  3\n4   5  6\n"", ""In [7]: df1.index.names = ['index']\n        df1.columns.names = ['column']\n"", 'In [8]: df1\nOut[8]: \ncolumn  B  C\nindex       \n1       2  3\n4       5  6\n']";"['rename', ""df.index.names = ['Date']\n"", 'Index', 'MultiIndex', ""In [1]: df = pd.DataFrame([[1, 2, 3], [4, 5 ,6]], columns=list('ABC'))\n\nIn [2]: df\nOut[2]: \n   A  B  C\n0  1  2  3\n1  4  5  6\n\nIn [3]: df1 = df.set_index('A')\n\nIn [4]: df1\nOut[4]: \n   B  C\nA      \n1  2  3\n4  5  6\n"", ""In [5]: df1.rename(index={1: 'a'})\nOut[5]: \n   B  C\nA      \na  2  3\n4  5  6\n\nIn [6]: df1.rename(columns={'B': 'BB'})\nOut[6]: \n   BB  C\nA       \n1   2  3\n4   5  6\n"", ""In [7]: df1.index.names = ['index']\n        df1.columns.names = ['column']\n"", 'In [8]: df1\nOut[8]: \ncolumn  B  C\nindex       \n1       2  3\n4       5  6\n']"
940;;5;19913845;19913659.0;2;154;;;"<pre><code>df['color'] = np.where(df['Set']=='Z', 'green', 'red')
</code></pre>

<hr>

<p>For example,</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})
df['color'] = np.where(df['Set']=='Z', 'green', 'red')
print(df)
</code></pre>

<p>yields</p>

<pre><code>  Set Type  color
0   Z    A  green
1   Z    B  green
2   X    B    red
3   Y    C    red
</code></pre>

<hr>

<p>If you had more conditions then use <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.select.html"" rel=""noreferrer""><code>np.select</code></a>. For example, if you want color to be </p>

<ul>
<li><code>yellow</code> when <code>(df['Set'] == 'Z') &amp; (df['Type'] == 'A')</code></li>
<li>otherwise <code>blue</code> when <code>(df['Set'] == 'Z') &amp; (df['Type'] == 'B')</code> </li>
<li>otherwise <code>purple</code> when <code>(df['Type'] == 'B')</code></li>
<li>otherwise <code>black</code>,</li>
</ul>

<p>then use</p>

<pre><code>df = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})
conditions = [
    (df['Set'] == 'Z') &amp; (df['Type'] == 'A'),
    (df['Set'] == 'Z') &amp; (df['Type'] == 'B'),
    (df['Type'] == 'B')]
choices = ['yellow', 'blue', 'purple']
df['color'] = np.select(conditions, choices, default='black')
print(df)
</code></pre>

<p>which yields</p>

<pre><code>  Set Type   color
0   Z    A  yellow
1   Z    B    blue
2   X    B  purple
3   Y    C   black
</code></pre>
";;"[""df['color'] = np.where(df['Set']=='Z', 'green', 'red')\n"", ""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})\ndf['color'] = np.where(df['Set']=='Z', 'green', 'red')\nprint(df)\n"", '  Set Type  color\n0   Z    A  green\n1   Z    B  green\n2   X    B    red\n3   Y    C    red\n', ""df = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})\nconditions = [\n    (df['Set'] == 'Z') & (df['Type'] == 'A'),\n    (df['Set'] == 'Z') & (df['Type'] == 'B'),\n    (df['Type'] == 'B')]\nchoices = ['yellow', 'blue', 'purple']\ndf['color'] = np.select(conditions, choices, default='black')\nprint(df)\n"", '  Set Type   color\n0   Z    A  yellow\n1   Z    B    blue\n2   X    B  purple\n3   Y    C   black\n']";"[""df['color'] = np.where(df['Set']=='Z', 'green', 'red')\n"", ""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})\ndf['color'] = np.where(df['Set']=='Z', 'green', 'red')\nprint(df)\n"", '  Set Type  color\n0   Z    A  green\n1   Z    B  green\n2   X    B    red\n3   Y    C    red\n', 'np.select', 'yellow', ""(df['Set'] == 'Z') & (df['Type'] == 'A')"", 'blue', ""(df['Set'] == 'Z') & (df['Type'] == 'B')"", 'purple', ""(df['Type'] == 'B')"", 'black', ""df = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})\nconditions = [\n    (df['Set'] == 'Z') & (df['Type'] == 'A'),\n    (df['Set'] == 'Z') & (df['Type'] == 'B'),\n    (df['Type'] == 'B')]\nchoices = ['yellow', 'blue', 'purple']\ndf['color'] = np.select(conditions, choices, default='black')\nprint(df)\n"", '  Set Type   color\n0   Z    A  yellow\n1   Z    B    blue\n2   X    B  purple\n3   Y    C   black\n']"
941;;1;19915115;19914937.0;2;13;;;"<p>This solves the problem:</p>

<pre><code>df['newcolumn'] = df.A * df.B
</code></pre>

<p>You could also do:</p>

<pre><code>def fab(row):
  return row['A'] * row['B']

df['newcolumn'] = df.apply(fab, axis=1)
</code></pre>
";;"[""df['newcolumn'] = df.A * df.B\n"", ""def fab(row):\n  return row['A'] * row['B']\n\ndf['newcolumn'] = df.apply(fab, axis=1)\n""]";"[""df['newcolumn'] = df.A * df.B\n"", ""def fab(row):\n  return row['A'] * row['B']\n\ndf['newcolumn'] = df.apply(fab, axis=1)\n""]"
942;;0;19922732;19914937.0;2;77;;;"<p>You can go with @greenAfrican example, if it's possible for you to rewrite your function. But if you don't want to rewrite your function, you can wrap it into anonymous function inside apply, like this:</p>

<pre><code>&gt;&gt;&gt; def fxy(x, y):
...     return x * y

&gt;&gt;&gt; df['newcolumn'] = df.apply(lambda x: fxy(x['A'], x['B']), axis=1)
&gt;&gt;&gt; df
    A   B  newcolumn
0  10  20        200
1  20  30        600
2  30  10        300
</code></pre>
";;"["">>> def fxy(x, y):\n...     return x * y\n\n>>> df['newcolumn'] = df.apply(lambda x: fxy(x['A'], x['B']), axis=1)\n>>> df\n    A   B  newcolumn\n0  10  20        200\n1  20  30        600\n2  30  10        300\n""]";"["">>> def fxy(x, y):\n...     return x * y\n\n>>> df['newcolumn'] = df.apply(lambda x: fxy(x['A'], x['B']), axis=1)\n>>> df\n    A   B  newcolumn\n0  10  20        200\n1  20  30        600\n2  30  10        300\n""]"
943;;0;19928288;19928284.0;2;30;;;"<p>Ah, of course there is a solution for this already:</p>

<pre><code>from pandas.util.testing import assert_frame_equal
</code></pre>
";;['from pandas.util.testing import assert_frame_equal\n'];['from pandas.util.testing import assert_frame_equal\n']
944;;0;19937902;18792918.0;2;9;;;"<p>In case anyone needs to try and merge two dataframes together on the index (instead of another column), this also works!</p>

<p>T1 and T2 are dataframes that have the same indices</p>

<pre><code>import pandas as pd
T1 = pd.merge(T1, T2, on=T1.index, how='outer')
</code></pre>

<p>P.S. I had to use merge because append would fill NaNs in unnecessarily.</p>
";;"[""import pandas as pd\nT1 = pd.merge(T1, T2, on=T1.index, how='outer')\n""]";"[""import pandas as pd\nT1 = pd.merge(T1, T2, on=T1.index, how='outer')\n""]"
945;;7;19960116;19960077.0;2;200;;;"<p>You can use <code>something.isin(somewhere)</code> and <code>~something.isin(somewhere)</code>:</p>

<pre><code>&gt;&gt;&gt; df
  countries
0        US
1        UK
2   Germany
3     China
&gt;&gt;&gt; countries
['UK', 'China']
&gt;&gt;&gt; df.countries.isin(countries)
0    False
1     True
2    False
3     True
Name: countries, dtype: bool
&gt;&gt;&gt; df[df.countries.isin(countries)]
  countries
1        UK
3     China
&gt;&gt;&gt; df[~df.countries.isin(countries)]
  countries
0        US
2   Germany
</code></pre>
";;"["">>> df\n  countries\n0        US\n1        UK\n2   Germany\n3     China\n>>> countries\n['UK', 'China']\n>>> df.countries.isin(countries)\n0    False\n1     True\n2    False\n3     True\nName: countries, dtype: bool\n>>> df[df.countries.isin(countries)]\n  countries\n1        UK\n3     China\n>>> df[~df.countries.isin(countries)]\n  countries\n0        US\n2   Germany\n""]";"['something.isin(somewhere)', '~something.isin(somewhere)', "">>> df\n  countries\n0        US\n1        UK\n2   Germany\n3     China\n>>> countries\n['UK', 'China']\n>>> df.countries.isin(countries)\n0    False\n1     True\n2    False\n3     True\nName: countries, dtype: bool\n>>> df[df.countries.isin(countries)]\n  countries\n1        UK\n3     China\n>>> df[~df.countries.isin(countries)]\n  countries\n0        US\n2   Germany\n""]"
946;;2;19960136;19960077.0;2;6;;;"<p>I've been usually doing generic filtering over rows like this:</p>

<pre><code>criterion = lambda row: row['countries'] not in countries
not_in = df[df.apply(criterion, axis=1)]
</code></pre>
";;"[""criterion = lambda row: row['countries'] not in countries\nnot_in = df[df.apply(criterion, axis=1)]\n""]";"[""criterion = lambda row: row['countries'] not in countries\nnot_in = df[df.apply(criterion, axis=1)]\n""]"
947;;1;19961557;19961490.0;2;42;;;"<p>You can pivot your DataFrame after creating:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(data)
&gt;&gt;&gt; df.pivot(index=0, columns=1, values=2)
# avg DataFrame
1      c1     c2
0               
r1  avg11  avg12
r2  avg21  avg22
&gt;&gt;&gt; df.pivot(index=0, columns=1, values=3)
# stdev DataFrame
1        c1       c2
0                   
r1  stdev11  stdev12
r2  stdev21  stdev22
</code></pre>
";;['>>> df = pd.DataFrame(data)\n>>> df.pivot(index=0, columns=1, values=2)\n# avg DataFrame\n1      c1     c2\n0               \nr1  avg11  avg12\nr2  avg21  avg22\n>>> df.pivot(index=0, columns=1, values=3)\n# stdev DataFrame\n1        c1       c2\n0                   \nr1  stdev11  stdev12\nr2  stdev21  stdev22\n'];['>>> df = pd.DataFrame(data)\n>>> df.pivot(index=0, columns=1, values=2)\n# avg DataFrame\n1      c1     c2\n0               \nr1  avg11  avg12\nr2  avg21  avg22\n>>> df.pivot(index=0, columns=1, values=3)\n# stdev DataFrame\n1        c1       c2\n0                   \nr1  stdev11  stdev12\nr2  stdev21  stdev22\n']
948;;4;19961872;19961490.0;2;24;;;"<p>I submit that it is better to leave your data stacked as it is:</p>

<pre><code>df = pandas.DataFrame(data, columns=['R_Number', 'C_Number', 'Avg', 'Std'])

# Possibly also this if these can always be the indexes:
# df = df.set_index(['R_Number', 'C_Number'])
</code></pre>

<p>Then it's a bit more intuitive to say</p>

<pre><code>df.set_index(['R_Number', 'C_Number']).Avg.unstack(level=1)
</code></pre>

<p>This way it is implicit that you're seeking to reshape the averages, or the standard deviations. Whereas, just using <code>pivot</code>, it's purely based on column convention as to what semantic entity it is that you are reshaping.</p>
";;"[""df = pandas.DataFrame(data, columns=['R_Number', 'C_Number', 'Avg', 'Std'])\n\n# Possibly also this if these can always be the indexes:\n# df = df.set_index(['R_Number', 'C_Number'])\n"", ""df.set_index(['R_Number', 'C_Number']).Avg.unstack(level=1)\n""]";"[""df = pandas.DataFrame(data, columns=['R_Number', 'C_Number', 'Avg', 'Std'])\n\n# Possibly also this if these can always be the indexes:\n# df = df.set_index(['R_Number', 'C_Number'])\n"", ""df.set_index(['R_Number', 'C_Number']).Avg.unstack(level=1)\n"", 'pivot']"
949;;0;19973722;11361985.0;2;10;;;"<p>In <code>ipython</code>, I use this to print a part of the dataframe that works quite well (prints the first 100 rows):</p>

<pre><code>print paramdata.head(100).to_string()
</code></pre>
";;['print paramdata.head(100).to_string()\n'];['ipython', 'print paramdata.head(100).to_string()\n']
950;;1;19976286;19914937.0;2;45;;;"<p>Alternatively, you can use numpy underlying function:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; df = pd.DataFrame({""A"": [10,20,30], ""B"": [20, 30, 10]})
&gt;&gt;&gt; df['new_column'] = np.multiply(df['A'], df['B'])
&gt;&gt;&gt; df
    A   B  new_column
0  10  20         200
1  20  30         600
2  30  10         300
</code></pre>

<p>or vectorize arbitrary function in general case:</p>

<pre><code>&gt;&gt;&gt; def fx(x, y):
...     return x*y
...
&gt;&gt;&gt; df['new_column'] = np.vectorize(fx)(df['A'], df['B'])
&gt;&gt;&gt; df
    A   B  new_column
0  10  20         200
1  20  30         600
2  30  10         300
</code></pre>
";;"['>>> import numpy as np\n>>> df = pd.DataFrame({""A"": [10,20,30], ""B"": [20, 30, 10]})\n>>> df[\'new_column\'] = np.multiply(df[\'A\'], df[\'B\'])\n>>> df\n    A   B  new_column\n0  10  20         200\n1  20  30         600\n2  30  10         300\n', "">>> def fx(x, y):\n...     return x*y\n...\n>>> df['new_column'] = np.vectorize(fx)(df['A'], df['B'])\n>>> df\n    A   B  new_column\n0  10  20         200\n1  20  30         600\n2  30  10         300\n""]";"['>>> import numpy as np\n>>> df = pd.DataFrame({""A"": [10,20,30], ""B"": [20, 30, 10]})\n>>> df[\'new_column\'] = np.multiply(df[\'A\'], df[\'B\'])\n>>> df\n    A   B  new_column\n0  10  20         200\n1  20  30         600\n2  30  10         300\n', "">>> def fx(x, y):\n...     return x*y\n...\n>>> df['new_column'] = np.vectorize(fx)(df['A'], df['B'])\n>>> df\n    A   B  new_column\n0  10  20         200\n1  20  30         600\n2  30  10         300\n""]"
951;;4;19991632;19991445.0;2;78;;;"<p>I think you can almost do exactly what you thought would be ideal, using the <a href=""http://statsmodels.sourceforge.net/"">statsmodels</a> package which is one of <code>pandas</code>' optional dependencies (it's used for a few things in <code>pandas.stats</code>.)</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; import statsmodels.formula.api as sm
&gt;&gt;&gt; df = pd.DataFrame({""A"": [10,20,30,40,50], ""B"": [20, 30, 10, 40, 50], ""C"": [32, 234, 23, 23, 42523]})
&gt;&gt;&gt; result = sm.ols(formula=""A ~ B + C"", data=df).fit()
&gt;&gt;&gt; print result.params
Intercept    14.952480
B             0.401182
C             0.000352
dtype: float64
&gt;&gt;&gt; print result.summary()
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      A   R-squared:                       0.579
Model:                            OLS   Adj. R-squared:                  0.158
Method:                 Least Squares   F-statistic:                     1.375
Date:                Thu, 14 Nov 2013   Prob (F-statistic):              0.421
Time:                        20:04:30   Log-Likelihood:                -18.178
No. Observations:                   5   AIC:                             42.36
Df Residuals:                       2   BIC:                             41.19
Df Model:                           2                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept     14.9525     17.764      0.842      0.489       -61.481    91.386
B              0.4012      0.650      0.617      0.600        -2.394     3.197
C              0.0004      0.001      0.650      0.583        -0.002     0.003
==============================================================================
Omnibus:                          nan   Durbin-Watson:                   1.061
Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.498
Skew:                          -0.123   Prob(JB):                        0.780
Kurtosis:                       1.474   Cond. No.                     5.21e+04
==============================================================================

Warnings:
[1] The condition number is large, 5.21e+04. This might indicate that there are
strong multicollinearity or other numerical problems.
</code></pre>
";;"['>>> import pandas as pd\n>>> import statsmodels.formula.api as sm\n>>> df = pd.DataFrame({""A"": [10,20,30,40,50], ""B"": [20, 30, 10, 40, 50], ""C"": [32, 234, 23, 23, 42523]})\n>>> result = sm.ols(formula=""A ~ B + C"", data=df).fit()\n>>> print result.params\nIntercept    14.952480\nB             0.401182\nC             0.000352\ndtype: float64\n>>> print result.summary()\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      A   R-squared:                       0.579\nModel:                            OLS   Adj. R-squared:                  0.158\nMethod:                 Least Squares   F-statistic:                     1.375\nDate:                Thu, 14 Nov 2013   Prob (F-statistic):              0.421\nTime:                        20:04:30   Log-Likelihood:                -18.178\nNo. Observations:                   5   AIC:                             42.36\nDf Residuals:                       2   BIC:                             41.19\nDf Model:                           2                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n------------------------------------------------------------------------------\nIntercept     14.9525     17.764      0.842      0.489       -61.481    91.386\nB              0.4012      0.650      0.617      0.600        -2.394     3.197\nC              0.0004      0.001      0.650      0.583        -0.002     0.003\n==============================================================================\nOmnibus:                          nan   Durbin-Watson:                   1.061\nProb(Omnibus):                    nan   Jarque-Bera (JB):                0.498\nSkew:                          -0.123   Prob(JB):                        0.780\nKurtosis:                       1.474   Cond. No.                     5.21e+04\n==============================================================================\n\nWarnings:\n[1] The condition number is large, 5.21e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n']";"['pandas', 'pandas.stats', '>>> import pandas as pd\n>>> import statsmodels.formula.api as sm\n>>> df = pd.DataFrame({""A"": [10,20,30,40,50], ""B"": [20, 30, 10, 40, 50], ""C"": [32, 234, 23, 23, 42523]})\n>>> result = sm.ols(formula=""A ~ B + C"", data=df).fit()\n>>> print result.params\nIntercept    14.952480\nB             0.401182\nC             0.000352\ndtype: float64\n>>> print result.summary()\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      A   R-squared:                       0.579\nModel:                            OLS   Adj. R-squared:                  0.158\nMethod:                 Least Squares   F-statistic:                     1.375\nDate:                Thu, 14 Nov 2013   Prob (F-statistic):              0.421\nTime:                        20:04:30   Log-Likelihood:                -18.178\nNo. Observations:                   5   AIC:                             42.36\nDf Residuals:                       2   BIC:                             41.19\nDf Model:                           2                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n------------------------------------------------------------------------------\nIntercept     14.9525     17.764      0.842      0.489       -61.481    91.386\nB              0.4012      0.650      0.617      0.600        -2.394     3.197\nC              0.0004      0.001      0.650      0.583        -0.002     0.003\n==============================================================================\nOmnibus:                          nan   Durbin-Watson:                   1.061\nProb(Omnibus):                    nan   Jarque-Bera (JB):                0.498\nSkew:                          -0.123   Prob(JB):                        0.780\nKurtosis:                       1.474   Cond. No.                     5.21e+04\n==============================================================================\n\nWarnings:\n[1] The condition number is large, 5.21e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n']"
952;;4;19996208;19991445.0;2;55;;;"<p><strong>Note:</strong> <code>pandas.stats</code> <a href=""http://pandas-docs.github.io/pandas-docs-travis/whatsnew.html#whatsnew-0200-prior-deprecations"" rel=""nofollow noreferrer"">has been removed</a> with 0.20.0</p>

<hr>

<p>It's possible to do this with <code>pandas.stats.ols</code>:</p>

<pre><code>&gt;&gt;&gt; from pandas.stats.api import ols
&gt;&gt;&gt; df = pd.DataFrame({""A"": [10,20,30,40,50], ""B"": [20, 30, 10, 40, 50], ""C"": [32, 234, 23, 23, 42523]})
&gt;&gt;&gt; res = ols(y=df['A'], x=df[['B','C']])
&gt;&gt;&gt; res
-------------------------Summary of Regression Analysis-------------------------

Formula: Y ~ &lt;B&gt; + &lt;C&gt; + &lt;intercept&gt;

Number of Observations:         5
Number of Degrees of Freedom:   3

R-squared:         0.5789
Adj R-squared:     0.1577

Rmse:             14.5108

F-stat (2, 2):     1.3746, p-value:     0.4211

Degrees of Freedom: model 2, resid 2

-----------------------Summary of Estimated Coefficients------------------------
      Variable       Coef    Std Err     t-stat    p-value    CI 2.5%   CI 97.5%
--------------------------------------------------------------------------------
             B     0.4012     0.6497       0.62     0.5999    -0.8723     1.6746
             C     0.0004     0.0005       0.65     0.5826    -0.0007     0.0014
     intercept    14.9525    17.7643       0.84     0.4886   -19.8655    49.7705
---------------------------------End of Summary---------------------------------
</code></pre>

<p>Note that you need to have <code>statsmodels</code> package installed, it is used internally by the <code>pandas.stats.ols</code> function.</p>
";;"['>>> from pandas.stats.api import ols\n>>> df = pd.DataFrame({""A"": [10,20,30,40,50], ""B"": [20, 30, 10, 40, 50], ""C"": [32, 234, 23, 23, 42523]})\n>>> res = ols(y=df[\'A\'], x=df[[\'B\',\'C\']])\n>>> res\n-------------------------Summary of Regression Analysis-------------------------\n\nFormula: Y ~ <B> + <C> + <intercept>\n\nNumber of Observations:         5\nNumber of Degrees of Freedom:   3\n\nR-squared:         0.5789\nAdj R-squared:     0.1577\n\nRmse:             14.5108\n\nF-stat (2, 2):     1.3746, p-value:     0.4211\n\nDegrees of Freedom: model 2, resid 2\n\n-----------------------Summary of Estimated Coefficients------------------------\n      Variable       Coef    Std Err     t-stat    p-value    CI 2.5%   CI 97.5%\n--------------------------------------------------------------------------------\n             B     0.4012     0.6497       0.62     0.5999    -0.8723     1.6746\n             C     0.0004     0.0005       0.65     0.5826    -0.0007     0.0014\n     intercept    14.9525    17.7643       0.84     0.4886   -19.8655    49.7705\n---------------------------------End of Summary---------------------------------\n']";"['pandas.stats', 'pandas.stats.ols', '>>> from pandas.stats.api import ols\n>>> df = pd.DataFrame({""A"": [10,20,30,40,50], ""B"": [20, 30, 10, 40, 50], ""C"": [32, 234, 23, 23, 42523]})\n>>> res = ols(y=df[\'A\'], x=df[[\'B\',\'C\']])\n>>> res\n-------------------------Summary of Regression Analysis-------------------------\n\nFormula: Y ~ <B> + <C> + <intercept>\n\nNumber of Observations:         5\nNumber of Degrees of Freedom:   3\n\nR-squared:         0.5789\nAdj R-squared:     0.1577\n\nRmse:             14.5108\n\nF-stat (2, 2):     1.3746, p-value:     0.4211\n\nDegrees of Freedom: model 2, resid 2\n\n-----------------------Summary of Estimated Coefficients------------------------\n      Variable       Coef    Std Err     t-stat    p-value    CI 2.5%   CI 97.5%\n--------------------------------------------------------------------------------\n             B     0.4012     0.6497       0.62     0.5999    -0.8723     1.6746\n             C     0.0004     0.0005       0.65     0.5826    -0.0007     0.0014\n     intercept    14.9525    17.7643       0.84     0.4886   -19.8655    49.7705\n---------------------------------End of Summary---------------------------------\n', 'statsmodels', 'pandas.stats.ols']"
953;;1;20006954;20003290.0;2;23;;;"<p>Change the type of column ""vals"" prior to exporting the data frame to a CSV file</p>

<pre><code>df_data['vals'] = df_data['vals'].map(lambda x: '%2.1f' % x)

df_data.to_csv(outfile, index=False, header=False, float_format='%11.6f')
</code></pre>
";;"[""df_data['vals'] = df_data['vals'].map(lambda x: '%2.1f' % x)\n\ndf_data.to_csv(outfile, index=False, header=False, float_format='%11.6f')\n""]";"[""df_data['vals'] = df_data['vals'].map(lambda x: '%2.1f' % x)\n\ndf_data.to_csv(outfile, index=False, header=False, float_format='%11.6f')\n""]"
954;;6;20019449;19991445.0;2;10;;;"<blockquote>
  <p>This would require me to reformat the data into lists inside lists, which seems to defeat the purpose of using pandas in the first place.</p>
</blockquote>

<p>No it doesn't, just convert to a NumPy array:</p>

<pre><code>&gt;&gt;&gt; data = np.asarray(df)
</code></pre>

<p>This takes constant time because it just creates a <em>view</em> on your data. Then feed it to scikit-learn:</p>

<pre><code>&gt;&gt;&gt; from sklearn.linear_model import LinearRegression
&gt;&gt;&gt; lr = LinearRegression()
&gt;&gt;&gt; X, y = data[:, 1:], data[:, 0]
&gt;&gt;&gt; lr.fit(X, y)
LinearRegression(copy_X=True, fit_intercept=True, normalize=False)
&gt;&gt;&gt; lr.coef_
array([  4.01182386e-01,   3.51587361e-04])
&gt;&gt;&gt; lr.intercept_
14.952479503953672
</code></pre>
";;['>>> data = np.asarray(df)\n', '>>> from sklearn.linear_model import LinearRegression\n>>> lr = LinearRegression()\n>>> X, y = data[:, 1:], data[:, 0]\n>>> lr.fit(X, y)\nLinearRegression(copy_X=True, fit_intercept=True, normalize=False)\n>>> lr.coef_\narray([  4.01182386e-01,   3.51587361e-04])\n>>> lr.intercept_\n14.952479503953672\n'];['>>> data = np.asarray(df)\n', '>>> from sklearn.linear_model import LinearRegression\n>>> lr = LinearRegression()\n>>> X, y = data[:, 1:], data[:, 0]\n>>> lr.fit(X, y)\nLinearRegression(copy_X=True, fit_intercept=True, normalize=False)\n>>> lr.coef_\narray([  4.01182386e-01,   3.51587361e-04])\n>>> lr.intercept_\n14.952479503953672\n']
955;;1;20033218;20033111.0;2;11;;;"<pre><code>&gt;&gt;&gt; frame['HighScore'] = frame[['test1','test2','test3']].apply(max, axis=1)
&gt;&gt;&gt; frame
    name  test1  test2  test3  HighScore
0   bill     85     35     51        85
1    joe     75     45     61        75
2  steve     85     83     45        85
</code></pre>
";;"["">>> frame['HighScore'] = frame[['test1','test2','test3']].apply(max, axis=1)\n>>> frame\n    name  test1  test2  test3  HighScore\n0   bill     85     35     51        85\n1    joe     75     45     61        75\n2  steve     85     83     45        85\n""]";"["">>> frame['HighScore'] = frame[['test1','test2','test3']].apply(max, axis=1)\n>>> frame\n    name  test1  test2  test3  HighScore\n0   bill     85     35     51        85\n1    joe     75     45     61        75\n2  steve     85     83     45        85\n""]"
956;;2;20033232;20033111.0;2;59;;;"<pre><code>&gt;&gt;&gt; frame['HighScore'] = frame[['test1','test2','test3']].max(axis=1)
&gt;&gt;&gt; frame
    name  test1  test2  test3  HighScore
0   bill     85     35     51         85
1    joe     75     45     61         75
2  steve     85     83     45         85
</code></pre>
";;"["">>> frame['HighScore'] = frame[['test1','test2','test3']].max(axis=1)\n>>> frame\n    name  test1  test2  test3  HighScore\n0   bill     85     35     51         85\n1    joe     75     45     61         75\n2  steve     85     83     45         85\n""]";"["">>> frame['HighScore'] = frame[['test1','test2','test3']].max(axis=1)\n>>> frame\n    name  test1  test2  test3  HighScore\n0   bill     85     35     51         85\n1    joe     75     45     61         75\n2  steve     85     83     45         85\n""]"
957;;0;20039057;14940743.0;2;44;;;"<p>You don't really need to convert that into a set:</p>

<pre><code>cols = [col for col in df.columns if col not in ['B', 'D']]
df2 = df[cols]
</code></pre>
";;"[""cols = [col for col in df.columns if col not in ['B', 'D']]\ndf2 = df[cols]\n""]";"[""cols = [col for col in df.columns if col not in ['B', 'D']]\ndf2 = df[cols]\n""]"
958;;0;20051631;15723628.0;2;7;;;"<p>There's also pd.factorize function to use:</p>

<pre><code># use the df data from @herrfz

In [150]: pd.factorize(df.b)
Out[150]: (array([0, 1, 0, 1, 2]), array(['yes', 'no', 'absent'], dtype=object))
In [152]: df['c'] = pd.factorize(df.b)[0]

In [153]: df
Out[153]: 
   a       b  c
0  1     yes  0
1  2      no  1
2  3     yes  0
3  4      no  1
4  5  absent  2
</code></pre>
";;"[""# use the df data from @herrfz\n\nIn [150]: pd.factorize(df.b)\nOut[150]: (array([0, 1, 0, 1, 2]), array(['yes', 'no', 'absent'], dtype=object))\nIn [152]: df['c'] = pd.factorize(df.b)[0]\n\nIn [153]: df\nOut[153]: \n   a       b  c\n0  1     yes  0\n1  2      no  1\n2  3     yes  0\n3  4      no  1\n4  5  absent  2\n""]";"[""# use the df data from @herrfz\n\nIn [150]: pd.factorize(df.b)\nOut[150]: (array([0, 1, 0, 1, 2]), array(['yes', 'no', 'absent'], dtype=object))\nIn [152]: df['c'] = pd.factorize(df.b)[0]\n\nIn [153]: df\nOut[153]: \n   a       b  c\n0  1     yes  0\n1  2      no  1\n2  3     yes  0\n3  4      no  1\n4  5  absent  2\n""]"
959;;4;20067665;20067636.0;2;78;;;"<pre><code>&gt;&gt;&gt; df.groupby('id').first()
     value
id        
1    first
2    first
3    first
4   second
5    first
6    first
7   fourth
</code></pre>

<p>If you need <code>id</code> as column:</p>

<pre><code>&gt;&gt;&gt; df.groupby('id').first().reset_index()
   id   value
0   1   first
1   2   first
2   3   first
3   4  second
4   5   first
5   6   first
6   7  fourth
</code></pre>

<p>To get n first records, you can use head():</p>

<pre><code>&gt;&gt;&gt; df.groupby('id').head(2).reset_index(drop=True)
    id   value
0    1   first
1    1  second
2    2   first
3    2  second
4    3   first
5    3   third
6    4  second
7    4   fifth
8    5   first
9    6   first
10   6  second
11   7  fourth
12   7   fifth
</code></pre>
";;"["">>> df.groupby('id').first()\n     value\nid        \n1    first\n2    first\n3    first\n4   second\n5    first\n6    first\n7   fourth\n"", "">>> df.groupby('id').first().reset_index()\n   id   value\n0   1   first\n1   2   first\n2   3   first\n3   4  second\n4   5   first\n5   6   first\n6   7  fourth\n"", "">>> df.groupby('id').head(2).reset_index(drop=True)\n    id   value\n0    1   first\n1    1  second\n2    2   first\n3    2  second\n4    3   first\n5    3   third\n6    4  second\n7    4   fifth\n8    5   first\n9    6   first\n10   6  second\n11   7  fourth\n12   7   fifth\n""]";"["">>> df.groupby('id').first()\n     value\nid        \n1    first\n2    first\n3    first\n4   second\n5    first\n6    first\n7   fourth\n"", 'id', "">>> df.groupby('id').first().reset_index()\n   id   value\n0   1   first\n1   2   first\n2   3   first\n3   4  second\n4   5   first\n5   6   first\n6   7  fourth\n"", "">>> df.groupby('id').head(2).reset_index(drop=True)\n    id   value\n0    1   first\n1    1  second\n2    2   first\n3    2  second\n4    3   first\n5    3   third\n6    4  second\n7    4   fifth\n8    5   first\n9    6   first\n10   6  second\n11   7  fourth\n12   7   fifth\n""]"
960;;11;20069379;20069009.0;2;61;;;"<p>Did you try <code>df.groupby('id').head(2)</code></p>

<p>Ouput generated: </p>

<pre><code>&gt;&gt;&gt; df.groupby('id').head(2)
       id  value
id             
1  0   1      1
   1   1      2 
2  3   2      1
   4   2      2
3  7   3      1
4  8   4      1
</code></pre>

<p>(Keep in mind that you might need to order/sort before, depending on your data)</p>

<p>EDIT: As mentioned by the questioner, use <code>df.groupby('id').head(2).reset_index(drop=True)</code> to remove the multindex and flatten the results.</p>

<pre><code>&gt;&gt;&gt; df.groupby('id').head(2).reset_index(drop=True)
    id  value
0   1      1
1   1      2
2   2      1
3   2      2
4   3      1
5   4      1
</code></pre>
";;"["">>> df.groupby('id').head(2)\n       id  value\nid             \n1  0   1      1\n   1   1      2 \n2  3   2      1\n   4   2      2\n3  7   3      1\n4  8   4      1\n"", "">>> df.groupby('id').head(2).reset_index(drop=True)\n    id  value\n0   1      1\n1   1      2\n2   2      1\n3   2      2\n4   3      1\n5   4      1\n""]";"[""df.groupby('id').head(2)"", "">>> df.groupby('id').head(2)\n       id  value\nid             \n1  0   1      1\n   1   1      2 \n2  3   2      1\n   4   2      2\n3  7   3      1\n4  8   4      1\n"", ""df.groupby('id').head(2).reset_index(drop=True)"", "">>> df.groupby('id').head(2).reset_index(drop=True)\n    id  value\n0   1      1\n1   1      2\n2   2      1\n3   2      2\n4   3      1\n5   4      1\n""]"
961;;1;20076611;20076195.0;2;62;;;"<p>I think <code>df['word'].value_counts()</code> should serve. By skipping the groupby machinery, you'll save some time. I'm not sure why <code>count</code> should be much slower than <code>max</code>. Both take some time to avoid missing values. (Compare with <code>size</code>.)</p>

<p>In any case, value_counts has been <a href=""https://github.com/pydata/pandas/commit/d1a1fa27e520c0d196981df58edb1bd5d3cc3161"">specifically optimized</a> to handle object type, like your words, so I doubt you'll do much better than that.</p>
";;[];"[""df['word'].value_counts()"", 'count', 'max', 'size']"
962;;21;20084843;20083098.0;2;7;;;"<p>Here is a similar comparison I just did. Its about 1/3 of the data 10M rows. The final size is abou 1.3GB</p>

<p>I define 3 timing functions:</p>

<p>Test the Fixed format (called Storer in 0.12). This writes in a PyTables Array format</p>

<pre><code>def f(df):
    store = pd.HDFStore('test.h5','w')
    store['df'] = df
    store.close()
</code></pre>

<p>Write in the Table format, using PyTables Table format. Do not create an index.</p>

<pre><code>def f2(df):
    store = pd.HDFStore('test.h5','w')
    store.append('df',df,index=False)
    store.close()
</code></pre>

<p>Same as f2, but create an index (which is normally done)</p>

<pre><code>def f3(df):
    store = pd.HDFStore('test.h5','w')
    store.append('df',df)
    store.close()
</code></pre>

<p>Create the frame</p>

<pre><code>In [25]: df = concat([DataFrame(np.random.randn(10000000,10)),DataFrame(np.random.randint(0,10,size=50000000).reshape(10000000,5))],axis=1)

In [26]: df
Out[26]:
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 10000000 entries, 0 to 9999999
Columns: 15 entries, 0 to 4
dtypes: float64(10), int64(5)


v0.12.0

In [27]: %timeit f(df)
1 loops, best of 3: 14.7 s per loop

In [28]: %timeit f2(df)
1 loops, best of 3: 32 s per loop

In [29]: %timeit f3(df)
1 loops, best of 3: 40.1 s per loop

master/v0.13.0

In [5]: %timeit f(df)
1 loops, best of 3: 12.9 s per loop

In [6]: %timeit f2(df)
1 loops, best of 3: 17.5 s per loop

In [7]: %timeit f3(df)
1 loops, best of 3: 24.3 s per loop
</code></pre>

<p>Timing Runs with the same file as provided by the OP (link is below)</p>

<pre><code>In [4]: df = pd.read_hdf('test.h5','df')

In [5]: df
Out[5]: 
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 28880943 entries, 0 to 28880942
Columns: 14 entries, node_id to kernel_type
dtypes: float64(4), int64(10)
</code></pre>

<p>Like f1, Fixed format</p>

<pre><code>In [6]: %timeit df.to_hdf('test.hdf','df',mode='w')
1 loops, best of 3: 36.2 s per loop
</code></pre>

<p>Like f2, Table format, no index</p>

<pre><code>In [7]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',index=False)
1 loops, best of 3: 45 s per loop

In [8]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',index=False,chunksize=2000000)
1 loops, best of 3: 44.5 s per loop
</code></pre>

<p>Like f3, Table format with index</p>

<pre><code>In [9]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',chunksize=2000000)
1 loops, best of 3: 1min 36s per loop
</code></pre>

<p>Like f3, Table format with index, compressed with blosc</p>

<pre><code>In [10]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',chunksize=2000000,complib='blosc')
1 loops, best of 3: 46.5 s per loop

In [11]: %timeit pd.read_hdf('test.hdf','df')
1 loops, best of 3: 10.8 s per loop
</code></pre>

<p>Show original file (test.h5, and compressed, test.hdf)</p>

<pre><code>In [13]: !ls -ltr test.h*
-rw-r--r-- 1 jreback users 3471518282 Nov 20 18:20 test.h5
-rw-rw-r-- 1 jreback users  649327780 Nov 20 21:17 test.hdf
</code></pre>

<p>Several points to note.</p>

<ul>
<li><p>Not creating an index can make a non-trivial difference in time. I also believe that if you have a string based index it can substantially worsen write time. That said, you always want to create an index to make retrieval very fast.</p>

<p>You didn't include what your index is, nor whether its sorted (though I only think this makes a small difference).</p></li>
<li><p>the write penalty in my examples are roughly 2x (though I have seen it be somewhat bigger when INCLUDING the index time). Thus your 7s (1/2 of my time), for 3x the number I am writing is quite suspect. I am using a reasonably fast disk array. If you were using a flash based disk then this is possible, though.</p></li>
<li><p>master/v0.13.0 (release very soon), improves the write times on tables substantially.</p></li>
<li><p>you can try setting the <code>chunksize</code> parameter to a bigger number when you write the data (its default is 100000). The purpose of the 'relatively' low number is to have a constant memory usage. (e.g. if is bigger you will use more memory, in theory it should write faster though).</p></li>
<li><p>Tables offer 2 advantages over Fixed format: 1) query retrieval, and 2) appendability. Reading the entire table doesn't take advantage of either, so if you ONLY want to read the entire table, then Fixed format is recommended. (In my experience the flexibility of Tables greatly outweights the write penalty, but YMMV)</p></li>
</ul>

<p>Bottom line is to repeat the timings (use ipython as it will run multiple tests). If you can reproduce your results, then pls post a %prun and I'll take a look.</p>

<p>Update:</p>

<p>so the recommended way for a table this size is to compress with blosc and use pandas master/0.13.0 along with PyTables 3.0.0</p>
";;"[""def f(df):\n    store = pd.HDFStore('test.h5','w')\n    store['df'] = df\n    store.close()\n"", ""def f2(df):\n    store = pd.HDFStore('test.h5','w')\n    store.append('df',df,index=False)\n    store.close()\n"", ""def f3(df):\n    store = pd.HDFStore('test.h5','w')\n    store.append('df',df)\n    store.close()\n"", ""In [25]: df = concat([DataFrame(np.random.randn(10000000,10)),DataFrame(np.random.randint(0,10,size=50000000).reshape(10000000,5))],axis=1)\n\nIn [26]: df\nOut[26]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10000000 entries, 0 to 9999999\nColumns: 15 entries, 0 to 4\ndtypes: float64(10), int64(5)\n\n\nv0.12.0\n\nIn [27]: %timeit f(df)\n1 loops, best of 3: 14.7 s per loop\n\nIn [28]: %timeit f2(df)\n1 loops, best of 3: 32 s per loop\n\nIn [29]: %timeit f3(df)\n1 loops, best of 3: 40.1 s per loop\n\nmaster/v0.13.0\n\nIn [5]: %timeit f(df)\n1 loops, best of 3: 12.9 s per loop\n\nIn [6]: %timeit f2(df)\n1 loops, best of 3: 17.5 s per loop\n\nIn [7]: %timeit f3(df)\n1 loops, best of 3: 24.3 s per loop\n"", ""In [4]: df = pd.read_hdf('test.h5','df')\n\nIn [5]: df\nOut[5]: \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 28880943 entries, 0 to 28880942\nColumns: 14 entries, node_id to kernel_type\ndtypes: float64(4), int64(10)\n"", ""In [6]: %timeit df.to_hdf('test.hdf','df',mode='w')\n1 loops, best of 3: 36.2 s per loop\n"", ""In [7]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',index=False)\n1 loops, best of 3: 45 s per loop\n\nIn [8]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',index=False,chunksize=2000000)\n1 loops, best of 3: 44.5 s per loop\n"", ""In [9]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',chunksize=2000000)\n1 loops, best of 3: 1min 36s per loop\n"", ""In [10]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',chunksize=2000000,complib='blosc')\n1 loops, best of 3: 46.5 s per loop\n\nIn [11]: %timeit pd.read_hdf('test.hdf','df')\n1 loops, best of 3: 10.8 s per loop\n"", 'In [13]: !ls -ltr test.h*\n-rw-r--r-- 1 jreback users 3471518282 Nov 20 18:20 test.h5\n-rw-rw-r-- 1 jreback users  649327780 Nov 20 21:17 test.hdf\n']";"[""def f(df):\n    store = pd.HDFStore('test.h5','w')\n    store['df'] = df\n    store.close()\n"", ""def f2(df):\n    store = pd.HDFStore('test.h5','w')\n    store.append('df',df,index=False)\n    store.close()\n"", ""def f3(df):\n    store = pd.HDFStore('test.h5','w')\n    store.append('df',df)\n    store.close()\n"", ""In [25]: df = concat([DataFrame(np.random.randn(10000000,10)),DataFrame(np.random.randint(0,10,size=50000000).reshape(10000000,5))],axis=1)\n\nIn [26]: df\nOut[26]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10000000 entries, 0 to 9999999\nColumns: 15 entries, 0 to 4\ndtypes: float64(10), int64(5)\n\n\nv0.12.0\n\nIn [27]: %timeit f(df)\n1 loops, best of 3: 14.7 s per loop\n\nIn [28]: %timeit f2(df)\n1 loops, best of 3: 32 s per loop\n\nIn [29]: %timeit f3(df)\n1 loops, best of 3: 40.1 s per loop\n\nmaster/v0.13.0\n\nIn [5]: %timeit f(df)\n1 loops, best of 3: 12.9 s per loop\n\nIn [6]: %timeit f2(df)\n1 loops, best of 3: 17.5 s per loop\n\nIn [7]: %timeit f3(df)\n1 loops, best of 3: 24.3 s per loop\n"", ""In [4]: df = pd.read_hdf('test.h5','df')\n\nIn [5]: df\nOut[5]: \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 28880943 entries, 0 to 28880942\nColumns: 14 entries, node_id to kernel_type\ndtypes: float64(4), int64(10)\n"", ""In [6]: %timeit df.to_hdf('test.hdf','df',mode='w')\n1 loops, best of 3: 36.2 s per loop\n"", ""In [7]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',index=False)\n1 loops, best of 3: 45 s per loop\n\nIn [8]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',index=False,chunksize=2000000)\n1 loops, best of 3: 44.5 s per loop\n"", ""In [9]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',chunksize=2000000)\n1 loops, best of 3: 1min 36s per loop\n"", ""In [10]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',chunksize=2000000,complib='blosc')\n1 loops, best of 3: 46.5 s per loop\n\nIn [11]: %timeit pd.read_hdf('test.hdf','df')\n1 loops, best of 3: 10.8 s per loop\n"", 'In [13]: !ls -ltr test.h*\n-rw-r--r-- 1 jreback users 3471518282 Nov 20 18:20 test.h5\n-rw-rw-r-- 1 jreback users  649327780 Nov 20 21:17 test.hdf\n', 'chunksize']"
963;;5;20084895;20084382.0;2;70;;;"<pre><code>In [1]: df = DataFrame(np.random.randint(0,10,size=100).reshape(10,10))

In [2]: df
Out[2]: 
   0  1  2  3  4  5  6  7  8  9
0  2  2  3  2  6  1  9  9  3  3
1  1  2  5  8  5  2  5  0  6  3
2  0  7  0  7  5  5  9  1  0  3
3  5  3  2  3  7  6  8  3  8  4
4  8  0  2  2  3  9  7  1  2  7
5  3  2  8  5  6  4  3  7  0  8
6  4  2  6  5  3  3  4  5  3  2
7  7  6  0  6  6  7  1  7  5  1
8  7  4  3  1  0  6  9  7  7  3
9  5  3  4  5  2  0  8  6  4  7

In [13]: Series(df.values.ravel()).unique()
Out[13]: array([9, 1, 4, 6, 0, 7, 5, 8, 3, 2])
</code></pre>

<p>Numpy unique sorts, so its faster to do it this way (and then sort if you need to)</p>

<pre><code>In [14]: df = DataFrame(np.random.randint(0,10,size=10000).reshape(100,100))

In [15]: %timeit Series(df.values.ravel()).unique()
10000 loops, best of 3: 137 ?s per loop

In [16]: %timeit np.unique(df.values.ravel())
1000 loops, best of 3: 270 ?s per loop
</code></pre>
";;['In [1]: df = DataFrame(np.random.randint(0,10,size=100).reshape(10,10))\n\nIn [2]: df\nOut[2]: \n   0  1  2  3  4  5  6  7  8  9\n0  2  2  3  2  6  1  9  9  3  3\n1  1  2  5  8  5  2  5  0  6  3\n2  0  7  0  7  5  5  9  1  0  3\n3  5  3  2  3  7  6  8  3  8  4\n4  8  0  2  2  3  9  7  1  2  7\n5  3  2  8  5  6  4  3  7  0  8\n6  4  2  6  5  3  3  4  5  3  2\n7  7  6  0  6  6  7  1  7  5  1\n8  7  4  3  1  0  6  9  7  7  3\n9  5  3  4  5  2  0  8  6  4  7\n\nIn [13]: Series(df.values.ravel()).unique()\nOut[13]: array([9, 1, 4, 6, 0, 7, 5, 8, 3, 2])\n', 'In [14]: df = DataFrame(np.random.randint(0,10,size=10000).reshape(100,100))\n\nIn [15]: %timeit Series(df.values.ravel()).unique()\n10000 loops, best of 3: 137 ?s per loop\n\nIn [16]: %timeit np.unique(df.values.ravel())\n1000 loops, best of 3: 270 ?s per loop\n'];['In [1]: df = DataFrame(np.random.randint(0,10,size=100).reshape(10,10))\n\nIn [2]: df\nOut[2]: \n   0  1  2  3  4  5  6  7  8  9\n0  2  2  3  2  6  1  9  9  3  3\n1  1  2  5  8  5  2  5  0  6  3\n2  0  7  0  7  5  5  9  1  0  3\n3  5  3  2  3  7  6  8  3  8  4\n4  8  0  2  2  3  9  7  1  2  7\n5  3  2  8  5  6  4  3  7  0  8\n6  4  2  6  5  3  3  4  5  3  2\n7  7  6  0  6  6  7  1  7  5  1\n8  7  4  3  1  0  6  9  7  7  3\n9  5  3  4  5  2  0  8  6  4  7\n\nIn [13]: Series(df.values.ravel()).unique()\nOut[13]: array([9, 1, 4, 6, 0, 7, 5, 8, 3, 2])\n', 'In [14]: df = DataFrame(np.random.randint(0,10,size=10000).reshape(100,100))\n\nIn [15]: %timeit Series(df.values.ravel()).unique()\n10000 loops, best of 3: 137 ?s per loop\n\nIn [16]: %timeit np.unique(df.values.ravel())\n1000 loops, best of 3: 270 ?s per loop\n']
964;;1;20096827;20095673.0;2;49;;;"<pre><code>In [44]: df.gdp = df.gdp.shift(-1)

In [45]: df
Out[45]: 
   y  gdp  cap
0  1    3    5
1  2    7    9
2  8    4    2
3  3    7    7
4  6  NaN    7

In [46]: df[:-1]                                                                                                                                                                                                                                                                                                               
Out[46]: 
   y  gdp  cap
0  1    3    5
1  2    7    9
2  8    4    2
3  3    7    7
</code></pre>
";;['In [44]: df.gdp = df.gdp.shift(-1)\n\nIn [45]: df\nOut[45]: \n   y  gdp  cap\n0  1    3    5\n1  2    7    9\n2  8    4    2\n3  3    7    7\n4  6  NaN    7\n\nIn [46]: df[:-1]                                                                                                                                                                                                                                                                                                               \nOut[46]: \n   y  gdp  cap\n0  1    3    5\n1  2    7    9\n2  8    4    2\n3  3    7    7\n'];['In [44]: df.gdp = df.gdp.shift(-1)\n\nIn [45]: df\nOut[45]: \n   y  gdp  cap\n0  1    3    5\n1  2    7    9\n2  8    4    2\n3  3    7    7\n4  6  NaN    7\n\nIn [46]: df[:-1]                                                                                                                                                                                                                                                                                                               \nOut[46]: \n   y  gdp  cap\n0  1    3    5\n1  2    7    9\n2  8    4    2\n3  3    7    7\n']
965;;5;20107825;20107570.0;2;22;;;"<p>DataFrames and Series always have an index. Although it displays alongside the column(s), it is not a column, which is why del df['index'] did not work.</p>

<p>If you want to replace the index with simple sequential numbers, use <code>df.reset_index()</code>. I strongly suggest reading a little bit of the pandas documentation, like <a href=""http://pandas.pydata.org/pandas-docs/stable/10min.html"">10 minutes to Pandas</a> to get a sense for why the index is there is how it is used. </p>
";;[];['df.reset_index()']
966;;2;20159305;20109391.0;2;100;;;"<p><em>Note: The ideas here are pretty generic for StackOverflow, indeed <a href=""http://sscce.org/"" rel=""nofollow noreferrer"">questions</a>.</em></p>

<h3>Disclaimer: Writing a good question is HARD.</h3>

<h2>The Good:</h2>

<ul>
<li><p>do include small* example DataFrame, either as runnable code:</p>

<pre><code>In [1]: df = pd.DataFrame([[1, 2], [1, 3], [4, 6]], columns=['A', 'B'])
</code></pre>

<p>or make it ""copy and pasteable"" using <code>pd.read_clipboard(sep='\s\s+')</code>, you can format the text for StackOverflow highlight and use Ctrl+K (or prepend four spaces to each line):</p>

<pre><code>In [2]: df
Out[2]: 
   A  B
0  1  2
1  1  3
2  4  6
</code></pre>

<p>test <code>pd.read_clipboard(sep='\s\s+')</code> yourself.</p>

<p>* <em>I really do mean <strong>small</strong>, the vast majority of example DataFrames could be fewer than 6 rows<sup>citation needed</sup>, and <strong>I bet I can do it in 5 rows.</strong> Can you reproduce the error with <code>df = df.head()</code>, if not fiddle around to see if you can make up a small DataFrame which exhibits the issue you are facing.</em></p>

<p>* <em>Every rule has an exception, the obvious one is for performance issues  (<a href=""http://ipython.org/ipython-doc/dev/interactive/tutorial.html#magic-functions"" rel=""nofollow noreferrer"">in which case definitely use %timeit and possibly %prun</a>), where you should generate (consider using np.random.seed so we have the exact same frame): <code>df = pd.DataFrame(np.random.randn(100000000, 10))</code>. Saying that, ""make this code fast for me"" is not strictly on topic for the site...</em></p></li>
<li><p>write out the outcome you desire (similarly to above)</p>

<pre><code>In [3]: iwantthis
Out[3]: 
   A  B
0  1  5
1  4  6
</code></pre>

<p><em>Explain what the numbers come from: the 5 is sum of the B column for the rows where A is 1.</em></p></li>
<li><p>do show <em>the code</em> you've tried:</p>

<pre><code>In [4]: df.groupby('A').sum()
Out[4]: 
   B
A   
1  5
4  6
</code></pre>

<p><em>But say what's incorrect: the A column is in the index rather than a column.</em></p></li>
<li><p>do show you've done some research (<a href=""http://pandas.pydata.org/pandas-docs/stable/search.html?q=groupby+sum"" rel=""nofollow noreferrer"">search the docs</a>, <a href=""https://stackoverflow.com/search?q=[pandas]+groupby+sum"">search StackOverflow</a>), give a summary:</p>

<blockquote>
  <p>The docstring for sum simply states ""Compute sum of group values""</p>
  
  <p>The <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#cython-optimized-aggregation-functions"" rel=""nofollow noreferrer"">groupby docs</a> don't give any examples for this.</p>
</blockquote>

<p><em>Aside: the answer here is to use <code>df.groupby('A', as_index=False).sum()</code>.</em></p></li>
<li><p>if it's relevant that you have Timestamp columns, e.g. you're resampling or something, then be explicit and apply <code>pd.to_datetime</code> to them for good measure**.</p>

<pre><code>df['date'] = pd.to_datetime(df['date']) # this column ought to be date..
</code></pre>

<p>** <em>Sometimes this is the issue itself: they were strings.</em></p></li>
</ul>

<h2>The Bad:</h2>

<ul>
<li><p>don't include a MultiIndex, which <strong>we can't copy and paste</strong> (see above), this is kind of a grievance with pandas default display but nonetheless annoying:</p>

<pre><code>In [11]: df
Out[11]:
     C
A B   
1 2  3
  2  6
</code></pre>

<p><em>The correct way is to include an ordinary DataFrame with a <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>set_index</code></a> call:</em></p>

<pre><code>In [12]: df = pd.DataFrame([[1, 2, 3], [1, 2, 6]], columns=['A', 'B', 'C']).set_index(['A', 'B'])

In [13]: df
Out[13]: 
     C
A B   
1 2  3
  2  6
</code></pre></li>
<li><p>do provide insight to what it is when giving the outcome you want:</p>

<pre><code>   B
A   
1  1
5  0
</code></pre>

<p><em>Be specific about how you got the numbers (what are they)... double check they're correct.</em></p></li>
<li><p>If your code throws an error, do include the entire stacktrace (this can be edited out later if it's too noisy). Show the line number (and the corresponding line of your code which it's raising against).</p></li>
</ul>

<h2>The Ugly:</h2>

<ul>
<li><p>don't link to a csv we don't have access to (ideally don't link to an external source at all...)</p>

<pre><code>df = pd.read_csv('my_secret_file.csv')  # ideally with lots of parsing options
</code></pre>

<p><em><strong>Most data is proprietary</strong> we get that: Make up similar data and see if you can reproduce the problem (something small).</em></p></li>
<li><p>don't explain the situation vaguely in words, like you have a DataFrame which is ""large"", mention some of the column names in passing (be sure not to mention their dtypes). Try and go into lots of detail about something which is completely meaningless without seeing the actual context. Presumably noone is even going to read to the end of this paragraph.</p>

<p><em>Essays are bad, it's easier with small examples.</em></p></li>
<li><p>don't include 10+ (100+??) lines of data munging before getting to your actual question.</p>

<p><em>Please, we see enough of this in our day jobs. We want to help, but <a href=""https://www.youtube.com/watch?v=ECfRp-jwbI4"" rel=""nofollow noreferrer"">not like this...</a>.</em><br>
<em>Cut the intro, and just show the relevant DataFrames (or small versions of them) in the step which is causing you trouble.</em></p></li>
</ul>

<h3>Anyways, have fun learning python, numpy and pandas!</h3>
";;"[""In [1]: df = pd.DataFrame([[1, 2], [1, 3], [4, 6]], columns=['A', 'B'])\n"", 'In [2]: df\nOut[2]: \n   A  B\n0  1  2\n1  1  3\n2  4  6\n', 'In [3]: iwantthis\nOut[3]: \n   A  B\n0  1  5\n1  4  6\n', ""In [4]: df.groupby('A').sum()\nOut[4]: \n   B\nA   \n1  5\n4  6\n"", ""df['date'] = pd.to_datetime(df['date']) # this column ought to be date..\n"", 'In [11]: df\nOut[11]:\n     C\nA B   \n1 2  3\n  2  6\n', ""In [12]: df = pd.DataFrame([[1, 2, 3], [1, 2, 6]], columns=['A', 'B', 'C']).set_index(['A', 'B'])\n\nIn [13]: df\nOut[13]: \n     C\nA B   \n1 2  3\n  2  6\n"", '   B\nA   \n1  1\n5  0\n', ""df = pd.read_csv('my_secret_file.csv')  # ideally with lots of parsing options\n""]";"[""In [1]: df = pd.DataFrame([[1, 2], [1, 3], [4, 6]], columns=['A', 'B'])\n"", ""pd.read_clipboard(sep='\\s\\s+')"", 'In [2]: df\nOut[2]: \n   A  B\n0  1  2\n1  1  3\n2  4  6\n', ""pd.read_clipboard(sep='\\s\\s+')"", 'df = df.head()', 'df = pd.DataFrame(np.random.randn(100000000, 10))', 'In [3]: iwantthis\nOut[3]: \n   A  B\n0  1  5\n1  4  6\n', ""In [4]: df.groupby('A').sum()\nOut[4]: \n   B\nA   \n1  5\n4  6\n"", ""df.groupby('A', as_index=False).sum()"", 'pd.to_datetime', ""df['date'] = pd.to_datetime(df['date']) # this column ought to be date..\n"", 'In [11]: df\nOut[11]:\n     C\nA B   \n1 2  3\n  2  6\n', 'set_index', ""In [12]: df = pd.DataFrame([[1, 2, 3], [1, 2, 6]], columns=['A', 'B', 'C']).set_index(['A', 'B'])\n\nIn [13]: df\nOut[13]: \n     C\nA B   \n1 2  3\n  2  6\n"", '   B\nA   \n1  1\n5  0\n', ""df = pd.read_csv('my_secret_file.csv')  # ideally with lots of parsing options\n""]"
967;;8;20221655;20219254.0;2;61;;;"<p>Pandas docs says it uses openpyxl for xlsx files. Quick look through the code in <code>ExcelWriter</code> gives a clue that something like this might work out:</p>

<pre><code>import pandas
from openpyxl import load_workbook

book = load_workbook('Masterfile.xlsx')
writer = pandas.ExcelWriter('Masterfile.xlsx', engine='openpyxl') 
writer.book = book
writer.sheets = dict((ws.title, ws) for ws in book.worksheets)

data_filtered.to_excel(writer, ""Main"", cols=['Diff1', 'Diff2'])

writer.save()
</code></pre>
";;"['import pandas\nfrom openpyxl import load_workbook\n\nbook = load_workbook(\'Masterfile.xlsx\')\nwriter = pandas.ExcelWriter(\'Masterfile.xlsx\', engine=\'openpyxl\') \nwriter.book = book\nwriter.sheets = dict((ws.title, ws) for ws in book.worksheets)\n\ndata_filtered.to_excel(writer, ""Main"", cols=[\'Diff1\', \'Diff2\'])\n\nwriter.save()\n']";"['ExcelWriter', 'import pandas\nfrom openpyxl import load_workbook\n\nbook = load_workbook(\'Masterfile.xlsx\')\nwriter = pandas.ExcelWriter(\'Masterfile.xlsx\', engine=\'openpyxl\') \nwriter.book = book\nwriter.sheets = dict((ws.title, ws) for ws in book.worksheets)\n\ndata_filtered.to_excel(writer, ""Main"", cols=[\'Diff1\', \'Diff2\'])\n\nwriter.save()\n']"
968;;3;20230859;20230326.0;2;46;;;"<p>use <code>drop</code> method:</p>

<pre><code>df.drop(column_name, axis=1)
</code></pre>
";;['df.drop(column_name, axis=1)\n'];['drop', 'df.drop(column_name, axis=1)\n']
969;;4;20231632;20230326.0;2;11;;;"<p>you can just select the columns you want without deleting or dropping:</p>

<pre><code>collist = ['col1', 'col2', 'col3']
df1 = df[collist]
</code></pre>

<p>Just pass a list of the columns you desire</p>

<p>You can also retrieve the list of columns and then select from that list</p>

<pre><code>collist = df.columns.tolist()
# you can now select from this list any arbritrary range
df1 = df[collist[0:1]]
# or remove a column
collist.remove('col2')
# now select
df1 = df[collist]
# df1 will now only have 'col1' and 'col3'
</code></pre>
";;"[""collist = ['col1', 'col2', 'col3']\ndf1 = df[collist]\n"", ""collist = df.columns.tolist()\n# you can now select from this list any arbritrary range\ndf1 = df[collist[0:1]]\n# or remove a column\ncollist.remove('col2')\n# now select\ndf1 = df[collist]\n# df1 will now only have 'col1' and 'col3'\n""]";"[""collist = ['col1', 'col2', 'col3']\ndf1 = df[collist]\n"", ""collist = df.columns.tolist()\n# you can now select from this list any arbritrary range\ndf1 = df[collist[0:1]]\n# or remove a column\ncollist.remove('col2')\n# now select\ndf1 = df[collist]\n# df1 will now only have 'col1' and 'col3'\n""]"
970;;3;20233649;20233071.0;2;30;;;"<p>You want <code>df.loc[df.index &lt; '2013-10-16 08:00:00']</code> since you're selecting by label (index) and not by value.</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#selection-by-label"">selecting by label</a></p>
";;[];"[""df.loc[df.index < '2013-10-16 08:00:00']""]"
971;;1;20235451;20235401.0;2;62;;;"<pre><code>&gt;&gt;&gt; s = pd.Series([1,2,3,4,np.NaN,5,np.NaN])
&gt;&gt;&gt; s[~s.isnull()]
0    1
1    2
2    3
3    4
5    5
</code></pre>

<p><strong>update</strong> or even better approach as @DSM suggested in comments, using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dropna.html"" rel=""noreferrer""><code>pandas.Series.dropna()</code></a>:</p>

<pre><code>&gt;&gt;&gt; s.dropna()
0    1
1    2
2    3
3    4
5    5
</code></pre>
";;['>>> s = pd.Series([1,2,3,4,np.NaN,5,np.NaN])\n>>> s[~s.isnull()]\n0    1\n1    2\n2    3\n3    4\n5    5\n', '>>> s.dropna()\n0    1\n1    2\n2    3\n3    4\n5    5\n'];['>>> s = pd.Series([1,2,3,4,np.NaN,5,np.NaN])\n>>> s[~s.isnull()]\n0    1\n1    2\n2    3\n3    4\n5    5\n', 'pandas.Series.dropna()', '>>> s.dropna()\n0    1\n1    2\n2    3\n3    4\n5    5\n']
972;;5;20250947;20250771.0;2;26;;;"<p>There is a bit of ambiguity in your question. There are at least <strike>three</strike> two interpretations:</p>

<ol>
<li>the keys in <code>di</code> refer to index values</li>
<li>the keys in <code>di</code> refer to <code>df['col1']</code> values</li>
<li>the keys in <code>di</code> refer to index locations (not the OP's question, but thrown in for fun.)</li>
</ol>

<p>Below is a solution for each case.</p>

<hr>

<p><strong>Case 1:</strong>
If the keys of <code>di</code> are meant to refer to index values, then you could use the <code>update</code> method:</p>

<pre><code>df['col1'].update(pd.Series(di))
</code></pre>

<p>For example,</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'col1':['w', 10, 20],
                   'col2': ['a', 30, np.nan]},
                  index=[1,2,0])
#   col1 col2
# 1    w    a
# 2   10   30
# 0   20  NaN

di = {0: ""A"", 2: ""B""}

# The value at the 0-index is mapped to 'A', the value at the 2-index is mapped to 'B'
df['col1'].update(pd.Series(di))
print(df)
</code></pre>

<p>yields</p>

<pre><code>  col1 col2
1    w    a
2    B   30
0    A  NaN
</code></pre>

<p>I've modified the values from your original post so it is clearer what <code>update</code> is doing.
Note how the keys in <code>di</code> are associated with index values. The order of the index values -- that is, the index <em>locations</em> -- does not matter.</p>

<hr>

<p><strong>Case 2:</strong>
If the keys in <code>di</code> refer to <code>df['col1']</code> values, then @DanAllan and @DSM show how to achieve this with <code>replace</code>:</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'col1':['w', 10, 20],
                   'col2': ['a', 30, np.nan]},
                  index=[1,2,0])
print(df)
#   col1 col2
# 1    w    a
# 2   10   30
# 0   20  NaN

di = {10: ""A"", 20: ""B""}

# The values 10 and 20 are replaced by 'A' and 'B'
df['col1'].replace(di, inplace=True)
print(df)
</code></pre>

<p>yields</p>

<pre><code>  col1 col2
1    w    a
2    A   30
0    B  NaN
</code></pre>

<p>Note how in this case the keys in <code>di</code> were changed to match <em>values</em> in <code>df['col1']</code>.</p>

<hr>

<p><strong>Case 3:</strong>
If the keys in <code>di</code> refer to index locations, then you could use</p>

<pre><code>df['col1'].put(di.keys(), di.values())
</code></pre>

<p>since</p>

<pre><code>df = pd.DataFrame({'col1':['w', 10, 20],
                   'col2': ['a', 30, np.nan]},
                  index=[1,2,0])
di = {0: ""A"", 2: ""B""}

# The values at the 0 and 2 index locations are replaced by 'A' and 'B'
df['col1'].put(di.keys(), di.values())
print(df)
</code></pre>

<p>yields</p>

<pre><code>  col1 col2
1    A    a
2   10   30
0    B  NaN
</code></pre>

<p>Here, the first and third rows were altered, because the keys in <code>di</code> are <code>0</code> and <code>2</code>, which with Python's 0-based indexing refer to the first and third locations.</p>
";;"[""df['col1'].update(pd.Series(di))\n"", 'import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\'col1\':[\'w\', 10, 20],\n                   \'col2\': [\'a\', 30, np.nan]},\n                  index=[1,2,0])\n#   col1 col2\n# 1    w    a\n# 2   10   30\n# 0   20  NaN\n\ndi = {0: ""A"", 2: ""B""}\n\n# The value at the 0-index is mapped to \'A\', the value at the 2-index is mapped to \'B\'\ndf[\'col1\'].update(pd.Series(di))\nprint(df)\n', '  col1 col2\n1    w    a\n2    B   30\n0    A  NaN\n', 'import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\'col1\':[\'w\', 10, 20],\n                   \'col2\': [\'a\', 30, np.nan]},\n                  index=[1,2,0])\nprint(df)\n#   col1 col2\n# 1    w    a\n# 2   10   30\n# 0   20  NaN\n\ndi = {10: ""A"", 20: ""B""}\n\n# The values 10 and 20 are replaced by \'A\' and \'B\'\ndf[\'col1\'].replace(di, inplace=True)\nprint(df)\n', '  col1 col2\n1    w    a\n2    A   30\n0    B  NaN\n', ""df['col1'].put(di.keys(), di.values())\n"", 'df = pd.DataFrame({\'col1\':[\'w\', 10, 20],\n                   \'col2\': [\'a\', 30, np.nan]},\n                  index=[1,2,0])\ndi = {0: ""A"", 2: ""B""}\n\n# The values at the 0 and 2 index locations are replaced by \'A\' and \'B\'\ndf[\'col1\'].put(di.keys(), di.values())\nprint(df)\n', '  col1 col2\n1    A    a\n2   10   30\n0    B  NaN\n']";"['di', 'di', ""df['col1']"", 'di', 'di', 'update', ""df['col1'].update(pd.Series(di))\n"", 'import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\'col1\':[\'w\', 10, 20],\n                   \'col2\': [\'a\', 30, np.nan]},\n                  index=[1,2,0])\n#   col1 col2\n# 1    w    a\n# 2   10   30\n# 0   20  NaN\n\ndi = {0: ""A"", 2: ""B""}\n\n# The value at the 0-index is mapped to \'A\', the value at the 2-index is mapped to \'B\'\ndf[\'col1\'].update(pd.Series(di))\nprint(df)\n', '  col1 col2\n1    w    a\n2    B   30\n0    A  NaN\n', 'update', 'di', 'di', ""df['col1']"", 'replace', 'import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\'col1\':[\'w\', 10, 20],\n                   \'col2\': [\'a\', 30, np.nan]},\n                  index=[1,2,0])\nprint(df)\n#   col1 col2\n# 1    w    a\n# 2   10   30\n# 0   20  NaN\n\ndi = {10: ""A"", 20: ""B""}\n\n# The values 10 and 20 are replaced by \'A\' and \'B\'\ndf[\'col1\'].replace(di, inplace=True)\nprint(df)\n', '  col1 col2\n1    w    a\n2    A   30\n0    B  NaN\n', 'di', ""df['col1']"", 'di', ""df['col1'].put(di.keys(), di.values())\n"", 'df = pd.DataFrame({\'col1\':[\'w\', 10, 20],\n                   \'col2\': [\'a\', 30, np.nan]},\n                  index=[1,2,0])\ndi = {0: ""A"", 2: ""B""}\n\n# The values at the 0 and 2 index locations are replaced by \'A\' and \'B\'\ndf[\'col1\'].put(di.keys(), di.values())\nprint(df)\n', '  col1 col2\n1    A    a\n2   10   30\n0    B  NaN\n', 'di', '0', '2']"
973;;1;20250996;20250771.0;2;85;;;"<p>You can use <code>.replace</code>.  For example:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'col2': {0: 'a', 1: 2, 2: np.nan}, 'col1': {0: 'w', 1: 1, 2: 2}})
&gt;&gt;&gt; di = {1: ""A"", 2: ""B""}
&gt;&gt;&gt; df
  col1 col2
0    w    a
1    1    2
2    2  NaN
&gt;&gt;&gt; df.replace({""col1"": di})
  col1 col2
0    w    a
1    A    2
2    B  NaN
</code></pre>

<p>or directly on the <code>Series</code>, i.e. <code>df[""col1""].replace(di, inplace=True)</code>.</p>
";;"['>>> df = pd.DataFrame({\'col2\': {0: \'a\', 1: 2, 2: np.nan}, \'col1\': {0: \'w\', 1: 1, 2: 2}})\n>>> di = {1: ""A"", 2: ""B""}\n>>> df\n  col1 col2\n0    w    a\n1    1    2\n2    2  NaN\n>>> df.replace({""col1"": di})\n  col1 col2\n0    w    a\n1    A    2\n2    B  NaN\n']";"['.replace', '>>> df = pd.DataFrame({\'col2\': {0: \'a\', 1: 2, 2: np.nan}, \'col1\': {0: \'w\', 1: 1, 2: 2}})\n>>> di = {1: ""A"", 2: ""B""}\n>>> df\n  col1 col2\n0    w    a\n1    1    2\n2    2  NaN\n>>> df.replace({""col1"": di})\n  col1 col2\n0    w    a\n1    A    2\n2    B  NaN\n', 'Series', 'df[""col1""].replace(di, inplace=True)']"
974;;3;20297639;20297332.0;2;86;;;"<p>Like so:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({""pear"": [1,2,3], ""apple"": [2,3,4], ""orange"": [3,4,5]})

len(df.columns)
3
</code></pre>
";;"['import pandas as pd\ndf = pd.DataFrame({""pear"": [1,2,3], ""apple"": [2,3,4], ""orange"": [3,4,5]})\n\nlen(df.columns)\n3\n']";"['import pandas as pd\ndf = pd.DataFrame({""pear"": [1,2,3], ""apple"": [2,3,4], ""orange"": [3,4,5]})\n\nlen(df.columns)\n3\n']"
975;;0;20301769;20297317.0;2;45;;;"<p>You can delete column on <code>i</code> index like this:</p>

<pre><code>df.drop(df.columns[i], axis=1)
</code></pre>

<p>It could work strange, if you have duplicate names in columns, so to do this you can rename column you want to delete column by new name. Or you can reassign DataFrame like this:</p>

<pre><code>df = df.iloc[:, [j for j, c in enumerate(df.columns) if j != i]]
</code></pre>
";;['df.drop(df.columns[i], axis=1)\n', 'df = df.iloc[:, [j for j, c in enumerate(df.columns) if j != i]]\n'];['i', 'df.drop(df.columns[i], axis=1)\n', 'df = df.iloc[:, [j for j, c in enumerate(df.columns) if j != i]]\n']
976;;1;20304311;20297332.0;2;32;;;"<p>Alternative: </p>

<pre><code>df.shape[1]
</code></pre>

<p>(<code>df.shape[0]</code> is the number of rows)</p>
";;['df.shape[1]\n'];['df.shape[1]\n', 'df.shape[0]']
977;;0;20334902;16522380.0;2;26;;;"<p>In the IPython notebook you could also use <code>%matplotlib inline</code> at the top of the notebook to automatically display the created plots in the output cells.</p>
";;[];['%matplotlib inline']
978;;2;20375692;20375561.0;2;47;;;"<p>you can use the left_on and right_on options as follows:</p>

<pre><code>pd.merge(frame_1, frame_2, left_on = 'county_ID', right_on = 'countyid')
</code></pre>

<p>I was not sure from the question if you only wanted to merge if the key was in the left hand dataframe. If that is the case then the following will do that (the above will in effect do a many to many merge)</p>

<pre><code>pd.merge(frame_1, frame_2, how = 'left', left_on = 'county_ID', right_on = 'countyid')
</code></pre>
";;"[""pd.merge(frame_1, frame_2, left_on = 'county_ID', right_on = 'countyid')\n"", ""pd.merge(frame_1, frame_2, how = 'left', left_on = 'county_ID', right_on = 'countyid')\n""]";"[""pd.merge(frame_1, frame_2, left_on = 'county_ID', right_on = 'countyid')\n"", ""pd.merge(frame_1, frame_2, how = 'left', left_on = 'county_ID', right_on = 'countyid')\n""]"
979;;5;20384317;20383647.0;2;27;;;"<p>Granted that the behavior is inconsistent, but I think it's easy to imagine cases where this is convenient. Anyway, to get a DataFrame every time, just pass a list to <code>loc</code>. There are other ways, but in my opinion this is the cleanest.</p>

<pre><code>In [2]: type(df.loc[[3]])
Out[2]: pandas.core.frame.DataFrame

In [3]: type(df.loc[[1]])
Out[3]: pandas.core.frame.DataFrame
</code></pre>
";;['In [2]: type(df.loc[[3]])\nOut[2]: pandas.core.frame.DataFrame\n\nIn [3]: type(df.loc[[1]])\nOut[3]: pandas.core.frame.DataFrame\n'];['loc', 'In [2]: type(df.loc[[3]])\nOut[2]: pandas.core.frame.DataFrame\n\nIn [3]: type(df.loc[[1]])\nOut[3]: pandas.core.frame.DataFrame\n']
980;;3;20444256;20444087.0;2;70;;;"<pre><code>data.reindex(index=data.index[::-1])
</code></pre>

<p>or simply:</p>

<pre><code>data.iloc[::-1]
</code></pre>

<p>will reverse your data frame, if you want to have a <code>for</code> loop which goes from down to up you may do:</p>

<pre><code>for idx in reversed(data.index):
    print(idx, data.loc[idx, 'Even'], data.loc[idx, 'Odd'])
</code></pre>

<p>or</p>

<pre><code>for idx in reversed(data.index):
    print(idx, data.Even[idx], data.Odd[idx])
</code></pre>

<p>You are getting an error because <code>reversed</code> first calls <code>data.__len__()</code> which returns 6. Then it tries to call <code>data[j - 1]</code> for <code>j</code> in <code>range(6, 0, -1)</code>, and the first call would be <code>data[5]</code>; but in pandas dataframe <code>data[5]</code> means column 5, and there is no column 5 so it will throw an exception. ( see <a href=""http://docs.python.org/2/library/functions.html#reversed"">docs</a> )</p>
";;"['data.reindex(index=data.index[::-1])\n', 'data.iloc[::-1]\n', ""for idx in reversed(data.index):\n    print(idx, data.loc[idx, 'Even'], data.loc[idx, 'Odd'])\n"", 'for idx in reversed(data.index):\n    print(idx, data.Even[idx], data.Odd[idx])\n']";"['data.reindex(index=data.index[::-1])\n', 'data.iloc[::-1]\n', 'for', ""for idx in reversed(data.index):\n    print(idx, data.loc[idx, 'Even'], data.loc[idx, 'Odd'])\n"", 'for idx in reversed(data.index):\n    print(idx, data.Even[idx], data.Odd[idx])\n', 'reversed', 'data.__len__()', 'data[j - 1]', 'j', 'range(6, 0, -1)', 'data[5]', 'data[5]']"
981;;2;20461206;20461165.0;2;231;;;"<p>either:</p>

<pre><code>df['index1'] = df.index
</code></pre>

<p>or, <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html#pandas.DataFrame.reset_index""><code>.reset_index</code></a>:</p>

<pre><code>df.reset_index(level=0, inplace=True)
</code></pre>

<hr>

<p>so, if you have a multi-index frame with 3 levels of index, like:</p>

<pre><code>&gt;&gt;&gt; df
                       val
tick       tag obs        
2016-02-26 C   2    0.0139
2016-02-27 A   2    0.5577
2016-02-28 C   6    0.0303
</code></pre>

<p>and you want to convert the 1st (<code>tick</code>) and 3rd (<code>obs</code>) levels in the index into columns, you would do:</p>

<pre><code>&gt;&gt;&gt; df.reset_index(level=['tick', 'obs'])
          tick  obs     val
tag                        
C   2016-02-26    2  0.0139
A   2016-02-27    2  0.5577
C   2016-02-28    6  0.0303
</code></pre>
";;"[""df['index1'] = df.index\n"", 'df.reset_index(level=0, inplace=True)\n', '>>> df\n                       val\ntick       tag obs        \n2016-02-26 C   2    0.0139\n2016-02-27 A   2    0.5577\n2016-02-28 C   6    0.0303\n', "">>> df.reset_index(level=['tick', 'obs'])\n          tick  obs     val\ntag                        \nC   2016-02-26    2  0.0139\nA   2016-02-27    2  0.5577\nC   2016-02-28    6  0.0303\n""]";"[""df['index1'] = df.index\n"", '.reset_index', 'df.reset_index(level=0, inplace=True)\n', '>>> df\n                       val\ntick       tag obs        \n2016-02-26 C   2    0.0139\n2016-02-27 A   2    0.5577\n2016-02-28 C   6    0.0303\n', 'tick', 'obs', "">>> df.reset_index(level=['tick', 'obs'])\n          tick  obs     val\ntag                        \nC   2016-02-26    2  0.0139\nA   2016-02-27    2  0.5577\nC   2016-02-28    6  0.0303\n""]"
982;;2;20491748;20490274.0;2;231;;;"<p><code>reset_index()</code> is what you're looking for. if you don't want it saved as a column, then</p>

<pre><code>df = df.reset_index(drop=True)
</code></pre>
";;['df = df.reset_index(drop=True)\n'];['reset_index()', 'df = df.reset_index(drop=True)\n']
983;;5;20574460;20574257.0;2;25;;;"<p>It's a simple linear algebra, you multiply matrix with its transpose (your example contains strings, don't forget to convert them to integer):</p>

<pre><code>&gt;&gt;&gt; df_asint = df.astype(int)
&gt;&gt;&gt; coocc = df_asint.T.dot(df_asint)
&gt;&gt;&gt; coocc
       Dop  Snack  Trans
Dop      4      2      3
Snack    2      3      2
Trans    3      2      4
</code></pre>

<p>if, as in R answer, you want to reset diagonal, you can use numpy's <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.fill_diagonal.html""><code>fill_diagonal</code></a>:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.fill_diagonal(coocc.values, 0)
&gt;&gt;&gt; coocc
       Dop  Snack  Trans
Dop      0      2      3
Snack    2      0      2
Trans    3      2      0
</code></pre>
";;['>>> df_asint = df.astype(int)\n>>> coocc = df_asint.T.dot(df_asint)\n>>> coocc\n       Dop  Snack  Trans\nDop      4      2      3\nSnack    2      3      2\nTrans    3      2      4\n', '>>> import numpy as np\n>>> np.fill_diagonal(coocc.values, 0)\n>>> coocc\n       Dop  Snack  Trans\nDop      0      2      3\nSnack    2      0      2\nTrans    3      2      0\n'];['>>> df_asint = df.astype(int)\n>>> coocc = df_asint.T.dot(df_asint)\n>>> coocc\n       Dop  Snack  Trans\nDop      4      2      3\nSnack    2      3      2\nTrans    3      2      4\n', 'fill_diagonal', '>>> import numpy as np\n>>> np.fill_diagonal(coocc.values, 0)\n>>> coocc\n       Dop  Snack  Trans\nDop      0      2      3\nSnack    2      0      2\nTrans    3      2      0\n']
984;;0;20612691;20612645.0;2;137;;;"<p>Check <code>pandas.__version__</code>:</p>

<pre><code>In [76]: import pandas as pd

In [77]: pd.__version__
Out[77]: '0.12.0-933-g281dc4e'
</code></pre>

<p>Pandas also provides a utility function, <code>pd.show_versions()</code>, which reports the version of its dependencies as well:</p>

<pre><code>In [53]: pd.show_versions(as_json=False)

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.6.final.0
python-bits: 64
OS: Linux
OS-release: 3.13.0-45-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8

pandas: 0.15.2-113-g5531341
nose: 1.3.1
Cython: 0.21.1
numpy: 1.8.2
scipy: 0.14.0.dev-371b4ff
statsmodels: 0.6.0.dev-a738b4f
IPython: 2.0.0-dev
sphinx: 1.2.2
patsy: 0.3.0
dateutil: 1.5
pytz: 2012c
bottleneck: None
tables: 3.1.1
numexpr: 2.2.2
matplotlib: 1.4.2
openpyxl: None
xlrd: 0.9.3
xlwt: 0.7.5
xlsxwriter: None
lxml: 3.3.3
bs4: 4.3.2
html5lib: 0.999
httplib2: 0.8
apiclient: None
rpy2: 2.5.5
sqlalchemy: 0.9.8
pymysql: None
psycopg2: 2.4.5 (dt dec mx pq3 ext)
</code></pre>
";;"[""In [76]: import pandas as pd\n\nIn [77]: pd.__version__\nOut[77]: '0.12.0-933-g281dc4e'\n"", 'In [53]: pd.show_versions(as_json=False)\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-45-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.15.2-113-g5531341\nnose: 1.3.1\nCython: 0.21.1\nnumpy: 1.8.2\nscipy: 0.14.0.dev-371b4ff\nstatsmodels: 0.6.0.dev-a738b4f\nIPython: 2.0.0-dev\nsphinx: 1.2.2\npatsy: 0.3.0\ndateutil: 1.5\npytz: 2012c\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.2.2\nmatplotlib: 1.4.2\nopenpyxl: None\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: None\nlxml: 3.3.3\nbs4: 4.3.2\nhtml5lib: 0.999\nhttplib2: 0.8\napiclient: None\nrpy2: 2.5.5\nsqlalchemy: 0.9.8\npymysql: None\npsycopg2: 2.4.5 (dt dec mx pq3 ext)\n']";"['pandas.__version__', ""In [76]: import pandas as pd\n\nIn [77]: pd.__version__\nOut[77]: '0.12.0-933-g281dc4e'\n"", 'pd.show_versions()', 'In [53]: pd.show_versions(as_json=False)\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-45-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.15.2-113-g5531341\nnose: 1.3.1\nCython: 0.21.1\nnumpy: 1.8.2\nscipy: 0.14.0.dev-371b4ff\nstatsmodels: 0.6.0.dev-a738b4f\nIPython: 2.0.0-dev\nsphinx: 1.2.2\npatsy: 0.3.0\ndateutil: 1.5\npytz: 2012c\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.2.2\nmatplotlib: 1.4.2\nopenpyxl: None\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: None\nlxml: 3.3.3\nbs4: 4.3.2\nhtml5lib: 0.999\nhttplib2: 0.8\napiclient: None\nrpy2: 2.5.5\nsqlalchemy: 0.9.8\npymysql: None\npsycopg2: 2.4.5 (dt dec mx pq3 ext)\n']"
985;;8;20627316;20625582.0;2;232;;;"<p>From what I gather, <code>SettingWithCopyWarning</code> was created to flag potentially confusing ""chained"" assignments, such as the following, which don't always work as expected, particularly when the first selection returns a <em>copy</em>.  [see <a href=""https://github.com/pydata/pandas/pull/5390"" rel=""noreferrer"">GH5390</a> and <a href=""https://github.com/pydata/pandas/issues/5597"" rel=""noreferrer"">GH5597</a> for background discussion.]</p>

<pre><code>df[df['A'] &gt; 2]['B'] = new_val  # new_val not set in df
</code></pre>

<p>The warning offers a suggestion to rewrite as follows:</p>

<pre><code>df.loc[df['A'] &gt; 2, 'B'] = new_val
</code></pre>

<p>However, this doesn't fit your usage, which is equivalent to:</p>

<pre><code>df = df[df['A'] &gt; 2]
df['B'] = new_val
</code></pre>

<p>While it's clear that you don't care about writes making it back to the original frame (since you overwrote the reference to it), unfortunately this pattern can not be differentiated from the first chained assignment example, hence the (false positive) warning.  The potential for false positives is addressed in the <a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html#returning-a-view-versus-a-copy"" rel=""noreferrer"">docs on indexing</a>, if you'd like to read further.  You can safely disable this new warning with the following assignment.</p>

<pre><code>pd.options.mode.chained_assignment = None  # default='warn'
</code></pre>
";;"[""df[df['A'] > 2]['B'] = new_val  # new_val not set in df\n"", ""df.loc[df['A'] > 2, 'B'] = new_val\n"", ""df = df[df['A'] > 2]\ndf['B'] = new_val\n"", ""pd.options.mode.chained_assignment = None  # default='warn'\n""]";"['SettingWithCopyWarning', ""df[df['A'] > 2]['B'] = new_val  # new_val not set in df\n"", ""df.loc[df['A'] > 2, 'B'] = new_val\n"", ""df = df[df['A'] > 2]\ndf['B'] = new_val\n"", ""pd.options.mode.chained_assignment = None  # default='warn'\n""]"
986;;2;20637559;20637439.0;2;37;;;"<p>You can try yourself:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; from StringIO import StringIO
&gt;&gt;&gt; s = """"""1, 2
... 3, 4
... 5, 6""""""
&gt;&gt;&gt; pd.read_csv(StringIO(s), skiprows=[1], header=None)
   0  1
0  1  2
1  5  6
&gt;&gt;&gt; pd.read_csv(StringIO(s), skiprows=1, header=None)
   0  1
0  3  4
1  5  6
</code></pre>
";;"['>>> import pandas as pd\n>>> from StringIO import StringIO\n>>> s = """"""1, 2\n... 3, 4\n... 5, 6""""""\n>>> pd.read_csv(StringIO(s), skiprows=[1], header=None)\n   0  1\n0  1  2\n1  5  6\n>>> pd.read_csv(StringIO(s), skiprows=1, header=None)\n   0  1\n0  3  4\n1  5  6\n']";"['>>> import pandas as pd\n>>> from StringIO import StringIO\n>>> s = """"""1, 2\n... 3, 4\n... 5, 6""""""\n>>> pd.read_csv(StringIO(s), skiprows=[1], header=None)\n   0  1\n0  1  2\n1  5  6\n>>> pd.read_csv(StringIO(s), skiprows=1, header=None)\n   0  1\n0  3  4\n1  5  6\n']"
987;;9;20638258;20638006.0;2;319;;;"<p>Supposing <code>d</code> is your list of dicts, simply:</p>

<pre><code>pd.DataFrame(d)
</code></pre>
";;['pd.DataFrame(d)\n'];['d', 'pd.DataFrame(d)\n']
988;;12;20644369;20625582.0;2;68;;;"<p>In general the point of the <code>SettingWithCopyWarning</code> is to show users (and esp new users) that they <em>may</em> be operating on a copy and not the original as they think. There <em>are</em> False positives (IOW you know what you are doing, so it <em>ok</em>). One possibility is simply to turn off the (by default <em>warn</em>) warning as @Garrett suggest.</p>

<p>Here is a nother, per option.</p>

<pre><code>In [1]: df = DataFrame(np.random.randn(5,2),columns=list('AB'))

In [2]: dfa = df.ix[:,[1,0]]

In [3]: dfa.is_copy
Out[3]: True

In [4]: dfa['A'] /= 2
/usr/local/bin/ipython:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead
  #!/usr/local/bin/python
</code></pre>

<p>You can set the <code>is_copy</code> flag to <code>False</code>, which will effectively turn off the check, *for that object``</p>

<pre><code>In [5]: dfa.is_copy = False

In [6]: dfa['A'] /= 2
</code></pre>

<p>If you explicity copy then you <em>know what you are doing</em>, so no further warning will happen.</p>

<pre><code>In [7]: dfa = df.ix[:,[1,0]].copy()

In [8]: dfa['A'] /= 2
</code></pre>

<p>The code the OP is showing above, while legitimate, and probably something I do as well, is technically a case for this warning, and not a False positive. Another way to <em>not</em> have the warning would be to do the selection operation via <code>reindex</code>, e.g.</p>

<pre><code>quote_df = quote_df(columns=['STK',.......])
</code></pre>
";;"[""In [1]: df = DataFrame(np.random.randn(5,2),columns=list('AB'))\n\nIn [2]: dfa = df.ix[:,[1,0]]\n\nIn [3]: dfa.is_copy\nOut[3]: True\n\nIn [4]: dfa['A'] /= 2\n/usr/local/bin/ipython:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  #!/usr/local/bin/python\n"", ""In [5]: dfa.is_copy = False\n\nIn [6]: dfa['A'] /= 2\n"", ""In [7]: dfa = df.ix[:,[1,0]].copy()\n\nIn [8]: dfa['A'] /= 2\n"", ""quote_df = quote_df(columns=['STK',.......])\n""]";"['SettingWithCopyWarning', ""In [1]: df = DataFrame(np.random.randn(5,2),columns=list('AB'))\n\nIn [2]: dfa = df.ix[:,[1,0]]\n\nIn [3]: dfa.is_copy\nOut[3]: True\n\nIn [4]: dfa['A'] /= 2\n/usr/local/bin/ipython:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  #!/usr/local/bin/python\n"", 'is_copy', 'False', ""In [5]: dfa.is_copy = False\n\nIn [6]: dfa['A'] /= 2\n"", ""In [7]: dfa = df.ix[:,[1,0]].copy()\n\nIn [8]: dfa['A'] /= 2\n"", 'reindex', ""quote_df = quote_df(columns=['STK',.......])\n""]"
989;;1;20657592;20656663.0;2;66;;;"<p>This error occurs among other things when you have NaN values in the Series. Could that be the case?</p>

<p>These NaN's are not handled well by the <code>hist</code> function of matplotlib. For example:</p>

<pre><code>s = pd.Series([1,2,3,2,2,3,5,2,3,2,np.nan])
fig, ax = plt.subplots()
ax.hist(s, alpha=0.9, color='blue')
</code></pre>

<p>produces the same error <code>AttributeError: max must be larger than min in range parameter.</code> One option is eg to remove the NaN's before plotting. This will work:</p>

<pre><code>ax.hist(s.dropna(), alpha=0.9, color='blue')
</code></pre>

<p>Another option is to use pandas <code>hist</code> method on your series and providing the <code>axes[0]</code> to the <code>ax</code> keyword:</p>

<pre><code>dfj2_MARKET1['VSPD1_perc'].hist(ax=axes[0], alpha=0.9, color='blue')
</code></pre>
";;"[""s = pd.Series([1,2,3,2,2,3,5,2,3,2,np.nan])\nfig, ax = plt.subplots()\nax.hist(s, alpha=0.9, color='blue')\n"", ""ax.hist(s.dropna(), alpha=0.9, color='blue')\n"", ""dfj2_MARKET1['VSPD1_perc'].hist(ax=axes[0], alpha=0.9, color='blue')\n""]";"['hist', ""s = pd.Series([1,2,3,2,2,3,5,2,3,2,np.nan])\nfig, ax = plt.subplots()\nax.hist(s, alpha=0.9, color='blue')\n"", 'AttributeError: max must be larger than min in range parameter.', ""ax.hist(s.dropna(), alpha=0.9, color='blue')\n"", 'hist', 'axes[0]', 'ax', ""dfj2_MARKET1['VSPD1_perc'].hist(ax=axes[0], alpha=0.9, color='blue')\n""]"
990;;1;20687887;19798153.0;2;9;;;"<p>@jeremiahbuddha mentioned that apply works on row/columns, while applymap works element-wise. But it seems you can still use apply for element-wise computation.... </p>

<pre><code>    frame.apply(np.sqrt)
    Out[102]: 
                   b         d         e
    Utah         NaN  1.435159       NaN
    Ohio    1.098164  0.510594  0.729748
    Texas        NaN  0.456436  0.697337
    Oregon  0.359079       NaN       NaN

    frame.applymap(np.sqrt)
    Out[103]: 
                   b         d         e
    Utah         NaN  1.435159       NaN
    Ohio    1.098164  0.510594  0.729748
    Texas        NaN  0.456436  0.697337
    Oregon  0.359079       NaN       NaN
</code></pre>
";;['    frame.apply(np.sqrt)\n    Out[102]: \n                   b         d         e\n    Utah         NaN  1.435159       NaN\n    Ohio    1.098164  0.510594  0.729748\n    Texas        NaN  0.456436  0.697337\n    Oregon  0.359079       NaN       NaN\n\n    frame.applymap(np.sqrt)\n    Out[103]: \n                   b         d         e\n    Utah         NaN  1.435159       NaN\n    Ohio    1.098164  0.510594  0.729748\n    Texas        NaN  0.456436  0.697337\n    Oregon  0.359079       NaN       NaN\n'];['    frame.apply(np.sqrt)\n    Out[102]: \n                   b         d         e\n    Utah         NaN  1.435159       NaN\n    Ohio    1.098164  0.510594  0.729748\n    Texas        NaN  0.456436  0.697337\n    Oregon  0.359079       NaN       NaN\n\n    frame.applymap(np.sqrt)\n    Out[103]: \n                   b         d         e\n    Utah         NaN  1.435159       NaN\n    Ohio    1.098164  0.510594  0.729748\n    Texas        NaN  0.456436  0.697337\n    Oregon  0.359079       NaN       NaN\n']
991;;0;20687984;17627219.0;2;26;;;"<p>The following method is about 30 times faster than <code>scipy.spatial.distance.pdist</code>. It works pretty quickly on large matrices (assuming you have enough RAM)</p>

<p>See below for a discussion of how to optimize for sparsity.</p>

<pre><code># base similarity matrix (all dot products)
# replace this with A.dot(A.T).toarray() for sparse representation
similarity = numpy.dot(A, A.T)


# squared magnitude of preference vectors (number of occurrences)
square_mag = numpy.diag(similarity)

# inverse squared magnitude
inv_square_mag = 1 / square_mag

# if it doesn't occur, set it's inverse magnitude to zero (instead of inf)
inv_square_mag[numpy.isinf(inv_square_mag)] = 0

# inverse of the magnitude
inv_mag = numpy.sqrt(inv_square_mag)

# cosine similarity (elementwise multiply by inverse magnitudes)
cosine = similarity * inv_mag
cosine = cosine.T * inv_mag
</code></pre>

<p>If your problem is typical for large scale binary preference problems, you have a lot more entries in one dimension than the other. Also, the short dimension is the one whose entries you want to calculate similarities between. Let's call this dimension the 'item' dimension.</p>

<p>If this is the case, list your 'items' in rows and create <code>A</code> using <a href=""http://docs.scipy.org/doc/scipy/reference/sparse.html"" rel=""noreferrer""><code>scipy.sparse</code></a>.  Then replace the first line as indicated.</p>

<p>If your problem is atypical you'll need more modifications. Those should be pretty straightforward replacements of basic <code>numpy</code> operations with their <code>scipy.sparse</code> equivalents.</p>
";;"[""# base similarity matrix (all dot products)\n# replace this with A.dot(A.T).toarray() for sparse representation\nsimilarity = numpy.dot(A, A.T)\n\n\n# squared magnitude of preference vectors (number of occurrences)\nsquare_mag = numpy.diag(similarity)\n\n# inverse squared magnitude\ninv_square_mag = 1 / square_mag\n\n# if it doesn't occur, set it's inverse magnitude to zero (instead of inf)\ninv_square_mag[numpy.isinf(inv_square_mag)] = 0\n\n# inverse of the magnitude\ninv_mag = numpy.sqrt(inv_square_mag)\n\n# cosine similarity (elementwise multiply by inverse magnitudes)\ncosine = similarity * inv_mag\ncosine = cosine.T * inv_mag\n""]";"['scipy.spatial.distance.pdist', ""# base similarity matrix (all dot products)\n# replace this with A.dot(A.T).toarray() for sparse representation\nsimilarity = numpy.dot(A, A.T)\n\n\n# squared magnitude of preference vectors (number of occurrences)\nsquare_mag = numpy.diag(similarity)\n\n# inverse squared magnitude\ninv_square_mag = 1 / square_mag\n\n# if it doesn't occur, set it's inverse magnitude to zero (instead of inf)\ninv_square_mag[numpy.isinf(inv_square_mag)] = 0\n\n# inverse of the magnitude\ninv_mag = numpy.sqrt(inv_square_mag)\n\n# cosine similarity (elementwise multiply by inverse magnitudes)\ncosine = similarity * inv_mag\ncosine = cosine.T * inv_mag\n"", 'A', 'scipy.sparse', 'numpy', 'scipy.sparse']"
992;;1;20690383;14262433.0;2;78;;;"<p>I think the answers above are missing a simple approach that I've found very useful. </p>

<p>When I have a file that is too large to load in memory, I break up the file into multiple smaller files (either by row or cols)</p>

<p>Example: In case of 30 days worth of trading data of ~30GB size, I break it into a file per day of ~1GB size. I subsequently process each file separately and aggregate results at the end</p>

<p>One of the biggest advantages is that it allows parallel processing of the files (either multiple threads or processes)</p>

<p>The other advantage is that file manipulation (like adding/removing dates in the example) can be accomplished by regular shell commands, which is not be possible in more advanced/complicated file formats</p>

<p>This approach doesn't cover all scenarios, but is very useful in a lot of them</p>
";;[];[]
993;;1;20693013;16249736.0;2;15;;;"<p><a href=""https://bitbucket.org/djcbeach/monary/wiki/Home""><code>Monary</code></a> does exactly that, and it's <em>super fast</em>. (<a href=""http://djcinnovations.com/index.php/archives/164"">another link</a>)</p>

<p>See <a href=""http://alexgaudio.com/2012/07/07/monarymongopandas.html"">this cool post</a> which includes a quick tutorial and some timings.</p>
";;[];['Monary']
994;;0;20763459;20763012.0;2;88;;;"<p>You need to specify <code>data</code>, <code>index</code> and <code>columns</code> to <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#pandas.DataFrame"" rel=""noreferrer""><code>DataFrame</code></a> constructor, as in:</p>

<pre><code>&gt;&gt;&gt; pd.DataFrame(data=data[1:,1:],    # values
...              index=data[1:,0],    # 1st column as index
...              columns=data[0,1:])  # 1st row as the column names
</code></pre>

<p><strong>edit</strong>: as in the @joris comment, you may need to change above to <code>np.int_(data[1:,1:])</code> to have correct data type.</p>
";;['>>> pd.DataFrame(data=data[1:,1:],    # values\n...              index=data[1:,0],    # 1st column as index\n...              columns=data[0,1:])  # 1st row as the column names\n'];['data', 'index', 'columns', 'DataFrame', '>>> pd.DataFrame(data=data[1:,1:],    # values\n...              index=data[1:,0],    # 1st column as index\n...              columns=data[0,1:])  # 1st row as the column names\n', 'np.int_(data[1:,1:])']
995;;0;20868446;20868394.0;2;185;;;"<p>A one liner does exist:</p>

<pre><code>In [27]: df=df.rename(columns = {'two':'new_name'})

In [28]: df
Out[28]: 
  one three  new_name
0    1     a         9
1    2     b         8
2    3     c         7
3    4     d         6
4    5     e         5
</code></pre>

<p>Following is the docstring for the <code>rename</code> method.</p>

<pre>
Definition: df.rename(self, index=None, columns=None, copy=True, inplace=False)
Docstring:
Alter index and / or columns using input function or
functions. Function / dict values must be unique (1-to-1). Labels not
contained in a dict / Series will be left as-is.

Parameters
----------
index : dict-like or function, optional
    Transformation to apply to index values
columns : dict-like or function, optional
    Transformation to apply to column values
copy : boolean, default True
    Also copy underlying data
inplace : boolean, default False
    Whether to return a new DataFrame. If True then value of copy is
    ignored.

See also
--------
Series.rename

Returns
-------
renamed : DataFrame (new object)
</pre>
";;"[""In [27]: df=df.rename(columns = {'two':'new_name'})\n\nIn [28]: df\nOut[28]: \n  one three  new_name\n0    1     a         9\n1    2     b         8\n2    3     c         7\n3    4     d         6\n4    5     e         5\n"", '\nDefinition: df.rename(self, index=None, columns=None, copy=True, inplace=False)\nDocstring:\nAlter index and / or columns using input function or\nfunctions. Function / dict values must be unique (1-to-1). Labels not\ncontained in a dict / Series will be left as-is.\n\nParameters\n----------\nindex : dict-like or function, optional\n    Transformation to apply to index values\ncolumns : dict-like or function, optional\n    Transformation to apply to column values\ncopy : boolean, default True\n    Also copy underlying data\ninplace : boolean, default False\n    Whether to return a new DataFrame. If True then value of copy is\n    ignored.\n\nSee also\n--------\nSeries.rename\n\nReturns\n-------\nrenamed : DataFrame (new object)\n']";"[""In [27]: df=df.rename(columns = {'two':'new_name'})\n\nIn [28]: df\nOut[28]: \n  one three  new_name\n0    1     a         9\n1    2     b         8\n2    3     c         7\n3    4     d         6\n4    5     e         5\n"", 'rename']"
996;;1;20937592;20937538.0;2;101;;;"<pre><code>import pandas as pd
pd.options.display.float_format = '${:,.2f}'.format
df = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],
                  index=['foo','bar','baz','quux'],
                  columns=['cost'])
print(df)
</code></pre>

<p>yields</p>

<pre><code>        cost
foo  $123.46
bar  $234.57
baz  $345.68
quux $456.79
</code></pre>

<p>but this only works if you want <em>every</em> float to be formatted with a dollar sign.</p>

<p>Otherwise, if you want dollar formatting for some floats only, then I think you'll have to pre-modify the dataframe (converting those floats to strings):</p>

<pre><code>import pandas as pd
df = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],
                  index=['foo','bar','baz','quux'],
                  columns=['cost'])
df['foo'] = df['cost']
df['cost'] = df['cost'].map('${:,.2f}'.format)
print(df)
</code></pre>

<p>yields</p>

<pre><code>         cost       foo
foo   $123.46  123.4567
bar   $234.57  234.5678
baz   $345.68  345.6789
quux  $456.79  456.7890
</code></pre>
";;"[""import pandas as pd\npd.options.display.float_format = '${:,.2f}'.format\ndf = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\nprint(df)\n"", '        cost\nfoo  $123.46\nbar  $234.57\nbaz  $345.68\nquux $456.79\n', ""import pandas as pd\ndf = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\ndf['foo'] = df['cost']\ndf['cost'] = df['cost'].map('${:,.2f}'.format)\nprint(df)\n"", '         cost       foo\nfoo   $123.46  123.4567\nbar   $234.57  234.5678\nbaz   $345.68  345.6789\nquux  $456.79  456.7890\n']";"[""import pandas as pd\npd.options.display.float_format = '${:,.2f}'.format\ndf = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\nprint(df)\n"", '        cost\nfoo  $123.46\nbar  $234.57\nbaz  $345.68\nquux $456.79\n', ""import pandas as pd\ndf = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\ndf['foo'] = df['cost']\ndf['cost'] = df['cost'].map('${:,.2f}'.format)\nprint(df)\n"", '         cost       foo\nfoo   $123.46  123.4567\nbar   $234.57  234.5678\nbaz   $345.68  345.6789\nquux  $456.79  456.7890\n']"
997;;4;20965090;20965046.0;2;44;;;"<pre><code>df['cum_sum'] = df.val1.cumsum()
df['cum_perc'] = 100*df.cum_sum/df.val1.sum()
</code></pre>

<p>This will add the columns to <code>df</code>.  If you want a copy, copy <code>df</code> first and then do these operations on the copy.</p>
";;"[""df['cum_sum'] = df.val1.cumsum()\ndf['cum_perc'] = 100*df.cum_sum/df.val1.sum()\n""]";"[""df['cum_sum'] = df.val1.cumsum()\ndf['cum_perc'] = 100*df.cum_sum/df.val1.sum()\n"", 'df', 'df']"
998;;8;20970328;20970279.0;2;34;;;"<p>First two letters for each value in a column:</p>

<pre><code>&gt;&gt;&gt; df['StateInitial'] = df['state'].str[:2]
&gt;&gt;&gt; df
   pop       state  year StateInitial
0  1.5    Auckland  2000           Au
1  1.7       Otago  2001           Ot
2  3.6  Wellington  2002           We
3  2.4     Dunedin  2001           Du
4  2.9    Hamilton  2002           Ha
</code></pre>

<p>For last two that would be <code>df['state'].str[-2:]</code>. Don't know what exactly you want for middle, but you can apply arbitrary function to a column with <code>apply</code> method:</p>

<pre><code>&gt;&gt;&gt; df['state'].apply(lambda x: x[len(x)/2-1:len(x)/2+1])
0    kl
1    ta
2    in
3    ne
4    il
</code></pre>
";;"["">>> df['StateInitial'] = df['state'].str[:2]\n>>> df\n   pop       state  year StateInitial\n0  1.5    Auckland  2000           Au\n1  1.7       Otago  2001           Ot\n2  3.6  Wellington  2002           We\n3  2.4     Dunedin  2001           Du\n4  2.9    Hamilton  2002           Ha\n"", "">>> df['state'].apply(lambda x: x[len(x)/2-1:len(x)/2+1])\n0    kl\n1    ta\n2    in\n3    ne\n4    il\n""]";"["">>> df['StateInitial'] = df['state'].str[:2]\n>>> df\n   pop       state  year StateInitial\n0  1.5    Auckland  2000           Au\n1  1.7       Otago  2001           Ot\n2  3.6  Wellington  2002           We\n3  2.4     Dunedin  2001           Du\n4  2.9    Hamilton  2002           Ha\n"", ""df['state'].str[-2:]"", 'apply', "">>> df['state'].apply(lambda x: x[len(x)/2-1:len(x)/2+1])\n0    kl\n1    ta\n2    in\n3    ne\n4    il\n""]"
999;;2;20995313;20995196.0;2;14;;;"<p>You didn't mention the fancy indexing capabilities of dataframes, e.g.:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({""class"":[1,1,1,2,2], ""value"":[1,2,3,4,5]})
&gt;&gt;&gt; df[df[""class""]==1].sum()
class    3
value    6
dtype: int64
&gt;&gt;&gt; df[df[""class""]==1].sum()[""value""]
6
&gt;&gt;&gt; df[df[""class""]==1].count()[""value""]
3
</code></pre>

<p>You could replace <code>df[""class""]==1</code>by another condition.</p>
";;"['>>> df = pd.DataFrame({""class"":[1,1,1,2,2], ""value"":[1,2,3,4,5]})\n>>> df[df[""class""]==1].sum()\nclass    3\nvalue    6\ndtype: int64\n>>> df[df[""class""]==1].sum()[""value""]\n6\n>>> df[df[""class""]==1].count()[""value""]\n3\n']";"['>>> df = pd.DataFrame({""class"":[1,1,1,2,2], ""value"":[1,2,3,4,5]})\n>>> df[df[""class""]==1].sum()\nclass    3\nvalue    6\ndtype: int64\n>>> df[df[""class""]==1].sum()[""value""]\n6\n>>> df[df[""class""]==1].count()[""value""]\n3\n', 'df[""class""]==1']"
1000;;4;20995428;20995196.0;2;24;;;"<p>You can first make a conditional selection, and sum up the results of the selection using the <code>sum</code> function.</p>

<pre><code>&gt;&gt; df = pd.DataFrame({'a': [1, 2, 3]})
&gt;&gt; df[df.a &gt; 1].sum()   
a    5
dtype: int64
</code></pre>

<p>Having more than one condition:</p>

<pre><code>&gt;&gt; df[(df.a &gt; 1) &amp; (df.a &lt; 3)].sum()
a    2
dtype: int64
</code></pre>
";;"["">> df = pd.DataFrame({'a': [1, 2, 3]})\n>> df[df.a > 1].sum()   \na    5\ndtype: int64\n"", '>> df[(df.a > 1) & (df.a < 3)].sum()\na    2\ndtype: int64\n']";"['sum', "">> df = pd.DataFrame({'a': [1, 2, 3]})\n>> df[df.a > 1].sum()   \na    5\ndtype: int64\n"", '>> df[(df.a > 1) & (df.a < 3)].sum()\na    2\ndtype: int64\n']"
1001;;1;21020411;21018654.0;2;89;;;"<p>The dtype object comes from NumPy, it describes the type of element in a ndarray. Every element in a ndarray must has the same size in byte. For int64 and float64, they are 8 bytes. But for strings, the length of the string is not fixed. So instead of save the bytes of strings in the ndarray directly, Pandas use object ndarray, which save pointers to objects, because of this the dtype of this kind ndarray is object.</p>

<p>Here is an example:</p>

<ul>
<li>the int64 array contains 4 int64 value.</li>
<li>the object array contains 4 pointers to 3 string objects.</li>
</ul>

<p><img src=""https://i.stack.imgur.com/pFF44.png"" alt=""enter image description here""></p>
";;[];[]
1002;;5;21032532;17116814.0;2;42;;;"<p>Differently from Dan, I consider his answer quite elegant... but unfortunately it is also very very inefficient. So, since the question mentioned <em>""a large csv file""</em>, let me suggest to try in a shell Dan's solution:</p>

<pre><code>time python -c ""import pandas as pd;
df = pd.DataFrame(['a b c']*100000, columns=['col']);
print df['col'].apply(lambda x : pd.Series(x.split(' '))).head()""
</code></pre>

<p>... compared to this alternative:</p>

<pre><code>time python -c ""import pandas as pd;
from scipy import array, concatenate;
df = pd.DataFrame(['a b c']*100000, columns=['col']);
print pd.DataFrame(concatenate(df['col'].apply( lambda x : [x.split(' ')]))).head()""
</code></pre>

<p>... and this:</p>

<pre><code>time python -c ""import pandas as pd;
df = pd.DataFrame(['a b c']*100000, columns=['col']);
print pd.DataFrame(dict(zip(range(3), [df['col'].apply(lambda x : x.split(' ')[i]) for i in range(3)]))).head()""
</code></pre>

<p>The second simply refrains from allocating 100 000 Series, and this is enough to make it around 10 times faster. But the third solution, which somewhat ironically wastes a lot of calls to str.split() (it is called once per column per row, so three times more than for the others two solutions), is around <em>40 times</em> faster than the first, because it even avoids to instance the 100 000 lists. And yes, it is certainly a little ugly...</p>

<p><strong>EDIT:</strong> <a href=""https://stackoverflow.com/a/12505089/2858145"">this answer</a> suggests how to use ""to_list()"" and to avoid the need for a lambda. The result is something like</p>

<pre><code>time python -c ""import pandas as pd;
df = pd.DataFrame(['a b c']*100000, columns=['col']);
print pd.DataFrame(df.col.str.split().tolist()).head()""
</code></pre>

<p>which is even more efficient than the third solution, and certainly much more elegant.</p>

<p><strong>EDIT:</strong> the even simpler</p>

<pre><code>time python -c ""import pandas as pd;
df = pd.DataFrame(['a b c']*100000, columns=['col']);
print pd.DataFrame(list(df.col.str.split())).head()""
</code></pre>

<p>works too, and is <strong>almost</strong> as efficient.</p>
";;"['time python -c ""import pandas as pd;\ndf = pd.DataFrame([\'a b c\']*100000, columns=[\'col\']);\nprint df[\'col\'].apply(lambda x : pd.Series(x.split(\' \'))).head()""\n', 'time python -c ""import pandas as pd;\nfrom scipy import array, concatenate;\ndf = pd.DataFrame([\'a b c\']*100000, columns=[\'col\']);\nprint pd.DataFrame(concatenate(df[\'col\'].apply( lambda x : [x.split(\' \')]))).head()""\n', 'time python -c ""import pandas as pd;\ndf = pd.DataFrame([\'a b c\']*100000, columns=[\'col\']);\nprint pd.DataFrame(dict(zip(range(3), [df[\'col\'].apply(lambda x : x.split(\' \')[i]) for i in range(3)]))).head()""\n', 'time python -c ""import pandas as pd;\ndf = pd.DataFrame([\'a b c\']*100000, columns=[\'col\']);\nprint pd.DataFrame(df.col.str.split().tolist()).head()""\n', 'time python -c ""import pandas as pd;\ndf = pd.DataFrame([\'a b c\']*100000, columns=[\'col\']);\nprint pd.DataFrame(list(df.col.str.split())).head()""\n']";"['time python -c ""import pandas as pd;\ndf = pd.DataFrame([\'a b c\']*100000, columns=[\'col\']);\nprint df[\'col\'].apply(lambda x : pd.Series(x.split(\' \'))).head()""\n', 'time python -c ""import pandas as pd;\nfrom scipy import array, concatenate;\ndf = pd.DataFrame([\'a b c\']*100000, columns=[\'col\']);\nprint pd.DataFrame(concatenate(df[\'col\'].apply( lambda x : [x.split(\' \')]))).head()""\n', 'time python -c ""import pandas as pd;\ndf = pd.DataFrame([\'a b c\']*100000, columns=[\'col\']);\nprint pd.DataFrame(dict(zip(range(3), [df[\'col\'].apply(lambda x : x.split(\' \')[i]) for i in range(3)]))).head()""\n', 'time python -c ""import pandas as pd;\ndf = pd.DataFrame([\'a b c\']*100000, columns=[\'col\']);\nprint pd.DataFrame(df.col.str.split().tolist()).head()""\n', 'time python -c ""import pandas as pd;\ndf = pd.DataFrame([\'a b c\']*100000, columns=[\'col\']);\nprint pd.DataFrame(list(df.col.str.split())).head()""\n']"
1003;;3;21140339;21137150.0;2;73;;;"<p>Granted, the answer I linked in the comments is not very helpful. You can specify your own string converter like so.</p>

<pre><code>In [25]: pd.set_option('display.float_format', lambda x: '%.3f' % x)

In [28]: Series(np.random.randn(3))*1000000000
Out[28]: 
0    -757322420.605
1   -1436160588.997
2   -1235116117.064
dtype: float64
</code></pre>

<p>I'm not sure if that's the preferred way to do this, but it works.</p>

<p>Converting numbers to strings purely for aesthetic purposes seems like a bad idea, but if you have a good reason, this is one way:</p>

<pre><code>In [6]: Series(np.random.randn(3)).apply(lambda x: '%.3f' % x)
Out[6]: 
0     0.026
1    -0.482
2    -0.694
dtype: object
</code></pre>
";;"[""In [25]: pd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nIn [28]: Series(np.random.randn(3))*1000000000\nOut[28]: \n0    -757322420.605\n1   -1436160588.997\n2   -1235116117.064\ndtype: float64\n"", ""In [6]: Series(np.random.randn(3)).apply(lambda x: '%.3f' % x)\nOut[6]: \n0     0.026\n1    -0.482\n2    -0.694\ndtype: object\n""]";"[""In [25]: pd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nIn [28]: Series(np.random.randn(3))*1000000000\nOut[28]: \n0    -757322420.605\n1   -1436160588.997\n2   -1235116117.064\ndtype: float64\n"", ""In [6]: Series(np.random.randn(3)).apply(lambda x: '%.3f' % x)\nOut[6]: \n0     0.026\n1    -0.482\n2    -0.694\ndtype: object\n""]"
1004;;2;21175114;18079563.0;2;12;;;"<p>Setup:</p>

<pre><code>s1 = pd.Series([4,5,6,20,42])
s2 = pd.Series([1,2,3,5,42])
</code></pre>

<p>Timings:</p>

<pre><code>%%timeit
pd.Series(list(set(s1).intersection(set(s2))))
10000 loops, best of 3: 57.7 s per loop

%%timeit
pd.Series(np.intersect1d(s1,s2))
1000 loops, best of 3: 659 s per loop

%%timeit
pd.Series(np.intersect1d(s1.values,s2.values))
10000 loops, best of 3: 64.7 s per loop
</code></pre>

<p>So the numpy solution can be comparable to the set solution even for small series, if one uses the <code>values</code> explicitely.</p>
";;['s1 = pd.Series([4,5,6,20,42])\ns2 = pd.Series([1,2,3,5,42])\n', '%%timeit\npd.Series(list(set(s1).intersection(set(s2))))\n10000 loops, best of 3: 57.7 s per loop\n\n%%timeit\npd.Series(np.intersect1d(s1,s2))\n1000 loops, best of 3: 659 s per loop\n\n%%timeit\npd.Series(np.intersect1d(s1.values,s2.values))\n10000 loops, best of 3: 64.7 s per loop\n'];['s1 = pd.Series([4,5,6,20,42])\ns2 = pd.Series([1,2,3,5,42])\n', '%%timeit\npd.Series(list(set(s1).intersection(set(s2))))\n10000 loops, best of 3: 57.7 s per loop\n\n%%timeit\npd.Series(np.intersect1d(s1,s2))\n1000 loops, best of 3: 659 s per loop\n\n%%timeit\npd.Series(np.intersect1d(s1.values,s2.values))\n10000 loops, best of 3: 64.7 s per loop\n', 'values']
1005;;6;21197863;21197774.0;2;28;;;"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.convert_objects.html""><code>convert_objects</code></a> to infer better dtypes:</p>

<pre><code>In [11]: df
Out[11]: 
   x  y
0  a  1
1  b  2

In [12]: df.dtypes
Out[12]: 
x    object
y    object
dtype: object

In [13]: df.convert_objects(convert_numeric=True)
Out[13]: 
   x  y
0  a  1
1  b  2

In [14]: df.convert_objects(convert_numeric=True).dtypes
Out[14]: 
x    object
y     int64
dtype: object
</code></pre>

<p><em>Magic!</em></p>
";;['In [11]: df\nOut[11]: \n   x  y\n0  a  1\n1  b  2\n\nIn [12]: df.dtypes\nOut[12]: \nx    object\ny    object\ndtype: object\n\nIn [13]: df.convert_objects(convert_numeric=True)\nOut[13]: \n   x  y\n0  a  1\n1  b  2\n\nIn [14]: df.convert_objects(convert_numeric=True).dtypes\nOut[14]: \nx    object\ny     int64\ndtype: object\n'];['convert_objects', 'In [11]: df\nOut[11]: \n   x  y\n0  a  1\n1  b  2\n\nIn [12]: df.dtypes\nOut[12]: \nx    object\ny    object\ndtype: object\n\nIn [13]: df.convert_objects(convert_numeric=True)\nOut[13]: \n   x  y\n0  a  1\n1  b  2\n\nIn [14]: df.convert_objects(convert_numeric=True).dtypes\nOut[14]: \nx    object\ny     int64\ndtype: object\n']
1006;;2;21221138;14349055.0;2;29;;;"<p>matplotlib >= 1.4 suports <a href=""https://github.com/matplotlib/matplotlib/blob/master/doc/users/style_sheets.rst"" rel=""noreferrer"">styles</a> (and ggplot-style is build in):</p>

<pre><code>In [1]: import matplotlib as mpl

In [2]: import matplotlib.pyplot as plt

In [3]: import numpy as np

In [4]: mpl.style.available
Out[4]: [u'dark_background', u'grayscale', u'ggplot']

In [5]: mpl.style.use('ggplot')

In [6]: plt.hist(np.random.randn(100000))
Out[6]: 
...
</code></pre>

<p><img src=""https://i.stack.imgur.com/arnXC.png"" alt=""enter image description here""></p>
";;"[""In [1]: import matplotlib as mpl\n\nIn [2]: import matplotlib.pyplot as plt\n\nIn [3]: import numpy as np\n\nIn [4]: mpl.style.available\nOut[4]: [u'dark_background', u'grayscale', u'ggplot']\n\nIn [5]: mpl.style.use('ggplot')\n\nIn [6]: plt.hist(np.random.randn(100000))\nOut[6]: \n...\n""]";"[""In [1]: import matplotlib as mpl\n\nIn [2]: import matplotlib.pyplot as plt\n\nIn [3]: import numpy as np\n\nIn [4]: mpl.style.available\nOut[4]: [u'dark_background', u'grayscale', u'ggplot']\n\nIn [5]: mpl.style.use('ggplot')\n\nIn [6]: plt.hist(np.random.randn(100000))\nOut[6]: \n...\n""]"
1007;;9;21232849;20906474.0;2;107;;;"<p>If you have same columns in all your <code>csv</code> files then you can try the code below.
I have added <code>header=0</code> so that after reading <code>csv</code> first row can be assigned as the column names.</p>

<pre><code>path =r'C:\DRO\DCL_rawdata_files' # use your path
allFiles = glob.glob(path + ""/*.csv"")
frame = pd.DataFrame()
list_ = []
for file_ in allFiles:
    df = pd.read_csv(file_,index_col=None, header=0)
    list_.append(df)
frame = pd.concat(list_)
</code></pre>
";;"['path =r\'C:\\DRO\\DCL_rawdata_files\' # use your path\nallFiles = glob.glob(path + ""/*.csv"")\nframe = pd.DataFrame()\nlist_ = []\nfor file_ in allFiles:\n    df = pd.read_csv(file_,index_col=None, header=0)\n    list_.append(df)\nframe = pd.concat(list_)\n']";"['csv', 'header=0', 'csv', 'path =r\'C:\\DRO\\DCL_rawdata_files\' # use your path\nallFiles = glob.glob(path + ""/*.csv"")\nframe = pd.DataFrame()\nlist_ = []\nfor file_ in allFiles:\n    df = pd.read_csv(file_,index_col=None, header=0)\n    list_.append(df)\nframe = pd.concat(list_)\n']"
1008;;1;21260328;10065051.0;2;9;;;"<p>MySQL example:</p>

<pre><code>import MySQLdb as db
from pandas import DataFrame
from pandas.io.sql import frame_query

database = db.connect('localhost','username','password','database')
data     = frame_query(""SELECT * FROM data"", database)
</code></pre>
";;"['import MySQLdb as db\nfrom pandas import DataFrame\nfrom pandas.io.sql import frame_query\n\ndatabase = db.connect(\'localhost\',\'username\',\'password\',\'database\')\ndata     = frame_query(""SELECT * FROM data"", database)\n']";"['import MySQLdb as db\nfrom pandas import DataFrame\nfrom pandas.io.sql import frame_query\n\ndatabase = db.connect(\'localhost\',\'username\',\'password\',\'database\')\ndata     = frame_query(""SELECT * FROM data"", database)\n']"
1009;;1;21266043;21104592.0;2;61;;;"<p>I found a quick and easy solution to what I wanted using json_normalize function included in the latest release of pandas 0.13.   </p>

<pre><code>from urllib2 import Request, urlopen
import json
from pandas.io.json import json_normalize

path1 = '42.974049,-81.205203|42.974298,-81.195755'
request=Request('http://maps.googleapis.com/maps/api/elevation/json?locations='+path1+'&amp;sensor=false')
response = urlopen(request)
elevations = response.read()
data = json.loads(elevations)
json_normalize(data['results'])
</code></pre>

<p>This gives a nice flattened dataframe with the json data that I got from the google maps API.</p>
";;"[""from urllib2 import Request, urlopen\nimport json\nfrom pandas.io.json import json_normalize\n\npath1 = '42.974049,-81.205203|42.974298,-81.195755'\nrequest=Request('http://maps.googleapis.com/maps/api/elevation/json?locations='+path1+'&sensor=false')\nresponse = urlopen(request)\nelevations = response.read()\ndata = json.loads(elevations)\njson_normalize(data['results'])\n""]";"[""from urllib2 import Request, urlopen\nimport json\nfrom pandas.io.json import json_normalize\n\npath1 = '42.974049,-81.205203|42.974298,-81.195755'\nrequest=Request('http://maps.googleapis.com/maps/api/elevation/json?locations='+path1+'&sensor=false')\nresponse = urlopen(request)\nelevations = response.read()\ndata = json.loads(elevations)\njson_normalize(data['results'])\n""]"
1010;;0;21271103;21269399.0;2;7;;;"<p>You might try passing actual types instead of strings.</p>

<pre><code>import pandas as pd
from datetime import datetime
headers = ['col1', 'col2', 'col3', 'col4'] 
dtypes = [datetime, datetime, str, float] 
pd.read_csv(file, sep='\t', header=None, names=headers, dtype=dtypes)
</code></pre>

<p>But it's going to be really hard to diagnose this without any of your data to tinker with.</p>

<p>And really, you probably want pandas to parse the the dates into TimeStamps, so that might be:</p>

<pre><code>pd.read_csv(file, sep='\t', header=None, names=headers, parse_dates=True)
</code></pre>
";;"[""import pandas as pd\nfrom datetime import datetime\nheaders = ['col1', 'col2', 'col3', 'col4'] \ndtypes = [datetime, datetime, str, float] \npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes)\n"", ""pd.read_csv(file, sep='\\t', header=None, names=headers, parse_dates=True)\n""]";"[""import pandas as pd\nfrom datetime import datetime\nheaders = ['col1', 'col2', 'col3', 'col4'] \ndtypes = [datetime, datetime, str, float] \npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes)\n"", ""pd.read_csv(file, sep='\\t', header=None, names=headers, parse_dates=True)\n""]"
1011;;11;21285575;21285380.0;2;60;;;"<p>Just iterate over <code>DataFrame.columns</code>, now this is an example in which you will end up with a list of column names that match:</p>

<pre><code>import pandas as pd

data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}
df = pd.DataFrame(data)

spike_cols = [col for col in df.columns if 'spike' in col]
print(list(df.columns))
print(spike_cols)
</code></pre>

<p>Output:</p>

<pre><code>['hey spke', 'no', 'spike-2', 'spiked-in']
['spike-2', 'spiked-in']
</code></pre>

<p>Explanation:</p>

<ol>
<li><code>df.columns</code> returns a list of column names</li>
<li><code>[col for col in df.columns if 'spike' in col]</code> iterates over the list <code>df.columns</code> with the variable <code>col</code> and adds it to the resulting list if <code>col</code> contains <code>'spike'</code>. This syntax is <a href=""http://docs.python.org/2/tutorial/datastructures.html#list-comprehensions"" rel=""noreferrer"">list comprehension</a>. </li>
</ol>

<p>If you only want the resulting data set with the columns that match you can do this:</p>

<pre><code>df2 = df.filter(regex='spike')
print(df2)
</code></pre>

<p>Output:</p>

<pre><code>   spike-2  spiked-in
0        1          7
1        2          8
2        3          9
</code></pre>
";;"[""import pandas as pd\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\n\nspike_cols = [col for col in df.columns if 'spike' in col]\nprint(list(df.columns))\nprint(spike_cols)\n"", ""['hey spke', 'no', 'spike-2', 'spiked-in']\n['spike-2', 'spiked-in']\n"", ""df2 = df.filter(regex='spike')\nprint(df2)\n"", '   spike-2  spiked-in\n0        1          7\n1        2          8\n2        3          9\n']";"['DataFrame.columns', ""import pandas as pd\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\n\nspike_cols = [col for col in df.columns if 'spike' in col]\nprint(list(df.columns))\nprint(spike_cols)\n"", ""['hey spke', 'no', 'spike-2', 'spiked-in']\n['spike-2', 'spiked-in']\n"", 'df.columns', ""[col for col in df.columns if 'spike' in col]"", 'df.columns', 'col', 'col', ""'spike'"", ""df2 = df.filter(regex='spike')\nprint(df2)\n"", '   spike-2  spiked-in\n0        1          7\n1        2          8\n2        3          9\n']"
1012;;0;21287539;13842088.0;2;25;;;"<p>The recommended way (according to the maintainers) to set a value is:</p>

<pre><code>df.ix['x','C']=10
</code></pre>

<p>Using 'chained indexing' (<code>df['x']['C']</code>) may lead to problems.</p>

<p>See:</p>

<ul>
<li><a href=""https://stackoverflow.com/a/21287235/1579844"">https://stackoverflow.com/a/21287235/1579844</a></li>
<li><a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html#indexing-view-versus-copy"" rel=""nofollow noreferrer"">http://pandas.pydata.org/pandas-docs/dev/indexing.html#indexing-view-versus-copy</a></li>
<li><a href=""https://github.com/pydata/pandas/pull/6031"" rel=""nofollow noreferrer"">https://github.com/pydata/pandas/pull/6031</a></li>
</ul>
";;"[""df.ix['x','C']=10\n""]";"[""df.ix['x','C']=10\n"", ""df['x']['C']""]"
1013;;5;21290084;21287624.0;2;48;;;"<p>The lack of NaN rep in integer columns is a <a href=""http://pandas.pydata.org/pandas-docs/stable/gotchas.html#support-for-integer-na"" rel=""noreferrer"">pandas ""gotcha""</a>.</p>

<p>The usual workaround is to simply use floats.</p>
";;[];[]
1014;;9;21291383;21291259.0;2;60;;;"<p>Use the .astype() function to manipulate column dtypes.</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(np.random.rand(3,4), columns=list(""ABCD""))
&gt;&gt;&gt; df
          A         B         C         D
0  0.542447  0.949988  0.669239  0.879887
1  0.068542  0.757775  0.891903  0.384542
2  0.021274  0.587504  0.180426  0.574300
&gt;&gt;&gt; df[list(""ABCD"")] = df[list(""ABCD"")].astype(int)
&gt;&gt;&gt; df
   A  B  C  D
0  0  0  0  0
1  0  0  0  0
2  0  0  0  0
</code></pre>

<p>EDIT:</p>

<p>To handle missing values:</p>

<pre><code>&gt;&gt;&gt; df
          A         B     C         D
0  0.475103  0.355453  0.66  0.869336
1  0.260395  0.200287   NaN  0.617024
2  0.517692  0.735613  0.18  0.657106
&gt;&gt;&gt; df[list(""ABCD"")] = df[list(""ABCD"")].fillna(0.0).astype(int)
&gt;&gt;&gt; df
   A  B  C  D
0  0  0  0  0
1  0  0  0  0
2  0  0  0  0
&gt;&gt;&gt;
</code></pre>
";;"['>>> df = pd.DataFrame(np.random.rand(3,4), columns=list(""ABCD""))\n>>> df\n          A         B         C         D\n0  0.542447  0.949988  0.669239  0.879887\n1  0.068542  0.757775  0.891903  0.384542\n2  0.021274  0.587504  0.180426  0.574300\n>>> df[list(""ABCD"")] = df[list(""ABCD"")].astype(int)\n>>> df\n   A  B  C  D\n0  0  0  0  0\n1  0  0  0  0\n2  0  0  0  0\n', '>>> df\n          A         B     C         D\n0  0.475103  0.355453  0.66  0.869336\n1  0.260395  0.200287   NaN  0.617024\n2  0.517692  0.735613  0.18  0.657106\n>>> df[list(""ABCD"")] = df[list(""ABCD"")].fillna(0.0).astype(int)\n>>> df\n   A  B  C  D\n0  0  0  0  0\n1  0  0  0  0\n2  0  0  0  0\n>>>\n']";"['>>> df = pd.DataFrame(np.random.rand(3,4), columns=list(""ABCD""))\n>>> df\n          A         B         C         D\n0  0.542447  0.949988  0.669239  0.879887\n1  0.068542  0.757775  0.891903  0.384542\n2  0.021274  0.587504  0.180426  0.574300\n>>> df[list(""ABCD"")] = df[list(""ABCD"")].astype(int)\n>>> df\n   A  B  C  D\n0  0  0  0  0\n1  0  0  0  0\n2  0  0  0  0\n', '>>> df\n          A         B     C         D\n0  0.475103  0.355453  0.66  0.869336\n1  0.260395  0.200287   NaN  0.617024\n2  0.517692  0.735613  0.18  0.657106\n>>> df[list(""ABCD"")] = df[list(""ABCD"")].fillna(0.0).astype(int)\n>>> df\n   A  B  C  D\n0  0  0  0  0\n1  0  0  0  0\n2  0  0  0  0\n>>>\n']"
1015;;4;21291622;21291259.0;2;55;;;"<p>To modify the float output do this:</p>

<pre><code>df= pd.DataFrame(range(5), columns=['a'])
df.a = df.a.astype(float)
df

Out[33]:

          a
0 0.0000000
1 1.0000000
2 2.0000000
3 3.0000000
4 4.0000000

pd.options.display.float_format = '{:,.0f}'.format
df

Out[35]:

   a
0  0
1  1
2  2
3  3
4  4
</code></pre>
";;"[""df= pd.DataFrame(range(5), columns=['a'])\ndf.a = df.a.astype(float)\ndf\n\nOut[33]:\n\n          a\n0 0.0000000\n1 1.0000000\n2 2.0000000\n3 3.0000000\n4 4.0000000\n\npd.options.display.float_format = '{:,.0f}'.format\ndf\n\nOut[35]:\n\n   a\n0  0\n1  1\n2  2\n3  3\n4  4\n""]";"[""df= pd.DataFrame(range(5), columns=['a'])\ndf.a = df.a.astype(float)\ndf\n\nOut[33]:\n\n          a\n0 0.0000000\n1 1.0000000\n2 2.0000000\n3 3.0000000\n4 4.0000000\n\npd.options.display.float_format = '{:,.0f}'.format\ndf\n\nOut[35]:\n\n   a\n0  0\n1  1\n2  2\n3  3\n4  4\n""]"
1016;;3;21296915;14745022.0;2;24;;;"<p>You can <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.extract.html"">extract</a> the different parts out quite neatly using a regex pattern:</p>

<pre><code>In [11]: df.row.str.extract('(?P&lt;fips&gt;\d{5})((?P&lt;state&gt;[A-Z ]*$)|(?P&lt;county&gt;.*?), (?P&lt;state_code&gt;[A-Z]{2}$))')
Out[11]: 
    fips                    1           state           county state_code
0  00000        UNITED STATES   UNITED STATES              NaN        NaN
1  01000              ALABAMA         ALABAMA              NaN        NaN
2  01001   Autauga County, AL             NaN   Autauga County         AL
3  01003   Baldwin County, AL             NaN   Baldwin County         AL
4  01005   Barbour County, AL             NaN   Barbour County         AL

[5 rows x 5 columns]
</code></pre>

<hr>

<p>To explain the somewhat long regex:</p>

<pre><code>(?P&lt;fips&gt;\d{5})
</code></pre>

<ul>
<li>Matches the five digits (<code>\d</code>) and names them <code>""fips""</code>.</li>
</ul>

<p>The next part:</p>

<pre><code>((?P&lt;state&gt;[A-Z ]*$)|(?P&lt;county&gt;.*?), (?P&lt;state_code&gt;[A-Z]{2}$))
</code></pre>

<p>Does either (<code>|</code>) one of two things:</p>

<pre><code>(?P&lt;state&gt;[A-Z ]*$)
</code></pre>

<ul>
<li>Matches any number (<code>*</code>) of capital letters or spaces (<code>[A-Z ]</code>) and names this <code>""state""</code> before the end of the string (<code>$</code>),</li>
</ul>

<p>or</p>

<pre><code>(?P&lt;county&gt;.*?), (?P&lt;state_code&gt;[A-Z]{2}$))
</code></pre>

<ul>
<li>matches anything else (<code>.*</code>) then</li>
<li>a comma and a space then  </li>
<li>matches the two digit <code>state_code</code> before the end of the string (<code>$</code>).</li>
</ul>

<p><em>In the example:</em><br>
<em>Note that the first two rows hit the ""state"" (leaving NaN in  the county and state_code columns), whilst the last three hit the county, state_code (leaving NaN in the state column).</em></p>
";;"[""In [11]: df.row.str.extract('(?P<fips>\\d{5})((?P<state>[A-Z ]*$)|(?P<county>.*?), (?P<state_code>[A-Z]{2}$))')\nOut[11]: \n    fips                    1           state           county state_code\n0  00000        UNITED STATES   UNITED STATES              NaN        NaN\n1  01000              ALABAMA         ALABAMA              NaN        NaN\n2  01001   Autauga County, AL             NaN   Autauga County         AL\n3  01003   Baldwin County, AL             NaN   Baldwin County         AL\n4  01005   Barbour County, AL             NaN   Barbour County         AL\n\n[5 rows x 5 columns]\n"", '(?P<fips>\\d{5})\n', '((?P<state>[A-Z ]*$)|(?P<county>.*?), (?P<state_code>[A-Z]{2}$))\n', '(?P<state>[A-Z ]*$)\n', '(?P<county>.*?), (?P<state_code>[A-Z]{2}$))\n']";"[""In [11]: df.row.str.extract('(?P<fips>\\d{5})((?P<state>[A-Z ]*$)|(?P<county>.*?), (?P<state_code>[A-Z]{2}$))')\nOut[11]: \n    fips                    1           state           county state_code\n0  00000        UNITED STATES   UNITED STATES              NaN        NaN\n1  01000              ALABAMA         ALABAMA              NaN        NaN\n2  01001   Autauga County, AL             NaN   Autauga County         AL\n3  01003   Baldwin County, AL             NaN   Baldwin County         AL\n4  01005   Barbour County, AL             NaN   Barbour County         AL\n\n[5 rows x 5 columns]\n"", '(?P<fips>\\d{5})\n', '\\d', '""fips""', '((?P<state>[A-Z ]*$)|(?P<county>.*?), (?P<state_code>[A-Z]{2}$))\n', '|', '(?P<state>[A-Z ]*$)\n', '*', '[A-Z ]', '""state""', '$', '(?P<county>.*?), (?P<state_code>[A-Z]{2}$))\n', '.*', 'state_code', '$']"
1017;;0;21315199;19482970.0;2;6;;;"<pre><code>[column for column in my_dataframe]
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#iteration"" rel=""nofollow"">pandas docs</a>: Iteration over dataframes return column labels</p>
";;['[column for column in my_dataframe]\n'];['[column for column in my_dataframe]\n']
1018;;4;21320011;21319929.0;2;35;;;"<p><code>in</code> of a Series checks whether the value is in the index:</p>

<pre><code>In [11]: s = pd.Series(list('abc'))

In [12]: s
Out[12]: 
0    a
1    b
2    c
dtype: object

In [13]: 1 in s
Out[13]: True

In [14]: 'a' in s
Out[14]: False
</code></pre>

<p>One option is to see if it's in <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html"">unique</a> values:</p>

<pre><code>In [21]: s.unique()
Out[21]: array(['a', 'b', 'c'], dtype=object)

In [22]: 'a' in s.unique()
Out[22]: True
</code></pre>

<p>or a python set:</p>

<pre><code>In [23]: set(s)
Out[23]: {'a', 'b', 'c'}

In [24]: 'a' in set(s)
Out[24]: True
</code></pre>

<p>As pointed out by @DSM, it may be more efficient (especially if you're just doing this for one value) to just use in directly on the values:</p>

<pre><code>In [31]: s.values
Out[31]: array(['a', 'b', 'c'], dtype=object)

In [32]: 'a' in s.values
Out[32]: True
</code></pre>
";;"[""In [11]: s = pd.Series(list('abc'))\n\nIn [12]: s\nOut[12]: \n0    a\n1    b\n2    c\ndtype: object\n\nIn [13]: 1 in s\nOut[13]: True\n\nIn [14]: 'a' in s\nOut[14]: False\n"", ""In [21]: s.unique()\nOut[21]: array(['a', 'b', 'c'], dtype=object)\n\nIn [22]: 'a' in s.unique()\nOut[22]: True\n"", ""In [23]: set(s)\nOut[23]: {'a', 'b', 'c'}\n\nIn [24]: 'a' in set(s)\nOut[24]: True\n"", ""In [31]: s.values\nOut[31]: array(['a', 'b', 'c'], dtype=object)\n\nIn [32]: 'a' in s.values\nOut[32]: True\n""]";"['in', ""In [11]: s = pd.Series(list('abc'))\n\nIn [12]: s\nOut[12]: \n0    a\n1    b\n2    c\ndtype: object\n\nIn [13]: 1 in s\nOut[13]: True\n\nIn [14]: 'a' in s\nOut[14]: False\n"", ""In [21]: s.unique()\nOut[21]: array(['a', 'b', 'c'], dtype=object)\n\nIn [22]: 'a' in s.unique()\nOut[22]: True\n"", ""In [23]: set(s)\nOut[23]: {'a', 'b', 'c'}\n\nIn [24]: 'a' in set(s)\nOut[24]: True\n"", ""In [31]: s.values\nOut[31]: array(['a', 'b', 'c'], dtype=object)\n\nIn [32]: 'a' in s.values\nOut[32]: True\n""]"
1019;;4;21361994;21360361.0;2;61;;;"<p>use <code>IPython.display</code> module:</p>

<pre><code>%matplotlib inline
import time
import pylab as pl
from IPython import display
for i in range(10):
    pl.plot(pl.randn(100))
    display.clear_output(wait=True)
    display.display(pl.gcf())
    time.sleep(1.0)
</code></pre>
";;['%matplotlib inline\nimport time\nimport pylab as pl\nfrom IPython import display\nfor i in range(10):\n    pl.plot(pl.randn(100))\n    display.clear_output(wait=True)\n    display.display(pl.gcf())\n    time.sleep(1.0)\n'];['IPython.display', '%matplotlib inline\nimport time\nimport pylab as pl\nfrom IPython import display\nfor i in range(10):\n    pl.plot(pl.randn(100))\n    display.clear_output(wait=True)\n    display.display(pl.gcf())\n    time.sleep(1.0)\n']
1020;;6;21415990;21415661.0;2;61;;;"<p>When you say</p>

<pre><code>(a['x']==1) and (a['y']==10)
</code></pre>

<p>You are implicitly asking Python to convert <code>(a['x']==1)</code> and <code>(a['y']==10)</code> to boolean values. </p>

<p>NumPy arrays (of length greater than 1) and Pandas objects such as Series do not have a boolean value -- in other words, they raise </p>

<pre><code>ValueError: The truth value of an array is ambiguous. Use a.empty, a.any() or a.all().
</code></pre>

<p>when used as a boolean value. That's because its <a href=""http://pandas.pydata.org/pandas-docs/dev/gotchas.html#using-if-truth-statements-with-pandas"" rel=""nofollow noreferrer"">unclear when it should be True or False</a>. Some users might assume they are True if they have non-zero length, like a Python list. Others might desire for it to be True only if <strong>all</strong> its elements are True. Others might want it to be True if <strong>any</strong> of its elements are True. </p>

<p>Because there are so many conflicting expectations, the designers of NumPy and Pandas refuse to guess, and instead raise a ValueError.</p>

<p>Instead, you must be explicit, by calling the <code>empty()</code>, <code>all()</code> or <code>any()</code> method to indicate which behavior you desire.</p>

<p>In this case, however, it looks like you do not want boolean evaluation, you want <strong>element-wise</strong> logical-and. That is what the <code>&amp;</code> binary operator performs:</p>

<pre><code>(a['x']==1) &amp; (a['y']==10)
</code></pre>

<p>returns a boolean array. </p>

<hr>

<p>By the way, as <a href=""https://stackoverflow.com/questions/21415661/logic-operator-for-boolean-indexing-in-pandas/21415990?noredirect=1#comment77317569_21415990"">alexpmil notes</a>, 
the parentheses are mandatory since <code>&amp;</code> has a higher <a href=""https://docs.python.org/3/reference/expressions.html#operator-precedence"" rel=""nofollow noreferrer"">operator precedence</a> than <code>==</code>.
Without the parentheses, <code>a['x']==1 &amp; a['y']==10</code> would be evaluated as <code>a['x'] == (1 &amp; a['y']) == 10</code> which would in turn be equivalent to the chained comparison <code>(a['x'] == (1 &amp; a['y'])) and ((1 &amp; a['y']) == 10)</code>. That is an expression of the form <code>Series and Series</code>.
The use of <code>and</code> with two Series would again trigger the same <code>ValueError</code> as above. That's why the parentheses are mandatory.</p>
";;"[""(a['x']==1) and (a['y']==10)\n"", 'ValueError: The truth value of an array is ambiguous. Use a.empty, a.any() or a.all().\n', ""(a['x']==1) & (a['y']==10)\n""]";"[""(a['x']==1) and (a['y']==10)\n"", ""(a['x']==1)"", ""(a['y']==10)"", 'ValueError: The truth value of an array is ambiguous. Use a.empty, a.any() or a.all().\n', 'empty()', 'all()', 'any()', '&', ""(a['x']==1) & (a['y']==10)\n"", '&', '==', ""a['x']==1 & a['y']==10"", ""a['x'] == (1 & a['y']) == 10"", ""(a['x'] == (1 & a['y'])) and ((1 & a['y']) == 10)"", 'Series and Series', 'and', 'ValueError']"
1021;;1;21441621;21441259.0;2;47;;;"<p>You might be interested in <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html"" rel=""noreferrer""><code>pd.cut</code></a>:</p>

<pre><code>&gt;&gt;&gt; df.groupby(pd.cut(df[""B""], np.arange(0, 1.0+0.155, 0.155))).sum()
                      A         B
B                                
(0, 0.155]     2.775458  0.246394
(0.155, 0.31]  1.123989  0.471618
(0.31, 0.465]  2.051814  1.882763
(0.465, 0.62]  2.277960  1.528492
(0.62, 0.775]  1.577419  2.810723
(0.775, 0.93]  0.535100  1.694955
(0.93, 1.085]       NaN       NaN

[7 rows x 2 columns]
</code></pre>
";;"['>>> df.groupby(pd.cut(df[""B""], np.arange(0, 1.0+0.155, 0.155))).sum()\n                      A         B\nB                                \n(0, 0.155]     2.775458  0.246394\n(0.155, 0.31]  1.123989  0.471618\n(0.31, 0.465]  2.051814  1.882763\n(0.465, 0.62]  2.277960  1.528492\n(0.62, 0.775]  1.577419  2.810723\n(0.775, 0.93]  0.535100  1.694955\n(0.93, 1.085]       NaN       NaN\n\n[7 rows x 2 columns]\n']";"['pd.cut', '>>> df.groupby(pd.cut(df[""B""], np.arange(0, 1.0+0.155, 0.155))).sum()\n                      A         B\nB                                \n(0, 0.155]     2.775458  0.246394\n(0.155, 0.31]  1.123989  0.471618\n(0.31, 0.465]  2.051814  1.882763\n(0.465, 0.62]  2.277960  1.528492\n(0.62, 0.775]  1.577419  2.810723\n(0.775, 0.93]  0.535100  1.694955\n(0.93, 1.085]       NaN       NaN\n\n[7 rows x 2 columns]\n']"
1022;;2;21487560;21487329.0;2;116;;;"<p>The <code>df.plot()</code> function returns a <code>matplotlib.axes.AxesSubplot</code> object. You can set the labels on that object.</p>

<pre><code>In [4]: ax = df2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')

In [6]: ax.set_xlabel(""x label"")
Out[6]: &lt;matplotlib.text.Text at 0x10e0af2d0&gt;

In [7]: ax.set_ylabel(""y label"")
Out[7]: &lt;matplotlib.text.Text at 0x10e0ba1d0&gt;
</code></pre>

<p><img src=""https://i.stack.imgur.com/ITpm2.png"" alt=""enter image description here""></p>

<p>Or, more succinctly: <code>ax.set(xlabel=""x label"", ylabel=""y label"")</code>.</p>

<p>Alternatively, the index x-axis label is automatically set to the Index name, if it has one. so <code>df2.index.name = 'x label'</code> would work too.</p>
";;"['In [4]: ax = df2.plot(lw=2,colormap=\'jet\',marker=\'.\',markersize=10,title=\'Video streaming dropout by category\')\n\nIn [6]: ax.set_xlabel(""x label"")\nOut[6]: <matplotlib.text.Text at 0x10e0af2d0>\n\nIn [7]: ax.set_ylabel(""y label"")\nOut[7]: <matplotlib.text.Text at 0x10e0ba1d0>\n']";"['df.plot()', 'matplotlib.axes.AxesSubplot', 'In [4]: ax = df2.plot(lw=2,colormap=\'jet\',marker=\'.\',markersize=10,title=\'Video streaming dropout by category\')\n\nIn [6]: ax.set_xlabel(""x label"")\nOut[6]: <matplotlib.text.Text at 0x10e0af2d0>\n\nIn [7]: ax.set_ylabel(""y label"")\nOut[7]: <matplotlib.text.Text at 0x10e0ba1d0>\n', 'ax.set(xlabel=""x label"", ylabel=""y label"")', ""df2.index.name = 'x label'""]"
1023;;0;21487868;21487329.0;2;14;;;"<p>You can use do it like this:</p>

<pre><code>import matplotlib.pyplot as plt 
import pandas as pd

plt.figure()
values = [[1,2], [2,5]]
df2 = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])
df2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')
plt.xlabel('xlabel')
plt.ylabel('ylabel')
plt.show()
</code></pre>

<p>Obviously you have to replace the strings 'xlabel' and 'ylabel' with what you want them to be.</p>
";;"[""import matplotlib.pyplot as plt \nimport pandas as pd\n\nplt.figure()\nvalues = [[1,2], [2,5]]\ndf2 = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])\ndf2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')\nplt.xlabel('xlabel')\nplt.ylabel('ylabel')\nplt.show()\n""]";"[""import matplotlib.pyplot as plt \nimport pandas as pd\n\nplt.figure()\nvalues = [[1,2], [2,5]]\ndf2 = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])\ndf2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')\nplt.xlabel('xlabel')\nplt.ylabel('ylabel')\nplt.show()\n""]"
1024;;1;21607530;21606987.0;2;48;;;"<p>You can give functions to the <code>rename</code> method. The <code>str.strip()</code> method should do what you want.</p>

<pre><code>In [5]: df
Out[5]: 
   Year  Month   Value
0     1       2      3

[1 rows x 3 columns]

In [6]: df.rename(columns=lambda x: x.strip())
Out[6]: 
   Year  Month  Value
0     1      2      3

[1 rows x 3 columns]
</code></pre>
";;['In [5]: df\nOut[5]: \n   Year  Month   Value\n0     1       2      3\n\n[1 rows x 3 columns]\n\nIn [6]: df.rename(columns=lambda x: x.strip())\nOut[6]: \n   Year  Month  Value\n0     1      2      3\n\n[1 rows x 3 columns]\n'];['rename', 'str.strip()', 'In [5]: df\nOut[5]: \n   Year  Month   Value\n0     1       2      3\n\n[1 rows x 3 columns]\n\nIn [6]: df.rename(columns=lambda x: x.strip())\nOut[6]: \n   Year  Month  Value\n0     1      2      3\n\n[1 rows x 3 columns]\n']
1025;;2;21608417;21608228.0;2;52;;;"<p>Try</p>

<pre><code>df.ix[df.my_channel &gt; 20000, 'my_channel'] = 0
</code></pre>
";;"[""df.ix[df.my_channel > 20000, 'my_channel'] = 0\n""]";"[""df.ix[df.my_channel > 20000, 'my_channel'] = 0\n""]"
1026;;1;21655221;21654635.0;2;14;;;"<p>With <code>plt.scatter</code>, I can only think of one: to use a proxy artist:</p>

<pre><code>df = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range('2010-01-01', freq = 'M', periods = 10), columns = ('one', 'two', 'three'))
df['key1'] = (4,4,4,6,6,6,8,8,8,8)
fig1 = plt.figure(1)
ax1 = fig1.add_subplot(111)
x=ax1.scatter(df['one'], df['two'], marker = 'o', c = df['key1'], alpha = 0.8)

ccm=x.get_cmap()
circles=[Line2D(range(1), range(1), color='w', marker='o', markersize=10, markerfacecolor=item) for item in ccm((array([4,6,8])-4.0)/4)]
leg = plt.legend(circles, ['4','6','8'], loc = ""center left"", bbox_to_anchor = (1, 0.5), numpoints = 1)
</code></pre>

<p>And the result is:</p>

<p><img src=""https://i.stack.imgur.com/7KzM5.png"" alt=""enter image description here""></p>
";;"['df = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range(\'2010-01-01\', freq = \'M\', periods = 10), columns = (\'one\', \'two\', \'three\'))\ndf[\'key1\'] = (4,4,4,6,6,6,8,8,8,8)\nfig1 = plt.figure(1)\nax1 = fig1.add_subplot(111)\nx=ax1.scatter(df[\'one\'], df[\'two\'], marker = \'o\', c = df[\'key1\'], alpha = 0.8)\n\nccm=x.get_cmap()\ncircles=[Line2D(range(1), range(1), color=\'w\', marker=\'o\', markersize=10, markerfacecolor=item) for item in ccm((array([4,6,8])-4.0)/4)]\nleg = plt.legend(circles, [\'4\',\'6\',\'8\'], loc = ""center left"", bbox_to_anchor = (1, 0.5), numpoints = 1)\n']";"['plt.scatter', 'df = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range(\'2010-01-01\', freq = \'M\', periods = 10), columns = (\'one\', \'two\', \'three\'))\ndf[\'key1\'] = (4,4,4,6,6,6,8,8,8,8)\nfig1 = plt.figure(1)\nax1 = fig1.add_subplot(111)\nx=ax1.scatter(df[\'one\'], df[\'two\'], marker = \'o\', c = df[\'key1\'], alpha = 0.8)\n\nccm=x.get_cmap()\ncircles=[Line2D(range(1), range(1), color=\'w\', marker=\'o\', markersize=10, markerfacecolor=item) for item in ccm((array([4,6,8])-4.0)/4)]\nleg = plt.legend(circles, [\'4\',\'6\',\'8\'], loc = ""center left"", bbox_to_anchor = (1, 0.5), numpoints = 1)\n']"
1027;;4;21655256;21654635.0;2;55;;;"<p>You can use <code>scatter</code> for this, but that requires having numerical values for your <code>key1</code>, and you won't have a legend, as you noticed.</p>

<p>It's better to just use <code>plot</code> for discrete categories like this.  For example:</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
np.random.seed(1974)

# Generate Data
num = 20
x, y = np.random.random((2, num))
labels = np.random.choice(['a', 'b', 'c'], num)
df = pd.DataFrame(dict(x=x, y=y, label=labels))

groups = df.groupby('label')

# Plot
fig, ax = plt.subplots()
ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling
for name, group in groups:
    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=name)
ax.legend()

plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/Svrkn.png"" alt=""enter image description here""></p>

<p>If you'd like things to look like the default <code>pandas</code> style, then just update the <code>rcParams</code> with the pandas stylesheet and use its color generator. (I'm also tweaking the legend slightly):</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
np.random.seed(1974)

# Generate Data
num = 20
x, y = np.random.random((2, num))
labels = np.random.choice(['a', 'b', 'c'], num)
df = pd.DataFrame(dict(x=x, y=y, label=labels))

groups = df.groupby('label')

# Plot
plt.rcParams.update(pd.tools.plotting.mpl_stylesheet)
colors = pd.tools.plotting._get_standard_colors(len(groups), color_type='random')

fig, ax = plt.subplots()
ax.set_color_cycle(colors)
ax.margins(0.05)
for name, group in groups:
    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=name)
ax.legend(numpoints=1, loc='upper left')

plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/VuZeq.png"" alt=""enter image description here""></p>
";;"[""import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nnp.random.seed(1974)\n\n# Generate Data\nnum = 20\nx, y = np.random.random((2, num))\nlabels = np.random.choice(['a', 'b', 'c'], num)\ndf = pd.DataFrame(dict(x=x, y=y, label=labels))\n\ngroups = df.groupby('label')\n\n# Plot\nfig, ax = plt.subplots()\nax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\nfor name, group in groups:\n    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=name)\nax.legend()\n\nplt.show()\n"", ""import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nnp.random.seed(1974)\n\n# Generate Data\nnum = 20\nx, y = np.random.random((2, num))\nlabels = np.random.choice(['a', 'b', 'c'], num)\ndf = pd.DataFrame(dict(x=x, y=y, label=labels))\n\ngroups = df.groupby('label')\n\n# Plot\nplt.rcParams.update(pd.tools.plotting.mpl_stylesheet)\ncolors = pd.tools.plotting._get_standard_colors(len(groups), color_type='random')\n\nfig, ax = plt.subplots()\nax.set_color_cycle(colors)\nax.margins(0.05)\nfor name, group in groups:\n    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=name)\nax.legend(numpoints=1, loc='upper left')\n\nplt.show()\n""]";"['scatter', 'key1', 'plot', ""import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nnp.random.seed(1974)\n\n# Generate Data\nnum = 20\nx, y = np.random.random((2, num))\nlabels = np.random.choice(['a', 'b', 'c'], num)\ndf = pd.DataFrame(dict(x=x, y=y, label=labels))\n\ngroups = df.groupby('label')\n\n# Plot\nfig, ax = plt.subplots()\nax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\nfor name, group in groups:\n    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=name)\nax.legend()\n\nplt.show()\n"", 'pandas', 'rcParams', ""import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nnp.random.seed(1974)\n\n# Generate Data\nnum = 20\nx, y = np.random.random((2, num))\nlabels = np.random.choice(['a', 'b', 'c'], num)\ndf = pd.DataFrame(dict(x=x, y=y, label=labels))\n\ngroups = df.groupby('label')\n\n# Plot\nplt.rcParams.update(pd.tools.plotting.mpl_stylesheet)\ncolors = pd.tools.plotting._get_standard_colors(len(groups), color_type='random')\n\nfig, ax = plt.subplots()\nax.set_color_cycle(colors)\nax.margins(0.05)\nfor name, group in groups:\n    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=name)\nax.legend(numpoints=1, loc='upper left')\n\nplt.show()\n""]"
1028;;5;21709413;15705630.0;2;18;;;"<p>Having tried the solution suggested by Zelazny on a relatively large DataFrame (~400k rows) I found it to be very slow.  Here is an alternative that I found to run orders of magnitude faster on my data set.</p>

<pre><code>df = pd.DataFrame({
    'sp' : ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],
    'mt' : ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],
    'val' : ['a', 'n', 'cb', 'mk', 'bg', 'dgb', 'rd', 'cb', 'uyi'],
    'count' : [3,2,5,8,10,1,2,2,7]
    })

df_grouped = df.groupby(['sp', 'mt']).agg({'count':'max'})

df_grouped = df_grouped.reset_index()

df_grouped = df_grouped.rename(columns={'count':'count_max'})

df = pd.merge(df, df_grouped, how='left', on=['sp', 'mt'])

df = df[df['count'] == df['count_max']]
</code></pre>
";;"[""df = pd.DataFrame({\n    'sp' : ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n    'mt' : ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n    'val' : ['a', 'n', 'cb', 'mk', 'bg', 'dgb', 'rd', 'cb', 'uyi'],\n    'count' : [3,2,5,8,10,1,2,2,7]\n    })\n\ndf_grouped = df.groupby(['sp', 'mt']).agg({'count':'max'})\n\ndf_grouped = df_grouped.reset_index()\n\ndf_grouped = df_grouped.rename(columns={'count':'count_max'})\n\ndf = pd.merge(df, df_grouped, how='left', on=['sp', 'mt'])\n\ndf = df[df['count'] == df['count_max']]\n""]";"[""df = pd.DataFrame({\n    'sp' : ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n    'mt' : ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n    'val' : ['a', 'n', 'cb', 'mk', 'bg', 'dgb', 'rd', 'cb', 'uyi'],\n    'count' : [3,2,5,8,10,1,2,2,7]\n    })\n\ndf_grouped = df.groupby(['sp', 'mt']).agg({'count':'max'})\n\ndf_grouped = df_grouped.reset_index()\n\ndf_grouped = df_grouped.rename(columns={'count':'count_max'})\n\ndf = pd.merge(df, df_grouped, how='left', on=['sp', 'mt'])\n\ndf = df[df['count'] == df['count_max']]\n""]"
1029;;3;21734254;21733893.0;2;40;;;"<p>The pandas DataFrame provides a nice querying ability.</p>

<p>What you are trying to do can be done simply with:</p>

<pre><code># Set a default value
df['Age_Group'] = '&lt;40'
# Set Age_Group value for all row indexes which Age are greater than 40
df['Age_Group'][df['Age'] &gt; 40] = '&gt;40'
# Set Age_Group value for all row indexes which Age are greater than 18 and &lt; 40
df['Age_Group'][(df['Age'] &gt; 18) &amp; (df['Age'] &lt; 40)] = '&gt;18'
# Set Age_Group value for all row indexes which Age are less than 18
df['Age_Group'][df['Age'] &lt; 18] = '&lt;18'
</code></pre>

<p>The querying here is a powerful tool of the dataframe and will allow you to manipulate the DataFrame as you need.</p>

<p>For more complex conditionals, you can specify multiple conditions by encapsulating each condition in parenthesis and separating them with a boolean operator ( eg. '&amp;' or '|')</p>

<p>You can see this in work here for the second conditional statement for setting >18.</p>

<p>Edit:</p>

<p>You can read more about indexing of DataFrame and conditionals:</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/dev/indexing.html#index-objects"">http://pandas.pydata.org/pandas-docs/dev/indexing.html#index-objects</a></p>

<p>Edit:</p>

<p>To see how it works:</p>

<pre><code>&gt;&gt;&gt; d = {'Age' : pd.Series([36., 42., 6., 66., 38.]) }
&gt;&gt;&gt; df = pd.DataFrame(d)
&gt;&gt;&gt; df
   Age
0   36
1   42
2    6
3   66
4   38
&gt;&gt;&gt; df['Age_Group'] = '&lt;40'
&gt;&gt;&gt; df['Age_Group'][df['Age'] &gt; 40] = '&gt;40'
&gt;&gt;&gt; df['Age_Group'][(df['Age'] &gt; 18) &amp; (df['Age'] &lt; 40)] = '&gt;18'
&gt;&gt;&gt; df['Age_Group'][df['Age'] &lt; 18] = '&lt;18'
&gt;&gt;&gt; df
   Age Age_Group
0   36       &gt;18
1   42       &gt;40
2    6       &lt;18
3   66       &gt;40
4   38       &gt;18
</code></pre>

<p>Edit:</p>

<p>To see how to do this without the chaining [using EdChums approach].</p>

<pre><code>&gt;&gt;&gt; df['Age_Group'] = '&lt;40'
&gt;&gt;&gt; df.loc[df['Age'] &lt; 40,'Age_Group'] = '&lt;40'
&gt;&gt;&gt; df.loc[(df['Age'] &gt; 18) &amp; (df['Age'] &lt; 40), 'Age_Group'] = '&gt;18'
&gt;&gt;&gt; df.loc[df['Age'] &lt; 18,'Age_Group'] = '&lt;18'
&gt;&gt;&gt; df
   Age Age_Group
0   36       &gt;18
1   42       &lt;40
2    6       &lt;18
3   66       &lt;40
4   38       &gt;18
</code></pre>
";;"[""# Set a default value\ndf['Age_Group'] = '<40'\n# Set Age_Group value for all row indexes which Age are greater than 40\ndf['Age_Group'][df['Age'] > 40] = '>40'\n# Set Age_Group value for all row indexes which Age are greater than 18 and < 40\ndf['Age_Group'][(df['Age'] > 18) & (df['Age'] < 40)] = '>18'\n# Set Age_Group value for all row indexes which Age are less than 18\ndf['Age_Group'][df['Age'] < 18] = '<18'\n"", "">>> d = {'Age' : pd.Series([36., 42., 6., 66., 38.]) }\n>>> df = pd.DataFrame(d)\n>>> df\n   Age\n0   36\n1   42\n2    6\n3   66\n4   38\n>>> df['Age_Group'] = '<40'\n>>> df['Age_Group'][df['Age'] > 40] = '>40'\n>>> df['Age_Group'][(df['Age'] > 18) & (df['Age'] < 40)] = '>18'\n>>> df['Age_Group'][df['Age'] < 18] = '<18'\n>>> df\n   Age Age_Group\n0   36       >18\n1   42       >40\n2    6       <18\n3   66       >40\n4   38       >18\n"", "">>> df['Age_Group'] = '<40'\n>>> df.loc[df['Age'] < 40,'Age_Group'] = '<40'\n>>> df.loc[(df['Age'] > 18) & (df['Age'] < 40), 'Age_Group'] = '>18'\n>>> df.loc[df['Age'] < 18,'Age_Group'] = '<18'\n>>> df\n   Age Age_Group\n0   36       >18\n1   42       <40\n2    6       <18\n3   66       <40\n4   38       >18\n""]";"[""# Set a default value\ndf['Age_Group'] = '<40'\n# Set Age_Group value for all row indexes which Age are greater than 40\ndf['Age_Group'][df['Age'] > 40] = '>40'\n# Set Age_Group value for all row indexes which Age are greater than 18 and < 40\ndf['Age_Group'][(df['Age'] > 18) & (df['Age'] < 40)] = '>18'\n# Set Age_Group value for all row indexes which Age are less than 18\ndf['Age_Group'][df['Age'] < 18] = '<18'\n"", "">>> d = {'Age' : pd.Series([36., 42., 6., 66., 38.]) }\n>>> df = pd.DataFrame(d)\n>>> df\n   Age\n0   36\n1   42\n2    6\n3   66\n4   38\n>>> df['Age_Group'] = '<40'\n>>> df['Age_Group'][df['Age'] > 40] = '>40'\n>>> df['Age_Group'][(df['Age'] > 18) & (df['Age'] < 40)] = '>18'\n>>> df['Age_Group'][df['Age'] < 18] = '<18'\n>>> df\n   Age Age_Group\n0   36       >18\n1   42       >40\n2    6       <18\n3   66       >40\n4   38       >18\n"", "">>> df['Age_Group'] = '<40'\n>>> df.loc[df['Age'] < 40,'Age_Group'] = '<40'\n>>> df.loc[(df['Age'] > 18) & (df['Age'] < 40), 'Age_Group'] = '>18'\n>>> df.loc[df['Age'] < 18,'Age_Group'] = '<18'\n>>> df\n   Age Age_Group\n0   36       >18\n1   42       <40\n2    6       <18\n3   66       <40\n4   38       >18\n""]"
1030;;2;21787325;21786490.0;2;53;;;"<p>Merge them in two steps, <code>df1</code> and <code>df2</code> first, and then the result of that to <code>df3</code>.</p>

<pre><code>In [33]: s1 = pd.merge(df1, df2, how='left', on=['Year', 'Week', 'Colour'])
</code></pre>

<p>I dropped year from df3 since you don't need it for the last join.</p>

<pre><code>In [39]: df = pd.merge(s1, df3[['Week', 'Colour', 'Val3']],
                       how='left', on=['Week', 'Colour'])

In [40]: df
Out[40]: 
   Year Week Colour  Val1  Val2 Val3
0  2014    A    Red    50   NaN  NaN
1  2014    B    Red    60   NaN   60
2  2014    B  Black    70   100   10
3  2014    C    Red    10    20  NaN
4  2014    D  Green    20   NaN   20

[5 rows x 6 columns]
</code></pre>
";;"[""In [33]: s1 = pd.merge(df1, df2, how='left', on=['Year', 'Week', 'Colour'])\n"", ""In [39]: df = pd.merge(s1, df3[['Week', 'Colour', 'Val3']],\n                       how='left', on=['Week', 'Colour'])\n\nIn [40]: df\nOut[40]: \n   Year Week Colour  Val1  Val2 Val3\n0  2014    A    Red    50   NaN  NaN\n1  2014    B    Red    60   NaN   60\n2  2014    B  Black    70   100   10\n3  2014    C    Red    10    20  NaN\n4  2014    D  Green    20   NaN   20\n\n[5 rows x 6 columns]\n""]";"['df1', 'df2', 'df3', ""In [33]: s1 = pd.merge(df1, df2, how='left', on=['Year', 'Week', 'Colour'])\n"", ""In [39]: df = pd.merge(s1, df3[['Week', 'Colour', 'Val3']],\n                       how='left', on=['Week', 'Colour'])\n\nIn [40]: df\nOut[40]: \n   Year Week Colour  Val1  Val2 Val3\n0  2014    A    Red    50   NaN  NaN\n1  2014    B    Red    60   NaN   60\n2  2014    B  Black    70   100   10\n3  2014    C    Red    10    20  NaN\n4  2014    D  Green    20   NaN   20\n\n[5 rows x 6 columns]\n""]"
1031;;4;21800319;21800169.0;2;120;;;"<p><code>df.iloc[i]</code> returns the <code>ith</code> row of <code>df</code>. <code>i</code> does not refer to the index label, <code>i</code> is a 0-based index.</p>

<p>In contrast, <strong>the attribute <code>index</code> returns actual index labels</strong>, not numeric row-indices:</p>

<pre><code>df[df['BoolCol'] == True].index.tolist()
</code></pre>

<p>or equivalently,</p>

<pre><code>df[df['BoolCol']].index.tolist()
</code></pre>

<p>You can see the difference quite clearly by playing with a DataFrame with
an ""unusual"" index:</p>

<pre><code>df = pd.DataFrame({'BoolCol': [True, False, False, True, True]},
       index=[10,20,30,40,50])

In [53]: df
Out[53]: 
   BoolCol
10    True
20   False
30   False
40    True
50    True

[5 rows x 1 columns]

In [54]: df[df['BoolCol']].index.tolist()
Out[54]: [10, 40, 50]
</code></pre>

<hr>

<p><strong>If you want to use the index labels</strong>, </p>

<pre><code>In [56]: idx = df[df['BoolCol']].index.tolist()

In [57]: idx
Out[57]: [10, 40, 50]
</code></pre>

<p><strong>then you can select the rows using <code>loc</code> instead of <code>iloc</code></strong>:</p>

<pre><code>In [58]: df.loc[idx]
Out[58]: 
   BoolCol
10    True
40    True
50    True

[3 rows x 1 columns]
</code></pre>

<hr>

<p>Note that <strong><code>loc</code> can also accept boolean arrays</strong>:</p>

<pre><code>In [55]: df.loc[df['BoolCol']]
Out[55]: 
   BoolCol
10    True
40    True
50    True

[3 rows x 1 columns]
</code></pre>

<hr>

<p><strong>If you have a boolean array, <code>mask</code>, and need ordinal index values, you can compute them using <code>np.flatnonzero</code></strong>:</p>

<pre><code>In [110]: np.flatnonzero(df['BoolCol'])
Out[112]: array([0, 3, 4])
</code></pre>

<p>Use <code>df.iloc</code> to select rows by ordinal index:</p>

<pre><code>In [113]: df.iloc[np.flatnonzero(df['BoolCol'])]
Out[113]: 
   BoolCol
10    True
40    True
50    True
</code></pre>
";;"[""df[df['BoolCol'] == True].index.tolist()\n"", ""df[df['BoolCol']].index.tolist()\n"", ""df = pd.DataFrame({'BoolCol': [True, False, False, True, True]},\n       index=[10,20,30,40,50])\n\nIn [53]: df\nOut[53]: \n   BoolCol\n10    True\n20   False\n30   False\n40    True\n50    True\n\n[5 rows x 1 columns]\n\nIn [54]: df[df['BoolCol']].index.tolist()\nOut[54]: [10, 40, 50]\n"", ""In [56]: idx = df[df['BoolCol']].index.tolist()\n\nIn [57]: idx\nOut[57]: [10, 40, 50]\n"", 'In [58]: df.loc[idx]\nOut[58]: \n   BoolCol\n10    True\n40    True\n50    True\n\n[3 rows x 1 columns]\n', ""In [55]: df.loc[df['BoolCol']]\nOut[55]: \n   BoolCol\n10    True\n40    True\n50    True\n\n[3 rows x 1 columns]\n"", ""In [110]: np.flatnonzero(df['BoolCol'])\nOut[112]: array([0, 3, 4])\n"", ""In [113]: df.iloc[np.flatnonzero(df['BoolCol'])]\nOut[113]: \n   BoolCol\n10    True\n40    True\n50    True\n""]";"['df.iloc[i]', 'ith', 'df', 'i', 'i', 'index', ""df[df['BoolCol'] == True].index.tolist()\n"", ""df[df['BoolCol']].index.tolist()\n"", ""df = pd.DataFrame({'BoolCol': [True, False, False, True, True]},\n       index=[10,20,30,40,50])\n\nIn [53]: df\nOut[53]: \n   BoolCol\n10    True\n20   False\n30   False\n40    True\n50    True\n\n[5 rows x 1 columns]\n\nIn [54]: df[df['BoolCol']].index.tolist()\nOut[54]: [10, 40, 50]\n"", ""In [56]: idx = df[df['BoolCol']].index.tolist()\n\nIn [57]: idx\nOut[57]: [10, 40, 50]\n"", 'loc', 'iloc', 'In [58]: df.loc[idx]\nOut[58]: \n   BoolCol\n10    True\n40    True\n50    True\n\n[3 rows x 1 columns]\n', 'loc', ""In [55]: df.loc[df['BoolCol']]\nOut[55]: \n   BoolCol\n10    True\n40    True\n50    True\n\n[3 rows x 1 columns]\n"", 'mask', 'np.flatnonzero', ""In [110]: np.flatnonzero(df['BoolCol'])\nOut[112]: array([0, 3, 4])\n"", 'df.iloc', ""In [113]: df.iloc[np.flatnonzero(df['BoolCol'])]\nOut[113]: \n   BoolCol\n10    True\n40    True\n50    True\n""]"
1032;;7;21916253;13703720.0;2;109;;;"<p>You can just use the pd.Timestamp constructor. The following diagram may be useful for this and related questions. </p>

<p><img src=""https://i.stack.imgur.com/uiXQd.png"" alt=""Conversions between time representations""></p>
";;[];[]
1033;;7;21942746;13445241.0;2;60;;;"<p>I think <code>df.replace()</code> does the job:</p>

<pre><code>df = pd.DataFrame([
    [-0.532681, 'foo', 0],
    [1.490752, 'bar', 1],
    [-1.387326, 'foo', 2],
    [0.814772, 'baz', ' '],     
    [-0.222552, '   ', 4],
    [-1.176781,  'qux', '  '],         
], columns='A B C'.split(), index=pd.date_range('2000-01-01','2000-01-06'))

print df.replace(r'\s+', np.nan, regex=True)
</code></pre>

<p>Produces:</p>

<pre><code>                   A    B   C
2000-01-01 -0.532681  foo   0
2000-01-02  1.490752  bar   1
2000-01-03 -1.387326  foo   2
2000-01-04  0.814772  baz NaN
2000-01-05 -0.222552  NaN   4
2000-01-06 -1.176781  qux NaN
</code></pre>
";;"[""df = pd.DataFrame([\n    [-0.532681, 'foo', 0],\n    [1.490752, 'bar', 1],\n    [-1.387326, 'foo', 2],\n    [0.814772, 'baz', ' '],     \n    [-0.222552, '   ', 4],\n    [-1.176781,  'qux', '  '],         \n], columns='A B C'.split(), index=pd.date_range('2000-01-01','2000-01-06'))\n\nprint df.replace(r'\\s+', np.nan, regex=True)\n"", '                   A    B   C\n2000-01-01 -0.532681  foo   0\n2000-01-02  1.490752  bar   1\n2000-01-03 -1.387326  foo   2\n2000-01-04  0.814772  baz NaN\n2000-01-05 -0.222552  NaN   4\n2000-01-06 -1.176781  qux NaN\n']";"['df.replace()', ""df = pd.DataFrame([\n    [-0.532681, 'foo', 0],\n    [1.490752, 'bar', 1],\n    [-1.387326, 'foo', 2],\n    [0.814772, 'baz', ' '],     \n    [-0.222552, '   ', 4],\n    [-1.176781,  'qux', '  '],         \n], columns='A B C'.split(), index=pd.date_range('2000-01-01','2000-01-06'))\n\nprint df.replace(r'\\s+', np.nan, regex=True)\n"", '                   A    B   C\n2000-01-01 -0.532681  foo   0\n2000-01-02  1.490752  bar   1\n2000-01-03 -1.387326  foo   2\n2000-01-04  0.814772  baz NaN\n2000-01-05 -0.222552  NaN   4\n2000-01-06 -1.176781  qux NaN\n']"
1034;;1;22006514;22005911.0;2;80;;;"<p>One way to convert to string is to use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html"" rel=""noreferrer"">astype</a>:</p>

<pre><code>total_rows['ColumnID'] = total_rows['ColumnID'].astype(str)
</code></pre>

<p>However, perhaps you are looking for the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html"" rel=""noreferrer""><code>to_json</code></a> function, which will convert keys to valid json (and therefore your keys to strings):</p>

<pre><code>In [11]: df = pd.DataFrame([['A', 2], ['A', 4], ['B', 6]])

In [12]: df.to_json()
Out[12]: '{""0"":{""0"":""A"",""1"":""A"",""2"":""B""},""1"":{""0"":2,""1"":4,""2"":6}}'

In [13]: df[0].to_json()
Out[13]: '{""0"":""A"",""1"":""A"",""2"":""B""}'
</code></pre>

<p><em>Note: you can pass in a buffer/file to save this to, along with some other options...</em></p>
";;"[""total_rows['ColumnID'] = total_rows['ColumnID'].astype(str)\n"", 'In [11]: df = pd.DataFrame([[\'A\', 2], [\'A\', 4], [\'B\', 6]])\n\nIn [12]: df.to_json()\nOut[12]: \'{""0"":{""0"":""A"",""1"":""A"",""2"":""B""},""1"":{""0"":2,""1"":4,""2"":6}}\'\n\nIn [13]: df[0].to_json()\nOut[13]: \'{""0"":""A"",""1"":""A"",""2"":""B""}\'\n']";"[""total_rows['ColumnID'] = total_rows['ColumnID'].astype(str)\n"", 'to_json', 'In [11]: df = pd.DataFrame([[\'A\', 2], [\'A\', 4], [\'B\', 6]])\n\nIn [12]: df.to_json()\nOut[12]: \'{""0"":{""0"":""A"",""1"":""A"",""2"":""B""},""1"":{""0"":2,""1"":4,""2"":6}}\'\n\nIn [13]: df[0].to_json()\nOut[13]: \'{""0"":""A"",""1"":""A"",""2"":""B""}\'\n']"
1035;;0;22018873;10665889.0;2;28;;;"<p>Also, Given a DataFrame </p>

<blockquote>
  <p>data</p>
</blockquote>

<p>as in your example, if you would like to extract column a and d only (e.i. the 1st and the 4th column), iloc mothod from the pandas dataframe is what you need and could be used very effectively. All you need to know is the index of the columns you would like to extract. For example:</p>

<pre><code>&gt;&gt;&gt; data.iloc[:,[0,3]]
</code></pre>

<p>will give you</p>

<pre><code>          a         d
0  0.883283  0.100975
1  0.614313  0.221731
2  0.438963  0.224361
3  0.466078  0.703347
4  0.955285  0.114033
5  0.268443  0.416996
6  0.613241  0.327548
7  0.370784  0.359159
8  0.692708  0.659410
9  0.806624  0.875476
</code></pre>
";;['>>> data.iloc[:,[0,3]]\n', '          a         d\n0  0.883283  0.100975\n1  0.614313  0.221731\n2  0.438963  0.224361\n3  0.466078  0.703347\n4  0.955285  0.114033\n5  0.268443  0.416996\n6  0.613241  0.327548\n7  0.370784  0.359159\n8  0.692708  0.659410\n9  0.806624  0.875476\n'];['>>> data.iloc[:,[0,3]]\n', '          a         d\n0  0.883283  0.100975\n1  0.614313  0.221731\n2  0.438963  0.224361\n3  0.466078  0.703347\n4  0.955285  0.114033\n5  0.268443  0.416996\n6  0.613241  0.327548\n7  0.370784  0.359159\n8  0.692708  0.659410\n9  0.806624  0.875476\n']
1036;;2;22084742;22084338.0;2;41;;;"<p>A dict is to a DataFrame as a bicycle is to a car.
You can pedal 10 feet on a bicycle faster than you can start a car, get it in gear, etc, etc. But if you need to go a mile, the car wins.</p>

<p>For certain small, targeted purposes, a dict may be faster.
And if that is all you need, then use a dict, for sure! But if you need/want the power and luxury of a DataFrame, then a dict is no substitute. It is meaningless to compare speed if the data structure does not first satisfy your needs.</p>

<p>Now for example -- to be more concrete -- a dict is good for accessing columns, but it is not so convenient for accessing rows. </p>

<pre><code>import timeit

setup = '''
import numpy, pandas
df = pandas.DataFrame(numpy.zeros(shape=[10, 1000]))
dictionary = df.to_dict()
'''

# f = ['value = dictionary[5][5]', 'value = df.loc[5, 5]', 'value = df.iloc[5, 5]']
f = ['value = [val[5] for col,val in dictionary.items()]', 'value = df.loc[5]', 'value = df.iloc[5]']

for func in f:
    print(func)
    print(min(timeit.Timer(func, setup).repeat(3, 100000)))
</code></pre>

<p>yields</p>

<pre><code>value = [val[5] for col,val in dictionary.iteritems()]
25.5416321754
value = df.loc[5]
5.68071913719
value = df.iloc[5]
4.56006002426
</code></pre>

<p>So the dict of lists is 5 times slower at retrieving rows than <code>df.iloc</code>. The speed deficit becomes greater as the number of columns grows. (The number of columns is like the number of feet in the bicycle analogy. The longer the distance, the more convenient the car becomes...)</p>

<p>This is just one example of when a dict of lists would be less convenient/slower than a DataFrame.</p>

<p>Another example would be when you have a DatetimeIndex for the rows and wish to select all rows between certain dates. With a DataFrame you can use</p>

<pre><code>df.loc['2000-1-1':'2000-3-31']
</code></pre>

<p>There is no easy analogue for that if you were to use a dict of lists. And the Python loops you would need to use to select the right rows would again be terribly slow compared to the DataFrame.</p>
";;"[""import timeit\n\nsetup = '''\nimport numpy, pandas\ndf = pandas.DataFrame(numpy.zeros(shape=[10, 1000]))\ndictionary = df.to_dict()\n'''\n\n# f = ['value = dictionary[5][5]', 'value = df.loc[5, 5]', 'value = df.iloc[5, 5]']\nf = ['value = [val[5] for col,val in dictionary.items()]', 'value = df.loc[5]', 'value = df.iloc[5]']\n\nfor func in f:\n    print(func)\n    print(min(timeit.Timer(func, setup).repeat(3, 100000)))\n"", 'value = [val[5] for col,val in dictionary.iteritems()]\n25.5416321754\nvalue = df.loc[5]\n5.68071913719\nvalue = df.iloc[5]\n4.56006002426\n', ""df.loc['2000-1-1':'2000-3-31']\n""]";"[""import timeit\n\nsetup = '''\nimport numpy, pandas\ndf = pandas.DataFrame(numpy.zeros(shape=[10, 1000]))\ndictionary = df.to_dict()\n'''\n\n# f = ['value = dictionary[5][5]', 'value = df.loc[5, 5]', 'value = df.iloc[5, 5]']\nf = ['value = [val[5] for col,val in dictionary.items()]', 'value = df.loc[5]', 'value = df.iloc[5]']\n\nfor func in f:\n    print(func)\n    print(min(timeit.Timer(func, setup).repeat(3, 100000)))\n"", 'value = [val[5] for col,val in dictionary.iteritems()]\n25.5416321754\nvalue = df.loc[5]\n5.68071913719\nvalue = df.iloc[5]\n4.56006002426\n', 'df.iloc', ""df.loc['2000-1-1':'2000-3-31']\n""]"
1037;;10;22086347;22086116.0;2;40;;;"<p>Using <code>&amp;</code> operator, don't forget to wrap the sub-statements with <code>()</code>:</p>

<pre><code>males = df[(df[Gender]=='Male') &amp; (df[Year]==2014)]
</code></pre>

<p>To store your dataframes in a <code>dict</code> using a for loop:</p>

<pre><code>from collections import defaultdict
dic={}
for g in ['male', 'female']:
  dic[g]=defaultdict(dict)
  for y in [2013, 2014]:
    dic[g][y]=df[(df[Gender]==g) &amp; (df[Year]==y)] #store the DataFrames to a dict of dict
</code></pre>

<h2>EDIT:</h2>

<p>A demo for your <code>getDF</code>:</p>

<pre><code>def getDF(dic, gender, year):
  return dic[gender][year]

print genDF(dic, 'male', 2014)
</code></pre>
";;"[""males = df[(df[Gender]=='Male') & (df[Year]==2014)]\n"", ""from collections import defaultdict\ndic={}\nfor g in ['male', 'female']:\n  dic[g]=defaultdict(dict)\n  for y in [2013, 2014]:\n    dic[g][y]=df[(df[Gender]==g) & (df[Year]==y)] #store the DataFrames to a dict of dict\n"", ""def getDF(dic, gender, year):\n  return dic[gender][year]\n\nprint genDF(dic, 'male', 2014)\n""]";"['&', '()', ""males = df[(df[Gender]=='Male') & (df[Year]==2014)]\n"", 'dict', ""from collections import defaultdict\ndic={}\nfor g in ['male', 'female']:\n  dic[g]=defaultdict(dict)\n  for y in [2013, 2014]:\n    dic[g][y]=df[(df[Gender]==g) & (df[Year]==y)] #store the DataFrames to a dict of dict\n"", 'getDF', ""def getDF(dic, gender, year):\n  return dic[gender][year]\n\nprint genDF(dic, 'male', 2014)\n""]"
1038;;0;22127685;22127569.0;2;30;;;"<p>there are a few ways;<br>
using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html#pandas.DataFrame.pivot"" rel=""noreferrer""><code>.pivot</code></a>:</p>

<pre><code>&gt;&gt;&gt; origin.pivot(index='label', columns='type')['value']
type   a  b  c
label         
x      1  2  3
y      4  5  6
z      7  8  9

[3 rows x 3 columns]
</code></pre>

<p>using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html#pandas.pivot_table"" rel=""noreferrer""><code>pivot_table</code></a>:</p>

<pre><code>&gt;&gt;&gt; origin.pivot_table(values='value', index='label', columns='type')
       value      
type       a  b  c
label             
x          1  2  3
y          4  5  6
z          7  8  9

[3 rows x 3 columns]
</code></pre>

<p>or <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby"" rel=""noreferrer""><code>.groupby</code></a> followed by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.unstack.html#pandas.DataFrame.unstack"" rel=""noreferrer""><code>.unstack</code></a>:</p>

<pre><code>&gt;&gt;&gt; origin.groupby(['label', 'type'])['value'].aggregate('mean').unstack()
type   a  b  c
label         
x      1  2  3
y      4  5  6
z      7  8  9

[3 rows x 3 columns]
</code></pre>
";;"["">>> origin.pivot(index='label', columns='type')['value']\ntype   a  b  c\nlabel         \nx      1  2  3\ny      4  5  6\nz      7  8  9\n\n[3 rows x 3 columns]\n"", "">>> origin.pivot_table(values='value', index='label', columns='type')\n       value      \ntype       a  b  c\nlabel             \nx          1  2  3\ny          4  5  6\nz          7  8  9\n\n[3 rows x 3 columns]\n"", "">>> origin.groupby(['label', 'type'])['value'].aggregate('mean').unstack()\ntype   a  b  c\nlabel         \nx      1  2  3\ny      4  5  6\nz      7  8  9\n\n[3 rows x 3 columns]\n""]";"['.pivot', "">>> origin.pivot(index='label', columns='type')['value']\ntype   a  b  c\nlabel         \nx      1  2  3\ny      4  5  6\nz      7  8  9\n\n[3 rows x 3 columns]\n"", 'pivot_table', "">>> origin.pivot_table(values='value', index='label', columns='type')\n       value      \ntype       a  b  c\nlabel             \nx          1  2  3\ny          4  5  6\nz          7  8  9\n\n[3 rows x 3 columns]\n"", '.groupby', '.unstack', "">>> origin.groupby(['label', 'type'])['value'].aggregate('mean').unstack()\ntype   a  b  c\nlabel         \nx      1  2  3\ny      4  5  6\nz      7  8  9\n\n[3 rows x 3 columns]\n""]"
1039;;7;22137890;22137723.0;2;38;;;"<p>You need to <a href=""https://stackoverflow.com/a/2308488/1240268"">set the locale</a> first:</p>

<pre><code>In [ 9]: import locale

In [10]: from locale import atof

In [11]: locale.setlocale(locale.LC_NUMERIC, '')
Out[11]: 'en_GB.UTF-8'

In [12]: df.applymap(atof)
Out[12]:
      0        1
0  1200  4200.00
1  7000    -0.03
2     5     0.00
</code></pre>

<p>If you're <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html"" rel=""nofollow noreferrer"">reading in from csv</a> then you can use the <a href=""https://stackoverflow.com/a/20667788/1240268"">thousands arg</a>:</p>

<pre><code>df.read_csv('foo.tsv', sep='\t', thousands=',')
</code></pre>
";;"[""In [ 9]: import locale\n\nIn [10]: from locale import atof\n\nIn [11]: locale.setlocale(locale.LC_NUMERIC, '')\nOut[11]: 'en_GB.UTF-8'\n\nIn [12]: df.applymap(atof)\nOut[12]:\n      0        1\n0  1200  4200.00\n1  7000    -0.03\n2     5     0.00\n"", ""df.read_csv('foo.tsv', sep='\\t', thousands=',')\n""]";"[""In [ 9]: import locale\n\nIn [10]: from locale import atof\n\nIn [11]: locale.setlocale(locale.LC_NUMERIC, '')\nOut[11]: 'en_GB.UTF-8'\n\nIn [12]: df.applymap(atof)\nOut[12]:\n      0        1\n0  1200  4200.00\n1  7000    -0.03\n2     5     0.00\n"", ""df.read_csv('foo.tsv', sep='\\t', thousands=',')\n""]"
1040;;4;22149930;22149584.0;2;130;;;"<p>It specifies the axis <strong>along which</strong> the means are computed. By default <code>axis=0</code>. This is consistent with the <code>numpy.mean</code> usage when <code>axis</code> is specified <em>explicitly</em> (in <code>numpy.mean</code>, axis==None by default, which computes the mean value over the flattened array) , in which <code>axis=0</code> along the <em>rows</em> (namely, <em>index</em> in pandas), and <code>axis=1</code> along the <em>columns</em>.</p>

<pre><code>+------------+---------+--------+
|            |  A      |  B     |
+------------+---------+---------
|      0     | 0.626386| 1.52325|----axis=1-----&gt;
+------------+---------+--------+
                |         |
                | axis=0  |
                ?         ?
</code></pre>
";;['+------------+---------+--------+\n|            |  A      |  B     |\n+------------+---------+---------\n|      0     | 0.626386| 1.52325|----axis=1----->\n+------------+---------+--------+\n                |         |\n                | axis=0  |\n                ?         ?\n'];['axis=0', 'numpy.mean', 'axis', 'numpy.mean', 'axis=0', 'axis=1', '+------------+---------+--------+\n|            |  A      |  B     |\n+------------+---------+---------\n|      0     | 0.626386| 1.52325|----axis=1----->\n+------------+---------+--------+\n                |         |\n                | axis=0  |\n                ?         ?\n']
1041;;2;22166224;10636024.0;2;6;;;"<p>Pandas 0.13 provides as an experimental feature:</p>

<p>PySide support for the qtpandas <code>DataFrameModel</code> and <code>DataFrameWidget</code></p>

<p>see <a href=""https://github.com/pydata/pandas/blob/master/doc/source/faq.rst"" rel=""noreferrer"">https://github.com/pydata/pandas/blob/master/doc/source/faq.rst</a></p>

<p>you can add this feature using</p>

<pre><code>from pandas.sandbox.qtpandas import DataFrameModel, DataFrameWidget
</code></pre>
";;['from pandas.sandbox.qtpandas import DataFrameModel, DataFrameWidget\n'];['DataFrameModel', 'DataFrameWidget', 'from pandas.sandbox.qtpandas import DataFrameModel, DataFrameWidget\n']
1042;;10;22181298;22180993.0;2;28;;;"<p>The following should work:</p>

<pre><code>@app.route('/analysis/&lt;filename&gt;')
def analysis(filename):
    x = pd.DataFrame(np.random.randn(20, 5))
    return render_template(""analysis.html"", name=filename, data=x.to_html())
                                                                # ^^^^^^^^^
</code></pre>

<p>Check <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_html.html"" rel=""nofollow noreferrer"">the documentation</a> for additional options like CSS styling.</p>

<p>Additionally, you need to adjust your template like so:</p>

<pre class=""lang-html prettyprint-override""><code>{% extends ""base.html"" %}
{% block content %}
&lt;h1&gt;{{name}}&lt;/h1&gt;
{{data | safe}}
{% endblock %}
</code></pre>

<p>in order to tell Jinja you're passing in markup. Thanks to <a href=""https://stackoverflow.com/questions/22180993/pandas-dataframe-display-on-a-webpage/22181298?noredirect=1#comment33671934_22181298"">@SeanVieira</a> for the tip.</p>
";;"['@app.route(\'/analysis/<filename>\')\ndef analysis(filename):\n    x = pd.DataFrame(np.random.randn(20, 5))\n    return render_template(""analysis.html"", name=filename, data=x.to_html())\n                                                                # ^^^^^^^^^\n', '{% extends ""base.html"" %}\n{% block content %}\n<h1>{{name}}</h1>\n{{data | safe}}\n{% endblock %}\n']";"['@app.route(\'/analysis/<filename>\')\ndef analysis(filename):\n    x = pd.DataFrame(np.random.randn(20, 5))\n    return render_template(""analysis.html"", name=filename, data=x.to_html())\n                                                                # ^^^^^^^^^\n', '{% extends ""base.html"" %}\n{% block content %}\n<h1>{{name}}</h1>\n{{data | safe}}\n{% endblock %}\n']"
1043;;1;22211821;22211737.0;2;40;;;"<p>Dataframes have a <code>sort_index</code> method which returns a copy by default. Pass <code>inplace=True</code> to operate in place.</p>

<pre><code>import pandas as pd
df = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150], columns=['A'])
df.sort_index(inplace=True)
print(df.to_string())
</code></pre>

<p>Gives me:</p>

<pre><code>     A
1    4
29   2
100  1
150  5
234  3
</code></pre>
";;"[""import pandas as pd\ndf = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150], columns=['A'])\ndf.sort_index(inplace=True)\nprint(df.to_string())\n"", '     A\n1    4\n29   2\n100  1\n150  5\n234  3\n']";"['sort_index', 'inplace=True', ""import pandas as pd\ndf = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150], columns=['A'])\ndf.sort_index(inplace=True)\nprint(df.to_string())\n"", '     A\n1    4\n29   2\n100  1\n150  5\n234  3\n']"
1044;;0;22221272;22219004.0;2;6;;;"<p>As you were saying the <code>groupby</code> method of a <code>pd.DataFrame</code> object can do the job.</p>

<p>Example</p>

<pre><code> L = ['A','A','B','B','B','C']
 N = [1,2,5,5,4,6]

 import pandas as pd
 df = pd.DataFrame(zip(L,N),columns = list('LN'))


 groups = df.groupby(df.L)

 groups.groups
      {'A': [0, 1], 'B': [2, 3, 4], 'C': [5]}
</code></pre>

<p>which gives and index-wise description of the groups.</p>

<p>To get elements of single groups, you can do, for instance</p>

<pre><code> groups.get_group('A')

     L  N
  0  A  1
  1  A  2

  groups.get_group('B')

     L  N
  2  B  5
  3  B  5
  4  B  4
</code></pre>
";;"["" L = ['A','A','B','B','B','C']\n N = [1,2,5,5,4,6]\n\n import pandas as pd\n df = pd.DataFrame(zip(L,N),columns = list('LN'))\n\n\n groups = df.groupby(df.L)\n\n groups.groups\n      {'A': [0, 1], 'B': [2, 3, 4], 'C': [5]}\n"", "" groups.get_group('A')\n\n     L  N\n  0  A  1\n  1  A  2\n\n  groups.get_group('B')\n\n     L  N\n  2  B  5\n  3  B  5\n  4  B  4\n""]";"['groupby', 'pd.DataFrame', "" L = ['A','A','B','B','B','C']\n N = [1,2,5,5,4,6]\n\n import pandas as pd\n df = pd.DataFrame(zip(L,N),columns = list('LN'))\n\n\n groups = df.groupby(df.L)\n\n groups.groups\n      {'A': [0, 1], 'B': [2, 3, 4], 'C': [5]}\n"", "" groups.get_group('A')\n\n     L  N\n  0  A  1\n  1  A  2\n\n  groups.get_group('B')\n\n     L  N\n  2  B  5\n  3  B  5\n  4  B  4\n""]"
1045;;9;22221675;22219004.0;2;62;;;"<p>You can do this using <code>groupby</code> to group on the column of interest and then <code>apply</code> <code>list</code> to every group:</p>

<pre><code>In [1]:
# create the dataframe    
df = pd.DataFrame( {'a':['A','A','B','B','B','C'], 'b':[1,2,5,5,4,6]})
df
Out[1]:
   a  b
0  A  1
1  A  2
2  B  5
3  B  5
4  B  4
5  C  6

[6 rows x 2 columns]

In [76]:
df.groupby('a')['b'].apply(list)

Out[76]:
a
A       [1, 2]
B    [5, 5, 4]
C          [6]
Name: b, dtype: object
</code></pre>
";;"[""In [1]:\n# create the dataframe    \ndf = pd.DataFrame( {'a':['A','A','B','B','B','C'], 'b':[1,2,5,5,4,6]})\ndf\nOut[1]:\n   a  b\n0  A  1\n1  A  2\n2  B  5\n3  B  5\n4  B  4\n5  C  6\n\n[6 rows x 2 columns]\n\nIn [76]:\ndf.groupby('a')['b'].apply(list)\n\nOut[76]:\na\nA       [1, 2]\nB    [5, 5, 4]\nC          [6]\nName: b, dtype: object\n""]";"['groupby', 'apply', 'list', ""In [1]:\n# create the dataframe    \ndf = pd.DataFrame( {'a':['A','A','B','B','B','C'], 'b':[1,2,5,5,4,6]})\ndf\nOut[1]:\n   a  b\n0  A  1\n1  A  2\n2  B  5\n3  B  5\n4  B  4\n5  C  6\n\n[6 rows x 2 columns]\n\nIn [76]:\ndf.groupby('a')['b'].apply(list)\n\nOut[76]:\na\nA       [1, 2]\nB    [5, 5, 4]\nC          [6]\nName: b, dtype: object\n""]"
1046;;1;22233719;22233488.0;2;97;;;"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/version/0.18.0/generated/pandas.MultiIndex.droplevel.html"" rel=""noreferrer""><code>MultiIndex.droplevel</code></a>:</p>

<pre><code>&gt;&gt;&gt; cols = pd.MultiIndex.from_tuples([(""a"", ""b""), (""a"", ""c"")])
&gt;&gt;&gt; df = pd.DataFrame([[1,2], [3,4]], columns=cols)
&gt;&gt;&gt; df
   a   
   b  c
0  1  2
1  3  4

[2 rows x 2 columns]
&gt;&gt;&gt; df.columns = df.columns.droplevel()
&gt;&gt;&gt; df
   b  c
0  1  2
1  3  4

[2 rows x 2 columns]
</code></pre>
";;"['>>> cols = pd.MultiIndex.from_tuples([(""a"", ""b""), (""a"", ""c"")])\n>>> df = pd.DataFrame([[1,2], [3,4]], columns=cols)\n>>> df\n   a   \n   b  c\n0  1  2\n1  3  4\n\n[2 rows x 2 columns]\n>>> df.columns = df.columns.droplevel()\n>>> df\n   b  c\n0  1  2\n1  3  4\n\n[2 rows x 2 columns]\n']";"['MultiIndex.droplevel', '>>> cols = pd.MultiIndex.from_tuples([(""a"", ""b""), (""a"", ""c"")])\n>>> df = pd.DataFrame([[1,2], [3,4]], columns=cols)\n>>> df\n   a   \n   b  c\n0  1  2\n1  3  4\n\n[2 rows x 2 columns]\n>>> df.columns = df.columns.droplevel()\n>>> df\n   b  c\n0  1  2\n1  3  4\n\n[2 rows x 2 columns]\n']"
1047;;1;22233851;22180993.0;2;15;;;"<p>Ok, I have managed to get some very nice results by now combining the hints I got here. In the actual Python viewer I use</p>

<pre><code>@app.route('/analysis/&lt;filename&gt;')
def analysis(filename):
    x = pd.DataFrame(np.random.randn(20, 5))
    return render_template(""analysis.html"", name=filename, data=x)
</code></pre>

<p>e.g. I send the complete dataframe to the html template. My html template is based on bootstrap. Hence I can simply write</p>

<pre><code>{% extends ""base.html"" %}
{% block content %}
&lt;h1&gt;{{name}}&lt;/h1&gt;
{{ data.to_html(classes=""table table-striped"") | safe}}
{% endblock %}
</code></pre>

<p>There are numerous other options with bootstrap, check out here:
<a href=""http://getbootstrap.com/css/#tables"" rel=""noreferrer"">http://getbootstrap.com/css/#tables</a></p>

<p>Base.html is essentially copied from here
<a href=""http://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-xii-facelift"" rel=""noreferrer"">http://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-xii-facelift</a></p>

<p>The next question is obviously how to plot such a frame. Anyone any experience with Bokeh?</p>

<p>Thank you both to Matt and Sean.</p>

<p>thomas</p>
";;"['@app.route(\'/analysis/<filename>\')\ndef analysis(filename):\n    x = pd.DataFrame(np.random.randn(20, 5))\n    return render_template(""analysis.html"", name=filename, data=x)\n', '{% extends ""base.html"" %}\n{% block content %}\n<h1>{{name}}</h1>\n{{ data.to_html(classes=""table table-striped"") | safe}}\n{% endblock %}\n']";"['@app.route(\'/analysis/<filename>\')\ndef analysis(filename):\n    x = pd.DataFrame(np.random.randn(20, 5))\n    return render_template(""analysis.html"", name=filename, data=x)\n', '{% extends ""base.html"" %}\n{% block content %}\n<h1>{{name}}</h1>\n{{ data.to_html(classes=""table table-striped"") | safe}}\n{% endblock %}\n']"
1048;;0;22235393;22235245.0;2;42;;;"<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html#pandas.DataFrame.describe""><code>describe</code></a> may give you everything you want otherwise you can perform aggregations using groupby and pass a list of agg functions: <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#applying-multiple-functions-at-once"">http://pandas.pydata.org/pandas-docs/stable/groupby.html#applying-multiple-functions-at-once</a></p>

<pre><code>In [43]:

df.describe()

Out[43]:

       shopper_num is_martian  number_of_items  count_pineapples
count      14.0000         14        14.000000                14
mean        7.5000          0         3.357143                 0
std         4.1833          0         6.452276                 0
min         1.0000      False         0.000000                 0
25%         4.2500          0         0.000000                 0
50%         7.5000          0         0.000000                 0
75%        10.7500          0         3.500000                 0
max        14.0000      False        22.000000                 0

[8 rows x 4 columns]
</code></pre>

<p>Note that some columns cannot be summarised as there is no logical way to summarise them, for instance columns containing string data</p>

<p>As you prefer you can transpose the result if you prefer:</p>

<pre><code>In [47]:

df.describe().transpose()

Out[47]:

                 count      mean       std    min   25%  50%    75%    max
shopper_num         14       7.5    4.1833      1  4.25  7.5  10.75     14
is_martian          14         0         0  False     0    0      0  False
number_of_items     14  3.357143  6.452276      0     0    0    3.5     22
count_pineapples    14         0         0      0     0    0      0      0

[4 rows x 8 columns]
</code></pre>
";;['In [43]:\n\ndf.describe()\n\nOut[43]:\n\n       shopper_num is_martian  number_of_items  count_pineapples\ncount      14.0000         14        14.000000                14\nmean        7.5000          0         3.357143                 0\nstd         4.1833          0         6.452276                 0\nmin         1.0000      False         0.000000                 0\n25%         4.2500          0         0.000000                 0\n50%         7.5000          0         0.000000                 0\n75%        10.7500          0         3.500000                 0\nmax        14.0000      False        22.000000                 0\n\n[8 rows x 4 columns]\n', 'In [47]:\n\ndf.describe().transpose()\n\nOut[47]:\n\n                 count      mean       std    min   25%  50%    75%    max\nshopper_num         14       7.5    4.1833      1  4.25  7.5  10.75     14\nis_martian          14         0         0  False     0    0      0  False\nnumber_of_items     14  3.357143  6.452276      0     0    0    3.5     22\ncount_pineapples    14         0         0      0     0    0      0      0\n\n[4 rows x 8 columns]\n'];['describe', 'In [43]:\n\ndf.describe()\n\nOut[43]:\n\n       shopper_num is_martian  number_of_items  count_pineapples\ncount      14.0000         14        14.000000                14\nmean        7.5000          0         3.357143                 0\nstd         4.1833          0         6.452276                 0\nmin         1.0000      False         0.000000                 0\n25%         4.2500          0         0.000000                 0\n50%         7.5000          0         0.000000                 0\n75%        10.7500          0         3.500000                 0\nmax        14.0000      False        22.000000                 0\n\n[8 rows x 4 columns]\n', 'In [47]:\n\ndf.describe().transpose()\n\nOut[47]:\n\n                 count      mean       std    min   25%  50%    75%    max\nshopper_num         14       7.5    4.1833      1  4.25  7.5  10.75     14\nis_martian          14         0         0  False     0    0      0  False\nnumber_of_items     14  3.357143  6.452276      0     0    0    3.5     22\ncount_pineapples    14         0         0      0     0    0      0      0\n\n[4 rows x 8 columns]\n']
1049;;3;22238380;13682044.0;2;14;;;"<p>In the particular case where you know the number of positions that you want to remove from the dataframe column, you can use string indexing inside a lambda function to get rid of that parts:</p>

<p>Last character:</p>

<pre><code>data['result'] = data['result'].map(lambda x: str(x)[:-1])
</code></pre>

<p>First two characters:</p>

<pre><code>data['result'] = data['result'].map(lambda x: str(x)[2:])
</code></pre>
";;"[""data['result'] = data['result'].map(lambda x: str(x)[:-1])\n"", ""data['result'] = data['result'].map(lambda x: str(x)[2:])\n""]";"[""data['result'] = data['result'].map(lambda x: str(x)[:-1])\n"", ""data['result'] = data['result'].map(lambda x: str(x)[2:])\n""]"
1050;;8;22341390;22341271.0;2;109;;;"<p>Pandas DataFrame columns are Pandas Series when you pull them out, which you can then call <code>.tolist()</code> on to turn them into a Python list</p>

<pre><code>from pandas import *

d = {'one' : Series([1., 2., 3.], index=['a', 'b', 'c']),
    'two' : Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])}

df = DataFrame(d)

#print df

print ""DF"", type(df['one']), ""\n"", df['one']

dfList = df['one'].tolist()

print ""DF list"", dfList, type(dfList)
</code></pre>

<p><a href=""https://stackoverflow.com/questions/14822680/convert-python-dataframe-to-list"">This question</a> might be helpful. And the <a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html"" rel=""noreferrer"">Pandas docs</a> are actually quite good once you get your head around their style. </p>

<p>So in your case you could:</p>

<p><code>my_list = df[""cluster""].tolist()</code></p>

<p>and then go from there.</p>
";;"['from pandas import *\n\nd = {\'one\' : Series([1., 2., 3.], index=[\'a\', \'b\', \'c\']),\n    \'two\' : Series([1., 2., 3., 4.], index=[\'a\', \'b\', \'c\', \'d\'])}\n\ndf = DataFrame(d)\n\n#print df\n\nprint ""DF"", type(df[\'one\']), ""\\n"", df[\'one\']\n\ndfList = df[\'one\'].tolist()\n\nprint ""DF list"", dfList, type(dfList)\n']";"['.tolist()', 'from pandas import *\n\nd = {\'one\' : Series([1., 2., 3.], index=[\'a\', \'b\', \'c\']),\n    \'two\' : Series([1., 2., 3., 4.], index=[\'a\', \'b\', \'c\', \'d\'])}\n\ndf = DataFrame(d)\n\n#print df\n\nprint ""DF"", type(df[\'one\']), ""\\n"", df[\'one\']\n\ndfList = df[\'one\'].tolist()\n\nprint ""DF list"", dfList, type(dfList)\n', 'my_list = df[""cluster""].tolist()']"
1051;;6;22391554;22391433.0;2;73;;;"<p>Use <code>groupby</code> and <code>count</code>:</p>

<pre><code>In [37]:
df = pd.DataFrame({'a':list('abssbab')})
df.groupby('a').count()

Out[37]:

   a
a   
a  2
b  3
s  2

[3 rows x 1 columns]
</code></pre>

<p>See the online docs: <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/groupby.html</a></p>

<p>Also <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html"" rel=""noreferrer""><code>value_counts()</code></a> as @DSM has commented, many ways to skin a cat here</p>

<pre><code>In [38]:
df['a'].value_counts()

Out[38]:

b    3
a    2
s    2
dtype: int64
</code></pre>

<p>If you wanted to add frequency back to the original dataframe use <code>transform</code> to return an aligned index:</p>

<pre><code>In [41]:
df['freq'] = df.groupby('a')['a'].transform('count')
df

Out[41]:

   a freq
0  a    2
1  b    3
2  s    2
3  s    2
4  b    3
5  a    2
6  b    3

[7 rows x 2 columns]
</code></pre>
";;"[""In [37]:\ndf = pd.DataFrame({'a':list('abssbab')})\ndf.groupby('a').count()\n\nOut[37]:\n\n   a\na   \na  2\nb  3\ns  2\n\n[3 rows x 1 columns]\n"", ""In [38]:\ndf['a'].value_counts()\n\nOut[38]:\n\nb    3\na    2\ns    2\ndtype: int64\n"", ""In [41]:\ndf['freq'] = df.groupby('a')['a'].transform('count')\ndf\n\nOut[41]:\n\n   a freq\n0  a    2\n1  b    3\n2  s    2\n3  s    2\n4  b    3\n5  a    2\n6  b    3\n\n[7 rows x 2 columns]\n""]";"['groupby', 'count', ""In [37]:\ndf = pd.DataFrame({'a':list('abssbab')})\ndf.groupby('a').count()\n\nOut[37]:\n\n   a\na   \na  2\nb  3\ns  2\n\n[3 rows x 1 columns]\n"", 'value_counts()', ""In [38]:\ndf['a'].value_counts()\n\nOut[38]:\n\nb    3\na    2\ns    2\ndtype: int64\n"", 'transform', ""In [41]:\ndf['freq'] = df.groupby('a')['a'].transform('count')\ndf\n\nOut[41]:\n\n   a freq\n0  a    2\n1  b    3\n2  s    2\n3  s    2\n4  b    3\n5  a    2\n6  b    3\n\n[7 rows x 2 columns]\n""]"
1052;;0;22455322;22403469.0;2;21;;;"<p>@behzad.nouri's solution worked perfectly to return the first and last non-<code>NaN values</code> using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.first_valid_index.html#pandas.Series.first_valid_index"" rel=""noreferrer"">Series.first_valid_index</a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.last_valid_index.html#pandas.Series.last_valid_index"" rel=""noreferrer"">Series.last_valid_index</a>, respectively.</p>
";;[];['NaN values']
1053;;1;22471217;22470690.0;2;13;;;"<p>You can use boolean mask on the dtypes attribute:</p>

<pre><code>In [11]: df = pd.DataFrame([[1, 2.3456, 'c']])

In [12]: df.dtypes
Out[12]: 
0      int64
1    float64
2     object
dtype: object

In [13]: msk = df.dtypes == np.float64  # or object, etc.

In [14]: msk
Out[14]: 
0    False
1     True
2    False
dtype: bool
</code></pre>

<p>You can look at just those columns with the desired dtype:</p>

<pre><code>In [15]: df.loc[:, msk]
Out[15]: 
        1
0  2.3456
</code></pre>

<p>Now you can use round (or whatever) and assign it back:</p>

<pre><code>In [16]: np.round(df.loc[:, msk], 2)
Out[16]: 
      1
0  2.35

In [17]: df.loc[:, msk] = np.round(df.loc[:, msk], 2)

In [18]: df
Out[18]: 
   0     1  2
0  1  2.35  c
</code></pre>
";;"[""In [11]: df = pd.DataFrame([[1, 2.3456, 'c']])\n\nIn [12]: df.dtypes\nOut[12]: \n0      int64\n1    float64\n2     object\ndtype: object\n\nIn [13]: msk = df.dtypes == np.float64  # or object, etc.\n\nIn [14]: msk\nOut[14]: \n0    False\n1     True\n2    False\ndtype: bool\n"", 'In [15]: df.loc[:, msk]\nOut[15]: \n        1\n0  2.3456\n', 'In [16]: np.round(df.loc[:, msk], 2)\nOut[16]: \n      1\n0  2.35\n\nIn [17]: df.loc[:, msk] = np.round(df.loc[:, msk], 2)\n\nIn [18]: df\nOut[18]: \n   0     1  2\n0  1  2.35  c\n']";"[""In [11]: df = pd.DataFrame([[1, 2.3456, 'c']])\n\nIn [12]: df.dtypes\nOut[12]: \n0      int64\n1    float64\n2     object\ndtype: object\n\nIn [13]: msk = df.dtypes == np.float64  # or object, etc.\n\nIn [14]: msk\nOut[14]: \n0    False\n1     True\n2    False\ndtype: bool\n"", 'In [15]: df.loc[:, msk]\nOut[15]: \n        1\n0  2.3456\n', 'In [16]: np.round(df.loc[:, msk], 2)\nOut[16]: \n      1\n0  2.35\n\nIn [17]: df.loc[:, msk] = np.round(df.loc[:, msk], 2)\n\nIn [18]: df\nOut[18]: \n   0     1  2\n0  1  2.35  c\n']"
1054;;2;22475141;22470690.0;2;126;;;"<p>If you want a list of columns of a certain type, you can use <code>groupby</code>:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([[1, 2.3456, 'c', 'd', 78]], columns=list(""ABCDE""))
&gt;&gt;&gt; df
   A       B  C  D   E
0  1  2.3456  c  d  78

[1 rows x 5 columns]
&gt;&gt;&gt; df.dtypes
A      int64
B    float64
C     object
D     object
E      int64
dtype: object
&gt;&gt;&gt; g = df.columns.to_series().groupby(df.dtypes).groups
&gt;&gt;&gt; g
{dtype('int64'): ['A', 'E'], dtype('float64'): ['B'], dtype('O'): ['C', 'D']}
&gt;&gt;&gt; {k.name: v for k, v in g.items()}
{'object': ['C', 'D'], 'int64': ['A', 'E'], 'float64': ['B']}
</code></pre>
";;"['>>> df = pd.DataFrame([[1, 2.3456, \'c\', \'d\', 78]], columns=list(""ABCDE""))\n>>> df\n   A       B  C  D   E\n0  1  2.3456  c  d  78\n\n[1 rows x 5 columns]\n>>> df.dtypes\nA      int64\nB    float64\nC     object\nD     object\nE      int64\ndtype: object\n>>> g = df.columns.to_series().groupby(df.dtypes).groups\n>>> g\n{dtype(\'int64\'): [\'A\', \'E\'], dtype(\'float64\'): [\'B\'], dtype(\'O\'): [\'C\', \'D\']}\n>>> {k.name: v for k, v in g.items()}\n{\'object\': [\'C\', \'D\'], \'int64\': [\'A\', \'E\'], \'float64\': [\'B\']}\n']";"['groupby', '>>> df = pd.DataFrame([[1, 2.3456, \'c\', \'d\', 78]], columns=list(""ABCDE""))\n>>> df\n   A       B  C  D   E\n0  1  2.3456  c  d  78\n\n[1 rows x 5 columns]\n>>> df.dtypes\nA      int64\nB    float64\nC     object\nD     object\nE      int64\ndtype: object\n>>> g = df.columns.to_series().groupby(df.dtypes).groups\n>>> g\n{dtype(\'int64\'): [\'A\', \'E\'], dtype(\'float64\'): [\'B\'], dtype(\'O\'): [\'C\', \'D\']}\n>>> {k.name: v for k, v in g.items()}\n{\'object\': [\'C\', \'D\'], \'int64\': [\'A\', \'E\'], \'float64\': [\'B\']}\n']"
1055;;4;22484249;22483588.0;2;86;;;"<p>You can manually create the subplots with matplotlib, and then plot the dataframes on a specific subplot using the <code>ax</code> keyword. For example for 4 subplots (2x2):</p>

<pre><code>import matplotlib.pyplot as plt

fig, axes = plt.subplots(nrows=2, ncols=2)

df1.plot(ax=axes[0,0])
df2.plot(ax=axes[0,1])
...
</code></pre>

<p>Here <code>axes</code> is an array which holds the different subplot axes, and you can access one just by indexing <code>axes</code>.<br>
If you want a shared x-axis, then you can provide <code>sharex=True</code> to <code>plt.subplots</code>.</p>
";;['import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\n\ndf1.plot(ax=axes[0,0])\ndf2.plot(ax=axes[0,1])\n...\n'];['ax', 'import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\n\ndf1.plot(ax=axes[0,0])\ndf2.plot(ax=axes[0,1])\n...\n', 'axes', 'axes', 'sharex=True', 'plt.subplots']
1056;;1;22496075;16852911.0;2;12;;;"<p>I imagine a lot of data comes into Pandas from CSV files, in which case you can simply convert the date during the initial CSV read:</p>

<p><code>dfcsv = pd.read_csv('xyz.csv', parse_dates=[0])</code> where the 0 refers to the column the date is in.<br>
You could also add <code>, index_col=0</code> in there if you want the date to be your index.</p>

<p>See <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html</a></p>
";;[];"[""dfcsv = pd.read_csv('xyz.csv', parse_dates=[0])"", ', index_col=0']"
1057;;0;22543333;22543208.0;2;21;;;"<p><strong>Update:</strong> If you have matplotlib >= 1.4, there is a new <code>style</code> module which has a <code>ggplot</code> style by default. To activate this, use:</p>

<pre><code>from matplotlib import pyplot as plt
plt.style.use('ggplot')
</code></pre>

<p>This is recommended above the styling through the pandas options as explained below (and is also used in the pandas docs now).</p>

<hr>

<p>For pandas, use:</p>

<pre><code>pd.options.display.mpl_style = 'default'
</code></pre>

<p>and this will give you the 'ggplot-like' style for matplotlib figures (Note that the name is a bit confusing as this is not enabled by default, and this should actually be added to the docs).  </p>

<p>For seaborn, as Paul H commented, it is enough to <code>import seaborn</code></p>

<p>By the way, if you really want something like ggplot in python with ggplot syntax (and not only ggplot-like style), there is also a <strong>python ggplot library</strong> based on pandas: <a href=""https://github.com/yhat/ggplot/"">https://github.com/yhat/ggplot/</a></p>
";;"[""from matplotlib import pyplot as plt\nplt.style.use('ggplot')\n"", ""pd.options.display.mpl_style = 'default'\n""]";"['style', 'ggplot', ""from matplotlib import pyplot as plt\nplt.style.use('ggplot')\n"", ""pd.options.display.mpl_style = 'default'\n"", 'import seaborn']"
1058;;6;22546459;22546425.0;2;40;;;"<p>You need to enclose multiple conditions in braces due to operator precedence and use the bitwise and (<code>&amp;</code>) and or (<code>|</code>) operators:</p>

<pre><code>foo = df.ix[(df['column1']==value) | (df['columns2'] == 'b') | (df['column3'] == 'c')]
</code></pre>

<p>If you use <code>and</code> or <code>or</code>, then pandas is likely to moan that the comparison is ambiguous. In that case, it is unclear whether we are comparing every value in a series in the condition, and what does it mean if only 1 or all but 1 match the condition. That is why you should use the bitwise operators or the numpy <code>np.all</code> or <code>np.any</code> to specify the matching criteria.</p>

<p>There is also the query method: <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.query.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.query.html</a></p>

<p>but there are some limitations mainly to do with issues where there could be ambiguity between column names and index values.</p>
";;"[""foo = df.ix[(df['column1']==value) | (df['columns2'] == 'b') | (df['column3'] == 'c')]\n""]";"['&', '|', ""foo = df.ix[(df['column1']==value) | (df['columns2'] == 'b') | (df['column3'] == 'c')]\n"", 'and', 'or', 'np.all', 'np.any']"
1059;;0;22547347;22546425.0;2;11;;;"<p>A more concise--but not necessarily faster--method is to use <code>DataFrame.isin()</code> and <code>DataFrame.any()</code></p>

<pre><code>In [27]: n = 10

In [28]: df = DataFrame(randint(4, size=(n, 2)), columns=list('ab'))

In [29]: df
Out[29]:
   a  b
0  0  0
1  1  1
2  1  1
3  2  3
4  2  3
5  0  2
6  1  2
7  3  0
8  1  1
9  2  2

[10 rows x 2 columns]

In [30]: df.isin([1, 2])
Out[30]:
       a      b
0  False  False
1   True   True
2   True   True
3   True  False
4   True  False
5  False   True
6   True   True
7  False  False
8   True   True
9   True   True

[10 rows x 2 columns]

In [31]: df.isin([1, 2]).any(1)
Out[31]:
0    False
1     True
2     True
3     True
4     True
5     True
6     True
7    False
8     True
9     True
dtype: bool

In [32]: df.loc[df.isin([1, 2]).any(1)]
Out[32]:
   a  b
1  1  1
2  1  1
3  2  3
4  2  3
5  0  2
6  1  2
8  1  1
9  2  2

[8 rows x 2 columns]
</code></pre>
";;"[""In [27]: n = 10\n\nIn [28]: df = DataFrame(randint(4, size=(n, 2)), columns=list('ab'))\n\nIn [29]: df\nOut[29]:\n   a  b\n0  0  0\n1  1  1\n2  1  1\n3  2  3\n4  2  3\n5  0  2\n6  1  2\n7  3  0\n8  1  1\n9  2  2\n\n[10 rows x 2 columns]\n\nIn [30]: df.isin([1, 2])\nOut[30]:\n       a      b\n0  False  False\n1   True   True\n2   True   True\n3   True  False\n4   True  False\n5  False   True\n6   True   True\n7  False  False\n8   True   True\n9   True   True\n\n[10 rows x 2 columns]\n\nIn [31]: df.isin([1, 2]).any(1)\nOut[31]:\n0    False\n1     True\n2     True\n3     True\n4     True\n5     True\n6     True\n7    False\n8     True\n9     True\ndtype: bool\n\nIn [32]: df.loc[df.isin([1, 2]).any(1)]\nOut[32]:\n   a  b\n1  1  1\n2  1  1\n3  2  3\n4  2  3\n5  0  2\n6  1  2\n8  1  1\n9  2  2\n\n[8 rows x 2 columns]\n""]";"['DataFrame.isin()', 'DataFrame.any()', ""In [27]: n = 10\n\nIn [28]: df = DataFrame(randint(4, size=(n, 2)), columns=list('ab'))\n\nIn [29]: df\nOut[29]:\n   a  b\n0  0  0\n1  1  1\n2  1  1\n3  2  3\n4  2  3\n5  0  2\n6  1  2\n7  3  0\n8  1  1\n9  2  2\n\n[10 rows x 2 columns]\n\nIn [30]: df.isin([1, 2])\nOut[30]:\n       a      b\n0  False  False\n1   True   True\n2   True   True\n3   True  False\n4   True  False\n5  False   True\n6   True   True\n7  False  False\n8   True   True\n9   True   True\n\n[10 rows x 2 columns]\n\nIn [31]: df.isin([1, 2]).any(1)\nOut[31]:\n0    False\n1     True\n2     True\n3     True\n4     True\n5     True\n6     True\n7    False\n8     True\n9     True\ndtype: bool\n\nIn [32]: df.loc[df.isin([1, 2]).any(1)]\nOut[32]:\n   a  b\n1  1  1\n2  1  1\n3  2  3\n4  2  3\n5  0  2\n6  1  2\n8  1  1\n9  2  2\n\n[8 rows x 2 columns]\n""]"
1060;;4;22553757;22551403.0;2;73;;;"<p>Just drop them:</p>

<pre><code>nms.dropna(thresh=2)
</code></pre>

<p>this will drop all rows where there are at least two <code>NaN</code></p>

<p>then you could then drop where name is <code>NaN</code>:</p>

<pre><code>In [87]:

nms
Out[87]:
  movie    name  rating
0   thg    John       3
1   thg     NaN       4
3   mol  Graham     NaN
4   lob     NaN     NaN
5   lob     NaN     NaN

[5 rows x 3 columns]
In [89]:

nms = nms.dropna(thresh=2)
In [90]:

nms[nms.name.notnull()]
Out[90]:
  movie    name  rating
0   thg    John       3
3   mol  Graham     NaN

[2 rows x 3 columns]
</code></pre>

<p><strong>EDIT</strong></p>

<p>Actually looking at what you originally want you can do just this without the <code>dropna</code> call:</p>

<pre><code>nms[nms.name.notnull()]
</code></pre>
";;['nms.dropna(thresh=2)\n', 'In [87]:\n\nnms\nOut[87]:\n  movie    name  rating\n0   thg    John       3\n1   thg     NaN       4\n3   mol  Graham     NaN\n4   lob     NaN     NaN\n5   lob     NaN     NaN\n\n[5 rows x 3 columns]\nIn [89]:\n\nnms = nms.dropna(thresh=2)\nIn [90]:\n\nnms[nms.name.notnull()]\nOut[90]:\n  movie    name  rating\n0   thg    John       3\n3   mol  Graham     NaN\n\n[2 rows x 3 columns]\n', 'nms[nms.name.notnull()]\n'];['nms.dropna(thresh=2)\n', 'NaN', 'NaN', 'In [87]:\n\nnms\nOut[87]:\n  movie    name  rating\n0   thg    John       3\n1   thg     NaN       4\n3   mol  Graham     NaN\n4   lob     NaN     NaN\n5   lob     NaN     NaN\n\n[5 rows x 3 columns]\nIn [89]:\n\nnms = nms.dropna(thresh=2)\nIn [90]:\n\nnms[nms.name.notnull()]\nOut[90]:\n  movie    name  rating\n0   thg    John       3\n3   mol  Graham     NaN\n\n[2 rows x 3 columns]\n', 'dropna', 'nms[nms.name.notnull()]\n']
1061;;7;22588340;22588316.0;2;10;;;"<p>You could remove all the non-digits using <code>re.sub()</code>:</p>

<pre><code>value = re.sub(r""[^0-9]+"", """", value)
</code></pre>

<p><a href=""http://regex101.com/r/yS7lG7"">regex101 demo</a></p>
";;"['value = re.sub(r""[^0-9]+"", """", value)\n']";"['re.sub()', 'value = re.sub(r""[^0-9]+"", """", value)\n']"
1062;;1;22591024;22588316.0;2;53;;;"<p>You could use <a href=""http://pandas.pydata.org/pandas-docs/stable/basics.html#vectorized-string-methods"" rel=""noreferrer""><code>Series.str.replace</code></a>:</p>

<pre><code>import pandas as pd

df = pd.DataFrame(['$40,000*','$40000 conditions attached'], columns=['P'])
print(df)
#                             P
# 0                    $40,000*
# 1  $40000 conditions attached

df['P'] = df['P'].str.replace(r'\D+', '').astype('int')
print(df)
</code></pre>

<p>yields</p>

<pre><code>       P
0  40000
1  40000
</code></pre>

<p>since <code>\D</code> matches any <a href=""https://docs.python.org/3/library/re.html#regular-expression-syntax"" rel=""noreferrer"">non-decimal digit</a>.</p>
";;"[""import pandas as pd\n\ndf = pd.DataFrame(['$40,000*','$40000 conditions attached'], columns=['P'])\nprint(df)\n#                             P\n# 0                    $40,000*\n# 1  $40000 conditions attached\n\ndf['P'] = df['P'].str.replace(r'\\D+', '').astype('int')\nprint(df)\n"", '       P\n0  40000\n1  40000\n']";"['Series.str.replace', ""import pandas as pd\n\ndf = pd.DataFrame(['$40,000*','$40000 conditions attached'], columns=['P'])\nprint(df)\n#                             P\n# 0                    $40,000*\n# 1  $40000 conditions attached\n\ndf['P'] = df['P'].str.replace(r'\\D+', '').astype('int')\nprint(df)\n"", '       P\n0  40000\n1  40000\n', '\\D']"
1063;;3;22591267;22591174.0;2;49;;;"<blockquote>
  <p>As you can see, the AND operator drops every row in which at least one
  value equals -1. On the other hand, the OR operator requires both
  values to be equal to -1 to drop them.</p>
</blockquote>

<p>That's right.  Remember that you're writing the condition in terms of what you want to <em>keep</em>, not in terms of what you want to drop.  For <code>df1</code>:</p>

<pre><code>df1 = df[(df.a != -1) &amp; (df.b != -1)]
</code></pre>

<p>You're saying ""keep the rows in which <code>df.a</code> isn't -1 and <code>df.b</code> isn't -1"", which is the same as dropping every row in which at least one value is -1.</p>

<p>For <code>df2</code>:</p>

<pre><code>df2 = df[(df.a != -1) | (df.b != -1)]
</code></pre>

<p>You're saying ""keep the rows in which either <code>df.a</code> or <code>df.b</code> is not -1"", which is the same as dropping rows where both values are -1.</p>

<p>PS: chained access like <code>df['a'][1] = -1</code> can get you into trouble.  It's better to get into the habit of using <code>.loc</code> and <code>.iloc</code>.</p>
";;['df1 = df[(df.a != -1) & (df.b != -1)]\n', 'df2 = df[(df.a != -1) | (df.b != -1)]\n'];"['df1', 'df1 = df[(df.a != -1) & (df.b != -1)]\n', 'df.a', 'df.b', 'df2', 'df2 = df[(df.a != -1) | (df.b != -1)]\n', 'df.a', 'df.b', ""df['a'][1] = -1"", '.loc', '.iloc']"
1064;;3;22596982;13411544.0;2;135;;;"<pre><code>df.drop([Column Name or list],inplace=True,axis=1)
</code></pre>

<p>will delete one or more columns inplace.</p>
";;['df.drop([Column Name or list],inplace=True,axis=1)\n'];['df.drop([Column Name or list],inplace=True,axis=1)\n']
1065;;1;22605281;22604564.0;2;138;;;"<p>Simple way to do this was to use <a href=""https://docs.python.org/2/library/io.html#io.StringIO"" rel=""noreferrer""><code>StringIO</code></a> and pass that to the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv"" rel=""noreferrer""><code>pandas.read_csv</code></a> function. E.g:</p>

<pre><code>import sys
if sys.version_info[0] &lt; 3: 
    from StringIO import StringIO
else:
    from io import StringIO

import pandas as pd

TESTDATA=StringIO(""""""col1;col2;col3
    1;4.4;99
    2;4.5;200
    3;4.7;65
    4;3.2;140
    """""")

df = pd.read_csv(TESTDATA, sep="";"")
</code></pre>
";;"['import sys\nif sys.version_info[0] < 3: \n    from StringIO import StringIO\nelse:\n    from io import StringIO\n\nimport pandas as pd\n\nTESTDATA=StringIO(""""""col1;col2;col3\n    1;4.4;99\n    2;4.5;200\n    3;4.7;65\n    4;3.2;140\n    """""")\n\ndf = pd.read_csv(TESTDATA, sep="";"")\n']";"['StringIO', 'pandas.read_csv', 'import sys\nif sys.version_info[0] < 3: \n    from StringIO import StringIO\nelse:\n    from io import StringIO\n\nimport pandas as pd\n\nTESTDATA=StringIO(""""""col1;col2;col3\n    1;4.4;99\n    2;4.5;200\n    3;4.7;65\n    4;3.2;140\n    """""")\n\ndf = pd.read_csv(TESTDATA, sep="";"")\n']"
1066;;6;22650075;22649693.0;2;34;;;"<p>It turns out this can be nicely expressed in a vectorized fashion:</p>

<pre><code>&gt; df = pd.DataFrame({'a':[0,0,1,1], 'b':[0,1,0,1]})
&gt; df = df[(df.T != 0).any()]
&gt; df
   a  b
1  0  1
2  1  0
3  1  1
</code></pre>
";;"[""> df = pd.DataFrame({'a':[0,0,1,1], 'b':[0,1,0,1]})\n> df = df[(df.T != 0).any()]\n> df\n   a  b\n1  0  1\n2  1  0\n3  1  1\n""]";"[""> df = pd.DataFrame({'a':[0,0,1,1], 'b':[0,1,0,1]})\n> df = df[(df.T != 0).any()]\n> df\n   a  b\n1  0  1\n2  1  0\n3  1  1\n""]"
1067;;3;22650162;22649693.0;2;32;;;"<p>One-liner.  No transpose needed:</p>

<pre><code>df.loc[~(df==0).all(axis=1)]
</code></pre>

<p>And for those who like symmetry, this also works...</p>

<pre><code>df.loc[(df!=0).any(axis=1)]
</code></pre>
";;['df.loc[~(df==0).all(axis=1)]\n', 'df.loc[(df!=0).any(axis=1)]\n'];['df.loc[~(df==0).all(axis=1)]\n', 'df.loc[(df!=0).any(axis=1)]\n']
1068;;2;22653050;13187778.0;2;22;;;"<p>I would just chain the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html"">DataFrame.reset_index()</a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.values.html"">DataFrame.values</a> functions to get the Numpy representation of the dataframe, including the index:</p>

<pre><code>In [8]: df
Out[8]: 
          A         B         C
0 -0.982726  0.150726  0.691625
1  0.617297 -0.471879  0.505547
2  0.417123 -1.356803 -1.013499
3 -0.166363 -0.957758  1.178659
4 -0.164103  0.074516 -0.674325
5 -0.340169 -0.293698  1.231791
6 -1.062825  0.556273  1.508058
7  0.959610  0.247539  0.091333

[8 rows x 3 columns]

In [9]: df.reset_index().values
Out[9]:
array([[ 0.        , -0.98272574,  0.150726  ,  0.69162512],
       [ 1.        ,  0.61729734, -0.47187926,  0.50554728],
       [ 2.        ,  0.4171228 , -1.35680324, -1.01349922],
       [ 3.        , -0.16636303, -0.95775849,  1.17865945],
       [ 4.        , -0.16410334,  0.0745164 , -0.67432474],
       [ 5.        , -0.34016865, -0.29369841,  1.23179064],
       [ 6.        , -1.06282542,  0.55627285,  1.50805754],
       [ 7.        ,  0.95961001,  0.24753911,  0.09133339]])
</code></pre>

<p>To get the dtypes we'd need to transform this ndarray into a structured array using <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.view.html"">view</a>:</p>

<pre><code>In [10]: df.reset_index().values.ravel().view(dtype=[('index', int), ('A', float), ('B', float), ('C', float)])
Out[10]:
array([( 0, -0.98272574,  0.150726  ,  0.69162512),
       ( 1,  0.61729734, -0.47187926,  0.50554728),
       ( 2,  0.4171228 , -1.35680324, -1.01349922),
       ( 3, -0.16636303, -0.95775849,  1.17865945),
       ( 4, -0.16410334,  0.0745164 , -0.67432474),
       ( 5, -0.34016865, -0.29369841,  1.23179064),
       ( 6, -1.06282542,  0.55627285,  1.50805754),
       ( 7,  0.95961001,  0.24753911,  0.09133339),
       dtype=[('index', '&lt;i8'), ('A', '&lt;f8'), ('B', '&lt;f8'), ('C', '&lt;f8')])
</code></pre>
";;"['In [8]: df\nOut[8]: \n          A         B         C\n0 -0.982726  0.150726  0.691625\n1  0.617297 -0.471879  0.505547\n2  0.417123 -1.356803 -1.013499\n3 -0.166363 -0.957758  1.178659\n4 -0.164103  0.074516 -0.674325\n5 -0.340169 -0.293698  1.231791\n6 -1.062825  0.556273  1.508058\n7  0.959610  0.247539  0.091333\n\n[8 rows x 3 columns]\n\nIn [9]: df.reset_index().values\nOut[9]:\narray([[ 0.        , -0.98272574,  0.150726  ,  0.69162512],\n       [ 1.        ,  0.61729734, -0.47187926,  0.50554728],\n       [ 2.        ,  0.4171228 , -1.35680324, -1.01349922],\n       [ 3.        , -0.16636303, -0.95775849,  1.17865945],\n       [ 4.        , -0.16410334,  0.0745164 , -0.67432474],\n       [ 5.        , -0.34016865, -0.29369841,  1.23179064],\n       [ 6.        , -1.06282542,  0.55627285,  1.50805754],\n       [ 7.        ,  0.95961001,  0.24753911,  0.09133339]])\n', ""In [10]: df.reset_index().values.ravel().view(dtype=[('index', int), ('A', float), ('B', float), ('C', float)])\nOut[10]:\narray([( 0, -0.98272574,  0.150726  ,  0.69162512),\n       ( 1,  0.61729734, -0.47187926,  0.50554728),\n       ( 2,  0.4171228 , -1.35680324, -1.01349922),\n       ( 3, -0.16636303, -0.95775849,  1.17865945),\n       ( 4, -0.16410334,  0.0745164 , -0.67432474),\n       ( 5, -0.34016865, -0.29369841,  1.23179064),\n       ( 6, -1.06282542,  0.55627285,  1.50805754),\n       ( 7,  0.95961001,  0.24753911,  0.09133339),\n       dtype=[('index', '<i8'), ('A', '<f8'), ('B', '<f8'), ('C', '<f8')])\n""]";"['In [8]: df\nOut[8]: \n          A         B         C\n0 -0.982726  0.150726  0.691625\n1  0.617297 -0.471879  0.505547\n2  0.417123 -1.356803 -1.013499\n3 -0.166363 -0.957758  1.178659\n4 -0.164103  0.074516 -0.674325\n5 -0.340169 -0.293698  1.231791\n6 -1.062825  0.556273  1.508058\n7  0.959610  0.247539  0.091333\n\n[8 rows x 3 columns]\n\nIn [9]: df.reset_index().values\nOut[9]:\narray([[ 0.        , -0.98272574,  0.150726  ,  0.69162512],\n       [ 1.        ,  0.61729734, -0.47187926,  0.50554728],\n       [ 2.        ,  0.4171228 , -1.35680324, -1.01349922],\n       [ 3.        , -0.16636303, -0.95775849,  1.17865945],\n       [ 4.        , -0.16410334,  0.0745164 , -0.67432474],\n       [ 5.        , -0.34016865, -0.29369841,  1.23179064],\n       [ 6.        , -1.06282542,  0.55627285,  1.50805754],\n       [ 7.        ,  0.95961001,  0.24753911,  0.09133339]])\n', ""In [10]: df.reset_index().values.ravel().view(dtype=[('index', int), ('A', float), ('B', float), ('C', float)])\nOut[10]:\narray([( 0, -0.98272574,  0.150726  ,  0.69162512),\n       ( 1,  0.61729734, -0.47187926,  0.50554728),\n       ( 2,  0.4171228 , -1.35680324, -1.01349922),\n       ( 3, -0.16636303, -0.95775849,  1.17865945),\n       ( 4, -0.16410334,  0.0745164 , -0.67432474),\n       ( 5, -0.34016865, -0.29369841,  1.23179064),\n       ( 6, -1.06282542,  0.55627285,  1.50805754),\n       ( 7,  0.95961001,  0.24753911,  0.09133339),\n       dtype=[('index', '<i8'), ('A', '<f8'), ('B', '<f8'), ('C', '<f8')])\n""]"
1069;;0;22657894;11346283.0;2;83;;;"<p>Since you only want to remove the $ sign in all column names, you could just do:</p>

<pre><code>df = df.rename(columns=lambda x: x.replace('$', ''))
</code></pre>

<p>OR</p>

<pre><code>df.rename(columns=lambda x: x.replace('$', ''), inplace=True)
</code></pre>
";;"[""df = df.rename(columns=lambda x: x.replace('$', ''))\n"", ""df.rename(columns=lambda x: x.replace('$', ''), inplace=True)\n""]";"[""df = df.rename(columns=lambda x: x.replace('$', ''))\n"", ""df.rename(columns=lambda x: x.replace('$', ''), inplace=True)\n""]"
1070;;0;22674279;19237878.0;2;14;;;"<p>Just for someone looking for a solution more similar to R:</p>

<pre><code>df[(df.Product == p_id) &amp; (df.Time&gt; start_time) &amp; (df.Time &lt; end_time)][['Time','Product']]
</code></pre>

<p>No need for <code>data.loc</code> or <code>query</code>, but I do think it is a bit long.</p>
";;"[""df[(df.Product == p_id) & (df.Time> start_time) & (df.Time < end_time)][['Time','Product']]\n""]";"[""df[(df.Product == p_id) & (df.Time> start_time) & (df.Time < end_time)][['Time','Product']]\n"", 'data.loc', 'query']"
1071;;2;22676213;22676081.0;2;27;;;"<p>I always use <code>join</code> on indices:</p>

<pre><code>import pandas as pd
left = pd.DataFrame({'key': ['foo', 'bar'], 'val': [1, 2]}).set_index('key')
right = pd.DataFrame({'key': ['foo', 'bar'], 'val': [4, 5]}).set_index('key')
left.join(right, lsuffix='_l', rsuffix='_r')

     val_l  val_r
key            
foo      1      4
bar      2      5
</code></pre>

<p>The same functionality can be had by using <code>merge</code> on the columns follows:</p>

<pre><code>left = pd.DataFrame({'key': ['foo', 'bar'], 'val': [1, 2]})
right = pd.DataFrame({'key': ['foo', 'bar'], 'val': [4, 5]})
left.merge(right, on=('key'), suffixes=('_l', '_r'))

   key  val_l  val_r
0  foo      1      4
1  bar      2      5
</code></pre>
";;"[""import pandas as pd\nleft = pd.DataFrame({'key': ['foo', 'bar'], 'val': [1, 2]}).set_index('key')\nright = pd.DataFrame({'key': ['foo', 'bar'], 'val': [4, 5]}).set_index('key')\nleft.join(right, lsuffix='_l', rsuffix='_r')\n\n     val_l  val_r\nkey            \nfoo      1      4\nbar      2      5\n"", ""left = pd.DataFrame({'key': ['foo', 'bar'], 'val': [1, 2]})\nright = pd.DataFrame({'key': ['foo', 'bar'], 'val': [4, 5]})\nleft.merge(right, on=('key'), suffixes=('_l', '_r'))\n\n   key  val_l  val_r\n0  foo      1      4\n1  bar      2      5\n""]";"['join', ""import pandas as pd\nleft = pd.DataFrame({'key': ['foo', 'bar'], 'val': [1, 2]}).set_index('key')\nright = pd.DataFrame({'key': ['foo', 'bar'], 'val': [4, 5]}).set_index('key')\nleft.join(right, lsuffix='_l', rsuffix='_r')\n\n     val_l  val_r\nkey            \nfoo      1      4\nbar      2      5\n"", 'merge', ""left = pd.DataFrame({'key': ['foo', 'bar'], 'val': [1, 2]})\nright = pd.DataFrame({'key': ['foo', 'bar'], 'val': [4, 5]})\nleft.merge(right, on=('key'), suffixes=('_l', '_r'))\n\n   key  val_l  val_r\n0  foo      1      4\n1  bar      2      5\n""]"
1072;;3;22697903;22697773.0;2;41;;;"<p>You can access the data-type of a column with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dtype.html""><code>dtype</code></a>:</p>

<pre><code>for y in agg.columns:
    if(agg[y].dtype == np.float64 or agg[y].dtype == np.int64):
          treat_numeric(agg[y])
    else:
          treat_str(agg[y])
</code></pre>
";;['for y in agg.columns:\n    if(agg[y].dtype == np.float64 or agg[y].dtype == np.int64):\n          treat_numeric(agg[y])\n    else:\n          treat_str(agg[y])\n'];['dtype', 'for y in agg.columns:\n    if(agg[y].dtype == np.float64 or agg[y].dtype == np.int64):\n          treat_numeric(agg[y])\n    else:\n          treat_str(agg[y])\n']
1073;;0;22719983;11858472.0;2;9;;;"<p>You could also use</p>

<pre><code>df['bar'] = df['bar'].str.cat(df['foo'].values.astype(str), sep=' is ')
</code></pre>
";;"[""df['bar'] = df['bar'].str.cat(df['foo'].values.astype(str), sep=' is ')\n""]";"[""df['bar'] = df['bar'].str.cat(df['foo'].values.astype(str), sep=' is ')\n""]"
1074;;1;22798849;13999850.0;2;45;;;"<p>With the new version of Pandas you can use the date_format parameter of the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html"">to_csv</a> method:</p>

<pre><code>df.to_csv(filename, date_format='%Y%m%d')
</code></pre>
";;"[""df.to_csv(filename, date_format='%Y%m%d')\n""]";"[""df.to_csv(filename, date_format='%Y%m%d')\n""]"
1075;;8;22845857;22787209.0;2;31;;;"<p>So, I eventually found a trick (edit: see below for using seaborn and longform dataframe):</p>

<h3>Solution with pandas and matplotlib</h3>

<p>Here it is with a more complete example :</p>

<pre><code>import pandas as pd
import matplotlib.cm as cm
import numpy as np
import matplotlib.pyplot as plt

def plot_clustered_stacked(dfall, labels=None, title=""multiple stacked bar plot"",  H=""/"", **kwargs):
    """"""Given a list of dataframes, with identical columns and index, create a clustered stacked bar plot. 
labels is a list of the names of the dataframe, used for the legend
title is a string for the title of the plot
H is the hatch used for identification of the different dataframe""""""

    n_df = len(dfall)
    n_col = len(dfall[0].columns) 
    n_ind = len(dfall[0].index)
    axe = plt.subplot(111)

    for df in dfall : # for each data frame
        axe = df.plot(kind=""bar"",
                      linewidth=0,
                      stacked=True,
                      ax=axe,
                      legend=False,
                      grid=False,
                      **kwargs)  # make bar plots

    h,l = axe.get_legend_handles_labels() # get the handles we want to modify
    for i in range(0, n_df * n_col, n_col): # len(h) = n_col * n_df
        for j, pa in enumerate(h[i:i+n_col]):
            for rect in pa.patches: # for each index
                rect.set_x(rect.get_x() + 1 / float(n_df + 1) * i / float(n_col))
                rect.set_hatch(H * int(i / n_col)) #edited part     
                rect.set_width(1 / float(n_df + 1))

    axe.set_xticks((np.arange(0, 2 * n_ind, 2) + 1 / float(n_df + 1)) / 2.)
    axe.set_xticklabels(df.index, rotation = 0)
    axe.set_title(title)

    # Add invisible data to add another legend
    n=[]        
    for i in range(n_df):
        n.append(axe.bar(0, 0, color=""gray"", hatch=H * i))

    l1 = axe.legend(h[:n_col], l[:n_col], loc=[1.01, 0.5])
    if labels is not None:
        l2 = plt.legend(n, labels, loc=[1.01, 0.1]) 
    axe.add_artist(l1)
    return axe

# create fake dataframes
df1 = pd.DataFrame(np.random.rand(4, 5),
                   index=[""A"", ""B"", ""C"", ""D""],
                   columns=[""I"", ""J"", ""K"", ""L"", ""M""])
df2 = pd.DataFrame(np.random.rand(4, 5),
                   index=[""A"", ""B"", ""C"", ""D""],
                   columns=[""I"", ""J"", ""K"", ""L"", ""M""])
df3 = pd.DataFrame(np.random.rand(4, 5),
                   index=[""A"", ""B"", ""C"", ""D""], 
                   columns=[""I"", ""J"", ""K"", ""L"", ""M""])

# Then, just call :
plot_clustered_stacked([df1, df2, df3],[""df1"", ""df2"", ""df3""])
</code></pre>

<p>And it gives that :</p>

<p><img src=""https://i.stack.imgur.com/3ZdAH.png"" alt=""multiple stacked bar plot""></p>

<p>You can change the colors of the bar by passing a <code>cmap</code> argument: </p>

<pre><code>plot_clustered_stacked([df1, df2, df3],
                       [""df1"", ""df2"", ""df3""],
                       cmap=plt.cm.viridis)
</code></pre>

<hr>

<h3>Solution with seaborn:</h3>

<p>Given the same df1, df2, df3, below, I convert them in a long form:</p>

<pre><code>df1[""Name""] = ""df1""
df2[""Name""] = ""df2""
df3[""Name""] = ""df3""
dfall = pd.concat([pd.melt(i.reset_index(),
                           id_vars=[""Name"", ""index""]) # transform in tidy format each df
                   for i in [df1, df2, df3]],
                   ignore_index=True)
</code></pre>

<p>The problem with seaborn is that it doesn't stack bars natively, so the trick is to plot the cumulative sum of each bar on top of each other:</p>

<pre><code>dfall.set_index([""Name"", ""index"", ""variable""], inplace=1)
dfall[""vcs""] = dfall.groupby(level=[""Name"", ""index""]).cumsum()
dfall.reset_index(inplace=True) 

&gt;&gt;&gt; dfall.head(6)
  Name index variable     value       vcs
0  df1     A        I  0.717286  0.717286
1  df1     B        I  0.236867  0.236867
2  df1     C        I  0.952557  0.952557
3  df1     D        I  0.487995  0.487995
4  df1     A        J  0.174489  0.891775
5  df1     B        J  0.332001  0.568868
</code></pre>

<p>Then loop over each group of <code>variable</code> and plot the cumulative sum:</p>

<pre><code>c = [""blue"", ""purple"", ""red"", ""green"", ""pink""]
for i, g in enumerate(dfall.groupby(""variable"")):
    ax = sns.barplot(data=g[1],
                     x=""index"",
                     y=""vcs"",
                     hue=""Name"",
                     color=c[i],
                     zorder=-i, # so first bars stay on top
                     edgecolor=""k"")
ax.legend_.remove() # remove the redundant legends 
</code></pre>

<p><a href=""https://i.stack.imgur.com/mVUc1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mVUc1.png"" alt=""multiple stack bar plot seaborn""></a></p>

<p>It lacks the legend that can be added easily I think. The problem is that instead of hatches (which can be added easily) to differentiate the dataframes we have a gradient of lightness, and it's a bit too light for the first one, and I don't really know how to change that without changing each rectangle one by one (as in the first solution).</p>

<p>Tell me if you don't understand something in the code.</p>

<p>Feel free to re-use this code which is under CC0.</p>
";;"['import pandas as pd\nimport matplotlib.cm as cm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_clustered_stacked(dfall, labels=None, title=""multiple stacked bar plot"",  H=""/"", **kwargs):\n    """"""Given a list of dataframes, with identical columns and index, create a clustered stacked bar plot. \nlabels is a list of the names of the dataframe, used for the legend\ntitle is a string for the title of the plot\nH is the hatch used for identification of the different dataframe""""""\n\n    n_df = len(dfall)\n    n_col = len(dfall[0].columns) \n    n_ind = len(dfall[0].index)\n    axe = plt.subplot(111)\n\n    for df in dfall : # for each data frame\n        axe = df.plot(kind=""bar"",\n                      linewidth=0,\n                      stacked=True,\n                      ax=axe,\n                      legend=False,\n                      grid=False,\n                      **kwargs)  # make bar plots\n\n    h,l = axe.get_legend_handles_labels() # get the handles we want to modify\n    for i in range(0, n_df * n_col, n_col): # len(h) = n_col * n_df\n        for j, pa in enumerate(h[i:i+n_col]):\n            for rect in pa.patches: # for each index\n                rect.set_x(rect.get_x() + 1 / float(n_df + 1) * i / float(n_col))\n                rect.set_hatch(H * int(i / n_col)) #edited part     \n                rect.set_width(1 / float(n_df + 1))\n\n    axe.set_xticks((np.arange(0, 2 * n_ind, 2) + 1 / float(n_df + 1)) / 2.)\n    axe.set_xticklabels(df.index, rotation = 0)\n    axe.set_title(title)\n\n    # Add invisible data to add another legend\n    n=[]        \n    for i in range(n_df):\n        n.append(axe.bar(0, 0, color=""gray"", hatch=H * i))\n\n    l1 = axe.legend(h[:n_col], l[:n_col], loc=[1.01, 0.5])\n    if labels is not None:\n        l2 = plt.legend(n, labels, loc=[1.01, 0.1]) \n    axe.add_artist(l1)\n    return axe\n\n# create fake dataframes\ndf1 = pd.DataFrame(np.random.rand(4, 5),\n                   index=[""A"", ""B"", ""C"", ""D""],\n                   columns=[""I"", ""J"", ""K"", ""L"", ""M""])\ndf2 = pd.DataFrame(np.random.rand(4, 5),\n                   index=[""A"", ""B"", ""C"", ""D""],\n                   columns=[""I"", ""J"", ""K"", ""L"", ""M""])\ndf3 = pd.DataFrame(np.random.rand(4, 5),\n                   index=[""A"", ""B"", ""C"", ""D""], \n                   columns=[""I"", ""J"", ""K"", ""L"", ""M""])\n\n# Then, just call :\nplot_clustered_stacked([df1, df2, df3],[""df1"", ""df2"", ""df3""])\n', 'plot_clustered_stacked([df1, df2, df3],\n                       [""df1"", ""df2"", ""df3""],\n                       cmap=plt.cm.viridis)\n', 'df1[""Name""] = ""df1""\ndf2[""Name""] = ""df2""\ndf3[""Name""] = ""df3""\ndfall = pd.concat([pd.melt(i.reset_index(),\n                           id_vars=[""Name"", ""index""]) # transform in tidy format each df\n                   for i in [df1, df2, df3]],\n                   ignore_index=True)\n', 'dfall.set_index([""Name"", ""index"", ""variable""], inplace=1)\ndfall[""vcs""] = dfall.groupby(level=[""Name"", ""index""]).cumsum()\ndfall.reset_index(inplace=True) \n\n>>> dfall.head(6)\n  Name index variable     value       vcs\n0  df1     A        I  0.717286  0.717286\n1  df1     B        I  0.236867  0.236867\n2  df1     C        I  0.952557  0.952557\n3  df1     D        I  0.487995  0.487995\n4  df1     A        J  0.174489  0.891775\n5  df1     B        J  0.332001  0.568868\n', 'c = [""blue"", ""purple"", ""red"", ""green"", ""pink""]\nfor i, g in enumerate(dfall.groupby(""variable"")):\n    ax = sns.barplot(data=g[1],\n                     x=""index"",\n                     y=""vcs"",\n                     hue=""Name"",\n                     color=c[i],\n                     zorder=-i, # so first bars stay on top\n                     edgecolor=""k"")\nax.legend_.remove() # remove the redundant legends \n']";"['import pandas as pd\nimport matplotlib.cm as cm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_clustered_stacked(dfall, labels=None, title=""multiple stacked bar plot"",  H=""/"", **kwargs):\n    """"""Given a list of dataframes, with identical columns and index, create a clustered stacked bar plot. \nlabels is a list of the names of the dataframe, used for the legend\ntitle is a string for the title of the plot\nH is the hatch used for identification of the different dataframe""""""\n\n    n_df = len(dfall)\n    n_col = len(dfall[0].columns) \n    n_ind = len(dfall[0].index)\n    axe = plt.subplot(111)\n\n    for df in dfall : # for each data frame\n        axe = df.plot(kind=""bar"",\n                      linewidth=0,\n                      stacked=True,\n                      ax=axe,\n                      legend=False,\n                      grid=False,\n                      **kwargs)  # make bar plots\n\n    h,l = axe.get_legend_handles_labels() # get the handles we want to modify\n    for i in range(0, n_df * n_col, n_col): # len(h) = n_col * n_df\n        for j, pa in enumerate(h[i:i+n_col]):\n            for rect in pa.patches: # for each index\n                rect.set_x(rect.get_x() + 1 / float(n_df + 1) * i / float(n_col))\n                rect.set_hatch(H * int(i / n_col)) #edited part     \n                rect.set_width(1 / float(n_df + 1))\n\n    axe.set_xticks((np.arange(0, 2 * n_ind, 2) + 1 / float(n_df + 1)) / 2.)\n    axe.set_xticklabels(df.index, rotation = 0)\n    axe.set_title(title)\n\n    # Add invisible data to add another legend\n    n=[]        \n    for i in range(n_df):\n        n.append(axe.bar(0, 0, color=""gray"", hatch=H * i))\n\n    l1 = axe.legend(h[:n_col], l[:n_col], loc=[1.01, 0.5])\n    if labels is not None:\n        l2 = plt.legend(n, labels, loc=[1.01, 0.1]) \n    axe.add_artist(l1)\n    return axe\n\n# create fake dataframes\ndf1 = pd.DataFrame(np.random.rand(4, 5),\n                   index=[""A"", ""B"", ""C"", ""D""],\n                   columns=[""I"", ""J"", ""K"", ""L"", ""M""])\ndf2 = pd.DataFrame(np.random.rand(4, 5),\n                   index=[""A"", ""B"", ""C"", ""D""],\n                   columns=[""I"", ""J"", ""K"", ""L"", ""M""])\ndf3 = pd.DataFrame(np.random.rand(4, 5),\n                   index=[""A"", ""B"", ""C"", ""D""], \n                   columns=[""I"", ""J"", ""K"", ""L"", ""M""])\n\n# Then, just call :\nplot_clustered_stacked([df1, df2, df3],[""df1"", ""df2"", ""df3""])\n', 'cmap', 'plot_clustered_stacked([df1, df2, df3],\n                       [""df1"", ""df2"", ""df3""],\n                       cmap=plt.cm.viridis)\n', 'df1[""Name""] = ""df1""\ndf2[""Name""] = ""df2""\ndf3[""Name""] = ""df3""\ndfall = pd.concat([pd.melt(i.reset_index(),\n                           id_vars=[""Name"", ""index""]) # transform in tidy format each df\n                   for i in [df1, df2, df3]],\n                   ignore_index=True)\n', 'dfall.set_index([""Name"", ""index"", ""variable""], inplace=1)\ndfall[""vcs""] = dfall.groupby(level=[""Name"", ""index""]).cumsum()\ndfall.reset_index(inplace=True) \n\n>>> dfall.head(6)\n  Name index variable     value       vcs\n0  df1     A        I  0.717286  0.717286\n1  df1     B        I  0.236867  0.236867\n2  df1     C        I  0.952557  0.952557\n3  df1     D        I  0.487995  0.487995\n4  df1     A        J  0.174489  0.891775\n5  df1     B        J  0.332001  0.568868\n', 'variable', 'c = [""blue"", ""purple"", ""red"", ""green"", ""pink""]\nfor i, g in enumerate(dfall.groupby(""variable"")):\n    ax = sns.barplot(data=g[1],\n                     x=""index"",\n                     y=""vcs"",\n                     hue=""Name"",\n                     color=c[i],\n                     zorder=-i, # so first bars stay on top\n                     edgecolor=""k"")\nax.legend_.remove() # remove the redundant legends \n']"
1076;;5;22898920;22898824.0;2;49;;;"<p>If it's the index, you should use the <code>.ix</code> or <code>.loc</code> selector.</p>

<p>For example:</p>

<pre><code>df.ix['2014-01-01':'2014-02-01']
</code></pre>

<p>See details here <a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#indexing-selection"">http://pandas.pydata.org/pandas-docs/stable/dsintro.html#indexing-selection</a></p>

<p>I guess it's smart to read up on pandas quite extensively before you start, as it's a very powerful library. Otherwise it will be hard to get anything meaningful done.</p>

<p><strong>UPDATE:</strong></p>

<p>If the column is not the index you have two choices:</p>

<ol>
<li>Make it the index (either temporarily or permanently if it's time-series data)</li>
<li><code>df[(df['date'] &gt; '2013-01-01') &amp; (df['date'] &lt; '2013-02-01')]</code></li>
</ol>

<p>See <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"">here</a> for the general explanation</p>
";;"[""df.ix['2014-01-01':'2014-02-01']\n""]";"['.ix', '.loc', ""df.ix['2014-01-01':'2014-02-01']\n"", ""df[(df['date'] > '2013-01-01') & (df['date'] < '2013-02-01')]""]"
1077;;2;22920808;20853474.0;2;120;;;"<p>On Ubuntu you may need to install the package manager <code>pip</code> first:</p>

<pre><code>sudo apt-get install python-pip
</code></pre>

<p>Then install the <code>python-dateutil</code> package with:</p>

<pre><code>sudo pip install python-dateutil
</code></pre>
";;['sudo apt-get install python-pip\n', 'sudo pip install python-dateutil\n'];['pip', 'sudo apt-get install python-pip\n', 'python-dateutil', 'sudo pip install python-dateutil\n']
1078;;2;22924683;22923775.0;2;29;;;"<p>Pandas timestamp differences returns a datetime.timedelta object. This can easily be converted into hours by using the *as_type* method, like so</p>

<pre><code>import pandas
df = pandas.DataFrame(columns=['to','fr','ans'])
df.to = [pandas.Timestamp('2014-01-24 13:03:12.050000'), pandas.Timestamp('2014-01-27 11:57:18.240000'), pandas.Timestamp('2014-01-23 10:07:47.660000')]
df.fr = [pandas.Timestamp('2014-01-26 23:41:21.870000'), pandas.Timestamp('2014-01-27 15:38:22.540000'), pandas.Timestamp('2014-01-23 18:50:41.420000')]
(df.fr-df.to).astype('timedelta64[h]')
</code></pre>

<p>to yield,</p>

<pre><code>0    58
1     3
2     8
dtype: float64
</code></pre>
";;"[""import pandas\ndf = pandas.DataFrame(columns=['to','fr','ans'])\ndf.to = [pandas.Timestamp('2014-01-24 13:03:12.050000'), pandas.Timestamp('2014-01-27 11:57:18.240000'), pandas.Timestamp('2014-01-23 10:07:47.660000')]\ndf.fr = [pandas.Timestamp('2014-01-26 23:41:21.870000'), pandas.Timestamp('2014-01-27 15:38:22.540000'), pandas.Timestamp('2014-01-23 18:50:41.420000')]\n(df.fr-df.to).astype('timedelta64[h]')\n"", '0    58\n1     3\n2     8\ndtype: float64\n']";"[""import pandas\ndf = pandas.DataFrame(columns=['to','fr','ans'])\ndf.to = [pandas.Timestamp('2014-01-24 13:03:12.050000'), pandas.Timestamp('2014-01-27 11:57:18.240000'), pandas.Timestamp('2014-01-23 10:07:47.660000')]\ndf.fr = [pandas.Timestamp('2014-01-26 23:41:21.870000'), pandas.Timestamp('2014-01-27 15:38:22.540000'), pandas.Timestamp('2014-01-23 18:50:41.420000')]\n(df.fr-df.to).astype('timedelta64[h]')\n"", '0    58\n1     3\n2     8\ndtype: float64\n']"
1079;;1;22964673;22963263.0;2;38;;;"<p>You can try this:</p>

<pre><code>d = pd.DataFrame(0, index=np.arange(len(data)), columns=feature_list)
</code></pre>
";;['d = pd.DataFrame(0, index=np.arange(len(data)), columns=feature_list)\n'];['d = pd.DataFrame(0, index=np.arange(len(data)), columns=feature_list)\n']
1080;;2;22974440;17097236.0;2;9;;;"<p><code>where</code> is probably what you're looking for. So</p>

<pre><code>data=data.where(data=='-', None) 
</code></pre>

<p>From the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.where.html"" rel=""noreferrer"" title=""docs"">panda docs</a>: </p>

<blockquote>
  <p><code>where</code> [returns] an object of same shape as self and whose corresponding entries are from self where cond is True and otherwise are from other).</p>
</blockquote>
";;"[""data=data.where(data=='-', None) \n""]";"['where', ""data=data.where(data=='-', None) \n"", 'where']"
1081;;1;22992568;11350770.0;2;17;;;"<p>Quick note: if you want to do selection based on a partial string contained in the index, try the following:</p>

<pre><code>df['stridx']=df.index
df[df['stridx'].str.contains(""Hello|Britain"")]
</code></pre>
";;"['df[\'stridx\']=df.index\ndf[df[\'stridx\'].str.contains(""Hello|Britain"")]\n']";"['df[\'stridx\']=df.index\ndf[df[\'stridx\'].str.contains(""Hello|Britain"")]\n']"
1082;;3;23088780;17095101.0;2;15;;;"<p>I have faced this issue, but found an answer before finding this post :</p>

<p>Based on unutbu's answer, load your data...</p>

<pre><code>import pandas as pd
import io

texts = ['''\
id   Name   score                    isEnrolled                       Date
111  Jack                            True              2013-05-01 12:00:00
112  Nick   1.11                     False             2013-05-12 15:05:23
     Zoe    4.12                     True                                  ''',

         '''\
id   Name   score                    isEnrolled                       Date
111  Jack   2.17                     True              2013-05-01 12:00:00
112  Nick   1.21                     False                                
     Zoe    4.12                     False             2013-05-01 12:00:00''']


df1 = pd.read_fwf(io.BytesIO(texts[0]), widths=[5,7,25,17,20], parse_dates=[4])
df2 = pd.read_fwf(io.BytesIO(texts[1]), widths=[5,7,25,17,20], parse_dates=[4])
</code></pre>

<p>...define your <em>diff</em> function...</p>

<pre><code>def report_diff(x):
    return x[0] if x[0] == x[1] else '{} | {}'.format(*x)
</code></pre>

<p>Then you can simply use a Panel to conclude :</p>

<pre><code>my_panel = pd.Panel(dict(df1=df1,df2=df2))
print my_panel.apply(report_diff, axis=0)

#          id  Name        score    isEnrolled                       Date
#0        111  Jack   nan | 2.17          True        2013-05-01 12:00:00
#1        112  Nick  1.11 | 1.21         False  2013-05-12 15:05:23 | NaT
#2  nan | nan   Zoe         4.12  True | False  NaT | 2013-05-01 12:00:00
</code></pre>

<p>By the way, if you're in IPython Notebook, you may like to use a colored <em>diff</em> function
to give colors depending whether cells are different, equal or left/right null :</p>

<pre><code>from IPython.display import HTML
pd.options.display.max_colwidth = 500  # You need this, otherwise pandas
#                          will limit your HTML strings to 50 characters

def report_diff(x):
    if x[0]==x[1]:
        return unicode(x[0].__str__())
    elif pd.isnull(x[0]) and pd.isnull(x[1]):
        return u'&lt;table style=""background-color:#00ff00;font-weight:bold;""&gt;'+\
            '&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;' % ('nan', 'nan')
    elif pd.isnull(x[0]) and ~pd.isnull(x[1]):
        return u'&lt;table style=""background-color:#ffff00;font-weight:bold;""&gt;'+\
            '&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;' % ('nan', x[1])
    elif ~pd.isnull(x[0]) and pd.isnull(x[1]):
        return u'&lt;table style=""background-color:#0000ff;font-weight:bold;""&gt;'+\
            '&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;' % (x[0],'nan')
    else:
        return u'&lt;table style=""background-color:#ff0000;font-weight:bold;""&gt;'+\
            '&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;' % (x[0], x[1])

HTML(my_panel.apply(report_diff, axis=0).to_html(escape=False))
</code></pre>
";;"[""import pandas as pd\nimport io\n\ntexts = ['''\\\nid   Name   score                    isEnrolled                       Date\n111  Jack                            True              2013-05-01 12:00:00\n112  Nick   1.11                     False             2013-05-12 15:05:23\n     Zoe    4.12                     True                                  ''',\n\n         '''\\\nid   Name   score                    isEnrolled                       Date\n111  Jack   2.17                     True              2013-05-01 12:00:00\n112  Nick   1.21                     False                                \n     Zoe    4.12                     False             2013-05-01 12:00:00''']\n\n\ndf1 = pd.read_fwf(io.BytesIO(texts[0]), widths=[5,7,25,17,20], parse_dates=[4])\ndf2 = pd.read_fwf(io.BytesIO(texts[1]), widths=[5,7,25,17,20], parse_dates=[4])\n"", ""def report_diff(x):\n    return x[0] if x[0] == x[1] else '{} | {}'.format(*x)\n"", 'my_panel = pd.Panel(dict(df1=df1,df2=df2))\nprint my_panel.apply(report_diff, axis=0)\n\n#          id  Name        score    isEnrolled                       Date\n#0        111  Jack   nan | 2.17          True        2013-05-01 12:00:00\n#1        112  Nick  1.11 | 1.21         False  2013-05-12 15:05:23 | NaT\n#2  nan | nan   Zoe         4.12  True | False  NaT | 2013-05-01 12:00:00\n', 'from IPython.display import HTML\npd.options.display.max_colwidth = 500  # You need this, otherwise pandas\n#                          will limit your HTML strings to 50 characters\n\ndef report_diff(x):\n    if x[0]==x[1]:\n        return unicode(x[0].__str__())\n    elif pd.isnull(x[0]) and pd.isnull(x[1]):\n        return u\'<table style=""background-color:#00ff00;font-weight:bold;"">\'+\\\n            \'<tr><td>%s</td></tr><tr><td>%s</td></tr></table>\' % (\'nan\', \'nan\')\n    elif pd.isnull(x[0]) and ~pd.isnull(x[1]):\n        return u\'<table style=""background-color:#ffff00;font-weight:bold;"">\'+\\\n            \'<tr><td>%s</td></tr><tr><td>%s</td></tr></table>\' % (\'nan\', x[1])\n    elif ~pd.isnull(x[0]) and pd.isnull(x[1]):\n        return u\'<table style=""background-color:#0000ff;font-weight:bold;"">\'+\\\n            \'<tr><td>%s</td></tr><tr><td>%s</td></tr></table>\' % (x[0],\'nan\')\n    else:\n        return u\'<table style=""background-color:#ff0000;font-weight:bold;"">\'+\\\n            \'<tr><td>%s</td></tr><tr><td>%s</td></tr></table>\' % (x[0], x[1])\n\nHTML(my_panel.apply(report_diff, axis=0).to_html(escape=False))\n']";"[""import pandas as pd\nimport io\n\ntexts = ['''\\\nid   Name   score                    isEnrolled                       Date\n111  Jack                            True              2013-05-01 12:00:00\n112  Nick   1.11                     False             2013-05-12 15:05:23\n     Zoe    4.12                     True                                  ''',\n\n         '''\\\nid   Name   score                    isEnrolled                       Date\n111  Jack   2.17                     True              2013-05-01 12:00:00\n112  Nick   1.21                     False                                \n     Zoe    4.12                     False             2013-05-01 12:00:00''']\n\n\ndf1 = pd.read_fwf(io.BytesIO(texts[0]), widths=[5,7,25,17,20], parse_dates=[4])\ndf2 = pd.read_fwf(io.BytesIO(texts[1]), widths=[5,7,25,17,20], parse_dates=[4])\n"", ""def report_diff(x):\n    return x[0] if x[0] == x[1] else '{} | {}'.format(*x)\n"", 'my_panel = pd.Panel(dict(df1=df1,df2=df2))\nprint my_panel.apply(report_diff, axis=0)\n\n#          id  Name        score    isEnrolled                       Date\n#0        111  Jack   nan | 2.17          True        2013-05-01 12:00:00\n#1        112  Nick  1.11 | 1.21         False  2013-05-12 15:05:23 | NaT\n#2  nan | nan   Zoe         4.12  True | False  NaT | 2013-05-01 12:00:00\n', 'from IPython.display import HTML\npd.options.display.max_colwidth = 500  # You need this, otherwise pandas\n#                          will limit your HTML strings to 50 characters\n\ndef report_diff(x):\n    if x[0]==x[1]:\n        return unicode(x[0].__str__())\n    elif pd.isnull(x[0]) and pd.isnull(x[1]):\n        return u\'<table style=""background-color:#00ff00;font-weight:bold;"">\'+\\\n            \'<tr><td>%s</td></tr><tr><td>%s</td></tr></table>\' % (\'nan\', \'nan\')\n    elif pd.isnull(x[0]) and ~pd.isnull(x[1]):\n        return u\'<table style=""background-color:#ffff00;font-weight:bold;"">\'+\\\n            \'<tr><td>%s</td></tr><tr><td>%s</td></tr></table>\' % (\'nan\', x[1])\n    elif ~pd.isnull(x[0]) and pd.isnull(x[1]):\n        return u\'<table style=""background-color:#0000ff;font-weight:bold;"">\'+\\\n            \'<tr><td>%s</td></tr><tr><td>%s</td></tr></table>\' % (x[0],\'nan\')\n    else:\n        return u\'<table style=""background-color:#ff0000;font-weight:bold;"">\'+\\\n            \'<tr><td>%s</td></tr><tr><td>%s</td></tr></table>\' % (x[0], x[1])\n\nHTML(my_panel.apply(report_diff, axis=0).to_html(escape=False))\n']"
1083;;6;23143081;23142967.0;2;48;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/version/0.7.0/generated/pandas.Series.shift.html"">shift</a>.</p>

<pre><code>df['dA'] = df['A'] - df['A'].shift(-1)
</code></pre>
";;"[""df['dA'] = df['A'] - df['A'].shift(-1)\n""]";"[""df['dA'] = df['A'] - df['A'].shift(-1)\n""]"
1084;;1;23143110;23142967.0;2;17;;;"<p>You could use <code>diff</code> and pass <code>-1</code> as the <code>periods</code> argument:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({""A"": [9, 4, 2, 1], ""B"": [12, 7, 5, 4]})
&gt;&gt;&gt; df[""dA""] = df[""A""].diff(-1)
&gt;&gt;&gt; df
   A   B  dA
0  9  12   5
1  4   7   2
2  2   5   1
3  1   4 NaN

[4 rows x 3 columns]
</code></pre>
";;"['>>> df = pd.DataFrame({""A"": [9, 4, 2, 1], ""B"": [12, 7, 5, 4]})\n>>> df[""dA""] = df[""A""].diff(-1)\n>>> df\n   A   B  dA\n0  9  12   5\n1  4   7   2\n2  2   5   1\n3  1   4 NaN\n\n[4 rows x 3 columns]\n']";"['diff', '-1', 'periods', '>>> df = pd.DataFrame({""A"": [9, 4, 2, 1], ""B"": [12, 7, 5, 4]})\n>>> df[""dA""] = df[""A""].diff(-1)\n>>> df\n   A   B  dA\n0  9  12   5\n1  4   7   2\n2  2   5   1\n3  1   4 NaN\n\n[4 rows x 3 columns]\n']"
1085;;5;23200666;23199796.0;2;60;;;"<p>Use <code>boolean</code> indexing as you would do in <code>numpy.array</code></p>

<pre><code>df=pd.DataFrame({'Data':np.random.normal(size=200)})  #example dataset of normally distributed data. 
df[np.abs(df.Data-df.Data.mean())&lt;=(3*df.Data.std())] #keep only the ones that are within +3 to -3 standard deviations in the column 'Data'.
df[~(np.abs(df.Data-df.Data.mean())&gt;(3*df.Data.std()))] #or if you prefer the other way around
</code></pre>

<p>For a series it is similar:</p>

<pre><code>S=pd.Series(np.random.normal(size=200))
S[~((S-S.mean()).abs()&gt;3*S.std())]
</code></pre>
";;"[""df=pd.DataFrame({'Data':np.random.normal(size=200)})  #example dataset of normally distributed data. \ndf[np.abs(df.Data-df.Data.mean())<=(3*df.Data.std())] #keep only the ones that are within +3 to -3 standard deviations in the column 'Data'.\ndf[~(np.abs(df.Data-df.Data.mean())>(3*df.Data.std()))] #or if you prefer the other way around\n"", 'S=pd.Series(np.random.normal(size=200))\nS[~((S-S.mean()).abs()>3*S.std())]\n']";"['boolean', 'numpy.array', ""df=pd.DataFrame({'Data':np.random.normal(size=200)})  #example dataset of normally distributed data. \ndf[np.abs(df.Data-df.Data.mean())<=(3*df.Data.std())] #keep only the ones that are within +3 to -3 standard deviations in the column 'Data'.\ndf[~(np.abs(df.Data-df.Data.mean())>(3*df.Data.std()))] #or if you prefer the other way around\n"", 'S=pd.Series(np.random.normal(size=200))\nS[~((S-S.mean()).abs()>3*S.std())]\n']"
1086;;2;23202269;23199796.0;2;51;;;"<p>If you have multiple columns in your dataframe and would like to remove all rows that have outliers in at least one column, the following expression would do that in one shot.</p>

<pre><code>df = pd.DataFrame(np.random.randn(100, 3))

from scipy import stats
df[(np.abs(stats.zscore(df)) &lt; 3).all(axis=1)]
</code></pre>
";;['df = pd.DataFrame(np.random.randn(100, 3))\n\nfrom scipy import stats\ndf[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n'];['df = pd.DataFrame(np.random.randn(100, 3))\n\nfrom scipy import stats\ndf[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n']
1087;;3;23235618;13413590.0;2;60;;;"<p>I know this has already been answered, but just for the sake of a purely pandas solution to this specific question as opposed to the general description from Aman (which was wonderful) and in case anyone else happens upon this:</p>

<pre><code>import pandas as pd
df = df[pd.notnull(df['EPS'])]
</code></pre>
";;"[""import pandas as pd\ndf = df[pd.notnull(df['EPS'])]\n""]";"[""import pandas as pd\ndf = df[pd.notnull(df['EPS'])]\n""]"
1088;;1;23282290;23282130.0;2;47;;;"<p>Most <a href=""http://scikit-learn.org/stable/"" rel=""nofollow noreferrer"">sklearn</a> objects work with <code>pandas</code> dataframes just fine, would something like this work for you?</p>

<pre><code>import pandas as pd
import numpy as np
from sklearn.decomposition import PCA

df = pd.DataFrame(data=np.random.normal(0, 1, (20, 10)))

pca = PCA(n_components=5)
pca.fit(df)
</code></pre>

<p>You can access the components themselves with</p>

<pre><code>pca.components_ 
</code></pre>
";;['import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\ndf = pd.DataFrame(data=np.random.normal(0, 1, (20, 10)))\n\npca = PCA(n_components=5)\npca.fit(df)\n', 'pca.components_ \n'];['pandas', 'import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\ndf = pd.DataFrame(data=np.random.normal(0, 1, (20, 10)))\n\npca = PCA(n_components=5)\npca.fit(df)\n', 'pca.components_ \n']
1089;;23;23296545;23296282.0;2;36;;;"<p>Here's the rules, subsequent override:</p>

<ul>
<li><p>All operations generate a copy</p></li>
<li><p>If <code>inplace=True</code> is provided, it will modify in-place; only some operations support this</p></li>
<li><p>An indexer that sets, e.g. <code>.loc/.ix/.iloc/.iat/.at</code> will set inplace.</p></li>
<li><p>An indexer that gets on a single-dtyped object is almost always a view (depending on the memory layout it may not be that's why this is not reliable). This is mainly for efficiency. (the example from above is for <code>.query</code>; this will <strong>always</strong> return a copy as its evaluated by <code>numexpr</code>)</p></li>
<li><p>An indexer that gets on a multiple-dtyped object is always a copy.</p></li>
</ul>

<p>Your example of <code>chained indexing</code></p>

<pre><code>df[df.C &lt;= df.B].ix[:,'B':'E']
</code></pre>

<p>is not guaranteed to work (and thus you shoulld <strong>never</strong> do this). </p>

<p>Instead do:</p>

<pre><code>df.ix[df.C &lt;= df.B, 'B':'E']
</code></pre>

<p>as this is <em>faster</em> and will always work</p>

<p>The chained indexing is 2 separate python operations and thus cannot be reliably intercepted by pandas (you will oftentimes get a <code>SettingWithCopyWarning</code>, but that is not 100% detectable either). The <a href=""http://pandas-docs.github.io/pandas-docs-travis/indexing.html#indexing-view-versus-copy"">dev docs</a>, which you pointed, offer a much more full explanation.</p>
";;"[""df[df.C <= df.B].ix[:,'B':'E']\n"", ""df.ix[df.C <= df.B, 'B':'E']\n""]";"['inplace=True', '.loc/.ix/.iloc/.iat/.at', '.query', 'numexpr', 'chained indexing', ""df[df.C <= df.B].ix[:,'B':'E']\n"", ""df.ix[df.C <= df.B, 'B':'E']\n"", 'SettingWithCopyWarning']"
1090;;2;23307361;23307301.0;2;72;;;"<p>If I understand right, you want something like this:</p>

<pre><code>w['female'] = w['female'].map({'female': 1, 'male': 0})
</code></pre>

<p>(Here I convert the values to numbers instead of strings containing numbers.  You can convert them to <code>""1""</code> and <code>""0""</code>, if you really want, but I'm not sure why you'd want that.)</p>

<p>The reason your code doesn't work is because using <code>['female']</code> on a column (the second <code>'female'</code> in your <code>w['female']['female']</code>) doesn't mean ""select rows where the value is 'female'"".  It means to select rows where the <em>index</em> is 'female', of which there may not be any in your DataFrame.</p>
";;"[""w['female'] = w['female'].map({'female': 1, 'male': 0})\n""]";"[""w['female'] = w['female'].map({'female': 1, 'male': 0})\n"", '""1""', '""0""', ""['female']"", ""'female'"", ""w['female']['female']""]"
1091;;0;23317595;23317342.0;2;33;;;"<p>I'd do something like the following:</p>

<pre><code>foo = lambda x: pd.Series([i for i in reversed(x.split(','))])
rev = df['City, State, Country'].apply(foo)
print rev

      0    1        2
0   HUN  NaN      NaN
1   ESP  NaN      NaN
2   GBR  NaN      NaN
3   ESP  NaN      NaN
4   FRA  NaN      NaN
5   USA   ID      NaN
6   USA   GA      NaN
7   USA   NJ  Hoboken
8   USA   NJ      NaN
9   AUS  NaN      NaN
</code></pre>

<p>I think that gets you what you want but if you also want to pretty things up and get a City, State, Country column order, you could add the following:</p>

<pre><code>rev.rename(columns={0:'Country',1:'State',2:'City'},inplace=True)
rev = rev[['City','State','Country']]
print rev

     City State Country
0      NaN   NaN     HUN
1      NaN   NaN     ESP
2      NaN   NaN     GBR
3      NaN   NaN     ESP
4      NaN   NaN     FRA
5      NaN    ID     USA
6      NaN    GA     USA
7  Hoboken    NJ     USA
8      NaN    NJ     USA
9      NaN   NaN     AUS
</code></pre>
";;"[""foo = lambda x: pd.Series([i for i in reversed(x.split(','))])\nrev = df['City, State, Country'].apply(foo)\nprint rev\n\n      0    1        2\n0   HUN  NaN      NaN\n1   ESP  NaN      NaN\n2   GBR  NaN      NaN\n3   ESP  NaN      NaN\n4   FRA  NaN      NaN\n5   USA   ID      NaN\n6   USA   GA      NaN\n7   USA   NJ  Hoboken\n8   USA   NJ      NaN\n9   AUS  NaN      NaN\n"", ""rev.rename(columns={0:'Country',1:'State',2:'City'},inplace=True)\nrev = rev[['City','State','Country']]\nprint rev\n\n     City State Country\n0      NaN   NaN     HUN\n1      NaN   NaN     ESP\n2      NaN   NaN     GBR\n3      NaN   NaN     ESP\n4      NaN   NaN     FRA\n5      NaN    ID     USA\n6      NaN    GA     USA\n7  Hoboken    NJ     USA\n8      NaN    NJ     USA\n9      NaN   NaN     AUS\n""]";"[""foo = lambda x: pd.Series([i for i in reversed(x.split(','))])\nrev = df['City, State, Country'].apply(foo)\nprint rev\n\n      0    1        2\n0   HUN  NaN      NaN\n1   ESP  NaN      NaN\n2   GBR  NaN      NaN\n3   ESP  NaN      NaN\n4   FRA  NaN      NaN\n5   USA   ID      NaN\n6   USA   GA      NaN\n7   USA   NJ  Hoboken\n8   USA   NJ      NaN\n9   AUS  NaN      NaN\n"", ""rev.rename(columns={0:'Country',1:'State',2:'City'},inplace=True)\nrev = rev[['City','State','Country']]\nprint rev\n\n     City State Country\n0      NaN   NaN     HUN\n1      NaN   NaN     ESP\n2      NaN   NaN     GBR\n3      NaN   NaN     ESP\n4      NaN   NaN     FRA\n5      NaN    ID     USA\n6      NaN    GA     USA\n7  Hoboken    NJ     USA\n8      NaN    NJ     USA\n9      NaN   NaN     AUS\n""]"
1092;;0;23331659;23330654.0;2;6;;;"<p>You should assign value by <code>df.ix[i, 'exp']=X</code> or <code>df.loc[i, 'exp']=X</code> instead of <code>df.ix[i]['ifor'] = x</code>. </p>

<p>Otherwise you are working on a view, and should get a warming:</p>

<p><code>-c:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_index,col_indexer] = value instead</code></p>

<p>But certainly, loop probably should better be replaced by some vectorized algorithm to make the full use of <code>DataFrame</code> as @Phillip Cloud suggested.</p>
";;[];"[""df.ix[i, 'exp']=X"", ""df.loc[i, 'exp']=X"", ""df.ix[i]['ifor'] = x"", '-c:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead', 'DataFrame']"
1093;;0;23331896;10065051.0;2;15;;;"<p>I prefer to create queries with <a href=""http://www.sqlalchemy.org/""><strong>SQLAlchemy</strong></a>, and then make a DataFrame from it. <strong>SQLAlchemy</strong> makes it easier to combine <strong>SQL</strong> conditions Pythonically if you intend to mix and match things over and over.</p>

<pre><code>from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import Table
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from pandas import DataFrame
import datetime

# We are connecting to an existing service
engine = create_engine('dialect://user:pwd@host:port/db', echo=False)
Session = sessionmaker(bind=engine)
session = Session()
Base = declarative_base()

# And we want to query an existing table
tablename = Table('tablename', 
    Base.metadata, 
    autoload=True, 
    autoload_with=engine, 
    schema='ownername')

# These are the ""Where"" parameters, but I could as easily 
# create joins and limit results
us = tablename.c.country_code.in_(['US','MX'])
dc = tablename.c.locn_name.like('%DC%')
dt = tablename.c.arr_date &gt;= datetime.date.today() # Give me convenience or...

q = session.query(tablename).\
            filter(us &amp; dc &amp; dt) # That's where the magic happens!!!

def querydb(query):
    """"""
    Function to execute query and return DataFrame.
    """"""
    df = DataFrame(query.all());
    df.columns = [x['name'] for x in query.column_descriptions]
    return df

querydb(q)
</code></pre>
";;"['from sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy import Table\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom pandas import DataFrame\nimport datetime\n\n# We are connecting to an existing service\nengine = create_engine(\'dialect://user:pwd@host:port/db\', echo=False)\nSession = sessionmaker(bind=engine)\nsession = Session()\nBase = declarative_base()\n\n# And we want to query an existing table\ntablename = Table(\'tablename\', \n    Base.metadata, \n    autoload=True, \n    autoload_with=engine, \n    schema=\'ownername\')\n\n# These are the ""Where"" parameters, but I could as easily \n# create joins and limit results\nus = tablename.c.country_code.in_([\'US\',\'MX\'])\ndc = tablename.c.locn_name.like(\'%DC%\')\ndt = tablename.c.arr_date >= datetime.date.today() # Give me convenience or...\n\nq = session.query(tablename).\\\n            filter(us & dc & dt) # That\'s where the magic happens!!!\n\ndef querydb(query):\n    """"""\n    Function to execute query and return DataFrame.\n    """"""\n    df = DataFrame(query.all());\n    df.columns = [x[\'name\'] for x in query.column_descriptions]\n    return df\n\nquerydb(q)\n']";"['from sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy import Table\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom pandas import DataFrame\nimport datetime\n\n# We are connecting to an existing service\nengine = create_engine(\'dialect://user:pwd@host:port/db\', echo=False)\nSession = sessionmaker(bind=engine)\nsession = Session()\nBase = declarative_base()\n\n# And we want to query an existing table\ntablename = Table(\'tablename\', \n    Base.metadata, \n    autoload=True, \n    autoload_with=engine, \n    schema=\'ownername\')\n\n# These are the ""Where"" parameters, but I could as easily \n# create joins and limit results\nus = tablename.c.country_code.in_([\'US\',\'MX\'])\ndc = tablename.c.locn_name.like(\'%DC%\')\ndt = tablename.c.arr_date >= datetime.date.today() # Give me convenience or...\n\nq = session.query(tablename).\\\n            filter(us & dc & dt) # That\'s where the magic happens!!!\n\ndef querydb(query):\n    """"""\n    Function to execute query and return DataFrame.\n    """"""\n    df = DataFrame(query.all());\n    df.columns = [x[\'name\'] for x in query.column_descriptions]\n    return df\n\nquerydb(q)\n']"
1094;;2;23377155;23377108.0;2;24;;;"<p>You need to make a second groupby object that groups by the states, and then use the <code>div</code> method:</p>

<pre><code>import numpy as np
import pandas as pd
np.random.seed(0)
df = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,
               'office_id': list(range(1, 7)) * 2,
               'sales': [np.random.randint(100000, 999999) for _ in range(12)]})

state_office = df.groupby(['state', 'office_id']).agg({'sales': 'sum'})
state = df.groupby(['state']).agg({'sales': 'sum'})
state_office.div(state, level='state') * 100


                     sales
state office_id           
AZ    2          16.981365
      4          19.250033
      6          63.768601
CA    1          19.331879
      3          33.858747
      5          46.809373
CO    1          36.851857
      3          19.874290
      5          43.273852
WA    2          34.707233
      4          35.511259
      6          29.781508
</code></pre>

<p>the <code>level='state'</code> kwarg in <code>div</code> tells pandas to broadcast/join the dataframes base on the values in the <code>state</code> level of the index.</p>
";;"[""import numpy as np\nimport pandas as pd\nnp.random.seed(0)\ndf = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,\n               'office_id': list(range(1, 7)) * 2,\n               'sales': [np.random.randint(100000, 999999) for _ in range(12)]})\n\nstate_office = df.groupby(['state', 'office_id']).agg({'sales': 'sum'})\nstate = df.groupby(['state']).agg({'sales': 'sum'})\nstate_office.div(state, level='state') * 100\n\n\n                     sales\nstate office_id           \nAZ    2          16.981365\n      4          19.250033\n      6          63.768601\nCA    1          19.331879\n      3          33.858747\n      5          46.809373\nCO    1          36.851857\n      3          19.874290\n      5          43.273852\nWA    2          34.707233\n      4          35.511259\n      6          29.781508\n""]";"['div', ""import numpy as np\nimport pandas as pd\nnp.random.seed(0)\ndf = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,\n               'office_id': list(range(1, 7)) * 2,\n               'sales': [np.random.randint(100000, 999999) for _ in range(12)]})\n\nstate_office = df.groupby(['state', 'office_id']).agg({'sales': 'sum'})\nstate = df.groupby(['state']).agg({'sales': 'sum'})\nstate_office.div(state, level='state') * 100\n\n\n                     sales\nstate office_id           \nAZ    2          16.981365\n      4          19.250033\n      6          63.768601\nCA    1          19.331879\n      3          33.858747\n      5          46.809373\nCO    1          36.851857\n      3          19.874290\n      5          43.273852\nWA    2          34.707233\n      4          35.511259\n      6          29.781508\n"", ""level='state'"", 'div', 'state']"
1095;;9;23377232;23377108.0;2;52;;;"<p><a href=""https://stackoverflow.com/a/23377155/3393459"">Paul H's answer</a> is right that you will have to make a second <code>groupby</code> object, but you can calculate the percentage in a simpler way -- just <code>groupby</code> the <code>state_office</code> and divide the <code>sales</code> column by its sum. Copying the beginning of Paul H's answer:</p>

<pre><code># From Paul H
import numpy as np
import pandas as pd
np.random.seed(0)
df = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,
                   'office_id': list(range(1, 7)) * 2,
                   'sales': [np.random.randint(100000, 999999)
                             for _ in range(12)]})
state_office = df.groupby(['state', 'office_id']).agg({'sales': 'sum'})
# Change: groupby state_office and divide by sum
state_pcts = state_office.groupby(level=0).apply(lambda x:
                                                 100 * x / float(x.sum()))
</code></pre>

<p>Returns:</p>

<pre><code>                     sales
state office_id           
AZ    2          16.981365
      4          19.250033
      6          63.768601
CA    1          19.331879
      3          33.858747
      5          46.809373
CO    1          36.851857
      3          19.874290
      5          43.273852
WA    2          34.707233
      4          35.511259
      6          29.781508
</code></pre>
";;"[""# From Paul H\nimport numpy as np\nimport pandas as pd\nnp.random.seed(0)\ndf = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,\n                   'office_id': list(range(1, 7)) * 2,\n                   'sales': [np.random.randint(100000, 999999)\n                             for _ in range(12)]})\nstate_office = df.groupby(['state', 'office_id']).agg({'sales': 'sum'})\n# Change: groupby state_office and divide by sum\nstate_pcts = state_office.groupby(level=0).apply(lambda x:\n                                                 100 * x / float(x.sum()))\n"", '                     sales\nstate office_id           \nAZ    2          16.981365\n      4          19.250033\n      6          63.768601\nCA    1          19.331879\n      3          33.858747\n      5          46.809373\nCO    1          36.851857\n      3          19.874290\n      5          43.273852\nWA    2          34.707233\n      4          35.511259\n      6          29.781508\n']";"['groupby', 'groupby', 'state_office', 'sales', ""# From Paul H\nimport numpy as np\nimport pandas as pd\nnp.random.seed(0)\ndf = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,\n                   'office_id': list(range(1, 7)) * 2,\n                   'sales': [np.random.randint(100000, 999999)\n                             for _ in range(12)]})\nstate_office = df.groupby(['state', 'office_id']).agg({'sales': 'sum'})\n# Change: groupby state_office and divide by sum\nstate_pcts = state_office.groupby(level=0).apply(lambda x:\n                                                 100 * x / float(x.sum()))\n"", '                     sales\nstate office_id           \nAZ    2          16.981365\n      4          19.250033\n      6          63.768601\nCA    1          19.331879\n      3          33.858747\n      5          46.809373\nCO    1          36.851857\n      3          19.874290\n      5          43.273852\nWA    2          34.707233\n      4          35.511259\n      6          29.781508\n']"
1096;;2;23394497;10715965.0;2;37;;;"<p>For efficient appending see <a href=""https://stackoverflow.com/questions/19365513/how-to-add-an-extra-row-to-a-pandas-dataframe/19368360#19368360"">How to add an extra row to a pandas dataframe</a> and <a href=""http://pandas-docs.github.io/pandas-docs-travis/indexing.html#setting-with-enlargement"" rel=""nofollow noreferrer""><em>Setting With Enlargement</em></a>.</p>

<p>Add rows through <code>loc/ix</code> on <strong>non existing</strong> key index data. e.g. :</p>

<pre><code>In [1]: se = pd.Series([1,2,3])

In [2]: se
Out[2]: 
0    1
1    2
2    3
dtype: int64

In [3]: se[5] = 5.

In [4]: se
Out[4]: 
0    1.0
1    2.0
2    3.0
5    5.0
dtype: float64
</code></pre>

<p>Or:</p>

<pre><code>In [1]: dfi = pd.DataFrame(np.arange(6).reshape(3,2),
   .....:                 columns=['A','B'])
   .....: 

In [2]: dfi
Out[2]: 
   A  B
0  0  1
1  2  3
2  4  5

In [3]: dfi.loc[:,'C'] = dfi.loc[:,'A']

In [4]: dfi
Out[4]: 
   A  B  C
0  0  1  0
1  2  3  2
2  4  5  4
In [5]: dfi.loc[3] = 5

In [6]: dfi
Out[6]: 
   A  B  C
0  0  1  0
1  2  3  2
2  4  5  4
3  5  5  5
</code></pre>
";;"['In [1]: se = pd.Series([1,2,3])\n\nIn [2]: se\nOut[2]: \n0    1\n1    2\n2    3\ndtype: int64\n\nIn [3]: se[5] = 5.\n\nIn [4]: se\nOut[4]: \n0    1.0\n1    2.0\n2    3.0\n5    5.0\ndtype: float64\n', ""In [1]: dfi = pd.DataFrame(np.arange(6).reshape(3,2),\n   .....:                 columns=['A','B'])\n   .....: \n\nIn [2]: dfi\nOut[2]: \n   A  B\n0  0  1\n1  2  3\n2  4  5\n\nIn [3]: dfi.loc[:,'C'] = dfi.loc[:,'A']\n\nIn [4]: dfi\nOut[4]: \n   A  B  C\n0  0  1  0\n1  2  3  2\n2  4  5  4\nIn [5]: dfi.loc[3] = 5\n\nIn [6]: dfi\nOut[6]: \n   A  B  C\n0  0  1  0\n1  2  3  2\n2  4  5  4\n3  5  5  5\n""]";"['loc/ix', 'In [1]: se = pd.Series([1,2,3])\n\nIn [2]: se\nOut[2]: \n0    1\n1    2\n2    3\ndtype: int64\n\nIn [3]: se[5] = 5.\n\nIn [4]: se\nOut[4]: \n0    1.0\n1    2.0\n2    3.0\n5    5.0\ndtype: float64\n', ""In [1]: dfi = pd.DataFrame(np.arange(6).reshape(3,2),\n   .....:                 columns=['A','B'])\n   .....: \n\nIn [2]: dfi\nOut[2]: \n   A  B\n0  0  1\n1  2  3\n2  4  5\n\nIn [3]: dfi.loc[:,'C'] = dfi.loc[:,'A']\n\nIn [4]: dfi\nOut[4]: \n   A  B  C\n0  0  1  0\n1  2  3  2\n2  4  5  4\nIn [5]: dfi.loc[3] = 5\n\nIn [6]: dfi\nOut[6]: \n   A  B  C\n0  0  1  0\n1  2  3  2\n2  4  5  4\n3  5  5  5\n""]"
1097;;2;23464103;15411158.0;2;21;;;"<p>Interestingly enough, very often <code>len(unique())</code> is a few times (3x-15x) faster than <code>nunique()</code>.</p>
";;[];['len(unique())', 'nunique()']
1098;;3;23478395;19851005.0;2;11;;;"<p>In Pandas version 0.13 and greater the index level names are immutable (type <code>FrozenList</code>) and can no longer be set directly.  You must first use <code>Index.rename()</code> to apply the new index level names to the Index and then use <code>DataFrame.reindex()</code> to apply the new index to the DataFrame.  Examples:</p>

<p><strong>For Pandas version &lt; 0.13</strong></p>

<pre><code>df.index.names = ['Date']
</code></pre>

<p><strong>For Pandas version >= 0.13</strong></p>

<pre><code>df = df.reindex(df.index.rename(['Date']))
</code></pre>
";;"[""df.index.names = ['Date']\n"", ""df = df.reindex(df.index.rename(['Date']))\n""]";"['FrozenList', 'Index.rename()', 'DataFrame.reindex()', ""df.index.names = ['Date']\n"", ""df = df.reindex(df.index.rename(['Date']))\n""]"
1099;;2;23549599;23549231.0;2;76;;;"<p>This should do the trick</p>

<pre><code>'g' in df.index
</code></pre>
";;"[""'g' in df.index\n""]";"[""'g' in df.index\n""]"
1100;;1;23671390;23668427.0;2;42;;;"<p>You could try this if you have 3 dataframes</p>

<pre><code># Merge multiple dataframes
df1 = pd.DataFrame(np.array([
    ['a', 5, 9],
    ['b', 4, 61],
    ['c', 24, 9]]),
    columns=['name', 'attr11', 'attr12'])
df2 = pd.DataFrame(np.array([
    ['a', 5, 19],
    ['b', 14, 16],
    ['c', 4, 9]]),
    columns=['name', 'attr21', 'attr22'])
df3 = pd.DataFrame(np.array([
    ['a', 15, 49],
    ['b', 4, 36],
    ['c', 14, 9]]),
    columns=['name', 'attr31', 'attr32'])

pd.merge(pd.merge(df1,df2,on='name'),df3,on='name')
</code></pre>

<p>alternatively, as mentioned by cwharland </p>

<pre><code>df1.merge(df2,on='name').merge(df3,on='name')
</code></pre>
";;"[""# Merge multiple dataframes\ndf1 = pd.DataFrame(np.array([\n    ['a', 5, 9],\n    ['b', 4, 61],\n    ['c', 24, 9]]),\n    columns=['name', 'attr11', 'attr12'])\ndf2 = pd.DataFrame(np.array([\n    ['a', 5, 19],\n    ['b', 14, 16],\n    ['c', 4, 9]]),\n    columns=['name', 'attr21', 'attr22'])\ndf3 = pd.DataFrame(np.array([\n    ['a', 15, 49],\n    ['b', 4, 36],\n    ['c', 14, 9]]),\n    columns=['name', 'attr31', 'attr32'])\n\npd.merge(pd.merge(df1,df2,on='name'),df3,on='name')\n"", ""df1.merge(df2,on='name').merge(df3,on='name')\n""]";"[""# Merge multiple dataframes\ndf1 = pd.DataFrame(np.array([\n    ['a', 5, 9],\n    ['b', 4, 61],\n    ['c', 24, 9]]),\n    columns=['name', 'attr11', 'attr12'])\ndf2 = pd.DataFrame(np.array([\n    ['a', 5, 19],\n    ['b', 14, 16],\n    ['c', 4, 9]]),\n    columns=['name', 'attr21', 'attr22'])\ndf3 = pd.DataFrame(np.array([\n    ['a', 15, 49],\n    ['b', 4, 36],\n    ['c', 14, 9]]),\n    columns=['name', 'attr31', 'attr32'])\n\npd.merge(pd.merge(df1,df2,on='name'),df3,on='name')\n"", ""df1.merge(df2,on='name').merge(df3,on='name')\n""]"
1101;;0;23691692;16947336.0;2;13;;;"<p>Joe Kington's answer was very helpful, however, I noticed that it does not bin all of the data. It actually leaves out the row with a = a.min(). Summing up <code>groups.size()</code> gave 99 instead of 100.</p>

<p>To guarantee that all data is binned, just pass in the number of bins to cut() and that function will automatically pad the first[last] bin by 0.1% to ensure all data is included.</p>

<pre><code>df = pandas.DataFrame({""a"": np.random.random(100), 
                    ""b"": np.random.random(100) + 10})

# Bin the data frame by ""a"" with 10 bins...
groups = df.groupby(pandas.cut(df.a, 10))

# Get the mean of b, binned by the values in a
print(groups.mean().b)
</code></pre>

<p>In this case, summing up groups.size() gave 100.</p>

<p>I know this is a picky point for this particular problem, but for a similar problem I was trying to solve, it was crucial to obtain the correct answer.</p>
";;"['df = pandas.DataFrame({""a"": np.random.random(100), \n                    ""b"": np.random.random(100) + 10})\n\n# Bin the data frame by ""a"" with 10 bins...\ngroups = df.groupby(pandas.cut(df.a, 10))\n\n# Get the mean of b, binned by the values in a\nprint(groups.mean().b)\n']";"['groups.size()', 'df = pandas.DataFrame({""a"": np.random.random(100), \n                    ""b"": np.random.random(100) + 10})\n\n# Bin the data frame by ""a"" with 10 bins...\ngroups = df.groupby(pandas.cut(df.a, 10))\n\n# Get the mean of b, binned by the values in a\nprint(groups.mean().b)\n']"
1102;;0;23739252;20637439.0;2;11;;;"<p>I don't have reputation to comment yet, but I want to add to <a href=""https://stackoverflow.com/a/20637559/1896459"">alko</a> answer for further reference.</p>

<p>From the <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#io-read-csv-table"" rel=""nofollow noreferrer"">docs</a>:</p>

<blockquote>
  <p>skiprows: A collection of numbers for rows in the file to skip. Can also be an integer to skip the first n rows</p>
</blockquote>
";;[];[]
1103;;2;23741480;13148429.0;2;101;;;"<p>You could also do something like this:</p>

<pre><code>df = df[['mean', '0', '1', '2', '3']]
</code></pre>

<p>You can get the list of columns with:</p>

<pre><code>cols = list(df.columns.values)
</code></pre>

<p>The output will produce:</p>

<pre><code>['0', '1', '2', '3', 'mean']
</code></pre>

<p>...which is then easy to rearrange manually before dropping it into the first function</p>
";;"[""df = df[['mean', '0', '1', '2', '3']]\n"", 'cols = list(df.columns.values)\n', ""['0', '1', '2', '3', 'mean']\n""]";"[""df = df[['mean', '0', '1', '2', '3']]\n"", 'cols = list(df.columns.values)\n', ""['0', '1', '2', '3', 'mean']\n""]"
1104;;6;23749057;23748995.0;2;88;;;"<p>Use <code>.values</code> to get a <code>numpy.array</code> and then <code>.tolist()</code> to get a list.</p>

<p>For example:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'a':[1,3,5,7,4,5,6,4,7,8,9],
                   'b':[3,5,6,2,4,6,7,8,7,8,9]})
</code></pre>

<p>Result:</p>

<pre><code>&gt;&gt;&gt; df['a'].values.tolist()
[1, 3, 5, 7, 4, 5, 6, 4, 7, 8, 9]
</code></pre>

<p>or you can just use</p>

<pre><code>&gt;&gt;&gt; df['a'].tolist()
[1, 3, 5, 7, 4, 5, 6, 4, 7, 8, 9]
</code></pre>

<p>To drop duplicates you can do one of the following:</p>

<pre><code>&gt;&gt;&gt; df['a'].drop_duplicates().values.tolist()
[1, 3, 5, 7, 4, 6, 8, 9]
&gt;&gt;&gt; list(set(df['a'])) # as pointed out by EdChum
[1, 3, 4, 5, 6, 7, 8, 9]
</code></pre>
";;"[""import pandas as pd\ndf = pd.DataFrame({'a':[1,3,5,7,4,5,6,4,7,8,9],\n                   'b':[3,5,6,2,4,6,7,8,7,8,9]})\n"", "">>> df['a'].values.tolist()\n[1, 3, 5, 7, 4, 5, 6, 4, 7, 8, 9]\n"", "">>> df['a'].tolist()\n[1, 3, 5, 7, 4, 5, 6, 4, 7, 8, 9]\n"", "">>> df['a'].drop_duplicates().values.tolist()\n[1, 3, 5, 7, 4, 6, 8, 9]\n>>> list(set(df['a'])) # as pointed out by EdChum\n[1, 3, 4, 5, 6, 7, 8, 9]\n""]";"['.values', 'numpy.array', '.tolist()', ""import pandas as pd\ndf = pd.DataFrame({'a':[1,3,5,7,4,5,6,4,7,8,9],\n                   'b':[3,5,6,2,4,6,7,8,7,8,9]})\n"", "">>> df['a'].values.tolist()\n[1, 3, 5, 7, 4, 5, 6, 4, 7, 8, 9]\n"", "">>> df['a'].tolist()\n[1, 3, 5, 7, 4, 5, 6, 4, 7, 8, 9]\n"", "">>> df['a'].drop_duplicates().values.tolist()\n[1, 3, 5, 7, 4, 6, 8, 9]\n>>> list(set(df['a'])) # as pointed out by EdChum\n[1, 3, 4, 5, 6, 7, 8, 9]\n""]"
1105;;4;23853569;23853553.0;2;34;;;"<p>If you only want to read the first 999,999 (non-header) rows:</p>

<pre><code>read_csv(..., nrows=999999)
</code></pre>

<p>If you only want to read rows 1,000,000 ... 1,999,999</p>

<pre><code>read_csv(..., skiprows=1000000, nrows=999999)
</code></pre>

<p><strong><em>nrows</em></strong> : int, default None Number of rows of file to read. Useful for
reading pieces of large files*</p>

<p><strong><em>skiprows</em></strong> : list-like or integer
Row numbers to skip (0-indexed) or number of rows to skip (int) at the start of the file</p>

<p>and for large files, you'll probably also want to use chunksize:</p>

<p><strong><em>chunksize</em></strong> : int, default None
Return TextFileReader object for iteration</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.io.parsers.read_csv.html"">pandas.io.parsers.read_csv documentation</a></p>
";;['read_csv(..., nrows=999999)\n', 'read_csv(..., skiprows=1000000, nrows=999999)\n'];['read_csv(..., nrows=999999)\n', 'read_csv(..., skiprows=1000000, nrows=999999)\n']
1106;;5;23901625;10511024.0;2;121;;;"<p>Note that --pylab is deprecated and has been removed from newer builds of IPython, so the accepted answer will no longer work. The recommended way to enable inline plotting in the IPython Notebook is now to run:</p>

<pre><code>%matplotlib inline
import matplotlib.pyplot as plt
</code></pre>

<p>See <a href=""http://mail.scipy.org/pipermail/ipython-dev/2014-March/013411.html"">this post</a> from the ipython-dev mailing list for more details.</p>
";;['%matplotlib inline\nimport matplotlib.pyplot as plt\n'];['%matplotlib inline\nimport matplotlib.pyplot as plt\n']
1107;;1;23922119;20937538.0;2;39;;;"<p>If you don't want to modify the dataframe, you could use a custom formatter for that column.</p>

<pre><code>import pandas as pd
pd.options.display.float_format = '${:,.2f}'.format
df = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],
                  index=['foo','bar','baz','quux'],
                  columns=['cost'])


print df.to_string(formatters={'cost':'${:,.2f}'.format})
</code></pre>

<p>yields</p>

<pre><code>        cost
foo  $123.46
bar  $234.57
baz  $345.68
quux $456.79
</code></pre>
";;"[""import pandas as pd\npd.options.display.float_format = '${:,.2f}'.format\ndf = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\n\n\nprint df.to_string(formatters={'cost':'${:,.2f}'.format})\n"", '        cost\nfoo  $123.46\nbar  $234.57\nbaz  $345.68\nquux $456.79\n']";"[""import pandas as pd\npd.options.display.float_format = '${:,.2f}'.format\ndf = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\n\n\nprint df.to_string(formatters={'cost':'${:,.2f}'.format})\n"", '        cost\nfoo  $123.46\nbar  $234.57\nbaz  $345.68\nquux $456.79\n']"
1108;;3;24040239;24039023.0;2;10;;;"<p>The reason this puts <code>NaN</code> into a column is because <code>df.index</code> and the <code>Index</code> of your right-hand-side object are different. @zach shows the proper way to assign a new column of zeros. In general, <code>pandas</code> tries to do as much alignment of indices as possible. One downside is that when indices are not aligned you get <code>NaN</code> wherever they <em>aren't</em> aligned. Play around with the <code>reindex</code> and <code>align</code> methods to gain some intuition for alignment works with objects that have partially, totally, and not-aligned-all aligned indices. For example here's how <code>DataFrame.align()</code> works with partially aligned indices:</p>

<pre><code>In [7]: from pandas import DataFrame

In [8]: from numpy.random import randint

In [9]: df = DataFrame({'a': randint(3, size=10)})

In [10]:

In [10]: df
Out[10]:
   a
0  0
1  2
2  0
3  1
4  0
5  0
6  0
7  0
8  0
9  0

In [11]: s = df.a[:5]

In [12]: dfa, sa = df.align(s, axis=0)

In [13]: dfa
Out[13]:
   a
0  0
1  2
2  0
3  1
4  0
5  0
6  0
7  0
8  0
9  0

In [14]: sa
Out[14]:
0     0
1     2
2     0
3     1
4     0
5   NaN
6   NaN
7   NaN
8   NaN
9   NaN
Name: a, dtype: float64
</code></pre>
";;"[""In [7]: from pandas import DataFrame\n\nIn [8]: from numpy.random import randint\n\nIn [9]: df = DataFrame({'a': randint(3, size=10)})\n\nIn [10]:\n\nIn [10]: df\nOut[10]:\n   a\n0  0\n1  2\n2  0\n3  1\n4  0\n5  0\n6  0\n7  0\n8  0\n9  0\n\nIn [11]: s = df.a[:5]\n\nIn [12]: dfa, sa = df.align(s, axis=0)\n\nIn [13]: dfa\nOut[13]:\n   a\n0  0\n1  2\n2  0\n3  1\n4  0\n5  0\n6  0\n7  0\n8  0\n9  0\n\nIn [14]: sa\nOut[14]:\n0     0\n1     2\n2     0\n3     1\n4     0\n5   NaN\n6   NaN\n7   NaN\n8   NaN\n9   NaN\nName: a, dtype: float64\n""]";"['NaN', 'df.index', 'Index', 'pandas', 'NaN', 'reindex', 'align', 'DataFrame.align()', ""In [7]: from pandas import DataFrame\n\nIn [8]: from numpy.random import randint\n\nIn [9]: df = DataFrame({'a': randint(3, size=10)})\n\nIn [10]:\n\nIn [10]: df\nOut[10]:\n   a\n0  0\n1  2\n2  0\n3  1\n4  0\n5  0\n6  0\n7  0\n8  0\n9  0\n\nIn [11]: s = df.a[:5]\n\nIn [12]: dfa, sa = df.align(s, axis=0)\n\nIn [13]: dfa\nOut[13]:\n   a\n0  0\n1  2\n2  0\n3  1\n4  0\n5  0\n6  0\n7  0\n8  0\n9  0\n\nIn [14]: sa\nOut[14]:\n0     0\n1     2\n2     0\n3     1\n4     0\n5   NaN\n6   NaN\n7   NaN\n8   NaN\n9   NaN\nName: a, dtype: float64\n""]"
1109;;0;24041761;24041436.0;2;31;;;"<p>When you pass inplace in makes the changes on the original variable and returns None, and the function <strong>does not</strong> return the modified dataframe, it returns None.</p>

<pre><code>is_none = df.set_index(['Company', 'date'], inplace=True)
df  # the dataframe you want
is_none # has the value None
</code></pre>

<p>so when you have a line like:</p>

<pre><code>df = df.set_index(['Company', 'date'], inplace=True)
</code></pre>

<p>it first modifies <code>df</code>... but then it sets <code>df</code> to None!</p>

<p>That is, you should just use the line:</p>

<pre><code>df.set_index(['Company', 'date'], inplace=True)
</code></pre>
";;"[""is_none = df.set_index(['Company', 'date'], inplace=True)\ndf  # the dataframe you want\nis_none # has the value None\n"", ""df = df.set_index(['Company', 'date'], inplace=True)\n"", ""df.set_index(['Company', 'date'], inplace=True)\n""]";"[""is_none = df.set_index(['Company', 'date'], inplace=True)\ndf  # the dataframe you want\nis_none # has the value None\n"", ""df = df.set_index(['Company', 'date'], inplace=True)\n"", 'df', 'df', ""df.set_index(['Company', 'date'], inplace=True)\n""]"
1110;;0;24074316;19913659.0;2;11;;;"<p>Another way in which this could be achieved is </p>

<pre><code>df['color'] = df.Set.map( lambda x: 'red' if x == 'Z' else 'green')
</code></pre>
";;"[""df['color'] = df.Set.map( lambda x: 'red' if x == 'Z' else 'green')\n""]";"[""df['color'] = df.Set.map( lambda x: 'red' if x == 'Z' else 'green')\n""]"
1111;;2;24083253;24082784.0;2;53;;;"<p>Managed to do it:</p>

<pre><code>pd.groupby(b,by=[b.index.month,b.index.year])
</code></pre>

<p>Or </p>

<pre><code>df.groupby(pd.TimeGrouper(freq='M'))
</code></pre>
";;"['pd.groupby(b,by=[b.index.month,b.index.year])\n', ""df.groupby(pd.TimeGrouper(freq='M'))\n""]";"['pd.groupby(b,by=[b.index.month,b.index.year])\n', ""df.groupby(pd.TimeGrouper(freq='M'))\n""]"
1112;;1;24112443;19530568.0;2;19;;;"<p>I am answering the question as stated in its title and first sentence: the following aggregates values to lists.</p>

<pre><code>import pandas as pd

df = pd.DataFrame( {'A' : [1, 1, 1, 1, 2, 2, 3], 'B' : [10, 12, 11, 10, 11, 12, 14], 'C' : [22, 20,     8, 10, 13, 10, 0]})
print df

df2=df.groupby(['A']).apply(lambda tdf: pd.Series(  dict([[vv,tdf[vv].unique().tolist()] for vv in tdf if vv not in ['A']])  )) 
print df2
</code></pre>

<p>The output is as follows:</p>

<pre><code>In [3]: run tmp
   A   B   C
0  1  10  22
1  1  12  20
2  1  11   8
3  1  10  10
4  2  11  13
5  2  12  10
6  3  14   0

[7 rows x 3 columns]
              B                C
A                               
1  [10, 12, 11]  [22, 20, 8, 10]
2      [11, 12]         [13, 10]
3          [14]              [0]

[3 rows x 2 columns]
</code></pre>
";;"[""import pandas as pd\n\ndf = pd.DataFrame( {'A' : [1, 1, 1, 1, 2, 2, 3], 'B' : [10, 12, 11, 10, 11, 12, 14], 'C' : [22, 20,     8, 10, 13, 10, 0]})\nprint df\n\ndf2=df.groupby(['A']).apply(lambda tdf: pd.Series(  dict([[vv,tdf[vv].unique().tolist()] for vv in tdf if vv not in ['A']])  )) \nprint df2\n"", 'In [3]: run tmp\n   A   B   C\n0  1  10  22\n1  1  12  20\n2  1  11   8\n3  1  10  10\n4  2  11  13\n5  2  12  10\n6  3  14   0\n\n[7 rows x 3 columns]\n              B                C\nA                               \n1  [10, 12, 11]  [22, 20, 8, 10]\n2      [11, 12]         [13, 10]\n3          [14]              [0]\n\n[3 rows x 2 columns]\n']";"[""import pandas as pd\n\ndf = pd.DataFrame( {'A' : [1, 1, 1, 1, 2, 2, 3], 'B' : [10, 12, 11, 10, 11, 12, 14], 'C' : [22, 20,     8, 10, 13, 10, 0]})\nprint df\n\ndf2=df.groupby(['A']).apply(lambda tdf: pd.Series(  dict([[vv,tdf[vv].unique().tolist()] for vv in tdf if vv not in ['A']])  )) \nprint df2\n"", 'In [3]: run tmp\n   A   B   C\n0  1  10  22\n1  1  12  20\n2  1  11   8\n3  1  10  10\n4  2  11  13\n5  2  12  10\n6  3  14   0\n\n[7 rows x 3 columns]\n              B                C\nA                               \n1  [10, 12, 11]  [22, 20, 8, 10]\n2      [11, 12]         [13, 10]\n3          [14]              [0]\n\n[3 rows x 2 columns]\n']"
1113;;11;24147363;24147278.0;2;111;;;"<p>I would just use numpy's <code>randn</code>:</p>

<pre><code>In [11]: df = pd.DataFrame(np.random.randn(100, 2))

In [12]: msk = np.random.rand(len(df)) &lt; 0.8

In [13]: train = df[msk]

In [14]: test = df[~msk]
</code></pre>

<p>And just to see this has worked:</p>

<pre><code>In [15]: len(test)
Out[15]: 21

In [16]: len(train)
Out[16]: 79
</code></pre>
";;['In [11]: df = pd.DataFrame(np.random.randn(100, 2))\n\nIn [12]: msk = np.random.rand(len(df)) < 0.8\n\nIn [13]: train = df[msk]\n\nIn [14]: test = df[~msk]\n', 'In [15]: len(test)\nOut[15]: 21\n\nIn [16]: len(train)\nOut[16]: 79\n'];['randn', 'In [11]: df = pd.DataFrame(np.random.randn(100, 2))\n\nIn [12]: msk = np.random.rand(len(df)) < 0.8\n\nIn [13]: train = df[msk]\n\nIn [14]: test = df[~msk]\n', 'In [15]: len(test)\nOut[15]: 21\n\nIn [16]: len(train)\nOut[16]: 79\n']
1114;;8;24151789;24147278.0;2;211;;;"<p>SciKit Learn's <code>train_test_split</code>  is a good one.</p>

<pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

train, test = train_test_split(df, test_size = 0.2)
</code></pre>
";;['import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(df, test_size = 0.2)\n'];['train_test_split', 'import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(df, test_size = 0.2)\n']
1115;;2;24216489;24216425.0;2;48;;;"<p>The right way of doing it will be <code>df[""B""] = df[""A""].map(equiv)</code>.</p>

<pre><code>In [55]:

import pandas as pd
equiv = {7001:1, 8001:2, 9001:3}
df = pd.DataFrame( {""A"": [7001, 8001, 9001]} )
df[""B""] = df[""A""].map(equiv)
print(df)
      A  B
0  7001  1
1  8001  2
2  9001  3

[3 rows x 2 columns]
</code></pre>

<p>And it will handle the situation when the key does not exist very nicely, considering the following example:</p>

<pre><code>In [56]:

import pandas as pd
equiv = {7001:1, 8001:2, 9001:3}
df = pd.DataFrame( {""A"": [7001, 8001, 9001, 10000]} )
df[""B""] = df[""A""].map(equiv)
print(df)
       A   B
0   7001   1
1   8001   2
2   9001   3
3  10000 NaN

[4 rows x 2 columns]
</code></pre>
";;"['In [55]:\n\nimport pandas as pd\nequiv = {7001:1, 8001:2, 9001:3}\ndf = pd.DataFrame( {""A"": [7001, 8001, 9001]} )\ndf[""B""] = df[""A""].map(equiv)\nprint(df)\n      A  B\n0  7001  1\n1  8001  2\n2  9001  3\n\n[3 rows x 2 columns]\n', 'In [56]:\n\nimport pandas as pd\nequiv = {7001:1, 8001:2, 9001:3}\ndf = pd.DataFrame( {""A"": [7001, 8001, 9001, 10000]} )\ndf[""B""] = df[""A""].map(equiv)\nprint(df)\n       A   B\n0   7001   1\n1   8001   2\n2   9001   3\n3  10000 NaN\n\n[4 rows x 2 columns]\n']";"['df[""B""] = df[""A""].map(equiv)', 'In [55]:\n\nimport pandas as pd\nequiv = {7001:1, 8001:2, 9001:3}\ndf = pd.DataFrame( {""A"": [7001, 8001, 9001]} )\ndf[""B""] = df[""A""].map(equiv)\nprint(df)\n      A  B\n0  7001  1\n1  8001  2\n2  9001  3\n\n[3 rows x 2 columns]\n', 'In [56]:\n\nimport pandas as pd\nequiv = {7001:1, 8001:2, 9001:3}\ndf = pd.DataFrame( {""A"": [7001, 8001, 9001, 10000]} )\ndf[""B""] = df[""A""].map(equiv)\nprint(df)\n       A   B\n0   7001   1\n1   8001   2\n2   9001   3\n3  10000 NaN\n\n[4 rows x 2 columns]\n']"
1116;;1;24222837;15118111.0;2;8;;;"<p>I always use lambdas and the built-in <code>map()</code> function to create new rows by combining other rows:</p>

<pre><code>st['a'] = map(lambda path, row: path + 2 * row, st['path'], st['row'])
</code></pre>

<p>It might be slightly more complicated than necessary for doing linear combinations of numerical columns.  On the other hand, I feel it's good to adopt as a convention as it can be used with more complicated combinations of rows (e.g. working with strings) or filling missing data in a column using functions of the other columns.</p>

<p>For example, lets say you have a table with columns gender, and title, and some of the titles are missing.  You can fill them with a function as follows:</p>

<pre><code>title_dict = {'male': 'mr.', 'female': 'ms.'}
table['title'] = map(lambda title,
    gender: title if title != None else title_dict[gender],
    table['title'], table['gender'])
</code></pre>
";;"[""st['a'] = map(lambda path, row: path + 2 * row, st['path'], st['row'])\n"", ""title_dict = {'male': 'mr.', 'female': 'ms.'}\ntable['title'] = map(lambda title,\n    gender: title if title != None else title_dict[gender],\n    table['title'], table['gender'])\n""]";"['map()', ""st['a'] = map(lambda path, row: path + 2 * row, st['path'], st['row'])\n"", ""title_dict = {'male': 'mr.', 'female': 'ms.'}\ntable['title'] = map(lambda title,\n    gender: title if title != None else title_dict[gender],\n    table['title'], table['gender'])\n""]"
1117;;2;24251426;24251219.0;2;16;;;"<p>Try:</p>

<pre><code>dashboard_df = pd.read_csv(p_file, sep=',', error_bad_lines=False, index_col=False, dtype='unicode')
</code></pre>

<p>According to the pandas documentation:</p>

<blockquote>
  <p>dtype : Type name or dict of column -> type</p>
</blockquote>

<p>As for low_memory, it's True <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.io.parsers.read_csv.html"">by default</a> and isn't yet documented. I don't think its relevant though. The error message is generic, so you shouldn't need to mess with low_memory anyway. Hope this helps and let me know if you have further problems</p>
";;"[""dashboard_df = pd.read_csv(p_file, sep=',', error_bad_lines=False, index_col=False, dtype='unicode')\n""]";"[""dashboard_df = pd.read_csv(p_file, sep=',', error_bad_lines=False, index_col=False, dtype='unicode')\n""]"
1118;;3;24283087;24193174.0;2;39;;;"<p>You can reset the colorcycle to the original with <a href=""http://matplotlib.org/api/axes_api.html?#matplotlib.axes.Axes.set_color_cycle"">Axes.set_color_cycle</a>. Looking at the code for this, there is a function to do the actual work:</p>

<pre><code>def set_color_cycle(self, clist=None):
    if clist is None:
        clist = rcParams['axes.color_cycle']
    self.color_cycle = itertools.cycle(clist
</code></pre>

<p>And a method on the Axes which uses it:</p>

<pre><code>def set_color_cycle(self, clist):
    """"""
    Set the color cycle for any future plot commands on this Axes.

    *clist* is a list of mpl color specifiers.
    """"""
    self._get_lines.set_color_cycle(clist)
    self._get_patches_for_fill.set_color_cycle(clist)
</code></pre>

<p>This basically means you can call the set_color_cycle with None as the only argument, and it will be replaced with the default cycle found in rcParams['axes.color_cycle'].</p>

<p>I tried this with the following code and got the expected result:</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np

for i in range(3):
    plt.plot(np.arange(10) + i)

plt.gca().set_color_cycle(None)

for i in range(3):
    plt.plot(np.arange(10, 1, -1) + i)

plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/2k3hr.png"" alt=""Code output, showing the color cycling reset functionality""></p>
";;"[""def set_color_cycle(self, clist=None):\n    if clist is None:\n        clist = rcParams['axes.color_cycle']\n    self.color_cycle = itertools.cycle(clist\n"", 'def set_color_cycle(self, clist):\n    """"""\n    Set the color cycle for any future plot commands on this Axes.\n\n    *clist* is a list of mpl color specifiers.\n    """"""\n    self._get_lines.set_color_cycle(clist)\n    self._get_patches_for_fill.set_color_cycle(clist)\n', 'import matplotlib.pyplot as plt\nimport numpy as np\n\nfor i in range(3):\n    plt.plot(np.arange(10) + i)\n\nplt.gca().set_color_cycle(None)\n\nfor i in range(3):\n    plt.plot(np.arange(10, 1, -1) + i)\n\nplt.show()\n']";"[""def set_color_cycle(self, clist=None):\n    if clist is None:\n        clist = rcParams['axes.color_cycle']\n    self.color_cycle = itertools.cycle(clist\n"", 'def set_color_cycle(self, clist):\n    """"""\n    Set the color cycle for any future plot commands on this Axes.\n\n    *clist* is a list of mpl color specifiers.\n    """"""\n    self._get_lines.set_color_cycle(clist)\n    self._get_patches_for_fill.set_color_cycle(clist)\n', 'import matplotlib.pyplot as plt\nimport numpy as np\n\nfor i in range(3):\n    plt.plot(np.arange(10) + i)\n\nplt.gca().set_color_cycle(None)\n\nfor i in range(3):\n    plt.plot(np.arange(10, 1, -1) + i)\n\nplt.show()\n']"
1119;;1;24284515;24284342.0;2;12;;;"<p>One way to achieve this is </p>

<pre><code>&gt;&gt;&gt; pd.DataFrame(np.array([[2, 3, 4]]), columns=['A', 'B', 'C']).append(df, ignore_index=True)
Out[330]: 
   A  B  C
0  2  3  4
1  5  6  7
2  7  8  9
</code></pre>

<p>Generally, it's easiest to append dataframes, not series. In your case, since you want the new row to be ""on top"" (with starting id), and there is no function <code>pd.prepend()</code>, I first create the new dataframe and then append your old one.</p>

<p><code>ignore_index</code> will ignore the old ongoing index in your dataframe and ensure that the first row actually starts with index <code>1</code> instead of restarting with index <code>0</code>.</p>

<p><em>Typical Disclaimer: Cetero censeo ... appending rows is a quite inefficient operation. If you care about performance and can somehow ensure to first create a dataframe with the correct (longer) index and then just <strong>inserting</strong> the additional row into the dataframe, you should definitely do that. See:</em></p>

<pre><code>&gt;&gt;&gt; index = np.array([0, 1, 2])
&gt;&gt;&gt; df2 = pd.DataFrame(columns=['A', 'B', 'C'], index=index)
&gt;&gt;&gt; df2.loc[0:1] = [list(s1), list(s2)]
&gt;&gt;&gt; df2
Out[336]: 
     A    B    C
0    5    6    7
1    7    8    9
2  NaN  NaN  NaN
&gt;&gt;&gt; df2 = pd.DataFrame(columns=['A', 'B', 'C'], index=index)
&gt;&gt;&gt; df2.loc[1:] = [list(s1), list(s2)]
</code></pre>

<p>So far, we have what you had as <code>df</code>:</p>

<pre><code>&gt;&gt;&gt; df2
Out[339]: 
     A    B    C
0  NaN  NaN  NaN
1    5    6    7
2    7    8    9
</code></pre>

<p>But now you can easily insert the row as follows. Since the space was preallocated, this is more efficient. </p>

<pre><code>&gt;&gt;&gt; df2.loc[0] = np.array([2, 3, 4])
&gt;&gt;&gt; df2
Out[341]: 
   A  B  C
0  2  3  4
1  5  6  7
2  7  8  9
</code></pre>
";;"["">>> pd.DataFrame(np.array([[2, 3, 4]]), columns=['A', 'B', 'C']).append(df, ignore_index=True)\nOut[330]: \n   A  B  C\n0  2  3  4\n1  5  6  7\n2  7  8  9\n"", "">>> index = np.array([0, 1, 2])\n>>> df2 = pd.DataFrame(columns=['A', 'B', 'C'], index=index)\n>>> df2.loc[0:1] = [list(s1), list(s2)]\n>>> df2\nOut[336]: \n     A    B    C\n0    5    6    7\n1    7    8    9\n2  NaN  NaN  NaN\n>>> df2 = pd.DataFrame(columns=['A', 'B', 'C'], index=index)\n>>> df2.loc[1:] = [list(s1), list(s2)]\n"", '>>> df2\nOut[339]: \n     A    B    C\n0  NaN  NaN  NaN\n1    5    6    7\n2    7    8    9\n', '>>> df2.loc[0] = np.array([2, 3, 4])\n>>> df2\nOut[341]: \n   A  B  C\n0  2  3  4\n1  5  6  7\n2  7  8  9\n']";"["">>> pd.DataFrame(np.array([[2, 3, 4]]), columns=['A', 'B', 'C']).append(df, ignore_index=True)\nOut[330]: \n   A  B  C\n0  2  3  4\n1  5  6  7\n2  7  8  9\n"", 'pd.prepend()', 'ignore_index', '1', '0', "">>> index = np.array([0, 1, 2])\n>>> df2 = pd.DataFrame(columns=['A', 'B', 'C'], index=index)\n>>> df2.loc[0:1] = [list(s1), list(s2)]\n>>> df2\nOut[336]: \n     A    B    C\n0    5    6    7\n1    7    8    9\n2  NaN  NaN  NaN\n>>> df2 = pd.DataFrame(columns=['A', 'B', 'C'], index=index)\n>>> df2.loc[1:] = [list(s1), list(s2)]\n"", 'df', '>>> df2\nOut[339]: \n     A    B    C\n0  NaN  NaN  NaN\n1    5    6    7\n2    7    8    9\n', '>>> df2.loc[0] = np.array([2, 3, 4])\n>>> df2\nOut[341]: \n   A  B  C\n0  2  3  4\n1  5  6  7\n2  7  8  9\n']"
1120;;2;24284680;24284342.0;2;36;;;"<p>Just assign row to a particular index, using <code>loc</code>:</p>

<pre><code> df.loc[-1] = [2, 3, 4]  # adding a row
 df.index = df.index + 1  # shifting index
 df = df.sort()  # sorting by index
</code></pre>

<p>And you get, as desired:</p>

<pre><code>    A  B  C
 0  2  3  4
 1  5  6  7
 2  7  8  9
</code></pre>

<p>See in Pandas documentation <a href=""http://pandas-docs.github.io/pandas-docs-travis/indexing.html#setting-with-enlargement"" rel=""noreferrer"">Indexing: Setting with enlargement</a>.</p>
";;[' df.loc[-1] = [2, 3, 4]  # adding a row\n df.index = df.index + 1  # shifting index\n df = df.sort()  # sorting by index\n', '    A  B  C\n 0  2  3  4\n 1  5  6  7\n 2  7  8  9\n'];['loc', ' df.loc[-1] = [2, 3, 4]  # adding a row\n df.index = df.index + 1  # shifting index\n df = df.sort()  # sorting by index\n', '    A  B  C\n 0  2  3  4\n 1  5  6  7\n 2  7  8  9\n']
1121;;0;24287210;24284342.0;2;6;;;"<p>Not sure how you were calling concat() but it should work as long as both objects are of the same type. Maybe the issue is that you need to cast your second vector to a dataframe? Using the df that you defined the following works for me.</p>

<pre><code>&gt;&gt;&gt;df2 = pd.DataFrame([[2,3,4]],columns=['A','B','C'])
&gt;&gt;&gt;pd.concat([df2,df])
</code></pre>
";;"["">>>df2 = pd.DataFrame([[2,3,4]],columns=['A','B','C'])\n>>>pd.concat([df2,df])\n""]";"["">>>df2 = pd.DataFrame([[2,3,4]],columns=['A','B','C'])\n>>>pd.concat([df2,df])\n""]"
1122;;2;24368660;18695605.0;2;14;;;"<p>The answers by joris in this thread and by punchagan in the <a href=""https://stackoverflow.com/questions/18012505/python-pandas-dataframe-columns-convert-to-dict-key-and-value"">duplicated thread</a> are very elegant, however they will not give correct results if the column used for the keys contains any duplicated value. </p>

<p>For example:</p>

<pre><code>&gt;&gt;&gt; ptest = p.DataFrame([['a',1],['a',2],['b',3]], columns=['id', 'value']) 
&gt;&gt;&gt; ptest
  id  value
0  a      1
1  a      2
2  b      3

# note that in both cases the association a-&gt;1 is lost:
&gt;&gt;&gt; ptest.set_index('id')['value'].to_dict()
{'a': 2, 'b': 3}
&gt;&gt;&gt; dict(zip(ptest.id, ptest.value))
{'a': 2, 'b': 3}
</code></pre>

<p>If you have duplicated entries and do not want to lose them, you can use this ugly but working code:</p>

<pre><code>&gt;&gt;&gt; mydict = {}
&gt;&gt;&gt; for x in range(len(ptest)):
...     currentid = ptest.iloc[x,0]
...     currentvalue = ptest.iloc[x,1]
...     mydict.setdefault(currentid, [])
...     mydict[currentid].append(currentvalue)
&gt;&gt;&gt; mydict
{'a': [1, 2], 'b': [3]}
</code></pre>
";;"["">>> ptest = p.DataFrame([['a',1],['a',2],['b',3]], columns=['id', 'value']) \n>>> ptest\n  id  value\n0  a      1\n1  a      2\n2  b      3\n\n# note that in both cases the association a->1 is lost:\n>>> ptest.set_index('id')['value'].to_dict()\n{'a': 2, 'b': 3}\n>>> dict(zip(ptest.id, ptest.value))\n{'a': 2, 'b': 3}\n"", "">>> mydict = {}\n>>> for x in range(len(ptest)):\n...     currentid = ptest.iloc[x,0]\n...     currentvalue = ptest.iloc[x,1]\n...     mydict.setdefault(currentid, [])\n...     mydict[currentid].append(currentvalue)\n>>> mydict\n{'a': [1, 2], 'b': [3]}\n""]";"["">>> ptest = p.DataFrame([['a',1],['a',2],['b',3]], columns=['id', 'value']) \n>>> ptest\n  id  value\n0  a      1\n1  a      2\n2  b      3\n\n# note that in both cases the association a->1 is lost:\n>>> ptest.set_index('id')['value'].to_dict()\n{'a': 2, 'b': 3}\n>>> dict(zip(ptest.id, ptest.value))\n{'a': 2, 'b': 3}\n"", "">>> mydict = {}\n>>> for x in range(len(ptest)):\n...     currentid = ptest.iloc[x,0]\n...     currentvalue = ptest.iloc[x,1]\n...     mydict.setdefault(currentid, [])\n...     mydict[currentid].append(currentvalue)\n>>> mydict\n{'a': [1, 2], 'b': [3]}\n""]"
1123;;2;24370510;18695605.0;2;27;;;"<p>If you want a simple way to preserve duplicates, you could use <code>groupby</code>:</p>

<pre><code>&gt;&gt;&gt; ptest = pd.DataFrame([['a',1],['a',2],['b',3]], columns=['id', 'value']) 
&gt;&gt;&gt; ptest
  id  value
0  a      1
1  a      2
2  b      3
&gt;&gt;&gt; {k: g[""value""].tolist() for k,g in ptest.groupby(""id"")}
{'a': [1, 2], 'b': [3]}
</code></pre>
";;"['>>> ptest = pd.DataFrame([[\'a\',1],[\'a\',2],[\'b\',3]], columns=[\'id\', \'value\']) \n>>> ptest\n  id  value\n0  a      1\n1  a      2\n2  b      3\n>>> {k: g[""value""].tolist() for k,g in ptest.groupby(""id"")}\n{\'a\': [1, 2], \'b\': [3]}\n']";"['groupby', '>>> ptest = pd.DataFrame([[\'a\',1],[\'a\',2],[\'b\',3]], columns=[\'id\', \'value\']) \n>>> ptest\n  id  value\n0  a      1\n1  a      2\n2  b      3\n>>> {k: g[""value""].tolist() for k,g in ptest.groupby(""id"")}\n{\'a\': [1, 2], \'b\': [3]}\n']"
1124;;1;24396554;11067027.0;2;17;;;"<p>You can just do:</p>

<pre>
df[sorted(df.columns)]
</pre>
";;['\ndf[sorted(df.columns)]\n'];[]
1125;;1;24489283;13888468.0;2;20;;;"<p>Andy Hayden's answer (<code>index.levels[blah]</code>) is great for some scenarios, but can lead to odd behavior in others.  My understanding is that Pandas goes to great lengths to ""reuse"" indices when possible to avoid having the indices of lots of similarly-indexed DataFrames taking up space in memory.  As a result, <a href=""https://stackoverflow.com/questions/24434724/dataframe-index-levels-shows-extra-values-after-paring-down-dataframe/"">I've found the following annoying behavior</a>:</p>

<pre><code>import pandas as pd
import numpy as np

np.random.seed(0)

idx = pd.MultiIndex.from_product([['John', 'Josh', 'Alex'], list('abcde')], 
                                 names=['Person', 'Letter'])
large = pd.DataFrame(data=np.random.randn(15, 2), 
                     index=idx, 
                     columns=['one', 'two'])
small = large.loc[['Jo'==d[0:2] for d in large.index.get_level_values('Person')]]

print small.index.levels[0]
print large.index.levels[0]
</code></pre>

<p>Which outputs</p>

<pre><code>Index([u'Alex', u'John', u'Josh'], dtype='object')
Index([u'Alex', u'John', u'Josh'], dtype='object')
</code></pre>

<p>rather than the expected</p>

<pre><code>Index([u'John', u'Josh'], dtype='object')
Index([u'Alex', u'John', u'Josh'], dtype='object')
</code></pre>

<p>As one person pointed out on the other thread, one idiom that seems very natural and works properly would be:</p>

<pre><code>small.index.get_level_values('Person').unique()
large.index.get_level_values('Person').unique()
</code></pre>

<p>I hope this helps someone else dodge the super-unexpected behavior that I ran into.</p>
";;"[""import pandas as pd\nimport numpy as np\n\nnp.random.seed(0)\n\nidx = pd.MultiIndex.from_product([['John', 'Josh', 'Alex'], list('abcde')], \n                                 names=['Person', 'Letter'])\nlarge = pd.DataFrame(data=np.random.randn(15, 2), \n                     index=idx, \n                     columns=['one', 'two'])\nsmall = large.loc[['Jo'==d[0:2] for d in large.index.get_level_values('Person')]]\n\nprint small.index.levels[0]\nprint large.index.levels[0]\n"", ""Index([u'Alex', u'John', u'Josh'], dtype='object')\nIndex([u'Alex', u'John', u'Josh'], dtype='object')\n"", ""Index([u'John', u'Josh'], dtype='object')\nIndex([u'Alex', u'John', u'Josh'], dtype='object')\n"", ""small.index.get_level_values('Person').unique()\nlarge.index.get_level_values('Person').unique()\n""]";"['index.levels[blah]', ""import pandas as pd\nimport numpy as np\n\nnp.random.seed(0)\n\nidx = pd.MultiIndex.from_product([['John', 'Josh', 'Alex'], list('abcde')], \n                                 names=['Person', 'Letter'])\nlarge = pd.DataFrame(data=np.random.randn(15, 2), \n                     index=idx, \n                     columns=['one', 'two'])\nsmall = large.loc[['Jo'==d[0:2] for d in large.index.get_level_values('Person')]]\n\nprint small.index.levels[0]\nprint large.index.levels[0]\n"", ""Index([u'Alex', u'John', u'Josh'], dtype='object')\nIndex([u'Alex', u'John', u'Josh'], dtype='object')\n"", ""Index([u'John', u'Josh'], dtype='object')\nIndex([u'Alex', u'John', u'Josh'], dtype='object')\n"", ""small.index.get_level_values('Person').unique()\nlarge.index.get_level_values('Person').unique()\n""]"
1126;;4;24489602;18172851.0;2;91;;;"<p>But for any future bypassers you could mention that <code>df = df[df.line_race != 0]</code> doesn't do anything when trying to filter for <code>None</code>/missing values.</p>

<p>Does work:</p>

<pre><code>df = df[df.line_race != 0]
</code></pre>

<p>Doesn't do anything:</p>

<pre><code>df = df[df.line_race != None]
</code></pre>

<p>Does work:</p>

<pre><code>df = df[df.line_race.notnull()]
</code></pre>
";;['df = df[df.line_race != 0]\n', 'df = df[df.line_race != None]\n', 'df = df[df.line_race.notnull()]\n'];['df = df[df.line_race != 0]', 'None', 'df = df[df.line_race != 0]\n', 'df = df[df.line_race != None]\n', 'df = df[df.line_race.notnull()]\n']
1127;;4;24496435;24495695.0;2;37;;;"<p>I guess u want unique values in a certain level (and by level names) of a multiindex. I usually do the following, which is a bit long.</p>

<pre><code>In [11]: df.index.get_level_values('co').unique()
Out[11]: array(['DE', 'FR'], dtype=object)
</code></pre>
";;"[""In [11]: df.index.get_level_values('co').unique()\nOut[11]: array(['DE', 'FR'], dtype=object)\n""]";"[""In [11]: df.index.get_level_values('co').unique()\nOut[11]: array(['DE', 'FR'], dtype=object)\n""]"
1128;;6;24517695;13842088.0;2;146;;;"<p>The fastest way to do this is using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_value.html"">set_value</a>. This method is ~100 times faster than <code>.ix</code> method. For example: </p>

<p><code>df.set_value('C', 'x', 10)</code></p>
";;[];"['.ix', ""df.set_value('C', 'x', 10)""]"
1129;;4;24674675;19555525.0;2;42;;;"<p>The gcf method is depricated in V 0.14, The below code works for me:</p>

<pre><code>plot = dtf.plot()
fig = plot.get_figure()
fig.savefig(""output.png"")
</code></pre>
";;"['plot = dtf.plot()\nfig = plot.get_figure()\nfig.savefig(""output.png"")\n']";"['plot = dtf.plot()\nfig = plot.get_figure()\nfig.savefig(""output.png"")\n']"
1130;;1;24775756;24775648.0;2;42;;;"<p>The corresponding operator is <code>|</code>:</p>

<pre><code> df[(df &lt; 3) | (df == 5)]
</code></pre>

<p>would elementwise check if value is less than 3 or equal to 5.</p>
";;[' df[(df < 3) | (df == 5)]\n'];['|', ' df[(df < 3) | (df == 5)]\n']
1131;;1;24793359;13187778.0;2;58;;;"<p>Pandas has something built in...</p>

<pre><code>numpyMatrix = df.as_matrix()
</code></pre>
";;['numpyMatrix = df.as_matrix()\n'];['numpyMatrix = df.as_matrix()\n']
1132;;2;24804512;21360361.0;2;23;;;"<p>You can further improve this by adding <code>wait=True</code> to <code>clear_output</code>:</p>

<pre><code>display.clear_output(wait=True)
display.display(pl.gcf())
</code></pre>
";;['display.clear_output(wait=True)\ndisplay.display(pl.gcf())\n'];['wait=True', 'clear_output', 'display.clear_output(wait=True)\ndisplay.display(pl.gcf())\n']
1133;;0;24870404;24870306.0;2;134;;;"<p>This will work:</p>

<pre><code>if 'A' in df:
</code></pre>

<p>But for clarity, I'd probably write it as:</p>

<pre><code>if 'A' in df.columns:
</code></pre>
";;"[""if 'A' in df:\n"", ""if 'A' in df.columns:\n""]";"[""if 'A' in df:\n"", ""if 'A' in df.columns:\n""]"
1134;;3;24871316;24870953.0;2;48;;;"<p>Generally, <code>iterrows</code> should only be used in very very specific cases. This is the general order of precedence for performance of various operations:</p>

<pre><code>1) vectorization
2) using a custom cython routine
3) apply
    a) reductions that can be performed in cython
    b) iteration in python space
4) itertuples
5) iterrows
6) updating an empty frame (e.g. using loc one-row-at-a-time)
</code></pre>

<p>Using a custom cython routine is usually too complicated, so let's skip that for now.</p>

<p>1) Vectorization is ALWAYS ALWAYS the first and best choice. However, their are a small set of cases which can not be vectorized in obvious ways (mostly involving a recurrence). Further on a smallish frame, it may be faster to do other methods.</p>

<p>3) Apply involves <em>can</em> usually be done by an iterator in cython space (this is done internally in pandas) (this is a) case.</p>

<p>This is dependent on what is going on inside the apply expression. e.g. <code>df.apply(lambda x: np.sum(x))</code> will be executed pretty swiftly (of course <code>df.sum(1)</code> is even better). However something like: <code>df.apply(lambda x: x['b'] + 1)</code> will be executed in python space, and consequently is slower.</p>

<p>4) <code>itertuples</code> does not box the data into a Series, just returns it as a tuple</p>

<p>5) <code>iterrows</code> DOES box the data into a Series. Unless you really need this, use another method.</p>

<p>6) updating an empty frame a-single-row-at-a-time. I have seen this method used WAY too much. It is by far the slowest. It is probably common place (and reasonably fast for some python structures), but a DataFrame does a fair number of checks on indexing, so this will always be very slow to update a row at a time. Much better to create new structures and <code>concat</code>.</p>
";;['1) vectorization\n2) using a custom cython routine\n3) apply\n    a) reductions that can be performed in cython\n    b) iteration in python space\n4) itertuples\n5) iterrows\n6) updating an empty frame (e.g. using loc one-row-at-a-time)\n'];"['iterrows', '1) vectorization\n2) using a custom cython routine\n3) apply\n    a) reductions that can be performed in cython\n    b) iteration in python space\n4) itertuples\n5) iterrows\n6) updating an empty frame (e.g. using loc one-row-at-a-time)\n', 'df.apply(lambda x: np.sum(x))', 'df.sum(1)', ""df.apply(lambda x: x['b'] + 1)"", 'itertuples', 'iterrows', 'concat']"
1135;;5;24888331;10715965.0;2;180;;;"<p>Example at @Nasser's answer:</p>

<pre><code>&gt;&gt;&gt; df = DataFrame(columns=('lib', 'qty1', 'qty2'))
&gt;&gt;&gt; for i in range(5):
&gt;&gt;&gt;     df.loc[i] = [randint(-1,1) for n in range(3)]
&gt;&gt;&gt;
&gt;&gt;&gt; print(df)
    lib  qty1  qty2
0    0     0    -1
1   -1    -1     1
2    1    -1     1
3    0     0     0
4    1    -1    -1

[5 rows x 3 columns]
</code></pre>
";;"["">>> df = DataFrame(columns=('lib', 'qty1', 'qty2'))\n>>> for i in range(5):\n>>>     df.loc[i] = [randint(-1,1) for n in range(3)]\n>>>\n>>> print(df)\n    lib  qty1  qty2\n0    0     0    -1\n1   -1    -1     1\n2    1    -1     1\n3    0     0     0\n4    1    -1    -1\n\n[5 rows x 3 columns]\n""]";"["">>> df = DataFrame(columns=('lib', 'qty1', 'qty2'))\n>>> for i in range(5):\n>>>     df.loc[i] = [randint(-1,1) for n in range(3)]\n>>>\n>>> print(df)\n    lib  qty1  qty2\n0    0     0    -1\n1   -1    -1     1\n2    1    -1     1\n3    0     0     0\n4    1    -1    -1\n\n[5 rows x 3 columns]\n""]"
1136;;2;24913075;10715965.0;2;42;;;"<p>If you know the number of entries ex ante, you should preallocate the space by also providing the index (taking the data example from a different answer):</p>

<pre><code>import pandas as pd
import numpy as np
# we know we're gonna have 5 rows of data
numberOfRows = 5
# create dataframe
df = pd.DataFrame(index=np.arange(0, numberOfRows), columns=('lib', 'qty1', 'qty2') )

# now fill it up row by row
for x in np.arange(0, numberOfRows):
    #loc or iloc both work here since the index is natural numbers
    df.loc[x] = [np.random.randint(-1,1) for n in range(3)]
In[23]: df
Out[23]: 
   lib  qty1  qty2
0   -1    -1    -1
1    0     0     0
2   -1     0    -1
3    0    -1     0
4   -1     0     0
</code></pre>

<p><strong>Speed comparison</strong></p>

<pre><code>In[30]: %timeit tryThis() # function wrapper for this answer
In[31]: %timeit tryOther() # function wrapper without index (see, for example, @fred)
1000 loops, best of 3: 1.23 ms per loop
100 loops, best of 3: 2.31 ms per loop
</code></pre>

<p>And - as from the comments - with a size of 6000, the speed difference becomes even larger:</p>

<blockquote>
  <p>Increasing the size of the array (12) and the number of rows (500) makes
  the speed difference more striking: 313ms vs 2.29s</p>
</blockquote>
";;"[""import pandas as pd\nimport numpy as np\n# we know we're gonna have 5 rows of data\nnumberOfRows = 5\n# create dataframe\ndf = pd.DataFrame(index=np.arange(0, numberOfRows), columns=('lib', 'qty1', 'qty2') )\n\n# now fill it up row by row\nfor x in np.arange(0, numberOfRows):\n    #loc or iloc both work here since the index is natural numbers\n    df.loc[x] = [np.random.randint(-1,1) for n in range(3)]\nIn[23]: df\nOut[23]: \n   lib  qty1  qty2\n0   -1    -1    -1\n1    0     0     0\n2   -1     0    -1\n3    0    -1     0\n4   -1     0     0\n"", 'In[30]: %timeit tryThis() # function wrapper for this answer\nIn[31]: %timeit tryOther() # function wrapper without index (see, for example, @fred)\n1000 loops, best of 3: 1.23 ms per loop\n100 loops, best of 3: 2.31 ms per loop\n']";"[""import pandas as pd\nimport numpy as np\n# we know we're gonna have 5 rows of data\nnumberOfRows = 5\n# create dataframe\ndf = pd.DataFrame(index=np.arange(0, numberOfRows), columns=('lib', 'qty1', 'qty2') )\n\n# now fill it up row by row\nfor x in np.arange(0, numberOfRows):\n    #loc or iloc both work here since the index is natural numbers\n    df.loc[x] = [np.random.randint(-1,1) for n in range(3)]\nIn[23]: df\nOut[23]: \n   lib  qty1  qty2\n0   -1    -1    -1\n1    0     0     0\n2   -1     0    -1\n3    0    -1     0\n4   -1     0     0\n"", 'In[30]: %timeit tryThis() # function wrapper for this answer\nIn[31]: %timeit tryOther() # function wrapper without index (see, for example, @fred)\n1000 loops, best of 3: 1.23 ms per loop\n100 loops, best of 3: 2.31 ms per loop\n']"
1137;;3;24933234;19828822.0;2;25;;;"<p>I use len function. It's much faster than empty(). len(df.index) is even faster.</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randn(10000, 4), columns=list('ABCD'))

def empty(df):
    return df.empty

def lenz(df):
    return len(df) == 0

def lenzi(df):
    return len(df.index) == 0

'''
%timeit empty(df)
%timeit lenz(df)
%timeit lenzi(df)

10000 loops, best of 3: 13.9 s per loop
100000 loops, best of 3: 2.34 s per loop
1000000 loops, best of 3: 695 ns per loop

len on index seems to be faster
'''
</code></pre>
";;"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randn(10000, 4), columns=list('ABCD'))\n\ndef empty(df):\n    return df.empty\n\ndef lenz(df):\n    return len(df) == 0\n\ndef lenzi(df):\n    return len(df.index) == 0\n\n'''\n%timeit empty(df)\n%timeit lenz(df)\n%timeit lenzi(df)\n\n10000 loops, best of 3: 13.9 s per loop\n100000 loops, best of 3: 2.34 s per loop\n1000000 loops, best of 3: 695 ns per loop\n\nlen on index seems to be faster\n'''\n""]";"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randn(10000, 4), columns=list('ABCD'))\n\ndef empty(df):\n    return df.empty\n\ndef lenz(df):\n    return len(df) == 0\n\ndef lenzi(df):\n    return len(df.index) == 0\n\n'''\n%timeit empty(df)\n%timeit lenz(df)\n%timeit lenzi(df)\n\n10000 loops, best of 3: 13.9 s per loop\n100000 loops, best of 3: 2.34 s per loop\n1000000 loops, best of 3: 695 ns per loop\n\nlen on index seems to be faster\n'''\n""]"
1138;;0;25023460;13148429.0;2;8;;;"<pre><code>def order(frame,var):
    varlist =[w for w in frame.columns if w not in var]
    frame = frame[var+varlist]
    return frame 
</code></pre>

<p>This function takes two arguments, the first is the dataset, the second are the columns in the data set that you want to bring to the front. </p>

<p>So in my case I have a data set called Frame with variables A1, A2, B1, B2, Total and Date. If I want to bring Total to the front then all I have to do is: </p>

<pre><code>frame = order(frame,['Total'])
</code></pre>

<p>If I want to bring Total and Date to the front then I do:</p>

<pre><code>frame = order(frame,['Total','Date'])
</code></pre>

<p>EDIT:</p>

<p>Another useful way to use this is, if you have an unfamiliar table and you're looking with variables with a particular term in them, like VAR1, VAR2,... you may execute something like: </p>

<pre><code>frame = order(frame,[v for v in frame.columns if ""VAR"" in v])
</code></pre>
";;"['def order(frame,var):\n    varlist =[w for w in frame.columns if w not in var]\n    frame = frame[var+varlist]\n    return frame \n', ""frame = order(frame,['Total'])\n"", ""frame = order(frame,['Total','Date'])\n"", 'frame = order(frame,[v for v in frame.columns if ""VAR"" in v])\n']";"['def order(frame,var):\n    varlist =[w for w in frame.columns if w not in var]\n    frame = frame[var+varlist]\n    return frame \n', ""frame = order(frame,['Total'])\n"", ""frame = order(frame,['Total','Date'])\n"", 'frame = order(frame,[v for v in frame.columns if ""VAR"" in v])\n']"
1139;;5;25030617;10065051.0;2;41;;;"<p>For recent readers of this question: pandas have the following warning in their <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html"" rel=""noreferrer"">docs for version 14.0</a>:</p>

<blockquote>
  <p>Warning: Some of the existing functions or function aliases have been
  deprecated and will be removed in future versions. This includes:
  tquery, uquery, read_frame, frame_query, write_frame. </p>
</blockquote>

<p>And:</p>

<blockquote>
  <p>Warning: The support for the mysql flavor when using DBAPI connection objects has
  been deprecated. MySQL will be further supported with SQLAlchemy
  engines (GH6900).</p>
</blockquote>

<p>This makes many of the answers here outdated. You should use <code>sqlalchemy</code>:</p>

<pre><code>from sqlalchemy import create_engine
import pandas as pd
engine = create_engine('dialect://user:pass@host:port/schema', echo=False)
f = pd.read_sql_query('SELECT * FROM mytable', engine, index_col = 'ID')
</code></pre>
";;"[""from sqlalchemy import create_engine\nimport pandas as pd\nengine = create_engine('dialect://user:pass@host:port/schema', echo=False)\nf = pd.read_sql_query('SELECT * FROM mytable', engine, index_col = 'ID')\n""]";"['sqlalchemy', ""from sqlalchemy import create_engine\nimport pandas as pd\nengine = create_engine('dialect://user:pass@host:port/schema', echo=False)\nf = pd.read_sql_query('SELECT * FROM mytable', engine, index_col = 'ID')\n""]"
1140;;1;25057724;25055712.0;2;41;;;"<p>I'd use <code>iloc</code>, which takes a row/column slice, both based on integer position and following normal python syntax.</p>

<pre><code>df.iloc[::5, :]
</code></pre>
";;['df.iloc[::5, :]\n'];['iloc', 'df.iloc[::5, :]\n']
1141;;3;25146337;25146121.0;2;42;;;"<p>You can directly access the <code>year</code> and <code>month</code> attributes, or request a <code>datetime.datetime</code>:</p>

<pre><code>In [15]: t = pandas.tslib.Timestamp.now()

In [16]: t
Out[16]: Timestamp('2014-08-05 14:49:39.643701', tz=None)

In [17]: t.to_datetime()
Out[17]: datetime.datetime(2014, 8, 5, 14, 49, 39, 643701)

In [18]: t.day
Out[18]: 5

In [19]: t.month
Out[19]: 8

In [20]: t.year
Out[20]: 2014
</code></pre>

<p>One way to combine year and month is to make an integer encoding them, such as: <code>201408</code> for August, 2014. Along a whole column, you could do this as:</p>

<pre><code>df['YearMonth'] = df['ArrivalDate'].map(lambda x: 1000*x.year + x.month)
</code></pre>

<p>or many variants thereof.</p>

<p>I'm not a big fan of doing this, though, since it makes date alignment and arithmetic painful later and especially painful for others who come upon your code or data without this same convention. A better way is to choose a day-of-month convention, such as final non-US-holiday weekday, or first day, etc., and leave the data in a date/time format with the chosen date convention.</p>

<p>The <code>calendar</code> module is useful for obtaining the number value of certain days such as the final weekday. Then you could do something like:</p>

<pre><code>import calendar
import datetime
df['AdjustedDateToEndOfMonth'] = df['ArrivalDate'].map(
    lambda x: datetime.datetime(
        x.year,
        x.month,
        max(calendar.monthcalendar(x.year, x.month)[-1][:5])
    )
)
</code></pre>

<p>If you happen to be looking for a way to solve the simpler problem of just formatting the datetime column into some stringified representation, for that you can just make use of the <a href=""http://strftime.org/""><code>strftime</code></a> function from the <code>datetime.datetime</code> class, like this:</p>

<pre><code>In [5]: df
Out[5]: 
            date_time
0 2014-10-17 22:00:03

In [6]: df.date_time
Out[6]: 
0   2014-10-17 22:00:03
Name: date_time, dtype: datetime64[ns]

In [7]: df.date_time.map(lambda x: x.strftime('%Y-%m-%d'))
Out[7]: 
0    2014-10-17
Name: date_time, dtype: object
</code></pre>
";;"[""In [15]: t = pandas.tslib.Timestamp.now()\n\nIn [16]: t\nOut[16]: Timestamp('2014-08-05 14:49:39.643701', tz=None)\n\nIn [17]: t.to_datetime()\nOut[17]: datetime.datetime(2014, 8, 5, 14, 49, 39, 643701)\n\nIn [18]: t.day\nOut[18]: 5\n\nIn [19]: t.month\nOut[19]: 8\n\nIn [20]: t.year\nOut[20]: 2014\n"", ""df['YearMonth'] = df['ArrivalDate'].map(lambda x: 1000*x.year + x.month)\n"", ""import calendar\nimport datetime\ndf['AdjustedDateToEndOfMonth'] = df['ArrivalDate'].map(\n    lambda x: datetime.datetime(\n        x.year,\n        x.month,\n        max(calendar.monthcalendar(x.year, x.month)[-1][:5])\n    )\n)\n"", ""In [5]: df\nOut[5]: \n            date_time\n0 2014-10-17 22:00:03\n\nIn [6]: df.date_time\nOut[6]: \n0   2014-10-17 22:00:03\nName: date_time, dtype: datetime64[ns]\n\nIn [7]: df.date_time.map(lambda x: x.strftime('%Y-%m-%d'))\nOut[7]: \n0    2014-10-17\nName: date_time, dtype: object\n""]";"['year', 'month', 'datetime.datetime', ""In [15]: t = pandas.tslib.Timestamp.now()\n\nIn [16]: t\nOut[16]: Timestamp('2014-08-05 14:49:39.643701', tz=None)\n\nIn [17]: t.to_datetime()\nOut[17]: datetime.datetime(2014, 8, 5, 14, 49, 39, 643701)\n\nIn [18]: t.day\nOut[18]: 5\n\nIn [19]: t.month\nOut[19]: 8\n\nIn [20]: t.year\nOut[20]: 2014\n"", '201408', ""df['YearMonth'] = df['ArrivalDate'].map(lambda x: 1000*x.year + x.month)\n"", 'calendar', ""import calendar\nimport datetime\ndf['AdjustedDateToEndOfMonth'] = df['ArrivalDate'].map(\n    lambda x: datetime.datetime(\n        x.year,\n        x.month,\n        max(calendar.monthcalendar(x.year, x.month)[-1][:5])\n    )\n)\n"", 'strftime', 'datetime.datetime', ""In [5]: df\nOut[5]: \n            date_time\n0 2014-10-17 22:00:03\n\nIn [6]: df.date_time\nOut[6]: \n0   2014-10-17 22:00:03\nName: date_time, dtype: datetime64[ns]\n\nIn [7]: df.date_time.map(lambda x: x.strftime('%Y-%m-%d'))\nOut[7]: \n0    2014-10-17\nName: date_time, dtype: object\n""]"
1142;;3;25149272;25146121.0;2;96;;;"<p>If you want new columns showing year and month separately you can do this:</p>

<pre><code>df['year'] = pd.DatetimeIndex(df['ArrivalDate']).year
df['month'] = pd.DatetimeIndex(df['ArrivalDate']).month
</code></pre>

<p>or...</p>

<pre><code>df['year'] = df['ArrivalDate'].dt.year
df['month'] = df['ArrivalDate'].dt.month
</code></pre>

<p>Then you can combine them or work with them just as they are.</p>
";;"[""df['year'] = pd.DatetimeIndex(df['ArrivalDate']).year\ndf['month'] = pd.DatetimeIndex(df['ArrivalDate']).month\n"", ""df['year'] = df['ArrivalDate'].dt.year\ndf['month'] = df['ArrivalDate'].dt.month\n""]";"[""df['year'] = pd.DatetimeIndex(df['ArrivalDate']).year\ndf['month'] = pd.DatetimeIndex(df['ArrivalDate']).month\n"", ""df['year'] = df['ArrivalDate'].dt.year\ndf['month'] = df['ArrivalDate'].dt.month\n""]"
1143;;5;25208947;18889588.0;2;37;;;"<p>I know it's been a while since this question was asked, but there is (at least <em>now</em> there is) a one-liner that is supported by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.get_dummies.html"" rel=""noreferrer"">the documentation</a>:</p>

<pre><code>In [4]: df
Out[4]:
      label
0  (a, c, e)
1     (a, d)
2       (b,)
3     (d, e)

In [5]: df['label'].str.join(sep='*').str.get_dummies(sep='*')
Out[5]:
   a  b  c  d  e
0  1  0  1  0  1
1  1  0  0  1  0
2  0  1  0  0  0
3  0  0  0  1  1
</code></pre>
";;"[""In [4]: df\nOut[4]:\n      label\n0  (a, c, e)\n1     (a, d)\n2       (b,)\n3     (d, e)\n\nIn [5]: df['label'].str.join(sep='*').str.get_dummies(sep='*')\nOut[5]:\n   a  b  c  d  e\n0  1  0  1  0  1\n1  1  0  0  1  0\n2  0  1  0  0  0\n3  0  0  0  1  1\n""]";"[""In [4]: df\nOut[4]:\n      label\n0  (a, c, e)\n1     (a, d)\n2       (b,)\n3     (d, e)\n\nIn [5]: df['label'].str.join(sep='*').str.get_dummies(sep='*')\nOut[5]:\n   a  b  c  d  e\n0  1  0  1  0  1\n1  1  0  0  1  0\n2  0  1  0  0  0\n3  0  0  0  1  1\n""]"
1144;;1;25213438;25212986.0;2;35;;;"<p>You need to get hold of the axes themselves. Probably the cleanest way is to change your last row:</p>

<pre><code>lm = sns.lmplot('X','Y',df,col='Z',sharex=False,sharey=False)
</code></pre>

<p>Then you can get hold of the axes objects (an array of axes):</p>

<pre><code>axes = lm.axes
</code></pre>

<p>After that you can tweak the axes properties</p>

<pre><code>axes[0,0].set_ylim(0,)
axes[0,1].set_ylim(0,)
</code></pre>

<p>creates:</p>

<p><img src=""https://i.stack.imgur.com/Cgfsu.png"" alt=""enter image description here""></p>
";;"[""lm = sns.lmplot('X','Y',df,col='Z',sharex=False,sharey=False)\n"", 'axes = lm.axes\n', 'axes[0,0].set_ylim(0,)\naxes[0,1].set_ylim(0,)\n']";"[""lm = sns.lmplot('X','Y',df,col='Z',sharex=False,sharey=False)\n"", 'axes = lm.axes\n', 'axes[0,0].set_ylim(0,)\naxes[0,1].set_ylim(0,)\n']"
1145;;2;25213614;25212986.0;2;48;;;"<p>The <code>lmplot</code> function returns a <code>FacetGrid</code> instance. This object has a method called <code>set</code>, to which you can pass <code>key=value</code> pairs and they will be set on each Axes object in the grid.</p>

<p>Secondly, you can set only one side of an Axes limit in matplotlib by passing <code>None</code> for the value you want to remain as the default.</p>

<p>Putting these together, we have:</p>

<pre><code>g = sns.lmplot('X', 'Y', df, col='Z', sharex=False, sharey=False)
g.set(ylim=(0, None))
</code></pre>

<p><img src=""https://i.stack.imgur.com/45rjT.png"" alt=""enter image description here""></p>
";;"[""g = sns.lmplot('X', 'Y', df, col='Z', sharex=False, sharey=False)\ng.set(ylim=(0, None))\n""]";"['lmplot', 'FacetGrid', 'set', 'key=value', 'None', ""g = sns.lmplot('X', 'Y', df, col='Z', sharex=False, sharey=False)\ng.set(ylim=(0, None))\n""]"
1146;;1;25217425;19736080.0;2;28;;;"<p>Here's a simple way to do that:</p>

<pre><code>In[20]: my_dict = dict( A = np.array([1,2]), B = np.array([1,2,3,4]) )
In[21]: df = pd.DataFrame.from_dict(my_dict, orient='index')
In[22]: df
Out[22]: 
   0  1   2   3
A  1  2 NaN NaN
B  1  2   3   4
In[23]: df.transpose()
Out[23]: 
    A  B
0   1  1
1   2  2
2 NaN  3
3 NaN  4
</code></pre>
";;"[""In[20]: my_dict = dict( A = np.array([1,2]), B = np.array([1,2,3,4]) )\nIn[21]: df = pd.DataFrame.from_dict(my_dict, orient='index')\nIn[22]: df\nOut[22]: \n   0  1   2   3\nA  1  2 NaN NaN\nB  1  2   3   4\nIn[23]: df.transpose()\nOut[23]: \n    A  B\n0   1  1\n1   2  2\n2 NaN  3\n3 NaN  4\n""]";"[""In[20]: my_dict = dict( A = np.array([1,2]), B = np.array([1,2,3,4]) )\nIn[21]: df = pd.DataFrame.from_dict(my_dict, orient='index')\nIn[22]: df\nOut[22]: \n   0  1   2   3\nA  1  2 NaN NaN\nB  1  2   3   4\nIn[23]: df.transpose()\nOut[23]: \n    A  B\n0   1  1\n1   2  2\n2 NaN  3\n3 NaN  4\n""]"
1147;;0;25230582;20845213.0;2;164;;;"<p>Use <code>index=False</code>.</p>

<pre><code>pd.to_csv('your.csv', index=False)
</code></pre>
";;"[""pd.to_csv('your.csv', index=False)\n""]";"['index=False', ""pd.to_csv('your.csv', index=False)\n""]"
1148;;0;25254087;25254016.0;2;115;;;"<p>To select the <code>ith</code> row, <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#different-choices-for-indexing-loc-iloc-and-ix"">use <code>iloc</code></a>:</p>

<pre><code>In [31]: df_test.iloc[0]
Out[31]: 
ATime     1.2
X         2.0
Y        15.0
Z         2.0
Btime     1.2
C        12.0
D        25.0
E        12.0
Name: 0, dtype: float64
</code></pre>

<p>To select the ith value in the <code>Btime</code> column you could use:</p>

<pre><code>In [30]: df_test['Btime'].iloc[0]
Out[30]: 1.2
</code></pre>

<hr>

<p><strong>Warning</strong>: I had previously suggested <code>df_test.ix[i, 'Btime']</code>. But this is not guaranteed to give you the <code>ith</code> value since <code>ix</code> tries to index by <em>label</em> before trying to index by <em>position</em>. So if the DataFrame has an integer index which is not in sorted order starting at 0, then using <code>ix[i]</code> will return the row <em>labeled</em> <code>i</code> rather than the <code>ith</code> row. For example,</p>

<pre><code>In [1]: df = pd.DataFrame({'foo':list('ABC')}, index=[0,2,1])

In [2]: df
Out[2]: 
  foo
0   A
2   B
1   C

In [4]: df.ix[1, 'foo']
Out[4]: 'C'
</code></pre>
";;"['In [31]: df_test.iloc[0]\nOut[31]: \nATime     1.2\nX         2.0\nY        15.0\nZ         2.0\nBtime     1.2\nC        12.0\nD        25.0\nE        12.0\nName: 0, dtype: float64\n', ""In [30]: df_test['Btime'].iloc[0]\nOut[30]: 1.2\n"", ""In [1]: df = pd.DataFrame({'foo':list('ABC')}, index=[0,2,1])\n\nIn [2]: df\nOut[2]: \n  foo\n0   A\n2   B\n1   C\n\nIn [4]: df.ix[1, 'foo']\nOut[4]: 'C'\n""]";"['ith', 'iloc', 'In [31]: df_test.iloc[0]\nOut[31]: \nATime     1.2\nX         2.0\nY        15.0\nZ         2.0\nBtime     1.2\nC        12.0\nD        25.0\nE        12.0\nName: 0, dtype: float64\n', 'Btime', ""In [30]: df_test['Btime'].iloc[0]\nOut[30]: 1.2\n"", ""df_test.ix[i, 'Btime']"", 'ith', 'ix', 'ix[i]', 'i', 'ith', ""In [1]: df = pd.DataFrame({'foo':list('ABC')}, index=[0,2,1])\n\nIn [2]: df\nOut[2]: \n  foo\n0   A\n2   B\n1   C\n\nIn [4]: df.ix[1, 'foo']\nOut[4]: 'C'\n""]"
1149;;3;25352191;25351968.0;2;43;;;"<p>Set the <code>display.max_colwidth</code> option to <code>-1</code>:</p>

<pre><code>pd.set_option('display.max_colwidth', -1)
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.html#pandas-set-option""><code>set_option</code> docs</a></p>
";;"[""pd.set_option('display.max_colwidth', -1)\n""]";"['display.max_colwidth', '-1', ""pd.set_option('display.max_colwidth', -1)\n"", 'set_option']"
1150;;4;25376997;19365513.0;2;43;;;"<p>Try this:</p>

<pre><code>df.loc[len(df)]=['8/19/2014','Jun','Fly','98765'] 
</code></pre>
";;"[""df.loc[len(df)]=['8/19/2014','Jun','Fly','98765'] \n""]";"[""df.loc[len(df)]=['8/19/2014','Jun','Fly','98765'] \n""]"
1151;;2;25401328;25386870.0;2;6;;;"<p>Using the following DataFrame ...</p>

<p><img src=""https://i.stack.imgur.com/psVUU.png"" alt=""DataFrame""></p>

<pre><code># using pandas version 0.14.1
from pandas import DataFrame
import pandas as pd
import matplotlib.pyplot as plt

data = {'ColB': {('A', 4): 3.0,
('C', 2): 0.0,
('B', 4): 51.0,
('B', 1): 0.0,
('C', 3): 0.0,
('B', 2): 7.0,
('Code', 'Month'): '',
('A', 3): 5.0,
('C', 1): 0.0,
('C', 4): 0.0,
('B', 3): 12.0},
'ColA': {('A', 4): 66.0,
('C', 2): 5.0,
('B', 4): 125.0,
('B', 1): 5.0,
('C', 3): 41.0,
('B', 2): 52.0,
('Code', 'Month'): '',
('A', 3): 22.0,
('C', 1): 14.0,
('C', 4): 51.0,
('B', 3): 122.0}}

df = DataFrame(data)
</code></pre>

<p>... you can plot the following (using cross-section):</p>

<pre><code>f, a = plt.subplots(3,1)
df.xs('A').plot(kind='bar',ax=a[0])
df.xs('B').plot(kind='bar',ax=a[1])
df.xs('C').plot(kind='bar',ax=a[2])
</code></pre>

<p><img src=""https://i.stack.imgur.com/sIbtl.png"" alt=""enter image description here""></p>

<p>One for A, one for B and one for C, x-axis: 'Month', the bars are ColA and ColB.
Maybe this is what you are looking for.</p>
";;"[""# using pandas version 0.14.1\nfrom pandas import DataFrame\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = {'ColB': {('A', 4): 3.0,\n('C', 2): 0.0,\n('B', 4): 51.0,\n('B', 1): 0.0,\n('C', 3): 0.0,\n('B', 2): 7.0,\n('Code', 'Month'): '',\n('A', 3): 5.0,\n('C', 1): 0.0,\n('C', 4): 0.0,\n('B', 3): 12.0},\n'ColA': {('A', 4): 66.0,\n('C', 2): 5.0,\n('B', 4): 125.0,\n('B', 1): 5.0,\n('C', 3): 41.0,\n('B', 2): 52.0,\n('Code', 'Month'): '',\n('A', 3): 22.0,\n('C', 1): 14.0,\n('C', 4): 51.0,\n('B', 3): 122.0}}\n\ndf = DataFrame(data)\n"", ""f, a = plt.subplots(3,1)\ndf.xs('A').plot(kind='bar',ax=a[0])\ndf.xs('B').plot(kind='bar',ax=a[1])\ndf.xs('C').plot(kind='bar',ax=a[2])\n""]";"[""# using pandas version 0.14.1\nfrom pandas import DataFrame\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = {'ColB': {('A', 4): 3.0,\n('C', 2): 0.0,\n('B', 4): 51.0,\n('B', 1): 0.0,\n('C', 3): 0.0,\n('B', 2): 7.0,\n('Code', 'Month'): '',\n('A', 3): 5.0,\n('C', 1): 0.0,\n('C', 4): 0.0,\n('B', 3): 12.0},\n'ColA': {('A', 4): 66.0,\n('C', 2): 5.0,\n('B', 4): 125.0,\n('B', 1): 5.0,\n('C', 3): 41.0,\n('B', 2): 52.0,\n('Code', 'Month'): '',\n('A', 3): 22.0,\n('C', 1): 14.0,\n('C', 4): 51.0,\n('B', 3): 122.0}}\n\ndf = DataFrame(data)\n"", ""f, a = plt.subplots(3,1)\ndf.xs('A').plot(kind='bar',ax=a[0])\ndf.xs('B').plot(kind='bar',ax=a[1])\ndf.xs('C').plot(kind='bar',ax=a[2])\n""]"
1152;;1;25412939;25386870.0;2;43;;;"<p>I found the unstack(level) method to work perfectly, which has the added benefit of not needing a priori knowledge about how many Codes there are.</p>

<pre><code>summed_group.unstack(level=0).plot(kind='bar', subplots=True)
</code></pre>
";;"[""summed_group.unstack(level=0).plot(kind='bar', subplots=True)\n""]";"[""summed_group.unstack(level=0).plot(kind='bar', subplots=True)\n""]"
1153;;1;25415404;11707586.0;2;59;;;"<p>Try this:</p>

<pre><code>pd.set_option('display.expand_frame_repr', False)
</code></pre>

<p>From the documentation:</p>

<blockquote>
  <p>display.expand_frame_repr : boolean</p>
  
  <p>Whether to print out the full DataFrame repr for wide DataFrames across multiple lines, max_columns is still respected, but the output will wrap-around across multiple pages if its width exceeds display.width. [default: True] [currently: True]</p>
</blockquote>

<p>See: <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.html</a></p>
";;"[""pd.set_option('display.expand_frame_repr', False)\n""]";"[""pd.set_option('display.expand_frame_repr', False)\n""]"
1154;;6;25493765;25493625.0;2;39;;;"<p>Perform a <code>left</code> merge, this will use <code>sku</code> column as the column to join on:</p>

<pre><code>In [26]:

df.merge(df1, on='sku', how='left')
Out[26]:
   sku  loc   flag dept
0  122   61   True    b
1  122   62   True    b
2  122   63  False    b
3  123   61   True    b
4  123   62  False    b
5  113   62   True    a
6  301   63   True    c
</code></pre>

<p>If <code>sku</code> is in fact your index then do this:</p>

<pre><code>In [28]:

df.merge(df1, left_index=True, right_index=True, how='left')
Out[28]:
     loc   flag dept
sku                 
113   62   True    a
122   61   True    b
122   62   True    b
122   63  False    b
123   61   True    b
123   62  False    b
301   63   True    c
</code></pre>

<p>Another method is to use <code>map</code>, if you set <code>sku</code> as the index on your second df, so in effect it becomes a Series then the code simplifies to this:</p>

<pre><code>In [19]:

df['dept']=df.sku.map(df1.dept)
df
Out[19]:
   sku  loc   flag dept
0  122   61   True    b
1  123   61   True    b
2  113   62   True    a
3  122   62   True    b
4  123   62  False    b
5  122   63  False    b
6  301   63   True    c
</code></pre>
";;"[""In [26]:\n\ndf.merge(df1, on='sku', how='left')\nOut[26]:\n   sku  loc   flag dept\n0  122   61   True    b\n1  122   62   True    b\n2  122   63  False    b\n3  123   61   True    b\n4  123   62  False    b\n5  113   62   True    a\n6  301   63   True    c\n"", ""In [28]:\n\ndf.merge(df1, left_index=True, right_index=True, how='left')\nOut[28]:\n     loc   flag dept\nsku                 \n113   62   True    a\n122   61   True    b\n122   62   True    b\n122   63  False    b\n123   61   True    b\n123   62  False    b\n301   63   True    c\n"", ""In [19]:\n\ndf['dept']=df.sku.map(df1.dept)\ndf\nOut[19]:\n   sku  loc   flag dept\n0  122   61   True    b\n1  123   61   True    b\n2  113   62   True    a\n3  122   62   True    b\n4  123   62  False    b\n5  122   63  False    b\n6  301   63   True    c\n""]";"['left', 'sku', ""In [26]:\n\ndf.merge(df1, on='sku', how='left')\nOut[26]:\n   sku  loc   flag dept\n0  122   61   True    b\n1  122   62   True    b\n2  122   63  False    b\n3  123   61   True    b\n4  123   62  False    b\n5  113   62   True    a\n6  301   63   True    c\n"", 'sku', ""In [28]:\n\ndf.merge(df1, left_index=True, right_index=True, how='left')\nOut[28]:\n     loc   flag dept\nsku                 \n113   62   True    a\n122   61   True    b\n122   62   True    b\n122   63  False    b\n123   61   True    b\n123   62  False    b\n301   63   True    c\n"", 'map', 'sku', ""In [19]:\n\ndf['dept']=df.sku.map(df1.dept)\ndf\nOut[19]:\n   sku  loc   flag dept\n0  122   61   True    b\n1  123   61   True    b\n2  113   62   True    a\n3  122   62   True    b\n4  123   62  False    b\n5  122   63  False    b\n6  301   63   True    c\n""]"
1155;;0;25535803;13148429.0;2;6;;;"<p>I ran into a similar question myself, and just wanted to add what I settled on. I liked the reindex_axis( ) method for changing column order. This worked:</p>

<p><code>df = df.reindex_axis(['mean'] + list(df.columns[:-1]), axis=1)</code></p>
";;[];"[""df = df.reindex_axis(['mean'] + list(df.columns[:-1]), axis=1)""]"
1156;;4;25562948;25239958.0;2;40;;;"<p>To use mean values for numeric columns and the most frequent value for non-numeric columns you could do something like this. You could further distinguish between integers and floats. I guess it might make sense to use the median for integer columns instead.</p>

<pre><code>import pandas as pd
import numpy as np

from sklearn.base import TransformerMixin

class DataFrameImputer(TransformerMixin):

    def __init__(self):
        """"""Impute missing values.

        Columns of dtype object are imputed with the most frequent value 
        in column.

        Columns of other types are imputed with mean of column.

        """"""
    def fit(self, X, y=None):

        self.fill = pd.Series([X[c].value_counts().index[0]
            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],
            index=X.columns)

        return self

    def transform(self, X, y=None):
        return X.fillna(self.fill)

data = [
    ['a', 1, 2],
    ['b', 1, 1],
    ['b', 2, 2],
    [np.nan, np.nan, np.nan]
]

X = pd.DataFrame(data)
xt = DataFrameImputer().fit_transform(X)

print('before...')
print(X)
print('after...')
print(xt)
</code></pre>

<p>which prints,</p>

<pre><code>before...
     0   1   2
0    a   1   2
1    b   1   1
2    b   2   2
3  NaN NaN NaN
after...
   0         1         2
0  a  1.000000  2.000000
1  b  1.000000  1.000000
2  b  2.000000  2.000000
3  b  1.333333  1.666667
</code></pre>
";;"['import pandas as pd\nimport numpy as np\n\nfrom sklearn.base import TransformerMixin\n\nclass DataFrameImputer(TransformerMixin):\n\n    def __init__(self):\n        """"""Impute missing values.\n\n        Columns of dtype object are imputed with the most frequent value \n        in column.\n\n        Columns of other types are imputed with mean of column.\n\n        """"""\n    def fit(self, X, y=None):\n\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype(\'O\') else X[c].mean() for c in X],\n            index=X.columns)\n\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n\ndata = [\n    [\'a\', 1, 2],\n    [\'b\', 1, 1],\n    [\'b\', 2, 2],\n    [np.nan, np.nan, np.nan]\n]\n\nX = pd.DataFrame(data)\nxt = DataFrameImputer().fit_transform(X)\n\nprint(\'before...\')\nprint(X)\nprint(\'after...\')\nprint(xt)\n', 'before...\n     0   1   2\n0    a   1   2\n1    b   1   1\n2    b   2   2\n3  NaN NaN NaN\nafter...\n   0         1         2\n0  a  1.000000  2.000000\n1  b  1.000000  1.000000\n2  b  2.000000  2.000000\n3  b  1.333333  1.666667\n']";"['import pandas as pd\nimport numpy as np\n\nfrom sklearn.base import TransformerMixin\n\nclass DataFrameImputer(TransformerMixin):\n\n    def __init__(self):\n        """"""Impute missing values.\n\n        Columns of dtype object are imputed with the most frequent value \n        in column.\n\n        Columns of other types are imputed with mean of column.\n\n        """"""\n    def fit(self, X, y=None):\n\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype(\'O\') else X[c].mean() for c in X],\n            index=X.columns)\n\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n\ndata = [\n    [\'a\', 1, 2],\n    [\'b\', 1, 1],\n    [\'b\', 2, 2],\n    [np.nan, np.nan, np.nan]\n]\n\nX = pd.DataFrame(data)\nxt = DataFrameImputer().fit_transform(X)\n\nprint(\'before...\')\nprint(X)\nprint(\'after...\')\nprint(xt)\n', 'before...\n     0   1   2\n0    a   1   2\n1    b   1   1\n2    b   2   2\n3  NaN NaN NaN\nafter...\n   0         1         2\n0  a  1.000000  2.000000\n1  b  1.000000  1.000000\n2  b  2.000000  2.000000\n3  b  1.333333  1.666667\n']"
1157;;0;25574089;20868394.0;2;61;;;"<p>Since <code>inplace</code> argument is available, you don't need to copy and assign the original data frame back to itself, but do as follows:</p>

<pre><code>df.rename(columns={'two':'new_name'}, inplace=True)
</code></pre>
";;"[""df.rename(columns={'two':'new_name'}, inplace=True)\n""]";"['inplace', ""df.rename(columns={'two':'new_name'}, inplace=True)\n""]"
1158;;2;25588487;19726663.0;2;25;;;"<p>You have to use the figure returned by the <code>DataFrame.plot()</code> command:</p>

<pre><code>ax = df.plot()
fig = ax.get_figure()
fig.savefig('asdf.png')
</code></pre>
";;"[""ax = df.plot()\nfig = ax.get_figure()\nfig.savefig('asdf.png')\n""]";"['DataFrame.plot()', ""ax = df.plot()\nfig = ax.get_figure()\nfig.savefig('asdf.png')\n""]"
1159;;0;25643178;11285613.0;2;9;;;"<p>You could provide a list of columns to be dropped and return back the DataFrame with only the columns needed using the <code>drop()</code> function on a Pandas DataFrame.</p>

<p>Just saying</p>

<pre><code>colsToDrop = ['a']
df.drop(colsToDrop, axis=1)
</code></pre>

<p>would return a DataFrame with just the columns <code>b</code> and <code>c</code>.</p>

<p>The <code>drop</code> method is documented <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"" rel=""noreferrer"">here</a>.</p>
";;"[""colsToDrop = ['a']\ndf.drop(colsToDrop, axis=1)\n""]";"['drop()', ""colsToDrop = ['a']\ndf.drop(colsToDrop, axis=1)\n"", 'b', 'c', 'drop']"
1160;;1;25646414;25646200.0;2;35;;;"<p>You could do this, where <code>td</code> is your series of timedeltas.  The division converts the nanosecond deltas into day deltas, and the conversion to int drops to whole days.</p>

<pre><code>import numpy as np

(td / np.timedelta64(1, 'D')).astype(int)
</code></pre>
";;"[""import numpy as np\n\n(td / np.timedelta64(1, 'D')).astype(int)\n""]";"['td', ""import numpy as np\n\n(td / np.timedelta64(1, 'D')).astype(int)\n""]"
1161;;5;25715719;14688306.0;2;8;;;"<p>Just ran into this issue myself. As of pandas 0.13, DataFrames have a _metadata attribute on them that does persist through functions that return new DataFrames. Also seems to survive serialization just fine (I've only tried json, but I imagine hdf is covered as well).</p>
";;[];[]
1162;;0;25733562;20110170.0;2;82;;;"<p>The <em>reset_index()</em> is a pandas DataFrame method that will transfer index values into the DataFrame as columns.  The default setting for the parameter is <em>drop=False</em> (which will keep the index values as columns).</p>

<p>All you have to do add <code>.reset_index(inplace=True)</code> after the name of the DataFrame:</p>

<pre><code>df.reset_index(inplace=True)  
</code></pre>
";;['df.reset_index(inplace=True)  \n'];['.reset_index(inplace=True)', 'df.reset_index(inplace=True)  \n']
1163;;4;25748741;25748683.0;2;10;;;"<p>If you have a just a few columns to sum, you can write: </p>

<pre><code>df['e'] = df.a + df.b + df.d
</code></pre>

<p>This creates new column <code>e</code> with the values:</p>

<pre><code>   a  b   c  d   e
0  1  2  dd  5   8
1  2  3  ee  9  14
2  3  4  ff  1   8
</code></pre>

<p>For longer lists of columns, EdChum's answer is preferred.</p>
";;"[""df['e'] = df.a + df.b + df.d\n"", '   a  b   c  d   e\n0  1  2  dd  5   8\n1  2  3  ee  9  14\n2  3  4  ff  1   8\n']";"[""df['e'] = df.a + df.b + df.d\n"", 'e', '   a  b   c  d   e\n0  1  2  dd  5   8\n1  2  3  ee  9  14\n2  3  4  ff  1   8\n']"
1164;;4;25748826;25748683.0;2;71;;;"<p>You can just <code>sum</code> and set param <code>axis=1</code> to sum the rows, this will ignore none numeric columns:</p>

<pre><code>In [91]:

df = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})
df['e'] = df.sum(axis=1)
df
Out[91]:
   a  b   c  d   e
0  1  2  dd  5   8
1  2  3  ee  9  14
2  3  4  ff  1   8
</code></pre>

<p>If you want to just sum specific columns then you can create a list of the columns and remove the ones you are not interested in:</p>

<pre><code>In [98]:

col_list= list(df)
col_list.remove('d')
col_list
Out[98]:
['a', 'b', 'c']
In [99]:

df['e'] = df[col_list].sum(axis=1)
df
Out[99]:
   a  b   c  d  e
0  1  2  dd  5  3
1  2  3  ee  9  5
2  3  4  ff  1  7
</code></pre>
";;"[""In [91]:\n\ndf = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})\ndf['e'] = df.sum(axis=1)\ndf\nOut[91]:\n   a  b   c  d   e\n0  1  2  dd  5   8\n1  2  3  ee  9  14\n2  3  4  ff  1   8\n"", ""In [98]:\n\ncol_list= list(df)\ncol_list.remove('d')\ncol_list\nOut[98]:\n['a', 'b', 'c']\nIn [99]:\n\ndf['e'] = df[col_list].sum(axis=1)\ndf\nOut[99]:\n   a  b   c  d  e\n0  1  2  dd  5  3\n1  2  3  ee  9  5\n2  3  4  ff  1  7\n""]";"['sum', 'axis=1', ""In [91]:\n\ndf = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})\ndf['e'] = df.sum(axis=1)\ndf\nOut[91]:\n   a  b   c  d   e\n0  1  2  dd  5   8\n1  2  3  ee  9  14\n2  3  4  ff  1   8\n"", ""In [98]:\n\ncol_list= list(df)\ncol_list.remove('d')\ncol_list\nOut[98]:\n['a', 'b', 'c']\nIn [99]:\n\ndf['e'] = df[col_list].sum(axis=1)\ndf\nOut[99]:\n   a  b   c  d  e\n0  1  2  dd  5  3\n1  2  3  ee  9  5\n2  3  4  ff  1  7\n""]"
1165;;4;25774395;25773245.0;2;107;;;"<p>It's perhaps simplest to remember it as <em>0=down</em> and <em>1=across</em>. </p>

<p>This means:</p>

<ul>
<li>Use <code>axis=0</code> to apply a method down each column, or to the row labels (the index).</li>
<li>Use <code>axis=1</code> to apply a method across each row, or to the column labels.</li>
</ul>

<p>Here's a picture to show the parts of a DataFrame that each axis refers to:</p>

<p><img src=""https://i.stack.imgur.com/DL0iQ.jpg"" width=""410"" height=""210""></p>

<p>It's also useful to remember that Pandas follows NumPy's use of the word <code>axis</code>. The usage is explained in NumPy's <a href=""http://docs.scipy.org/doc/numpy/glossary.html"">glossary of terms</a>:</p>

<blockquote>
  <p>Axes are defined for arrays with more than one dimension. A 2-dimensional array has two corresponding axes: the first running vertically <strong>downwards across rows (axis 0)</strong>, and the second running <strong>horizontally across columns (axis 1)</strong>. [<em>my emphasis</em>] </p>
</blockquote>

<p>So, concerning the method in the question, <code>df.mean(axis=1)</code>, seems to be correctly defined. It takes the mean of entries <em>horizontally across columns</em>, that is, along each individual row. On the other hand, <code>df.mean(axis=0)</code> would be an operation acting vertically <em>downwards across rows</em>.</p>

<p>Similarly, <code>df.drop(name, axis=1)</code> refers to an action on column labels, because they intuitively go across the horizontal axis. Specifying <code>axis=0</code> would make the method act on rows instead.</p>
";;[];['axis=0', 'axis=1', 'axis', 'df.mean(axis=1)', 'df.mean(axis=0)', 'df.drop(name, axis=1)', 'axis=0']
1166;;2;25774932;10202570.0;2;11;;;"<p>Both above answers would only return one index if there are multiple rows that take the maximum value. If you want all the rows, there does not seem to have a function.
But it is not hard to do. Below is an example for Series; the same can be done for DataFrame:</p>

<pre><code>In [1]: from pandas import Series, DataFrame

In [2]: s=Series([2,4,4,3],index=['a','b','c','d'])

In [3]: s.idxmax()
Out[3]: 'b'

In [4]: s[s==s.max()]
Out[4]: 
b    4
c    4
dtype: int64
</code></pre>
";;"[""In [1]: from pandas import Series, DataFrame\n\nIn [2]: s=Series([2,4,4,3],index=['a','b','c','d'])\n\nIn [3]: s.idxmax()\nOut[3]: 'b'\n\nIn [4]: s[s==s.max()]\nOut[4]: \nb    4\nc    4\ndtype: int64\n""]";"[""In [1]: from pandas import Series, DataFrame\n\nIn [2]: s=Series([2,4,4,3],index=['a','b','c','d'])\n\nIn [3]: s.idxmax()\nOut[3]: 'b'\n\nIn [4]: s[s==s.max()]\nOut[4]: \nb    4\nc    4\ndtype: int64\n""]"
1167;;2;25799781;16074392.0;2;20;;;"<p><code>plt.gca().xaxis.grid(True)</code> proved to be the solution for me</p>
";;[];['plt.gca().xaxis.grid(True)']
1168;;0;25959539;15006298.0;2;26;;;"<pre><code># Say you have a df object containing your dataframe
df.head(5) # will print out the first 5 rows
df.tail(5) # will print out the 5 last rows
# Note: it is similar to R
</code></pre>
";;['# Say you have a df object containing your dataframe\ndf.head(5) # will print out the first 5 rows\ndf.tail(5) # will print out the 5 last rows\n# Note: it is similar to R\n'];['# Say you have a df object containing your dataframe\ndf.head(5) # will print out the first 5 rows\ndf.tail(5) # will print out the 5 last rows\n# Note: it is similar to R\n']
1169;;12;25962187;25962114.0;2;60;;;"<p>The error shows that the machine does not have enough memory to read the entire
CSV into a DataFrame at one time. Assuming you do not need the entire dataset in
memory all at one time, one way to avoid the problem would be to <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html"">process the CSV in
chunks</a> (by specifying the <code>chunksize</code> parameter):</p>

<pre><code>chunksize = 10 ** 6
for chunk in pd.read_csv(filename, chunksize=chunksize):
    process(chunk)
</code></pre>
";;['chunksize = 10 ** 6\nfor chunk in pd.read_csv(filename, chunksize=chunksize):\n    process(chunk)\n'];['chunksize', 'chunksize = 10 ** 6\nfor chunk in pd.read_csv(filename, chunksize=chunksize):\n    process(chunk)\n']
1170;;2;26017289;25962114.0;2;18;;;"<p>I proceeded like this:</p>

<pre><code>chunks=pd.read_table('aphro.csv',chunksize=1000000,sep=';',\
       names=['lat','long','rf','date','slno'],index_col='slno',\
       header=None,parse_dates=['date'])

df=pd.DataFrame()
%time df=pd.concat(chunk.groupby(['lat','long',chunk['date'].map(lambda x: x.year)])['rf'].agg(['sum']) for chunk in chunks)
</code></pre>
";;"[""chunks=pd.read_table('aphro.csv',chunksize=1000000,sep=';',\\\n       names=['lat','long','rf','date','slno'],index_col='slno',\\\n       header=None,parse_dates=['date'])\n\ndf=pd.DataFrame()\n%time df=pd.concat(chunk.groupby(['lat','long',chunk['date'].map(lambda x: x.year)])['rf'].agg(['sum']) for chunk in chunks)\n""]";"[""chunks=pd.read_table('aphro.csv',chunksize=1000000,sep=';',\\\n       names=['lat','long','rf','date','slno'],index_col='slno',\\\n       header=None,parse_dates=['date'])\n\ndf=pd.DataFrame()\n%time df=pd.concat(chunk.groupby(['lat','long',chunk['date'].map(lambda x: x.year)])['rf'].agg(['sum']) for chunk in chunks)\n""]"
1171;;5;26139658;26139423.0;2;58;;;"<p>You can pass <code>plt.scatter</code> a <code>c</code> argument which will allow you to select the colors. The code below defines a <code>colors</code> dictionary to map your diamond colors to the plotting colors.</p>

<pre><code>import matplotlib.pyplot as plt
import pandas as pd

carat = [5, 10, 20, 30, 5, 10, 20, 30, 5, 10, 20, 30]
price = [100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 600, 600]
color =['D', 'D', 'D', 'E', 'E', 'E', 'F', 'F', 'F', 'G', 'G', 'G',]

df = pd.DataFrame(dict(carat=carat, price=price, color=color))

fig, ax = plt.subplots()

colors = {'D':'red', 'E':'blue', 'F':'green', 'G':'black'}

ax.scatter(df['carat'], df['price'], c=df['color'].apply(lambda x: colors[x]))

plt.show()
</code></pre>

<p><code>df['color'].apply(lambda x: colors[x])</code> effectively maps the colours from ""diamond"" to ""plotting"".</p>

<p><em>(Forgive me for not putting another example image up, I think 2 is enough :P)</em></p>

<h2>With <code>seaborn</code></h2>

<p>You can use <code>seaborn</code> which is a wrapper around <code>matplotlib</code> that makes it look prettier by default (rather opinion-based, I know :P) but also adds some plotting functions.</p>

<p>For this you could use <a href=""http://web.stanford.edu/~mwaskom/software/seaborn/generated/seaborn.lmplot.html#seaborn-lmplot""><code>seaborn.lmplot</code></a> with <code>fit_reg=False</code> (which prevents it from automatically doing some regression).</p>

<p>The code below uses an example dataset. By selecting <code>hue='color'</code> you tell seaborn to split your dataframe up based on your colours and then plot each one.</p>

<pre><code>import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd

carat = [5, 10, 20, 30, 5, 10, 20, 30, 5, 10, 20, 30]
price = [100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 600, 600]
color =['D', 'D', 'D', 'E', 'E', 'E', 'F', 'F', 'F', 'G', 'G', 'G',]

df = pd.DataFrame(dict(carat=carat, price=price, color=color))

sns.lmplot('carat', 'price', data=df, hue='color', fit_reg=False)

plt.show()
</code></pre>

<p><img src=""https://i.stack.imgur.com/vOut1.png"" alt=""enter image description here""></p>

<h2>Without <code>seaborn</code> using <code>pandas.groupby</code></h2>

<p>If you don't want to use seaborn then you can use <code>pandas.groupby</code> to get the colors alone and then plot them using just matplotlib, but you'll have to manually assign colors as you go, I've added an example below:</p>

<pre><code>fig, ax = plt.subplots()

colors = {'D':'red', 'E':'blue', 'F':'green', 'G':'black'}

grouped = df.groupby('color')
for key, group in grouped:
    group.plot(ax=ax, kind='scatter', x='carat', y='price', label=key, color=colors[key])

plt.show()
</code></pre>

<p>This code assumes the same DataFrame as above and then groups it based on <code>color</code>. It then iterates over these groups, plotting for each one. To select a color I've created a <code>colors</code> dictionary which can map the diamond color (for instance <code>D</code>) to a real color (for instance <code>red</code>).</p>

<p><img src=""https://i.stack.imgur.com/mWjhz.png"" alt=""enter image description here""></p>
";;"[""import matplotlib.pyplot as plt\nimport pandas as pd\n\ncarat = [5, 10, 20, 30, 5, 10, 20, 30, 5, 10, 20, 30]\nprice = [100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 600, 600]\ncolor =['D', 'D', 'D', 'E', 'E', 'E', 'F', 'F', 'F', 'G', 'G', 'G',]\n\ndf = pd.DataFrame(dict(carat=carat, price=price, color=color))\n\nfig, ax = plt.subplots()\n\ncolors = {'D':'red', 'E':'blue', 'F':'green', 'G':'black'}\n\nax.scatter(df['carat'], df['price'], c=df['color'].apply(lambda x: colors[x]))\n\nplt.show()\n"", ""import matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pandas as pd\n\ncarat = [5, 10, 20, 30, 5, 10, 20, 30, 5, 10, 20, 30]\nprice = [100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 600, 600]\ncolor =['D', 'D', 'D', 'E', 'E', 'E', 'F', 'F', 'F', 'G', 'G', 'G',]\n\ndf = pd.DataFrame(dict(carat=carat, price=price, color=color))\n\nsns.lmplot('carat', 'price', data=df, hue='color', fit_reg=False)\n\nplt.show()\n"", ""fig, ax = plt.subplots()\n\ncolors = {'D':'red', 'E':'blue', 'F':'green', 'G':'black'}\n\ngrouped = df.groupby('color')\nfor key, group in grouped:\n    group.plot(ax=ax, kind='scatter', x='carat', y='price', label=key, color=colors[key])\n\nplt.show()\n""]";"['plt.scatter', 'c', 'colors', ""import matplotlib.pyplot as plt\nimport pandas as pd\n\ncarat = [5, 10, 20, 30, 5, 10, 20, 30, 5, 10, 20, 30]\nprice = [100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 600, 600]\ncolor =['D', 'D', 'D', 'E', 'E', 'E', 'F', 'F', 'F', 'G', 'G', 'G',]\n\ndf = pd.DataFrame(dict(carat=carat, price=price, color=color))\n\nfig, ax = plt.subplots()\n\ncolors = {'D':'red', 'E':'blue', 'F':'green', 'G':'black'}\n\nax.scatter(df['carat'], df['price'], c=df['color'].apply(lambda x: colors[x]))\n\nplt.show()\n"", ""df['color'].apply(lambda x: colors[x])"", 'seaborn', 'seaborn', 'matplotlib', 'seaborn.lmplot', 'fit_reg=False', ""hue='color'"", ""import matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pandas as pd\n\ncarat = [5, 10, 20, 30, 5, 10, 20, 30, 5, 10, 20, 30]\nprice = [100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 600, 600]\ncolor =['D', 'D', 'D', 'E', 'E', 'E', 'F', 'F', 'F', 'G', 'G', 'G',]\n\ndf = pd.DataFrame(dict(carat=carat, price=price, color=color))\n\nsns.lmplot('carat', 'price', data=df, hue='color', fit_reg=False)\n\nplt.show()\n"", 'seaborn', 'pandas.groupby', 'pandas.groupby', ""fig, ax = plt.subplots()\n\ncolors = {'D':'red', 'E':'blue', 'F':'green', 'G':'black'}\n\ngrouped = df.groupby('color')\nfor key, group in grouped:\n    group.plot(ax=ax, kind='scatter', x='carat', y='price', label=key, color=colors[key])\n\nplt.show()\n"", 'color', 'colors', 'D', 'red']"
1172;;3;26240208;26047209.0;2;56;;;"<p>Quoting the <a href=""http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.DataFrame.html"">Pandas docs</a></p>

<blockquote>
  <p>pandas.<strong>DataFrame</strong>(<em>data=None, index=None, columns=None, dtype=None, copy=False</em>)</p>
  
  <p>Two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes 
  (rows and columns). Arithmetic operations align on both row and column labels. 
  <strong>Can be thought of as a dict-like container for Series objects.</strong> The primary pandas data structure</p>
</blockquote>

<p>(Emphasis mine, sentence fragment not mine)</p>

<p>So the Series is the datastructure for a single column of a <code>DataFrame</code>, not only conceptually, but literally i.e. the data in a <code>DataFrame</code> is actually stored in memory as a collection of <code>Series</code>. </p>

<p>Analogously: <em>We need both lists and matrices, because matrices are built with lists. Single row matricies, while equivalent to lists in functionality still cannot exists without the list(s) they're composed of.</em></p>

<p>They both have extremely similar APIs, but you'll find that <code>DataFrame</code> methods always cater to the possibility that you have more than one column. And of course, you can always add another <code>Series</code> (or equivalent object) to a <code>DataFrame</code>, while adding a <code>Series</code> to another <code>Series</code> involves creating a <code>DataFrame</code>.</p>
";;[];['DataFrame', 'DataFrame', 'Series', 'DataFrame', 'Series', 'DataFrame', 'Series', 'Series', 'DataFrame']
1173;;2;26266439;26266362.0;2;30;;;"<p>You could subtract the total length from the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.count.html"">count</a> of non-nan values:</p>

<pre><code>count_nan = len(df) - df.count()
</code></pre>

<p>You should time it on your data. For small Series got a 3x speed up in comparison with the <code>isnull</code> solution.</p>
";;['count_nan = len(df) - df.count()\n'];['count_nan = len(df) - df.count()\n', 'isnull']
1174;;4;26266451;26266362.0;2;191;;;"<p>You can use the <code>isnull()</code> method and then sum to count the nan values. For one column:</p>

<pre><code>In [1]: s = pd.Series([1,2,3, np.nan, np.nan])

In [4]: s.isnull().sum()
Out[4]: 2
</code></pre>

<p>For several columns, it also works:</p>

<pre><code>In [5]: df = pd.DataFrame({'a':[1,2,np.nan], 'b':[np.nan,1,np.nan]})

In [6]: df.isnull().sum()
Out[6]:
a    1
b    2
dtype: int64
</code></pre>
";;"['In [1]: s = pd.Series([1,2,3, np.nan, np.nan])\n\nIn [4]: s.isnull().sum()\nOut[4]: 2\n', ""In [5]: df = pd.DataFrame({'a':[1,2,np.nan], 'b':[np.nan,1,np.nan]})\n\nIn [6]: df.isnull().sum()\nOut[6]:\na    1\nb    2\ndtype: int64\n""]";"['isnull()', 'In [1]: s = pd.Series([1,2,3, np.nan, np.nan])\n\nIn [4]: s.isnull().sum()\nOut[4]: 2\n', ""In [5]: df = pd.DataFrame({'a':[1,2,np.nan], 'b':[np.nan,1,np.nan]})\n\nIn [6]: df.isnull().sum()\nOut[6]:\na    1\nb    2\ndtype: int64\n""]"
1175;;0;26272425;26266362.0;2;15;;;"<p>Since pandas 0.14.1 my suggestion <a href=""https://github.com/pydata/pandas/issues/5569"">here</a> to have a keyword argument in the value_counts method has been implemented:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'a':[1,2,np.nan], 'b':[np.nan,1,np.nan]})
for col in df:
    print df[col].value_counts(dropna=False)

2     1
 1     1
NaN    1
dtype: int64
NaN    2
 1     1
dtype: int64
</code></pre>
";;"[""import pandas as pd\ndf = pd.DataFrame({'a':[1,2,np.nan], 'b':[np.nan,1,np.nan]})\nfor col in df:\n    print df[col].value_counts(dropna=False)\n\n2     1\n 1     1\nNaN    1\ndtype: int64\nNaN    2\n 1     1\ndtype: int64\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'a':[1,2,np.nan], 'b':[np.nan,1,np.nan]})\nfor col in df:\n    print df[col].value_counts(dropna=False)\n\n2     1\n 1     1\nNaN    1\ndtype: int64\nNaN    2\n 1     1\ndtype: int64\n""]"
1176;;0;26286140;14262433.0;2;7;;;"<p>Consider <a href=""http://www.ruffus.org.uk/"" rel=""noreferrer"">Ruffus</a> if you go the simple path of creating a data pipeline which is broken down into multiple smaller files. </p>
";;[];[]
1177;;1;26301947;26277757.0;2;49;;;"<p>What you are seeing is pandas truncating the output for display purposes only. </p>

<p>The default <code>max_colwidth</code> value is 50 which is what you are seeing.</p>

<p>You can set this value to whatever you desire or you can set it to -1 which effectively turns this off:</p>

<pre><code>pd.set_option('display.max_colwidth', -1)
</code></pre>

<p>Although I would advise against this, it would be better to set it to something that can be displayed easily in your console or ipython.</p>

<p>A list of the options can be found here: <a href=""http://pandas.pydata.org/pandas-docs/stable/options.html"">http://pandas.pydata.org/pandas-docs/stable/options.html</a></p>
";;"[""pd.set_option('display.max_colwidth', -1)\n""]";"['max_colwidth', ""pd.set_option('display.max_colwidth', -1)\n""]"
1178;;1;26310294;26309962.0;2;14;;;"<p>Could you do something like this?</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame(columns=['col1', 'col2'])
&gt;&gt;&gt; df = df.append(pd.Series(['a', 'b'], index=['col1','col2']), ignore_index=True)
&gt;&gt;&gt; df = df.append(pd.Series(['d', 'e'], index=['col1','col2']), ignore_index=True) 
&gt;&gt;&gt; df
  col1 col2
0    a    b
1    d    e
</code></pre>

<p>Does anyone have a more elegant solution?</p>
";;"["">>> import pandas as pd\n>>> df = pd.DataFrame(columns=['col1', 'col2'])\n>>> df = df.append(pd.Series(['a', 'b'], index=['col1','col2']), ignore_index=True)\n>>> df = df.append(pd.Series(['d', 'e'], index=['col1','col2']), ignore_index=True) \n>>> df\n  col1 col2\n0    a    b\n1    d    e\n""]";"["">>> import pandas as pd\n>>> df = pd.DataFrame(columns=['col1', 'col2'])\n>>> df = df.append(pd.Series(['a', 'b'], index=['col1','col2']), ignore_index=True)\n>>> df = df.append(pd.Series(['d', 'e'], index=['col1','col2']), ignore_index=True) \n>>> df\n  col1 col2\n0    a    b\n1    d    e\n""]"
1179;;0;26311118;26309962.0;2;22;;;"<p>Here's a simple and dumb solution:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame()
&gt;&gt;&gt; df = df.append({'foo':1, 'bar':2}, ignore_index=True)
</code></pre>
";;"["">>> import pandas as pd\n>>> df = pd.DataFrame()\n>>> df = df.append({'foo':1, 'bar':2}, ignore_index=True)\n""]";"["">>> import pandas as pd\n>>> df = pd.DataFrame()\n>>> df = df.append({'foo':1, 'bar':2}, ignore_index=True)\n""]"
1180;;0;26320276;10665889.0;2;14;;;"<p>And if you came here looking for slicing two ranges of columns and combining them together (like me) you can do something like </p>

<pre><code>op = df[list(df.columns[0:899]) + list(df.columns[3593:])]
print op
</code></pre>

<p>This will create a new dataframe with first 900 columns and (all) columns > 3593 (assuming you have some 4000 columns in your data set). </p>
";;['op = df[list(df.columns[0:899]) + list(df.columns[3593:])]\nprint op\n'];['op = df[list(df.columns[0:899]) + list(df.columns[3593:])]\nprint op\n']
1181;;3;26347456;26347412.0;2;60;;;"<p>You don't need to wrap it in a list with <code>[..]</code>, just provide the subselection of the columns index:</p>

<pre><code>df.drop(df.columns[[1, 69]], axis=1, inplace=True)
</code></pre>

<p>as the index object is already regarded as list-like.</p>
";;['df.drop(df.columns[[1, 69]], axis=1, inplace=True)\n'];['[..]', 'df.drop(df.columns[[1, 69]], axis=1, inplace=True)\n']
1182;;0;26356675;14940743.0;2;9;;;"<p>Also have a look into the built-in <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.filter.html"" rel=""noreferrer""><code>DataFrame.filter</code></a> function.</p>

<p>Minimalistic but greedy approach (sufficient for the given df):
</p>

<pre><code>df.filter(regex=""[^BD]"")
</code></pre>

<p>Conservative/lazy approach (exact matches only):</p>

<pre class=""lang-py prettyprint-override""><code>df.filter(regex=""^(?!(B|D)$).*$"")
</code></pre>

<p>Conservative and generic:</p>

<pre class=""lang-py prettyprint-override""><code>exclude_cols = ['B','C']
df.filter(regex=""^(?!({0})$).*$"".format('|'.join(exclude_cols)))
</code></pre>
";;"['df.filter(regex=""[^BD]"")\n', 'df.filter(regex=""^(?!(B|D)$).*$"")\n', 'exclude_cols = [\'B\',\'C\']\ndf.filter(regex=""^(?!({0})$).*$"".format(\'|\'.join(exclude_cols)))\n']";"['DataFrame.filter', 'df.filter(regex=""[^BD]"")\n', 'df.filter(regex=""^(?!(B|D)$).*$"")\n', 'exclude_cols = [\'B\',\'C\']\ndf.filter(regex=""^(?!({0})$).*$"".format(\'|\'.join(exclude_cols)))\n']"
1183;;6;26474062;26473681.0;2;42;;;"<p>I had this exact problem recently and used</p>

<pre><code>apt-get install python-numpy
</code></pre>

<p>This adds numpy to your system python interpreter.  I may have had to do the same for matplotlib.  To use in a virtualenv, you have to create your environment using the</p>

<pre><code>--system-site-packages
</code></pre>

<p>option</p>

<p><a href=""http://www.scipy.org/install.html"">http://www.scipy.org/install.html</a></p>
";;['apt-get install python-numpy\n', '--system-site-packages\n'];['apt-get install python-numpy\n', '--system-site-packages\n']
1184;;0;26510251;26483254.0;2;23;;;"<p><code>df3.set_value(1, 'B', abc)</code> works for any dataframe. Take care of the data type of column 'B'. Eg. a list can not be inserted into a float column, at that case <code>df['B'] = df['B'].astype(object)</code> can help.</p>
";;[];"[""df3.set_value(1, 'B', abc)"", ""df['B'] = df['B'].astype(object)""]"
1185;;1;26599892;18039057.0;2;24;;;"<p>It might be an issue with </p>

<ul>
<li>the delimiters in your data</li>
<li>the first row, as @TomAugspurger assiduously noted</li>
</ul>

<p>To solve it, try specifying the <code>sep</code> and/or <code>header</code> arguments when calling <code>read_csv</code>. For instance, </p>

<pre><code>df = pandas.read_csv(fileName, sep='delimiter', header=None)
</code></pre>

<p>In the code above, <code>sep</code> defines your delimiter and <code>header=None</code> tells pandas that your source data has no row for headers / column titles. Thus saith <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html"">the docs</a>: ""If file contains no header row, then you should explicitly pass header=None"". In this instance, pandas automatically creates whole-number indeces for each field {0,1,2,...}. </p>

<p>According to the docs, the delimiter thing should <em>not</em> be an issue. The docs say that ""if sep is None [not specified], will try to automatically determine this."" I however have not had good luck with this, including instances with obvious delimiters. But, defining <code>sep</code> is not arduous. </p>
";;"[""df = pandas.read_csv(fileName, sep='delimiter', header=None)\n""]";"['sep', 'header', 'read_csv', ""df = pandas.read_csv(fileName, sep='delimiter', header=None)\n"", 'sep', 'header=None', 'sep']"
1186;;0;26640189;26640145.0;2;37;;;"<pre><code>df.index
</code></pre>

<p>will output the row names as pandas Index object.  For a pure list,</p>

<pre><code>list(df.index)
</code></pre>

<p>Lastly, the index supports label slicing similar to columns, </p>

<pre><code>df.index['Row 2':'Row 5'] 
</code></pre>
";;"['df.index\n', 'list(df.index)\n', ""df.index['Row 2':'Row 5'] \n""]";"['df.index\n', 'list(df.index)\n', ""df.index['Row 2':'Row 5'] \n""]"
1187;;4;26654201;26645515.0;2;42;;;"<p>Your error on the snippet of data you posted is a little cryptic, in that because there are no common values, the join operation fails because the values don't overlap it requires you to supply a suffix for the left and right hand side:</p>

<pre><code>In [173]:

df_a.join(df_b, on='mukey', how='left', lsuffix='_left', rsuffix='_right')
Out[173]:
       mukey_left  DI  PI  mukey_right  niccdcd
index                                          
0          100000  35  14          NaN      NaN
1         1000005  44  14          NaN      NaN
2         1000006  44  14          NaN      NaN
3         1000007  43  13          NaN      NaN
4         1000008  43  13          NaN      NaN
</code></pre>

<p><code>merge</code> works because it doesn't have this restriction:</p>

<pre><code>In [176]:

df_a.merge(df_b, on='mukey', how='left')
Out[176]:
     mukey  DI  PI  niccdcd
0   100000  35  14      NaN
1  1000005  44  14      NaN
2  1000006  44  14      NaN
3  1000007  43  13      NaN
4  1000008  43  13      NaN
</code></pre>
";;"[""In [173]:\n\ndf_a.join(df_b, on='mukey', how='left', lsuffix='_left', rsuffix='_right')\nOut[173]:\n       mukey_left  DI  PI  mukey_right  niccdcd\nindex                                          \n0          100000  35  14          NaN      NaN\n1         1000005  44  14          NaN      NaN\n2         1000006  44  14          NaN      NaN\n3         1000007  43  13          NaN      NaN\n4         1000008  43  13          NaN      NaN\n"", ""In [176]:\n\ndf_a.merge(df_b, on='mukey', how='left')\nOut[176]:\n     mukey  DI  PI  niccdcd\n0   100000  35  14      NaN\n1  1000005  44  14      NaN\n2  1000006  44  14      NaN\n3  1000007  43  13      NaN\n4  1000008  43  13      NaN\n""]";"[""In [173]:\n\ndf_a.join(df_b, on='mukey', how='left', lsuffix='_left', rsuffix='_right')\nOut[173]:\n       mukey_left  DI  PI  mukey_right  niccdcd\nindex                                          \n0          100000  35  14          NaN      NaN\n1         1000005  44  14          NaN      NaN\n2         1000006  44  14          NaN      NaN\n3         1000007  43  13          NaN      NaN\n4         1000008  43  13          NaN      NaN\n"", 'merge', ""In [176]:\n\ndf_a.merge(df_b, on='mukey', how='left')\nOut[176]:\n     mukey  DI  PI  niccdcd\n0   100000  35  14      NaN\n1  1000005  44  14      NaN\n2  1000006  44  14      NaN\n3  1000007  43  13      NaN\n4  1000008  43  13      NaN\n""]"
1188;;2;26658301;26658240.0;2;28;;;"<p>To access the index in this case you access the <code>name</code> attribute:</p>

<pre><code>In [182]:

df = pd.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])
def rowFunc(row):
    return row['a'] + row['b'] * row['c']

def rowIndex(row):
    return row.name
df['d'] = df.apply(rowFunc, axis=1)
df['rowIndex'] = df.apply(rowIndex, axis=1)
df
Out[182]:
   a  b  c   d  rowIndex
0  1  2  3   7         0
1  4  5  6  34         1
</code></pre>

<p>Note that if this is really what you are trying to do that the following works and is much faster:</p>

<pre><code>In [198]:

df['d'] = df['a'] + df['b'] * df['c']
df
Out[198]:
   a  b  c   d
0  1  2  3   7
1  4  5  6  34

In [199]:

%timeit df['a'] + df['b'] * df['c']
%timeit df.apply(rowIndex, axis=1)
10000 loops, best of 3: 163 s per loop
1000 loops, best of 3: 286 s per loop
</code></pre>
";;"[""In [182]:\n\ndf = pd.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])\ndef rowFunc(row):\n    return row['a'] + row['b'] * row['c']\n\ndef rowIndex(row):\n    return row.name\ndf['d'] = df.apply(rowFunc, axis=1)\ndf['rowIndex'] = df.apply(rowIndex, axis=1)\ndf\nOut[182]:\n   a  b  c   d  rowIndex\n0  1  2  3   7         0\n1  4  5  6  34         1\n"", ""In [198]:\n\ndf['d'] = df['a'] + df['b'] * df['c']\ndf\nOut[198]:\n   a  b  c   d\n0  1  2  3   7\n1  4  5  6  34\n\nIn [199]:\n\n%timeit df['a'] + df['b'] * df['c']\n%timeit df.apply(rowIndex, axis=1)\n10000 loops, best of 3: 163 s per loop\n1000 loops, best of 3: 286 s per loop\n""]";"['name', ""In [182]:\n\ndf = pd.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])\ndef rowFunc(row):\n    return row['a'] + row['b'] * row['c']\n\ndef rowIndex(row):\n    return row.name\ndf['d'] = df.apply(rowFunc, axis=1)\ndf['rowIndex'] = df.apply(rowIndex, axis=1)\ndf\nOut[182]:\n   a  b  c   d  rowIndex\n0  1  2  3   7         0\n1  4  5  6  34         1\n"", ""In [198]:\n\ndf['d'] = df['a'] + df['b'] * df['c']\ndf\nOut[198]:\n   a  b  c   d\n0  1  2  3   7\n1  4  5  6  34\n\nIn [199]:\n\n%timeit df['a'] + df['b'] * df['c']\n%timeit df.apply(rowIndex, axis=1)\n10000 loops, best of 3: 163 s per loop\n1000 loops, best of 3: 286 s per loop\n""]"
1189;;0;26716759;26716616.0;2;8;;;"<p>Try to use <code>Zip</code></p>

<pre><code>df = pd.read_csv(""file"")
d= dict([(i,[a,b,c ]) for i, a,b,c in zip(df.ID, df.A,df.B,df.C)])
print d
</code></pre>

<p>Output: </p>

<pre><code>{'p': [1, 3, 2], 'q': [4, 3, 2], 'r': [4, 0, 9]}
</code></pre>
";;"['df = pd.read_csv(""file"")\nd= dict([(i,[a,b,c ]) for i, a,b,c in zip(df.ID, df.A,df.B,df.C)])\nprint d\n', ""{'p': [1, 3, 2], 'q': [4, 3, 2], 'r': [4, 0, 9]}\n""]";"['Zip', 'df = pd.read_csv(""file"")\nd= dict([(i,[a,b,c ]) for i, a,b,c in zip(df.ID, df.A,df.B,df.C)])\nprint d\n', ""{'p': [1, 3, 2], 'q': [4, 3, 2], 'r': [4, 0, 9]}\n""]"
1190;;3;26716774;26716616.0;2;56;;;"<p>The <code>to_dict()</code> method sets the column names as dictionary keys so you'll need to reshape your DataFrame slightly. Setting the 'ID' column as the index and then transposing the DataFrame is one way to achieve this.</p>

<p><code>to_dict()</code> also accepts an <code>outtype</code> keyword argument which you'll need in order to output a list of values for each column. Otherwise, a dictionary of the form <code>{index:value}</code> will be returned for each column.</p>

<p>These steps can be done with the following line:</p>

<pre><code>&gt;&gt;&gt; df.set_index('ID').T.to_dict('list')
{'p': [1, 3, 2], 'q': [4, 3, 2], 'r': [4, 0, 9]}
</code></pre>
";;"["">>> df.set_index('ID').T.to_dict('list')\n{'p': [1, 3, 2], 'q': [4, 3, 2], 'r': [4, 0, 9]}\n""]";"['to_dict()', 'to_dict()', 'outtype', '{index:value}', "">>> df.set_index('ID').T.to_dict('list')\n{'p': [1, 3, 2], 'q': [4, 3, 2], 'r': [4, 0, 9]}\n""]"
1191;;4;26721808;23549231.0;2;15;;;"<p>Just for reference as it was something I was looking for, you can test for presence within the values or the index by appending the "".values"" method, e.g.</p>

<pre><code>g in df.&lt;your selected field&gt;.values
g in df.index.values
</code></pre>

<p>I find that adding the "".values"" to get a simple list or ndarray out makes exist or ""in"" checks run more smoothly with the other python tools. Just thought I'd toss that out there for people.</p>
";;['g in df.<your selected field>.values\ng in df.index.values\n'];['g in df.<your selected field>.values\ng in df.index.values\n']
1192;;0;26724725;12065885.0;2;71;;;"<p><code>isin()</code> is ideal if you have a list of exact matches, but if you have a list of partial matches or substrings to look for, you can filter using the <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.core.strings.StringMethods.contains.html""><code>str.contains</code></a> method and regular expressions.</p>

<p>For example, if we want to return a DataFrame where all of the stock IDs which begin with <code>'600'</code> and then are followed by any three digits:</p>

<pre><code>&gt;&gt;&gt; rpt[rpt['STK_ID'].str.contains(r'^600[0-9]{3}$')] # ^ means start of string
...   STK_ID   ...                                    # [0-9]{3} means any three digits
...  '600809'  ...                                    # $ means end of string
...  '600141'  ...
...  '600329'  ...
...      ...   ...
</code></pre>

<p>Suppose now we have a list of strings which we want the values in <code>'STK_ID'</code> to end with, e.g.</p>

<pre><code>endstrings = ['01$', '02$', '05$']
</code></pre>

<p>We can join these strings with the regex 'or' character <code>|</code> and pass the string to <code>str.contains</code> to filter the DataFrame: </p>

<pre><code>&gt;&gt;&gt; rpt[rpt['STK_ID'].str.contains('|'.join(endstrings)]
...   STK_ID   ...
...  '155905'  ...
...  '633101'  ...
...  '210302'  ...
...      ...   ...
</code></pre>

<p>Finally, <code>contains</code> can ignore case (by setting <code>case=False</code>), allowing you to be more general when specifying the strings you want to match.</p>

<p>For example,</p>

<pre><code>str.contains('pandas', case=False)
</code></pre>

<p>would match <code>PANDAS</code>, <code>PanDAs</code>, <code>paNdAs123</code>, and so on.</p>
";;"["">>> rpt[rpt['STK_ID'].str.contains(r'^600[0-9]{3}$')] # ^ means start of string\n...   STK_ID   ...                                    # [0-9]{3} means any three digits\n...  '600809'  ...                                    # $ means end of string\n...  '600141'  ...\n...  '600329'  ...\n...      ...   ...\n"", ""endstrings = ['01$', '02$', '05$']\n"", "">>> rpt[rpt['STK_ID'].str.contains('|'.join(endstrings)]\n...   STK_ID   ...\n...  '155905'  ...\n...  '633101'  ...\n...  '210302'  ...\n...      ...   ...\n"", ""str.contains('pandas', case=False)\n""]";"['isin()', 'str.contains', ""'600'"", "">>> rpt[rpt['STK_ID'].str.contains(r'^600[0-9]{3}$')] # ^ means start of string\n...   STK_ID   ...                                    # [0-9]{3} means any three digits\n...  '600809'  ...                                    # $ means end of string\n...  '600141'  ...\n...  '600329'  ...\n...      ...   ...\n"", ""'STK_ID'"", ""endstrings = ['01$', '02$', '05$']\n"", '|', 'str.contains', "">>> rpt[rpt['STK_ID'].str.contains('|'.join(endstrings)]\n...   STK_ID   ...\n...  '155905'  ...\n...  '633101'  ...\n...  '210302'  ...\n...      ...   ...\n"", 'contains', 'case=False', ""str.contains('pandas', case=False)\n"", 'PANDAS', 'PanDAs', 'paNdAs123']"
1193;;1;26763793;26763344.0;2;93;;;"<p>Use the <a href=""http://pandas.pydata.org/pandas-docs/dev/timeseries.html#converting-to-timestamps""><code>to_datetime</code></a> function, specifying a <a href=""http://strftime.org/"">format</a> to match your data.</p>

<pre><code>raw_data['Mycol'] =  pd.to_datetime(raw_data['Mycol'], format='%d%b%Y:%H:%M:%S.%f')
</code></pre>
";;"[""raw_data['Mycol'] =  pd.to_datetime(raw_data['Mycol'], format='%d%b%Y:%H:%M:%S.%f')\n""]";"['to_datetime', ""raw_data['Mycol'] =  pd.to_datetime(raw_data['Mycol'], format='%d%b%Y:%H:%M:%S.%f')\n""]"
1194;;2;26763810;26763344.0;2;19;;;"<p>You can use the DataFrame method <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.apply.html"" rel=""noreferrer""><code>.apply()</code></a> to operate on the values in Mycol:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(['05SEP2014:00:00:00.000'],columns=['Mycol'])
&gt;&gt;&gt; df
                    Mycol
0  05SEP2014:00:00:00.000
&gt;&gt;&gt; import datetime as dt
&gt;&gt;&gt; df['Mycol'] = df['Mycol'].apply(lambda x: 
                                    dt.datetime.strptime(x,'%d%b%Y:%H:%M:%S.%f'))
&gt;&gt;&gt; df
       Mycol
0 2014-09-05
</code></pre>
";;"["">>> df = pd.DataFrame(['05SEP2014:00:00:00.000'],columns=['Mycol'])\n>>> df\n                    Mycol\n0  05SEP2014:00:00:00.000\n>>> import datetime as dt\n>>> df['Mycol'] = df['Mycol'].apply(lambda x: \n                                    dt.datetime.strptime(x,'%d%b%Y:%H:%M:%S.%f'))\n>>> df\n       Mycol\n0 2014-09-05\n""]";"['.apply()', "">>> df = pd.DataFrame(['05SEP2014:00:00:00.000'],columns=['Mycol'])\n>>> df\n                    Mycol\n0  05SEP2014:00:00:00.000\n>>> import datetime as dt\n>>> df['Mycol'] = df['Mycol'].apply(lambda x: \n                                    dt.datetime.strptime(x,'%d%b%Y:%H:%M:%S.%f'))\n>>> df\n       Mycol\n0 2014-09-05\n""]"
1195;;3;26787032;26786960.0;2;42;;;"<p>What you are seeing is the index column. Just set <code>index=False</code>:</p>

<pre><code>df_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w', index=False)
</code></pre>
";;"[""df_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w', index=False)\n""]";"['index=False', ""df_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w', index=False)\n""]"
1196;;2;26803731;13575090.0;2;7;;;"<p>So I used to use a for loop for iterating through the dictionary as well, but one thing I've found that works much faster is to convert to a panel and then to a dataframe. 
Say you have a dictionary d</p>

<pre><code>import pandas as pd
d
{'RAY Index': {datetime.date(2014, 11, 3): {'PX_LAST': 1199.46,
'PX_OPEN': 1200.14},
datetime.date(2014, 11, 4): {'PX_LAST': 1195.323, 'PX_OPEN': 1197.69},
datetime.date(2014, 11, 5): {'PX_LAST': 1200.936, 'PX_OPEN': 1195.32},
datetime.date(2014, 11, 6): {'PX_LAST': 1206.061, 'PX_OPEN': 1200.62}},
'SPX Index': {datetime.date(2014, 11, 3): {'PX_LAST': 2017.81,
'PX_OPEN': 2018.21},
datetime.date(2014, 11, 4): {'PX_LAST': 2012.1, 'PX_OPEN': 2015.81},
datetime.date(2014, 11, 5): {'PX_LAST': 2023.57, 'PX_OPEN': 2015.29},
datetime.date(2014, 11, 6): {'PX_LAST': 2031.21, 'PX_OPEN': 2023.33}}}
</code></pre>

<p>The command</p>

<pre><code>pd.Panel(d)
&lt;class 'pandas.core.panel.Panel'&gt;
Dimensions: 2 (items) x 2 (major_axis) x 4 (minor_axis)
Items axis: RAY Index to SPX Index
Major_axis axis: PX_LAST to PX_OPEN
Minor_axis axis: 2014-11-03 to 2014-11-06
</code></pre>

<p>where pd.Panel(d)[item] yields a dataframe</p>

<pre><code>pd.Panel(d)['SPX Index']
2014-11-03  2014-11-04  2014-11-05 2014-11-06
PX_LAST 2017.81 2012.10 2023.57 2031.21
PX_OPEN 2018.21 2015.81 2015.29 2023.33
</code></pre>

<p>You can then hit the command to_frame() to turn it into a dataframe. I use reset_index as well to turn the major and minor axis into columns rather than have them as indices.</p>

<pre><code>pd.Panel(d).to_frame().reset_index()
major   minor      RAY Index    SPX Index
PX_LAST 2014-11-03  1199.460    2017.81
PX_LAST 2014-11-04  1195.323    2012.10
PX_LAST 2014-11-05  1200.936    2023.57
PX_LAST 2014-11-06  1206.061    2031.21
PX_OPEN 2014-11-03  1200.140    2018.21
PX_OPEN 2014-11-04  1197.690    2015.81
PX_OPEN 2014-11-05  1195.320    2015.29
PX_OPEN 2014-11-06  1200.620    2023.33
</code></pre>

<p>Finally, if you don't like the way the frame looks you can use the transpose function of panel to change the appearance before calling to_frame() see documentation here 
<a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.Panel.transpose.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/dev/generated/pandas.Panel.transpose.html</a></p>

<p>Just as an example</p>

<pre><code>pd.Panel(d).transpose(2,0,1).to_frame().reset_index()
major        minor  2014-11-03  2014-11-04  2014-11-05  2014-11-06
RAY Index   PX_LAST 1199.46    1195.323     1200.936    1206.061
RAY Index   PX_OPEN 1200.14    1197.690     1195.320    1200.620
SPX Index   PX_LAST 2017.81    2012.100     2023.570    2031.210
SPX Index   PX_OPEN 2018.21    2015.810     2015.290    2023.330
</code></pre>

<p>Hope this helps.</p>
";;"[""import pandas as pd\nd\n{'RAY Index': {datetime.date(2014, 11, 3): {'PX_LAST': 1199.46,\n'PX_OPEN': 1200.14},\ndatetime.date(2014, 11, 4): {'PX_LAST': 1195.323, 'PX_OPEN': 1197.69},\ndatetime.date(2014, 11, 5): {'PX_LAST': 1200.936, 'PX_OPEN': 1195.32},\ndatetime.date(2014, 11, 6): {'PX_LAST': 1206.061, 'PX_OPEN': 1200.62}},\n'SPX Index': {datetime.date(2014, 11, 3): {'PX_LAST': 2017.81,\n'PX_OPEN': 2018.21},\ndatetime.date(2014, 11, 4): {'PX_LAST': 2012.1, 'PX_OPEN': 2015.81},\ndatetime.date(2014, 11, 5): {'PX_LAST': 2023.57, 'PX_OPEN': 2015.29},\ndatetime.date(2014, 11, 6): {'PX_LAST': 2031.21, 'PX_OPEN': 2023.33}}}\n"", ""pd.Panel(d)\n<class 'pandas.core.panel.Panel'>\nDimensions: 2 (items) x 2 (major_axis) x 4 (minor_axis)\nItems axis: RAY Index to SPX Index\nMajor_axis axis: PX_LAST to PX_OPEN\nMinor_axis axis: 2014-11-03 to 2014-11-06\n"", ""pd.Panel(d)['SPX Index']\n2014-11-03  2014-11-04  2014-11-05 2014-11-06\nPX_LAST 2017.81 2012.10 2023.57 2031.21\nPX_OPEN 2018.21 2015.81 2015.29 2023.33\n"", 'pd.Panel(d).to_frame().reset_index()\nmajor   minor      RAY Index    SPX Index\nPX_LAST 2014-11-03  1199.460    2017.81\nPX_LAST 2014-11-04  1195.323    2012.10\nPX_LAST 2014-11-05  1200.936    2023.57\nPX_LAST 2014-11-06  1206.061    2031.21\nPX_OPEN 2014-11-03  1200.140    2018.21\nPX_OPEN 2014-11-04  1197.690    2015.81\nPX_OPEN 2014-11-05  1195.320    2015.29\nPX_OPEN 2014-11-06  1200.620    2023.33\n', 'pd.Panel(d).transpose(2,0,1).to_frame().reset_index()\nmajor        minor  2014-11-03  2014-11-04  2014-11-05  2014-11-06\nRAY Index   PX_LAST 1199.46    1195.323     1200.936    1206.061\nRAY Index   PX_OPEN 1200.14    1197.690     1195.320    1200.620\nSPX Index   PX_LAST 2017.81    2012.100     2023.570    2031.210\nSPX Index   PX_OPEN 2018.21    2015.810     2015.290    2023.330\n']";"[""import pandas as pd\nd\n{'RAY Index': {datetime.date(2014, 11, 3): {'PX_LAST': 1199.46,\n'PX_OPEN': 1200.14},\ndatetime.date(2014, 11, 4): {'PX_LAST': 1195.323, 'PX_OPEN': 1197.69},\ndatetime.date(2014, 11, 5): {'PX_LAST': 1200.936, 'PX_OPEN': 1195.32},\ndatetime.date(2014, 11, 6): {'PX_LAST': 1206.061, 'PX_OPEN': 1200.62}},\n'SPX Index': {datetime.date(2014, 11, 3): {'PX_LAST': 2017.81,\n'PX_OPEN': 2018.21},\ndatetime.date(2014, 11, 4): {'PX_LAST': 2012.1, 'PX_OPEN': 2015.81},\ndatetime.date(2014, 11, 5): {'PX_LAST': 2023.57, 'PX_OPEN': 2015.29},\ndatetime.date(2014, 11, 6): {'PX_LAST': 2031.21, 'PX_OPEN': 2023.33}}}\n"", ""pd.Panel(d)\n<class 'pandas.core.panel.Panel'>\nDimensions: 2 (items) x 2 (major_axis) x 4 (minor_axis)\nItems axis: RAY Index to SPX Index\nMajor_axis axis: PX_LAST to PX_OPEN\nMinor_axis axis: 2014-11-03 to 2014-11-06\n"", ""pd.Panel(d)['SPX Index']\n2014-11-03  2014-11-04  2014-11-05 2014-11-06\nPX_LAST 2017.81 2012.10 2023.57 2031.21\nPX_OPEN 2018.21 2015.81 2015.29 2023.33\n"", 'pd.Panel(d).to_frame().reset_index()\nmajor   minor      RAY Index    SPX Index\nPX_LAST 2014-11-03  1199.460    2017.81\nPX_LAST 2014-11-04  1195.323    2012.10\nPX_LAST 2014-11-05  1200.936    2023.57\nPX_LAST 2014-11-06  1206.061    2031.21\nPX_OPEN 2014-11-03  1200.140    2018.21\nPX_OPEN 2014-11-04  1197.690    2015.81\nPX_OPEN 2014-11-05  1195.320    2015.29\nPX_OPEN 2014-11-06  1200.620    2023.33\n', 'pd.Panel(d).transpose(2,0,1).to_frame().reset_index()\nmajor        minor  2014-11-03  2014-11-04  2014-11-05  2014-11-06\nRAY Index   PX_LAST 1199.46    1195.323     1200.936    1206.061\nRAY Index   PX_OPEN 1200.14    1197.690     1195.320    1200.620\nSPX Index   PX_LAST 2017.81    2012.100     2023.570    2031.210\nSPX Index   PX_OPEN 2018.21    2015.810     2015.290    2023.330\n']"
1197;;4;26816746;17241004.0;2;12;;;"<p>Since pandas 0.13 you can also use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.get_values.html""><code>get_values</code></a>:</p>

<pre><code>df.index.get_values()
</code></pre>
";;['df.index.get_values()\n'];['get_values', 'df.index.get_values()\n']
1198;;2;26838140;26837998.0;2;32;;;"<pre><code>import numpy as np
df1 = df.replace(np.nan, '', regex=True)
</code></pre>

<p>This might help. It will replace all NaNs with an empty string.</p>
";;"[""import numpy as np\ndf1 = df.replace(np.nan, '', regex=True)\n""]";"[""import numpy as np\ndf1 = df.replace(np.nan, '', regex=True)\n""]"
1199;;1;26849064;11350770.0;2;64;;;"<p>I am using pandas 0.14.1 on macos in ipython notebook.  I tried the proposed line above:</p>

<pre><code>df[df['A'].str.contains(""Hello|Britain"")]
</code></pre>

<p>and got an error: </p>

<pre><code>""cannot index with vector containing NA / NaN values""
</code></pre>

<p>but it worked perfectly when an ""==True"" condition was added, like this:</p>

<pre><code>df[df['A'].str.contains(""Hello|Britain"")==True]
</code></pre>
";;"['df[df[\'A\'].str.contains(""Hello|Britain"")]\n', '""cannot index with vector containing NA / NaN values""\n', 'df[df[\'A\'].str.contains(""Hello|Britain"")==True]\n']";"['df[df[\'A\'].str.contains(""Hello|Britain"")]\n', '""cannot index with vector containing NA / NaN values""\n', 'df[df[\'A\'].str.contains(""Hello|Britain"")==True]\n']"
1200;;1;26851412;11350770.0;2;15;;;"<p>Say you have the following <code>DataFrame</code>:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([['hello', 'hello world'], ['abcd', 'defg']], columns=['a','b'])
&gt;&gt;&gt; df
       a            b
0  hello  hello world
1   abcd         defg
</code></pre>

<p>You can always use the <code>in</code> operator in a lambda expression to create your filter.</p>

<pre><code>&gt;&gt;&gt; df.apply(lambda x: x['a'] in x['b'], axis=1)
0     True
1    False
dtype: bool
</code></pre>

<p>The trick here is to use the <code>axis=1</code> option in the <code>apply</code> to pass elements to the lambda function row by row, as opposed to column by column.</p>
";;"["">>> df = pd.DataFrame([['hello', 'hello world'], ['abcd', 'defg']], columns=['a','b'])\n>>> df\n       a            b\n0  hello  hello world\n1   abcd         defg\n"", "">>> df.apply(lambda x: x['a'] in x['b'], axis=1)\n0     True\n1    False\ndtype: bool\n""]";"['DataFrame', "">>> df = pd.DataFrame([['hello', 'hello world'], ['abcd', 'defg']], columns=['a','b'])\n>>> df\n       a            b\n0  hello  hello world\n1   abcd         defg\n"", 'in', "">>> df.apply(lambda x: x['a'] in x['b'], axis=1)\n0     True\n1    False\ndtype: bool\n"", 'axis=1', 'apply']"
1201;;0;26865524;20853474.0;2;10;;;"<p>You can find the dateutil package at <a href=""https://pypi.python.org/pypi/python-dateutil"" rel=""noreferrer"">https://pypi.python.org/pypi/python-dateutil</a>. Extract it to somewhere and run the command:</p>

<p><code>python setup.py install</code></p>

<p>It worked for me!</p>
";;[];['python setup.py install']
1202;;6;26887820;26886653.0;2;102;;;"<p>OK, two steps to this - first is to write a function that does the translation you want - I've put an example together based on your pseudo-code:</p>

<pre><code>def label_race (row):
   if row['eri_hispanic'] == 1 :
      return 'Hispanic'
   if row['eri_afr_amer'] + row['eri_asian'] + row['eri_hawaiian'] + row['eri_nat_amer'] + row['eri_white'] &gt; 1 :
      return 'Two Or More'
   if row['eri_nat_amer'] == 1 :
      return 'A/I AK Native'
   if row['eri_asian'] == 1:
      return 'Asian'
   if row['eri_afr_amer']  == 1:
      return 'Black/AA'
   if row['eri_hawaiian'] == 1:
      return 'Haw/Pac Isl.'
   if row['eri_white'] == 1:
      return 'White'
   return 'Other'
</code></pre>

<p>You may want to go over this, but it seems to do the trick - notice that the parameter going into the function is considered to be a Series object labelled ""row"".</p>

<p>Next, use the apply function in pandas to apply the function - e.g.</p>

<pre><code>df.apply (lambda row: label_race (row),axis=1)
</code></pre>

<p>Note the axis=1 specifier, that means that the application is done at a row, rather than a column level. The results are here:</p>

<pre><code>0           White
1        Hispanic
2           White
3           White
4           Other
5           White
6     Two Or More
7           White
8    Haw/Pac Isl.
9           White
</code></pre>

<p>If you're happy with those results, then run it again, posting the results into a new column in your original dataframe.</p>

<pre><code>df['race_label'] = df.apply (lambda row: label_race (row),axis=1)
</code></pre>

<p>The resultant dataframe looks like this (scroll to the right to see the new column):</p>

<pre><code>      lname   fname rno_cd  eri_afr_amer  eri_asian  eri_hawaiian   eri_hispanic  eri_nat_amer  eri_white rno_defined    race_label
0      MOST    JEFF      E             0          0             0              0             0          1       White         White
1    CRUISE     TOM      E             0          0             0              1             0          0       White      Hispanic
2      DEPP  JOHNNY    NaN             0          0             0              0             0          1     Unknown         White
3     DICAP     LEO    NaN             0          0             0              0             0          1     Unknown         White
4    BRANDO  MARLON      E             0          0             0              0             0          0       White         Other
5     HANKS     TOM    NaN             0          0             0              0             0          1     Unknown         White
6    DENIRO  ROBERT      E             0          1             0              0             0          1       White   Two Or More
7    PACINO      AL      E             0          0             0              0             0          1       White         White
8  WILLIAMS   ROBIN      E             0          0             1              0             0          0       White  Haw/Pac Isl.
9  EASTWOOD   CLINT      E             0          0             0              0             0          1       White         White
</code></pre>
";;"[""def label_race (row):\n   if row['eri_hispanic'] == 1 :\n      return 'Hispanic'\n   if row['eri_afr_amer'] + row['eri_asian'] + row['eri_hawaiian'] + row['eri_nat_amer'] + row['eri_white'] > 1 :\n      return 'Two Or More'\n   if row['eri_nat_amer'] == 1 :\n      return 'A/I AK Native'\n   if row['eri_asian'] == 1:\n      return 'Asian'\n   if row['eri_afr_amer']  == 1:\n      return 'Black/AA'\n   if row['eri_hawaiian'] == 1:\n      return 'Haw/Pac Isl.'\n   if row['eri_white'] == 1:\n      return 'White'\n   return 'Other'\n"", 'df.apply (lambda row: label_race (row),axis=1)\n', '0           White\n1        Hispanic\n2           White\n3           White\n4           Other\n5           White\n6     Two Or More\n7           White\n8    Haw/Pac Isl.\n9           White\n', ""df['race_label'] = df.apply (lambda row: label_race (row),axis=1)\n"", '      lname   fname rno_cd  eri_afr_amer  eri_asian  eri_hawaiian   eri_hispanic  eri_nat_amer  eri_white rno_defined    race_label\n0      MOST    JEFF      E             0          0             0              0             0          1       White         White\n1    CRUISE     TOM      E             0          0             0              1             0          0       White      Hispanic\n2      DEPP  JOHNNY    NaN             0          0             0              0             0          1     Unknown         White\n3     DICAP     LEO    NaN             0          0             0              0             0          1     Unknown         White\n4    BRANDO  MARLON      E             0          0             0              0             0          0       White         Other\n5     HANKS     TOM    NaN             0          0             0              0             0          1     Unknown         White\n6    DENIRO  ROBERT      E             0          1             0              0             0          1       White   Two Or More\n7    PACINO      AL      E             0          0             0              0             0          1       White         White\n8  WILLIAMS   ROBIN      E             0          0             1              0             0          0       White  Haw/Pac Isl.\n9  EASTWOOD   CLINT      E             0          0             0              0             0          1       White         White\n']";"[""def label_race (row):\n   if row['eri_hispanic'] == 1 :\n      return 'Hispanic'\n   if row['eri_afr_amer'] + row['eri_asian'] + row['eri_hawaiian'] + row['eri_nat_amer'] + row['eri_white'] > 1 :\n      return 'Two Or More'\n   if row['eri_nat_amer'] == 1 :\n      return 'A/I AK Native'\n   if row['eri_asian'] == 1:\n      return 'Asian'\n   if row['eri_afr_amer']  == 1:\n      return 'Black/AA'\n   if row['eri_hawaiian'] == 1:\n      return 'Haw/Pac Isl.'\n   if row['eri_white'] == 1:\n      return 'White'\n   return 'Other'\n"", 'df.apply (lambda row: label_race (row),axis=1)\n', '0           White\n1        Hispanic\n2           White\n3           White\n4           Other\n5           White\n6     Two Or More\n7           White\n8    Haw/Pac Isl.\n9           White\n', ""df['race_label'] = df.apply (lambda row: label_race (row),axis=1)\n"", '      lname   fname rno_cd  eri_afr_amer  eri_asian  eri_hawaiian   eri_hispanic  eri_nat_amer  eri_white rno_defined    race_label\n0      MOST    JEFF      E             0          0             0              0             0          1       White         White\n1    CRUISE     TOM      E             0          0             0              1             0          0       White      Hispanic\n2      DEPP  JOHNNY    NaN             0          0             0              0             0          1     Unknown         White\n3     DICAP     LEO    NaN             0          0             0              0             0          1     Unknown         White\n4    BRANDO  MARLON      E             0          0             0              0             0          0       White         Other\n5     HANKS     TOM    NaN             0          0             0              0             0          1     Unknown         White\n6    DENIRO  ROBERT      E             0          1             0              0             0          1       White   Two Or More\n7    PACINO      AL      E             0          0             0              0             0          1       White         White\n8  WILLIAMS   ROBIN      E             0          0             1              0             0          0       White  Haw/Pac Isl.\n9  EASTWOOD   CLINT      E             0          0             0              0             0          1       White         White\n']"
1203;;6;26918510;12182744.0;2;22;;;"<pre><code>Series.apply(func, convert_dtype=True, args=(), **kwds)

args : tuple

x = my_series.apply(my_function, args = (arg1,))
</code></pre>
";;['Series.apply(func, convert_dtype=True, args=(), **kwds)\n\nargs : tuple\n\nx = my_series.apply(my_function, args = (arg1,))\n'];['Series.apply(func, convert_dtype=True, args=(), **kwds)\n\nargs : tuple\n\nx = my_series.apply(my_function, args = (arg1,))\n']
1204;;3;26977495;26977076.0;2;56;;;"<p>One way is to select the columns and pass them to <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html""><code>np.unique</code></a>:</p>

<pre><code>&gt;&gt;&gt; np.unique(df[['Col1', 'Col2']])
array(['Bill', 'Bob', 'Joe', 'Mary', 'Steve'], dtype=object)
</code></pre>

<p>Note that some versions of Pandas/NumPy may require you to explicitly pass the values from the columns with the <code>.values</code> attribute:</p>

<pre><code>np.unique(df[['Col1', 'Col2']].values)
</code></pre>

<p>A faster way is to use <code>pd.unique</code>. This function uses a hashtable-based algorithm instead of NumPy's sort-based algorithm. You will need to pass a 1D array using <code>ravel()</code>:</p>

<pre><code>&gt;&gt;&gt; pd.unique(df[['Col1', 'Col2']].values.ravel())
array(['Bob', 'Joe', 'Steve', 'Bill', 'Mary'], dtype=object)
</code></pre>

<p>The difference in speed is significant for larger DataFrames:</p>

<pre><code>&gt;&gt;&gt; df1 = pd.concat([df]*100000) # DataFrame with 500000 rows
&gt;&gt;&gt; %timeit np.unique(df1[['Col1', 'Col2']].values)
1 loops, best of 3: 619 ms per loop

&gt;&gt;&gt; %timeit pd.unique(df1[['Col1', 'Col2']].values.ravel())
10 loops, best of 3: 49.9 ms per loop
</code></pre>
";;"["">>> np.unique(df[['Col1', 'Col2']])\narray(['Bill', 'Bob', 'Joe', 'Mary', 'Steve'], dtype=object)\n"", ""np.unique(df[['Col1', 'Col2']].values)\n"", "">>> pd.unique(df[['Col1', 'Col2']].values.ravel())\narray(['Bob', 'Joe', 'Steve', 'Bill', 'Mary'], dtype=object)\n"", "">>> df1 = pd.concat([df]*100000) # DataFrame with 500000 rows\n>>> %timeit np.unique(df1[['Col1', 'Col2']].values)\n1 loops, best of 3: 619 ms per loop\n\n>>> %timeit pd.unique(df1[['Col1', 'Col2']].values.ravel())\n10 loops, best of 3: 49.9 ms per loop\n""]";"['np.unique', "">>> np.unique(df[['Col1', 'Col2']])\narray(['Bill', 'Bob', 'Joe', 'Mary', 'Steve'], dtype=object)\n"", '.values', ""np.unique(df[['Col1', 'Col2']].values)\n"", 'pd.unique', 'ravel()', "">>> pd.unique(df[['Col1', 'Col2']].values.ravel())\narray(['Bob', 'Joe', 'Steve', 'Bill', 'Mary'], dtype=object)\n"", "">>> df1 = pd.concat([df]*100000) # DataFrame with 500000 rows\n>>> %timeit np.unique(df1[['Col1', 'Col2']].values)\n1 loops, best of 3: 619 ms per loop\n\n>>> %timeit pd.unique(df1[['Col1', 'Col2']].values.ravel())\n10 loops, best of 3: 49.9 ms per loop\n""]"
1205;;2;27009771;13838405.0;2;8;;;"<p>A bit late to the game, but here's a way to create a function that sorts pandas Series, DataFrame, and multiindex DataFrame objects using arbitrary functions.</p>

<p>I make use of the <code>df.iloc[index]</code> method, which references a row in a Series/DataFrame by position (compared to <code>df.loc</code>, which references by value). Using this, we just have to have a function that returns a series of positional arguments:</p>

<pre><code>def sort_pd(key=None,reverse=False,cmp=None):
    def sorter(series):
        series_list = list(series)
        return [series_list.index(i) 
           for i in sorted(series_list,key=key,reverse=reverse,cmp=cmp)]
    return sorter
</code></pre>

<p>You can use this to create custom sorting functions. This works on the dataframe used in Andy Hayden's answer:</p>

<pre><code>df = pd.DataFrame([
    [1, 2, 'March'],
    [5, 6, 'Dec'],
    [3, 4, 'April']], 
  columns=['a','b','m'])

custom_dict = {'March':0, 'April':1, 'Dec':3}
sort_by_custom_dict = sort_pd(key=custom_dict.get)

In [6]: df.iloc[sort_by_custom_dict(df['m'])]
Out[6]:
   a  b  m
0  1  2  March
2  3  4  April
1  5  6  Dec
</code></pre>

<p>This also works on multiindex DataFrames and Series objects:</p>

<pre><code>months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']

df = pd.DataFrame([
    ['New York','Mar',12714],
    ['New York','Apr',89238],
    ['Atlanta','Jan',8161],
    ['Atlanta','Sep',5885],
  ],columns=['location','month','sales']).set_index(['location','month'])

sort_by_month = sort_pd(key=months.index)

In [10]: df.iloc[sort_by_month(df.index.get_level_values('month'))]
Out[10]:
                 sales
location  month  
Atlanta   Jan    8161
New York  Mar    12714
          Apr    89238
Atlanta   Sep    5885

sort_by_last_digit = sort_pd(key=lambda x: x%10)

In [12]: pd.Series(list(df['sales'])).iloc[sort_by_last_digit(df['sales'])]
Out[12]:
2    8161
0   12714
3    5885
1   89238
</code></pre>

<p>To me this feels clean, but it uses python operations heavily rather than relying on optimized pandas operations. I haven't done any stress testing but I'd imagine this could get slow on very large DataFrames. Not sure how the performance compares to adding, sorting, then deleting a column. Any tips on speeding up the code would be appreciated!</p>
";;"['def sort_pd(key=None,reverse=False,cmp=None):\n    def sorter(series):\n        series_list = list(series)\n        return [series_list.index(i) \n           for i in sorted(series_list,key=key,reverse=reverse,cmp=cmp)]\n    return sorter\n', ""df = pd.DataFrame([\n    [1, 2, 'March'],\n    [5, 6, 'Dec'],\n    [3, 4, 'April']], \n  columns=['a','b','m'])\n\ncustom_dict = {'March':0, 'April':1, 'Dec':3}\nsort_by_custom_dict = sort_pd(key=custom_dict.get)\n\nIn [6]: df.iloc[sort_by_custom_dict(df['m'])]\nOut[6]:\n   a  b  m\n0  1  2  March\n2  3  4  April\n1  5  6  Dec\n"", ""months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n\ndf = pd.DataFrame([\n    ['New York','Mar',12714],\n    ['New York','Apr',89238],\n    ['Atlanta','Jan',8161],\n    ['Atlanta','Sep',5885],\n  ],columns=['location','month','sales']).set_index(['location','month'])\n\nsort_by_month = sort_pd(key=months.index)\n\nIn [10]: df.iloc[sort_by_month(df.index.get_level_values('month'))]\nOut[10]:\n                 sales\nlocation  month  \nAtlanta   Jan    8161\nNew York  Mar    12714\n          Apr    89238\nAtlanta   Sep    5885\n\nsort_by_last_digit = sort_pd(key=lambda x: x%10)\n\nIn [12]: pd.Series(list(df['sales'])).iloc[sort_by_last_digit(df['sales'])]\nOut[12]:\n2    8161\n0   12714\n3    5885\n1   89238\n""]";"['df.iloc[index]', 'df.loc', 'def sort_pd(key=None,reverse=False,cmp=None):\n    def sorter(series):\n        series_list = list(series)\n        return [series_list.index(i) \n           for i in sorted(series_list,key=key,reverse=reverse,cmp=cmp)]\n    return sorter\n', ""df = pd.DataFrame([\n    [1, 2, 'March'],\n    [5, 6, 'Dec'],\n    [3, 4, 'April']], \n  columns=['a','b','m'])\n\ncustom_dict = {'March':0, 'April':1, 'Dec':3}\nsort_by_custom_dict = sort_pd(key=custom_dict.get)\n\nIn [6]: df.iloc[sort_by_custom_dict(df['m'])]\nOut[6]:\n   a  b  m\n0  1  2  March\n2  3  4  April\n1  5  6  Dec\n"", ""months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n\ndf = pd.DataFrame([\n    ['New York','Mar',12714],\n    ['New York','Apr',89238],\n    ['Atlanta','Jan',8161],\n    ['Atlanta','Sep',5885],\n  ],columns=['location','month','sales']).set_index(['location','month'])\n\nsort_by_month = sort_pd(key=months.index)\n\nIn [10]: df.iloc[sort_by_month(df.index.get_level_values('month'))]\nOut[10]:\n                 sales\nlocation  month  \nAtlanta   Jan    8161\nNew York  Mar    12714\n          Apr    89238\nAtlanta   Sep    5885\n\nsort_by_last_digit = sort_pd(key=lambda x: x%10)\n\nIn [12]: pd.Series(list(df['sales'])).iloc[sort_by_last_digit(df['sales'])]\nOut[12]:\n2    8161\n0   12714\n3    5885\n1   89238\n""]"
1206;;4;27026479;26187759.0;2;9;;;"<p>I have a hack I use for getting parallelization in Pandas. I break my dataframe into chunks, put each chunk into the element of a list, and then use ipython's parallel bits to do a parallel apply on the list of dataframes. Then I put the list back together using pandas <code>concat</code> function. </p>

<p>This is not generally applicable, however. It works for me because the function I want to apply to each chunk of the dataframe takes about a minute. And the pulling apart and putting together of my data does not take all that long. So this is clearly a kludge. With that said, here's an example. I'm using Ipython notebook so you'll see <code>%%time</code> magic in my code:</p>

<pre><code>## make some example data
import pandas as pd

np.random.seed(1)
n=10000
df = pd.DataFrame({'mygroup' : np.random.randint(1000, size=n), 
                   'data' : np.random.rand(n)})
grouped = df.groupby('mygroup')
</code></pre>

<p>For this example I'm going to make 'chunks' based on the above groupby, but this does not have to be how the data is chunked. Although it's a pretty common pattern. </p>

<pre><code>dflist = []
for name, group in grouped:
    dflist.append(group)
</code></pre>

<p>set up the parallel bits</p>

<pre><code>from IPython.parallel import Client
rc = Client()
lview = rc.load_balanced_view()
lview.block = True
</code></pre>

<p>write a silly function to apply to our data</p>

<pre><code>def myFunc(inDf):
    inDf['newCol'] = inDf.data ** 10
    return inDf
</code></pre>

<p>now let's run the code in serial then in parallel. 
serial first:</p>

<pre><code>%%time
serial_list = map(myFunc, dflist)
CPU times: user 14 s, sys: 19.9 ms, total: 14 s
Wall time: 14 s
</code></pre>

<p>now parallel </p>

<pre><code>%%time
parallel_list = lview.map(myFunc, dflist)

CPU times: user 1.46 s, sys: 86.9 ms, total: 1.54 s
Wall time: 1.56 s
</code></pre>

<p>then it only takes a few ms to merge them back into one dataframe</p>

<pre><code>%%time
combinedDf = pd.concat(parallel_list)
 CPU times: user 296 ms, sys: 5.27 ms, total: 301 ms
Wall time: 300 ms
</code></pre>

<p>I'm running 6 IPython engines on my MacBook, but you can see it drops the execute time down to 2s from 14s. </p>

<p>For really long running stochastic simulations I can use AWS backend by firing up a cluster with <a href=""http://star.mit.edu/cluster/"" rel=""noreferrer"">StarCluster</a>. Much of the time, however, I parallelize just across 8 CPUs on my MBP. </p>
";;"[""## make some example data\nimport pandas as pd\n\nnp.random.seed(1)\nn=10000\ndf = pd.DataFrame({'mygroup' : np.random.randint(1000, size=n), \n                   'data' : np.random.rand(n)})\ngrouped = df.groupby('mygroup')\n"", 'dflist = []\nfor name, group in grouped:\n    dflist.append(group)\n', 'from IPython.parallel import Client\nrc = Client()\nlview = rc.load_balanced_view()\nlview.block = True\n', ""def myFunc(inDf):\n    inDf['newCol'] = inDf.data ** 10\n    return inDf\n"", '%%time\nserial_list = map(myFunc, dflist)\nCPU times: user 14 s, sys: 19.9 ms, total: 14 s\nWall time: 14 s\n', '%%time\nparallel_list = lview.map(myFunc, dflist)\n\nCPU times: user 1.46 s, sys: 86.9 ms, total: 1.54 s\nWall time: 1.56 s\n', '%%time\ncombinedDf = pd.concat(parallel_list)\n CPU times: user 296 ms, sys: 5.27 ms, total: 301 ms\nWall time: 300 ms\n']";"['concat', '%%time', ""## make some example data\nimport pandas as pd\n\nnp.random.seed(1)\nn=10000\ndf = pd.DataFrame({'mygroup' : np.random.randint(1000, size=n), \n                   'data' : np.random.rand(n)})\ngrouped = df.groupby('mygroup')\n"", 'dflist = []\nfor name, group in grouped:\n    dflist.append(group)\n', 'from IPython.parallel import Client\nrc = Client()\nlview = rc.load_balanced_view()\nlview.block = True\n', ""def myFunc(inDf):\n    inDf['newCol'] = inDf.data ** 10\n    return inDf\n"", '%%time\nserial_list = map(myFunc, dflist)\nCPU times: user 14 s, sys: 19.9 ms, total: 14 s\nWall time: 14 s\n', '%%time\nparallel_list = lview.map(myFunc, dflist)\n\nCPU times: user 1.46 s, sys: 86.9 ms, total: 1.54 s\nWall time: 1.56 s\n', '%%time\ncombinedDf = pd.concat(parallel_list)\n CPU times: user 296 ms, sys: 5.27 ms, total: 301 ms\nWall time: 300 ms\n']"
1207;;4;27027632;26187759.0;2;43;;;"<p>This seems to work, although it really should be built in to pandas</p>

<pre><code>import pandas as pd
from joblib import Parallel, delayed
import multiprocessing

def tmpFunc(df):
    df['c'] = df.a + df.b
    return df

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)

if __name__ == '__main__':
    df = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])
    print 'parallel version: '
    print applyParallel(df.groupby(df.index), tmpFunc)

    print 'regular version: '
    print df.groupby(df.index).apply(tmpFunc)

    print 'ideal version (does not work): '
    print df.groupby(df.index).applyParallel(tmpFunc)
</code></pre>
";;"[""import pandas as pd\nfrom joblib import Parallel, delayed\nimport multiprocessing\n\ndef tmpFunc(df):\n    df['c'] = df.a + df.b\n    return df\n\ndef applyParallel(dfGrouped, func):\n    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)\n    return pd.concat(retLst)\n\nif __name__ == '__main__':\n    df = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])\n    print 'parallel version: '\n    print applyParallel(df.groupby(df.index), tmpFunc)\n\n    print 'regular version: '\n    print df.groupby(df.index).apply(tmpFunc)\n\n    print 'ideal version (does not work): '\n    print df.groupby(df.index).applyParallel(tmpFunc)\n""]";"[""import pandas as pd\nfrom joblib import Parallel, delayed\nimport multiprocessing\n\ndef tmpFunc(df):\n    df['c'] = df.a + df.b\n    return df\n\ndef applyParallel(dfGrouped, func):\n    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)\n    return pd.concat(retLst)\n\nif __name__ == '__main__':\n    df = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])\n    print 'parallel version: '\n    print applyParallel(df.groupby(df.index), tmpFunc)\n\n    print 'regular version: '\n    print df.groupby(df.index).apply(tmpFunc)\n\n    print 'ideal version (does not work): '\n    print df.groupby(df.index).applyParallel(tmpFunc)\n""]"
1208;;9;27232309;24251219.0;2;119;;;"<h1>The deprecated low_memory option</h1>

<p>The <code>low_memory</code> option is not properly deprecated, but it should be, since it does not actually do anything differently[<a href=""https://github.com/pydata/pandas/issues/5888"" rel=""noreferrer"">source</a>]</p>

<p>The reason you get this <code>low_memory</code> warning is because guessing dtypes for each column is very memory demanding. Pandas tries to determine what dtype to set by analyzing the data in each column.</p>

<h1>Dtype Guessing (very bad)</h1>

<p>Pandas can only determine what dtype a column should have once the whole file is read. This means nothing can really be parsed before the whole file is read unless you risk having to change the dtype of that column when you read the last value.</p>

<p>Consider the example of one file which has a column called user_id.
It contains 10 million rows where the user_id is always numbers.
Since pandas cannot know it is only numbers, it will probably keep it as the original strings until it has read the whole file.</p>

<h1>Specifying dtypes (should always be done)</h1>

<p>adding</p>

<pre><code>dtype={'user_id': int}
</code></pre>

<p>to the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""noreferrer""><code>pd.read_csv()</code></a> call will make pandas know when it starts reading the file, that this is only integers.</p>

<p>Also worth noting is that if the last line in the file would have <code>""foobar""</code> written in the <code>user_id</code> column, the loading would crash if the above dtype was specified.</p>

<h3>Example of broken data that breaks when dtypes are defined</h3>

<pre><code>import pandas as pd
from StringIO import StringIO


csvdata = """"""user_id,username
1,Alice
3,Bob
foobar,Caesar""""""
sio = StringIO(csvdata)
pd.read_csv(sio, dtype={""user_id"": int, ""username"": object})

ValueError: invalid literal for long() with base 10: 'foobar'
</code></pre>

<p>dtypes are typically a numpy thing, read more about them here:
<a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.dtype.html"" rel=""noreferrer"">http://docs.scipy.org/doc/numpy/reference/generated/numpy.dtype.html</a></p>

<h1>Gotchas, caveats, notes</h1>

<p>Setting <code>dtype=object</code> will silence the above warning, but will not make it more memory efficient, only process efficient if anything.</p>

<p>Setting <code>dtype=unicode</code> will not do anything, since to numpy, a <code>unicode</code> is represented as <code>object</code>.</p>

<h3>Usage of converters</h3>

<p>@sparrow correctly points out the usage of converters to avoid pandas blowing up when encountering <code>'foobar'</code> in a column specified as <code>int</code>. I would like to add that converters are really heavy and inefficient to use in pandas and should be used as a last resort. This is because the read_csv process is a single process.</p>

<p>CSV files can be processed line by line and thus can be processed by multiple converters in parallel more efficiently by simply cutting the file into segments and running multiple processes, something that pandas does not support. But this is a different story.</p>
";;"[""dtype={'user_id': int}\n"", 'import pandas as pd\nfrom StringIO import StringIO\n\n\ncsvdata = """"""user_id,username\n1,Alice\n3,Bob\nfoobar,Caesar""""""\nsio = StringIO(csvdata)\npd.read_csv(sio, dtype={""user_id"": int, ""username"": object})\n\nValueError: invalid literal for long() with base 10: \'foobar\'\n']";"['low_memory', 'low_memory', ""dtype={'user_id': int}\n"", 'pd.read_csv()', '""foobar""', 'user_id', 'import pandas as pd\nfrom StringIO import StringIO\n\n\ncsvdata = """"""user_id,username\n1,Alice\n3,Bob\nfoobar,Caesar""""""\nsio = StringIO(csvdata)\npd.read_csv(sio, dtype={""user_id"": int, ""username"": object})\n\nValueError: invalid literal for long() with base 10: \'foobar\'\n', 'dtype=object', 'dtype=unicode', 'unicode', 'object', ""'foobar'"", 'int']"
1209;;0;27236748;19482970.0;2;44;;;"<p>Did some quick tests, and perhaps unsurprisingly the built-in version using <code>dataframe.columns.values.tolist()</code> is the fastest:</p>

<pre><code>In [1]: %timeit [column for column in df]
1000 loops, best of 3: 81.6 s per loop

In [2]: %timeit df.columns.values.tolist()
10000 loops, best of 3: 16.1 s per loop

In [3]: %timeit list(df)
10000 loops, best of 3: 44.9 s per loop

In [4]: % timeit list(df.columns.values)
10000 loops, best of 3: 38.4 s per loop
</code></pre>

<p>(I still really like the <code>list(dataframe)</code> though, so thanks EdChum!)</p>
";;['In [1]: %timeit [column for column in df]\n1000 loops, best of 3: 81.6 s per loop\n\nIn [2]: %timeit df.columns.values.tolist()\n10000 loops, best of 3: 16.1 s per loop\n\nIn [3]: %timeit list(df)\n10000 loops, best of 3: 44.9 s per loop\n\nIn [4]: % timeit list(df.columns.values)\n10000 loops, best of 3: 38.4 s per loop\n'];['dataframe.columns.values.tolist()', 'In [1]: %timeit [column for column in df]\n1000 loops, best of 3: 81.6 s per loop\n\nIn [2]: %timeit df.columns.values.tolist()\n10000 loops, best of 3: 16.1 s per loop\n\nIn [3]: %timeit list(df)\n10000 loops, best of 3: 44.9 s per loop\n\nIn [4]: % timeit list(df.columns.values)\n10000 loops, best of 3: 38.4 s per loop\n', 'list(dataframe)']
1210;;1;27242735;27236275.0;2;44;;;"<p>This error usually rises when you join / assign to a column when the index has duplicate values. Since you are assigning to a row, I suspect that there is a duplicate value in <code>affinity_matrix.columns</code>, perhaps not shown in your question.</p>
";;[];['affinity_matrix.columns']
1211;;2;27266225;27263805.0;2;30;;;"<p>A bit longer than I expected:</p>

<pre><code>&gt;&gt;&gt; df
                samples  subject  trial_num
0  [-0.07, -2.9, -2.44]        1          1
1   [-1.52, -0.35, 0.1]        1          2
2  [-0.17, 0.57, -0.65]        1          3
3  [-0.82, -1.06, 0.47]        2          1
4   [0.79, 1.35, -0.09]        2          2
5   [1.17, 1.14, -1.79]        2          3
&gt;&gt;&gt;
&gt;&gt;&gt; s = df.apply(lambda x: pd.Series(x['samples']),axis=1).stack().reset_index(level=1, drop=True)
&gt;&gt;&gt; s.name = 'sample'
&gt;&gt;&gt;
&gt;&gt;&gt; df.drop('samples', axis=1).join(s)
   subject  trial_num  sample
0        1          1   -0.07
0        1          1   -2.90
0        1          1   -2.44
1        1          2   -1.52
1        1          2   -0.35
1        1          2    0.10
2        1          3   -0.17
2        1          3    0.57
2        1          3   -0.65
3        2          1   -0.82
3        2          1   -1.06
3        2          1    0.47
4        2          2    0.79
4        2          2    1.35
4        2          2   -0.09
5        2          3    1.17
5        2          3    1.14
5        2          3   -1.79
</code></pre>

<p>If you want sequential index, you can apply <code>reset_index(drop=True)</code> to the result.</p>

<p><strong>update</strong>:</p>

<pre><code>&gt;&gt;&gt; res = df.set_index(['subject', 'trial_num'])['samples'].apply(pd.Series).stack()
&gt;&gt;&gt; res = res.reset_index()
&gt;&gt;&gt; res.columns = ['subject','trial_num','sample_num','sample']
&gt;&gt;&gt; res
    subject  trial_num  sample_num  sample
0         1          1           0    1.89
1         1          1           1   -2.92
2         1          1           2    0.34
3         1          2           0    0.85
4         1          2           1    0.24
5         1          2           2    0.72
6         1          3           0   -0.96
7         1          3           1   -2.72
8         1          3           2   -0.11
9         2          1           0   -1.33
10        2          1           1    3.13
11        2          1           2   -0.65
12        2          2           0    0.10
13        2          2           1    0.65
14        2          2           2    0.15
15        2          3           0    0.64
16        2          3           1   -0.10
17        2          3           2   -0.76
</code></pre>
";;"["">>> df\n                samples  subject  trial_num\n0  [-0.07, -2.9, -2.44]        1          1\n1   [-1.52, -0.35, 0.1]        1          2\n2  [-0.17, 0.57, -0.65]        1          3\n3  [-0.82, -1.06, 0.47]        2          1\n4   [0.79, 1.35, -0.09]        2          2\n5   [1.17, 1.14, -1.79]        2          3\n>>>\n>>> s = df.apply(lambda x: pd.Series(x['samples']),axis=1).stack().reset_index(level=1, drop=True)\n>>> s.name = 'sample'\n>>>\n>>> df.drop('samples', axis=1).join(s)\n   subject  trial_num  sample\n0        1          1   -0.07\n0        1          1   -2.90\n0        1          1   -2.44\n1        1          2   -1.52\n1        1          2   -0.35\n1        1          2    0.10\n2        1          3   -0.17\n2        1          3    0.57\n2        1          3   -0.65\n3        2          1   -0.82\n3        2          1   -1.06\n3        2          1    0.47\n4        2          2    0.79\n4        2          2    1.35\n4        2          2   -0.09\n5        2          3    1.17\n5        2          3    1.14\n5        2          3   -1.79\n"", "">>> res = df.set_index(['subject', 'trial_num'])['samples'].apply(pd.Series).stack()\n>>> res = res.reset_index()\n>>> res.columns = ['subject','trial_num','sample_num','sample']\n>>> res\n    subject  trial_num  sample_num  sample\n0         1          1           0    1.89\n1         1          1           1   -2.92\n2         1          1           2    0.34\n3         1          2           0    0.85\n4         1          2           1    0.24\n5         1          2           2    0.72\n6         1          3           0   -0.96\n7         1          3           1   -2.72\n8         1          3           2   -0.11\n9         2          1           0   -1.33\n10        2          1           1    3.13\n11        2          1           2   -0.65\n12        2          2           0    0.10\n13        2          2           1    0.65\n14        2          2           2    0.15\n15        2          3           0    0.64\n16        2          3           1   -0.10\n17        2          3           2   -0.76\n""]";"["">>> df\n                samples  subject  trial_num\n0  [-0.07, -2.9, -2.44]        1          1\n1   [-1.52, -0.35, 0.1]        1          2\n2  [-0.17, 0.57, -0.65]        1          3\n3  [-0.82, -1.06, 0.47]        2          1\n4   [0.79, 1.35, -0.09]        2          2\n5   [1.17, 1.14, -1.79]        2          3\n>>>\n>>> s = df.apply(lambda x: pd.Series(x['samples']),axis=1).stack().reset_index(level=1, drop=True)\n>>> s.name = 'sample'\n>>>\n>>> df.drop('samples', axis=1).join(s)\n   subject  trial_num  sample\n0        1          1   -0.07\n0        1          1   -2.90\n0        1          1   -2.44\n1        1          2   -1.52\n1        1          2   -0.35\n1        1          2    0.10\n2        1          3   -0.17\n2        1          3    0.57\n2        1          3   -0.65\n3        2          1   -0.82\n3        2          1   -1.06\n3        2          1    0.47\n4        2          2    0.79\n4        2          2    1.35\n4        2          2   -0.09\n5        2          3    1.17\n5        2          3    1.14\n5        2          3   -1.79\n"", 'reset_index(drop=True)', "">>> res = df.set_index(['subject', 'trial_num'])['samples'].apply(pd.Series).stack()\n>>> res = res.reset_index()\n>>> res.columns = ['subject','trial_num','sample_num','sample']\n>>> res\n    subject  trial_num  sample_num  sample\n0         1          1           0    1.89\n1         1          1           1   -2.92\n2         1          1           2    0.34\n3         1          2           0    0.85\n4         1          2           1    0.24\n5         1          2           2    0.72\n6         1          3           0   -0.96\n7         1          3           1   -2.72\n8         1          3           2   -0.11\n9         2          1           0   -1.33\n10        2          1           1    3.13\n11        2          1           2   -0.65\n12        2          2           0    0.10\n13        2          2           1    0.65\n14        2          2           2    0.15\n15        2          3           0    0.64\n16        2          3           1   -0.10\n17        2          3           2   -0.76\n""]"
1212;;1;27282644;14262433.0;2;40;;;"<p>I know this is an old thread but I think the <a href=""https://github.com/ContinuumIO/blaze"">Blaze</a> library is worth checking out.  It's built for these types of situations.</p>

<p><strong>From the docs:</strong></p>

<p>Blaze extends the usability of NumPy and Pandas to distributed and out-of-core computing. Blaze provides an interface similar to that of the NumPy ND-Array or Pandas DataFrame but maps these familiar interfaces onto a variety of other computational engines like Postgres or Spark.</p>

<p><strong>Edit:</strong> By the way, it's supported by ContinuumIO and Travis Oliphant, author of NumPy.</p>
";;[];[]
1213;;5;27360130;13851535.0;2;151;;;"<p>To directly answer this question's title (which I understand is not necessarily the OP's problem but could help other users coming across this question) one way to do this is to use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"" rel=""noreferrer"">drop</a> method:</p>

<p><code>df = df.drop(some labels)</code></p>

<p><code>df = df.drop(df[&lt;some boolean condition&gt;].index)</code></p>

<p><strong>Example</strong></p>

<p>To remove all rows where column 'score' is &lt; 50:</p>

<p><code>df = df.drop(df[df.score &lt; 50].index)</code></p>

<p>In place version (as pointed out in comments)</p>

<p><code>df.drop(df[df.score &lt; 50].index, inplace=True)</code></p>

<p><strong>Multiple conditions</strong></p>

<p>(see <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"" rel=""noreferrer"">Boolean Indexing</a>)</p>

<blockquote>
  <p>The operators are: <code>|</code> for <code>or</code>, <code>&amp;</code> for <code>and</code>, and <code>~</code> for <code>not</code>. These must be
  grouped by using parentheses.</p>
</blockquote>

<p>To remove all rows where column 'score' is &lt; 50 and > 20</p>

<p><code>df = df.drop(df[(df.score &lt; 50) &amp; (df.score &gt; 20)].index)</code></p>
";;[];['df = df.drop(some labels)', 'df = df.drop(df[<some boolean condition>].index)', 'df = df.drop(df[df.score < 50].index)', 'df.drop(df[df.score < 50].index, inplace=True)', '|', 'or', '&', 'and', '~', 'not', 'df = df.drop(df[(df.score < 50) & (df.score > 20)].index)']
1214;;0;27361326;11067027.0;2;13;;;"<p>Don't forget to add ""inplace=True"" to Wes' answer or set the result to a new DataFrame.</p>

<pre><code>df.sort_index(axis=1, inplace=True)
</code></pre>
";;['df.sort_index(axis=1, inplace=True)\n'];['df.sort_index(axis=1, inplace=True)\n']
1215;;2;27362540;17383094.0;2;68;;;"<p>Just to very explicitly answer the question of how to convert a single column of boolean values to a column of integers 1 or 0:</p>

<p><code>df.somecolumn = df.somecolumn.astype(int)</code></p>
";;[];['df.somecolumn = df.somecolumn.astype(int)']
1216;;1;27368948;19798153.0;2;13;;;"<p>Adding to the other answers, in a <code>Series</code> there are also <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html"">map</a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html"">apply</a>. </p>

<p><strong>Apply can make a DataFrame out of a series</strong>; however, map will just put a series in every cell of another series, which is probably not what you want.</p>

<pre><code>In [41]: p=pd.Series([1,2,3])

In [42]: p.apply(lambda x: pd.Series([x, x]))
Out[42]: 
   0  1
0  1  1
1  2  2
2  3  3

In [43]: p.map(lambda x: pd.Series([x, x]))
Out[43]: 
0    0    1
1    1
dtype: int64
1    0    2
1    2
dtype: int64
2    0    3
1    3
dtype: int64
dtype: object
</code></pre>

<p>Also if I had a function with side effects, such as ""connect to a web server"", I'd probably use <code>apply</code> just for the sake of clarity.</p>

<pre><code>series.apply(download_file_for_every_element) 
</code></pre>

<p><strong><code>Map</code> can use not only a function, but also a dictionary or another series.</strong> Let's say you want to manipulate <a href=""http://en.wikipedia.org/wiki/Permutation"">permutations</a>.</p>

<p>Take</p>

<pre><code>1 2 3 4 5
2 1 4 5 3
</code></pre>

<p>The square of this permutation is</p>

<pre><code>1 2 3 4 5
1 2 5 3 4
</code></pre>

<p>You can compute it using <code>map</code>. Not sure if self-application is documented, but it works in <code>0.15.1</code>. </p>

<pre><code>In [39]: p=pd.Series([1,0,3,4,2])

In [40]: p.map(p)
Out[40]: 
0    0
1    1
2    4
3    2
4    3
dtype: int64
</code></pre>
";;['In [41]: p=pd.Series([1,2,3])\n\nIn [42]: p.apply(lambda x: pd.Series([x, x]))\nOut[42]: \n   0  1\n0  1  1\n1  2  2\n2  3  3\n\nIn [43]: p.map(lambda x: pd.Series([x, x]))\nOut[43]: \n0    0    1\n1    1\ndtype: int64\n1    0    2\n1    2\ndtype: int64\n2    0    3\n1    3\ndtype: int64\ndtype: object\n', 'series.apply(download_file_for_every_element) \n', '1 2 3 4 5\n2 1 4 5 3\n', '1 2 3 4 5\n1 2 5 3 4\n', 'In [39]: p=pd.Series([1,0,3,4,2])\n\nIn [40]: p.map(p)\nOut[40]: \n0    0\n1    1\n2    4\n3    2\n4    3\ndtype: int64\n'];['Series', 'In [41]: p=pd.Series([1,2,3])\n\nIn [42]: p.apply(lambda x: pd.Series([x, x]))\nOut[42]: \n   0  1\n0  1  1\n1  2  2\n2  3  3\n\nIn [43]: p.map(lambda x: pd.Series([x, x]))\nOut[43]: \n0    0    1\n1    1\ndtype: int64\n1    0    2\n1    2\ndtype: int64\n2    0    3\n1    3\ndtype: int64\ndtype: object\n', 'apply', 'series.apply(download_file_for_every_element) \n', 'Map', '1 2 3 4 5\n2 1 4 5 3\n', '1 2 3 4 5\n1 2 5 3 4\n', 'map', '0.15.1', 'In [39]: p=pd.Series([1,0,3,4,2])\n\nIn [40]: p.map(p)\nOut[40]: \n0    0\n1    1\n2    4\n3    2\n4    3\ndtype: int64\n']
1217;;4;27385043;16236684.0;2;65;;;"<p>I usually do this using <code>zip</code>:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([[i] for i in range(10)], columns=['num'])
&gt;&gt;&gt; df
    num
0    0
1    1
2    2
3    3
4    4
5    5
6    6
7    7
8    8
9    9

&gt;&gt;&gt; def powers(x):
&gt;&gt;&gt;     return x, x**2, x**3, x**4, x**5, x**6

&gt;&gt;&gt; df['p1'], df['p2'], df['p3'], df['p4'], df['p5'], df['p6'] = \
&gt;&gt;&gt;     zip(*df['num'].map(powers))

&gt;&gt;&gt; df
        num     p1      p2      p3      p4      p5      p6
0       0       0       0       0       0       0       0
1       1       1       1       1       1       1       1
2       2       2       4       8       16      32      64
3       3       3       9       27      81      243     729
4       4       4       16      64      256     1024    4096
5       5       5       25      125     625     3125    15625
6       6       6       36      216     1296    7776    46656
7       7       7       49      343     2401    16807   117649
8       8       8       64      512     4096    32768   262144
9       9       9       81      729     6561    59049   531441
</code></pre>
";;"["">>> df = pd.DataFrame([[i] for i in range(10)], columns=['num'])\n>>> df\n    num\n0    0\n1    1\n2    2\n3    3\n4    4\n5    5\n6    6\n7    7\n8    8\n9    9\n\n>>> def powers(x):\n>>>     return x, x**2, x**3, x**4, x**5, x**6\n\n>>> df['p1'], df['p2'], df['p3'], df['p4'], df['p5'], df['p6'] = \\\n>>>     zip(*df['num'].map(powers))\n\n>>> df\n        num     p1      p2      p3      p4      p5      p6\n0       0       0       0       0       0       0       0\n1       1       1       1       1       1       1       1\n2       2       2       4       8       16      32      64\n3       3       3       9       27      81      243     729\n4       4       4       16      64      256     1024    4096\n5       5       5       25      125     625     3125    15625\n6       6       6       36      216     1296    7776    46656\n7       7       7       49      343     2401    16807   117649\n8       8       8       64      512     4096    32768   262144\n9       9       9       81      729     6561    59049   531441\n""]";"['zip', "">>> df = pd.DataFrame([[i] for i in range(10)], columns=['num'])\n>>> df\n    num\n0    0\n1    1\n2    2\n3    3\n4    4\n5    5\n6    6\n7    7\n8    8\n9    9\n\n>>> def powers(x):\n>>>     return x, x**2, x**3, x**4, x**5, x**6\n\n>>> df['p1'], df['p2'], df['p3'], df['p4'], df['p5'], df['p6'] = \\\n>>>     zip(*df['num'].map(powers))\n\n>>> df\n        num     p1      p2      p3      p4      p5      p6\n0       0       0       0       0       0       0       0\n1       1       1       1       1       1       1       1\n2       2       2       4       8       16      32      64\n3       3       3       9       27      81      243     729\n4       4       4       16      64      256     1024    4096\n5       5       5       25      125     625     3125    15625\n6       6       6       36      216     1296    7776    46656\n7       7       7       49      343     2401    16807   117649\n8       8       8       64      512     4096    32768   262144\n9       9       9       81      729     6561    59049   531441\n""]"
1218;;0;27422749;27405483.0;2;37;;;"<p><code>df.groupby('l_customer_id_i').agg(lambda x: ','.join(x))</code> does already return a dataframe, so you cannot loop over the groups anymore.</p>

<p>In general:</p>

<ul>
<li><p><code>df.groupby(...)</code> returns a <code>GroupBy</code> object (a DataFrameGroupBy or SeriesGroupBy), and with this, you can iterate through the groups (as explained in the docs <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#iterating-through-groups"">here</a>). You can do something like:</p>

<pre><code>grouped = df.groupby('A')

for name, group in grouped:
    ...
</code></pre></li>
<li><p>When you apply a function on the groupby, in your example <code>df.groupby(...).agg(...)</code> (but this can also be <code>transform</code>, <code>apply</code>, <code>mean</code>, ...), you <strong>combine</strong> the result of <strong>applying</strong> the function to the different groups together in one dataframe (the apply and combine step of the 'split-apply-combine' paradigm of groupby). So the result of this will always be again a DataFrame (or a Series depending on the applied function).</p></li>
</ul>
";;"[""grouped = df.groupby('A')\n\nfor name, group in grouped:\n    ...\n""]";"[""df.groupby('l_customer_id_i').agg(lambda x: ','.join(x))"", 'df.groupby(...)', 'GroupBy', ""grouped = df.groupby('A')\n\nfor name, group in grouped:\n    ...\n"", 'df.groupby(...).agg(...)', 'transform', 'apply', 'mean']"
1219;;3;27520877;21487329.0;2;17;;;"<p>If you label the columns and index of your DataFrame, pandas will automatically supply appropriate labels:</p>

<pre><code>import pandas as pd
values = [[1,2], [2,5]]
df = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])
df.columns.name = 'Type'
df.index.name = 'Index'
df.plot(lw=2, colormap='jet', marker='.', markersize=10, title='Video streaming dropout by category')
</code></pre>

<p><img src=""https://i.stack.imgur.com/9Ejsk.png"" alt=""enter image description here""></p>

<p>In this case, you'll still need to supply y-labels manually (e.g., via <code>plt.ylabel</code> as shown in the other answers).</p>
";;"[""import pandas as pd\nvalues = [[1,2], [2,5]]\ndf = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])\ndf.columns.name = 'Type'\ndf.index.name = 'Index'\ndf.plot(lw=2, colormap='jet', marker='.', markersize=10, title='Video streaming dropout by category')\n""]";"[""import pandas as pd\nvalues = [[1,2], [2,5]]\ndf = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])\ndf.columns.name = 'Type'\ndf.index.name = 'Index'\ndf.plot(lw=2, colormap='jet', marker='.', markersize=10, title='Video streaming dropout by category')\n"", 'plt.ylabel']"
1220;;0;27617290;16249736.0;2;10;;;"<p>You can load your mongodb data to pandas DataFrame using this code. It works for me. Hopefully for you too.</p>

<pre><code>import pymongo
import pandas as pd
from pymongo import MongoClient
client = MongoClient()
db = client.database_name
collection = db.collection_name
data = pd.DataFrame(list(collection.find()))
</code></pre>
";;['import pymongo\nimport pandas as pd\nfrom pymongo import MongoClient\nclient = MongoClient()\ndb = client.database_name\ncollection = db.collection_name\ndata = pd.DataFrame(list(collection.find()))\n'];['import pymongo\nimport pandas as pd\nfrom pymongo import MongoClient\nclient = MongoClient()\ndb = client.database_name\ncollection = db.collection_name\ndata = pd.DataFrame(list(collection.find()))\n']
1221;;5;27667801;27667759.0;2;50;;;"<p>Please refer to the doc <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#different-choices-for-indexing"" rel=""noreferrer"">Different Choices for Indexing</a>, it states clearly when and why you should use <strong>.loc, .iloc</strong> over <strong>.ix</strong>, it's about explicit use case:</p>

<blockquote>
  <p>.ix supports mixed integer and label based access. It is primarily
  label based, but will fall back to integer positional access unless
  the corresponding axis is of integer type. .ix is the most general and
  will support any of the inputs in .loc and .iloc. .ix also supports
  floating point label schemes. .ix is exceptionally useful when dealing
  with mixed positional and label based hierachical indexes.</p>
  
  <p>However, when an axis is integer based, ONLY label based access and
  not positional access is supported. Thus, in such cases, its usually
  better to be explicit and use .iloc or .loc.</p>
</blockquote>

<p>Hope this helps.</p>

<h3>Update 22 Mar 2017</h3>

<p>Thanks to comment from @Alexander, <strong>Pandas</strong> is going to deprecate <code>ix</code> in <strong>0.20</strong>, details in <a href=""http://pandas-docs.github.io/pandas-docs-travis/whatsnew.html#deprecate-ix"" rel=""noreferrer"">here</a>.</p>

<p>One of the strong reason behind is because mixing indexes -- positional and label (effectively using <code>ix</code>) has been a significant source of problems for users.</p>

<p>It is expected to migrate to use <code>iloc</code> and <code>loc</code> instead, here is a link on <a href=""http://pandas-docs.github.io/pandas-docs-travis/indexing.html#indexing-deprecate-ix"" rel=""noreferrer"">how to convert code</a>.</p>
";;[];['ix', 'ix', 'iloc', 'loc']
1222;;0;27680109;27673231.0;2;41;;;"<p>This expands on Paul's answer. In Pandas, indexing a DataFrame returns a reference to the initial DataFrame. Thus, changing the subset will change the initial DataFrame. Thus, you'd want to use the copy if you want to make sure the initial DataFrame shouldn't change. Consider the following code:</p>

<pre><code>df = DataFrame({'x': [1,2]})
df_sub = df[0:1]
df_sub.x = -1
print(df)
</code></pre>

<p>You'll get: </p>

<pre><code>x
0 -1
1  2
</code></pre>

<p>In contrast, the following leaves df unchanged:</p>

<pre><code>df_sub_copy = df[0:1].copy()
df_sub_copy.x = -1
</code></pre>
";;"[""df = DataFrame({'x': [1,2]})\ndf_sub = df[0:1]\ndf_sub.x = -1\nprint(df)\n"", 'x\n0 -1\n1  2\n', 'df_sub_copy = df[0:1].copy()\ndf_sub_copy.x = -1\n']";"[""df = DataFrame({'x': [1,2]})\ndf_sub = df[0:1]\ndf_sub.x = -1\nprint(df)\n"", 'x\n0 -1\n1  2\n', 'df_sub_copy = df[0:1].copy()\ndf_sub_copy.x = -1\n']"
1223;;2;27747726;21269399.0;2;6;;;"<p>I tried using the dtypes=[datetime, ...] option, but</p>

<pre><code>import pandas as pd
from datetime import datetime
headers = ['col1', 'col2', 'col3', 'col4'] 
dtypes = [datetime, datetime, str, float] 
pd.read_csv(file, sep='\t', header=None, names=headers, dtype=dtypes)
</code></pre>

<p>I encountered the following error:</p>

<pre><code>TypeError: data type not understood
</code></pre>

<p>The only change I had to make is to replace datetime with datetime.datetime</p>

<pre><code>import pandas as pd
from datetime import datetime
headers = ['col1', 'col2', 'col3', 'col4'] 
dtypes = [datetime.datetime, datetime.datetime, str, float] 
pd.read_csv(file, sep='\t', header=None, names=headers, dtype=dtypes)
</code></pre>
";;"[""import pandas as pd\nfrom datetime import datetime\nheaders = ['col1', 'col2', 'col3', 'col4'] \ndtypes = [datetime, datetime, str, float] \npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes)\n"", 'TypeError: data type not understood\n', ""import pandas as pd\nfrom datetime import datetime\nheaders = ['col1', 'col2', 'col3', 'col4'] \ndtypes = [datetime.datetime, datetime.datetime, str, float] \npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes)\n""]";"[""import pandas as pd\nfrom datetime import datetime\nheaders = ['col1', 'col2', 'col3', 'col4'] \ndtypes = [datetime, datetime, str, float] \npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes)\n"", 'TypeError: data type not understood\n', ""import pandas as pd\nfrom datetime import datetime\nheaders = ['col1', 'col2', 'col3', 'col4'] \ndtypes = [datetime.datetime, datetime.datetime, str, float] \npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes)\n""]"
1224;;2;27791362;15017072.0;2;32;;;"<p>The answer by @chip completely misses the point of two keyword arguments.</p>

<ul>
<li><strong>names</strong> is only necessary when there is no header and you want to specify other arguments using column names rather than integer indices.</li>
<li><strong>usecols</strong> is supposed to provide a filter before reading the whole DataFrame into memory; if used properly, there should never be a need to delete columns after reading.</li>
</ul>

<p>This solution corrects those oddities:</p>

<pre><code>import pandas as pd
from StringIO import StringIO

csv = r""""""dummy,date,loc,x
bar,20090101,a,1
bar,20090102,a,3
bar,20090103,a,5
bar,20090101,b,1
bar,20090102,b,3
bar,20090103,b,5""""""

df = pd.read_csv(StringIO(csv),
        header=0,
        index_col=[""date"", ""loc""], 
        usecols=[""date"", ""loc"", ""x""],
        parse_dates=[""date""])
</code></pre>

<p>Which gives us:</p>

<pre><code>                x
date       loc
2009-01-01 a    1
2009-01-02 a    3
2009-01-03 a    5
2009-01-01 b    1
2009-01-02 b    3
2009-01-03 b    5
</code></pre>
";;"['import pandas as pd\nfrom StringIO import StringIO\n\ncsv = r""""""dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nbar,20090103,b,5""""""\n\ndf = pd.read_csv(StringIO(csv),\n        header=0,\n        index_col=[""date"", ""loc""], \n        usecols=[""date"", ""loc"", ""x""],\n        parse_dates=[""date""])\n', '                x\ndate       loc\n2009-01-01 a    1\n2009-01-02 a    3\n2009-01-03 a    5\n2009-01-01 b    1\n2009-01-02 b    3\n2009-01-03 b    5\n']";"['import pandas as pd\nfrom StringIO import StringIO\n\ncsv = r""""""dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nbar,20090103,b,5""""""\n\ndf = pd.read_csv(StringIO(csv),\n        header=0,\n        index_col=[""date"", ""loc""], \n        usecols=[""date"", ""loc"", ""x""],\n        parse_dates=[""date""])\n', '                x\ndate       loc\n2009-01-01 a    1\n2009-01-02 a    3\n2009-01-03 a    5\n2009-01-01 b    1\n2009-01-02 b    3\n2009-01-03 b    5\n']"
1225;;2;27844045;27842613.0;2;39;;;"<p>What you want to do is actually again a groupby (on the result of the first groupby): sort and take the first three elements per group.</p>

<p>Starting from the result of the first groupby:</p>

<pre><code>In [60]: df_agg = df.groupby(['job','source']).agg({'count':sum})
</code></pre>

<p>We group by the first level of the index:</p>

<pre><code>In [63]: g = df_agg['count'].groupby(level=0, group_keys=False)
</code></pre>

<p>Then we want to sort ('order') each group and take the first three elements:</p>

<pre><code>In [64]: res = g.apply(lambda x: x.order(ascending=False).head(3))
</code></pre>

<p>However, for this, there is a shortcut function to do this, <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.nlargest.html""><code>nlargest</code></a>:</p>

<pre><code>In [65]: g.nlargest(3)
Out[65]:
job     source
market  A         5
        D         4
        B         3
sales   E         7
        C         6
        B         4
dtype: int64
</code></pre>
";;"[""In [60]: df_agg = df.groupby(['job','source']).agg({'count':sum})\n"", ""In [63]: g = df_agg['count'].groupby(level=0, group_keys=False)\n"", 'In [64]: res = g.apply(lambda x: x.order(ascending=False).head(3))\n', 'In [65]: g.nlargest(3)\nOut[65]:\njob     source\nmarket  A         5\n        D         4\n        B         3\nsales   E         7\n        C         6\n        B         4\ndtype: int64\n']";"[""In [60]: df_agg = df.groupby(['job','source']).agg({'count':sum})\n"", ""In [63]: g = df_agg['count'].groupby(level=0, group_keys=False)\n"", 'In [64]: res = g.apply(lambda x: x.order(ascending=False).head(3))\n', 'nlargest', 'In [65]: g.nlargest(3)\nOut[65]:\njob     source\nmarket  A         5\n        D         4\n        B         3\nsales   E         7\n        C         6\n        B         4\ndtype: int64\n']"
1226;;0;27905350;27905295.0;2;35;;;"<p>You could use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html"" rel=""noreferrer""><code>fillna</code></a> method on the DataFrame and specify the method as <code>ffill</code> (forward fill):</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])
&gt;&gt;&gt; df.fillna(method='ffill')
   0  1  2
0  1  2  3
1  4  2  3
2  4  2  9
</code></pre>

<p>This method...</p>

<blockquote>
  <p>propagate[s] last valid observation forward to next valid</p>
</blockquote>

<p>To go the opposite way, there's also a <code>bfill</code> method.</p>

<p>This method doesn't modify the DataFrame inplace - you'll need to rebind the returned DataFrame to a variable or else specify <code>inplace=True</code>:</p>

<pre><code>df.fillna(method='ffill', inplace=True)
</code></pre>
";;"["">>> df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])\n>>> df.fillna(method='ffill')\n   0  1  2\n0  1  2  3\n1  4  2  3\n2  4  2  9\n"", ""df.fillna(method='ffill', inplace=True)\n""]";"['fillna', 'ffill', "">>> df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])\n>>> df.fillna(method='ffill')\n   0  1  2\n0  1  2  3\n1  4  2  3\n2  4  2  9\n"", 'bfill', 'inplace=True', ""df.fillna(method='ffill', inplace=True)\n""]"
1227;;0;27905354;27905295.0;2;7;;;"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html"" rel=""noreferrer""><code>pandas.DataFrame.fillna</code></a> with the <code>method='ffill'</code> option. <code>'ffill'</code> stands for 'forward fill' and will propagate last valid observation forward. The alternative is <code>'bfill'</code> which works the same way, but backwards.</p>

<pre><code>import pandas as pd

df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])
df = df.fillna(method='ffill')

print(df)
#   0  1  2
#0  1  2  3
#1  4  2  3
#2  4  2  9
</code></pre>

<p>There is also a direct synonym function for this, <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.ffill.html"" rel=""noreferrer""><code>pandas.DataFrame.ffill</code></a>, to make things simpler.</p>
";;"[""import pandas as pd\n\ndf = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])\ndf = df.fillna(method='ffill')\n\nprint(df)\n#   0  1  2\n#0  1  2  3\n#1  4  2  3\n#2  4  2  9\n""]";"['pandas.DataFrame.fillna', ""method='ffill'"", ""'ffill'"", ""'bfill'"", ""import pandas as pd\n\ndf = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])\ndf = df.fillna(method='ffill')\n\nprint(df)\n#   0  1  2\n#0  1  2  3\n#1  4  2  3\n#2  4  2  9\n"", 'pandas.DataFrame.ffill']"
1228;;0;27951930;27517425.0;2;59;;;"<p>As I felt similarly confused with <code>.transform</code> operation vs. <code>.apply</code> I found a few answers shedding some light on the issue. <a href=""https://stackoverflow.com/a/13854901/4077912"">This answer</a> for example was very helpful.</p>

<p>My takeout so far is that <code>.transform</code> will work (or deal) with <code>Series</code> (columns) <strong>in isolation from each other</strong>. What this means is that in your last two calls:</p>

<pre><code>df.groupby('A').transform(lambda x: (x['C'] - x['D']))
df.groupby('A').transform(lambda x: (x['C'] - x['D']).mean())
</code></pre>

<p>You asked <code>.transform</code> to take values from two columns and 'it' actually does not 'see' both of them at the same time (so to speak). <code>transform</code> will look at the dataframe columns one by one and return back a series (or group of series) 'made' of scalars which are repeated <code>len(input_column)</code> times.</p>

<p>So this scalar, that should be used by <code>.transform</code> to make the <code>Series</code> is a result of some reduction function applied on an input <code>Series</code> (and only on ONE series/column at a time).</p>

<p>Consider this example (on your dataframe):</p>

<pre><code>zscore = lambda x: (x - x.mean()) / x.std() # Note that it does not reference anything outside of 'x' and for transform 'x' is one column.
df.groupby('A').transform(zscore)
</code></pre>

<p>will yield:</p>

<pre><code>       C      D
0  0.989  0.128
1 -0.478  0.489
2  0.889 -0.589
3 -0.671 -1.150
4  0.034 -0.285
5  1.149  0.662
6 -1.404 -0.907
7 -0.509  1.653
</code></pre>

<p>Which is exactly the same as if you would use it on only on one column at a time:</p>

<pre><code>df.groupby('A')['C'].transform(zscore)
</code></pre>

<p>yielding:</p>

<pre><code>0    0.989
1   -0.478
2    0.889
3   -0.671
4    0.034
5    1.149
6   -1.404
7   -0.509
</code></pre>

<p>Note that <code>.apply</code> in the last example (<code>df.groupby('A')['C'].apply(zscore)</code>) would work in exactly the same way, but it would fail if you tried using it on a dataframe:</p>

<pre><code>df.groupby('A').apply(zscore)
</code></pre>

<p>gives error:</p>

<pre><code>ValueError: operands could not be broadcast together with shapes (6,) (2,)
</code></pre>

<p>So where else  is <code>.transform</code> useful? The simplest case is trying to assign results of reduction function back to original dataframe.</p>

<pre><code>df['sum_C'] = df.groupby('A')['C'].transform(sum)
df.sort('A') # to clearly see the scalar ('sum') applies to the whole column of the group
</code></pre>

<p>yielding:</p>

<pre><code>     A      B      C      D  sum_C
1  bar    one  1.998  0.593  3.973
3  bar  three  1.287 -0.639  3.973
5  bar    two  0.687 -1.027  3.973
4  foo    two  0.205  1.274  4.373
2  foo    two  0.128  0.924  4.373
6  foo    one  2.113 -0.516  4.373
7  foo  three  0.657 -1.179  4.373
0  foo    one  1.270  0.201  4.373
</code></pre>

<p>Trying the same with <code>.apply</code> would give <code>NaNs</code> in <code>sum_C</code>.
Because <code>.apply</code> would return a reduced <code>Series</code>, which it does not know how to broadcast back:</p>

<pre><code>df.groupby('A')['C'].apply(sum)
</code></pre>

<p>giving:</p>

<pre><code>A
bar    3.973
foo    4.373
</code></pre>

<p>There are also cases when <code>.transform</code> is used to filter the data:</p>

<pre><code>df[df.groupby(['B'])['D'].transform(sum) &lt; -1]

     A      B      C      D
3  bar  three  1.287 -0.639
7  foo  three  0.657 -1.179
</code></pre>

<p>I hope this adds a bit more clarity.</p>
";;"[""df.groupby('A').transform(lambda x: (x['C'] - x['D']))\ndf.groupby('A').transform(lambda x: (x['C'] - x['D']).mean())\n"", ""zscore = lambda x: (x - x.mean()) / x.std() # Note that it does not reference anything outside of 'x' and for transform 'x' is one column.\ndf.groupby('A').transform(zscore)\n"", '       C      D\n0  0.989  0.128\n1 -0.478  0.489\n2  0.889 -0.589\n3 -0.671 -1.150\n4  0.034 -0.285\n5  1.149  0.662\n6 -1.404 -0.907\n7 -0.509  1.653\n', ""df.groupby('A')['C'].transform(zscore)\n"", '0    0.989\n1   -0.478\n2    0.889\n3   -0.671\n4    0.034\n5    1.149\n6   -1.404\n7   -0.509\n', ""df.groupby('A').apply(zscore)\n"", 'ValueError: operands could not be broadcast together with shapes (6,) (2,)\n', ""df['sum_C'] = df.groupby('A')['C'].transform(sum)\ndf.sort('A') # to clearly see the scalar ('sum') applies to the whole column of the group\n"", '     A      B      C      D  sum_C\n1  bar    one  1.998  0.593  3.973\n3  bar  three  1.287 -0.639  3.973\n5  bar    two  0.687 -1.027  3.973\n4  foo    two  0.205  1.274  4.373\n2  foo    two  0.128  0.924  4.373\n6  foo    one  2.113 -0.516  4.373\n7  foo  three  0.657 -1.179  4.373\n0  foo    one  1.270  0.201  4.373\n', ""df.groupby('A')['C'].apply(sum)\n"", 'A\nbar    3.973\nfoo    4.373\n', ""df[df.groupby(['B'])['D'].transform(sum) < -1]\n\n     A      B      C      D\n3  bar  three  1.287 -0.639\n7  foo  three  0.657 -1.179\n""]";"['.transform', '.apply', '.transform', 'Series', ""df.groupby('A').transform(lambda x: (x['C'] - x['D']))\ndf.groupby('A').transform(lambda x: (x['C'] - x['D']).mean())\n"", '.transform', 'transform', 'len(input_column)', '.transform', 'Series', 'Series', ""zscore = lambda x: (x - x.mean()) / x.std() # Note that it does not reference anything outside of 'x' and for transform 'x' is one column.\ndf.groupby('A').transform(zscore)\n"", '       C      D\n0  0.989  0.128\n1 -0.478  0.489\n2  0.889 -0.589\n3 -0.671 -1.150\n4  0.034 -0.285\n5  1.149  0.662\n6 -1.404 -0.907\n7 -0.509  1.653\n', ""df.groupby('A')['C'].transform(zscore)\n"", '0    0.989\n1   -0.478\n2    0.889\n3   -0.671\n4    0.034\n5    1.149\n6   -1.404\n7   -0.509\n', '.apply', ""df.groupby('A')['C'].apply(zscore)"", ""df.groupby('A').apply(zscore)\n"", 'ValueError: operands could not be broadcast together with shapes (6,) (2,)\n', '.transform', ""df['sum_C'] = df.groupby('A')['C'].transform(sum)\ndf.sort('A') # to clearly see the scalar ('sum') applies to the whole column of the group\n"", '     A      B      C      D  sum_C\n1  bar    one  1.998  0.593  3.973\n3  bar  three  1.287 -0.639  3.973\n5  bar    two  0.687 -1.027  3.973\n4  foo    two  0.205  1.274  4.373\n2  foo    two  0.128  0.924  4.373\n6  foo    one  2.113 -0.516  4.373\n7  foo  three  0.657 -1.179  4.373\n0  foo    one  1.270  0.201  4.373\n', '.apply', 'NaNs', 'sum_C', '.apply', 'Series', ""df.groupby('A')['C'].apply(sum)\n"", 'A\nbar    3.973\nfoo    4.373\n', '.transform', ""df[df.groupby(['B'])['D'].transform(sum) < -1]\n\n     A      B      C      D\n3  bar  three  1.287 -0.639\n7  foo  three  0.657 -1.179\n""]"
1229;;0;27954411;22470690.0;2;52;;;"<p>As of pandas v0.14.1, you can utilize <code>select_dtypes()</code> to select columns by dtype</p>

<pre><code>In [2]: df = pd.DataFrame({'NAME': list('abcdef'),
    'On_Time': [True, False] * 3,
    'On_Budget': [False, True] * 3})

In [3]: df.select_dtypes(include=['bool'])
Out[3]:
  On_Budget On_Time
0     False    True
1      True   False
2     False    True
3      True   False
4     False    True
5      True   False

In [4]: mylist = list(df.select_dtypes(include=['bool']).columns)

In [5]: mylist
Out[5]: ['On_Budget', 'On_Time']
</code></pre>
";;"[""In [2]: df = pd.DataFrame({'NAME': list('abcdef'),\n    'On_Time': [True, False] * 3,\n    'On_Budget': [False, True] * 3})\n\nIn [3]: df.select_dtypes(include=['bool'])\nOut[3]:\n  On_Budget On_Time\n0     False    True\n1      True   False\n2     False    True\n3      True   False\n4     False    True\n5      True   False\n\nIn [4]: mylist = list(df.select_dtypes(include=['bool']).columns)\n\nIn [5]: mylist\nOut[5]: ['On_Budget', 'On_Time']\n""]";"['select_dtypes()', ""In [2]: df = pd.DataFrame({'NAME': list('abcdef'),\n    'On_Time': [True, False] * 3,\n    'On_Budget': [False, True] * 3})\n\nIn [3]: df.select_dtypes(include=['bool'])\nOut[3]:\n  On_Budget On_Time\n0     False    True\n1      True   False\n2     False    True\n3      True   False\n4     False    True\n5      True   False\n\nIn [4]: mylist = list(df.select_dtypes(include=['bool']).columns)\n\nIn [5]: mylist\nOut[5]: ['On_Budget', 'On_Time']\n""]"
1230;;0;27975191;27975069.0;2;6;;;"<pre><code>&gt;&gt;&gt; mask = df['ids'].str.contains('ball')    
&gt;&gt;&gt; mask
0     True
1     True
2    False
3     True
Name: ids, dtype: bool

&gt;&gt;&gt; df[mask]
     ids  vals
0  aball     1
1  bball     2
3  fball     4
</code></pre>
";;"["">>> mask = df['ids'].str.contains('ball')    \n>>> mask\n0     True\n1     True\n2    False\n3     True\nName: ids, dtype: bool\n\n>>> df[mask]\n     ids  vals\n0  aball     1\n1  bball     2\n3  fball     4\n""]";"["">>> mask = df['ids'].str.contains('ball')    \n>>> mask\n0     True\n1     True\n2    False\n3     True\nName: ids, dtype: bool\n\n>>> df[mask]\n     ids  vals\n0  aball     1\n1  bball     2\n3  fball     4\n""]"
1231;;7;27975230;27975069.0;2;47;;;"<pre><code>In [3]: df[df['ids'].str.contains(""ball"")]
Out[3]:
     ids  vals
0  aball     1
1  bball     2
3  fball     4
</code></pre>
";;"['In [3]: df[df[\'ids\'].str.contains(""ball"")]\nOut[3]:\n     ids  vals\n0  aball     1\n1  bball     2\n3  fball     4\n']";"['In [3]: df[df[\'ids\'].str.contains(""ball"")]\nOut[3]:\n     ids  vals\n0  aball     1\n1  bball     2\n3  fball     4\n']"
1232;;3;27975789;27975069.0;2;25;;;"<pre><code>df[df['ids'].str.contains('ball', na = False)] # valid for (at least) pandas version 0.17.1
</code></pre>

<p>Step-by-step explanation (from inner to outer):</p>

<ul>
<li><code>df['ids']</code> selects the <code>ids</code> column of the data frame (techincally, the object <code>df[ids]</code> is of type <code>pandas.Series</code>)</li>
<li><code>df['ids'].str</code> allows us to apply vectorized string methods (e.g., <code>lower</code>, <code>contains</code>) to the Series</li>
<li><code>df['ids'].str.contains('ball')</code> checks <em>each</em> element of the Series as to whether the element value has the string 'ball' as a substring. The result is a Series of Booleans indicating <code>True</code> or <code>False</code> about the existence of a 'ball' substring.</li>
<li><code>df[df['ids'].str.contains('ball')]</code> applies the Boolean 'mask' to the dataframe and returns a view containing appropriate records.</li>
<li><code>na = False</code> removes NA / NaN values from consideration; otherwise a ValueError may be returned.</li>
</ul>
";;"[""df[df['ids'].str.contains('ball', na = False)] # valid for (at least) pandas version 0.17.1\n""]";"[""df[df['ids'].str.contains('ball', na = False)] # valid for (at least) pandas version 0.17.1\n"", ""df['ids']"", 'ids', 'df[ids]', 'pandas.Series', ""df['ids'].str"", 'lower', 'contains', ""df['ids'].str.contains('ball')"", 'True', 'False', ""df[df['ids'].str.contains('ball')]"", 'na = False']"
1233;;3;27999688;17709641.0;2;35;;;"<p>For me (Mac OS X Maverics, Python 2.7)</p>

<pre><code>easy_install --upgrade numpy
</code></pre>

<p>helped. After this you can install up-to-date packages <em>pandas</em>, <em>scikit-learn</em>, e.t.c. using <strong>pip</strong>:</p>

<pre><code>pip install pandas
</code></pre>
";;['easy_install --upgrade numpy\n', 'pip install pandas\n'];['easy_install --upgrade numpy\n', 'pip install pandas\n']
1234;;3;28006809;28006793.0;2;50;;;"<p>You could access the underlying array and call its <code>tolist</code> method:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([[1,2,3],[3,4,5]])
&gt;&gt;&gt; lol = df.values.tolist()
&gt;&gt;&gt; lol
[[1L, 2L, 3L], [3L, 4L, 5L]]
</code></pre>
";;['>>> df = pd.DataFrame([[1,2,3],[3,4,5]])\n>>> lol = df.values.tolist()\n>>> lol\n[[1L, 2L, 3L], [3L, 4L, 5L]]\n'];['tolist', '>>> df = pd.DataFrame([[1,2,3],[3,4,5]])\n>>> lol = df.values.tolist()\n>>> lol\n[[1L, 2L, 3L], [3L, 4L, 5L]]\n']
1235;;0;28142820;17071871.0;2;7;;;"<p>I just tried editing this, but I wasn't logged in, so I'm not sure where my edit went. I was trying to incorporate multiple selection. So I think a better answer is:</p>

<p>For a single value, the most straightforward (human readable) is probably:</p>

<pre><code>df.loc[df['column_name'] == some_value]
</code></pre>

<p>For lists of values you can also use:</p>

<pre><code>df.loc[df['column_name'].isin(some_values)]
</code></pre>

<p>For example,</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),
               'B': 'one one two three two two one three'.split(),
               'C': np.arange(8), 'D': np.arange(8) * 2})
print(df)
#      A      B  C   D
# 0  foo    one  0   0
# 1  bar    one  1   2
# 2  foo    two  2   4
# 3  bar  three  3   6
# 4  foo    two  4   8
# 5  bar    two  5  10
# 6  foo    one  6  12
# 7  foo  three  7  14

print(df.loc[df['A'] == 'foo'])
</code></pre>

<p>yields</p>

<pre><code>     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14
</code></pre>

<p>If you have multiple criteria you want to select against, you can put them in a list and use 'isin':</p>

<pre><code>print(df.loc[df['B'].isin(['one','three'])])
</code></pre>

<p>yields</p>

<pre><code>      A      B  C   D
0  foo    one  0   0
1  bar    one  1   2
3  bar  three  3   6
6  foo    one  6  12
7  foo  three  7  14
</code></pre>

<p>Note, however, that if you wish to do this many times, it is more efficient to make A the index first, and then use df.loc:</p>

<pre><code>df = df.set_index(['A'])
print(df.loc['foo'])
</code></pre>

<p>yields</p>

<pre><code>  A      B  C   D
foo    one  0   0
foo    two  2   4
foo    two  4   8
foo    one  6  12
foo  three  7  14
</code></pre>
";;"[""df.loc[df['column_name'] == some_value]\n"", ""df.loc[df['column_name'].isin(some_values)]\n"", ""import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n               'B': 'one one two three two two one three'.split(),\n               'C': np.arange(8), 'D': np.arange(8) * 2})\nprint(df)\n#      A      B  C   D\n# 0  foo    one  0   0\n# 1  bar    one  1   2\n# 2  foo    two  2   4\n# 3  bar  three  3   6\n# 4  foo    two  4   8\n# 5  bar    two  5  10\n# 6  foo    one  6  12\n# 7  foo  three  7  14\n\nprint(df.loc[df['A'] == 'foo'])\n"", '     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n', ""print(df.loc[df['B'].isin(['one','three'])])\n"", '      A      B  C   D\n0  foo    one  0   0\n1  bar    one  1   2\n3  bar  three  3   6\n6  foo    one  6  12\n7  foo  three  7  14\n', ""df = df.set_index(['A'])\nprint(df.loc['foo'])\n"", '  A      B  C   D\nfoo    one  0   0\nfoo    two  2   4\nfoo    two  4   8\nfoo    one  6  12\nfoo  three  7  14\n']";"[""df.loc[df['column_name'] == some_value]\n"", ""df.loc[df['column_name'].isin(some_values)]\n"", ""import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n               'B': 'one one two three two two one three'.split(),\n               'C': np.arange(8), 'D': np.arange(8) * 2})\nprint(df)\n#      A      B  C   D\n# 0  foo    one  0   0\n# 1  bar    one  1   2\n# 2  foo    two  2   4\n# 3  bar  three  3   6\n# 4  foo    two  4   8\n# 5  bar    two  5  10\n# 6  foo    one  6  12\n# 7  foo  three  7  14\n\nprint(df.loc[df['A'] == 'foo'])\n"", '     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n', ""print(df.loc[df['B'].isin(['one','three'])])\n"", '      A      B  C   D\n0  foo    one  0   0\n1  bar    one  1   2\n3  bar  three  3   6\n6  foo    one  6  12\n7  foo  three  7  14\n', ""df = df.set_index(['A'])\nprint(df.loc['foo'])\n"", '  A      B  C   D\nfoo    one  0   0\nfoo    two  2   4\nfoo    two  4   8\nfoo    one  6  12\nfoo  three  7  14\n']"
1236;;2;28150450;20297332.0;2;9;;;"<p>If the variable holding the dataframe is called df, then:</p>

<pre><code>len(df.columns)
</code></pre>

<p>gives the number of columns.</p>

<p>And for those who want the number of rows:</p>

<pre><code>len(df.index)
</code></pre>

<p>For a tuple containing the number of both rows and columns:</p>

<pre><code>df.shape
</code></pre>
";;['len(df.columns)\n', 'len(df.index)\n', 'df.shape\n'];['len(df.columns)\n', 'len(df.index)\n', 'df.shape\n']
1237;;2;28155580;25039626.0;2;38;;;"<p>You could use <code>select_dtypes</code> method of DataFrame. It includes two parameters include and exclude. So isNumeric would look like:</p>

<pre><code>numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']

newdf = df.select_dtypes(include=numerics)
</code></pre>
";;"[""numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnewdf = df.select_dtypes(include=numerics)\n""]";"['select_dtypes', ""numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnewdf = df.select_dtypes(include=numerics)\n""]"
1238;;2;28159296;11869910.0;2;47;;;"<p>Filters can be chained using a Pandas <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html"">query</a>:</p>

<pre><code>df = pd.DataFrame( np.random.randn(30,3), columns = ['a','b','c'])
df_filtered = df.query('a&gt;0').query('0&lt;b&lt;2')
</code></pre>

<p>Filters can also be combined in a single query:</p>

<pre><code>df_filtered = df.query('a&gt;0 and 0&lt;b&lt;2')
</code></pre>
";;"[""df = pd.DataFrame( np.random.randn(30,3), columns = ['a','b','c'])\ndf_filtered = df.query('a>0').query('0<b<2')\n"", ""df_filtered = df.query('a>0 and 0<b<2')\n""]";"[""df = pd.DataFrame( np.random.randn(30,3), columns = ['a','b','c'])\ndf_filtered = df.query('a>0').query('0<b<2')\n"", ""df_filtered = df.query('a>0 and 0<b<2')\n""]"
1239;;1;28182629;12680754.0;2;36;;;"<p>After painful experimentation to find something faster than the accepted answer, I got this to work. It ran around 100x faster on the dataset I tried it on.</p>

<p>If someone knows a way to make this more elegant, by all means please modify my code. I couldn't find a way that works without setting the other columns you want to keep as the index and then resetting the index and re-naming the columns, but I'd imagine there's something else that works.</p>

<pre><code>b = DataFrame(a.var1.str.split(',').tolist(), index=a.var2).stack()
b = b.reset_index()[[0, 'var2']] # var1 variable is currently labeled 0
b.columns = ['var1', 'var2'] # renaming var1
</code></pre>
";;"[""b = DataFrame(a.var1.str.split(',').tolist(), index=a.var2).stack()\nb = b.reset_index()[[0, 'var2']] # var1 variable is currently labeled 0\nb.columns = ['var1', 'var2'] # renaming var1\n""]";"[""b = DataFrame(a.var1.str.split(',').tolist(), index=a.var2).stack()\nb = b.reset_index()[[0, 'var2']] # var1 variable is currently labeled 0\nb.columns = ['var1', 'var2'] # renaming var1\n""]"
1240;;0;28192263;12207326.0;2;6;;;"<p>You can use list comprehension on a dataframe to count frequencies of the columns as such</p>

<pre><code>[my_series[c].value_counts() for c in list(my_series.select_dtypes(include=['O']).columns)]
</code></pre>

<p>Breakdown:</p>

<pre><code>my_series.select_dtypes(include=['O']) 
</code></pre>

<blockquote>
  <blockquote>
    <p>Selects just the categorical data</p>
  </blockquote>
</blockquote>

<pre><code>list(my_series.select_dtypes(include=['O']).columns) 
</code></pre>

<blockquote>
  <blockquote>
    <p>Turns the columns from above into a list</p>
  </blockquote>
</blockquote>

<pre><code>[my_series[c].value_counts() for c in list(my_series.select_dtypes(include=['O']).columns)] 
</code></pre>

<blockquote>
  <blockquote>
    <p>Iterates through the list above and applies value_counts() to each of the columns</p>
  </blockquote>
</blockquote>
";;"[""[my_series[c].value_counts() for c in list(my_series.select_dtypes(include=['O']).columns)]\n"", ""my_series.select_dtypes(include=['O']) \n"", ""list(my_series.select_dtypes(include=['O']).columns) \n"", ""[my_series[c].value_counts() for c in list(my_series.select_dtypes(include=['O']).columns)] \n""]";"[""[my_series[c].value_counts() for c in list(my_series.select_dtypes(include=['O']).columns)]\n"", ""my_series.select_dtypes(include=['O']) \n"", ""list(my_series.select_dtypes(include=['O']).columns) \n"", ""[my_series[c].value_counts() for c in list(my_series.select_dtypes(include=['O']).columns)] \n""]"
1241;;0;28218909;28218698.0;2;11;;;"<p>You can index dataframe columns by the position using <code>ix</code>.</p>

<pre><code>df1.ix[:,1]
</code></pre>

<p>This returns the first column for example. (0 would be the index)</p>

<pre><code>df1.ix[0,]
</code></pre>

<p>This returns the first row.</p>

<pre><code>df1.ix[:,1]
</code></pre>

<p>This would be the value at the intersection of row 0 and column 1:</p>

<pre><code>df1.ix[0,1]
</code></pre>

<p>and so on. So you can <code>enumerate()</code> <code>returns.keys():</code> and use the number to index the dataframe.</p>
";;['df1.ix[:,1]\n', 'df1.ix[0,]\n', 'df1.ix[:,1]\n', 'df1.ix[0,1]\n'];['ix', 'df1.ix[:,1]\n', 'df1.ix[0,]\n', 'df1.ix[:,1]\n', 'df1.ix[0,1]\n', 'enumerate()', 'returns.keys():']
1242;;6;28236391;28236305.0;2;23;;;"<p>It's quite common to use boolean indexing for this kind of task. With this method, you find out where column <code>a</code> is equal to <code>1</code> and then sum the corresponding rows of column <code>b</code>. You can use <code>loc</code> to handle the indexing:</p>

<pre><code>&gt;&gt;&gt; df.loc[df['a'] == 1, 'b'].sum()
15
</code></pre>

<p>The alternative approach is to use <code>groupby</code> to split the dataframe into parts according to the value in column <code>a</code>. You can then sum each part and pull out the value that the 1's added up to:</p>

<pre><code>&gt;&gt;&gt; df.groupby('a')['b'].sum()[1]
15
</code></pre>

<p>The <code>groupby</code> approach is much slower than using boolean indexing, but is useful if you want check the sums for other values in column <code>a</code>.</p>
";;"["">>> df.loc[df['a'] == 1, 'b'].sum()\n15\n"", "">>> df.groupby('a')['b'].sum()[1]\n15\n""]";"['a', '1', 'b', 'loc', "">>> df.loc[df['a'] == 1, 'b'].sum()\n15\n"", 'groupby', 'a', "">>> df.groupby('a')['b'].sum()[1]\n15\n"", 'groupby', 'a']"
1243;;2;28252957;13682044.0;2;19;;;"<p>i'd use the pandas replace function, very simple and powerful as you can use regex. Below i'm using the regex \D to remove any non-digit characters but obviously you could get quite creative with regex.</p>

<pre><code>data['result'].replace(regex=True,inplace=True,to_replace=r'\D',value=r'')
</code></pre>
";;"[""data['result'].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n""]";"[""data['result'].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n""]"
1244;;2;28312011;28311655.0;2;40;;;"<p>There's a flag for that:</p>

<pre><code>In [11]: df = pd.DataFrame([[""foo1""], [""foo2""], [""bar""], [np.nan]], columns=['a'])

In [12]: df.a.str.contains(""foo"")
Out[12]:
0     True
1     True
2    False
3      NaN
Name: a, dtype: object

In [13]: df.a.str.contains(""foo"", na=False)
Out[13]:
0     True
1     True
2    False
3    False
Name: a, dtype: bool
</code></pre>

<p>See the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.strings.StringMethods.replace.html""><code>str.replace</code></a> docs:</p>

<blockquote>
  <p>na : default NaN, fill value for missing values.</p>
</blockquote>

<hr>

<p>So you can do the following:</p>

<pre><code>In [21]: df.loc[df.a.str.contains(""foo"", na=False)]
Out[21]:
      a
0  foo1
1  foo2
</code></pre>
";;"['In [11]: df = pd.DataFrame([[""foo1""], [""foo2""], [""bar""], [np.nan]], columns=[\'a\'])\n\nIn [12]: df.a.str.contains(""foo"")\nOut[12]:\n0     True\n1     True\n2    False\n3      NaN\nName: a, dtype: object\n\nIn [13]: df.a.str.contains(""foo"", na=False)\nOut[13]:\n0     True\n1     True\n2    False\n3    False\nName: a, dtype: bool\n', 'In [21]: df.loc[df.a.str.contains(""foo"", na=False)]\nOut[21]:\n      a\n0  foo1\n1  foo2\n']";"['In [11]: df = pd.DataFrame([[""foo1""], [""foo2""], [""bar""], [np.nan]], columns=[\'a\'])\n\nIn [12]: df.a.str.contains(""foo"")\nOut[12]:\n0     True\n1     True\n2    False\n3      NaN\nName: a, dtype: object\n\nIn [13]: df.a.str.contains(""foo"", na=False)\nOut[13]:\n0     True\n1     True\n2    False\n3    False\nName: a, dtype: bool\n', 'str.replace', 'In [21]: df.loc[df.a.str.contains(""foo"", na=False)]\nOut[21]:\n      a\n0  foo1\n1  foo2\n']"
1245;;3;28371706;11622652.0;2;31;;;"<p>This is an older thread, but I just wanted to dump my workaround solution here. I initially tried the <code>chunksize</code> parameter (even with quite small values like 10000), but it didn't help much; had still technical issues with the memory size (my CSV was ~ 7.5 Gb). </p>

<p>Right now, I just read chunks of the CSV files in a for-loop approach and add them e.g., to an SQLite database step by step:</p>

<pre><code>import pandas as pd
import sqlite3
from pandas.io import sql
import subprocess

# In and output file paths
in_csv = '../data/my_large.csv'
out_sqlite = '../data/my.sqlite'

table_name = 'my_table' # name for the SQLite database table
chunksize = 100000 # number of lines to process at each iteration

# columns that should be read from the CSV file
columns = ['molecule_id','charge','db','drugsnow','hba','hbd','loc','nrb','smiles']

# Get number of lines in the CSV file
nlines = subprocess.check_output('wc -l %s' % in_csv, shell=True)
nlines = int(nlines.split()[0]) 

# connect to database
cnx = sqlite3.connect(out_sqlite)

# Iteratively read CSV and dump lines into the SQLite table
for i in range(0, nlines, chunksize):

    df = pd.read_csv(in_csv,  
            header=None,  # no header, define column header manually later
            nrows=chunksize, # number of rows to read at each iteration
            skiprows=i)   # skip rows that were already read

    # columns to read        
    df.columns = columns

    sql.to_sql(df, 
                name=table_name, 
                con=cnx, 
                index=False, # don't use CSV file index
                index_label='molecule_id', # use a unique column from DataFrame as index
                if_exists='append') 
cnx.close()    
</code></pre>
";;"[""import pandas as pd\nimport sqlite3\nfrom pandas.io import sql\nimport subprocess\n\n# In and output file paths\nin_csv = '../data/my_large.csv'\nout_sqlite = '../data/my.sqlite'\n\ntable_name = 'my_table' # name for the SQLite database table\nchunksize = 100000 # number of lines to process at each iteration\n\n# columns that should be read from the CSV file\ncolumns = ['molecule_id','charge','db','drugsnow','hba','hbd','loc','nrb','smiles']\n\n# Get number of lines in the CSV file\nnlines = subprocess.check_output('wc -l %s' % in_csv, shell=True)\nnlines = int(nlines.split()[0]) \n\n# connect to database\ncnx = sqlite3.connect(out_sqlite)\n\n# Iteratively read CSV and dump lines into the SQLite table\nfor i in range(0, nlines, chunksize):\n\n    df = pd.read_csv(in_csv,  \n            header=None,  # no header, define column header manually later\n            nrows=chunksize, # number of rows to read at each iteration\n            skiprows=i)   # skip rows that were already read\n\n    # columns to read        \n    df.columns = columns\n\n    sql.to_sql(df, \n                name=table_name, \n                con=cnx, \n                index=False, # don't use CSV file index\n                index_label='molecule_id', # use a unique column from DataFrame as index\n                if_exists='append') \ncnx.close()    \n""]";"['chunksize', ""import pandas as pd\nimport sqlite3\nfrom pandas.io import sql\nimport subprocess\n\n# In and output file paths\nin_csv = '../data/my_large.csv'\nout_sqlite = '../data/my.sqlite'\n\ntable_name = 'my_table' # name for the SQLite database table\nchunksize = 100000 # number of lines to process at each iteration\n\n# columns that should be read from the CSV file\ncolumns = ['molecule_id','charge','db','drugsnow','hba','hbd','loc','nrb','smiles']\n\n# Get number of lines in the CSV file\nnlines = subprocess.check_output('wc -l %s' % in_csv, shell=True)\nnlines = int(nlines.split()[0]) \n\n# connect to database\ncnx = sqlite3.connect(out_sqlite)\n\n# Iteratively read CSV and dump lines into the SQLite table\nfor i in range(0, nlines, chunksize):\n\n    df = pd.read_csv(in_csv,  \n            header=None,  # no header, define column header manually later\n            nrows=chunksize, # number of rows to read at each iteration\n            skiprows=i)   # skip rows that were already read\n\n    # columns to read        \n    df.columns = columns\n\n    sql.to_sql(df, \n                name=table_name, \n                con=cnx, \n                index=False, # don't use CSV file index\n                index_label='molecule_id', # use a unique column from DataFrame as index\n                if_exists='append') \ncnx.close()    \n""]"
1246;;3;28390992;26837998.0;2;95;;;"<p>Slightly shorter is:</p>

<pre><code>df = df.fillna('')
</code></pre>

<p>This will fill na's (e.g. NaN's) with ''.</p>

<p>Edit:
If you want to fill a single column, you can use:</p>

<pre><code>df.column1 = df.column1.fillna('')
</code></pre>
";;"[""df = df.fillna('')\n"", ""df.column1 = df.column1.fillna('')\n""]";"[""df = df.fillna('')\n"", ""df.column1 = df.column1.fillna('')\n""]"
1247;;3;28479181;24645153.0;2;15;;;"<p>Like this?</p>

<pre><code>dfTest = pd.DataFrame({
           'A':[14.00,90.20,90.95,96.27,91.21],
           'B':[103.02,107.26,110.35,114.23,114.68], 
           'C':['big','small','big','small','small']
         })
dfTest[['A','B']] = dfTest[['A','B']].apply(
                           lambda x: MinMaxScaler().fit_transform(x))
dfTest

    A           B           C
0   0.000000    0.000000    big
1   0.926219    0.363636    small
2   0.935335    0.628645    big
3   1.000000    0.961407    small
4   0.938495    1.000000    small
</code></pre>
";;"[""dfTest = pd.DataFrame({\n           'A':[14.00,90.20,90.95,96.27,91.21],\n           'B':[103.02,107.26,110.35,114.23,114.68], \n           'C':['big','small','big','small','small']\n         })\ndfTest[['A','B']] = dfTest[['A','B']].apply(\n                           lambda x: MinMaxScaler().fit_transform(x))\ndfTest\n\n    A           B           C\n0   0.000000    0.000000    big\n1   0.926219    0.363636    small\n2   0.935335    0.628645    big\n3   1.000000    0.961407    small\n4   0.938495    1.000000    small\n""]";"[""dfTest = pd.DataFrame({\n           'A':[14.00,90.20,90.95,96.27,91.21],\n           'B':[103.02,107.26,110.35,114.23,114.68], \n           'C':['big','small','big','small','small']\n         })\ndfTest[['A','B']] = dfTest[['A','B']].apply(\n                           lambda x: MinMaxScaler().fit_transform(x))\ndfTest\n\n    A           B           C\n0   0.000000    0.000000    big\n1   0.926219    0.363636    small\n2   0.935335    0.628645    big\n3   1.000000    0.961407    small\n4   0.938495    1.000000    small\n""]"
1248;;2;28507257;26873127.0;2;20;;;"<p>Posting @joris 's comment as an answer:</p>

<blockquote>
  <p>display(df) (with from IPython.display import display), or print df.to_html()</p>
</blockquote>

<hr>

<p>Or in code :</p>

<pre><code>from IPython.display import display
display(df)  # OR
print df.to_html()
</code></pre>
";;['from IPython.display import display\ndisplay(df)  # OR\nprint df.to_html()\n'];['from IPython.display import display\ndisplay(df)  # OR\nprint df.to_html()\n']
1249;;2;28541443;23307301.0;2;45;;;"<p>You can edit a subset of a dataframe by using loc:</p>

<pre><code>df.loc[&lt;row selection&gt;, &lt;column selection&gt;]
</code></pre>

<p>In this case:</p>

<pre><code>w.loc[w.female != 'female', 'female'] = 0
w.loc[w.female == 'female', 'female'] = 1
</code></pre>
";;"['df.loc[<row selection>, <column selection>]\n', ""w.loc[w.female != 'female', 'female'] = 0\nw.loc[w.female == 'female', 'female'] = 1\n""]";"['df.loc[<row selection>, <column selection>]\n', ""w.loc[w.female != 'female', 'female'] = 0\nw.loc[w.female == 'female', 'female'] = 1\n""]"
1250;;7;28648923;15891038.0;2;270;;;"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html"" rel=""noreferrer""><code>pd.to_numeric</code></a> (introduced in version 0.17) to convert a column or a Series to a numeric type. The function can also be applied over multiple columns of a DataFrame using <code>apply</code>.</p>

<p>Importantly, the function also takes an <code>errors</code> key word argument that lets you force not-numeric values to be <code>NaN</code>, or simply ignore columns containing these values.</p>

<p>Example uses are shown below.</p>

<h3>Individual column / Series</h3>

<p>Here's an example using a Series of strings <code>s</code> which has the object dtype:</p>

<pre><code>&gt;&gt;&gt; s = pd.Series(['1', '2', '4.7', 'pandas', '10'])
&gt;&gt;&gt; s
0         1
1         2
2       4.7
3    pandas
4        10
dtype: object
</code></pre>

<p>The function's default behaviour is to raise if it can't convert a value. In this case, it can't cope with the string 'pandas':</p>

<pre><code>&gt;&gt;&gt; pd.to_numeric(s) # or pd.to_numeric(s, errors='raise')
ValueError: Unable to parse string
</code></pre>

<p>Rather than fail, we might want 'pandas' to be considered a missing/bad value. We can coerce invalid values to <code>NaN</code> as follows:</p>

<pre><code>&gt;&gt;&gt; pd.to_numeric(s, errors='coerce')
0     1.0
1     2.0
2     4.7
3     NaN
4    10.0
dtype: float64
</code></pre>

<p>The third option is just to ignore the operation if an invalid value is encountered:</p>

<pre><code>&gt;&gt;&gt; pd.to_numeric(s, errors='ignore')
# the original Series is returned untouched
</code></pre>

<h3>Multiple columns / entire DataFrames</h3>

<p>We might want to apply this operation to multiple columns. Processing each column in turn is tedious, so we can use <code>DataFrame.apply</code> to have the function act on each column.</p>

<p>Borrowing the DataFrame from the question:</p>

<pre><code>&gt;&gt;&gt; a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]
&gt;&gt;&gt; df = pd.DataFrame(a, columns=['col1','col2','col3'])
&gt;&gt;&gt; df
  col1 col2  col3
0    a  1.2   4.2
1    b   70  0.03
2    x    5     0
</code></pre>

<p>Then we can write:</p>

<pre><code>df[['col2','col3']] = df[['col2','col3']].apply(pd.to_numeric)
</code></pre>

<p>and now 'col2' and 'col3' have dtype <code>float64</code> as desired.</p>

<p>However, we might not know which of our columns can be converted reliably to a numeric type. In that case we can just write:</p>

<pre><code>df.apply(pd.to_numeric, errors='ignore')
</code></pre>

<p>Then the function will be applied to the <em>whole</em> DataFrame. Columns that can be converted to a numeric type will be converted, while columns that cannot (e.g. they contain non-digit strings or dates) will be left alone.</p>

<p>There is also <code>pd.to_datetime</code> and <code>pd.to_timedelta</code> for conversion to dates and timestamps.</p>

<h3>Soft conversions</h3>

<p>Version 0.21.0 introduces the method <code>infer_objects()</code> for converting columns of a DataFrame that have an object datatype to a more specific type.</p>

<p>For example, let's create a DataFrame with two columns of object type, with one holding integers and the other holding strings of integers:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'a': [7, 1, 5], 'b': ['3','2','1']}, dtype='object')
&gt;&gt;&gt; df.dtypes
a    object
b    object
dtype: object
</code></pre>

<p>Then using <code>infer_objects()</code>, we can change the type of column 'a' to int64:</p>

<pre><code>&gt;&gt;&gt; df = df.infer_objects()
&gt;&gt;&gt; df.dtypes
a     int64
b    object
dtype: object
</code></pre>

<p>Column 'b' has been left alone since its values were strings, not integers. If we wanted to try and force the conversion of both columns to an integer type, we could use <code>df.astype(int)</code> instead.</p>
";;"["">>> s = pd.Series(['1', '2', '4.7', 'pandas', '10'])\n>>> s\n0         1\n1         2\n2       4.7\n3    pandas\n4        10\ndtype: object\n"", "">>> pd.to_numeric(s) # or pd.to_numeric(s, errors='raise')\nValueError: Unable to parse string\n"", "">>> pd.to_numeric(s, errors='coerce')\n0     1.0\n1     2.0\n2     4.7\n3     NaN\n4    10.0\ndtype: float64\n"", "">>> pd.to_numeric(s, errors='ignore')\n# the original Series is returned untouched\n"", "">>> a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\n>>> df = pd.DataFrame(a, columns=['col1','col2','col3'])\n>>> df\n  col1 col2  col3\n0    a  1.2   4.2\n1    b   70  0.03\n2    x    5     0\n"", ""df[['col2','col3']] = df[['col2','col3']].apply(pd.to_numeric)\n"", ""df.apply(pd.to_numeric, errors='ignore')\n"", "">>> df = pd.DataFrame({'a': [7, 1, 5], 'b': ['3','2','1']}, dtype='object')\n>>> df.dtypes\na    object\nb    object\ndtype: object\n"", '>>> df = df.infer_objects()\n>>> df.dtypes\na     int64\nb    object\ndtype: object\n']";"['pd.to_numeric', 'apply', 'errors', 'NaN', 's', "">>> s = pd.Series(['1', '2', '4.7', 'pandas', '10'])\n>>> s\n0         1\n1         2\n2       4.7\n3    pandas\n4        10\ndtype: object\n"", "">>> pd.to_numeric(s) # or pd.to_numeric(s, errors='raise')\nValueError: Unable to parse string\n"", 'NaN', "">>> pd.to_numeric(s, errors='coerce')\n0     1.0\n1     2.0\n2     4.7\n3     NaN\n4    10.0\ndtype: float64\n"", "">>> pd.to_numeric(s, errors='ignore')\n# the original Series is returned untouched\n"", 'DataFrame.apply', "">>> a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\n>>> df = pd.DataFrame(a, columns=['col1','col2','col3'])\n>>> df\n  col1 col2  col3\n0    a  1.2   4.2\n1    b   70  0.03\n2    x    5     0\n"", ""df[['col2','col3']] = df[['col2','col3']].apply(pd.to_numeric)\n"", 'float64', ""df.apply(pd.to_numeric, errors='ignore')\n"", 'pd.to_datetime', 'pd.to_timedelta', 'infer_objects()', "">>> df = pd.DataFrame({'a': [7, 1, 5], 'b': ['3','2','1']}, dtype='object')\n>>> df.dtypes\na    object\nb    object\ndtype: object\n"", 'infer_objects()', '>>> df = df.infer_objects()\n>>> df.dtypes\na     int64\nb    object\ndtype: object\n', 'df.astype(int)']"
1251;;1;28680078;28679930.0;2;39;;;"<p>pandas has vectorized string operations, so you can just filter out the rows that contain the string you don't want:</p>

<pre><code>In [91]: df = pd.DataFrame(dict(A=[5,3,5,6], C=[""foo"",""bar"",""fooXYZbar"", ""bat""]))

In [92]: df
Out[92]:
   A          C
0  5        foo
1  3        bar
2  5  fooXYZbar
3  6        bat

In [93]: df[df.C.str.contains(""XYZ"") == False]
Out[93]:
   A    C
0  5  foo
1  3  bar
3  6  bat
</code></pre>
";;"['In [91]: df = pd.DataFrame(dict(A=[5,3,5,6], C=[""foo"",""bar"",""fooXYZbar"", ""bat""]))\n\nIn [92]: df\nOut[92]:\n   A          C\n0  5        foo\n1  3        bar\n2  5  fooXYZbar\n3  6        bat\n\nIn [93]: df[df.C.str.contains(""XYZ"") == False]\nOut[93]:\n   A    C\n0  5  foo\n1  3  bar\n3  6  bat\n']";"['In [91]: df = pd.DataFrame(dict(A=[5,3,5,6], C=[""foo"",""bar"",""fooXYZbar"", ""bat""]))\n\nIn [92]: df\nOut[92]:\n   A          C\n0  5        foo\n1  3        bar\n2  5  fooXYZbar\n3  6        bat\n\nIn [93]: df[df.C.str.contains(""XYZ"") == False]\nOut[93]:\n   A    C\n0  5  foo\n1  3  bar\n3  6  bat\n']"
1252;;0;28756099;25537399.0;2;12;;;"<p>I set up a direct comparison to test them, found that their assumptions can <a href=""https://stackoverflow.com/questions/28755617/why-do-r-and-statsmodels-give-slightly-different-anova-results"" title=""differ slightly"">differ slightly</a> , got a hint from a statistician, and here is an example of ANOVA on a pandas dataframe matching R's results:</p>

<pre><code>import pandas as pd
import statsmodels.api as sm
from statsmodels.formula.api import ols


# R code on R sample dataset

#&gt; anova(with(ChickWeight, lm(weight ~ Time + Diet)))
#Analysis of Variance Table
#
#Response: weight
#           Df  Sum Sq Mean Sq  F value    Pr(&gt;F)
#Time        1 2042344 2042344 1576.460 &lt; 2.2e-16 ***
#Diet        3  129876   43292   33.417 &lt; 2.2e-16 ***
#Residuals 573  742336    1296
#write.csv(file='ChickWeight.csv', x=ChickWeight, row.names=F)

cw = pd.read_csv('ChickWeight.csv')

cw_lm=ols('weight ~ Time + C(Diet)', data=cw).fit() #Specify C for Categorical
print(sm.stats.anova_lm(cw_lm, typ=2))
#                  sum_sq   df            F         PR(&gt;F)
#C(Diet)    129876.056995    3    33.416570   6.473189e-20
#Time      2016357.148493    1  1556.400956  1.803038e-165
#Residual   742336.119560  573          NaN            NaN
</code></pre>
";;"[""import pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n\n# R code on R sample dataset\n\n#> anova(with(ChickWeight, lm(weight ~ Time + Diet)))\n#Analysis of Variance Table\n#\n#Response: weight\n#           Df  Sum Sq Mean Sq  F value    Pr(>F)\n#Time        1 2042344 2042344 1576.460 < 2.2e-16 ***\n#Diet        3  129876   43292   33.417 < 2.2e-16 ***\n#Residuals 573  742336    1296\n#write.csv(file='ChickWeight.csv', x=ChickWeight, row.names=F)\n\ncw = pd.read_csv('ChickWeight.csv')\n\ncw_lm=ols('weight ~ Time + C(Diet)', data=cw).fit() #Specify C for Categorical\nprint(sm.stats.anova_lm(cw_lm, typ=2))\n#                  sum_sq   df            F         PR(>F)\n#C(Diet)    129876.056995    3    33.416570   6.473189e-20\n#Time      2016357.148493    1  1556.400956  1.803038e-165\n#Residual   742336.119560  573          NaN            NaN\n""]";"[""import pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n\n# R code on R sample dataset\n\n#> anova(with(ChickWeight, lm(weight ~ Time + Diet)))\n#Analysis of Variance Table\n#\n#Response: weight\n#           Df  Sum Sq Mean Sq  F value    Pr(>F)\n#Time        1 2042344 2042344 1576.460 < 2.2e-16 ***\n#Diet        3  129876   43292   33.417 < 2.2e-16 ***\n#Residuals 573  742336    1296\n#write.csv(file='ChickWeight.csv', x=ChickWeight, row.names=F)\n\ncw = pd.read_csv('ChickWeight.csv')\n\ncw_lm=ols('weight ~ Time + C(Diet)', data=cw).fit() #Specify C for Categorical\nprint(sm.stats.anova_lm(cw_lm, typ=2))\n#                  sum_sq   df            F         PR(>F)\n#C(Diet)    129876.056995    3    33.416570   6.473189e-20\n#Time      2016357.148493    1  1556.400956  1.803038e-165\n#Residual   742336.119560  573          NaN            NaN\n""]"
1253;;1;28847219;14734533.0;2;12;;;"<p>Rather than</p>

<pre><code>gb.get_group('foo')
</code></pre>

<p>I prefer using <code>gb.groups</code></p>

<pre><code>df.loc[gb.groups['foo']]
</code></pre>

<p>Because in this way you can choose multiple columns as well. for example:</p>

<pre><code>df.loc[gb.groups['foo'],('A','B')]
</code></pre>
";;"[""gb.get_group('foo')\n"", ""df.loc[gb.groups['foo']]\n"", ""df.loc[gb.groups['foo'],('A','B')]\n""]";"[""gb.get_group('foo')\n"", 'gb.groups', ""df.loc[gb.groups['foo']]\n"", ""df.loc[gb.groups['foo'],('A','B')]\n""]"
1254;;1;28881373;13331698.0;2;9;;;"<p>The method you are looking for is Series.combine. 
However, it seems some care has to be taken around datatypes. 
In your example, you would (as I did when testing the answer) naively call </p>

<pre><code>df['col_3'] = df.col_1.combine(df.col_2, func=get_sublist)
</code></pre>

<p>However, this throws the error: </p>

<pre><code>ValueError: setting an array element with a sequence.
</code></pre>

<p>My best guess is that it seems to expect the result to be of the same type as the series calling the method (df.col_1 here). However, the following works:</p>

<pre><code>df['col_3'] = df.col_1.astype(object).combine(df.col_2, func=get_sublist)

df

   ID   col_1   col_2   col_3
0   1   0   1   [a, b]
1   2   2   4   [c, d, e]
2   3   3   5   [d, e, f]
</code></pre>
";;"[""df['col_3'] = df.col_1.combine(df.col_2, func=get_sublist)\n"", 'ValueError: setting an array element with a sequence.\n', ""df['col_3'] = df.col_1.astype(object).combine(df.col_2, func=get_sublist)\n\ndf\n\n   ID   col_1   col_2   col_3\n0   1   0   1   [a, b]\n1   2   2   4   [c, d, e]\n2   3   3   5   [d, e, f]\n""]";"[""df['col_3'] = df.col_1.combine(df.col_2, func=get_sublist)\n"", 'ValueError: setting an array element with a sequence.\n', ""df['col_3'] = df.col_1.astype(object).combine(df.col_2, func=get_sublist)\n\ndf\n\n   ID   col_1   col_2   col_3\n0   1   0   1   [a, b]\n1   2   2   4   [c, d, e]\n2   3   3   5   [d, e, f]\n""]"
1255;;6;28902170;28901683.0;2;46;;;"<p>One method would be to store the result of an inner merge form both dfs, then we can simply select the rows when one column's values are not in this common:</p>

<pre><code>In [119]:

common = df1.merge(df2,on=['col1','col2'])
print(common)
df1[(~df1.col1.isin(common.col1))&amp;(~df1.col2.isin(common.col2))]
   col1  col2
0     1    10
1     2    11
2     3    12
Out[119]:
   col1  col2
3     4    13
4     5    14
</code></pre>

<p><strong>EDIT</strong></p>

<p>Another method as you've found is to use <code>isin</code> which will produce <code>NaN</code> rows which you can drop:</p>

<pre><code>In [138]:

df1[~df1.isin(df2)].dropna()
Out[138]:
   col1  col2
3     4    13
4     5    14
</code></pre>

<p>However if df2 does not start rows in the same manner then this won't work:</p>

<pre><code>df2 = pd.DataFrame(data = {'col1' : [2, 3,4], 'col2' : [11, 12,13]})
</code></pre>

<p>will produce the entire df:</p>

<pre><code>In [140]:

df1[~df1.isin(df2)].dropna()
Out[140]:
   col1  col2
0     1    10
1     2    11
2     3    12
3     4    13
4     5    14
</code></pre>
";;"[""In [119]:\n\ncommon = df1.merge(df2,on=['col1','col2'])\nprint(common)\ndf1[(~df1.col1.isin(common.col1))&(~df1.col2.isin(common.col2))]\n   col1  col2\n0     1    10\n1     2    11\n2     3    12\nOut[119]:\n   col1  col2\n3     4    13\n4     5    14\n"", 'In [138]:\n\ndf1[~df1.isin(df2)].dropna()\nOut[138]:\n   col1  col2\n3     4    13\n4     5    14\n', ""df2 = pd.DataFrame(data = {'col1' : [2, 3,4], 'col2' : [11, 12,13]})\n"", 'In [140]:\n\ndf1[~df1.isin(df2)].dropna()\nOut[140]:\n   col1  col2\n0     1    10\n1     2    11\n2     3    12\n3     4    13\n4     5    14\n']";"[""In [119]:\n\ncommon = df1.merge(df2,on=['col1','col2'])\nprint(common)\ndf1[(~df1.col1.isin(common.col1))&(~df1.col2.isin(common.col2))]\n   col1  col2\n0     1    10\n1     2    11\n2     3    12\nOut[119]:\n   col1  col2\n3     4    13\n4     5    14\n"", 'isin', 'NaN', 'In [138]:\n\ndf1[~df1.isin(df2)].dropna()\nOut[138]:\n   col1  col2\n3     4    13\n4     5    14\n', ""df2 = pd.DataFrame(data = {'col1' : [2, 3,4], 'col2' : [11, 12,13]})\n"", 'In [140]:\n\ndf1[~df1.isin(df2)].dropna()\nOut[140]:\n   col1  col2\n0     1    10\n1     2    11\n2     3    12\n3     4    13\n4     5    14\n']"
1256;;6;28931750;28931224.0;2;49;;;"<p>Firstly <code>freq_series.plot</code> returns an axis <em>not</em> a figure so to make my answer a little more clear I've changed your given code to refer to it as <code>ax</code> rather than <code>fig</code> to be more consistent with other code examples.</p>

<p>You can get the list of the bars produced in the plot from the <code>ax.patches</code> member. Then you can use the technique demonstrated in <a href=""http://matplotlib.org/examples/api/barchart_demo.html"">this matplotlib gallery example</a> to add the labels using the <a href=""http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.text""><code>ax.text</code></a> method.</p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

frequencies = [6, 16, 75, 160, 244, 260, 145, 73, 16, 4, 1]   # bring some raw data

freq_series = pd.Series.from_array(frequencies)   # in my original code I create a series and run on that, so for consistency I create a series from the list.

x_labels = [108300.0, 110540.0, 112780.0, 115020.0, 117260.0, 119500.0, 121740.0, 123980.0, 126220.0, 128460.0, 130700.0]

# now to plot the figure...
plt.figure(figsize=(12, 8))
ax = freq_series.plot(kind='bar')
ax.set_title(""Amount Frequency"")
ax.set_xlabel(""Amount ($)"")
ax.set_ylabel(""Frequency"")
ax.set_xticklabels(x_labels)

rects = ax.patches

# Now make some labels
labels = [""label%d"" % i for i in xrange(len(rects))]

for rect, label in zip(rects, labels):
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')

plt.savefig(""image.png"")
</code></pre>

<p>This produces a labelled plot that looks like:</p>

<p><img src=""https://i.stack.imgur.com/0RMaz.png"" alt=""enter image description here""></p>
";;"['import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrequencies = [6, 16, 75, 160, 244, 260, 145, 73, 16, 4, 1]   # bring some raw data\n\nfreq_series = pd.Series.from_array(frequencies)   # in my original code I create a series and run on that, so for consistency I create a series from the list.\n\nx_labels = [108300.0, 110540.0, 112780.0, 115020.0, 117260.0, 119500.0, 121740.0, 123980.0, 126220.0, 128460.0, 130700.0]\n\n# now to plot the figure...\nplt.figure(figsize=(12, 8))\nax = freq_series.plot(kind=\'bar\')\nax.set_title(""Amount Frequency"")\nax.set_xlabel(""Amount ($)"")\nax.set_ylabel(""Frequency"")\nax.set_xticklabels(x_labels)\n\nrects = ax.patches\n\n# Now make some labels\nlabels = [""label%d"" % i for i in xrange(len(rects))]\n\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha=\'center\', va=\'bottom\')\n\nplt.savefig(""image.png"")\n']";"['freq_series.plot', 'ax', 'fig', 'ax.patches', 'ax.text', 'import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrequencies = [6, 16, 75, 160, 244, 260, 145, 73, 16, 4, 1]   # bring some raw data\n\nfreq_series = pd.Series.from_array(frequencies)   # in my original code I create a series and run on that, so for consistency I create a series from the list.\n\nx_labels = [108300.0, 110540.0, 112780.0, 115020.0, 117260.0, 119500.0, 121740.0, 123980.0, 126220.0, 128460.0, 130700.0]\n\n# now to plot the figure...\nplt.figure(figsize=(12, 8))\nax = freq_series.plot(kind=\'bar\')\nax.set_title(""Amount Frequency"")\nax.set_xlabel(""Amount ($)"")\nax.set_ylabel(""Frequency"")\nax.set_xticklabels(x_labels)\n\nrects = ax.patches\n\n# Now make some labels\nlabels = [""label%d"" % i for i in xrange(len(rects))]\n\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha=\'center\', va=\'bottom\')\n\nplt.savefig(""image.png"")\n']"
1257;;0;28991603;17465045.0;2;9;;;"<p>Perhaps the pandas interface has changed since @Rutger answered, but in the version I'm using (0.15.2), the <code>date_parser</code> function receives a list of dates instead of a single value. In this case, his code should be updated like so:</p>

<pre><code>dateparse = lambda dates: [pd.datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in dates]

df = pd.read_csv(infile, parse_dates=['datetime'], date_parser=dateparse)
</code></pre>
";;"[""dateparse = lambda dates: [pd.datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in dates]\n\ndf = pd.read_csv(infile, parse_dates=['datetime'], date_parser=dateparse)\n""]";"['date_parser', ""dateparse = lambda dates: [pd.datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in dates]\n\ndf = pd.read_csv(infile, parse_dates=['datetime'], date_parser=dateparse)\n""]"
1258;;8;29036738;27365467.0;2;61;;;"<p>Given this df:</p>

<pre><code>        date
0 2001-08-10
1 2002-08-31
2 2003-08-29
3 2006-06-21
4 2002-03-27
5 2003-07-14
6 2004-06-15
7 2003-08-14
8 2003-07-29
</code></pre>

<p>and, if it's not already the case:</p>

<pre><code>df[""date""] = df[""date""].astype(""datetime64"")
</code></pre>

<p>To show the count of dates by month: </p>

<pre><code>df.groupby(df[""date""].dt.month).count().plot(kind=""bar"")
</code></pre>

<p><code>.dt</code> allows you to access the datetime properties.</p>

<p>Which will give you:</p>

<p><img src=""https://i.stack.imgur.com/eom7L.png"" alt=""groupby date month""></p>

<p>You can replace month by year, day, etc..</p>

<p>If you want to distinguish year and month for instance, just do:</p>

<pre><code>df.groupby([df[""date""].dt.year, df[""date""].dt.month]).count().plot(kind=""bar"")
</code></pre>

<p>Which gives:</p>

<p><img src=""https://i.stack.imgur.com/wHObD.png"" alt=""groupby date month year""></p>

<p>Was it what you wanted ? Is this clear ?</p>

<p>Hope this helps !</p>
";;"['        date\n0 2001-08-10\n1 2002-08-31\n2 2003-08-29\n3 2006-06-21\n4 2002-03-27\n5 2003-07-14\n6 2004-06-15\n7 2003-08-14\n8 2003-07-29\n', 'df[""date""] = df[""date""].astype(""datetime64"")\n', 'df.groupby(df[""date""].dt.month).count().plot(kind=""bar"")\n', 'df.groupby([df[""date""].dt.year, df[""date""].dt.month]).count().plot(kind=""bar"")\n']";"['        date\n0 2001-08-10\n1 2002-08-31\n2 2003-08-29\n3 2006-06-21\n4 2002-03-27\n5 2003-07-14\n6 2004-06-15\n7 2003-08-14\n8 2003-07-29\n', 'df[""date""] = df[""date""].astype(""datetime64"")\n', 'df.groupby(df[""date""].dt.month).count().plot(kind=""bar"")\n', '.dt', 'df.groupby([df[""date""].dt.year, df[""date""].dt.month]).count().plot(kind=""bar"")\n']"
1259;;7;29074073;11707586.0;2;15;;;"<p>You can set the output display to match your current terminal width:</p>

<pre><code>pd.set_option('display.width', pd.util.terminal.get_terminal_size()[0])
</code></pre>
";;"[""pd.set_option('display.width', pd.util.terminal.get_terminal_size()[0])\n""]";"[""pd.set_option('display.width', pd.util.terminal.get_terminal_size()[0])\n""]"
1260;;1;29108799;12065885.0;2;29;;;"<p>You can also directly <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html"" rel=""noreferrer"">query</a> your DataFrame for this information.</p>

<pre><code>rpt.query('STK_ID in (600809,600141,600329)')
</code></pre>

<p>Or similarly search for ranges:</p>

<pre><code>rpt.query('60000 &lt; STK_ID &lt; 70000')
</code></pre>
";;"[""rpt.query('STK_ID in (600809,600141,600329)')\n"", ""rpt.query('60000 < STK_ID < 70000')\n""]";"[""rpt.query('STK_ID in (600809,600141,600329)')\n"", ""rpt.query('60000 < STK_ID < 70000')\n""]"
1261;;0;29177664;29177498.0;2;42;;;"<p>Assuming your DataFrame is in <code>df</code>:</p>

<pre><code>df.Temp_Rating.fillna(df.Farheit, inplace=True)
del df['Farheit']
df.columns = 'File heat Observations'.split()
</code></pre>

<p>First replace any <code>NaN</code> values with the corresponding value of <code>df.Farheit</code>. Delete the <code>'Farheit'</code> column. Then rename the columns. Here's the resulting <code>DataFrame</code>:</p>

<p><img src=""https://i.stack.imgur.com/jhdfe.png"" alt=""resulting DataFrame""></p>
";;"[""df.Temp_Rating.fillna(df.Farheit, inplace=True)\ndel df['Farheit']\ndf.columns = 'File heat Observations'.split()\n""]";"['df', ""df.Temp_Rating.fillna(df.Farheit, inplace=True)\ndel df['Farheit']\ndf.columns = 'File heat Observations'.split()\n"", 'NaN', 'df.Farheit', ""'Farheit'"", 'DataFrame']"
1262;;7;29233999;29226210.0;2;26;;;"<p>Using spark to read in a CSV file to <code>pandas</code> is quite a roundabout method for achieving the end goal of reading a CSV file into memory.</p>

<p>It seems like you might be misunderstanding the use cases of the technologies in play here.</p>

<p>Spark is for distributed computing (though it can be used locally). It's generally far too heavyweight to be used for simply reading in a CSV file.</p>

<p>In your example, the <code>sc.textFile</code> method will simply give you a spark RDD that is effectively a list of text lines. This likely isn't what you want. No type inference will be performed, so if you want to sum a column of numbers in your CSV file, you won't be able to because they are still strings as far as Spark is concerned.</p>

<p>Just use <code>pandas.read_csv</code> and read the whole CSV into memory. Pandas will automatically infer the type of each column. Spark doesn't do this.</p>

<p>Now to answer your questions:</p>

<p><strong>Does it store the Pandas object to local memory</strong>:</p>

<p>Yes. <code>toPandas()</code> will convert the Spark DataFrame into a Pandas DataFrame, which is of course in memory.</p>

<p><strong>Does Pandas low-level computation handled all by Spark</strong></p>

<p>No. Pandas runs its own computations, there's no interplay between spark and pandas, there's simply <em>some</em> API compatibility.</p>

<p><strong>Does it exposed all pandas dataframe functionality?</strong></p>

<p>No. For example, <code>Series</code> objects have an <code>interpolate</code> method which isn't available in PySpark <code>Column</code> objects. There are many many methods and functions that are in the pandas API that are not in the PySpark API.</p>

<p><strong>Can I convert it toPandas and just be done with it, without so much touching DataFrame API?</strong></p>

<p>Absolutely. In fact, you probably shouldn't even use Spark at all in this case. <code>pandas.read_csv</code> will likely handle your use case unless you're working with a <em>huge</em> amount of data.</p>

<p>Try to solve your problem with simple, low-tech, easy-to-understand libraries, and <em>only</em> go to something more complicated as you need it. Many times, you won't need the more complex technology.</p>
";;[];['pandas', 'sc.textFile', 'pandas.read_csv', 'toPandas()', 'Series', 'interpolate', 'Column', 'pandas.read_csv']
1263;;1;29247205;11869910.0;2;6;;;"<p>I had the same question except that I wanted to combine the criteria into an OR condition.  The format given by Wouter Overmeire combines the criteria into an AND condition such that both must be satisfied:</p>

<pre><code>In [96]: df
Out[96]:
   A  B  C  D
a  1  4  9  1
b  4  5  0  2
c  5  5  1  0
d  1  3  9  6

In [99]: df[(df.A == 1) &amp; (df.D == 6)]
Out[99]:
   A  B  C  D
d  1  3  9  6
</code></pre>

<p>But I found that, if you wrap each condition in <code>(... == True)</code> and join the criteria with a pipe, the criteria are combined in an OR condition, satisfied whenever either of them is true: </p>

<pre><code>df[((df.A==1) == True) | ((df.D==6) == True)]
</code></pre>
";;['In [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [99]: df[(df.A == 1) & (df.D == 6)]\nOut[99]:\n   A  B  C  D\nd  1  3  9  6\n', 'df[((df.A==1) == True) | ((df.D==6) == True)]\n'];['In [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [99]: df[(df.A == 1) & (df.D == 6)]\nOut[99]:\n   A  B  C  D\nd  1  3  9  6\n', '(... == True)', 'df[((df.A==1) == True) | ((df.D==6) == True)]\n']
1264;;3;29262040;23330654.0;2;42;;;"<p>You can assign values in the loop using df.set_value:</p>

<pre><code>for i, row in df.iterrows():
  ifor_val = something
  if &lt;condition&gt;:
    ifor_val = something_else
  df.set_value(i,'ifor',ifor_val)
</code></pre>

<p>if you don't need the row values you could simply iterate over the indices of df, but I kept the original for-loop in case you need the row value for something not shown here.  </p>
";;"[""for i, row in df.iterrows():\n  ifor_val = something\n  if <condition>:\n    ifor_val = something_else\n  df.set_value(i,'ifor',ifor_val)\n""]";"[""for i, row in df.iterrows():\n  ifor_val = something\n  if <condition>:\n    ifor_val = something_else\n  df.set_value(i,'ifor',ifor_val)\n""]"
1265;;0;29281494;26187759.0;2;20;;;"<p>Ivan's answer is great, but it looks like it can be slightly simplified, also removing the need to depend on joblib:</p>

<pre><code>from multiprocessing import Pool, cpu_count

def applyParallel(dfGrouped, func):
    with Pool(cpu_count()) as p:
        ret_list = p.map(func, [group for name, group in dfGrouped])
    return pandas.concat(ret_list)
</code></pre>

<p>By the way: this can not replace <em>any</em> groupby.apply(), but it will cover the typical cases: e.g. it should cover cases 2 and 3 <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.apply.html"" rel=""noreferrer"" title=""typical cases"">in the documentation</a>, while you should obtain the behaviour of case 1 by giving the argument <code>axis=1</code> to the final <code>pandas.concat()</code> call.</p>
";;['from multiprocessing import Pool, cpu_count\n\ndef applyParallel(dfGrouped, func):\n    with Pool(cpu_count()) as p:\n        ret_list = p.map(func, [group for name, group in dfGrouped])\n    return pandas.concat(ret_list)\n'];['from multiprocessing import Pool, cpu_count\n\ndef applyParallel(dfGrouped, func):\n    with Pool(cpu_count()) as p:\n        ret_list = p.map(func, [group for name, group in dfGrouped])\n    return pandas.concat(ret_list)\n', 'axis=1', 'pandas.concat()']
1266;;0;29287549;29287224.0;2;41;;;"<p>In order to read a csv in that doesn't have a header and for only certain columns you need to pass params <code>header=None</code> and <code>usecols=[3,6]</code> for the 4th and 7th columns:</p>

<pre><code>df = pd.read_csv(file_path, header=None, usecols=[3,6])
</code></pre>

<p>See the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv"" rel=""noreferrer"">docs</a></p>
";;['df = pd.read_csv(file_path, header=None, usecols=[3,6])\n'];['header=None', 'usecols=[3,6]', 'df = pd.read_csv(file_path, header=None, usecols=[3,6])\n']
1267;;0;29319200;14940743.0;2;113;;;"<p>You can either Drop the columns you do not need OR Select the ones you need</p>

<pre><code>    ##Using DataFrame.drop
    df.drop(df.columns[[1, 2]], axis=1, inplace=True)

    # drop by Name
    df1 = df1.drop(['B', 'C'], axis=1)


    ## Select the ones you want
    df1 = df[['a','d']]
</code></pre>
";;"[""    ##Using DataFrame.drop\n    df.drop(df.columns[[1, 2]], axis=1, inplace=True)\n\n    # drop by Name\n    df1 = df1.drop(['B', 'C'], axis=1)\n\n\n    ## Select the ones you want\n    df1 = df[['a','d']]\n""]";"[""    ##Using DataFrame.drop\n    df.drop(df.columns[[1, 2]], axis=1, inplace=True)\n\n    # drop by Name\n    df1 = df1.drop(['B', 'C'], axis=1)\n\n\n    ## Select the ones you want\n    df1 = df[['a','d']]\n""]"
1268;;0;29370182;29370057.0;2;104;;;"<p>There are two possible solutions:</p>

<ul>
<li>Use a boolean mask, then use <code>df.loc[mask]</code></li>
<li>Set the date column as a DatetimeIndex, then use <code>df[start_date : end_date]</code></li>
</ul>

<hr>

<p><strong>Using a boolean mask</strong>:</p>

<p>Ensure <code>df['date']</code> is a Series with dtype <code>datetime64[ns]</code>:</p>

<pre><code>df['date'] = pd.to_datetime(df['date'])  
</code></pre>

<p>Make a boolean mask. <code>start_date</code> and <code>end_date</code> can be <code>datetime.datetime</code>s,
<code>np.datetime64</code>s, <code>pd.Timestamp</code>s, or even datetime strings:</p>

<pre><code>mask = (df['date'] &gt; start_date) &amp; (df['date'] &lt;= end_date)
</code></pre>

<p>Select the sub-DataFrame:</p>

<pre><code>df.loc[mask]
</code></pre>

<p>or re-assign to <code>df</code></p>

<pre><code>df = df.loc[mask]
</code></pre>

<hr>

<p>For example,</p>

<pre><code>import numpy as np
import pandas as pd

df = pd.DataFrame(np.random.random((200,3)))
df['date'] = pd.date_range('2000-1-1', periods=200, freq='D')
mask = (df['date'] &gt; '2000-6-1') &amp; (df['date'] &lt;= '2000-6-10')
print(df.loc[mask])
</code></pre>

<p>yields</p>

<pre><code>            0         1         2       date
153  0.208875  0.727656  0.037787 2000-06-02
154  0.750800  0.776498  0.237716 2000-06-03
155  0.812008  0.127338  0.397240 2000-06-04
156  0.639937  0.207359  0.533527 2000-06-05
157  0.416998  0.845658  0.872826 2000-06-06
158  0.440069  0.338690  0.847545 2000-06-07
159  0.202354  0.624833  0.740254 2000-06-08
160  0.465746  0.080888  0.155452 2000-06-09
161  0.858232  0.190321  0.432574 2000-06-10
</code></pre>

<hr>

<p><strong>Using a DatetimeIndex</strong>:</p>

<p>If you are going to do a lot of selections by date, it may be quicker to set the
<code>date</code> column as the index first. Then you can select rows by date using
<code>df.loc[start_date:end_date]</code>.</p>

<pre><code>import numpy as np
import pandas as pd

df = pd.DataFrame(np.random.random((200,3)))
df['date'] = pd.date_range('2000-1-1', periods=200, freq='D')
df = df.set_index(['date'])
print(df.loc['2000-6-1':'2000-6-10'])
</code></pre>

<p>yields</p>

<pre><code>                   0         1         2
date                                    
2000-06-01  0.040457  0.326594  0.492136    # &lt;- includes start_date
2000-06-02  0.279323  0.877446  0.464523
2000-06-03  0.328068  0.837669  0.608559
2000-06-04  0.107959  0.678297  0.517435
2000-06-05  0.131555  0.418380  0.025725
2000-06-06  0.999961  0.619517  0.206108
2000-06-07  0.129270  0.024533  0.154769
2000-06-08  0.441010  0.741781  0.470402
2000-06-09  0.682101  0.375660  0.009916
2000-06-10  0.754488  0.352293  0.339337
</code></pre>

<p>Some caveats:</p>

<ul>
<li>When using <code>df.loc[start_date : end_date]</code> both end-points are included in result. </li>
<li>Unlike the boolean mask solution, the <code>start_date</code> and <code>end_date</code> must be dates in the DatetimeIndex. </li>
</ul>

<hr>

<p>Also note that <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.io.parsers.read_csv.html"" rel=""noreferrer""><code>pd.read_csv</code> has a <code>parse_dates</code> parameter</a> which you could use to parse the <code>date</code> column as <code>datetime64</code>s. Thus, if you use <code>parse_dates</code>, you would not need to use <code>df['date'] = pd.to_datetime(df['date'])</code>. </p>
";;"[""df['date'] = pd.to_datetime(df['date'])  \n"", ""mask = (df['date'] > start_date) & (df['date'] <= end_date)\n"", 'df.loc[mask]\n', 'df = df.loc[mask]\n', ""import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.random((200,3)))\ndf['date'] = pd.date_range('2000-1-1', periods=200, freq='D')\nmask = (df['date'] > '2000-6-1') & (df['date'] <= '2000-6-10')\nprint(df.loc[mask])\n"", '            0         1         2       date\n153  0.208875  0.727656  0.037787 2000-06-02\n154  0.750800  0.776498  0.237716 2000-06-03\n155  0.812008  0.127338  0.397240 2000-06-04\n156  0.639937  0.207359  0.533527 2000-06-05\n157  0.416998  0.845658  0.872826 2000-06-06\n158  0.440069  0.338690  0.847545 2000-06-07\n159  0.202354  0.624833  0.740254 2000-06-08\n160  0.465746  0.080888  0.155452 2000-06-09\n161  0.858232  0.190321  0.432574 2000-06-10\n', ""import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.random((200,3)))\ndf['date'] = pd.date_range('2000-1-1', periods=200, freq='D')\ndf = df.set_index(['date'])\nprint(df.loc['2000-6-1':'2000-6-10'])\n"", '                   0         1         2\ndate                                    \n2000-06-01  0.040457  0.326594  0.492136    # <- includes start_date\n2000-06-02  0.279323  0.877446  0.464523\n2000-06-03  0.328068  0.837669  0.608559\n2000-06-04  0.107959  0.678297  0.517435\n2000-06-05  0.131555  0.418380  0.025725\n2000-06-06  0.999961  0.619517  0.206108\n2000-06-07  0.129270  0.024533  0.154769\n2000-06-08  0.441010  0.741781  0.470402\n2000-06-09  0.682101  0.375660  0.009916\n2000-06-10  0.754488  0.352293  0.339337\n']";"['df.loc[mask]', 'df[start_date : end_date]', ""df['date']"", 'datetime64[ns]', ""df['date'] = pd.to_datetime(df['date'])  \n"", 'start_date', 'end_date', 'datetime.datetime', 'np.datetime64', 'pd.Timestamp', ""mask = (df['date'] > start_date) & (df['date'] <= end_date)\n"", 'df.loc[mask]\n', 'df', 'df = df.loc[mask]\n', ""import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.random((200,3)))\ndf['date'] = pd.date_range('2000-1-1', periods=200, freq='D')\nmask = (df['date'] > '2000-6-1') & (df['date'] <= '2000-6-10')\nprint(df.loc[mask])\n"", '            0         1         2       date\n153  0.208875  0.727656  0.037787 2000-06-02\n154  0.750800  0.776498  0.237716 2000-06-03\n155  0.812008  0.127338  0.397240 2000-06-04\n156  0.639937  0.207359  0.533527 2000-06-05\n157  0.416998  0.845658  0.872826 2000-06-06\n158  0.440069  0.338690  0.847545 2000-06-07\n159  0.202354  0.624833  0.740254 2000-06-08\n160  0.465746  0.080888  0.155452 2000-06-09\n161  0.858232  0.190321  0.432574 2000-06-10\n', 'date', 'df.loc[start_date:end_date]', ""import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.random((200,3)))\ndf['date'] = pd.date_range('2000-1-1', periods=200, freq='D')\ndf = df.set_index(['date'])\nprint(df.loc['2000-6-1':'2000-6-10'])\n"", '                   0         1         2\ndate                                    \n2000-06-01  0.040457  0.326594  0.492136    # <- includes start_date\n2000-06-02  0.279323  0.877446  0.464523\n2000-06-03  0.328068  0.837669  0.608559\n2000-06-04  0.107959  0.678297  0.517435\n2000-06-05  0.131555  0.418380  0.025725\n2000-06-06  0.999961  0.619517  0.206108\n2000-06-07  0.129270  0.024533  0.154769\n2000-06-08  0.441010  0.741781  0.470402\n2000-06-09  0.682101  0.375660  0.009916\n2000-06-10  0.754488  0.352293  0.339337\n', 'df.loc[start_date : end_date]', 'start_date', 'end_date', 'pd.read_csv', 'parse_dates', 'date', 'datetime64', 'parse_dates', ""df['date'] = pd.to_datetime(df['date'])""]"
1269;;2;29383624;18039057.0;2;7;;;"<p>This is definitely an issue of delimiter, as most of the csv CSV are got create using <code>sep='/t'</code> so try to <code>read_csv</code> using the tab character <code>(\t)</code> using separator <code>/t</code>. so, try to open using following code line.</p>

<pre><code>data=pd.read_csv(""File_path"", sep='\t')
</code></pre>
";;"['data=pd.read_csv(""File_path"", sep=\'\\t\')\n']";"[""sep='/t'"", 'read_csv', '(\\t)', '/t', 'data=pd.read_csv(""File_path"", sep=\'\\t\')\n']"
1270;;0;29432741;29432629.0;2;64;;;"<p>You can use <a href=""http://matplotlib.org/examples/pylab_examples/matshow.html"" rel=""noreferrer""><code>matshow()</code></a>  from matplotlib:</p>

<p><code>plt.matshow(dataframe.corr())</code></p>
";;[];['matshow()', 'plt.matshow(dataframe.corr())']
1271;;0;29494537;19482970.0;2;19;;;"<p>Its gets even simpler (by pandas 0.16.0) : </p>

<pre><code>df.columns.tolist()
</code></pre>

<p>will give you the column names in a nice list.</p>
";;['df.columns.tolist()\n'];['df.columns.tolist()\n']
1272;;4;29500330;19530568.0;2;40;;;"<p>I used the following </p>

<pre><code>grouped = df.groupby('A')

df = grouped.aggregate(lambda x: tuple(x))

df['grouped'] = df['B'] + df['C']
</code></pre>
";;"[""grouped = df.groupby('A')\n\ndf = grouped.aggregate(lambda x: tuple(x))\n\ndf['grouped'] = df['B'] + df['C']\n""]";"[""grouped = df.groupby('A')\n\ndf = grouped.aggregate(lambda x: tuple(x))\n\ndf['grouped'] = df['B'] + df['C']\n""]"
1273;;0;29517089;29517072.0;2;40;;;"<p><code>df['Name']='abc'</code> will add the new column and set all rows to that value:</p>

<pre><code>In [79]:

df
Out[79]:
         Date, Open, High,  Low,  Close
0  01-01-2015,  565,  600,  400,    450
In [80]:

df['Name'] = 'abc'
df
Out[80]:
         Date, Open, High,  Low,  Close Name
0  01-01-2015,  565,  600,  400,    450  abc
</code></pre>
";;"[""In [79]:\n\ndf\nOut[79]:\n         Date, Open, High,  Low,  Close\n0  01-01-2015,  565,  600,  400,    450\nIn [80]:\n\ndf['Name'] = 'abc'\ndf\nOut[80]:\n         Date, Open, High,  Low,  Close Name\n0  01-01-2015,  565,  600,  400,    450  abc\n""]";"[""df['Name']='abc'"", ""In [79]:\n\ndf\nOut[79]:\n         Date, Open, High,  Low,  Close\n0  01-01-2015,  565,  600,  400,    450\nIn [80]:\n\ndf['Name'] = 'abc'\ndf\nOut[80]:\n         Date, Open, High,  Low,  Close Name\n0  01-01-2015,  565,  600,  400,    450  abc\n""]"
1274;;0;29517102;29517072.0;2;11;;;"<p>Single liner works</p>

<pre><code>df['Name'] = 'abc'
</code></pre>

<p>Creates a <code>Name</code> column and sets all rows to <code>abc</code> value</p>
";;"[""df['Name'] = 'abc'\n""]";"[""df['Name'] = 'abc'\n"", 'Name', 'abc']"
1275;;7;29528483;12286607.0;2;62;;;"<p>For people looking at this today, I would recommend the Seaborn <code>heatmap()</code> as documented with a tutorial <a href=""http://seaborn.pydata.org/generated/seaborn.heatmap.html"" rel=""nofollow noreferrer"">here</a>.</p>

<p>The example above would be done as follows:
</p>

<pre><code>import numpy as np 
from pandas import DataFrame
import seaborn as sns
%matplotlib

Index= ['aaa', 'bbb', 'ccc', 'ddd', 'eee']
Cols = ['A', 'B', 'C', 'D']
df = DataFrame(abs(np.random.randn(5, 4)), index=Index, columns=Cols)

sns.heatmap(df)
</code></pre>

<p>Where <code>%matplotlib</code> is an IPython magic function for those unfamiliar.</p>
";;"[""import numpy as np \nfrom pandas import DataFrame\nimport seaborn as sns\n%matplotlib\n\nIndex= ['aaa', 'bbb', 'ccc', 'ddd', 'eee']\nCols = ['A', 'B', 'C', 'D']\ndf = DataFrame(abs(np.random.randn(5, 4)), index=Index, columns=Cols)\n\nsns.heatmap(df)\n""]";"['heatmap()', ""import numpy as np \nfrom pandas import DataFrame\nimport seaborn as sns\n%matplotlib\n\nIndex= ['aaa', 'bbb', 'ccc', 'ddd', 'eee']\nCols = ['A', 'B', 'C', 'D']\ndf = DataFrame(abs(np.random.randn(5, 4)), index=Index, columns=Cols)\n\nsns.heatmap(df)\n"", '%matplotlib']"
1276;;5;29528804;29525808.0;2;67;;;"<p>Below should work in most cases:</p>

<pre><code>df = pd.read_sql(query.statement, query.session.bind)
</code></pre>

<p>See <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html"" rel=""noreferrer""><code>pandas.read_sql</code></a> documentation for more information on the parameters.</p>
";;['df = pd.read_sql(query.statement, query.session.bind)\n'];['df = pd.read_sql(query.statement, query.session.bind)\n', 'pandas.read_sql']
1277;;0;29530303;29530232.0;2;11;;;"<p><code>df.isnull().any().any()</code> should do it.</p>
";;[];['df.isnull().any().any()']
1278;;0;29530559;29530232.0;2;59;;;"<p>You have a couple options. </p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randn(10,6))
# Make a few areas have NaN values
df.iloc[1:3,1] = np.nan
df.iloc[5,3] = np.nan
df.iloc[7:9,5] = np.nan
</code></pre>

<p>Now the data frame looks something like this:</p>

<pre><code>          0         1         2         3         4         5
0  0.520113  0.884000  1.260966 -0.236597  0.312972 -0.196281
1 -0.837552       NaN  0.143017  0.862355  0.346550  0.842952
2 -0.452595       NaN -0.420790  0.456215  1.203459  0.527425
3  0.317503 -0.917042  1.780938 -1.584102  0.432745  0.389797
4 -0.722852  1.704820 -0.113821 -1.466458  0.083002  0.011722
5 -0.622851 -0.251935 -1.498837       NaN  1.098323  0.273814
6  0.329585  0.075312 -0.690209 -3.807924  0.489317 -0.841368
7 -1.123433 -1.187496  1.868894 -2.046456 -0.949718       NaN
8  1.133880 -0.110447  0.050385 -1.158387  0.188222       NaN
9 -0.513741  1.196259  0.704537  0.982395 -0.585040 -1.693810
</code></pre>

<ul>
<li><strong>Option 1</strong>: <code>df.isnull().any().any()</code> - This returns a boolean value</li>
</ul>

<p>You know of the <code>isnull()</code> which would return a dataframe like this:</p>

<pre><code>       0      1      2      3      4      5
0  False  False  False  False  False  False
1  False   True  False  False  False  False
2  False   True  False  False  False  False
3  False  False  False  False  False  False
4  False  False  False  False  False  False
5  False  False  False   True  False  False
6  False  False  False  False  False  False
7  False  False  False  False  False   True
8  False  False  False  False  False   True
9  False  False  False  False  False  False
</code></pre>

<p>If you make it <code>df.isnull().any()</code>, you can find just the columns that have <code>NaN</code> values:</p>

<pre><code>0    False
1     True
2    False
3     True
4    False
5     True
dtype: bool
</code></pre>

<p>One more <code>.any()</code> will tell you if any of the above are <code>True</code></p>

<pre><code>&gt; df.isnull().any().any()
True
</code></pre>

<ul>
<li><strong>Option 2</strong>: <code>df.isnull().sum().sum()</code> - This returns an integer of the total number of <code>NaN</code> values:</li>
</ul>

<p>This operates the same way as the <code>.any().any()</code> does, by first giving a summation of the number of <code>NaN</code> values in a column, then the summation of those values:</p>

<pre><code>df.isnull().sum()
0    0
1    2
2    0
3    1
4    0
5    2
dtype: int64
</code></pre>

<p>Then to get the total:</p>

<pre><code>df.isnull().sum().sum()
5
</code></pre>
";;['import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randn(10,6))\n# Make a few areas have NaN values\ndf.iloc[1:3,1] = np.nan\ndf.iloc[5,3] = np.nan\ndf.iloc[7:9,5] = np.nan\n', '          0         1         2         3         4         5\n0  0.520113  0.884000  1.260966 -0.236597  0.312972 -0.196281\n1 -0.837552       NaN  0.143017  0.862355  0.346550  0.842952\n2 -0.452595       NaN -0.420790  0.456215  1.203459  0.527425\n3  0.317503 -0.917042  1.780938 -1.584102  0.432745  0.389797\n4 -0.722852  1.704820 -0.113821 -1.466458  0.083002  0.011722\n5 -0.622851 -0.251935 -1.498837       NaN  1.098323  0.273814\n6  0.329585  0.075312 -0.690209 -3.807924  0.489317 -0.841368\n7 -1.123433 -1.187496  1.868894 -2.046456 -0.949718       NaN\n8  1.133880 -0.110447  0.050385 -1.158387  0.188222       NaN\n9 -0.513741  1.196259  0.704537  0.982395 -0.585040 -1.693810\n', '       0      1      2      3      4      5\n0  False  False  False  False  False  False\n1  False   True  False  False  False  False\n2  False   True  False  False  False  False\n3  False  False  False  False  False  False\n4  False  False  False  False  False  False\n5  False  False  False   True  False  False\n6  False  False  False  False  False  False\n7  False  False  False  False  False   True\n8  False  False  False  False  False   True\n9  False  False  False  False  False  False\n', '0    False\n1     True\n2    False\n3     True\n4    False\n5     True\ndtype: bool\n', '> df.isnull().any().any()\nTrue\n', 'df.isnull().sum()\n0    0\n1    2\n2    0\n3    1\n4    0\n5    2\ndtype: int64\n', 'df.isnull().sum().sum()\n5\n'];['import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randn(10,6))\n# Make a few areas have NaN values\ndf.iloc[1:3,1] = np.nan\ndf.iloc[5,3] = np.nan\ndf.iloc[7:9,5] = np.nan\n', '          0         1         2         3         4         5\n0  0.520113  0.884000  1.260966 -0.236597  0.312972 -0.196281\n1 -0.837552       NaN  0.143017  0.862355  0.346550  0.842952\n2 -0.452595       NaN -0.420790  0.456215  1.203459  0.527425\n3  0.317503 -0.917042  1.780938 -1.584102  0.432745  0.389797\n4 -0.722852  1.704820 -0.113821 -1.466458  0.083002  0.011722\n5 -0.622851 -0.251935 -1.498837       NaN  1.098323  0.273814\n6  0.329585  0.075312 -0.690209 -3.807924  0.489317 -0.841368\n7 -1.123433 -1.187496  1.868894 -2.046456 -0.949718       NaN\n8  1.133880 -0.110447  0.050385 -1.158387  0.188222       NaN\n9 -0.513741  1.196259  0.704537  0.982395 -0.585040 -1.693810\n', 'df.isnull().any().any()', 'isnull()', '       0      1      2      3      4      5\n0  False  False  False  False  False  False\n1  False   True  False  False  False  False\n2  False   True  False  False  False  False\n3  False  False  False  False  False  False\n4  False  False  False  False  False  False\n5  False  False  False   True  False  False\n6  False  False  False  False  False  False\n7  False  False  False  False  False   True\n8  False  False  False  False  False   True\n9  False  False  False  False  False  False\n', 'df.isnull().any()', 'NaN', '0    False\n1     True\n2    False\n3     True\n4    False\n5     True\ndtype: bool\n', '.any()', 'True', '> df.isnull().any().any()\nTrue\n', 'df.isnull().sum().sum()', 'NaN', '.any().any()', 'NaN', 'df.isnull().sum()\n0    0\n1    2\n2    0\n3    1\n4    0\n5    2\ndtype: int64\n', 'df.isnull().sum().sum()\n5\n']
1279;;8;29530601;29530232.0;2;134;;;"<p><a href=""https://stackoverflow.com/users/1567452/jwilner"">jwilner</a>'s response is spot on. I was exploring to see if there's a faster option, since in my experience, summing flat arrays is (strangely) faster than counting. This code seems faster:</p>

<pre><code>df.isnull().values.any()
</code></pre>

<p>For example:</p>

<pre><code>In [2]: df = pd.DataFrame(np.random.randn(1000,1000))

In [3]: df[df &gt; 0.9] = pd.np.nan

In [4]: %timeit df.isnull().any().any()
100 loops, best of 3: 14.7 ms per loop

In [5]: %timeit df.isnull().values.sum()
100 loops, best of 3: 2.15 ms per loop

In [6]: %timeit df.isnull().sum().sum()
100 loops, best of 3: 18 ms per loop

In [7]: %timeit df.isnull().values.any()
1000 loops, best of 3: 948 s per loop
</code></pre>

<p><code>df.isnull().sum().sum()</code> is a bit slower, but of course, has additional information -- the number of <code>NaNs</code>.</p>
";;['df.isnull().values.any()\n', 'In [2]: df = pd.DataFrame(np.random.randn(1000,1000))\n\nIn [3]: df[df > 0.9] = pd.np.nan\n\nIn [4]: %timeit df.isnull().any().any()\n100 loops, best of 3: 14.7 ms per loop\n\nIn [5]: %timeit df.isnull().values.sum()\n100 loops, best of 3: 2.15 ms per loop\n\nIn [6]: %timeit df.isnull().sum().sum()\n100 loops, best of 3: 18 ms per loop\n\nIn [7]: %timeit df.isnull().values.any()\n1000 loops, best of 3: 948 s per loop\n'];['df.isnull().values.any()\n', 'In [2]: df = pd.DataFrame(np.random.randn(1000,1000))\n\nIn [3]: df[df > 0.9] = pd.np.nan\n\nIn [4]: %timeit df.isnull().any().any()\n100 loops, best of 3: 14.7 ms per loop\n\nIn [5]: %timeit df.isnull().values.sum()\n100 loops, best of 3: 2.15 ms per loop\n\nIn [6]: %timeit df.isnull().sum().sum()\n100 loops, best of 3: 18 ms per loop\n\nIn [7]: %timeit df.isnull().values.any()\n1000 loops, best of 3: 948 s per loop\n', 'df.isnull().sum().sum()', 'NaNs']
1280;;0;29576803;29576430.0;2;33;;;"<p>You can shuffle the rows of a dataframe by indexing with a shuffled index. For this, you can eg use <code>np.random.permutation</code> (but <code>np.random.choice</code> is also a possibility):</p>

<pre><code>In [12]: df = pd.read_csv(StringIO(s), sep=""\s+"")

In [13]: df
Out[13]: 
    Col1  Col2  Col3  Type
0      1     2     3     1
1      4     5     6     1
20     7     8     9     2
21    10    11    12     2
45    13    14    15     3
46    16    17    18     3

In [14]: df.iloc[np.random.permutation(len(df))]
Out[14]: 
    Col1  Col2  Col3  Type
46    16    17    18     3
45    13    14    15     3
20     7     8     9     2
0      1     2     3     1
1      4     5     6     1
21    10    11    12     2
</code></pre>

<p>If you want to keep the index numbered from 1, 2, .., n as in your example, you can simply reset the index: <code>df_shuffled.reset_index(drop=True)</code></p>
";;"['In [12]: df = pd.read_csv(StringIO(s), sep=""\\s+"")\n\nIn [13]: df\nOut[13]: \n    Col1  Col2  Col3  Type\n0      1     2     3     1\n1      4     5     6     1\n20     7     8     9     2\n21    10    11    12     2\n45    13    14    15     3\n46    16    17    18     3\n\nIn [14]: df.iloc[np.random.permutation(len(df))]\nOut[14]: \n    Col1  Col2  Col3  Type\n46    16    17    18     3\n45    13    14    15     3\n20     7     8     9     2\n0      1     2     3     1\n1      4     5     6     1\n21    10    11    12     2\n']";"['np.random.permutation', 'np.random.choice', 'In [12]: df = pd.read_csv(StringIO(s), sep=""\\s+"")\n\nIn [13]: df\nOut[13]: \n    Col1  Col2  Col3  Type\n0      1     2     3     1\n1      4     5     6     1\n20     7     8     9     2\n21    10    11    12     2\n45    13    14    15     3\n46    16    17    18     3\n\nIn [14]: df.iloc[np.random.permutation(len(df))]\nOut[14]: \n    Col1  Col2  Col3  Type\n46    16    17    18     3\n45    13    14    15     3\n20     7     8     9     2\n0      1     2     3     1\n1      4     5     6     1\n21    10    11    12     2\n', 'df_shuffled.reset_index(drop=True)']"
1281;;1;29665452;26873127.0;2;119;;;"<p>You'll need to use the <code>HTML()</code> or <code>display()</code> functions from IPython's display module:</p>

<pre class=""lang-python prettyprint-override""><code>from IPython.display import display, HTML

# Assuming that dataframes df1 and df2 are already defined:
print ""Dataframe 1:""
display(df1)
print ""Dataframe 2:""
HTML(df2.to_html())
</code></pre>

<p>Note that if you just <code>print df1.to_html()</code> you'll get the raw, unrendered HTML. </p>

<p>You can also import from <code>IPython.core.display</code> with the same effect</p>
";;"['from IPython.display import display, HTML\n\n# Assuming that dataframes df1 and df2 are already defined:\nprint ""Dataframe 1:""\ndisplay(df1)\nprint ""Dataframe 2:""\nHTML(df2.to_html())\n']";"['HTML()', 'display()', 'from IPython.display import display, HTML\n\n# Assuming that dataframes df1 and df2 are already defined:\nprint ""Dataframe 1:""\ndisplay(df1)\nprint ""Dataframe 2:""\nHTML(df2.to_html())\n', 'print df1.to_html()', 'IPython.core.display']"
1282;;3;29675706;21360361.0;2;9;;;"<p>A couple of improvement's on <a href=""https://stackoverflow.com/posts/21361994/revisions"">HYRY's answer</a>:</p>

<ul>
<li>call <code>display</code> after <code>clear_output</code> so that you end up with one plot, rather than two, when the cell is interrupted.</li>
<li>catch the <code>KeyboardInterrupt</code>, so that the cell output isn't littered with the traceback.</li>
</ul>



<pre><code>import matplotlib.pylab as plt
import pandas as pd
import numpy as np
import time
from IPython import display
%matplotlib inline

i = pd.date_range('2013-1-1',periods=100,freq='s')

while True:
    try:
        plt.plot(pd.Series(data=np.random.randn(100), index=i))
        display.display(plt.gcf())
        display.clear_output(wait=True)
        time.sleep(1)
    except KeyboardInterrupt:
        break
</code></pre>
";;"[""import matplotlib.pylab as plt\nimport pandas as pd\nimport numpy as np\nimport time\nfrom IPython import display\n%matplotlib inline\n\ni = pd.date_range('2013-1-1',periods=100,freq='s')\n\nwhile True:\n    try:\n        plt.plot(pd.Series(data=np.random.randn(100), index=i))\n        display.display(plt.gcf())\n        display.clear_output(wait=True)\n        time.sleep(1)\n    except KeyboardInterrupt:\n        break\n""]";"['display', 'clear_output', 'KeyboardInterrupt', ""import matplotlib.pylab as plt\nimport pandas as pd\nimport numpy as np\nimport time\nfrom IPython import display\n%matplotlib inline\n\ni = pd.date_range('2013-1-1',periods=100,freq='s')\n\nwhile True:\n    try:\n        plt.plot(pd.Series(data=np.random.randn(100), index=i))\n        display.display(plt.gcf())\n        display.clear_output(wait=True)\n        time.sleep(1)\n    except KeyboardInterrupt:\n        break\n""]"
1283;;2;29763653;29763620.0;2;52;;;"<p>When you don't have a MultiIndex, <code>df.columns</code> is just an array of column names so you can do:</p>

<pre><code>df.ix[:, df.columns != 'b']

          a         c         d
0  0.561196  0.013768  0.772827
1  0.882641  0.615396  0.075381
2  0.368824  0.651378  0.397203
3  0.788730  0.568099  0.869127
</code></pre>
";;"[""df.ix[:, df.columns != 'b']\n\n          a         c         d\n0  0.561196  0.013768  0.772827\n1  0.882641  0.615396  0.075381\n2  0.368824  0.651378  0.397203\n3  0.788730  0.568099  0.869127\n""]";"['df.columns', ""df.ix[:, df.columns != 'b']\n\n          a         c         d\n0  0.561196  0.013768  0.772827\n1  0.882641  0.615396  0.075381\n2  0.368824  0.651378  0.397203\n3  0.788730  0.568099  0.869127\n""]"
1284;;0;29774704;17241004.0;2;18;;;"<p>If you are dealing with a multi-index dataframe, you may be interested in extracting only the column of one name of the multi-index. You can do this as</p>

<pre><code>df.index.get_level_values('name_sub_index')
</code></pre>

<p>and of course <code>name_sub_index</code> must be an element of the <code>FrozenList</code> <code>df.index.names</code></p>
";;"[""df.index.get_level_values('name_sub_index')\n""]";"[""df.index.get_level_values('name_sub_index')\n"", 'name_sub_index', 'FrozenList', 'df.index.names']"
1285;;2;29815523;29815129.0;2;45;;;"<p>Use <code>df.T.to_dict().values()</code>, like below:</p>

<pre><code>In [1]: df
Out[1]:
   customer  item1   item2   item3
0         1  apple    milk  tomato
1         2  water  orange  potato
2         3  juice   mango   chips

In [2]: df.T.to_dict().values()
Out[2]:
[{'customer': 1.0, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},
 {'customer': 2.0, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},
 {'customer': 3.0, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]
</code></pre>

<hr>

<p>As John Galt mentions in <a href=""https://stackoverflow.com/a/29816143/2358206"">his answer </a>, you should probably instead use <code>df.to_dict('records')</code>. It's faster than transposing manually.</p>

<pre><code>In [20]: timeit df.T.to_dict().values()
1000 loops, best of 3: 395 s per loop

In [21]: timeit df.to_dict('records')
10000 loops, best of 3: 53 s per loop
</code></pre>
";;"[""In [1]: df\nOut[1]:\n   customer  item1   item2   item3\n0         1  apple    milk  tomato\n1         2  water  orange  potato\n2         3  juice   mango   chips\n\nIn [2]: df.T.to_dict().values()\nOut[2]:\n[{'customer': 1.0, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},\n {'customer': 2.0, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},\n {'customer': 3.0, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]\n"", ""In [20]: timeit df.T.to_dict().values()\n1000 loops, best of 3: 395 s per loop\n\nIn [21]: timeit df.to_dict('records')\n10000 loops, best of 3: 53 s per loop\n""]";"['df.T.to_dict().values()', ""In [1]: df\nOut[1]:\n   customer  item1   item2   item3\n0         1  apple    milk  tomato\n1         2  water  orange  potato\n2         3  juice   mango   chips\n\nIn [2]: df.T.to_dict().values()\nOut[2]:\n[{'customer': 1.0, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},\n {'customer': 2.0, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},\n {'customer': 3.0, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]\n"", ""df.to_dict('records')"", ""In [20]: timeit df.T.to_dict().values()\n1000 loops, best of 3: 395 s per loop\n\nIn [21]: timeit df.to_dict('records')\n10000 loops, best of 3: 53 s per loop\n""]"
1286;;2;29816143;29815129.0;2;67;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.to_dict.html#pandas.DataFrame.to_dict""><code>df.to_dict('records')</code></a> -- gives the output without having to transpose externally.</p>

<pre><code>In [2]: df.to_dict('records')
Out[2]:
[{'customer': 1L, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},
 {'customer': 2L, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},
 {'customer': 3L, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]
</code></pre>
";;"[""In [2]: df.to_dict('records')\nOut[2]:\n[{'customer': 1L, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},\n {'customer': 2L, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},\n {'customer': 3L, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]\n""]";"[""df.to_dict('records')"", ""In [2]: df.to_dict('records')\nOut[2]:\n[{'customer': 1L, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},\n {'customer': 2L, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},\n {'customer': 3L, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]\n""]"
1287;;0;29837754;13331698.0;2;24;;;"<p>A interesting question! my answer as below:</p>

<pre><code>import pandas as pd

def sublst(row):
    return lst[row['J1']:row['J2']]

df = pd.DataFrame({'ID':['1','2','3'], 'J1': [0,2,3], 'J2':[1,4,5]})
print df
lst = ['a','b','c','d','e','f']

df['J3'] = df.apply(sublst,axis=1)
print df
</code></pre>

<p>Output:</p>

<pre><code>  ID  J1  J2
0  1   0   1
1  2   2   4
2  3   3   5
  ID  J1  J2      J3
0  1   0   1     [a]
1  2   2   4  [c, d]
2  3   3   5  [d, e]
</code></pre>

<p>I changed the column name to ID,J1,J2,J3 to ensure ID &lt; J1 &lt; J2 &lt; J3, so the column display in right sequence.</p>

<p>One more brief version:</p>

<pre><code>import pandas as pd

df = pd.DataFrame({'ID':['1','2','3'], 'J1': [0,2,3], 'J2':[1,4,5]})
print df
lst = ['a','b','c','d','e','f']

df['J3'] = df.apply(lambda row:lst[row['J1']:row['J2']],axis=1)
print df
</code></pre>
";;"[""import pandas as pd\n\ndef sublst(row):\n    return lst[row['J1']:row['J2']]\n\ndf = pd.DataFrame({'ID':['1','2','3'], 'J1': [0,2,3], 'J2':[1,4,5]})\nprint df\nlst = ['a','b','c','d','e','f']\n\ndf['J3'] = df.apply(sublst,axis=1)\nprint df\n"", '  ID  J1  J2\n0  1   0   1\n1  2   2   4\n2  3   3   5\n  ID  J1  J2      J3\n0  1   0   1     [a]\n1  2   2   4  [c, d]\n2  3   3   5  [d, e]\n', ""import pandas as pd\n\ndf = pd.DataFrame({'ID':['1','2','3'], 'J1': [0,2,3], 'J2':[1,4,5]})\nprint df\nlst = ['a','b','c','d','e','f']\n\ndf['J3'] = df.apply(lambda row:lst[row['J1']:row['J2']],axis=1)\nprint df\n""]";"[""import pandas as pd\n\ndef sublst(row):\n    return lst[row['J1']:row['J2']]\n\ndf = pd.DataFrame({'ID':['1','2','3'], 'J1': [0,2,3], 'J2':[1,4,5]})\nprint df\nlst = ['a','b','c','d','e','f']\n\ndf['J3'] = df.apply(sublst,axis=1)\nprint df\n"", '  ID  J1  J2\n0  1   0   1\n1  2   2   4\n2  3   3   5\n  ID  J1  J2      J3\n0  1   0   1     [a]\n1  2   2   4  [c, d]\n2  3   3   5  [d, e]\n', ""import pandas as pd\n\ndf = pd.DataFrame({'ID':['1','2','3'], 'J1': [0,2,3], 'J2':[1,4,5]})\nprint df\nlst = ['a','b','c','d','e','f']\n\ndf['J3'] = df.apply(lambda row:lst[row['J1']:row['J2']],axis=1)\nprint df\n""]"
1288;;0;29910919;14262433.0;2;11;;;"<p>One more variation</p>

<p>Many of the operations done in pandas can also be done as a db query (sql, mongo)</p>

<p>Using a RDBMS or mongodb allows you to perform some of the aggregations in the DB Query (which is optimized for large data, and uses cache and indexes efficiently)</p>

<p>Later, you can perform post processing using pandas.</p>

<p>The advantage of this method is that you gain the DB optimizations for working with large data, while still defining the logic in a high level declarative syntax - and not having to deal with the details of deciding what to do in memory and what to do out of core.</p>

<p>And although the query language and pandas are different, it's usually not complicated to translate part of the logic from one to another.</p>
";;[];[]
1289;;7;29916004;13148429.0;2;8;;;"<p>Simply do,</p>

<pre><code>df = df[['mean'] + df.columns[:-1].tolist()]
</code></pre>
";;"[""df = df[['mean'] + df.columns[:-1].tolist()]\n""]";"[""df = df[['mean'] + df.columns[:-1].tolist()]\n""]"
1290;;2;29919489;29919306.0;2;45;;;"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.idxmax.html""><code>idxmax()</code></a> to find the column with the greatest value on each row:</p>

<pre><code>&gt;&gt;&gt; df.idxmax(axis=1)
0    Communications
1          Business
2    Communications
3    Communications
4          Business
dtype: object
</code></pre>

<p>To create the new column use <code>df['Max'] = df.idxmax(axis=1)</code>.</p>
";;['>>> df.idxmax(axis=1)\n0    Communications\n1          Business\n2    Communications\n3    Communications\n4          Business\ndtype: object\n'];"['idxmax()', '>>> df.idxmax(axis=1)\n0    Communications\n1          Business\n2    Communications\n3    Communications\n4          Business\ndtype: object\n', ""df['Max'] = df.idxmax(axis=1)""]"
1291;;3;29922207;13148429.0;2;72;;;"<p>Just assign the column names in the order you want them, to <code>&lt;dataframe&gt;.columns</code> like below:</p>

<pre><code>In [39]: df
Out[39]: 
          0         1         2         3         4  mean
0  0.172742  0.915661  0.043387  0.712833  0.190717     1
1  0.128186  0.424771  0.590779  0.771080  0.617472     1
2  0.125709  0.085894  0.989798  0.829491  0.155563     1
3  0.742578  0.104061  0.299708  0.616751  0.951802     1
4  0.721118  0.528156  0.421360  0.105886  0.322311     1
5  0.900878  0.082047  0.224656  0.195162  0.736652     1
6  0.897832  0.558108  0.318016  0.586563  0.507564     1
7  0.027178  0.375183  0.930248  0.921786  0.337060     1
8  0.763028  0.182905  0.931756  0.110675  0.423398     1
9  0.848996  0.310562  0.140873  0.304561  0.417808     1

In [40]: df = df[['mean', 4,3,2,1]]
</code></pre>

<p>Now, 'mean' column comes out in the front:</p>

<pre><code>In [41]: df
Out[41]: 
   mean         4         3         2         1
0     1  0.190717  0.712833  0.043387  0.915661
1     1  0.617472  0.771080  0.590779  0.424771
2     1  0.155563  0.829491  0.989798  0.085894
3     1  0.951802  0.616751  0.299708  0.104061
4     1  0.322311  0.105886  0.421360  0.528156
5     1  0.736652  0.195162  0.224656  0.082047
6     1  0.507564  0.586563  0.318016  0.558108
7     1  0.337060  0.921786  0.930248  0.375183
8     1  0.423398  0.110675  0.931756  0.182905
9     1  0.417808  0.304561  0.140873  0.310562
</code></pre>
";;"[""In [39]: df\nOut[39]: \n          0         1         2         3         4  mean\n0  0.172742  0.915661  0.043387  0.712833  0.190717     1\n1  0.128186  0.424771  0.590779  0.771080  0.617472     1\n2  0.125709  0.085894  0.989798  0.829491  0.155563     1\n3  0.742578  0.104061  0.299708  0.616751  0.951802     1\n4  0.721118  0.528156  0.421360  0.105886  0.322311     1\n5  0.900878  0.082047  0.224656  0.195162  0.736652     1\n6  0.897832  0.558108  0.318016  0.586563  0.507564     1\n7  0.027178  0.375183  0.930248  0.921786  0.337060     1\n8  0.763028  0.182905  0.931756  0.110675  0.423398     1\n9  0.848996  0.310562  0.140873  0.304561  0.417808     1\n\nIn [40]: df = df[['mean', 4,3,2,1]]\n"", 'In [41]: df\nOut[41]: \n   mean         4         3         2         1\n0     1  0.190717  0.712833  0.043387  0.915661\n1     1  0.617472  0.771080  0.590779  0.424771\n2     1  0.155563  0.829491  0.989798  0.085894\n3     1  0.951802  0.616751  0.299708  0.104061\n4     1  0.322311  0.105886  0.421360  0.528156\n5     1  0.736652  0.195162  0.224656  0.082047\n6     1  0.507564  0.586563  0.318016  0.558108\n7     1  0.337060  0.921786  0.930248  0.375183\n8     1  0.423398  0.110675  0.931756  0.182905\n9     1  0.417808  0.304561  0.140873  0.310562\n']";"['<dataframe>.columns', ""In [39]: df\nOut[39]: \n          0         1         2         3         4  mean\n0  0.172742  0.915661  0.043387  0.712833  0.190717     1\n1  0.128186  0.424771  0.590779  0.771080  0.617472     1\n2  0.125709  0.085894  0.989798  0.829491  0.155563     1\n3  0.742578  0.104061  0.299708  0.616751  0.951802     1\n4  0.721118  0.528156  0.421360  0.105886  0.322311     1\n5  0.900878  0.082047  0.224656  0.195162  0.736652     1\n6  0.897832  0.558108  0.318016  0.586563  0.507564     1\n7  0.027178  0.375183  0.930248  0.921786  0.337060     1\n8  0.763028  0.182905  0.931756  0.110675  0.423398     1\n9  0.848996  0.310562  0.140873  0.304561  0.417808     1\n\nIn [40]: df = df[['mean', 4,3,2,1]]\n"", 'In [41]: df\nOut[41]: \n   mean         4         3         2         1\n0     1  0.190717  0.712833  0.043387  0.915661\n1     1  0.617472  0.771080  0.590779  0.424771\n2     1  0.155563  0.829491  0.989798  0.085894\n3     1  0.951802  0.616751  0.299708  0.104061\n4     1  0.322311  0.105886  0.421360  0.528156\n5     1  0.736652  0.195162  0.224656  0.082047\n6     1  0.507564  0.586563  0.318016  0.558108\n7     1  0.337060  0.921786  0.930248  0.375183\n8     1  0.423398  0.110675  0.931756  0.182905\n9     1  0.417808  0.304561  0.140873  0.310562\n']"
1292;;1;29930255;20158597.0;2;12;;;"<p>You ask about binning with non-unique bin edges, for which I have a fairly simple answer.  In the case of your example, your intent and the behavior of qcut diverge where in the <code>pandas.tools.tile.qcut</code> function where bins are defined:</p>

<p><code>bins = algos.quantile(x, quantiles)</code></p>

<p>Which, because your data is 50% 0s, causes bins to be returned with multiple bin edges at the value 0 for any value of quantiles greater than 2.  I see two possible resolutions.  In the first, the fractile space is divided evenly, binning all 0s, but not only 0s, in the first bin.  In the second, the fractile space is divided evenly for values greater than 0, binning all 0s and only 0s in the first bin.</p>

<pre><code>import numpy as np
import pandas as pd
import pandas.core.algorithms as algos
from pandas import Series
</code></pre>

<p>In both cases, I'll create some random sample data fitting your description of 50% zeroes and the remaining values between 1 and 100</p>

<pre><code>zs = np.zeros(300)
rs = np.random.randint(1, 100, size=300)
arr=np.concatenate((zs, rs))
ser = Series(arr)
</code></pre>

<p><strong>Solution 1: bin 1 contains both 0s and low values</strong></p>

<pre><code>bins = algos.quantile(np.unique(ser), np.linspace(0, 1, 11))
result = pd.tools.tile._bins_to_cuts(ser, bins, include_lowest=True)
</code></pre>

<p>The result is</p>

<pre><code>In[61]: result.value_counts()
Out[61]: 
[0, 9.3]        323
(27.9, 38.2]     37
(9.3, 18.6]      37
(88.7, 99]       35
(57.8, 68.1]     32
(68.1, 78.4]     31
(78.4, 88.7]     30
(38.2, 48.5]     27
(48.5, 57.8]     26
(18.6, 27.9]     22
dtype: int64
</code></pre>

<p><strong>Solution 2: bin1 contains only 0s</strong></p>

<pre><code>mx = np.ma.masked_equal(arr, 0, copy=True)
bins = algos.quantile(arr[~mx.mask], np.linspace(0, 1, 11))
bins = np.insert(bins, 0, 0)
bins[1] = bins[1]-(bins[1]/2)
result = pd.tools.tile._bins_to_cuts(arr, bins, include_lowest=True)
</code></pre>

<p>The result is:    </p>

<pre><code>In[133]: result.value_counts()
Out[133]: 
[0, 0.5]        300
(0.5, 11]        32
(11, 18.8]       28
(18.8, 29.7]     30
(29.7, 39]       35
(39, 50]         26
(50, 59]         31
(59, 71]         31
(71, 79.2]       27
(79.2, 90.2]     30
(90.2, 99]       30
dtype: int64
</code></pre>

<p>There is work that could be done to Solution 2 to make it a little prettier I think, but you can see that the masked array is a useful tool to approach your goals.</p>
";;['import numpy as np\nimport pandas as pd\nimport pandas.core.algorithms as algos\nfrom pandas import Series\n', 'zs = np.zeros(300)\nrs = np.random.randint(1, 100, size=300)\narr=np.concatenate((zs, rs))\nser = Series(arr)\n', 'bins = algos.quantile(np.unique(ser), np.linspace(0, 1, 11))\nresult = pd.tools.tile._bins_to_cuts(ser, bins, include_lowest=True)\n', 'In[61]: result.value_counts()\nOut[61]: \n[0, 9.3]        323\n(27.9, 38.2]     37\n(9.3, 18.6]      37\n(88.7, 99]       35\n(57.8, 68.1]     32\n(68.1, 78.4]     31\n(78.4, 88.7]     30\n(38.2, 48.5]     27\n(48.5, 57.8]     26\n(18.6, 27.9]     22\ndtype: int64\n', 'mx = np.ma.masked_equal(arr, 0, copy=True)\nbins = algos.quantile(arr[~mx.mask], np.linspace(0, 1, 11))\nbins = np.insert(bins, 0, 0)\nbins[1] = bins[1]-(bins[1]/2)\nresult = pd.tools.tile._bins_to_cuts(arr, bins, include_lowest=True)\n', 'In[133]: result.value_counts()\nOut[133]: \n[0, 0.5]        300\n(0.5, 11]        32\n(11, 18.8]       28\n(18.8, 29.7]     30\n(29.7, 39]       35\n(39, 50]         26\n(50, 59]         31\n(59, 71]         31\n(71, 79.2]       27\n(79.2, 90.2]     30\n(90.2, 99]       30\ndtype: int64\n'];['pandas.tools.tile.qcut', 'bins = algos.quantile(x, quantiles)', 'import numpy as np\nimport pandas as pd\nimport pandas.core.algorithms as algos\nfrom pandas import Series\n', 'zs = np.zeros(300)\nrs = np.random.randint(1, 100, size=300)\narr=np.concatenate((zs, rs))\nser = Series(arr)\n', 'bins = algos.quantile(np.unique(ser), np.linspace(0, 1, 11))\nresult = pd.tools.tile._bins_to_cuts(ser, bins, include_lowest=True)\n', 'In[61]: result.value_counts()\nOut[61]: \n[0, 9.3]        323\n(27.9, 38.2]     37\n(9.3, 18.6]      37\n(88.7, 99]       35\n(57.8, 68.1]     32\n(68.1, 78.4]     31\n(78.4, 88.7]     30\n(38.2, 48.5]     27\n(48.5, 57.8]     26\n(18.6, 27.9]     22\ndtype: int64\n', 'mx = np.ma.masked_equal(arr, 0, copy=True)\nbins = algos.quantile(arr[~mx.mask], np.linspace(0, 1, 11))\nbins = np.insert(bins, 0, 0)\nbins[1] = bins[1]-(bins[1]/2)\nresult = pd.tools.tile._bins_to_cuts(arr, bins, include_lowest=True)\n', 'In[133]: result.value_counts()\nOut[133]: \n[0, 0.5]        300\n(0.5, 11]        32\n(11, 18.8]       28\n(18.8, 29.7]     30\n(29.7, 39]       35\n(39, 50]         26\n(50, 59]         31\n(59, 71]         31\n(71, 79.2]       27\n(79.2, 90.2]     30\n(90.2, 99]       30\ndtype: int64\n']
1293;;1;29990874;11697887.0;2;6;;;"<p>Django Pandas solves this rather neatly: <a href=""https://github.com/chrisdev/django-pandas/"" rel=""noreferrer"">https://github.com/chrisdev/django-pandas/</a></p>

<p>From the README:</p>

<pre><code>class MyModel(models.Model):
    full_name = models.CharField(max_length=25)
    age = models.IntegerField()
    department = models.CharField(max_length=3)
    wage = models.FloatField()

from django_pandas.io import read_frame
qs = MyModel.objects.all()
df = read_frame(qs)
</code></pre>
";;['class MyModel(models.Model):\n    full_name = models.CharField(max_length=25)\n    age = models.IntegerField()\n    department = models.CharField(max_length=3)\n    wage = models.FloatField()\n\nfrom django_pandas.io import read_frame\nqs = MyModel.objects.all()\ndf = read_frame(qs)\n'];['class MyModel(models.Model):\n    full_name = models.CharField(max_length=25)\n    age = models.IntegerField()\n    department = models.CharField(max_length=3)\n    wage = models.FloatField()\n\nfrom django_pandas.io import read_frame\nqs = MyModel.objects.all()\ndf = read_frame(qs)\n']
1294;;2;30022658;28757389.0;2;70;;;"<p><strong>loc:</strong> only work on index<Br>
<strong>iloc:</strong> work on position<Br>
<strong>ix:</strong> You can get data from dataframe without it being in the index<Br>
<strong>at:</strong> get scalar values. It's a very fast loc<Br>
<strong>iat:</strong> Get scalar values. It's a very fast iloc<Br></p>

<p><a href=""http://pyciencia.blogspot.com/2015/05/obtener-y-filtrar-datos-de-un-dataframe.html"">http://pyciencia.blogspot.com/2015/05/obtener-y-filtrar-datos-de-un-dataframe.html</a></p>
";;[];[]
1295;;1;30201213;12021730.0;2;24;;;"<p>You should be able to just do this, which @DSM just taught me in another thread:</p>

<pre><code>data = read_table('sample.txt', skiprows=3, header=None, delim_whitespace=True)
</code></pre>

<p><a href=""http://pandas-docs.github.io/pandas-docs-travis/io.html#csv-text-files"" rel=""noreferrer"">Documentation</a></p>
";;"[""data = read_table('sample.txt', skiprows=3, header=None, delim_whitespace=True)\n""]";"[""data = read_table('sample.txt', skiprows=3, header=None, delim_whitespace=True)\n""]"
1296;;9;30267328;24458645.0;2;43;;;"<p>As mentioned by larsmans, <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"" rel=""noreferrer"">LabelEncoder() only takes a 1-d array as an argument</a>. That said, it is quite easy to roll your own label encoder that operates on multiple columns of your choosing, and returns a transformed dataframe. My code here is based in part on Zac Stewart's excellent blog post found <a href=""http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html"" rel=""noreferrer"">here</a>.</p>

<p>Creating a custom encoder involves simply creating a class that responds to the <code>fit()</code>, <code>transform()</code>, and <code>fit_transform()</code> methods. In your case, a good start might be something like this:
</p>

<pre><code>import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline

# Create some toy data in a Pandas dataframe
fruit_data = pd.DataFrame({
    'fruit':  ['apple','orange','pear','orange'],
    'color':  ['red','orange','green','green'],
    'weight': [5,6,3,4]
})

class MultiColumnLabelEncoder:
    def __init__(self,columns = None):
        self.columns = columns # array of column names to encode

    def fit(self,X,y=None):
        return self # not relevant here

    def transform(self,X):
        '''
        Transforms columns of X specified in self.columns using
        LabelEncoder(). If no columns specified, transforms all
        columns in X.
        '''
        output = X.copy()
        if self.columns is not None:
            for col in self.columns:
                output[col] = LabelEncoder().fit_transform(output[col])
        else:
            for colname,col in output.iteritems():
                output[colname] = LabelEncoder().fit_transform(col)
        return output

    def fit_transform(self,X,y=None):
        return self.fit(X,y).transform(X)
</code></pre>

<p>Suppose we want to encode our two categorical attributes (<code>fruit</code> and <code>color</code>), while leaving the numeric attribute <code>weight</code> alone. We could do this as follows:
</p>

<pre><code>MultiColumnLabelEncoder(columns = ['fruit','color']).fit_transform(fruit_data)
</code></pre>

<p>Which transforms our <code>fruit_data</code> dataset from</p>

<p><img src=""https://i.stack.imgur.com/aqGcU.png"" alt=""enter image description here""> to </p>

<p><img src=""https://i.stack.imgur.com/xISwE.png"" alt=""enter image description here""></p>

<p>Passing it a dataframe consisting entirely of categorical variables and omitting the <code>columns</code> parameter will result in every column being encoded (which I believe is what you were originally looking for):
</p>

<pre><code>MultiColumnLabelEncoder().fit_transform(fruit_data.drop('weight',axis=1))
</code></pre>

<p>This transforms</p>

<p><img src=""https://i.stack.imgur.com/zKgcI.png"" alt=""enter image description here""> to</p>

<p><img src=""https://i.stack.imgur.com/5KwKW.png"" alt=""enter image description here"">.</p>

<p>Note that it'll probably choke when it tries to encode attributes that are already numeric (add some code to handle this if you like).</p>

<p>Another nice feature about this is that we can use this custom transformer in a pipeline:
</p>

<pre><code>encoding_pipeline = Pipeline([
    ('encoding',MultiColumnLabelEncoder(columns=['fruit','color']))
    # add more pipeline steps as needed
])
encoding_pipeline.fit_transform(fruit_data)
</code></pre>
";;"[""import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\n\n# Create some toy data in a Pandas dataframe\nfruit_data = pd.DataFrame({\n    'fruit':  ['apple','orange','pear','orange'],\n    'color':  ['red','orange','green','green'],\n    'weight': [5,6,3,4]\n})\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n"", ""MultiColumnLabelEncoder(columns = ['fruit','color']).fit_transform(fruit_data)\n"", ""MultiColumnLabelEncoder().fit_transform(fruit_data.drop('weight',axis=1))\n"", ""encoding_pipeline = Pipeline([\n    ('encoding',MultiColumnLabelEncoder(columns=['fruit','color']))\n    # add more pipeline steps as needed\n])\nencoding_pipeline.fit_transform(fruit_data)\n""]";"['fit()', 'transform()', 'fit_transform()', ""import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\n\n# Create some toy data in a Pandas dataframe\nfruit_data = pd.DataFrame({\n    'fruit':  ['apple','orange','pear','orange'],\n    'color':  ['red','orange','green','green'],\n    'weight': [5,6,3,4]\n})\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n"", 'fruit', 'color', 'weight', ""MultiColumnLabelEncoder(columns = ['fruit','color']).fit_transform(fruit_data)\n"", 'fruit_data', 'columns', ""MultiColumnLabelEncoder().fit_transform(fruit_data.drop('weight',axis=1))\n"", ""encoding_pipeline = Pipeline([\n    ('encoding',MultiColumnLabelEncoder(columns=['fruit','color']))\n    # add more pipeline steps as needed\n])\nencoding_pipeline.fit_transform(fruit_data)\n""]"
1297;;0;30292938;17530542.0;2;10;;;"<p>A little helper function I use with some header checking safeguards to handle it all:</p>

<pre><code>def appendDFToCSV_void(df, csvFilePath, sep="",""):
    import os
    if not os.path.isfile(csvFilePath):
        df.to_csv(csvFilePath, mode='a', index=False, sep=sep)
    elif len(df.columns) != len(pd.read_csv(csvFilePath, nrows=1, sep=sep).columns):
        raise Exception(""Columns do not match!! Dataframe has "" + str(len(df.columns)) + "" columns. CSV file has "" + str(len(pd.read_csv(csvFilePath, nrows=1, sep=sep).columns)) + "" columns."")
    elif not (df.columns == pd.read_csv(csvFilePath, nrows=1, sep=sep).columns).all():
        raise Exception(""Columns and column order of dataframe and csv file do not match!!"")
    else:
        df.to_csv(csvFilePath, mode='a', index=False, sep=sep, header=False)
</code></pre>
";;"['def appendDFToCSV_void(df, csvFilePath, sep="",""):\n    import os\n    if not os.path.isfile(csvFilePath):\n        df.to_csv(csvFilePath, mode=\'a\', index=False, sep=sep)\n    elif len(df.columns) != len(pd.read_csv(csvFilePath, nrows=1, sep=sep).columns):\n        raise Exception(""Columns do not match!! Dataframe has "" + str(len(df.columns)) + "" columns. CSV file has "" + str(len(pd.read_csv(csvFilePath, nrows=1, sep=sep).columns)) + "" columns."")\n    elif not (df.columns == pd.read_csv(csvFilePath, nrows=1, sep=sep).columns).all():\n        raise Exception(""Columns and column order of dataframe and csv file do not match!!"")\n    else:\n        df.to_csv(csvFilePath, mode=\'a\', index=False, sep=sep, header=False)\n']";"['def appendDFToCSV_void(df, csvFilePath, sep="",""):\n    import os\n    if not os.path.isfile(csvFilePath):\n        df.to_csv(csvFilePath, mode=\'a\', index=False, sep=sep)\n    elif len(df.columns) != len(pd.read_csv(csvFilePath, nrows=1, sep=sep).columns):\n        raise Exception(""Columns do not match!! Dataframe has "" + str(len(df.columns)) + "" columns. CSV file has "" + str(len(pd.read_csv(csvFilePath, nrows=1, sep=sep).columns)) + "" columns."")\n    elif not (df.columns == pd.read_csv(csvFilePath, nrows=1, sep=sep).columns).all():\n        raise Exception(""Columns and column order of dataframe and csv file do not match!!"")\n    else:\n        df.to_csv(csvFilePath, mode=\'a\', index=False, sep=sep, header=False)\n']"
1298;;0;30370897;11728836.0;2;10;;;"<p>From the comments above, it seems that this is planned for <code>pandas</code> some time (there's also an interesting-looking <a href=""https://pypi.python.org/pypi/rosetta/0.2.4""><code>rosetta</code> project</a> which I just noticed).</p>

<p>However, until every parallel functionality is incorporated into <code>pandas</code>, I noticed that it's very easy to write efficient &amp; non-memory-copying parallel augmentations to <code>pandas</code> directly using <a href=""http://cython.org/""><code>cython</code></a> + <a href=""http://www.google.co.il/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CB0QFjAA&amp;url=http%3A%2F%2Fwww.openmp.org%2F&amp;ei=HKpdVfyVJcj8ULXHgcAF&amp;usg=AFQjCNGlD5aZM8ZP3Qx7WXT74Y7C54jLNQ&amp;bvm=bv.93756505,d.d24"">OpenMP</a> and C++.</p>

<p>Here's a short example of writing a parallel groupby-sum, whose use is something like this:</p>

<pre><code>import pandas as pd
import para_group_demo

df = pd.DataFrame({'a': [1, 2, 1, 2, 1, 1, 0], 'b': range(7)})
print para_group_demo.sum(df.a, df.b)
</code></pre>

<p>and output is:</p>

<pre><code>     sum
key     
0      6
1      11
2      4
</code></pre>

<hr>

<p><strong>Note</strong> Doubtlessly, this simple example's functionality will eventually be part of <code>pandas</code>. Some things, however, will be more natural to parallelize in C++ for some time, and it's important to be aware of how easy it is to combine this into <code>pandas</code>.</p>

<hr>

<p>To do this, I wrote a simple single-source-file extension whose code follows.</p>

<p>It starts with some imports and type definitions</p>

<pre><code>from libc.stdint cimport int64_t, uint64_t
from libcpp.vector cimport vector
from libcpp.unordered_map cimport unordered_map

cimport cython
from cython.operator cimport dereference as deref, preincrement as inc
from cython.parallel import prange

import pandas as pd

ctypedef unordered_map[int64_t, uint64_t] counts_t
ctypedef unordered_map[int64_t, uint64_t].iterator counts_it_t
ctypedef vector[counts_t] counts_vec_t
</code></pre>

<p>The C++ <code>unordered_map</code> type is for summing by a single thread, and the <code>vector</code> is for summing by all threads.</p>

<p>Now to the function <code>sum</code>. It starts off with <a href=""http://docs.cython.org/src/userguide/memoryviews.html"">typed memory views</a> for fast access:</p>

<pre><code>def sum(crit, vals):
    cdef int64_t[:] crit_view = crit.values
    cdef int64_t[:] vals_view = vals.values
</code></pre>

<p>The function continues by dividing the semi-equally to the threads (here hardcoded to 4), and having each thread sum the entries in its range:</p>

<pre><code>    cdef uint64_t num_threads = 4
    cdef uint64_t l = len(crit)
    cdef uint64_t s = l / num_threads + 1
    cdef uint64_t i, j, e
    cdef counts_vec_t counts
    counts = counts_vec_t(num_threads)
    counts.resize(num_threads)
    with cython.boundscheck(False):
        for i in prange(num_threads, nogil=True): 
            j = i * s
            e = j + s
            if e &gt; l:
                e = l
            while j &lt; e:
                counts[i][crit_view[j]] += vals_view[j]
                inc(j)
</code></pre>

<p>When the threads have completed, the function merges all the results (from the different ranges) into a single <code>unordered_map</code>:</p>

<pre><code>    cdef counts_t total
    cdef counts_it_t it, e_it
    for i in range(num_threads):
        it = counts[i].begin()
        e_it = counts[i].end()
        while it != e_it:
            total[deref(it).first] += deref(it).second
            inc(it)        
</code></pre>

<p>All that's left is to create a <code>DataFrame</code> and return the results:</p>

<pre><code>    key, sum_ = [], []
    it = total.begin()
    e_it = total.end()
    while it != e_it:
        key.append(deref(it).first)
        sum_.append(deref(it).second)
        inc(it)

    df = pd.DataFrame({'key': key, 'sum': sum_})
    df.set_index('key', inplace=True)
    return df
</code></pre>
";;"[""import pandas as pd\nimport para_group_demo\n\ndf = pd.DataFrame({'a': [1, 2, 1, 2, 1, 1, 0], 'b': range(7)})\nprint para_group_demo.sum(df.a, df.b)\n"", '     sum\nkey     \n0      6\n1      11\n2      4\n', 'from libc.stdint cimport int64_t, uint64_t\nfrom libcpp.vector cimport vector\nfrom libcpp.unordered_map cimport unordered_map\n\ncimport cython\nfrom cython.operator cimport dereference as deref, preincrement as inc\nfrom cython.parallel import prange\n\nimport pandas as pd\n\nctypedef unordered_map[int64_t, uint64_t] counts_t\nctypedef unordered_map[int64_t, uint64_t].iterator counts_it_t\nctypedef vector[counts_t] counts_vec_t\n', 'def sum(crit, vals):\n    cdef int64_t[:] crit_view = crit.values\n    cdef int64_t[:] vals_view = vals.values\n', '    cdef uint64_t num_threads = 4\n    cdef uint64_t l = len(crit)\n    cdef uint64_t s = l / num_threads + 1\n    cdef uint64_t i, j, e\n    cdef counts_vec_t counts\n    counts = counts_vec_t(num_threads)\n    counts.resize(num_threads)\n    with cython.boundscheck(False):\n        for i in prange(num_threads, nogil=True): \n            j = i * s\n            e = j + s\n            if e > l:\n                e = l\n            while j < e:\n                counts[i][crit_view[j]] += vals_view[j]\n                inc(j)\n', '    cdef counts_t total\n    cdef counts_it_t it, e_it\n    for i in range(num_threads):\n        it = counts[i].begin()\n        e_it = counts[i].end()\n        while it != e_it:\n            total[deref(it).first] += deref(it).second\n            inc(it)        \n', ""    key, sum_ = [], []\n    it = total.begin()\n    e_it = total.end()\n    while it != e_it:\n        key.append(deref(it).first)\n        sum_.append(deref(it).second)\n        inc(it)\n\n    df = pd.DataFrame({'key': key, 'sum': sum_})\n    df.set_index('key', inplace=True)\n    return df\n""]";"['pandas', 'rosetta', 'pandas', 'pandas', 'cython', ""import pandas as pd\nimport para_group_demo\n\ndf = pd.DataFrame({'a': [1, 2, 1, 2, 1, 1, 0], 'b': range(7)})\nprint para_group_demo.sum(df.a, df.b)\n"", '     sum\nkey     \n0      6\n1      11\n2      4\n', 'pandas', 'pandas', 'from libc.stdint cimport int64_t, uint64_t\nfrom libcpp.vector cimport vector\nfrom libcpp.unordered_map cimport unordered_map\n\ncimport cython\nfrom cython.operator cimport dereference as deref, preincrement as inc\nfrom cython.parallel import prange\n\nimport pandas as pd\n\nctypedef unordered_map[int64_t, uint64_t] counts_t\nctypedef unordered_map[int64_t, uint64_t].iterator counts_it_t\nctypedef vector[counts_t] counts_vec_t\n', 'unordered_map', 'vector', 'sum', 'def sum(crit, vals):\n    cdef int64_t[:] crit_view = crit.values\n    cdef int64_t[:] vals_view = vals.values\n', '    cdef uint64_t num_threads = 4\n    cdef uint64_t l = len(crit)\n    cdef uint64_t s = l / num_threads + 1\n    cdef uint64_t i, j, e\n    cdef counts_vec_t counts\n    counts = counts_vec_t(num_threads)\n    counts.resize(num_threads)\n    with cython.boundscheck(False):\n        for i in prange(num_threads, nogil=True): \n            j = i * s\n            e = j + s\n            if e > l:\n                e = l\n            while j < e:\n                counts[i][crit_view[j]] += vals_view[j]\n                inc(j)\n', 'unordered_map', '    cdef counts_t total\n    cdef counts_it_t it, e_it\n    for i in range(num_threads):\n        it = counts[i].begin()\n        e_it = counts[i].end()\n        while it != e_it:\n            total[deref(it).first] += deref(it).second\n            inc(it)        \n', 'DataFrame', ""    key, sum_ = [], []\n    it = total.begin()\n    e_it = total.end()\n    while it != e_it:\n        key.append(deref(it).first)\n        sum_.append(deref(it).second)\n        inc(it)\n\n    df = pd.DataFrame({'key': key, 'sum': sum_})\n    df.set_index('key', inplace=True)\n    return df\n""]"
1299;;4;30380922;11346283.0;2;38;;;"<pre><code>old_names = ['$a', '$b', '$c', '$d', '$e'] 
new_names = ['a', 'b', 'c', 'd', 'e']
df.rename(columns=dict(zip(old_names, new_names)), inplace=True)
</code></pre>

<p>This way you can manually edit the <code>new_names</code> as you wish.
Works great when you need to rename only a few columns to correct mispellings, accents, remove special characters etc.</p>
";;"[""old_names = ['$a', '$b', '$c', '$d', '$e'] \nnew_names = ['a', 'b', 'c', 'd', 'e']\ndf.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n""]";"[""old_names = ['$a', '$b', '$c', '$d', '$e'] \nnew_names = ['a', 'b', 'c', 'd', 'e']\ndf.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n"", 'new_names']"
1300;;2;30424537;20109391.0;2;28;;;"<h2>How to create sample datasets</h2>

<p>This is to mainly to expand on @AndyHayden's answer by providing examples of how you can create sample dataframes.  Pandas and (especially) numpy give you a variety of tools for this such that you can generally create a reasonable facsimile of any real dataset with just a few lines of code.</p>

<p>After importing numpy and pandas, be sure to provide a random seed if you want folks to be able to exactly reproduce your data and results.</p>

<pre><code>import numpy as np
import pandas as pd

np.random.seed(123)
</code></pre>

<h3>A kitchen sink example</h3>

<p>Here's an example showing a variety of things you can do.  All kinds of useful sample dataframes could be created from a subset of this:</p>

<pre><code>df = pd.DataFrame({ 

    # some ways to create random data
    'a':np.random.randn(6),
    'b':np.random.choice( [5,7,np.nan], 6),
    'c':np.random.choice( ['panda','python','shark'], 6),

    # some ways to create systematic groups for indexing or groupby
    # this is similar to r's expand.grid(), see note 2 below
    'd':np.repeat( range(3), 2 ),
    'e':np.tile(   range(2), 3 ),

    # a date range and set of random dates
    'f':pd.date_range('1/1/2011', periods=6, freq='D'),
    'g':np.random.choice( pd.date_range('1/1/2011', periods=365, 
                          freq='D'), 6, replace=False) 
    })
</code></pre>

<p>This produces:</p>

<pre><code>          a   b       c  d  e          f          g
0 -1.085631 NaN   panda  0  0 2011-01-01 2011-08-12
1  0.997345   7   shark  0  1 2011-01-02 2011-11-10
2  0.282978   5   panda  1  0 2011-01-03 2011-10-30
3 -1.506295   7  python  1  1 2011-01-04 2011-09-07
4 -0.578600 NaN   shark  2  0 2011-01-05 2011-02-27
5  1.651437   7  python  2  1 2011-01-06 2011-02-03
</code></pre>

<p>Some notes:</p>

<ol>
<li><code>np.repeat</code> and <code>np.tile</code> (columns <code>d</code> and <code>e</code>) are very useful for creating groups and indices in a very regular way.  For 2 columns, this can be used to easily duplicate r's <code>expand.grid()</code> but is also more flexible in ability to provide a subset of all permutations.  However, for 3 or more columns the syntax quickly becomes unwieldy.</li>
<li>For a more direct replacement for r's <code>expand.grid()</code> see the <code>itertools</code> solution in the <a href=""http://pandas.pydata.org/pandas-docs/version/0.16.1/cookbook.html#creating-example-data"" rel=""nofollow noreferrer"">pandas cookbook</a> or the <code>np.meshgrid</code> solution shown <a href=""https://stackoverflow.com/questions/12130883/r-expand-grid-function-in-python"">here</a>.  Those will allow any number of dimensions.</li>
<li>You can do quite a bit with <code>np.random.choice</code>.  For example, in column <code>g</code>, we have a random selection of 6 dates from 2011.  Additionally, by setting <code>replace=False</code> we can assure these dates are unique -- very handy if we want to use this as an index with unique values.</li>
</ol>

<h3>Fake stock market data</h3>

<p>In addition to taking subsets of the above code, you can further combine the techniques to do just about anything.  For example, here's a short example that combines <code>np.tile</code> and <code>date_range</code> to create sample ticker data for 4 stocks covering the same dates:</p>

<pre><code>stocks = pd.DataFrame({ 
    'ticker':np.repeat( ['aapl','goog','yhoo','msft'], 25 ),
    'date':np.tile( pd.date_range('1/1/2011', periods=25, freq='D'), 4 ),
    'price':(np.random.randn(100).cumsum() + 10) })
</code></pre>

<p>Now we have a sample dataset with 100 lines (25 dates per ticker), but we have only used 4 lines to do it, making it easy for everyone else to reproduce without copying and pasting 100 lines of code.  You can then display subsets of the data if it helps to explain your question:</p>

<pre><code>&gt;&gt;&gt; stocks.head(5)

        date      price ticker
0 2011-01-01   9.497412   aapl
1 2011-01-02  10.261908   aapl
2 2011-01-03   9.438538   aapl
3 2011-01-04   9.515958   aapl
4 2011-01-05   7.554070   aapl

&gt;&gt;&gt; stocks.groupby('ticker').head(2)

         date      price ticker
0  2011-01-01   9.497412   aapl
1  2011-01-02  10.261908   aapl
25 2011-01-01   8.277772   goog
26 2011-01-02   7.714916   goog
50 2011-01-01   5.613023   yhoo
51 2011-01-02   6.397686   yhoo
75 2011-01-01  11.736584   msft
76 2011-01-02  11.944519   msft
</code></pre>
";;"['import numpy as np\nimport pandas as pd\n\nnp.random.seed(123)\n', ""df = pd.DataFrame({ \n\n    # some ways to create random data\n    'a':np.random.randn(6),\n    'b':np.random.choice( [5,7,np.nan], 6),\n    'c':np.random.choice( ['panda','python','shark'], 6),\n\n    # some ways to create systematic groups for indexing or groupby\n    # this is similar to r's expand.grid(), see note 2 below\n    'd':np.repeat( range(3), 2 ),\n    'e':np.tile(   range(2), 3 ),\n\n    # a date range and set of random dates\n    'f':pd.date_range('1/1/2011', periods=6, freq='D'),\n    'g':np.random.choice( pd.date_range('1/1/2011', periods=365, \n                          freq='D'), 6, replace=False) \n    })\n"", '          a   b       c  d  e          f          g\n0 -1.085631 NaN   panda  0  0 2011-01-01 2011-08-12\n1  0.997345   7   shark  0  1 2011-01-02 2011-11-10\n2  0.282978   5   panda  1  0 2011-01-03 2011-10-30\n3 -1.506295   7  python  1  1 2011-01-04 2011-09-07\n4 -0.578600 NaN   shark  2  0 2011-01-05 2011-02-27\n5  1.651437   7  python  2  1 2011-01-06 2011-02-03\n', ""stocks = pd.DataFrame({ \n    'ticker':np.repeat( ['aapl','goog','yhoo','msft'], 25 ),\n    'date':np.tile( pd.date_range('1/1/2011', periods=25, freq='D'), 4 ),\n    'price':(np.random.randn(100).cumsum() + 10) })\n"", "">>> stocks.head(5)\n\n        date      price ticker\n0 2011-01-01   9.497412   aapl\n1 2011-01-02  10.261908   aapl\n2 2011-01-03   9.438538   aapl\n3 2011-01-04   9.515958   aapl\n4 2011-01-05   7.554070   aapl\n\n>>> stocks.groupby('ticker').head(2)\n\n         date      price ticker\n0  2011-01-01   9.497412   aapl\n1  2011-01-02  10.261908   aapl\n25 2011-01-01   8.277772   goog\n26 2011-01-02   7.714916   goog\n50 2011-01-01   5.613023   yhoo\n51 2011-01-02   6.397686   yhoo\n75 2011-01-01  11.736584   msft\n76 2011-01-02  11.944519   msft\n""]";"['import numpy as np\nimport pandas as pd\n\nnp.random.seed(123)\n', ""df = pd.DataFrame({ \n\n    # some ways to create random data\n    'a':np.random.randn(6),\n    'b':np.random.choice( [5,7,np.nan], 6),\n    'c':np.random.choice( ['panda','python','shark'], 6),\n\n    # some ways to create systematic groups for indexing or groupby\n    # this is similar to r's expand.grid(), see note 2 below\n    'd':np.repeat( range(3), 2 ),\n    'e':np.tile(   range(2), 3 ),\n\n    # a date range and set of random dates\n    'f':pd.date_range('1/1/2011', periods=6, freq='D'),\n    'g':np.random.choice( pd.date_range('1/1/2011', periods=365, \n                          freq='D'), 6, replace=False) \n    })\n"", '          a   b       c  d  e          f          g\n0 -1.085631 NaN   panda  0  0 2011-01-01 2011-08-12\n1  0.997345   7   shark  0  1 2011-01-02 2011-11-10\n2  0.282978   5   panda  1  0 2011-01-03 2011-10-30\n3 -1.506295   7  python  1  1 2011-01-04 2011-09-07\n4 -0.578600 NaN   shark  2  0 2011-01-05 2011-02-27\n5  1.651437   7  python  2  1 2011-01-06 2011-02-03\n', 'np.repeat', 'np.tile', 'd', 'e', 'expand.grid()', 'expand.grid()', 'itertools', 'np.meshgrid', 'np.random.choice', 'g', 'replace=False', 'np.tile', 'date_range', ""stocks = pd.DataFrame({ \n    'ticker':np.repeat( ['aapl','goog','yhoo','msft'], 25 ),\n    'date':np.tile( pd.date_range('1/1/2011', periods=25, freq='D'), 4 ),\n    'price':(np.random.randn(100).cumsum() + 10) })\n"", "">>> stocks.head(5)\n\n        date      price ticker\n0 2011-01-01   9.497412   aapl\n1 2011-01-02  10.261908   aapl\n2 2011-01-03   9.438538   aapl\n3 2011-01-04   9.515958   aapl\n4 2011-01-05   7.554070   aapl\n\n>>> stocks.groupby('ticker').head(2)\n\n         date      price ticker\n0  2011-01-01   9.497412   aapl\n1  2011-01-02  10.261908   aapl\n25 2011-01-01   8.277772   goog\n26 2011-01-02   7.714916   goog\n50 2011-01-01   5.613023   yhoo\n51 2011-01-02   6.397686   yhoo\n75 2011-01-01  11.736584   msft\n76 2011-01-02  11.944519   msft\n""]"
1301;;2;30454743;24147278.0;2;20;;;"<p>I would use scikit-learn's own training_test_split, and generate it from the index</p>

<pre><code>from sklearn.cross_validation import train_test_split


y = df.pop('output')
X = df

X_train,X_test,y_train,y_test = train_test_split(X.index,y,test_size=0.2)
X.iloc[X_train] # return dataframe train
</code></pre>
";;"[""from sklearn.cross_validation import train_test_split\n\n\ny = df.pop('output')\nX = df\n\nX_train,X_test,y_train,y_test = train_test_split(X.index,y,test_size=0.2)\nX.iloc[X_train] # return dataframe train\n""]";"[""from sklearn.cross_validation import train_test_split\n\n\ny = df.pop('output')\nX = df\n\nX_train,X_test,y_train,y_test = train_test_split(X.index,y,test_size=0.2)\nX.iloc[X_train] # return dataframe train\n""]"
1302;;3;30511605;19482970.0;2;21;;;"<pre><code>&gt;&gt;&gt; list(my_dataframe)
['y', 'gdp', 'cap']
</code></pre>

<p>To list the columns of a dataframe while in debugger mode, use a list comprehension:</p>

<pre><code>&gt;&gt;&gt; [c for c in my_dataframe]
['y', 'gdp', 'cap']
</code></pre>
";;"["">>> list(my_dataframe)\n['y', 'gdp', 'cap']\n"", "">>> [c for c in my_dataframe]\n['y', 'gdp', 'cap']\n""]";"["">>> list(my_dataframe)\n['y', 'gdp', 'cap']\n"", "">>> [c for c in my_dataframe]\n['y', 'gdp', 'cap']\n""]"
1303;;4;30512931;23668427.0;2;133;;;"<p>Assumed imports:</p>

<pre><code>import pandas as pd
</code></pre>

<p><a href=""https://stackoverflow.com/a/23671390/366309"">John Galt's answer</a> is basically a <code>reduce</code> operation.  If I have more than a handful of dataframes, I'd put them in a list like this (generated via list comprehensions or loops or whatnot):</p>

<pre><code>dfs = [df0, df1, df2, dfN]
</code></pre>

<p>Assuming they have some common column, like <code>name</code> in your example, I'd do the following:</p>

<pre><code>df_final = reduce(lambda left,right: pd.merge(left,right,on='name'), dfs)
</code></pre>

<p>That way, your code should work with whatever number of dataframes you want to merge.</p>

<p><em>Edit August 1, 2016</em>: For those using Python 3: <code>reduce</code> has been moved into <code>functools</code>. So to use this function, you'll first need to import that module.</p>
";;"['import pandas as pd\n', 'dfs = [df0, df1, df2, dfN]\n', ""df_final = reduce(lambda left,right: pd.merge(left,right,on='name'), dfs)\n""]";"['import pandas as pd\n', 'reduce', 'dfs = [df0, df1, df2, dfN]\n', 'name', ""df_final = reduce(lambda left,right: pd.merge(left,right,on='name'), dfs)\n"", 'reduce', 'functools']"
1304;;2;30522778;30522724.0;2;59;;;"<p>I think you're almost there, try removing the extra square brackets around the lst's (Also you don't need to specify the column names when you're creating a dataframe from a dict like this):</p>

<pre><code>lst1 = range(100)
lst2 = range(100)
lst3 = range(100)
percentile_list = pd.DataFrame(
    {'lst1Tite': lst1,
     'lst2Tite': lst2,
     'lst3Tite': lst3
    })

percentile_list
    lst1Tite  lst2Tite  lst3Tite
0          0         0         0
1          1         1         1
2          2         2         2
3          3         3         3
4          4         4         4
5          5         5         5
6          6         6         6
...
</code></pre>

<p>If you need a somewhat more performant solution you can use <code>np.column_stack</code> rather than <code>zip</code> as in your first attempt, this has around a 2x speedup on the example here, however comes at bit of a cost of readability in my opinion:</p>

<pre><code>percentile_list = pd.DataFrame(np.column_stack([lst1, lst2, lst3]), 
                               columns=['lst1tite', 'lst2itie', 'lst3tite'])
</code></pre>
";;"[""lst1 = range(100)\nlst2 = range(100)\nlst3 = range(100)\npercentile_list = pd.DataFrame(\n    {'lst1Tite': lst1,\n     'lst2Tite': lst2,\n     'lst3Tite': lst3\n    })\n\npercentile_list\n    lst1Tite  lst2Tite  lst3Tite\n0          0         0         0\n1          1         1         1\n2          2         2         2\n3          3         3         3\n4          4         4         4\n5          5         5         5\n6          6         6         6\n...\n"", ""percentile_list = pd.DataFrame(np.column_stack([lst1, lst2, lst3]), \n                               columns=['lst1tite', 'lst2itie', 'lst3tite'])\n""]";"[""lst1 = range(100)\nlst2 = range(100)\nlst3 = range(100)\npercentile_list = pd.DataFrame(\n    {'lst1Tite': lst1,\n     'lst2Tite': lst2,\n     'lst3Tite': lst3\n    })\n\npercentile_list\n    lst1Tite  lst2Tite  lst3Tite\n0          0         0         0\n1          1         1         1\n2          2         2         2\n3          3         3         3\n4          4         4         4\n5          5         5         5\n6          6         6         6\n...\n"", 'np.column_stack', 'zip', ""percentile_list = pd.DataFrame(np.column_stack([lst1, lst2, lst3]), \n                               columns=['lst1tite', 'lst2itie', 'lst3tite'])\n""]"
1305;;0;30523225;30522982.0;2;6;;;"<p><strong>Users</strong> </p>

<ol>
<li><p>When you need to append some new user just make a new list of all user details and append it</p></li>
<li><p>Easily sortable as @StevenRumbalski suggested</p></li>
<li><p>Searching will be easy</p></li>
<li><p>This is more compact and easily manageable as record grows (for some very high number of records I think we will need something better than users too)</p></li>
</ol>

<p><strong>Users2</strong></p>

<ol>
<li>Personally I am seeing this for the first time and I wouldn't approach this if I have a high number of records.</li>
</ol>

<p>PS: But I would like to learn advantages of users2 over users
    Again a nice question</p>
";;[];[]
1306;;0;30525128;30522982.0;2;23;;;"<p>This relates to <a href=""https://en.wikipedia.org/wiki/Column-oriented_DBMS"">column oriented databases</a> versus row oriented. Your first example is a row oriented data structure, and the second is column oriented. In the particular case of Python, the first could be made notably more efficient using <a href=""https://docs.python.org/3/reference/datamodel.html#slots"">slots</a>, such that the dictionary of columns doesn't need to be duplicated for every row. </p>

<p>Which form works better depends a lot on what you do with the data; for instance, row oriented is natural if you only ever access all of any row. Column oriented meanwhile makes much better use of caches and such when you're searching by a particular field (in Python, this may be reduced by the heavy use of references; types like <a href=""https://docs.python.org/3/library/array.html"">array</a> can optimize that). Traditional row oriented databases frequently use column oriented sorted indices to speed up lookups, and knowing these techniques you can implement any combination using a key-value store. </p>

<p>Pandas does convert both your examples to the same format, but the conversion itself is more expensive for the row oriented structure, simply because every individual dictionary must be read. All of these costs may be marginal. </p>

<p>There's a third option not evident in your example: In this case, you only have two columns, one of which is an integer ID in a contiguous range from 0. This can be stored in the order of the entries itself, meaning the entire structure would be found in the list you've called <code>users2['name']</code>; but notably, the entries are incomplete without their position. The list translates into rows using enumerate(). It is common for databases to have this special case also (for instance, sqlite <a href=""https://www.sqlite.org/lang_createtable.html#rowid"">rowid</a>). </p>

<p>In general, start with a data structure that keeps your code sensible, and optimize only when you know your use cases and have a measurable performance issue. Tools like Pandas probably means most projects will function just fine without finetuning. </p>
";;[];"[""users2['name']""]"
1307;;1;30531939;30530663.0;2;52;;;"<p>You can use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html""><code>drop_duplicates</code></a> method to get the unique rows in a DataFrame:</p>

<pre><code>In [29]: df = pd.DataFrame({'a':[1,2,1,2], 'b':[3,4,3,5]})

In [30]: df
Out[30]:
   a  b
0  1  3
1  2  4
2  1  3
3  2  5

In [32]: df.drop_duplicates()
Out[32]:
   a  b
0  1  3
1  2  4
3  2  5
</code></pre>

<p>You can also provide the <code>subset</code> keyword argument if you only want to use certain columns to determine uniqueness. See the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html"">docstring</a>.</p>
";;"[""In [29]: df = pd.DataFrame({'a':[1,2,1,2], 'b':[3,4,3,5]})\n\nIn [30]: df\nOut[30]:\n   a  b\n0  1  3\n1  2  4\n2  1  3\n3  2  5\n\nIn [32]: df.drop_duplicates()\nOut[32]:\n   a  b\n0  1  3\n1  2  4\n3  2  5\n""]";"['drop_duplicates', ""In [29]: df = pd.DataFrame({'a':[1,2,1,2], 'b':[3,4,3,5]})\n\nIn [30]: df\nOut[30]:\n   a  b\n0  1  3\n1  2  4\n2  1  3\n3  2  5\n\nIn [32]: df.drop_duplicates()\nOut[32]:\n   a  b\n0  1  3\n1  2  4\n3  2  5\n"", 'subset']"
1308;;0;30535957;19618912.0;2;32;;;"<p>My understanding is that this question is better answered over in <a href=""https://stackoverflow.com/questions/26921943/pandas-intersection-of-two-data-frames-based-on-column-entries"">this post</a>.</p>

<p>But briefly, the answer to the OP with this method is simply:</p>

<pre><code>s1 = pd.merge(df1, df2, how='inner', on=['user_id'])
</code></pre>

<p>Which gives s1 with 5 columns: user_id and the other two columns from each of df1 and df2.</p>
";;"[""s1 = pd.merge(df1, df2, how='inner', on=['user_id'])\n""]";"[""s1 = pd.merge(df1, df2, how='inner', on=['user_id'])\n""]"
1309;;0;30546734;11346283.0;2;76;;;"<p>As documented in <a href=""http://pandas.pydata.org/pandas-docs/stable/text.html"">http://pandas.pydata.org/pandas-docs/stable/text.html</a>:</p>

<pre><code>df.columns = df.columns.str.replace('$','')
</code></pre>
";;"[""df.columns = df.columns.str.replace('$','')\n""]";"[""df.columns = df.columns.str.replace('$','')\n""]"
1310;;3;30566899;16476924.0;2;37;;;"<p>You can also use <code>df.apply()</code> to iterate over rows and access multiple columns for a function.</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html"">docs: DataFrame.apply()</a></p>

<pre><code>def valuation_formula(x, y):
    return x * y * 0.5

df['price'] = df.apply(lambda row: valuation_formula(row['x'], row['y']), axis=1)
</code></pre>
";;"[""def valuation_formula(x, y):\n    return x * y * 0.5\n\ndf['price'] = df.apply(lambda row: valuation_formula(row['x'], row['y']), axis=1)\n""]";"['df.apply()', ""def valuation_formula(x, y):\n    return x * y * 0.5\n\ndf['price'] = df.apply(lambda row: valuation_formula(row['x'], row['y']), axis=1)\n""]"
1311;;0;30587837;13295735.0;2;15;;;"<p>I just wanted to provide a bit of an update/special case since it looks like people still come here. If you're using a multi-index or otherwise using an index-slicer the inplace=True option may not be enough to update the slice you've chosen. For example in a 2x2 level multi-index this will not change any values (as of pandas 0.15):</p>

<pre class=""lang-py prettyprint-override""><code>idx = pd.IndexSlice
df.loc[idx[:,mask_1],idx[mask_2,:]].fillna(value=0,inplace=True)
</code></pre>

<p>The ""problem"" is that the chaining breaks the fillna ability to update the original dataframe. I put ""problem"" in quotes because there are good reasons for the design decisions that led to not interpreting through these chains in certain situations. Also, this is a complex example (though I really ran into it), but the same may apply to fewer levels of indexes depending on how you slice.</p>

<p>The solution is DataFrame.update:</p>

<pre class=""lang-py prettyprint-override""><code>df.update(df.loc[idx[:,mask_1],idx[[mask_2],:]].fillna(value=0))
</code></pre>

<p>It's one line, reads reasonably well (sort of) and eliminates any unnecessary messing with intermediate variables or loops while allowing you to apply fillna to any multi-level slice you like!</p>

<p>If anybody can find places this doesn't work please post in the comments, I've been messing with it and looking at the source and it seems to solve at least my multi-index slice problems.</p>
";;['idx = pd.IndexSlice\ndf.loc[idx[:,mask_1],idx[mask_2,:]].fillna(value=0,inplace=True)\n', 'df.update(df.loc[idx[:,mask_1],idx[[mask_2],:]].fillna(value=0))\n'];['idx = pd.IndexSlice\ndf.loc[idx[:,mask_1],idx[mask_2,:]].fillna(value=0,inplace=True)\n', 'df.update(df.loc[idx[:,mask_1],idx[[mask_2],:]].fillna(value=0))\n']
1312;;3;30590280;20025325.0;2;30;;;"<p>As already suggested by HYRY in the comments, <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html"" rel=""noreferrer"">Series.map</a> is the way to go here. Just set the index to the resulting series.</p>

<p>Simple example:</p>

<pre><code>df = pd.DataFrame({'d': [1, 2, 3]}, index=['FOO', 'BAR', 'BAZ'])
df
        d
FOO     1
BAR     2
BAZ     3

df.index = df.index.map(str.lower)
df
        d
foo     1
bar     2
baz     3
</code></pre>

<h1>Index != Series</h1>

<p>As pointed out by @OP. the <code>df.index.map(str.lower)</code> call returns a numpy array.
This is because dataframe indices <em>are</em> based on numpy arrays, not Series.</p>

<p>The only way of making the index into a Series is to create a Series from it.</p>

<pre><code>pd.Series(df.index.map(str.lower))
</code></pre>

<h3>Caveat</h3>

<p>The <code>Index</code> class now subclasses the <code>StringAccessorMixin</code>, which means that you can do the above operation as follows</p>

<pre><code>df.index.str.lower()
</code></pre>

<p>This still produces an Index object, not a Series.</p>
";;"[""df = pd.DataFrame({'d': [1, 2, 3]}, index=['FOO', 'BAR', 'BAZ'])\ndf\n        d\nFOO     1\nBAR     2\nBAZ     3\n\ndf.index = df.index.map(str.lower)\ndf\n        d\nfoo     1\nbar     2\nbaz     3\n"", 'pd.Series(df.index.map(str.lower))\n', 'df.index.str.lower()\n']";"[""df = pd.DataFrame({'d': [1, 2, 3]}, index=['FOO', 'BAR', 'BAZ'])\ndf\n        d\nFOO     1\nBAR     2\nBAZ     3\n\ndf.index = df.index.map(str.lower)\ndf\n        d\nfoo     1\nbar     2\nbaz     3\n"", 'df.index.map(str.lower)', 'pd.Series(df.index.map(str.lower))\n', 'Index', 'StringAccessorMixin', 'df.index.str.lower()\n']"
1313;;1;30633167;18022845.0;2;7;;;"<p>If you do not want to create a new row but simply put it in the empty cell then use:</p>

<pre><code>df.columns.name = 'foo'
</code></pre>

<p>Otherwise use: </p>

<pre><code>df.index.name = 'foo'
</code></pre>
";;"[""df.columns.name = 'foo'\n"", ""df.index.name = 'foo'\n""]";"[""df.columns.name = 'foo'\n"", ""df.index.name = 'foo'\n""]"
1314;;1;30647987;22483588.0;2;13;;;"<p>You can see e.gs. in the <a href=""http://pandas.pydata.org/pandas-docs/version/0.13.1/visualization.html#targeting-different-subplots"" rel=""nofollow noreferrer"">documentation</a> demonstrating joris answer. Also from the documentation, you could also set <code>subplots=True</code> and <code>layout=(,)</code> within the pandas <code>plot</code> function:  </p>

<pre><code>df.plot(subplots=True, layout=(1,2))
</code></pre>

<p>You could also use <code>fig.add_subplot()</code> which takes subplot grid parameters such as 221, 222, 223, 224, etc. as described in the post <a href=""https://stackoverflow.com/questions/3584805/in-matplotlib-what-does-111-mean-in-fig-add-subplot111"">here</a>. Nice examples of plot on pandas data frame, including subplots, can be seen in <a href=""http://nbviewer.ipython.org/urls/gist.github.com/fonnesbeck/5850463/raw/a29d9ffb863bfab09ff6c1fc853e1d5bf69fe3e4/3.+Plotting+and+Visualization.ipynb"" rel=""nofollow noreferrer"">this ipython notebook</a>.</p>
";;['df.plot(subplots=True, layout=(1,2))\n'];['subplots=True', 'layout=(,)', 'plot', 'df.plot(subplots=True, layout=(1,2))\n', 'fig.add_subplot()']
1315;;2;30653988;30631325.0;2;37;;;"<p>Using the engine in place of the raw_connection() worked:</p>

<pre><code>import pandas as pd
import mysql.connector
from sqlalchemy import create_engine

engine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)
data.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)
</code></pre>

<p>not clear on why when I tried this yesterday it gave me the earlier error</p>
";;"[""import pandas as pd\nimport mysql.connector\nfrom sqlalchemy import create_engine\n\nengine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)\ndata.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)\n""]";"[""import pandas as pd\nimport mysql.connector\nfrom sqlalchemy import create_engine\n\nengine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)\ndata.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)\n""]"
1316;;6;30691921;19124601.0;2;221;;;"<p>You can also use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.option_context.html"" rel=""noreferrer"">option_context</a>, with one or more options:</p>

<pre><code>with pd.option_context('display.max_rows', None, 'display.max_columns', 3):
    print(df)
</code></pre>

<p>This will automatically return the options to their previous values.</p>
";;"[""with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n    print(df)\n""]";"[""with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n    print(df)\n""]"
1317;;1;30733959;10464738.0;2;6;;;"<p>Old thread but thought I would share my solution with 2d extrapolation/interpolation, respecting index values, which also works on demand. Code ended up a bit weird so let me know if there is a better solution:</p>

<pre><code>import pandas
from   numpy import nan
import numpy

dataGrid = pandas.DataFrame({1: {1: 1, 3: 2},
                             2: {1: 3, 3: 4}})


def getExtrapolatedInterpolatedValue(x, y):
    global dataGrid
    if x not in dataGrid.index:
        dataGrid.ix[x] = nan
        dataGrid = dataGrid.sort()
        dataGrid = dataGrid.interpolate(method='index', axis=0).ffill(axis=0).bfill(axis=0)

    if y not in dataGrid.columns.values:
        dataGrid = dataGrid.reindex(columns=numpy.append(dataGrid.columns.values, y))
        dataGrid = dataGrid.sort_index(axis=1)
        dataGrid = dataGrid.interpolate(method='index', axis=1).ffill(axis=1).bfill(axis=1)

    return dataGrid[y][x]


print getExtrapolatedInterpolatedValue(2, 1.4)
&gt;&gt;2.3
</code></pre>
";;"[""import pandas\nfrom   numpy import nan\nimport numpy\n\ndataGrid = pandas.DataFrame({1: {1: 1, 3: 2},\n                             2: {1: 3, 3: 4}})\n\n\ndef getExtrapolatedInterpolatedValue(x, y):\n    global dataGrid\n    if x not in dataGrid.index:\n        dataGrid.ix[x] = nan\n        dataGrid = dataGrid.sort()\n        dataGrid = dataGrid.interpolate(method='index', axis=0).ffill(axis=0).bfill(axis=0)\n\n    if y not in dataGrid.columns.values:\n        dataGrid = dataGrid.reindex(columns=numpy.append(dataGrid.columns.values, y))\n        dataGrid = dataGrid.sort_index(axis=1)\n        dataGrid = dataGrid.interpolate(method='index', axis=1).ffill(axis=1).bfill(axis=1)\n\n    return dataGrid[y][x]\n\n\nprint getExtrapolatedInterpolatedValue(2, 1.4)\n>>2.3\n""]";"[""import pandas\nfrom   numpy import nan\nimport numpy\n\ndataGrid = pandas.DataFrame({1: {1: 1, 3: 2},\n                             2: {1: 3, 3: 4}})\n\n\ndef getExtrapolatedInterpolatedValue(x, y):\n    global dataGrid\n    if x not in dataGrid.index:\n        dataGrid.ix[x] = nan\n        dataGrid = dataGrid.sort()\n        dataGrid = dataGrid.interpolate(method='index', axis=0).ffill(axis=0).bfill(axis=0)\n\n    if y not in dataGrid.columns.values:\n        dataGrid = dataGrid.reindex(columns=numpy.append(dataGrid.columns.values, y))\n        dataGrid = dataGrid.sort_index(axis=1)\n        dataGrid = dataGrid.interpolate(method='index', axis=1).ffill(axis=1).bfill(axis=1)\n\n    return dataGrid[y][x]\n\n\nprint getExtrapolatedInterpolatedValue(2, 1.4)\n>>2.3\n""]"
1318;;1;30777185;12555323.0;2;12;;;"<p>I got the dreaded <code>SettingWithCopyWarning</code>, and it wasn't fixed by using the iloc syntax. My DataFrame was created by read_sql from an ODBC source. Using a suggestion by lowtech above, the following worked for me:</p>

<pre><code>df.insert(len(df.columns), 'e', pd.Series(np.random.randn(sLength),  index=df.index))
</code></pre>

<p>This worked fine to insert the column at the end. I don't know if it is the most efficient, but I don't like warning messages. I think there is a better solution, but I can't find it, and I think it depends on some aspect of the index.<br>
<em>Note</em>. That this only works once and will give an error message if trying to overwrite and existing column.<br>
<strong>Note</strong> As above and from 0.16.0 assign is the best solution. See documentation <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html#pandas.DataFrame.assign"" rel=""nofollow"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html#pandas.DataFrame.assign</a> 
Works well for data flow type where you don't overwrite your intermediate values.</p>
";;"[""df.insert(len(df.columns), 'e', pd.Series(np.random.randn(sLength),  index=df.index))\n""]";"['SettingWithCopyWarning', ""df.insert(len(df.columns), 'e', pd.Series(np.random.randn(sLength),  index=df.index))\n""]"
1319;;3;30778300;13611065.0;2;10;;;"<p>Chaining conditions creates long lines, which are discouraged by pep8. 
Using the .query method forces to use strings, which is powerful but unpythonic and not very dynamic. </p>

<p>Once each of the filters is in place, one approach is</p>

<pre><code>import numpy as np
import functools
def conjunction(*conditions):
    return functools.reduce(np.logical_and, conditions)

c_1 = data.col1 == True
c_2 = data.col2 &lt; 64
c_3 = data.col3 != 4

data_filtered = data[conjunction(c1,c2,c3)]
</code></pre>

<p>np.logical operates on and is fast, but does not take more than two arguments, which is handled by functools.reduce. </p>

<p>Note that this still has some redundancies: a) shortcutting does not happen on a global level b) Each of the individual conditions runs on the whole initial data. Still, I expect this to be efficient enough for many applications and it is very readable. </p>
";;['import numpy as np\nimport functools\ndef conjunction(*conditions):\n    return functools.reduce(np.logical_and, conditions)\n\nc_1 = data.col1 == True\nc_2 = data.col2 < 64\nc_3 = data.col3 != 4\n\ndata_filtered = data[conjunction(c1,c2,c3)]\n'];['import numpy as np\nimport functools\ndef conjunction(*conditions):\n    return functools.reduce(np.logical_and, conditions)\n\nc_1 = data.col1 == True\nc_2 = data.col2 < 64\nc_3 = data.col3 != 4\n\ndata_filtered = data[conjunction(c1,c2,c3)]\n']
1320;;2;30858753;12190874.0;2;6;;;"<p>If you're using pandas.read_csv you can directly sample when loading the data, by using the skiprows parameter. Here is a short article I've written on this - <a href=""https://nikolaygrozev.wordpress.com/2015/06/16/fast-and-simple-sampling-in-pandas-when-loading-data-from-files/"">https://nikolaygrozev.wordpress.com/2015/06/16/fast-and-simple-sampling-in-pandas-when-loading-data-from-files/</a></p>
";;[];[]
1321;;6;30926717;30926670.0;2;24;;;"<p>I'd <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html#pandas.concat""><code>concat</code></a> using a DataFrame ctor:</p>

<pre><code>In [23]:
df = pd.DataFrame(columns=['A'])
df

Out[23]:
Empty DataFrame
Columns: [A]
Index: []

In [24]:    
pd.concat([df,pd.DataFrame(columns=list('BCD'))])

Out[24]:
Empty DataFrame
Columns: [A, B, C, D]
Index: []
</code></pre>

<p>So by passing a list containing your original df, and a new one with the columns you wish to add, this will return a new df with the additional columns.</p>
";;"[""In [23]:\ndf = pd.DataFrame(columns=['A'])\ndf\n\nOut[23]:\nEmpty DataFrame\nColumns: [A]\nIndex: []\n\nIn [24]:    \npd.concat([df,pd.DataFrame(columns=list('BCD'))])\n\nOut[24]:\nEmpty DataFrame\nColumns: [A, B, C, D]\nIndex: []\n""]";"['concat', ""In [23]:\ndf = pd.DataFrame(columns=['A'])\ndf\n\nOut[23]:\nEmpty DataFrame\nColumns: [A]\nIndex: []\n\nIn [24]:    \npd.concat([df,pd.DataFrame(columns=list('BCD'))])\n\nOut[24]:\nEmpty DataFrame\nColumns: [A, B, C, D]\nIndex: []\n""]"
1322;;0;30943503;30926670.0;2;26;;;"<p>You could use <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.reindex.html""><code>df.reindex</code></a> to add new columns:</p>

<pre><code>In [18]: df = pd.DataFrame(np.random.randint(10, size=(5,1)), columns=['A'])

In [19]: df
Out[19]: 
   A
0  4
1  7
2  0
3  7
4  6

In [20]: df.reindex(columns=list('ABCD'))
Out[20]: 
   A   B   C   D
0  4 NaN NaN NaN
1  7 NaN NaN NaN
2  0 NaN NaN NaN
3  7 NaN NaN NaN
4  6 NaN NaN NaN
</code></pre>

<p><code>reindex</code> will return a new DataFrame, with columns appearing in the order they are listed:</p>

<pre><code>In [31]: df.reindex(columns=list('DCBA'))
Out[31]: 
    D   C   B  A
0 NaN NaN NaN  4
1 NaN NaN NaN  7
2 NaN NaN NaN  0
3 NaN NaN NaN  7
4 NaN NaN NaN  6
</code></pre>

<p>The <code>reindex</code> method as a <code>fill_value</code> parameter as well:</p>

<pre><code>In [22]: df.reindex(columns=list('ABCD'), fill_value=0)
Out[22]: 
   A  B  C  D
0  4  0  0  0
1  7  0  0  0
2  0  0  0  0
3  7  0  0  0
4  6  0  0  0
</code></pre>
";;"[""In [18]: df = pd.DataFrame(np.random.randint(10, size=(5,1)), columns=['A'])\n\nIn [19]: df\nOut[19]: \n   A\n0  4\n1  7\n2  0\n3  7\n4  6\n\nIn [20]: df.reindex(columns=list('ABCD'))\nOut[20]: \n   A   B   C   D\n0  4 NaN NaN NaN\n1  7 NaN NaN NaN\n2  0 NaN NaN NaN\n3  7 NaN NaN NaN\n4  6 NaN NaN NaN\n"", ""In [31]: df.reindex(columns=list('DCBA'))\nOut[31]: \n    D   C   B  A\n0 NaN NaN NaN  4\n1 NaN NaN NaN  7\n2 NaN NaN NaN  0\n3 NaN NaN NaN  7\n4 NaN NaN NaN  6\n"", ""In [22]: df.reindex(columns=list('ABCD'), fill_value=0)\nOut[22]: \n   A  B  C  D\n0  4  0  0  0\n1  7  0  0  0\n2  0  0  0  0\n3  7  0  0  0\n4  6  0  0  0\n""]";"['df.reindex', ""In [18]: df = pd.DataFrame(np.random.randint(10, size=(5,1)), columns=['A'])\n\nIn [19]: df\nOut[19]: \n   A\n0  4\n1  7\n2  0\n3  7\n4  6\n\nIn [20]: df.reindex(columns=list('ABCD'))\nOut[20]: \n   A   B   C   D\n0  4 NaN NaN NaN\n1  7 NaN NaN NaN\n2  0 NaN NaN NaN\n3  7 NaN NaN NaN\n4  6 NaN NaN NaN\n"", 'reindex', ""In [31]: df.reindex(columns=list('DCBA'))\nOut[31]: \n    D   C   B  A\n0 NaN NaN NaN  4\n1 NaN NaN NaN  7\n2 NaN NaN NaN  0\n3 NaN NaN NaN  7\n4 NaN NaN NaN  6\n"", 'reindex', 'fill_value', ""In [22]: df.reindex(columns=list('ABCD'), fill_value=0)\nOut[22]: \n   A  B  C  D\n0  4  0  0  0\n1  7  0  0  0\n2  0  0  0  0\n3  7  0  0  0\n4  6  0  0  0\n""]"
1323;;2;30971633;12190874.0;2;13;;;"<p>Pandas 0.16.1 have a <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html"">sample</a> method for that.</p>
";;[];[]
1324;;0;30991980;22233488.0;2;10;;;"<p>You could also achieve that by renaming the columns:</p>

<p><code>df.columns = ['a', 'b']</code></p>

<p>This involves a manual step but could be an option especially if you would eventually rename your data frame.</p>
";;[];"[""df.columns = ['a', 'b']""]"
1325;;0;31017785;20937538.0;2;7;;;"<p>Similar to unutbu above, you could also use <code>applymap</code> as follows:</p>

<pre><code>import pandas as pd
df = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],
                  index=['foo','bar','baz','quux'],
                  columns=['cost'])

df = df.applymap(""${0:.2f}"".format)
</code></pre>
";;"['import pandas as pd\ndf = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=[\'foo\',\'bar\',\'baz\',\'quux\'],\n                  columns=[\'cost\'])\n\ndf = df.applymap(""${0:.2f}"".format)\n']";"['applymap', 'import pandas as pd\ndf = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=[\'foo\',\'bar\',\'baz\',\'quux\'],\n                  columns=[\'cost\'])\n\ndf = df.applymap(""${0:.2f}"".format)\n']"
1326;;1;31029857;31029560.0;2;43;;;"<p>You can simply use <code>value_counts</code> on the series:</p>

<pre><code>df['colour'].value_counts().plot(kind='bar')
</code></pre>

<p><a href=""https://i.stack.imgur.com/ouoSE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ouoSE.png"" alt=""enter image description here""></a></p>
";;"[""df['colour'].value_counts().plot(kind='bar')\n""]";"['value_counts', ""df['colour'].value_counts().plot(kind='bar')\n""]"
1327;;0;31029861;31029560.0;2;9;;;"<p>like this : </p>

<pre><code>df.groupby('colour').size().plot(kind='bar')
</code></pre>
";;"[""df.groupby('colour').size().plot(kind='bar')\n""]";"[""df.groupby('colour').size().plot(kind='bar')\n""]"
1328;;2;31033603;31029560.0;2;6;;;"<p>You might find useful <a href=""http://statsmodels.sourceforge.net/stable/generated/statsmodels.graphics.mosaicplot.mosaic.html"" rel=""nofollow noreferrer""><code>mosaic</code></a> plot from statsmodels. Which can also give statistical highlighting for the variances.</p>

<pre><code>from statsmodels.graphics.mosaicplot import mosaic
plt.rcParams['font.size'] = 16.0
mosaic(df, ['direction', 'colour']);
</code></pre>

<p><img src=""https://i.stack.imgur.com/xo97n.png"" alt=""enter image description here""></p>

<p>But beware of the 0 sized cell - they will cause problems with labels.</p>

<p>See <a href=""https://stackoverflow.com/a/31031988/4077912"">this answer</a> for details</p>
";;"[""from statsmodels.graphics.mosaicplot import mosaic\nplt.rcParams['font.size'] = 16.0\nmosaic(df, ['direction', 'colour']);\n""]";"['mosaic', ""from statsmodels.graphics.mosaicplot import mosaic\nplt.rcParams['font.size'] = 16.0\nmosaic(df, ['direction', 'colour']);\n""]"
1329;;1;31036962;12680754.0;2;7;;;"<p>Similar question as: <a href=""https://stackoverflow.com/questions/17116814/pandas-how-do-i-split-text-in-a-column-into-multiple-columns"">pandas: How do I split text in a column into multiple rows?</a></p>

<p>You could do:</p>

<pre><code>&gt;&gt; a=pd.DataFrame({""var1"":""a,b,c d,e,f"".split(),""var2"":[1,2]})
&gt;&gt; s = a.var1.str.split("","").apply(pd.Series, 1).stack()
&gt;&gt; s.index = s.index.droplevel(-1)
&gt;&gt; del a['var1']
&gt;&gt; a.join(s)
   var2 var1
0     1    a
0     1    b
0     1    c
1     2    d
1     2    e
1     2    f
</code></pre>
";;"['>> a=pd.DataFrame({""var1"":""a,b,c d,e,f"".split(),""var2"":[1,2]})\n>> s = a.var1.str.split("","").apply(pd.Series, 1).stack()\n>> s.index = s.index.droplevel(-1)\n>> del a[\'var1\']\n>> a.join(s)\n   var2 var1\n0     1    a\n0     1    b\n0     1    c\n1     2    d\n1     2    e\n1     2    f\n']";"['>> a=pd.DataFrame({""var1"":""a,b,c d,e,f"".split(),""var2"":[1,2]})\n>> s = a.var1.str.split("","").apply(pd.Series, 1).stack()\n>> s.index = s.index.droplevel(-1)\n>> del a[\'var1\']\n>> a.join(s)\n   var2 var1\n0     1    a\n0     1    b\n0     1    c\n1     2    d\n1     2    e\n1     2    f\n']"
1330;;1;31037040;10715965.0;2;34;;;"<pre><code>mycolumns = ['A', 'B']
df = pd.DataFrame(columns=mycolumns)
rows = [[1,2],[3,4],[5,6]]
for row in rows:
    df.loc[len(df)] = row
</code></pre>
";;"[""mycolumns = ['A', 'B']\ndf = pd.DataFrame(columns=mycolumns)\nrows = [[1,2],[3,4],[5,6]]\nfor row in rows:\n    df.loc[len(df)] = row\n""]";"[""mycolumns = ['A', 'B']\ndf = pd.DataFrame(columns=mycolumns)\nrows = [[1,2],[3,4],[5,6]]\nfor row in rows:\n    df.loc[len(df)] = row\n""]"
1331;;4;31061820;20219254.0;2;21;;;"<p>For me skyjur's answer almost worked. I had to set the engine for the writer explicitly with:</p>

<pre><code>writer = pd.ExcelWriter(excel_file, engine='openpyxl')
</code></pre>

<p>otherwise it would throw</p>

<pre><code>AttributeError: 'Workbook' object has no attribute 'add_worksheet'
</code></pre>
";;"[""writer = pd.ExcelWriter(excel_file, engine='openpyxl')\n"", ""AttributeError: 'Workbook' object has no attribute 'add_worksheet'\n""]";"[""writer = pd.ExcelWriter(excel_file, engine='openpyxl')\n"", ""AttributeError: 'Workbook' object has no attribute 'add_worksheet'\n""]"
1332;;0;31075478;20461165.0;2;18;;;"<p>For MultiIndex you can extract its subindex using </p>

<pre><code>df['si_name'] = R.index.get_level_values('si_name') 
</code></pre>

<p>where <code>si_name</code> is the name of the subindex.</p>
";;"[""df['si_name'] = R.index.get_level_values('si_name') \n""]";"[""df['si_name'] = R.index.get_level_values('si_name') \n"", 'si_name']"
1333;;2;31076657;15325182.0;2;6;;;"<p>Multiple column search with dataframe:</p>

<pre><code>frame[frame.filename.str.match('*.'+MetaData+'.*') &amp; frame.file_path.str.match('C:\test\test.txt')]
</code></pre>
";;"[""frame[frame.filename.str.match('*.'+MetaData+'.*') & frame.file_path.str.match('C:\\test\\test.txt')]\n""]";"[""frame[frame.filename.str.match('*.'+MetaData+'.*') & frame.file_path.str.match('C:\\test\\test.txt')]\n""]"
1334;;1;31173785;19913659.0;2;42;;;"<p>List comprehension is another way to create another column conditionally. If you are working with object dtypes in columns, like in your example, list comprehensions typically outperform most other methods.</p>

<p>Example list comprehension:</p>

<pre><code>df['color'] = ['red' if x == 'Z' else 'green' for x in df['Set']]
</code></pre>

<p><strong>%timeit tests:</strong></p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})
%timeit df['color'] = ['red' if x == 'Z' else 'green' for x in df['Set']]
%timeit df['color'] = np.where(df['Set']=='Z', 'green', 'red')
%timeit df['color'] = df.Set.map( lambda x: 'red' if x == 'Z' else 'green')

1000 loops, best of 3: 239 s per loop
1000 loops, best of 3: 523 s per loop
1000 loops, best of 3: 263 s per loop
</code></pre>
";;"[""df['color'] = ['red' if x == 'Z' else 'green' for x in df['Set']]\n"", ""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})\n%timeit df['color'] = ['red' if x == 'Z' else 'green' for x in df['Set']]\n%timeit df['color'] = np.where(df['Set']=='Z', 'green', 'red')\n%timeit df['color'] = df.Set.map( lambda x: 'red' if x == 'Z' else 'green')\n\n1000 loops, best of 3: 239 s per loop\n1000 loops, best of 3: 523 s per loop\n1000 loops, best of 3: 263 s per loop\n""]";"[""df['color'] = ['red' if x == 'Z' else 'green' for x in df['Set']]\n"", ""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})\n%timeit df['color'] = ['red' if x == 'Z' else 'green' for x in df['Set']]\n%timeit df['color'] = np.where(df['Set']=='Z', 'green', 'red')\n%timeit df['color'] = df.Set.map( lambda x: 'red' if x == 'Z' else 'green')\n\n1000 loops, best of 3: 239 s per loop\n1000 loops, best of 3: 523 s per loop\n1000 loops, best of 3: 263 s per loop\n""]"
1335;;2;31296878;17071871.0;2;58;;;"<h3>tl;dr</h3>

<p>The pandas equivalent to </p>

<pre><code>select * from table where column_name = some_value
</code></pre>

<p>is</p>

<pre><code>table[table.column_name == some_value]
</code></pre>

<p>Multiple conditions:</p>

<pre><code>table((table.column_name == some_value) | (table.column_name2 == some_value2))
</code></pre>

<p>or</p>

<pre><code>table.query('column_name == some_value | column_name2 == some_value2')
</code></pre>

<h3>Code example</h3>

<pre><code>import pandas as pd

# Create data set
d = {'foo':[100, 111, 222], 
     'bar':[333, 444, 555]}
df = pd.DataFrame(d)

# Full dataframe:
df

# Shows:
#    bar   foo 
# 0  333   100
# 1  444   111
# 2  555   222

# Output only the row(s) in df where foo is 222:
df[df.foo == 222]

# Shows:
#    bar  foo
# 2  555  222
</code></pre>

<p>In the above code it is the line <code>df[df.foo == 222]</code> that gives the rows based on the column value, <code>222</code> in this case.</p>

<p>Multiple conditions are also possible:</p>

<pre><code>df[(df.foo == 222) | (df.bar == 444)]
#    bar  foo
# 1  444  111
# 2  555  222
</code></pre>

<p>But at that point I would recommend using the query function, since it's less verbose and yields the same result:</p>

<pre><code>df.query('foo == 222 | bar == 444')
</code></pre>
";;"['select * from table where column_name = some_value\n', 'table[table.column_name == some_value]\n', 'table((table.column_name == some_value) | (table.column_name2 == some_value2))\n', ""table.query('column_name == some_value | column_name2 == some_value2')\n"", ""import pandas as pd\n\n# Create data set\nd = {'foo':[100, 111, 222], \n     'bar':[333, 444, 555]}\ndf = pd.DataFrame(d)\n\n# Full dataframe:\ndf\n\n# Shows:\n#    bar   foo \n# 0  333   100\n# 1  444   111\n# 2  555   222\n\n# Output only the row(s) in df where foo is 222:\ndf[df.foo == 222]\n\n# Shows:\n#    bar  foo\n# 2  555  222\n"", 'df[(df.foo == 222) | (df.bar == 444)]\n#    bar  foo\n# 1  444  111\n# 2  555  222\n', ""df.query('foo == 222 | bar == 444')\n""]";"['select * from table where column_name = some_value\n', 'table[table.column_name == some_value]\n', 'table((table.column_name == some_value) | (table.column_name2 == some_value2))\n', ""table.query('column_name == some_value | column_name2 == some_value2')\n"", ""import pandas as pd\n\n# Create data set\nd = {'foo':[100, 111, 222], \n     'bar':[333, 444, 555]}\ndf = pd.DataFrame(d)\n\n# Full dataframe:\ndf\n\n# Shows:\n#    bar   foo \n# 0  333   100\n# 1  444   111\n# 2  555   222\n\n# Output only the row(s) in df where foo is 222:\ndf[df.foo == 222]\n\n# Shows:\n#    bar  foo\n# 2  555  222\n"", 'df[df.foo == 222]', '222', 'df[(df.foo == 222) | (df.bar == 444)]\n#    bar  foo\n# 1  444  111\n# 2  555  222\n', ""df.query('foo == 222 | bar == 444')\n""]"
1336;;2;31357733;31357611.0;2;40;;;"<p>pandas dataframe plot will return the <code>ax</code> for you, And then you can start to manipulate the axes whatever you want.</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randn(100,5))

# you get ax from here
ax = df.plot()
type(ax)  # matplotlib.axes._subplots.AxesSubplot

# manipulate
vals = ax.get_yticks()
ax.set_yticklabels(['{:3.2f}%'.format(x*100) for x in vals])
</code></pre>

<p><img src=""https://i.stack.imgur.com/lZTy0.png"" alt=""enter image description here""></p>
";;"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randn(100,5))\n\n# you get ax from here\nax = df.plot()\ntype(ax)  # matplotlib.axes._subplots.AxesSubplot\n\n# manipulate\nvals = ax.get_yticks()\nax.set_yticklabels(['{:3.2f}%'.format(x*100) for x in vals])\n""]";"['ax', ""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randn(100,5))\n\n# you get ax from here\nax = df.plot()\ntype(ax)  # matplotlib.axes._subplots.AxesSubplot\n\n# manipulate\nvals = ax.get_yticks()\nax.set_yticklabels(['{:3.2f}%'.format(x*100) for x in vals])\n""]"
1337;;0;31364094;22483588.0;2;7;;;"<p>You can use the familiar Matplotlib style calling a <code>figure</code> and <code>subplot</code>, but you simply need to specify the current axis using <code>plt.gca()</code>. An example:</p>

<pre><code>plt.figure(1)
plt.subplot(2,2,1)
df.A.plot() #no need to specify for first axis
plt.subplot(2,2,2)
df.B.plot(ax=plt.gca())
plt.subplot(2,2,3)
df.C.plot(ax=plt.gca())
</code></pre>

<p>etc...</p>
";;['plt.figure(1)\nplt.subplot(2,2,1)\ndf.A.plot() #no need to specify for first axis\nplt.subplot(2,2,2)\ndf.B.plot(ax=plt.gca())\nplt.subplot(2,2,3)\ndf.C.plot(ax=plt.gca())\n'];['figure', 'subplot', 'plt.gca()', 'plt.figure(1)\nplt.subplot(2,2,1)\ndf.A.plot() #no need to specify for first axis\nplt.subplot(2,2,2)\ndf.B.plot(ax=plt.gca())\nplt.subplot(2,2,3)\ndf.C.plot(ax=plt.gca())\n']
1338;;5;31364127;31361721.0;2;30;;;"<h3><code>map_partitions</code></h3>

<p>You can apply your function to all of the partitions of your dataframe with the <code>map_partitions</code> function.</p>

<pre><code>df.map_partitions(func, columns=...)
</code></pre>

<p>Note that func will be given only part of the dataset at a time, not the entire dataset like with <code>pandas apply</code> (which presumably you wouldn't want if you want to do parallelism.)</p>

<h3><code>map</code> / <code>apply</code></h3>

<p>You can map a function row-wise across a series with <code>map</code></p>

<pre><code>df.mycolumn.map(func)
</code></pre>

<p>You can map a function row-wise across a dataframe with <code>apply</code></p>

<pre><code>df.apply(func, axis=1)
</code></pre>

<h3>Threads vs Processes</h3>

<p>As of version 0.6.0 <code>dask.dataframes</code> parallelizes with threads. Custom Python functions will not receive much benefit from thread-based parallelism.  You could try processes instead</p>

<pre><code>df = dd.read_csv(...)

from dask.multiprocessing import get
df.map_partitions(func, columns=...).compute(get=get)
</code></pre>

<h3>But avoid <code>apply</code></h3>

<p>However, you should really avoid <code>apply</code> with custom Python functions, both in Pandas and in Dask.  This is often a source of poor performance.  It could be that if you find a way to do your operation in a vectorized manner then it could be that your Pandas code will be 100x faster and you won't need dask.dataframe at all.</p>

<h3>Consider <code>numba</code></h3>

<p>For your particular problem you might consider <a href=""http://numba.pydata.org/""><code>numba</code></a>.  This significantly improves your performance.</p>

<pre><code>In [1]: import numpy as np
In [2]: import pandas as pd
In [3]: s = pd.Series([10000]*120)

In [4]: %paste
def slow_func(k):
    A = np.random.normal(size = k) # k = 10000
    s = 0
    for a in A:
        if a &gt; 0:
            s += 1
        else:
            s -= 1
    return s
## -- End pasted text --

In [5]: %time _ = s.apply(slow_func)
CPU times: user 345 ms, sys: 3.28 ms, total: 348 ms
Wall time: 347 ms

In [6]: import numba
In [7]: fast_func = numba.jit(slow_func)

In [8]: %time _ = s.apply(fast_func)  # First time incurs compilation overhead
CPU times: user 179 ms, sys: 0 ns, total: 179 ms
Wall time: 175 ms

In [9]: %time _ = s.apply(fast_func)  # Subsequent times are all gain
CPU times: user 68.8 ms, sys: 27 s, total: 68.8 ms
Wall time: 68.7 ms
</code></pre>

<p>Disclaimer, I work for the company that makes both <code>numba</code> and <code>dask</code> and employs many of the <code>pandas</code> developers.</p>
";;['df.map_partitions(func, columns=...)\n', 'df.mycolumn.map(func)\n', 'df.apply(func, axis=1)\n', 'df = dd.read_csv(...)\n\nfrom dask.multiprocessing import get\ndf.map_partitions(func, columns=...).compute(get=get)\n', 'In [1]: import numpy as np\nIn [2]: import pandas as pd\nIn [3]: s = pd.Series([10000]*120)\n\nIn [4]: %paste\ndef slow_func(k):\n    A = np.random.normal(size = k) # k = 10000\n    s = 0\n    for a in A:\n        if a > 0:\n            s += 1\n        else:\n            s -= 1\n    return s\n## -- End pasted text --\n\nIn [5]: %time _ = s.apply(slow_func)\nCPU times: user 345 ms, sys: 3.28 ms, total: 348 ms\nWall time: 347 ms\n\nIn [6]: import numba\nIn [7]: fast_func = numba.jit(slow_func)\n\nIn [8]: %time _ = s.apply(fast_func)  # First time incurs compilation overhead\nCPU times: user 179 ms, sys: 0 ns, total: 179 ms\nWall time: 175 ms\n\nIn [9]: %time _ = s.apply(fast_func)  # Subsequent times are all gain\nCPU times: user 68.8 ms, sys: 27 s, total: 68.8 ms\nWall time: 68.7 ms\n'];['map_partitions', 'map_partitions', 'df.map_partitions(func, columns=...)\n', 'pandas apply', 'map', 'apply', 'map', 'df.mycolumn.map(func)\n', 'apply', 'df.apply(func, axis=1)\n', 'dask.dataframes', 'df = dd.read_csv(...)\n\nfrom dask.multiprocessing import get\ndf.map_partitions(func, columns=...).compute(get=get)\n', 'apply', 'apply', 'numba', 'numba', 'In [1]: import numpy as np\nIn [2]: import pandas as pd\nIn [3]: s = pd.Series([10000]*120)\n\nIn [4]: %paste\ndef slow_func(k):\n    A = np.random.normal(size = k) # k = 10000\n    s = 0\n    for a in A:\n        if a > 0:\n            s += 1\n        else:\n            s -= 1\n    return s\n## -- End pasted text --\n\nIn [5]: %time _ = s.apply(slow_func)\nCPU times: user 345 ms, sys: 3.28 ms, total: 348 ms\nWall time: 347 ms\n\nIn [6]: import numba\nIn [7]: fast_func = numba.jit(slow_func)\n\nIn [8]: %time _ = s.apply(fast_func)  # First time incurs compilation overhead\nCPU times: user 179 ms, sys: 0 ns, total: 179 ms\nWall time: 175 ms\n\nIn [9]: %time _ = s.apply(fast_func)  # Subsequent times are all gain\nCPU times: user 68.8 ms, sys: 27 s, total: 68.8 ms\nWall time: 68.7 ms\n', 'numba', 'dask', 'pandas']
1339;;0;31384328;29432629.0;2;32;;;"<p>Try this function, which also displays variable names for the correlation matrix:</p>

<pre><code>def plot_corr(df,size=10):
    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.

    Input:
        df: pandas DataFrame
        size: vertical and horizontal size of the plot'''

    corr = df.corr()
    fig, ax = plt.subplots(figsize=(size, size))
    ax.matshow(corr)
    plt.xticks(range(len(corr.columns)), corr.columns);
    plt.yticks(range(len(corr.columns)), corr.columns);
</code></pre>
";;"[""def plot_corr(df,size=10):\n    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n\n    Input:\n        df: pandas DataFrame\n        size: vertical and horizontal size of the plot'''\n\n    corr = df.corr()\n    fig, ax = plt.subplots(figsize=(size, size))\n    ax.matshow(corr)\n    plt.xticks(range(len(corr.columns)), corr.columns);\n    plt.yticks(range(len(corr.columns)), corr.columns);\n""]";"[""def plot_corr(df,size=10):\n    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n\n    Input:\n        df: pandas DataFrame\n        size: vertical and horizontal size of the plot'''\n\n    corr = df.corr()\n    fig, ax = plt.subplots(figsize=(size, size))\n    ax.matshow(corr)\n    plt.xticks(range(len(corr.columns)), corr.columns);\n    plt.yticks(range(len(corr.columns)), corr.columns);\n""]"
1340;;3;31431997;13411544.0;2;56;;;"<h2>Drop by index</h2>

<p>delete first, second and fourth columns:</p>

<pre><code>df.drop(df.columns[[0,1,3]], axis=1, inplace=True)
</code></pre>

<p>delete first column:</p>

<pre><code>df.drop(df.columns[[0]], axis=1, inplace=True)
</code></pre>

<p>There is an optional parameter <code>inplace</code> so that the original
data can be modified without creating a copy.</p>

<h2>Popped</h2>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion"">Column selection, addition, deletion</a></p>

<p>delete column <code>column-name</code>:</p>

<pre><code>df.pop('column-name')
</code></pre>

<h2>Examples:</h2>

<pre><code>df = DataFrame.from_items([('A', [1, 2, 3]), ('B', [4, 5, 6]), ('C', [7,8, 9])], orient='index', columns=['one', 'two', 'three'])
</code></pre>

<p><code>print df</code>:</p>

<pre><code>   one  two  three
A    1    2      3
B    4    5      6
C    7    8      9
</code></pre>

<p><code>df.drop(df.columns[[0]], axis=1, inplace=True)</code><br>
<code>print df</code>:</p>

<pre><code>   two  three
A    2      3
B    5      6
C    8      9
</code></pre>

<p><code>three = df.pop('three')</code><br>
<code>print df</code>:</p>

<pre><code>   two
A    2
B    5
C    8
</code></pre>
";;"['df.drop(df.columns[[0,1,3]], axis=1, inplace=True)\n', 'df.drop(df.columns[[0]], axis=1, inplace=True)\n', ""df.pop('column-name')\n"", ""df = DataFrame.from_items([('A', [1, 2, 3]), ('B', [4, 5, 6]), ('C', [7,8, 9])], orient='index', columns=['one', 'two', 'three'])\n"", '   one  two  three\nA    1    2      3\nB    4    5      6\nC    7    8      9\n', '   two  three\nA    2      3\nB    5      6\nC    8      9\n', '   two\nA    2\nB    5\nC    8\n']";"['df.drop(df.columns[[0,1,3]], axis=1, inplace=True)\n', 'df.drop(df.columns[[0]], axis=1, inplace=True)\n', 'inplace', 'column-name', ""df.pop('column-name')\n"", ""df = DataFrame.from_items([('A', [1, 2, 3]), ('B', [4, 5, 6]), ('C', [7,8, 9])], orient='index', columns=['one', 'two', 'three'])\n"", 'print df', '   one  two  three\nA    1    2      3\nB    4    5      6\nC    7    8      9\n', 'df.drop(df.columns[[0]], axis=1, inplace=True)', 'print df', '   two  three\nA    2      3\nB    5      6\nC    8      9\n', ""three = df.pop('three')"", 'print df', '   two\nA    2\nB    5\nC    8\n']"
1341;;0;31502974;23199796.0;2;13;;;"<p>This answer is similar to that provided by @tanemaki, but uses a <code>lambda</code> expression instead of <code>scipy stats</code>.</p>

<pre><code>df = pd.DataFrame(np.random.randn(100, 3), columns=list('ABC'))

df[df.apply(lambda x: np.abs(x - x.mean()) / x.std() &lt; 3).all(axis=1)]
</code></pre>

<p>To filter the DataFrame where only ONE column (e.g. 'B') is within three standard deviations:</p>

<pre><code>df[((df.B - df.B.mean()) / df.B.std()).abs() &lt; 3]
</code></pre>
";;"[""df = pd.DataFrame(np.random.randn(100, 3), columns=list('ABC'))\n\ndf[df.apply(lambda x: np.abs(x - x.mean()) / x.std() < 3).all(axis=1)]\n"", 'df[((df.B - df.B.mean()) / df.B.std()).abs() < 3]\n']";"['lambda', 'scipy stats', ""df = pd.DataFrame(np.random.randn(100, 3), columns=list('ABC'))\n\ndf[df.apply(lambda x: np.abs(x - x.mean()) / x.std() < 3).all(axis=1)]\n"", 'df[((df.B - df.B.mean()) / df.B.std()).abs() < 3]\n']"
1342;;3;31541600;17557074.0;2;12;;;"<h1>Windows memory limitation</h1>

<p>Memory errors happens a lot with python when using the 32bit version in Windows. This is because 32bit processes <a href=""https://msdn.microsoft.com/en-us/library/aa366778.aspx"" rel=""noreferrer"">only gets 2GB of memory to play with</a> by default.</p>

<h1>Tricks for lowering memory usage</h1>

<p>If you are not using 32bit python in windows but are looking to improve on your memory efficiency while reading csv files, there is a trick.</p>

<p>The <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""noreferrer"">pandas.read_csv function</a> takes an option called <code>dtype</code>. This lets pandas know what types exist inside your csv data.</p>

<h3>How this works</h3>

<p>By default, pandas will try to guess what dtypes your csv file has. This is a very heavy operation because while it is determining the dtype, it has to keep all raw data as objects (strings) in memory.</p>

<h3>Example</h3>

<p>Let's say your csv looks like this:</p>

<pre><code>name, age, birthday
Alice, 30, 1985-01-01
Bob, 35, 1980-01-01
Charlie, 25, 1990-01-01
</code></pre>

<p>This example is of course no problem to read into memory, but it's just an example.</p>

<p>If pandas were to read the above csv file <em>without</em> any dtype option, the age would be stored as strings in memory until pandas has read enough lines of the csv file to make a qualified guess.</p>

<p>I think the default in pandas is to read 1,000,000 rows before guessing the dtype.</p>

<h3>Solution</h3>

<p>By specifying <code>dtype={'age':int}</code> as an option to the <code>.read_csv()</code> will let pandas know that age should be interpreted as a number. This saves you lots of memory.</p>

<h3>Problem with corrupt data</h3>

<p>However, if your csv file would be corrupted, like this:</p>

<pre><code>name, age, birthday
Alice, 30, 1985-01-01
Bob, 35, 1980-01-01
Charlie, 25, 1990-01-01
Dennis, 40+, None-Ur-Bz
</code></pre>

<p>Then specifying <code>dtype={'age':int}</code> will break the <code>.read_csv()</code> command, because it cannot cast <code>""40+""</code> to int. So sanitize your data carefully!</p>

<p>Here you can see how the memory usage of a pandas dataframe is a lot higher when floats are kept as strings:</p>

<h2>Try it yourself</h2>

<pre><code>df = pd.DataFrame(pd.np.random.choice(['1.0', '0.6666667', '150000.1'],(100000, 10)))
resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
# 224544 (~224 MB)

df = pd.DataFrame(pd.np.random.choice([1.0, 0.6666667, 150000.1],(100000, 10)))
resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
# 79560 (~79 MB)
</code></pre>
";;"['name, age, birthday\nAlice, 30, 1985-01-01\nBob, 35, 1980-01-01\nCharlie, 25, 1990-01-01\n', 'name, age, birthday\nAlice, 30, 1985-01-01\nBob, 35, 1980-01-01\nCharlie, 25, 1990-01-01\nDennis, 40+, None-Ur-Bz\n', ""df = pd.DataFrame(pd.np.random.choice(['1.0', '0.6666667', '150000.1'],(100000, 10)))\nresource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# 224544 (~224 MB)\n\ndf = pd.DataFrame(pd.np.random.choice([1.0, 0.6666667, 150000.1],(100000, 10)))\nresource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# 79560 (~79 MB)\n""]";"['dtype', 'name, age, birthday\nAlice, 30, 1985-01-01\nBob, 35, 1980-01-01\nCharlie, 25, 1990-01-01\n', ""dtype={'age':int}"", '.read_csv()', 'name, age, birthday\nAlice, 30, 1985-01-01\nBob, 35, 1980-01-01\nCharlie, 25, 1990-01-01\nDennis, 40+, None-Ur-Bz\n', ""dtype={'age':int}"", '.read_csv()', '""40+""', ""df = pd.DataFrame(pd.np.random.choice(['1.0', '0.6666667', '150000.1'],(100000, 10)))\nresource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# 224544 (~224 MB)\n\ndf = pd.DataFrame(pd.np.random.choice([1.0, 0.6666667, 150000.1],(100000, 10)))\nresource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# 79560 (~79 MB)\n""]"
1343;;0;31543407;18089667.0;2;10;;;"<p>I thought I would bring some more data to the discussion.</p>

<p>I ran a series of tests on this issue.</p>

<p>By using the python <code>resource</code> package I got the memory usage of my process.</p>

<p>And by writing the csv into a <code>StringIO</code> buffer, I could easily measure the size of it in bytes.</p>

<p>I ran two experiments, each one creating 20 dataframes of increasing sizes between 10,000 lines and 1,000,000 lines. Both having 10 columns.</p>

<p>In the first experiment I used only floats in my dataset.</p>

<p>This is how the memory increased in comparison to the csv file as a function of the number of lines. (Size in Megabytes)</p>

<p><img src=""https://i.stack.imgur.com/x9cQs.png"" alt=""Memory and CSV size in Megabytes as a function of the number of rows with float entries""></p>

<p>The second experiment I had the same approach, but the data in the dataset consisted of only short strings.</p>

<p><img src=""https://i.stack.imgur.com/5shue.png"" alt=""Memory and CSV size in Megabytes as a function of the number of rows with string entries""></p>

<p>It seems that the relation of the size of the csv and the size of the dataframe can vary quite a lot, but the size in memory will always be bigger by a factor of 2-3 (for the frame sizes in this experiment)</p>

<p>I would love to complete this answer with more experiments, please comment if you want me to try something special.</p>
";;[];['resource', 'StringIO']
1344;;0;31570270;28218698.0;2;6;;;"<p>A workaround is to transpose the <code>DataFrame</code> and iterate over the rows.</p>

<pre><code>for column_name, column in df.transpose().iterrows():
    print column_name
</code></pre>
";;['for column_name, column in df.transpose().iterrows():\n    print column_name\n'];['DataFrame', 'for column_name, column in df.transpose().iterrows():\n    print column_name\n']
1345;;5;31573180;26473681.0;2;29;;;"<p>For me @Charles Duffy comment solved it.
Put this in your env:</p>

<p><code>LC_ALL=C</code></p>

<p>You can add it to your .bashrc with a line like this:</p>

<p><code>export LC_ALL=C</code></p>

<p>But take in care that you'll affect all other programs. So you may want to use it just for the pip run:</p>

<p><code>$ LC_ALL=C pip install ...</code></p>
";;[];['LC_ALL=C', 'export LC_ALL=C', '$ LC_ALL=C pip install ...']
1346;;11;31593712;31593201.0;2;370;;;"<p><em>Note: in pandas version 0.20.0 and above, <code>ix</code> is <a href=""http://pandas-docs.github.io/pandas-docs-travis/indexing.html#ix-indexer-is-deprecated"" rel=""noreferrer"">deprecated</a> and the use of <code>loc</code> and <code>iloc</code> is encouraged instead. I have left the parts of this answer that describe <code>ix</code> intact as a reference for users of earlier versions of pandas. Examples have been added below showing alternatives to  <code>ix</code></em>.</p>

<hr>

<p>First, a recap:</p>

<ul>
<li><code>loc</code> works on <em>labels</em> in the index. </li>
<li><code>iloc</code> works on the <em>positions</em> in the index (so it only takes integers).</li>
<li><code>ix</code> usually tries to behave like <code>loc</code> but falls back to behaving like <code>iloc</code> if the label is not in the index.</li>
</ul>

<p>It's important to note some subtleties that can make <code>ix</code> slightly tricky to use:</p>

<ul>
<li><p>if the index is of integer type, <code>ix</code> will only use label-based indexing and not fall back to position-based indexing. If the label is not in the index, an error is raised.</p></li>
<li><p>if the index does not contain <em>only</em> integers, then given an integer, <code>ix</code> will immediately use position-based indexing rather than label-based indexing. If however <code>ix</code> is given another type (e.g. a string), it can use label-based indexing. </p></li>
</ul>

<hr>

<p>To illustrate the differences between the three methods, consider the following Series:</p>

<pre><code>&gt;&gt;&gt; s = pd.Series(np.nan, index=[49,48,47,46,45, 1, 2, 3, 4, 5])
&gt;&gt;&gt; s
49   NaN
48   NaN
47   NaN
46   NaN
45   NaN
1    NaN
2    NaN
3    NaN
4    NaN
5    NaN
</code></pre>

<p>Then <code>s.iloc[:3]</code> returns the first 3 rows (since it looks at the position) and <code>s.loc[:3]</code> returns the first 8 rows (since it looks at the labels):</p>

<pre><code>&gt;&gt;&gt; s.iloc[:3] # slice the first three rows
49   NaN
48   NaN
47   NaN

&gt;&gt;&gt; s.loc[:3] # slice up to and including label 3
49   NaN
48   NaN
47   NaN
46   NaN
45   NaN
1    NaN
2    NaN
3    NaN

&gt;&gt;&gt; s.ix[:3] # the integer is in the index so s.ix[:3] works like loc
49   NaN
48   NaN
47   NaN
46   NaN
45   NaN
1    NaN
2    NaN
3    NaN
</code></pre>

<p>Notice <code>s.ix[:3]</code> returns the same Series as <code>s.loc[:3]</code> since it looks for the label first rather than going by position (and the index is of integer type).</p>

<p>What if we try with an integer label that isn't in the index (say <code>6</code>)?</p>

<p>Here <code>s.iloc[:6]</code> returns the first 6 rows of the Series as expected. However, <code>s.loc[:6]</code> raises a KeyError since <code>6</code> is not in the index. </p>

<pre><code>&gt;&gt;&gt; s.iloc[:6]
49   NaN
48   NaN
47   NaN
46   NaN
45   NaN
1    NaN

&gt;&gt;&gt; s.loc[:6]
KeyError: 6

&gt;&gt;&gt; s.ix[:6]
KeyError: 6
</code></pre>

<p>As per the subtleties noted above, <code>s.ix[:6]</code> now raises a KeyError because it tries to work like <code>loc</code> but can't find a <code>6</code> in the index. Because our index is of integer type it doesn't fall back to behaving like <code>iloc</code>.</p>

<p>If, however, our index was of mixed type, given an integer <code>ix</code> would behave like <code>iloc</code> immediately instead of raising a KeyError:</p>

<pre><code>&gt;&gt;&gt; s2 = pd.Series(np.nan, index=['a','b','c','d','e', 1, 2, 3, 4, 5])
&gt;&gt;&gt; s2.index.is_mixed() # index is mix of types
True
&gt;&gt;&gt; s2.ix[:6] # behaves like iloc given integer
a   NaN
b   NaN
c   NaN
d   NaN
e   NaN
1   NaN
</code></pre>

<p>Keep in mind that <code>ix</code> can still accept non-integers and behave like <code>loc</code>:</p>

<pre><code>&gt;&gt;&gt; s2.ix[:'c'] # behaves like loc given non-integer
a   NaN
b   NaN
c   NaN
</code></pre>

<p>As general advice, if you're only indexing using labels, or only indexing using integer positions, stick with <code>loc</code> or <code>iloc</code> to avoid unexpected results - try not use <code>ix</code>.</p>

<hr>

<h3>Combining position-based and label-based indexing</h3>

<p>Sometimes given a DataFrame, you will want to mix label and positional indexing methods for the rows and columns.</p>

<p>For example, consider the following DataFrame. How best to slice the rows up to and including 'c' <em>and</em> take the first four columns?</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(np.nan, 
                      index=list('abcde'),
                      columns=['x','y','z', 8, 9])
&gt;&gt;&gt; df
    x   y   z   8   9
a NaN NaN NaN NaN NaN
b NaN NaN NaN NaN NaN
c NaN NaN NaN NaN NaN
d NaN NaN NaN NaN NaN
e NaN NaN NaN NaN NaN
</code></pre>

<p>In earlier versions of pandas (before 0.20.0) <code>ix</code> lets you do this quite neatly - we can slice the rows by label and the columns by position (note that for the columns, <code>ix</code> default to position-based slicing since the label <code>4</code> is not a column name):</p>

<pre><code>&gt;&gt;&gt; df.ix[:'c', :4]
    x   y   z   8
a NaN NaN NaN NaN
b NaN NaN NaN NaN
c NaN NaN NaN NaN
</code></pre>

<p>In later versions of pandas, we can achieve this result using <code>iloc</code> and the help of another method:</p>

<pre><code>&gt;&gt;&gt; df.iloc[:df.index.get_loc('c') + 1, :4]
    x   y   z   8
a NaN NaN NaN NaN
b NaN NaN NaN NaN
c NaN NaN NaN NaN
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/version/0.19.1/generated/pandas.Index.get_loc.html"" rel=""noreferrer""><code>get_loc()</code></a> is an index method meaning ""get the position of the label in this index"". Note that since slicing with <code>iloc</code> is exclusive of its endpoint, we must add 1 to this value if we want row 'c' as well.</p>

<p>There are further examples in pandas' documentation <a href=""http://pandas-docs.github.io/pandas-docs-travis/indexing.html#ix-indexer-is-deprecated"" rel=""noreferrer"">here</a>.</p>
";;"['>>> s = pd.Series(np.nan, index=[49,48,47,46,45, 1, 2, 3, 4, 5])\n>>> s\n49   NaN\n48   NaN\n47   NaN\n46   NaN\n45   NaN\n1    NaN\n2    NaN\n3    NaN\n4    NaN\n5    NaN\n', '>>> s.iloc[:3] # slice the first three rows\n49   NaN\n48   NaN\n47   NaN\n\n>>> s.loc[:3] # slice up to and including label 3\n49   NaN\n48   NaN\n47   NaN\n46   NaN\n45   NaN\n1    NaN\n2    NaN\n3    NaN\n\n>>> s.ix[:3] # the integer is in the index so s.ix[:3] works like loc\n49   NaN\n48   NaN\n47   NaN\n46   NaN\n45   NaN\n1    NaN\n2    NaN\n3    NaN\n', '>>> s.iloc[:6]\n49   NaN\n48   NaN\n47   NaN\n46   NaN\n45   NaN\n1    NaN\n\n>>> s.loc[:6]\nKeyError: 6\n\n>>> s.ix[:6]\nKeyError: 6\n', "">>> s2 = pd.Series(np.nan, index=['a','b','c','d','e', 1, 2, 3, 4, 5])\n>>> s2.index.is_mixed() # index is mix of types\nTrue\n>>> s2.ix[:6] # behaves like iloc given integer\na   NaN\nb   NaN\nc   NaN\nd   NaN\ne   NaN\n1   NaN\n"", "">>> s2.ix[:'c'] # behaves like loc given non-integer\na   NaN\nb   NaN\nc   NaN\n"", "">>> df = pd.DataFrame(np.nan, \n                      index=list('abcde'),\n                      columns=['x','y','z', 8, 9])\n>>> df\n    x   y   z   8   9\na NaN NaN NaN NaN NaN\nb NaN NaN NaN NaN NaN\nc NaN NaN NaN NaN NaN\nd NaN NaN NaN NaN NaN\ne NaN NaN NaN NaN NaN\n"", "">>> df.ix[:'c', :4]\n    x   y   z   8\na NaN NaN NaN NaN\nb NaN NaN NaN NaN\nc NaN NaN NaN NaN\n"", "">>> df.iloc[:df.index.get_loc('c') + 1, :4]\n    x   y   z   8\na NaN NaN NaN NaN\nb NaN NaN NaN NaN\nc NaN NaN NaN NaN\n""]";"['ix', 'loc', 'iloc', 'ix', 'ix', 'loc', 'iloc', 'ix', 'loc', 'iloc', 'ix', 'ix', 'ix', 'ix', '>>> s = pd.Series(np.nan, index=[49,48,47,46,45, 1, 2, 3, 4, 5])\n>>> s\n49   NaN\n48   NaN\n47   NaN\n46   NaN\n45   NaN\n1    NaN\n2    NaN\n3    NaN\n4    NaN\n5    NaN\n', 's.iloc[:3]', 's.loc[:3]', '>>> s.iloc[:3] # slice the first three rows\n49   NaN\n48   NaN\n47   NaN\n\n>>> s.loc[:3] # slice up to and including label 3\n49   NaN\n48   NaN\n47   NaN\n46   NaN\n45   NaN\n1    NaN\n2    NaN\n3    NaN\n\n>>> s.ix[:3] # the integer is in the index so s.ix[:3] works like loc\n49   NaN\n48   NaN\n47   NaN\n46   NaN\n45   NaN\n1    NaN\n2    NaN\n3    NaN\n', 's.ix[:3]', 's.loc[:3]', '6', 's.iloc[:6]', 's.loc[:6]', '6', '>>> s.iloc[:6]\n49   NaN\n48   NaN\n47   NaN\n46   NaN\n45   NaN\n1    NaN\n\n>>> s.loc[:6]\nKeyError: 6\n\n>>> s.ix[:6]\nKeyError: 6\n', 's.ix[:6]', 'loc', '6', 'iloc', 'ix', 'iloc', "">>> s2 = pd.Series(np.nan, index=['a','b','c','d','e', 1, 2, 3, 4, 5])\n>>> s2.index.is_mixed() # index is mix of types\nTrue\n>>> s2.ix[:6] # behaves like iloc given integer\na   NaN\nb   NaN\nc   NaN\nd   NaN\ne   NaN\n1   NaN\n"", 'ix', 'loc', "">>> s2.ix[:'c'] # behaves like loc given non-integer\na   NaN\nb   NaN\nc   NaN\n"", 'loc', 'iloc', 'ix', "">>> df = pd.DataFrame(np.nan, \n                      index=list('abcde'),\n                      columns=['x','y','z', 8, 9])\n>>> df\n    x   y   z   8   9\na NaN NaN NaN NaN NaN\nb NaN NaN NaN NaN NaN\nc NaN NaN NaN NaN NaN\nd NaN NaN NaN NaN NaN\ne NaN NaN NaN NaN NaN\n"", 'ix', 'ix', '4', "">>> df.ix[:'c', :4]\n    x   y   z   8\na NaN NaN NaN NaN\nb NaN NaN NaN NaN\nc NaN NaN NaN NaN\n"", 'iloc', "">>> df.iloc[:df.index.get_loc('c') + 1, :4]\n    x   y   z   8\na NaN NaN NaN NaN\nb NaN NaN NaN NaN\nc NaN NaN NaN NaN\n"", 'get_loc()', 'iloc']"
1347;;2;31594055;31593201.0;2;59;;;"<p><code>iloc</code> works based on integer positioning. So no matter what your row labels are, you can always, e.g., get the first row by doing</p>

<pre><code>df.iloc[0]
</code></pre>

<p>or the last five rows by doing</p>

<pre><code>df.iloc[-5:]
</code></pre>

<p>You can also use it on the columns. This retrieves the 3rd column:</p>

<pre><code>df.iloc[:, 2]    # the : in the first position indicates all rows
</code></pre>

<p>You can combine them to get intersections of rows and columns:</p>

<pre><code>df.iloc[:3, :3] # The upper-left 3 X 3 entries (assuming df has 3+ rows and columns)
</code></pre>

<p>On the other hand, <code>.loc</code> use named indices. Let's set up a data frame with strings as row and column labels:</p>

<pre><code>df = pd.DataFrame(index=['a', 'b', 'c'], columns=['time', 'date', 'name'])
</code></pre>

<p>Then we can get the first row by</p>

<pre><code>df.loc['a']     # equivalent to df.iloc[0]
</code></pre>

<p>and the second two rows of the <code>'date'</code> column by </p>

<pre><code>df.loc['b':, 'date']   # equivalent to df.iloc[1:, 1]
</code></pre>

<p>and so on. Now, it's probably worth pointing out that the default row and column indices for a <code>DataFrame</code> are integers from 0 and in this case <code>iloc</code> and <code>loc</code> would work in the same way. This is why your three examples are equivalent. <strong>If you had a non-numeric index such as strings or datetimes,</strong> <code>df.loc[:5]</code> <strong>would raise an error.</strong> </p>

<p>Also, you can do column retrieval just by using the data frame's <code>__getitem__</code>:</p>

<pre><code>df['time']    # equivalent to df.loc[:, 'time']
</code></pre>

<p>Now suppose you want to mix position and named indexing, that is, indexing using names on rows and positions on columns (to clarify, I mean select from our data frame, rather than creating a data frame with strings in the row index and integers in the column index). This is where <code>.ix</code> comes in:</p>

<pre><code>df.ix[:2, 'time']    # the first two rows of the 'time' column
</code></pre>

<p>EDIT:
I think it's also worth mentioning that you can pass boolean vectors to the <code>loc</code> method as well. For example:</p>

<pre><code> b = [True, False, True]
 df.loc[b] 
</code></pre>

<p>Will return the 1st and 3rd rows of <code>df</code>. This is equivalent to <code>df[b]</code> for selection, but it can also be used for assigning via boolean vectors: </p>

<pre><code>df.loc[b, 'name'] = 'Mary', 'John'
</code></pre>
";;"['df.iloc[0]\n', 'df.iloc[-5:]\n', 'df.iloc[:, 2]    # the : in the first position indicates all rows\n', 'df.iloc[:3, :3] # The upper-left 3 X 3 entries (assuming df has 3+ rows and columns)\n', ""df = pd.DataFrame(index=['a', 'b', 'c'], columns=['time', 'date', 'name'])\n"", ""df.loc['a']     # equivalent to df.iloc[0]\n"", ""df.loc['b':, 'date']   # equivalent to df.iloc[1:, 1]\n"", ""df['time']    # equivalent to df.loc[:, 'time']\n"", ""df.ix[:2, 'time']    # the first two rows of the 'time' column\n"", ' b = [True, False, True]\n df.loc[b] \n', ""df.loc[b, 'name'] = 'Mary', 'John'\n""]";"['iloc', 'df.iloc[0]\n', 'df.iloc[-5:]\n', 'df.iloc[:, 2]    # the : in the first position indicates all rows\n', 'df.iloc[:3, :3] # The upper-left 3 X 3 entries (assuming df has 3+ rows and columns)\n', '.loc', ""df = pd.DataFrame(index=['a', 'b', 'c'], columns=['time', 'date', 'name'])\n"", ""df.loc['a']     # equivalent to df.iloc[0]\n"", ""'date'"", ""df.loc['b':, 'date']   # equivalent to df.iloc[1:, 1]\n"", 'DataFrame', 'iloc', 'loc', 'df.loc[:5]', '__getitem__', ""df['time']    # equivalent to df.loc[:, 'time']\n"", '.ix', ""df.ix[:2, 'time']    # the first two rows of the 'time' column\n"", 'loc', ' b = [True, False, True]\n df.loc[b] \n', 'df', 'df[b]', ""df.loc[b, 'name'] = 'Mary', 'John'\n""]"
1348;;3;31611678;31609600.0;2;71;;;"<p>I believe the pylab magic was removed when they transitioned from IPython to a more general Jupyter notebook.</p>

<p>Try:</p>

<pre><code>%matplotlib inline
</code></pre>

<p>Also when you get a message like:</p>

<pre><code>""matplotlib.axes._subplots.AxesSubplot at 0xebf8b70"".
</code></pre>

<p>That's just IPython displaying the object.  You need to specify IPython display it.  Hence the matplotlib inline magic.</p>
";;"['%matplotlib inline\n', '""matplotlib.axes._subplots.AxesSubplot at 0xebf8b70"".\n']";"['%matplotlib inline\n', '""matplotlib.axes._subplots.AxesSubplot at 0xebf8b70"".\n']"
1349;;1;31839240;16597265.0;2;39;;;"<p>And if you want to add a row, you can use a dictionary:</p>

<pre><code>df = pd.DataFrame()
df = df.append({'name': 'Zed', 'age': 9, 'height': 2}, ignore_index=True)
</code></pre>

<p>which gives you:</p>

<pre><code>   age  height name
0    9       2  Zed
</code></pre>
";;"[""df = pd.DataFrame()\ndf = df.append({'name': 'Zed', 'age': 9, 'height': 2}, ignore_index=True)\n"", '   age  height name\n0    9       2  Zed\n']";"[""df = pd.DataFrame()\ndf = df.append({'name': 'Zed', 'age': 9, 'height': 2}, ignore_index=True)\n"", '   age  height name\n0    9       2  Zed\n']"
1350;;0;31859215;28757389.0;2;28;;;"<pre><code>df = pd.DataFrame({'A':['a', 'b', 'c'], 'B':[54, 67, 89]}, index=[100, 200, 300])

df

                        A   B
                100     a   54
                200     b   67
                300     c   89
In [19]:    
df.loc[100]

Out[19]:
A     a
B    54
Name: 100, dtype: object

In [20]:    
df.iloc[0]

Out[20]:
A     a
B    54
Name: 100, dtype: object

In [24]:    
df2 = df.set_index([df.index,'A'])
df2

Out[24]:
        B
    A   
100 a   54
200 b   67
300 c   89

In [25]:    
df2.ix[100, 'a']

Out[25]:    
B    54
Name: (100, a), dtype: int64
</code></pre>
";;"[""df = pd.DataFrame({'A':['a', 'b', 'c'], 'B':[54, 67, 89]}, index=[100, 200, 300])\n\ndf\n\n                        A   B\n                100     a   54\n                200     b   67\n                300     c   89\nIn [19]:    \ndf.loc[100]\n\nOut[19]:\nA     a\nB    54\nName: 100, dtype: object\n\nIn [20]:    \ndf.iloc[0]\n\nOut[20]:\nA     a\nB    54\nName: 100, dtype: object\n\nIn [24]:    \ndf2 = df.set_index([df.index,'A'])\ndf2\n\nOut[24]:\n        B\n    A   \n100 a   54\n200 b   67\n300 c   89\n\nIn [25]:    \ndf2.ix[100, 'a']\n\nOut[25]:    \nB    54\nName: (100, a), dtype: int64\n""]";"[""df = pd.DataFrame({'A':['a', 'b', 'c'], 'B':[54, 67, 89]}, index=[100, 200, 300])\n\ndf\n\n                        A   B\n                100     a   54\n                200     b   67\n                300     c   89\nIn [19]:    \ndf.loc[100]\n\nOut[19]:\nA     a\nB    54\nName: 100, dtype: object\n\nIn [20]:    \ndf.iloc[0]\n\nOut[20]:\nA     a\nB    54\nName: 100, dtype: object\n\nIn [24]:    \ndf2 = df.set_index([df.index,'A'])\ndf2\n\nOut[24]:\n        B\n    A   \n100 a   54\n200 b   67\n300 c   89\n\nIn [25]:    \ndf2.ix[100, 'a']\n\nOut[25]:    \nB    54\nName: (100, a), dtype: int64\n""]"
1351;;10;31939145;24458645.0;2;121;;;"<p>You can easily do this though,</p>

<pre><code>df.apply(LabelEncoder().fit_transform)
</code></pre>

<p>EDIT:</p>

<p>Since this answer is over a year ago, and generated many upvotes (including a bounty), I should probably extend this further.</p>

<p>For inverse_transform and transform, you have to do a little bit of hack.</p>

<pre><code>from collections import defaultdict
d = defaultdict(LabelEncoder)
</code></pre>

<p>With this, you now retain all columns <code>LabelEncoder</code> as dictionary.</p>

<pre><code># Encoding the variable
fit = df.apply(lambda x: d[x.name].fit_transform(x))

# Inverse the encoded
fit.apply(lambda x: d[x.name].inverse_transform(x))

# Using the dictionary to label future data
df.apply(lambda x: d[x.name].transform(x))
</code></pre>
";;['df.apply(LabelEncoder().fit_transform)\n', 'from collections import defaultdict\nd = defaultdict(LabelEncoder)\n', '# Encoding the variable\nfit = df.apply(lambda x: d[x.name].fit_transform(x))\n\n# Inverse the encoded\nfit.apply(lambda x: d[x.name].inverse_transform(x))\n\n# Using the dictionary to label future data\ndf.apply(lambda x: d[x.name].transform(x))\n'];['df.apply(LabelEncoder().fit_transform)\n', 'from collections import defaultdict\nd = defaultdict(LabelEncoder)\n', 'LabelEncoder', '# Encoding the variable\nfit = df.apply(lambda x: d[x.name].fit_transform(x))\n\n# Inverse the encoded\nfit.apply(lambda x: d[x.name].inverse_transform(x))\n\n# Using the dictionary to label future data\ndf.apply(lambda x: d[x.name].transform(x))\n']
1352;;3;32011969;32011359.0;2;55;;;"<p>First, to convert a Categorical column to its numerical codes, you can do this easier with: <code>dataframe['c'].cat.codes</code>.<br>
Further, it is possible to select automatically all columns with a certain dtype in a dataframe using <code>select_dtypes</code>. This way, you can apply above operation on multiple and automatically selected columns.</p>

<p>First making an example dataframe:</p>

<pre><code>In [75]: df = pd.DataFrame({'col1':[1,2,3,4,5], 'col2':list('abcab'),  'col3':list('ababb')})

In [76]: df['col2'] = df['col2'].astype('category')

In [77]: df['col3'] = df['col3'].astype('category')

In [78]: df.dtypes
Out[78]:
col1       int64
col2    category
col3    category
dtype: object
</code></pre>

<p>Then by using <code>select_dtypes</code> to select the columns, and then applying <code>.cat.codes</code> on each of these columns, you can get the following result:</p>

<pre><code>In [80]: cat_columns = df.select_dtypes(['category']).columns

In [81]: cat_columns
Out[81]: Index([u'col2', u'col3'], dtype='object')

In [83]: df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)

In [84]: df
Out[84]:
   col1  col2  col3
0     1     0     0
1     2     1     1
2     3     2     0
3     4     0     1
4     5     1     1
</code></pre>
";;"[""In [75]: df = pd.DataFrame({'col1':[1,2,3,4,5], 'col2':list('abcab'),  'col3':list('ababb')})\n\nIn [76]: df['col2'] = df['col2'].astype('category')\n\nIn [77]: df['col3'] = df['col3'].astype('category')\n\nIn [78]: df.dtypes\nOut[78]:\ncol1       int64\ncol2    category\ncol3    category\ndtype: object\n"", ""In [80]: cat_columns = df.select_dtypes(['category']).columns\n\nIn [81]: cat_columns\nOut[81]: Index([u'col2', u'col3'], dtype='object')\n\nIn [83]: df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n\nIn [84]: df\nOut[84]:\n   col1  col2  col3\n0     1     0     0\n1     2     1     1\n2     3     2     0\n3     4     0     1\n4     5     1     1\n""]";"[""dataframe['c'].cat.codes"", 'select_dtypes', ""In [75]: df = pd.DataFrame({'col1':[1,2,3,4,5], 'col2':list('abcab'),  'col3':list('ababb')})\n\nIn [76]: df['col2'] = df['col2'].astype('category')\n\nIn [77]: df['col3'] = df['col3'].astype('category')\n\nIn [78]: df.dtypes\nOut[78]:\ncol1       int64\ncol2    category\ncol3    category\ndtype: object\n"", 'select_dtypes', '.cat.codes', ""In [80]: cat_columns = df.select_dtypes(['category']).columns\n\nIn [81]: cat_columns\nOut[81]: Index([u'col2', u'col3'], dtype='object')\n\nIn [83]: df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n\nIn [84]: df\nOut[84]:\n   col1  col2  col3\n0     1     0     0\n1     2     1     1\n2     3     2     0\n3     4     0     1\n4     5     1     1\n""]"
1353;;0;32066997;16266019.0;2;6;;;"<p>Came across this when I was searching for this type of groupby. Wes' code above didn't work for me, not sure if it's because changes in <code>pandas</code> over time. </p>

<p>In <code>pandas 0.16.2</code>, what I did in the end was:</p>

<pre><code>grp = data.groupby(by=[data.datetime_col.map(lambda x : (x.hour, x.minute))])
grp.count()
</code></pre>

<p>You'd have (hour, minute) tuples as the grouped index. If you want multi-index:</p>

<pre><code>grp = data.groupby(by=[data.datetime_col.map(lambda x : x.hour),
                       data.datetime_col.map(lambda x : x.minute)])
</code></pre>
";;['grp = data.groupby(by=[data.datetime_col.map(lambda x : (x.hour, x.minute))])\ngrp.count()\n', 'grp = data.groupby(by=[data.datetime_col.map(lambda x : x.hour),\n                       data.datetime_col.map(lambda x : x.minute)])\n'];['pandas', 'pandas 0.16.2', 'grp = data.groupby(by=[data.datetime_col.map(lambda x : (x.hour, x.minute))])\ngrp.count()\n', 'grp = data.groupby(by=[data.datetime_col.map(lambda x : x.hour),\n                       data.datetime_col.map(lambda x : x.minute)])\n']
1354;;1;32103253;25254016.0;2;9;;;"<p>Note that the answer from @unutbu will be correct until you want to set the value to something new, then it will not work if your dataframe is a view.</p>

<pre><code>In [4]: df = pd.DataFrame({'foo':list('ABC')}, index=[0,2,1])
In [5]: df['bar'] = 100
In [6]: df['bar'].iloc[0] = 99
/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas-0.16.0_19_g8d2818e-py2.7-macosx-10.9-x86_64.egg/pandas/core/indexing.py:118: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  self._setitem_with_indexer(indexer, value)
</code></pre>

<p>Another approach that will consistently work with both setting and getting is:</p>

<pre><code>In [7]: df.loc[df.index[0], 'foo']
Out[7]: 'A'
In [8]: df.loc[df.index[0], 'bar'] = 99
In [9]: df
Out[9]:
  foo  bar
0   A   99
2   B  100
1   C  100
</code></pre>
";;"[""In [4]: df = pd.DataFrame({'foo':list('ABC')}, index=[0,2,1])\nIn [5]: df['bar'] = 100\nIn [6]: df['bar'].iloc[0] = 99\n/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas-0.16.0_19_g8d2818e-py2.7-macosx-10.9-x86_64.egg/pandas/core/indexing.py:118: SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._setitem_with_indexer(indexer, value)\n"", ""In [7]: df.loc[df.index[0], 'foo']\nOut[7]: 'A'\nIn [8]: df.loc[df.index[0], 'bar'] = 99\nIn [9]: df\nOut[9]:\n  foo  bar\n0   A   99\n2   B  100\n1   C  100\n""]";"[""In [4]: df = pd.DataFrame({'foo':list('ABC')}, index=[0,2,1])\nIn [5]: df['bar'] = 100\nIn [6]: df['bar'].iloc[0] = 99\n/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas-0.16.0_19_g8d2818e-py2.7-macosx-10.9-x86_64.egg/pandas/core/indexing.py:118: SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._setitem_with_indexer(indexer, value)\n"", ""In [7]: df.loc[df.index[0], 'foo']\nOut[7]: 'A'\nIn [8]: df.loc[df.index[0], 'bar'] = 99\nIn [9]: df\nOut[9]:\n  foo  bar\n0   A   99\n2   B  100\n1   C  100\n""]"
1355;;0;32103678;15943769.0;2;11;;;"<p>Apart from above answers use can use <code>df.axes</code> to get the tuple with row and column indexes and then use <code>len()</code> function:</p>

<pre><code>total_rows=len(df.axes[0])
total_cols=len(df.axes[1])
</code></pre>
";;['total_rows=len(df.axes[0])\ntotal_cols=len(df.axes[1])\n'];['df.axes', 'len()', 'total_rows=len(df.axes[0])\ntotal_cols=len(df.axes[1])\n']
1356;;0;32131398;13148429.0;2;16;;;"<p>You need to create a new list of your columns in the desired order, then use <code>df = df[cols]</code> to rearrange the columns in this new order.</p>

<pre><code>cols = ['mean']  + [col for col in df if col != 'mean']
df = df[cols]
</code></pre>

<p>You can also use a more general approach.  In this example, the last column (indicated by -1) is inserted as the first column.</p>

<pre><code>cols = [df.columns[-1]] + [col for col in df if col != df.columns[-1]]
df = df[cols]
</code></pre>

<p>You can also use this approach for reordering columns in a desired order if they are present in the DataFrame.</p>

<pre><code>inserted_cols = ['a', 'b', 'c']
cols = ([col for col in inserted_cols if col in df] 
        + [col for col in df if col not in inserted cols])
df = df[cols]
</code></pre>
";;"[""cols = ['mean']  + [col for col in df if col != 'mean']\ndf = df[cols]\n"", 'cols = [df.columns[-1]] + [col for col in df if col != df.columns[-1]]\ndf = df[cols]\n', ""inserted_cols = ['a', 'b', 'c']\ncols = ([col for col in inserted_cols if col in df] \n        + [col for col in df if col not in inserted cols])\ndf = df[cols]\n""]";"['df = df[cols]', ""cols = ['mean']  + [col for col in df if col != 'mean']\ndf = df[cols]\n"", 'cols = [df.columns[-1]] + [col for col in df if col != df.columns[-1]]\ndf = df[cols]\n', ""inserted_cols = ['a', 'b', 'c']\ncols = ([col for col in inserted_cols if col in df] \n        + [col for col in df if col not in inserted cols])\ndf = df[cols]\n""]"
1357;;1;32152755;13003051.0;2;8;;;"<p>Note, a modification to @Chang She's response, as of pandas 0.16, the ""-"" operator is scheduled for deprecation.  The <code>difference()</code> method is encouraged in its place.</p>

<pre><code>exclude = ['bad col1', 'bad col2']
df.ix[:, df.columns.difference(exclude)].hist() 
</code></pre>
";;"[""exclude = ['bad col1', 'bad col2']\ndf.ix[:, df.columns.difference(exclude)].hist() \n""]";"['difference()', ""exclude = ['bad col1', 'bad col2']\ndf.ix[:, df.columns.difference(exclude)].hist() \n""]"
1358;;0;32244161;32244019.0;2;52;;;"<p>Pass param <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html#pandas.DataFrame.plot""><code>rot=0</code></a> to rotate the xticks:</p>

<pre><code>import matplotlib
matplotlib.style.use('ggplot')
import matplotlib.pyplot as plt
import pandas as pd

df = pd.DataFrame({ 'celltype':[""foo"",""bar"",""qux"",""woz""], 's1':[5,9,1,7], 's2':[12,90,13,87]})
df = df[[""celltype"",""s1"",""s2""]]
df.set_index([""celltype""],inplace=True)
df.plot(kind='bar',alpha=0.75, rot=0)
plt.xlabel("""")
plt.show()
</code></pre>

<p>yields plot:</p>

<p><a href=""https://i.stack.imgur.com/JPdxv.png""><img src=""https://i.stack.imgur.com/JPdxv.png"" alt=""enter image description here""></a></p>
";;"['import matplotlib\nmatplotlib.style.use(\'ggplot\')\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.DataFrame({ \'celltype\':[""foo"",""bar"",""qux"",""woz""], \'s1\':[5,9,1,7], \'s2\':[12,90,13,87]})\ndf = df[[""celltype"",""s1"",""s2""]]\ndf.set_index([""celltype""],inplace=True)\ndf.plot(kind=\'bar\',alpha=0.75, rot=0)\nplt.xlabel("""")\nplt.show()\n']";"['rot=0', 'import matplotlib\nmatplotlib.style.use(\'ggplot\')\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.DataFrame({ \'celltype\':[""foo"",""bar"",""qux"",""woz""], \'s1\':[5,9,1,7], \'s2\':[12,90,13,87]})\ndf = df[[""celltype"",""s1"",""s2""]]\ndf.set_index([""celltype""],inplace=True)\ndf.plot(kind=\'bar\',alpha=0.75, rot=0)\nplt.xlabel("""")\nplt.show()\n']"
1359;;0;32245025;32244753.0;2;9;;;"<p>You should just be able to use the <code>savefig</code> method of <code>sns_plot</code> directly.</p>

<pre><code>sns_plot.savefig(""output.png"")
</code></pre>

<p>For clarity with your code if you did want to access the matplotlib figure that <code>sns_plot</code> resides in then you can get it directly with</p>

<pre><code>fig = sns_plot.fig
</code></pre>

<p>In this case there is no <code>get_figure</code> method as your code assumes.</p>
";;"['sns_plot.savefig(""output.png"")\n', 'fig = sns_plot.fig\n']";"['savefig', 'sns_plot', 'sns_plot.savefig(""output.png"")\n', 'sns_plot', 'fig = sns_plot.fig\n', 'get_figure']"
1360;;2;32245026;32244753.0;2;34;;;"

<p>Remove the <code>get_figure</code> and just use <code>sns_plot.savefig('output.png')</code></p>

<pre class=""lang-py prettyprint-override""><code>df = sns.load_dataset('iris')
sns_plot = sns.pairplot(df, hue='species', size=2.5)
sns_plot.savefig(""output.png"")
</code></pre>
";;"['df = sns.load_dataset(\'iris\')\nsns_plot = sns.pairplot(df, hue=\'species\', size=2.5)\nsns_plot.savefig(""output.png"")\n']";"['get_figure', ""sns_plot.savefig('output.png')"", 'df = sns.load_dataset(\'iris\')\nsns_plot = sns.pairplot(df, hue=\'species\', size=2.5)\nsns_plot.savefig(""output.png"")\n']"
1361;;3;32307259;10373660.0;2;54;;;"<p>I want to little bit change answer by Wes, because version 0.16.2 need set <code>as_index=False</code>. If you don't set it, you get empty dataframe.</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#aggregation"" rel=""noreferrer"">Source</a>:</p>

<blockquote>
  <p>Aggregation functions will not return the groups that you are aggregating over if they are named columns, when <code>as_index=True</code>, the default. The grouped columns will be the indices of the returned object.</p>
  
  <p>Passing <code>as_index=False</code> will return the groups that you are aggregating over, if they are named columns.</p>
  
  <p>Aggregating functions are ones that reduce the dimension of the returned objects, for example: <code>mean</code>, <code>sum</code>, <code>size</code>, <code>count</code>, <code>std</code>, <code>var</code>, <code>sem</code>, <code>describe</code>, <code>first</code>, <code>last</code>, <code>nth</code>, <code>min</code>, <code>max</code>. This is what happens when you do for example <code>DataFrame.sum()</code> and get back a <code>Series</code>.  </p>
  
  <p>nth can act as a reducer or a filter, see <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#groupby-nth"" rel=""noreferrer"">here</a>.</p>
</blockquote>

<pre><code>import pandas as pd

df1 = pd.DataFrame({""Name"":[""Alice"", ""Bob"", ""Mallory"", ""Mallory"", ""Bob"" , ""Mallory""],
                    ""City"":[""Seattle"",""Seattle"",""Portland"",""Seattle"",""Seattle"",""Portland""]})
print df1
#
#       City     Name
#0   Seattle    Alice
#1   Seattle      Bob
#2  Portland  Mallory
#3   Seattle  Mallory
#4   Seattle      Bob
#5  Portland  Mallory
#
g1 = df1.groupby([""Name"", ""City""], as_index=False).count()
print g1
#
#                  City  Name
#Name    City
#Alice   Seattle      1     1
#Bob     Seattle      2     2
#Mallory Portland     2     2
#        Seattle      1     1
#
</code></pre>

<p>EDIT:</p>

<p>In version <code>0.17.1</code> and later you can use <code>subset</code> in <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.count.html"" rel=""noreferrer""><code>count</code></a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.reset_index.html"" rel=""noreferrer""><code>reset_index</code></a> with parameter <code>name</code> in <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.size.html"" rel=""noreferrer""><code>size</code></a>:</p>

<pre><code>print df1.groupby([""Name"", ""City""], as_index=False ).count()
#IndexError: list index out of range

print df1.groupby([""Name"", ""City""]).count()
#Empty DataFrame
#Columns: []
#Index: [(Alice, Seattle), (Bob, Seattle), (Mallory, Portland), (Mallory, Seattle)]

print df1.groupby([""Name"", ""City""])[['Name','City']].count()
#                  Name  City
#Name    City                
#Alice   Seattle      1     1
#Bob     Seattle      2     2
#Mallory Portland     2     2
#        Seattle      1     1

print df1.groupby([""Name"", ""City""]).size().reset_index(name='count')
#      Name      City  count
#0    Alice   Seattle      1
#1      Bob   Seattle      2
#2  Mallory  Portland      2
#3  Mallory   Seattle      1
</code></pre>

<p>Differences between <code>count</code> and <code>size</code> in <a class='doc-link' href=""https://stackoverflow.com/documentation/pandas/1822/grouping-data/6874/aggregating-by-size-and-count#t=201607220906502658034"">SO Documentation</a>.</p>
";;"['import pandas as pd\n\ndf1 = pd.DataFrame({""Name"":[""Alice"", ""Bob"", ""Mallory"", ""Mallory"", ""Bob"" , ""Mallory""],\n                    ""City"":[""Seattle"",""Seattle"",""Portland"",""Seattle"",""Seattle"",""Portland""]})\nprint df1\n#\n#       City     Name\n#0   Seattle    Alice\n#1   Seattle      Bob\n#2  Portland  Mallory\n#3   Seattle  Mallory\n#4   Seattle      Bob\n#5  Portland  Mallory\n#\ng1 = df1.groupby([""Name"", ""City""], as_index=False).count()\nprint g1\n#\n#                  City  Name\n#Name    City\n#Alice   Seattle      1     1\n#Bob     Seattle      2     2\n#Mallory Portland     2     2\n#        Seattle      1     1\n#\n', 'print df1.groupby([""Name"", ""City""], as_index=False ).count()\n#IndexError: list index out of range\n\nprint df1.groupby([""Name"", ""City""]).count()\n#Empty DataFrame\n#Columns: []\n#Index: [(Alice, Seattle), (Bob, Seattle), (Mallory, Portland), (Mallory, Seattle)]\n\nprint df1.groupby([""Name"", ""City""])[[\'Name\',\'City\']].count()\n#                  Name  City\n#Name    City                \n#Alice   Seattle      1     1\n#Bob     Seattle      2     2\n#Mallory Portland     2     2\n#        Seattle      1     1\n\nprint df1.groupby([""Name"", ""City""]).size().reset_index(name=\'count\')\n#      Name      City  count\n#0    Alice   Seattle      1\n#1      Bob   Seattle      2\n#2  Mallory  Portland      2\n#3  Mallory   Seattle      1\n']";"['as_index=False', 'as_index=True', 'as_index=False', 'mean', 'sum', 'size', 'count', 'std', 'var', 'sem', 'describe', 'first', 'last', 'nth', 'min', 'max', 'DataFrame.sum()', 'Series', 'import pandas as pd\n\ndf1 = pd.DataFrame({""Name"":[""Alice"", ""Bob"", ""Mallory"", ""Mallory"", ""Bob"" , ""Mallory""],\n                    ""City"":[""Seattle"",""Seattle"",""Portland"",""Seattle"",""Seattle"",""Portland""]})\nprint df1\n#\n#       City     Name\n#0   Seattle    Alice\n#1   Seattle      Bob\n#2  Portland  Mallory\n#3   Seattle  Mallory\n#4   Seattle      Bob\n#5  Portland  Mallory\n#\ng1 = df1.groupby([""Name"", ""City""], as_index=False).count()\nprint g1\n#\n#                  City  Name\n#Name    City\n#Alice   Seattle      1     1\n#Bob     Seattle      2     2\n#Mallory Portland     2     2\n#        Seattle      1     1\n#\n', '0.17.1', 'subset', 'count', 'reset_index', 'name', 'size', 'print df1.groupby([""Name"", ""City""], as_index=False ).count()\n#IndexError: list index out of range\n\nprint df1.groupby([""Name"", ""City""]).count()\n#Empty DataFrame\n#Columns: []\n#Index: [(Alice, Seattle), (Bob, Seattle), (Mallory, Portland), (Mallory, Seattle)]\n\nprint df1.groupby([""Name"", ""City""])[[\'Name\',\'City\']].count()\n#                  Name  City\n#Name    City                \n#Alice   Seattle      1     1\n#Bob     Seattle      2     2\n#Mallory Portland     2     2\n#        Seattle      1     1\n\nprint df1.groupby([""Name"", ""City""]).size().reset_index(name=\'count\')\n#      Name      City  count\n#0    Alice   Seattle      1\n#1      Bob   Seattle      2\n#2  Mallory  Portland      2\n#3  Mallory   Seattle      1\n', 'count', 'size']"
1362;;2;32322596;11346283.0;2;14;;;"<p>If you've got the dataframe, df.columns dumps everything into a list you can manipulate and then reassign into your dataframe as the names of columns...</p>

<pre><code>columns = df.columns
columns = [row.replace(""$"","""") for row in columns]
df.rename(columns=dict(zip(columns, things)), inplace=True)
df.head() #to validate the output
</code></pre>

<p>Best way? IDK. A way - yes.</p>

<p>A better way of evaluating all the main techniques put forward in the answers to the question is below using cProfile to gage memory &amp; execution time. @kadee, @kaitlyn, &amp; @eumiro had the functions with the fastest execution times - though these functions are so fast we're comparing the rounding of .000 and .001 seconds for all the answers. Moral: my answer above likely isn't the 'Best' way.</p>

<pre><code>import pandas as pd
import cProfile, pstats, re

old_names = ['$a', '$b', '$c', '$d', '$e']
new_names = ['a', 'b', 'c', 'd', 'e']
col_dict = {'$a': 'a', '$b': 'b','$c':'c','$d':'d','$e':'e'}

df = pd.DataFrame({'$a':[1,2], '$b': [10,20],'$c':['bleep','blorp'],'$d':[1,2],'$e':['texa$','']})

df.head()

def eumiro(df,nn):
    df.columns = nn
    #This direct renaming approach is duplicated in methodology in several other answers: 
    return df

def lexual1(df):
    return df.rename(columns=col_dict)

def lexual2(df,col_dict):
    return df.rename(columns=col_dict, inplace=True)

def Panda_Master_Hayden(df):
    return df.rename(columns=lambda x: x[1:], inplace=True)

def paulo1(df):
    return df.rename(columns=lambda x: x.replace('$', ''))

def paulo2(df):
    return df.rename(columns=lambda x: x.replace('$', ''), inplace=True)

def migloo(df,on,nn):
    return df.rename(columns=dict(zip(on, nn)), inplace=True)

def kadee(df):
    return df.columns.str.replace('$','')

def awo(df):
    columns = df.columns
    columns = [row.replace(""$"","""") for row in columns]
    return df.rename(columns=dict(zip(columns, '')), inplace=True)

def kaitlyn(df):
    df.columns = [col.strip('$') for col in df.columns]
    return df

print 'eumiro'
cProfile.run('eumiro(df,new_names)')
print 'lexual1'
cProfile.run('lexual1(df)')
print 'lexual2'
cProfile.run('lexual2(df,col_dict)')
print 'andy hayden'
cProfile.run('Panda_Master_Hayden(df)')
print 'paulo1'
cProfile.run('paulo1(df)')
print 'paulo2'
cProfile.run('paulo2(df)')
print 'migloo'
cProfile.run('migloo(df,old_names,new_names)')
print 'kadee'
cProfile.run('kadee(df)')
print 'awo'
cProfile.run('awo(df)')
print 'kaitlyn'
cProfile.run('kaitlyn(df)')
</code></pre>
";;"['columns = df.columns\ncolumns = [row.replace(""$"","""") for row in columns]\ndf.rename(columns=dict(zip(columns, things)), inplace=True)\ndf.head() #to validate the output\n', 'import pandas as pd\nimport cProfile, pstats, re\n\nold_names = [\'$a\', \'$b\', \'$c\', \'$d\', \'$e\']\nnew_names = [\'a\', \'b\', \'c\', \'d\', \'e\']\ncol_dict = {\'$a\': \'a\', \'$b\': \'b\',\'$c\':\'c\',\'$d\':\'d\',\'$e\':\'e\'}\n\ndf = pd.DataFrame({\'$a\':[1,2], \'$b\': [10,20],\'$c\':[\'bleep\',\'blorp\'],\'$d\':[1,2],\'$e\':[\'texa$\',\'\']})\n\ndf.head()\n\ndef eumiro(df,nn):\n    df.columns = nn\n    #This direct renaming approach is duplicated in methodology in several other answers: \n    return df\n\ndef lexual1(df):\n    return df.rename(columns=col_dict)\n\ndef lexual2(df,col_dict):\n    return df.rename(columns=col_dict, inplace=True)\n\ndef Panda_Master_Hayden(df):\n    return df.rename(columns=lambda x: x[1:], inplace=True)\n\ndef paulo1(df):\n    return df.rename(columns=lambda x: x.replace(\'$\', \'\'))\n\ndef paulo2(df):\n    return df.rename(columns=lambda x: x.replace(\'$\', \'\'), inplace=True)\n\ndef migloo(df,on,nn):\n    return df.rename(columns=dict(zip(on, nn)), inplace=True)\n\ndef kadee(df):\n    return df.columns.str.replace(\'$\',\'\')\n\ndef awo(df):\n    columns = df.columns\n    columns = [row.replace(""$"","""") for row in columns]\n    return df.rename(columns=dict(zip(columns, \'\')), inplace=True)\n\ndef kaitlyn(df):\n    df.columns = [col.strip(\'$\') for col in df.columns]\n    return df\n\nprint \'eumiro\'\ncProfile.run(\'eumiro(df,new_names)\')\nprint \'lexual1\'\ncProfile.run(\'lexual1(df)\')\nprint \'lexual2\'\ncProfile.run(\'lexual2(df,col_dict)\')\nprint \'andy hayden\'\ncProfile.run(\'Panda_Master_Hayden(df)\')\nprint \'paulo1\'\ncProfile.run(\'paulo1(df)\')\nprint \'paulo2\'\ncProfile.run(\'paulo2(df)\')\nprint \'migloo\'\ncProfile.run(\'migloo(df,old_names,new_names)\')\nprint \'kadee\'\ncProfile.run(\'kadee(df)\')\nprint \'awo\'\ncProfile.run(\'awo(df)\')\nprint \'kaitlyn\'\ncProfile.run(\'kaitlyn(df)\')\n']";"['columns = df.columns\ncolumns = [row.replace(""$"","""") for row in columns]\ndf.rename(columns=dict(zip(columns, things)), inplace=True)\ndf.head() #to validate the output\n', 'import pandas as pd\nimport cProfile, pstats, re\n\nold_names = [\'$a\', \'$b\', \'$c\', \'$d\', \'$e\']\nnew_names = [\'a\', \'b\', \'c\', \'d\', \'e\']\ncol_dict = {\'$a\': \'a\', \'$b\': \'b\',\'$c\':\'c\',\'$d\':\'d\',\'$e\':\'e\'}\n\ndf = pd.DataFrame({\'$a\':[1,2], \'$b\': [10,20],\'$c\':[\'bleep\',\'blorp\'],\'$d\':[1,2],\'$e\':[\'texa$\',\'\']})\n\ndf.head()\n\ndef eumiro(df,nn):\n    df.columns = nn\n    #This direct renaming approach is duplicated in methodology in several other answers: \n    return df\n\ndef lexual1(df):\n    return df.rename(columns=col_dict)\n\ndef lexual2(df,col_dict):\n    return df.rename(columns=col_dict, inplace=True)\n\ndef Panda_Master_Hayden(df):\n    return df.rename(columns=lambda x: x[1:], inplace=True)\n\ndef paulo1(df):\n    return df.rename(columns=lambda x: x.replace(\'$\', \'\'))\n\ndef paulo2(df):\n    return df.rename(columns=lambda x: x.replace(\'$\', \'\'), inplace=True)\n\ndef migloo(df,on,nn):\n    return df.rename(columns=dict(zip(on, nn)), inplace=True)\n\ndef kadee(df):\n    return df.columns.str.replace(\'$\',\'\')\n\ndef awo(df):\n    columns = df.columns\n    columns = [row.replace(""$"","""") for row in columns]\n    return df.rename(columns=dict(zip(columns, \'\')), inplace=True)\n\ndef kaitlyn(df):\n    df.columns = [col.strip(\'$\') for col in df.columns]\n    return df\n\nprint \'eumiro\'\ncProfile.run(\'eumiro(df,new_names)\')\nprint \'lexual1\'\ncProfile.run(\'lexual1(df)\')\nprint \'lexual2\'\ncProfile.run(\'lexual2(df,col_dict)\')\nprint \'andy hayden\'\ncProfile.run(\'Panda_Master_Hayden(df)\')\nprint \'paulo1\'\ncProfile.run(\'paulo1(df)\')\nprint \'paulo2\'\ncProfile.run(\'paulo2(df)\')\nprint \'migloo\'\ncProfile.run(\'migloo(df,old_names,new_names)\')\nprint \'kadee\'\ncProfile.run(\'kadee(df)\')\nprint \'awo\'\ncProfile.run(\'awo(df)\')\nprint \'kaitlyn\'\ncProfile.run(\'kaitlyn(df)\')\n']"
1363;;2;32344037;18837262.0;2;42;;;"<p>As explained on another answer using <code>pandas.DataFrame()</code> directly here will not act as you think.</p>

<p>What you can do is use <code>pandas.DataFrame.from_dict</code>with <strong><code>orient='index'</code></strong>: </p>

<pre><code>In[7]: pandas.DataFrame.from_dict({u'2012-06-08': 388,
 u'2012-06-09': 388,
 u'2012-06-10': 388,
 u'2012-06-11': 389,
 u'2012-06-12': 389,
 u'2012-06-13': 389,
 u'2012-06-14': 389,
 u'2012-06-15': 389,
 u'2012-06-16': 389,
 u'2012-06-17': 389,
 u'2012-06-18': 390,
 u'2012-06-19': 390,
 u'2012-06-20': 390,
 u'2012-06-21': 390,
 u'2012-06-22': 390,
 u'2012-06-23': 390,
 u'2012-06-24': 390,
 u'2012-06-25': 391,
 u'2012-06-26': 391,
 u'2012-06-27': 391,
 u'2012-06-28': 391,
 u'2012-06-29': 391,
 u'2012-06-30': 391,
 u'2012-07-01': 391,
 u'2012-07-02': 392,
 u'2012-07-03': 392,
 u'2012-07-04': 392,
 u'2012-07-05': 392,
 u'2012-07-06': 392}, orient='index')
Out[7]: 
              0
2012-06-13  389
2012-06-16  389
2012-06-12  389
2012-07-03  392
2012-07-02  392
2012-06-29  391
2012-06-30  391
2012-07-01  391
2012-06-15  389
2012-06-08  388
2012-06-09  388
2012-07-05  392
2012-07-04  392
2012-06-14  389
2012-07-06  392
2012-06-17  389
2012-06-20  390
2012-06-21  390
2012-06-22  390
2012-06-23  390
2012-06-11  389
2012-06-10  388
2012-06-26  391
2012-06-27  391
2012-06-28  391
2012-06-24  390
2012-06-19  390
2012-06-18  390
2012-06-25  391
</code></pre>
";;"[""In[7]: pandas.DataFrame.from_dict({u'2012-06-08': 388,\n u'2012-06-09': 388,\n u'2012-06-10': 388,\n u'2012-06-11': 389,\n u'2012-06-12': 389,\n u'2012-06-13': 389,\n u'2012-06-14': 389,\n u'2012-06-15': 389,\n u'2012-06-16': 389,\n u'2012-06-17': 389,\n u'2012-06-18': 390,\n u'2012-06-19': 390,\n u'2012-06-20': 390,\n u'2012-06-21': 390,\n u'2012-06-22': 390,\n u'2012-06-23': 390,\n u'2012-06-24': 390,\n u'2012-06-25': 391,\n u'2012-06-26': 391,\n u'2012-06-27': 391,\n u'2012-06-28': 391,\n u'2012-06-29': 391,\n u'2012-06-30': 391,\n u'2012-07-01': 391,\n u'2012-07-02': 392,\n u'2012-07-03': 392,\n u'2012-07-04': 392,\n u'2012-07-05': 392,\n u'2012-07-06': 392}, orient='index')\nOut[7]: \n              0\n2012-06-13  389\n2012-06-16  389\n2012-06-12  389\n2012-07-03  392\n2012-07-02  392\n2012-06-29  391\n2012-06-30  391\n2012-07-01  391\n2012-06-15  389\n2012-06-08  388\n2012-06-09  388\n2012-07-05  392\n2012-07-04  392\n2012-06-14  389\n2012-07-06  392\n2012-06-17  389\n2012-06-20  390\n2012-06-21  390\n2012-06-22  390\n2012-06-23  390\n2012-06-11  389\n2012-06-10  388\n2012-06-26  391\n2012-06-27  391\n2012-06-28  391\n2012-06-24  390\n2012-06-19  390\n2012-06-18  390\n2012-06-25  391\n""]";"['pandas.DataFrame()', 'pandas.DataFrame.from_dict', ""orient='index'"", ""In[7]: pandas.DataFrame.from_dict({u'2012-06-08': 388,\n u'2012-06-09': 388,\n u'2012-06-10': 388,\n u'2012-06-11': 389,\n u'2012-06-12': 389,\n u'2012-06-13': 389,\n u'2012-06-14': 389,\n u'2012-06-15': 389,\n u'2012-06-16': 389,\n u'2012-06-17': 389,\n u'2012-06-18': 390,\n u'2012-06-19': 390,\n u'2012-06-20': 390,\n u'2012-06-21': 390,\n u'2012-06-22': 390,\n u'2012-06-23': 390,\n u'2012-06-24': 390,\n u'2012-06-25': 391,\n u'2012-06-26': 391,\n u'2012-06-27': 391,\n u'2012-06-28': 391,\n u'2012-06-29': 391,\n u'2012-06-30': 391,\n u'2012-07-01': 391,\n u'2012-07-02': 392,\n u'2012-07-03': 392,\n u'2012-07-04': 392,\n u'2012-07-05': 392,\n u'2012-07-06': 392}, orient='index')\nOut[7]: \n              0\n2012-06-13  389\n2012-06-16  389\n2012-06-12  389\n2012-07-03  392\n2012-07-02  392\n2012-06-29  391\n2012-06-30  391\n2012-07-01  391\n2012-06-15  389\n2012-06-08  388\n2012-06-09  388\n2012-07-05  392\n2012-07-04  392\n2012-06-14  389\n2012-07-06  392\n2012-06-17  389\n2012-06-20  390\n2012-06-21  390\n2012-06-22  390\n2012-06-23  390\n2012-06-11  389\n2012-06-10  388\n2012-06-26  391\n2012-06-27  391\n2012-06-28  391\n2012-06-24  390\n2012-06-19  390\n2012-06-18  390\n2012-06-25  391\n""]"
1364;;0;32366268;16266019.0;2;16;;;"<p>Wes' code didn't work for me.  But the DatetimeIndex function (<a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DatetimeIndex.html"" rel=""nofollow noreferrer"">docs</a>) did:</p>

<pre><code>times = pd.DatetimeIndex(data.datetime_col)
grouped = df.groupby([times.hour, times.minute])
</code></pre>

<p>The DatetimeIndex object is a representation of times in pandas.  The first line creates a array of the datetimes.  The second line uses this array to get the hour and minute data for all of the rows, allowing the data to be grouped (<a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html"" rel=""nofollow noreferrer"">docs</a>) by these values.</p>
";;['times = pd.DatetimeIndex(data.datetime_col)\ngrouped = df.groupby([times.hour, times.minute])\n'];['times = pd.DatetimeIndex(data.datetime_col)\ngrouped = df.groupby([times.hour, times.minute])\n']
1365;;0;32397818;20069009.0;2;50;;;"<p><a href=""https://github.com/pydata/pandas/issues/7053"" rel=""noreferrer"">Since 0.14.1</a>, you can now do <code>nlargest</code> and <code>nsmallest</code> on a <code>groupby</code> object:</p>

<pre><code>In [23]: df.groupby('id')['value'].nlargest(2)
Out[23]: 
id   
1   2    3
    1    2
2   6    4
    5    3
3   7    1
4   8    1
dtype: int64
</code></pre>

<p>There's a slight weirdness that you get the original index in there as well, but this might be really useful depending on what your original index <em>was</em>.</p>

<p>If you're not interested in it, you can do <code>.reset_index(level=1, drop=True)</code> to get rid of it altogether.</p>

<p>(Note: <a href=""https://github.com/pydata/pandas/pull/10393"" rel=""noreferrer"">From 0.17.1</a> you'll be able to do this on a DataFrameGroupBy too but for now it only works with <code>Series</code> and <code>SeriesGroupBy</code>.)</p>
";;"[""In [23]: df.groupby('id')['value'].nlargest(2)\nOut[23]: \nid   \n1   2    3\n    1    2\n2   6    4\n    5    3\n3   7    1\n4   8    1\ndtype: int64\n""]";"['nlargest', 'nsmallest', 'groupby', ""In [23]: df.groupby('id')['value'].nlargest(2)\nOut[23]: \nid   \n1   2    3\n    1    2\n2   6    4\n    5    3\n3   7    1\n4   8    1\ndtype: int64\n"", '.reset_index(level=1, drop=True)', 'Series', 'SeriesGroupBy']"
1366;;3;32399908;12725417.0;2;14;;;"<p>To avoid using a private method you can also use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.select_dtypes.html?highlight=dtypes#pandas.DataFrame.select_dtypes"" rel=""nofollow noreferrer"">select_dtypes</a>, where you can either include or exclude the dtypes you want. </p>

<p>Ran into it on <a href=""https://github.com/pydata/pandas/issues/5775"" rel=""nofollow noreferrer"">this post</a> on the exact same thing. </p>

<p>Or in your case, specifically:<br>
<code>source.select_dtypes(['number']) or source.select_dtypes([np.number]</code></p>
";;[];"[""source.select_dtypes(['number']) or source.select_dtypes([np.number]""]"
1367;;4;32400969;32400867.0;2;32;;;"<p>Just as the error suggests , <code>pandas.read_csv</code> needs a file-like object as the first argument.</p>

<p>If you want to read the csv from a string, you can use <a href=""https://docs.python.org/3/library/io.html#io.StringIO"" rel=""nofollow noreferrer""><code>io.StringIO</code></a> (Python 3.x) or <a href=""https://docs.python.org/2/library/stringio.html#StringIO.StringIO"" rel=""nofollow noreferrer""><code>StringIO.StringIO</code> (Python 2.x)</a> . </p>

<p>Also, for the URL - <a href=""https://github.com/cs109/2014_data/blob/master/countries.csv"" rel=""nofollow noreferrer"">https://github.com/cs109/2014_data/blob/master/countries.csv</a> - you are getting back <code>html</code> response , not raw csv, you should use the url given by the <code>Raw</code> link in the github page for getting raw csv response , which is - <a href=""https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv"" rel=""nofollow noreferrer"">https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv</a></p>

<p>Example -</p>

<pre><code>import pandas as pd
import io
import requests
url=""https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv""
s=requests.get(url).content
c=pd.read_csv(io.StringIO(s.decode('utf-8')))
</code></pre>

<h2>Update</h2>

<p>From pandas <code>0.19.2</code> you can now just <a href=""https://stackoverflow.com/a/41880513/2071807"">pass the url directly</a>.</p>
";;"['import pandas as pd\nimport io\nimport requests\nurl=""https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv""\ns=requests.get(url).content\nc=pd.read_csv(io.StringIO(s.decode(\'utf-8\')))\n']";"['pandas.read_csv', 'io.StringIO', 'StringIO.StringIO', 'html', 'Raw', 'import pandas as pd\nimport io\nimport requests\nurl=""https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv""\ns=requests.get(url).content\nc=pd.read_csv(io.StringIO(s.decode(\'utf-8\')))\n', '0.19.2']"
1368;;3;32401251;32400867.0;2;6;;;"<p>As I commented you need to use a StringIO  object and decode i.e <code>c=pd.read_csv(io.StringIO(s.decode(""utf-8"")))</code> if using requests, you need to decode as .content returns <em>bytes</em> if you used .text you would just need to pass s as is <code>s = requests.get(url).text</code> c = <code>pd.read_csv(StringIO(s))</code>. </p>

<p>A simpler approach is to pass the correct url of the <em>raw</em> data directly to <code>read_csv</code>, you <strong>don't</strong> have to pass a file like object, you can pass a url so you don't need requests at all:</p>

<pre><code>c = pd.read_csv(""https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv"")

print(c)
</code></pre>

<p>Output:</p>

<pre><code>                              Country         Region
0                             Algeria         AFRICA
1                              Angola         AFRICA
2                               Benin         AFRICA
3                            Botswana         AFRICA
4                             Burkina         AFRICA
5                             Burundi         AFRICA
6                            Cameroon         AFRICA
..................................
</code></pre>

<p>From the  <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""noreferrer"">docs</a>:</p>

<p><strong>filepath_or_buffer</strong> :</p>

<blockquote>
  <p>string or file handle / StringIO
  The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. For instance, a local file could be file ://localhost/path/to/table.csv</p>
</blockquote>
";;"['c = pd.read_csv(""https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv"")\n\nprint(c)\n', '                              Country         Region\n0                             Algeria         AFRICA\n1                              Angola         AFRICA\n2                               Benin         AFRICA\n3                            Botswana         AFRICA\n4                             Burkina         AFRICA\n5                             Burundi         AFRICA\n6                            Cameroon         AFRICA\n..................................\n']";"['c=pd.read_csv(io.StringIO(s.decode(""utf-8"")))', 's = requests.get(url).text', 'pd.read_csv(StringIO(s))', 'read_csv', 'c = pd.read_csv(""https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv"")\n\nprint(c)\n', '                              Country         Region\n0                             Algeria         AFRICA\n1                              Angola         AFRICA\n2                               Benin         AFRICA\n3                            Botswana         AFRICA\n4                             Burkina         AFRICA\n5                             Burundi         AFRICA\n6                            Cameroon         AFRICA\n..................................\n']"
1369;;0;32469151;32468402.0;2;7;;;"<p>I think this a really good question, in Hive you would use <code>EXPLODE</code>, I think there is a case to be made that Pandas should include this functionality by default. You could explode your list column like this:</p>

<pre><code>import numpy as np

df = pd.DataFrame({'listcol':[[1,2,3],[4,5,6]]})
X = pd.concat([pd.DataFrame(v, index=np.repeat(k,len(v))) 
            for k,v in df.listcol.to_dict().items()])    
</code></pre>

<p>Then you can use <code>pd.merge</code> to join this back to your original dataframe as @helpanderr suggested in the comment to your original question.</p>
";;"[""import numpy as np\n\ndf = pd.DataFrame({'listcol':[[1,2,3],[4,5,6]]})\nX = pd.concat([pd.DataFrame(v, index=np.repeat(k,len(v))) \n            for k,v in df.listcol.to_dict().items()])    \n""]";"['EXPLODE', ""import numpy as np\n\ndf = pd.DataFrame({'listcol':[[1,2,3],[4,5,6]]})\nX = pd.concat([pd.DataFrame(v, index=np.repeat(k,len(v))) \n            for k,v in df.listcol.to_dict().items()])    \n"", 'pd.merge']"
1370;;0;32470490;32468402.0;2;22;;;"<p>In the code below, I first reset the index to make the row iteration easier.  </p>

<p>I create a list of lists where each element of the outer list is a row of the target DataFrame and each element of the inner list is one of the columns.  This nested list will ultimately be concatenated to create the desired DataFrame.</p>

<p>I use a <code>lambda</code> function together with list iteration to create a row for each element of the <code>nearest_neighbors</code> paired with the relevant <code>name</code> and <code>opponent</code>.  </p>

<p>Finally, I create a new DataFrame from this list (using the original column names and setting the index back to <code>name</code> and <code>opponent</code>).</p>

<pre><code>df = (pd.DataFrame({'name': ['A.J. Price'] * 3, 
                    'opponent': ['76ers', 'blazers', 'bobcats'], 
                    'nearest_neighbors': [['Zach LaVine', 'Jeremy Lin', 'Nate Robinson', 'Isaia']] * 3})
      .set_index(['name', 'opponent']))

&gt;&gt;&gt; df
                                                    nearest_neighbors
name       opponent                                                  
A.J. Price 76ers     [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]
           blazers   [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]
           bobcats   [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]

df.reset_index(inplace=True)
rows = []
_ = df.apply(lambda row: [rows.append([row['name'], row['opponent'], nn]) 
                         for nn in row.nearest_neighbors], axis=1)
df_new = pd.DataFrame(rows, columns=df.columns).set_index(['name', 'opponent'])

&gt;&gt;&gt; df_new
                    nearest_neighbors
name       opponent                  
A.J. Price 76ers          Zach LaVine
           76ers           Jeremy Lin
           76ers        Nate Robinson
           76ers                Isaia
           blazers        Zach LaVine
           blazers         Jeremy Lin
           blazers      Nate Robinson
           blazers              Isaia
           bobcats        Zach LaVine
           bobcats         Jeremy Lin
           bobcats      Nate Robinson
           bobcats              Isaia
</code></pre>

<p><strong>EDIT JUNE 2017</strong></p>

<p>An alternative method is as follows:</p>

<pre><code>&gt;&gt;&gt; (pd.melt(df.nearest_neighbors.apply(pd.Series).reset_index(), 
             id_vars=['name', 'opponent'],
             value_name='nearest_neighbors')
     .set_index(['name', 'opponent'])
     .drop('variable', axis=1)
     .dropna()
     .sort_index()
     )
</code></pre>
";;"[""df = (pd.DataFrame({'name': ['A.J. Price'] * 3, \n                    'opponent': ['76ers', 'blazers', 'bobcats'], \n                    'nearest_neighbors': [['Zach LaVine', 'Jeremy Lin', 'Nate Robinson', 'Isaia']] * 3})\n      .set_index(['name', 'opponent']))\n\n>>> df\n                                                    nearest_neighbors\nname       opponent                                                  \nA.J. Price 76ers     [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]\n           blazers   [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]\n           bobcats   [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]\n\ndf.reset_index(inplace=True)\nrows = []\n_ = df.apply(lambda row: [rows.append([row['name'], row['opponent'], nn]) \n                         for nn in row.nearest_neighbors], axis=1)\ndf_new = pd.DataFrame(rows, columns=df.columns).set_index(['name', 'opponent'])\n\n>>> df_new\n                    nearest_neighbors\nname       opponent                  \nA.J. Price 76ers          Zach LaVine\n           76ers           Jeremy Lin\n           76ers        Nate Robinson\n           76ers                Isaia\n           blazers        Zach LaVine\n           blazers         Jeremy Lin\n           blazers      Nate Robinson\n           blazers              Isaia\n           bobcats        Zach LaVine\n           bobcats         Jeremy Lin\n           bobcats      Nate Robinson\n           bobcats              Isaia\n"", "">>> (pd.melt(df.nearest_neighbors.apply(pd.Series).reset_index(), \n             id_vars=['name', 'opponent'],\n             value_name='nearest_neighbors')\n     .set_index(['name', 'opponent'])\n     .drop('variable', axis=1)\n     .dropna()\n     .sort_index()\n     )\n""]";"['lambda', 'nearest_neighbors', 'name', 'opponent', 'name', 'opponent', ""df = (pd.DataFrame({'name': ['A.J. Price'] * 3, \n                    'opponent': ['76ers', 'blazers', 'bobcats'], \n                    'nearest_neighbors': [['Zach LaVine', 'Jeremy Lin', 'Nate Robinson', 'Isaia']] * 3})\n      .set_index(['name', 'opponent']))\n\n>>> df\n                                                    nearest_neighbors\nname       opponent                                                  \nA.J. Price 76ers     [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]\n           blazers   [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]\n           bobcats   [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]\n\ndf.reset_index(inplace=True)\nrows = []\n_ = df.apply(lambda row: [rows.append([row['name'], row['opponent'], nn]) \n                         for nn in row.nearest_neighbors], axis=1)\ndf_new = pd.DataFrame(rows, columns=df.columns).set_index(['name', 'opponent'])\n\n>>> df_new\n                    nearest_neighbors\nname       opponent                  \nA.J. Price 76ers          Zach LaVine\n           76ers           Jeremy Lin\n           76ers        Nate Robinson\n           76ers                Isaia\n           blazers        Zach LaVine\n           blazers         Jeremy Lin\n           blazers      Nate Robinson\n           blazers              Isaia\n           bobcats        Zach LaVine\n           bobcats         Jeremy Lin\n           bobcats      Nate Robinson\n           bobcats              Isaia\n"", "">>> (pd.melt(df.nearest_neighbors.apply(pd.Series).reset_index(), \n             id_vars=['name', 'opponent'],\n             value_name='nearest_neighbors')\n     .set_index(['name', 'opponent'])\n     .drop('variable', axis=1)\n     .dropna()\n     .sort_index()\n     )\n""]"
1371;;3;32489918;8991709.0;2;24;;;"<p>This topic is two years old but seems like a probable place for people to land when they search for comparisons of Pandas and data.table</p>

<p>Since both of these have evolved over time, I want to post a relatively newer comparison (from 2014) here for the interested users: <a href=""https://github.com/Rdatatable/data.table/wiki/Benchmarks-:-Grouping"" rel=""noreferrer"">https://github.com/Rdatatable/data.table/wiki/Benchmarks-:-Grouping</a></p>

<p>It would be interesting to know if Wes and/or Matt (who, by the way, are creators of Pandas and data.table respectively and have both commented above) have any news to add here as well.</p>

<p>-- UPDATE --</p>

<p>A comment posted below by jangorecki contains a link that I think is very useful: <a href=""https://github.com/szilard/benchm-databases"" rel=""noreferrer"">https://github.com/szilard/benchm-databases</a></p>

<p><a href=""https://i.stack.imgur.com/dAnZO.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/dAnZO.png"" alt=""https://github.com/szilard/benchm-databases/blob/master/plot.png""></a></p>

<p>This graph depicts the average times of aggregation and join operations for different technologies (<strong>lower = faster</strong>; comparison last updated in Sept 2016). It was really educational for me.</p>

<p>Going back to the question, <code>R DT key</code> and <code>R DT</code> refer to the keyed/unkeyed flavors of R's data.table and happen to be faster in this benchmark than Python's Pandas (<code>Py pandas</code>). </p>
";;[];['R DT key', 'R DT', 'Py pandas']
1372;;4;32529152;19377969.0;2;57;;;"<pre><code>df = pd.DataFrame({'Year': ['2014', '2015'], 'quarter': ['q1', 'q2']})
df['period'] = df[['Year', 'quarter']].apply(lambda x: ''.join(x), axis=1)
</code></pre>

<p>Yields this dataframe</p>

<pre><code>   Year quarter  period
0  2014      q1  2014q1
1  2015      q2  2015q2
</code></pre>

<p>This method generalizes to an arbitrary number of string columns by replacing <code>df[['Year', 'quarter']]</code> with any column slice of your dataframe, e.g. <code>df.iloc[:,0:2].apply(lambda x: ''.join(x), axis=1)</code>.</p>
";;"[""df = pd.DataFrame({'Year': ['2014', '2015'], 'quarter': ['q1', 'q2']})\ndf['period'] = df[['Year', 'quarter']].apply(lambda x: ''.join(x), axis=1)\n"", '   Year quarter  period\n0  2014      q1  2014q1\n1  2015      q2  2015q2\n']";"[""df = pd.DataFrame({'Year': ['2014', '2015'], 'quarter': ['q1', 'q2']})\ndf['period'] = df[['Year', 'quarter']].apply(lambda x: ''.join(x), axis=1)\n"", '   Year quarter  period\n0  2014      q1  2014q1\n1  2015      q2  2015q2\n', ""df[['Year', 'quarter']]"", ""df.iloc[:,0:2].apply(lambda x: ''.join(x), axis=1)""]"
1373;;0;32536193;20109391.0;2;7;;;"<p><strong>The Challenge</strong> One of the most challenging aspects of responding to SO questions is the time it takes to recreate the problem (including the data).  Questions which don't have a clear way to reproduce the data are less likely to be answered.  Given that you are taking the time to write a question and you have an issue that you'd like help with, you can easily help yourself by providing data that others can then use to help solve your problem.</p>

<p>The instructions provided by @Andy for writing good Pandas questions are an excellent place to start.  For more information, refer to <a href=""https://stackoverflow.com/help/how-to-ask"">how to ask</a> and how to create <a href=""https://stackoverflow.com/help/mcve"">Minimal, Complete, and Verifiable examples</a>.</p>

<p><strong>Please clearly state your question upfront.</strong>  After taking the time to write your question and any sample code, try to read it and provide an 'Executive Summary' for your reader which summarizes the problem and clearly states the question.</p>

<p><em>Original question</em>:</p>

<blockquote>
  <p>I have this data... </p>
  
  <p>I want to do this... </p>
  
  <p>I want my result to look like this... </p>
  
  <p>However, when I try to do [this], I get the following problem...</p>
  
  <p>I've tried to find solutions by doing [this] and [that].</p>
  
  <p>How do I fix it?</p>
</blockquote>

<p>Depending on the amount of data, sample code and error stacks provided, the reader needs to go a long way before understanding what the problem is.  Try restating your question so that the question itself is on top, and then provide the necessary details.</p>

<p><em>Revised Question</em>:</p>

<blockquote>
  <p><strong>Qustion:</strong>  How can I do [this]? </p>
  
  <p>I've tried to find solutions by doing [this] and [that].</p>
  
  <p>When I've tried to do [this], I get the following problem...</p>
  
  <p>I'd like my final results to look like this...</p>
  
  <p>Here is some minimal code that can reproduce my problem...</p>
  
  <p>And here is how to recreate my sample data:
      <code>df = pd.DataFrame({'A': [...], 'B': [...], ...})</code></p>
</blockquote>

<p><strong>PROVIDE SAMPLE DATA IF NEEDED!!!</strong></p>

<p>Sometimes just the head or tail of the DataFrame is all that is needed.  You can also use the methods proposed by @JohnE to create larger datasets that can be reproduced by others.  Using his example to generate a 100 row DataFrame of stock prices:</p>

<pre><code>stocks = pd.DataFrame({ 
    'ticker':np.repeat( ['aapl','goog','yhoo','msft'], 25 ),
    'date':np.tile( pd.date_range('1/1/2011', periods=25, freq='D'), 4 ),
    'price':(np.random.randn(100).cumsum() + 10) })
</code></pre>

<p>If this was your actual data, you may just want to include the head and/or tail of the dataframe as follows (be sure to anonymize any sensitive data):</p>

<pre><code>&gt;&gt;&gt; stocks.head(5).to_dict()
{'date': {0: Timestamp('2011-01-01 00:00:00'),
  1: Timestamp('2011-01-01 00:00:00'),
  2: Timestamp('2011-01-01 00:00:00'),
  3: Timestamp('2011-01-01 00:00:00'),
  4: Timestamp('2011-01-02 00:00:00')},
 'price': {0: 10.284260107718254,
  1: 11.930300761831457,
  2: 10.93741046217319,
  3: 10.884574289565609,
  4: 11.78005850418319},
 'ticker': {0: 'aapl', 1: 'aapl', 2: 'aapl', 3: 'aapl', 4: 'aapl'}}

&gt;&gt;&gt; pd.concat([stocks.head(), stocks.tail()], ignore_index=True).to_dict()
{'date': {0: Timestamp('2011-01-01 00:00:00'),
  1: Timestamp('2011-01-01 00:00:00'),
  2: Timestamp('2011-01-01 00:00:00'),
  3: Timestamp('2011-01-01 00:00:00'),
  4: Timestamp('2011-01-02 00:00:00'),
  5: Timestamp('2011-01-24 00:00:00'),
  6: Timestamp('2011-01-25 00:00:00'),
  7: Timestamp('2011-01-25 00:00:00'),
  8: Timestamp('2011-01-25 00:00:00'),
  9: Timestamp('2011-01-25 00:00:00')},
 'price': {0: 10.284260107718254,
  1: 11.930300761831457,
  2: 10.93741046217319,
  3: 10.884574289565609,
  4: 11.78005850418319,
  5: 10.017209045035006,
  6: 10.57090128181566,
  7: 11.442792747870204,
  8: 11.592953372130493,
  9: 12.864146419530938},
 'ticker': {0: 'aapl',
  1: 'aapl',
  2: 'aapl',
  3: 'aapl',
  4: 'aapl',
  5: 'msft',
  6: 'msft',
  7: 'msft',
  8: 'msft',
  9: 'msft'}}
</code></pre>

<p>You may also want to provide a description of the DataFrame (using only the relevant columns).  This makes it easier for others to check the data types of each column and identify other common errors (e.g. dates as string vs. datetime64 vs. object):</p>

<pre><code>stocks.info()
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 100 entries, 0 to 99
Data columns (total 3 columns):
date      100 non-null datetime64[ns]
price     100 non-null float64
ticker    100 non-null object
dtypes: datetime64[ns](1), float64(1), object(1)
</code></pre>

<p><strong>NOTE:  If your DataFrame has a MultiIndex:</strong></p>

<p>If your DataFrame has a multiindex, you must first reset before calling <code>to_dict</code>.  You then need to recreate the index using <code>set_index</code>:</p>

<pre><code># MultiIndex example.  First create a MultiIndex DataFrame.
df = stocks.set_index(['date', 'ticker'])
&gt;&gt;&gt; df
    price
date       ticker           
2011-01-01 aapl    10.284260
           aapl    11.930301
           aapl    10.937410
           aapl    10.884574
2011-01-02 aapl    11.780059
...

# After resetting the index and passing the DataFrame to `to_dict`, make sure to use 
# `set_index` to restore the original MultiIndex.  This DataFrame can then be restored.

d = df.reset_index().to_dict()
df_new = pd.DataFrame(d).set_index(['date', 'ticker'])
&gt;&gt;&gt; df_new.head()
                       price
date       ticker           
2011-01-01 aapl    10.284260
           aapl    11.930301
           aapl    10.937410
           aapl    10.884574
2011-01-02 aapl    11.780059
</code></pre>
";;"[""stocks = pd.DataFrame({ \n    'ticker':np.repeat( ['aapl','goog','yhoo','msft'], 25 ),\n    'date':np.tile( pd.date_range('1/1/2011', periods=25, freq='D'), 4 ),\n    'price':(np.random.randn(100).cumsum() + 10) })\n"", "">>> stocks.head(5).to_dict()\n{'date': {0: Timestamp('2011-01-01 00:00:00'),\n  1: Timestamp('2011-01-01 00:00:00'),\n  2: Timestamp('2011-01-01 00:00:00'),\n  3: Timestamp('2011-01-01 00:00:00'),\n  4: Timestamp('2011-01-02 00:00:00')},\n 'price': {0: 10.284260107718254,\n  1: 11.930300761831457,\n  2: 10.93741046217319,\n  3: 10.884574289565609,\n  4: 11.78005850418319},\n 'ticker': {0: 'aapl', 1: 'aapl', 2: 'aapl', 3: 'aapl', 4: 'aapl'}}\n\n>>> pd.concat([stocks.head(), stocks.tail()], ignore_index=True).to_dict()\n{'date': {0: Timestamp('2011-01-01 00:00:00'),\n  1: Timestamp('2011-01-01 00:00:00'),\n  2: Timestamp('2011-01-01 00:00:00'),\n  3: Timestamp('2011-01-01 00:00:00'),\n  4: Timestamp('2011-01-02 00:00:00'),\n  5: Timestamp('2011-01-24 00:00:00'),\n  6: Timestamp('2011-01-25 00:00:00'),\n  7: Timestamp('2011-01-25 00:00:00'),\n  8: Timestamp('2011-01-25 00:00:00'),\n  9: Timestamp('2011-01-25 00:00:00')},\n 'price': {0: 10.284260107718254,\n  1: 11.930300761831457,\n  2: 10.93741046217319,\n  3: 10.884574289565609,\n  4: 11.78005850418319,\n  5: 10.017209045035006,\n  6: 10.57090128181566,\n  7: 11.442792747870204,\n  8: 11.592953372130493,\n  9: 12.864146419530938},\n 'ticker': {0: 'aapl',\n  1: 'aapl',\n  2: 'aapl',\n  3: 'aapl',\n  4: 'aapl',\n  5: 'msft',\n  6: 'msft',\n  7: 'msft',\n  8: 'msft',\n  9: 'msft'}}\n"", ""stocks.info()\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 100 entries, 0 to 99\nData columns (total 3 columns):\ndate      100 non-null datetime64[ns]\nprice     100 non-null float64\nticker    100 non-null object\ndtypes: datetime64[ns](1), float64(1), object(1)\n"", ""# MultiIndex example.  First create a MultiIndex DataFrame.\ndf = stocks.set_index(['date', 'ticker'])\n>>> df\n    price\ndate       ticker           \n2011-01-01 aapl    10.284260\n           aapl    11.930301\n           aapl    10.937410\n           aapl    10.884574\n2011-01-02 aapl    11.780059\n...\n\n# After resetting the index and passing the DataFrame to `to_dict`, make sure to use \n# `set_index` to restore the original MultiIndex.  This DataFrame can then be restored.\n\nd = df.reset_index().to_dict()\ndf_new = pd.DataFrame(d).set_index(['date', 'ticker'])\n>>> df_new.head()\n                       price\ndate       ticker           \n2011-01-01 aapl    10.284260\n           aapl    11.930301\n           aapl    10.937410\n           aapl    10.884574\n2011-01-02 aapl    11.780059\n""]";"[""df = pd.DataFrame({'A': [...], 'B': [...], ...})"", ""stocks = pd.DataFrame({ \n    'ticker':np.repeat( ['aapl','goog','yhoo','msft'], 25 ),\n    'date':np.tile( pd.date_range('1/1/2011', periods=25, freq='D'), 4 ),\n    'price':(np.random.randn(100).cumsum() + 10) })\n"", "">>> stocks.head(5).to_dict()\n{'date': {0: Timestamp('2011-01-01 00:00:00'),\n  1: Timestamp('2011-01-01 00:00:00'),\n  2: Timestamp('2011-01-01 00:00:00'),\n  3: Timestamp('2011-01-01 00:00:00'),\n  4: Timestamp('2011-01-02 00:00:00')},\n 'price': {0: 10.284260107718254,\n  1: 11.930300761831457,\n  2: 10.93741046217319,\n  3: 10.884574289565609,\n  4: 11.78005850418319},\n 'ticker': {0: 'aapl', 1: 'aapl', 2: 'aapl', 3: 'aapl', 4: 'aapl'}}\n\n>>> pd.concat([stocks.head(), stocks.tail()], ignore_index=True).to_dict()\n{'date': {0: Timestamp('2011-01-01 00:00:00'),\n  1: Timestamp('2011-01-01 00:00:00'),\n  2: Timestamp('2011-01-01 00:00:00'),\n  3: Timestamp('2011-01-01 00:00:00'),\n  4: Timestamp('2011-01-02 00:00:00'),\n  5: Timestamp('2011-01-24 00:00:00'),\n  6: Timestamp('2011-01-25 00:00:00'),\n  7: Timestamp('2011-01-25 00:00:00'),\n  8: Timestamp('2011-01-25 00:00:00'),\n  9: Timestamp('2011-01-25 00:00:00')},\n 'price': {0: 10.284260107718254,\n  1: 11.930300761831457,\n  2: 10.93741046217319,\n  3: 10.884574289565609,\n  4: 11.78005850418319,\n  5: 10.017209045035006,\n  6: 10.57090128181566,\n  7: 11.442792747870204,\n  8: 11.592953372130493,\n  9: 12.864146419530938},\n 'ticker': {0: 'aapl',\n  1: 'aapl',\n  2: 'aapl',\n  3: 'aapl',\n  4: 'aapl',\n  5: 'msft',\n  6: 'msft',\n  7: 'msft',\n  8: 'msft',\n  9: 'msft'}}\n"", ""stocks.info()\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 100 entries, 0 to 99\nData columns (total 3 columns):\ndate      100 non-null datetime64[ns]\nprice     100 non-null float64\nticker    100 non-null object\ndtypes: datetime64[ns](1), float64(1), object(1)\n"", 'to_dict', 'set_index', ""# MultiIndex example.  First create a MultiIndex DataFrame.\ndf = stocks.set_index(['date', 'ticker'])\n>>> df\n    price\ndate       ticker           \n2011-01-01 aapl    10.284260\n           aapl    11.930301\n           aapl    10.937410\n           aapl    10.884574\n2011-01-02 aapl    11.780059\n...\n\n# After resetting the index and passing the DataFrame to `to_dict`, make sure to use \n# `set_index` to restore the original MultiIndex.  This DataFrame can then be restored.\n\nd = df.reset_index().to_dict()\ndf_new = pd.DataFrame(d).set_index(['date', 'ticker'])\n>>> df_new.head()\n                       price\ndate       ticker           \n2011-01-01 aapl    10.284260\n           aapl    11.930301\n           aapl    10.937410\n           aapl    10.884574\n2011-01-02 aapl    11.780059\n""]"
1374;;3;32558621;28218698.0;2;87;;;"<pre><code>for column in df:
    print(df[column])
</code></pre>
";;['for column in df:\n    print(df[column])\n'];['for column in df:\n    print(df[column])\n']
1375;;2;32606673;15923826.0;2;66;;;"<p>With pandas version <code>0.16.x</code>, there is now a <code>DataFrame.sample</code> <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html"">method built-in</a>:</p>

<pre><code>import pandas

df = pandas.DataFrame(data)

# Randomly sample 70% of your dataframe
df_0.7 = df.sample(frac=0.7)

# Randomly sample 7 elements from your dataframe
df_7 = df.sample(n=7)
</code></pre>

<p>For either approach above, you can get the rest of the rows by doing:</p>

<pre><code>df_rest = df.loc[~df.index.isin(df_0.7.index)]
</code></pre>
";;['import pandas\n\ndf = pandas.DataFrame(data)\n\n# Randomly sample 70% of your dataframe\ndf_0.7 = df.sample(frac=0.7)\n\n# Randomly sample 7 elements from your dataframe\ndf_7 = df.sample(n=7)\n', 'df_rest = df.loc[~df.index.isin(df_0.7.index)]\n'];['0.16.x', 'DataFrame.sample', 'import pandas\n\ndf = pandas.DataFrame(data)\n\n# Randomly sample 70% of your dataframe\ndf_0.7 = df.sample(frac=0.7)\n\n# Randomly sample 7 elements from your dataframe\ndf_7 = df.sample(n=7)\n', 'df_rest = df.loc[~df.index.isin(df_0.7.index)]\n']
1376;;0;32658847;22084338.0;2;9;;;"<p>+1 for using <code>at</code> or <code>iat</code> for scalar operations. Example benchmark:</p>

<pre><code>In [1]: import numpy, pandas
   ...: df = pandas.DataFrame(numpy.zeros(shape=[10, 10]))
   ...: dictionary = df.to_dict()

In [2]: %timeit value = dictionary[5][5]
The slowest run took 34.06 times longer than the fastest. This could mean that an intermediate result is being cached 
1000000 loops, best of 3: 310 ns per loop

In [4]: %timeit value = df.loc[5, 5]
10000 loops, best of 3: 104 s per loop

In [5]: %timeit value = df.iloc[5, 5]
10000 loops, best of 3: 98.8 s per loop

In [6]: %timeit value = df.iat[5, 5]
The slowest run took 6.67 times longer than the fastest. This could mean that an intermediate result is being cached 
100000 loops, best of 3: 9.58 s per loop

In [7]: %timeit value = df.at[5, 5]
The slowest run took 6.59 times longer than the fastest. This could mean that an intermediate result is being cached 
100000 loops, best of 3: 9.26 s per loop
</code></pre>

<p>It seems using <code>at</code> (<code>iat</code>) is about 10 times faster than <code>loc</code> (<code>iloc</code>).</p>
";;['In [1]: import numpy, pandas\n   ...: df = pandas.DataFrame(numpy.zeros(shape=[10, 10]))\n   ...: dictionary = df.to_dict()\n\nIn [2]: %timeit value = dictionary[5][5]\nThe slowest run took 34.06 times longer than the fastest. This could mean that an intermediate result is being cached \n1000000 loops, best of 3: 310 ns per loop\n\nIn [4]: %timeit value = df.loc[5, 5]\n10000 loops, best of 3: 104 s per loop\n\nIn [5]: %timeit value = df.iloc[5, 5]\n10000 loops, best of 3: 98.8 s per loop\n\nIn [6]: %timeit value = df.iat[5, 5]\nThe slowest run took 6.67 times longer than the fastest. This could mean that an intermediate result is being cached \n100000 loops, best of 3: 9.58 s per loop\n\nIn [7]: %timeit value = df.at[5, 5]\nThe slowest run took 6.59 times longer than the fastest. This could mean that an intermediate result is being cached \n100000 loops, best of 3: 9.26 s per loop\n'];['at', 'iat', 'In [1]: import numpy, pandas\n   ...: df = pandas.DataFrame(numpy.zeros(shape=[10, 10]))\n   ...: dictionary = df.to_dict()\n\nIn [2]: %timeit value = dictionary[5][5]\nThe slowest run took 34.06 times longer than the fastest. This could mean that an intermediate result is being cached \n1000000 loops, best of 3: 310 ns per loop\n\nIn [4]: %timeit value = df.loc[5, 5]\n10000 loops, best of 3: 104 s per loop\n\nIn [5]: %timeit value = df.iloc[5, 5]\n10000 loops, best of 3: 98.8 s per loop\n\nIn [6]: %timeit value = df.iat[5, 5]\nThe slowest run took 6.67 times longer than the fastest. This could mean that an intermediate result is being cached \n100000 loops, best of 3: 9.58 s per loop\n\nIn [7]: %timeit value = df.at[5, 5]\nThe slowest run took 6.59 times longer than the fastest. This could mean that an intermediate result is being cached \n100000 loops, best of 3: 9.26 s per loop\n', 'at', 'iat', 'loc', 'iloc']
1377;;0;32662331;24644656.0;2;48;;;"<pre><code>print df.to_string(index=False)
</code></pre>
";;['print df.to_string(index=False)\n'];['print df.to_string(index=False)\n']
1378;;4;32680162;16476924.0;2;65;;;"<p>While <code>iterrows()</code> is a good option, sometimes <code>itertuples()</code> can be much faster:</p>

<pre><code>df = pd.DataFrame({'a': randn(1000), 'b': randn(1000),'N': randint(100, 1000, (1000)), 'x': 'x'})

%timeit [row.a * 2 for idx, row in df.iterrows()]
# =&gt; 10 loops, best of 3: 50.3 ms per loop

%timeit [row[1] * 2 for row in df.itertuples()]
# =&gt; 1000 loops, best of 3: 541 s per loop
</code></pre>
";;"[""df = pd.DataFrame({'a': randn(1000), 'b': randn(1000),'N': randint(100, 1000, (1000)), 'x': 'x'})\n\n%timeit [row.a * 2 for idx, row in df.iterrows()]\n# => 10 loops, best of 3: 50.3 ms per loop\n\n%timeit [row[1] * 2 for row in df.itertuples()]\n# => 1000 loops, best of 3: 541 s per loop\n""]";"['iterrows()', 'itertuples()', ""df = pd.DataFrame({'a': randn(1000), 'b': randn(1000),'N': randint(100, 1000, (1000)), 'x': 'x'})\n\n%timeit [row.a * 2 for idx, row in df.iterrows()]\n# => 10 loops, best of 3: 50.3 ms per loop\n\n%timeit [row[1] * 2 for row in df.itertuples()]\n# => 1000 loops, best of 3: 541 s per loop\n""]"
1379;;3;32700453;13269890.0;2;13;;;"<p>This won't win a code golf competition, and borrows from the previous answers - but clearly shows how the key is added, and how the join works. This creates 2 new data frames from lists, then adds the key to do the cartesian product on.</p>

<p>My use case was that I needed a list of all store IDs on for each week in my list. So, I created a list of all the weeks I wanted to have, then a list of all the store IDs I wanted to map them against.</p>

<p>The merge I chose left, but would be semantically the same as inner in this setup. You can see this <a href=""http://pandas.pydata.org/pandas-docs/stable/merging.html#brief-primer-on-merge-methods-relational-algebra"" rel=""nofollow noreferrer"">in the documentation on merging</a>, which states it does a Cartesian product if key combination appears more than once in both tables - which is what we set up.</p>

<pre><code>days = pd.DataFrame({'date':list_of_days})
stores = pd.DataFrame({'store_id':list_of_stores})
stores['key'] = 0
days['key'] = 0
days_and_stores = days.merge(stores, how='left', on = 'key')
days_and_stores.drop('key',1, inplace=True)
</code></pre>
";;"[""days = pd.DataFrame({'date':list_of_days})\nstores = pd.DataFrame({'store_id':list_of_stores})\nstores['key'] = 0\ndays['key'] = 0\ndays_and_stores = days.merge(stores, how='left', on = 'key')\ndays_and_stores.drop('key',1, inplace=True)\n""]";"[""days = pd.DataFrame({'date':list_of_days})\nstores = pd.DataFrame({'store_id':list_of_stores})\nstores['key'] = 0\ndays['key'] = 0\ndays_and_stores = days.merge(stores, how='left', on = 'key')\ndays_and_stores.drop('key',1, inplace=True)\n""]"
1380;;0;32748510;17874063.0;2;8;;;"<p>Pandas plotting can accept any extra keyword arguments from the respective matplotlib function. So for completeness from the comments of others here, this is how one would do it:</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randn(100,2), columns=list('AB'))

df.hist(normed=1)
</code></pre>

<p>Also, for direct comparison this may be a good way as well:</p>

<pre><code>df.plot(kind='hist', normed=1, bins=20, stacked=False, alpha=.5)
</code></pre>
";;"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randn(100,2), columns=list('AB'))\n\ndf.hist(normed=1)\n"", ""df.plot(kind='hist', normed=1, bins=20, stacked=False, alpha=.5)\n""]";"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randn(100,2), columns=list('AB'))\n\ndf.hist(normed=1)\n"", ""df.plot(kind='hist', normed=1, bins=20, stacked=False, alpha=.5)\n""]"
1381;;2;32783825;11391969.0;2;27;;;"<p>I'm using pandas 0.16.2. This has better performance on my large dataset:</p>

<pre><code>data.groupby(data.date.dt.year)
</code></pre>

<p>Using the <code>dt</code> option and playing around with <code>weekofyear</code>, <code>dayofweek</code> etc. becomes far easier.</p>
";;['data.groupby(data.date.dt.year)\n'];['data.groupby(data.date.dt.year)\n', 'dt', 'weekofyear', 'dayofweek']
1382;;2;32801170;19384532.0;2;215;;;"<h2>tl;dr</h2>

<p>If you just want to count the number of rows per group, do:</p>

<pre><code>df.groupby(key_columns).size()
</code></pre>

<p>where <code>key_columns</code> is the column or list of columns you are grouping by. For example <code>key_columns = ['col1','col2']</code></p>

<p><br></p>

<h3>A simple example:</h3>

<pre><code>In [1]: import pandas as pd

In [2]: df = pd.DataFrame([['a', 1],
                           ['b', 2],
                           ['c', 3],
                           ['a', 4],
                           ['b', 5]], 
                          columns=['col1', 'col2'])

In [3]: counts = df.groupby('col1').size(); counts
Out[3]: 
col1
a    2
b    2
c    1
dtype: int64
</code></pre>

<p>Note that <code>counts</code> is a pandas Series:</p>

<pre><code>In [4]: type(counts)
Out[4]: pandas.core.series.Series
</code></pre>

<p>If you want the results as a pandas Dataframe, do the following:</p>

<pre><code>In [5]: counts_df = pd.DataFrame(df.groupby('col1').size().rename('counts'))

In [6]: counts_df
Out[6]: 
      counts
col1        
a          2
b          2
c          1

In [7]: type(counts_df)
Out[7]: pandas.core.frame.DataFrame
</code></pre>

<p><br></p>

<hr>

<p>In what follows I will elaborate some more.</p>

<h3>Setup some test data</h3>

<pre><code>In[1]:
import numpy as np
import pandas as pd 

keys = np.array([
        ['A', 'B'],
        ['A', 'B'],
        ['A', 'B'],
        ['A', 'B'],
        ['C', 'D'],
        ['C', 'D'],
        ['C', 'D'],
        ['E', 'F'],
        ['E', 'F'],
        ['G', 'H'] 
        ])

df = pd.DataFrame(np.hstack([keys,np.random.randn(10,4).round(2)]), 
                  columns = ['col1', 'col2', 'col3', 'col4', 'col5', 'col6'])

df[['col3', 'col4', 'col5', 'col6']] = \
    df[['col3', 'col4', 'col5', 'col6']].astype(float)
</code></pre>

<p><br>
Below we show the data types and data for the test dataframe:</p>

<pre><code>In [2]: df.dtypes
Out[2]:
col1     object
col2     object
col3    float64
col4    float64
col5    float64
col6    float64
dtype: object

In [3]: df
Out[3]:
  col1 col2  col3  col4  col5  col6
0    A    B  1.50 -1.70 -0.46 -0.30
1    A    B  0.04 -0.22 -0.91  2.43
2    A    B  0.25 -1.00 -0.78  0.46
3    A    B  2.66 -1.56 -0.30 -0.44
4    C    D -1.05  1.04 -0.31 -0.88
5    C    D -0.19 -1.08  0.31 -0.91
6    C    D -1.34 -1.83 -2.06 -2.09
7    E    F  1.83  1.56  0.86 -0.70
8    E    F  0.87 -1.03 -2.59 -1.35
9    G    H -0.13  0.53 -0.40 -1.64
</code></pre>

<p><br> </p>

<p>Now, suppose you want to get the <code>mean</code> and the <code>count</code> for some of the columns. Let's go ahead and run a simple aggreagation (<code>agg</code>) to do this:</p>

<h3>One <code>count</code> per aggregated column</h3>

<pre><code>In [8]: df[['col1', 'col2', 'col3', 'col4']]\
            .groupby(['col1', 'col2']).agg(['mean', 'count'])
Out[8]:
             col3            col4      
             mean count      mean count
col1 col2                              
A    B     1.1125     4 -1.120000     4
C    D    -0.8600     3 -0.623333     3
E    F     1.3500     2  0.265000     2
G    H    -0.1300     1  0.530000     1
</code></pre>

<p>It is kind of annoying that you get one <code>count</code> column for each of the columns aggregated.  If all of your data is valid (i.e., you do not have any <code>NaN</code> cells) then all of the <code>count</code> columns will be redundant.  </p>

<p><br>    </p>

<h3>One <code>count</code> per group</h3>

<p>To end up with a single <code>count</code> column, we can save the <code>groupby</code> results to a variable, and use it separately to calculate the mean, and to get the size of each group. </p>

<p>We then <code>join</code> the <code>means</code> with the <code>counts</code> (renaming the columns along the  way for clarity) to end up with a single dataframe:</p>

<pre><code>In [9]: groupby_object = df[['col1', 'col2', 'col3', 'col4']]\
            .groupby(['col1', 'col2'])

In [10]: groupby_object.agg('mean')\
             .rename(columns = lambda x: x + ' mean')\
             .join(pd.DataFrame(groupby_object.size(), 
                                columns=['counts']))
Out[10]:
           col3 mean  col4 mean  counts
col1 col2                              
A    B        1.1125  -1.120000       4
C    D       -0.8600  -0.623333       3
E    F        1.3500   0.265000       2
G    H       -0.1300   0.530000       1
</code></pre>

<p><br>
<strong>Disclaimer:</strong></p>

<p>If some of the columns that you are aggregating have null values, then you really want to be looking at the group sizes independently for each aggregated column. Otherwise you may be misled as to how many records are actually being used to calculate the mean.</p>
";;"['df.groupby(key_columns).size()\n', ""In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame([['a', 1],\n                           ['b', 2],\n                           ['c', 3],\n                           ['a', 4],\n                           ['b', 5]], \n                          columns=['col1', 'col2'])\n\nIn [3]: counts = df.groupby('col1').size(); counts\nOut[3]: \ncol1\na    2\nb    2\nc    1\ndtype: int64\n"", 'In [4]: type(counts)\nOut[4]: pandas.core.series.Series\n', ""In [5]: counts_df = pd.DataFrame(df.groupby('col1').size().rename('counts'))\n\nIn [6]: counts_df\nOut[6]: \n      counts\ncol1        \na          2\nb          2\nc          1\n\nIn [7]: type(counts_df)\nOut[7]: pandas.core.frame.DataFrame\n"", ""In[1]:\nimport numpy as np\nimport pandas as pd \n\nkeys = np.array([\n        ['A', 'B'],\n        ['A', 'B'],\n        ['A', 'B'],\n        ['A', 'B'],\n        ['C', 'D'],\n        ['C', 'D'],\n        ['C', 'D'],\n        ['E', 'F'],\n        ['E', 'F'],\n        ['G', 'H'] \n        ])\n\ndf = pd.DataFrame(np.hstack([keys,np.random.randn(10,4).round(2)]), \n                  columns = ['col1', 'col2', 'col3', 'col4', 'col5', 'col6'])\n\ndf[['col3', 'col4', 'col5', 'col6']] = \\\n    df[['col3', 'col4', 'col5', 'col6']].astype(float)\n"", 'In [2]: df.dtypes\nOut[2]:\ncol1     object\ncol2     object\ncol3    float64\ncol4    float64\ncol5    float64\ncol6    float64\ndtype: object\n\nIn [3]: df\nOut[3]:\n  col1 col2  col3  col4  col5  col6\n0    A    B  1.50 -1.70 -0.46 -0.30\n1    A    B  0.04 -0.22 -0.91  2.43\n2    A    B  0.25 -1.00 -0.78  0.46\n3    A    B  2.66 -1.56 -0.30 -0.44\n4    C    D -1.05  1.04 -0.31 -0.88\n5    C    D -0.19 -1.08  0.31 -0.91\n6    C    D -1.34 -1.83 -2.06 -2.09\n7    E    F  1.83  1.56  0.86 -0.70\n8    E    F  0.87 -1.03 -2.59 -1.35\n9    G    H -0.13  0.53 -0.40 -1.64\n', ""In [8]: df[['col1', 'col2', 'col3', 'col4']]\\\n            .groupby(['col1', 'col2']).agg(['mean', 'count'])\nOut[8]:\n             col3            col4      \n             mean count      mean count\ncol1 col2                              \nA    B     1.1125     4 -1.120000     4\nC    D    -0.8600     3 -0.623333     3\nE    F     1.3500     2  0.265000     2\nG    H    -0.1300     1  0.530000     1\n"", ""In [9]: groupby_object = df[['col1', 'col2', 'col3', 'col4']]\\\n            .groupby(['col1', 'col2'])\n\nIn [10]: groupby_object.agg('mean')\\\n             .rename(columns = lambda x: x + ' mean')\\\n             .join(pd.DataFrame(groupby_object.size(), \n                                columns=['counts']))\nOut[10]:\n           col3 mean  col4 mean  counts\ncol1 col2                              \nA    B        1.1125  -1.120000       4\nC    D       -0.8600  -0.623333       3\nE    F        1.3500   0.265000       2\nG    H       -0.1300   0.530000       1\n""]";"['df.groupby(key_columns).size()\n', 'key_columns', ""key_columns = ['col1','col2']"", ""In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame([['a', 1],\n                           ['b', 2],\n                           ['c', 3],\n                           ['a', 4],\n                           ['b', 5]], \n                          columns=['col1', 'col2'])\n\nIn [3]: counts = df.groupby('col1').size(); counts\nOut[3]: \ncol1\na    2\nb    2\nc    1\ndtype: int64\n"", 'counts', 'In [4]: type(counts)\nOut[4]: pandas.core.series.Series\n', ""In [5]: counts_df = pd.DataFrame(df.groupby('col1').size().rename('counts'))\n\nIn [6]: counts_df\nOut[6]: \n      counts\ncol1        \na          2\nb          2\nc          1\n\nIn [7]: type(counts_df)\nOut[7]: pandas.core.frame.DataFrame\n"", ""In[1]:\nimport numpy as np\nimport pandas as pd \n\nkeys = np.array([\n        ['A', 'B'],\n        ['A', 'B'],\n        ['A', 'B'],\n        ['A', 'B'],\n        ['C', 'D'],\n        ['C', 'D'],\n        ['C', 'D'],\n        ['E', 'F'],\n        ['E', 'F'],\n        ['G', 'H'] \n        ])\n\ndf = pd.DataFrame(np.hstack([keys,np.random.randn(10,4).round(2)]), \n                  columns = ['col1', 'col2', 'col3', 'col4', 'col5', 'col6'])\n\ndf[['col3', 'col4', 'col5', 'col6']] = \\\n    df[['col3', 'col4', 'col5', 'col6']].astype(float)\n"", 'In [2]: df.dtypes\nOut[2]:\ncol1     object\ncol2     object\ncol3    float64\ncol4    float64\ncol5    float64\ncol6    float64\ndtype: object\n\nIn [3]: df\nOut[3]:\n  col1 col2  col3  col4  col5  col6\n0    A    B  1.50 -1.70 -0.46 -0.30\n1    A    B  0.04 -0.22 -0.91  2.43\n2    A    B  0.25 -1.00 -0.78  0.46\n3    A    B  2.66 -1.56 -0.30 -0.44\n4    C    D -1.05  1.04 -0.31 -0.88\n5    C    D -0.19 -1.08  0.31 -0.91\n6    C    D -1.34 -1.83 -2.06 -2.09\n7    E    F  1.83  1.56  0.86 -0.70\n8    E    F  0.87 -1.03 -2.59 -1.35\n9    G    H -0.13  0.53 -0.40 -1.64\n', 'mean', 'count', 'agg', 'count', ""In [8]: df[['col1', 'col2', 'col3', 'col4']]\\\n            .groupby(['col1', 'col2']).agg(['mean', 'count'])\nOut[8]:\n             col3            col4      \n             mean count      mean count\ncol1 col2                              \nA    B     1.1125     4 -1.120000     4\nC    D    -0.8600     3 -0.623333     3\nE    F     1.3500     2  0.265000     2\nG    H    -0.1300     1  0.530000     1\n"", 'count', 'NaN', 'count', 'count', 'count', 'groupby', 'join', 'means', 'counts', ""In [9]: groupby_object = df[['col1', 'col2', 'col3', 'col4']]\\\n            .groupby(['col1', 'col2'])\n\nIn [10]: groupby_object.agg('mean')\\\n             .rename(columns = lambda x: x + ' mean')\\\n             .join(pd.DataFrame(groupby_object.size(), \n                                columns=['counts']))\nOut[10]:\n           col3 mean  col4 mean  counts\ncol1 col2                              \nA    B        1.1125  -1.120000       4\nC    D       -0.8600  -0.623333       3\nE    F        1.3500   0.265000       2\nG    H       -0.1300   0.530000       1\n""]"
1383;;2;32850652;14745022.0;2;12;;;"<p>If you don't want to create a new dataframe, or if your dataframe has more columns than just the ones you want to split, you could:</p>

<pre><code>df[""flips""], df[""row_name""] = zip(*df[""row""].str.split().tolist())
del df[""row""]  
</code></pre>
";;"['df[""flips""], df[""row_name""] = zip(*df[""row""].str.split().tolist())\ndel df[""row""]  \n']";"['df[""flips""], df[""row_name""] = zip(*df[""row""].str.split().tolist())\ndel df[""row""]  \n']"
1384;;2;32909107;20297317.0;2;17;;;"<p>Drop multiple columns like this:</p>

<pre><code>cols = [1,2,4,5,12]
df.drop(df.columns[cols],axis=1,inplace=True)
</code></pre>
";;['cols = [1,2,4,5,12]\ndf.drop(df.columns[cols],axis=1,inplace=True)\n'];['cols = [1,2,4,5,12]\ndf.drop(df.columns[cols],axis=1,inplace=True)\n']
1385;;0;32944421;18022845.0;2;6;;;"<p><code>df.columns.values</code> also give us the column names</p>
";;[];['df.columns.values']
1386;;5;32961145;14984119.0;2;8;;;"<p>Transposing is inefficient for large DataFrames.  Here is an alternative:</p>

<pre><code>def duplicate_columns(frame):
    groups = frame.columns.to_series().groupby(frame.dtypes).groups
    dups = []
    for t, v in groups.items():
        dcols = frame[v].to_dict(orient=""list"")

        vs = dcols.values()
        ks = dcols.keys()
        lvs = len(vs)

        for i in range(lvs):
            for j in range(i+1,lvs):
                if vs[i] == vs[j]: 
                    dups.append(ks[i])
                    break

    return dups       
</code></pre>

<p>Use it like this:</p>

<pre><code>dups = duplicate_columns(frame)
frame = frame.drop(dups, axis=1)
</code></pre>

<p><strong>Edit</strong></p>

<p>A memory efficient version that treats nans like any other value:</p>

<pre><code>from pandas.core.common import array_equivalent

def duplicate_columns(frame):
    groups = frame.columns.to_series().groupby(frame.dtypes).groups
    dups = []

    for t, v in groups.items():

        cs = frame[v].columns
        vs = frame[v]
        lcs = len(cs)

        for i in range(lcs):
            ia = vs.iloc[:,i].values
            for j in range(i+1, lcs):
                ja = vs.iloc[:,j].values
                if array_equivalent(ia, ja):
                    dups.append(cs[i])
                    break

    return dups
</code></pre>
";;"['def duplicate_columns(frame):\n    groups = frame.columns.to_series().groupby(frame.dtypes).groups\n    dups = []\n    for t, v in groups.items():\n        dcols = frame[v].to_dict(orient=""list"")\n\n        vs = dcols.values()\n        ks = dcols.keys()\n        lvs = len(vs)\n\n        for i in range(lvs):\n            for j in range(i+1,lvs):\n                if vs[i] == vs[j]: \n                    dups.append(ks[i])\n                    break\n\n    return dups       \n', 'dups = duplicate_columns(frame)\nframe = frame.drop(dups, axis=1)\n', 'from pandas.core.common import array_equivalent\n\ndef duplicate_columns(frame):\n    groups = frame.columns.to_series().groupby(frame.dtypes).groups\n    dups = []\n\n    for t, v in groups.items():\n\n        cs = frame[v].columns\n        vs = frame[v]\n        lcs = len(cs)\n\n        for i in range(lcs):\n            ia = vs.iloc[:,i].values\n            for j in range(i+1, lcs):\n                ja = vs.iloc[:,j].values\n                if array_equivalent(ia, ja):\n                    dups.append(cs[i])\n                    break\n\n    return dups\n']";"['def duplicate_columns(frame):\n    groups = frame.columns.to_series().groupby(frame.dtypes).groups\n    dups = []\n    for t, v in groups.items():\n        dcols = frame[v].to_dict(orient=""list"")\n\n        vs = dcols.values()\n        ks = dcols.keys()\n        lvs = len(vs)\n\n        for i in range(lvs):\n            for j in range(i+1,lvs):\n                if vs[i] == vs[j]: \n                    dups.append(ks[i])\n                    break\n\n    return dups       \n', 'dups = duplicate_columns(frame)\nframe = frame.drop(dups, axis=1)\n', 'from pandas.core.common import array_equivalent\n\ndef duplicate_columns(frame):\n    groups = frame.columns.to_series().groupby(frame.dtypes).groups\n    dups = []\n\n    for t, v in groups.items():\n\n        cs = frame[v].columns\n        vs = frame[v]\n        lcs = len(cs)\n\n        for i in range(lcs):\n            ia = vs.iloc[:,i].values\n            for j in range(i+1, lcs):\n                ja = vs.iloc[:,j].values\n                if array_equivalent(ia, ja):\n                    dups.append(cs[i])\n                    break\n\n    return dups\n']"
1387;;3;32970117;18089667.0;2;24;;;"<p><code>df.memory_usage()</code> will return how much each column occupies:</p>

<pre><code>&gt;&gt;&gt; df.memory_usage()

Row_ID            20906600
Household_ID      20906600
Vehicle           20906600
Calendar_Year     20906600
Model_Year        20906600
...
</code></pre>

<p>To include indexes, pass <code>index=True</code>.</p>

<p>So to get overall memory consumption:</p>

<pre><code>&gt;&gt;&gt; df.memory_usage(index=True).sum()
731731000
</code></pre>
";;['>>> df.memory_usage()\n\nRow_ID            20906600\nHousehold_ID      20906600\nVehicle           20906600\nCalendar_Year     20906600\nModel_Year        20906600\n...\n', '>>> df.memory_usage(index=True).sum()\n731731000\n'];['df.memory_usage()', '>>> df.memory_usage()\n\nRow_ID            20906600\nHousehold_ID      20906600\nVehicle           20906600\nCalendar_Year     20906600\nModel_Year        20906600\n...\n', 'index=True', '>>> df.memory_usage(index=True).sum()\n731731000\n']
1388;;3;32993553;18885175.0;2;41;;;"<p>If you want to read a zipped or a tar.gz file into pandas dataframe, the read_csv methods includes this particular implementation.     </p>

<pre><code>df = pd.read_csv(filename.tar.gz, compression='gzip', header=0, sep=',', quotechar='""')
</code></pre>

<p>compression : {gzip, bz2, infer, None}, default infer
For on-the-fly decompression of on-disk data. If infer, then use gzip or bz2 if filepath_or_buffer is a string ending in .gz or .bz2, respectively, and no decompression otherwise. Set to None for no decompression.
<a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html</a></p>
";;"['df = pd.read_csv(filename.tar.gz, compression=\'gzip\', header=0, sep=\',\', quotechar=\'""\')\n']";"['df = pd.read_csv(filename.tar.gz, compression=\'gzip\', header=0, sep=\',\', quotechar=\'""\')\n']"
1389;;5;33020669;20638006.0;2;27;;;"<p>in pandas 16.2, I had to do <code>pd.DataFrame.from_records(d)</code> to get this to work. </p>
";;[];['pd.DataFrame.from_records(d)']
1390;;2;33040290;22391433.0;2;9;;;"<p>Code:</p>

<pre><code>df = pd.DataFrame({'a':list('tuhimerisabhain')})
df.a.value_counts()

&gt;&gt;&gt; df.a.value_counts()
i    3
h    2
a    2
n    1
b    1
m    1
r    1
t    1
e    1
u    1
s    1
</code></pre>
";;"[""df = pd.DataFrame({'a':list('tuhimerisabhain')})\ndf.a.value_counts()\n\n>>> df.a.value_counts()\ni    3\nh    2\na    2\nn    1\nb    1\nm    1\nr    1\nt    1\ne    1\nu    1\ns    1\n""]";"[""df = pd.DataFrame({'a':list('tuhimerisabhain')})\ndf.a.value_counts()\n\n>>> df.a.value_counts()\ni    3\nh    2\na    2\nn    1\nb    1\nm    1\nr    1\nt    1\ne    1\nu    1\ns    1\n""]"
1391;;1;33050438;17098654.0;2;8;;;"<p>Pickle works good!</p>

<pre><code>import pandas as pd
df.to_pickle('123.pkl')    #to save the dataframe, df to 123.pkl
df1 = pd.read_pickle('123.pkl') #to load 123.pkl back to the dataframe df
</code></pre>
";;"[""import pandas as pd\ndf.to_pickle('123.pkl')    #to save the dataframe, df to 123.pkl\ndf1 = pd.read_pickle('123.pkl') #to load 123.pkl back to the dataframe df\n""]";"[""import pandas as pd\ndf.to_pickle('123.pkl')    #to save the dataframe, df to 123.pkl\ndf1 = pd.read_pickle('123.pkl') #to load 123.pkl back to the dataframe df\n""]"
1392;;1;33149986;13842088.0;2;7;;;"<p>Try using <code>df.loc[row_index,col_indexer] = value</code></p>
";;[];['df.loc[row_index,col_indexer] = value']
1393;;2;33161955;24251219.0;2;7;;;"<pre><code>df = pd.read_csv('somefile.csv', low_memory=False)
</code></pre>

<p>This should solve the issue. I got exactly the same error, when reading 1.8M rows from a CSV.</p>
";;"[""df = pd.read_csv('somefile.csv', low_memory=False)\n""]";"[""df = pd.read_csv('somefile.csv', low_memory=False)\n""]"
1394;;0;33178896;18689823.0;2;9;;;"<p>Try:</p>

<pre><code>sub2['income'].fillna((sub2['income'].mean()), inplace=True)
</code></pre>
";;"[""sub2['income'].fillna((sub2['income'].mean()), inplace=True)\n""]";"[""sub2['income'].fillna((sub2['income'].mean()), inplace=True)\n""]"
1395;;0;33250288;12525722.0;2;19;;;"<p>You can use <code>apply</code> for this, and it's a bit neater:</p>

<pre><code>import numpy as np
import pandas as pd

np.random.seed(1)

df = pd.DataFrame(np.random.randn(4,4)* 4 + 3)

          0         1         2         3
0  9.497381  0.552974  0.887313 -1.291874
1  6.461631 -6.206155  9.979247 -0.044828
2  4.276156  2.002518  8.848432 -5.240563
3  1.710331  1.463783  7.535078 -1.399565

df.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))

          0         1         2         3
0  0.515087  0.133967 -0.651699  0.135175
1  0.125241 -0.689446  0.348301  0.375188
2 -0.155414  0.310554  0.223925 -0.624812
3 -0.484913  0.244924  0.079473  0.114448
</code></pre>

<p>Also, it works nicely with <code>groupby</code>, if you select the relevant columns:</p>

<pre><code>df['grp'] = ['A', 'A', 'B', 'B']

          0         1         2         3 grp
0  9.497381  0.552974  0.887313 -1.291874   A
1  6.461631 -6.206155  9.979247 -0.044828   A
2  4.276156  2.002518  8.848432 -5.240563   B
3  1.710331  1.463783  7.535078 -1.399565   B


df.groupby(['grp'])[[0,1,2,3]].apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))

     0    1    2    3
0  0.5  0.5 -0.5 -0.5
1 -0.5 -0.5  0.5  0.5
2  0.5  0.5  0.5 -0.5
3 -0.5 -0.5 -0.5  0.5
</code></pre>
";;"['import numpy as np\nimport pandas as pd\n\nnp.random.seed(1)\n\ndf = pd.DataFrame(np.random.randn(4,4)* 4 + 3)\n\n          0         1         2         3\n0  9.497381  0.552974  0.887313 -1.291874\n1  6.461631 -6.206155  9.979247 -0.044828\n2  4.276156  2.002518  8.848432 -5.240563\n3  1.710331  1.463783  7.535078 -1.399565\n\ndf.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n\n          0         1         2         3\n0  0.515087  0.133967 -0.651699  0.135175\n1  0.125241 -0.689446  0.348301  0.375188\n2 -0.155414  0.310554  0.223925 -0.624812\n3 -0.484913  0.244924  0.079473  0.114448\n', ""df['grp'] = ['A', 'A', 'B', 'B']\n\n          0         1         2         3 grp\n0  9.497381  0.552974  0.887313 -1.291874   A\n1  6.461631 -6.206155  9.979247 -0.044828   A\n2  4.276156  2.002518  8.848432 -5.240563   B\n3  1.710331  1.463783  7.535078 -1.399565   B\n\n\ndf.groupby(['grp'])[[0,1,2,3]].apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n\n     0    1    2    3\n0  0.5  0.5 -0.5 -0.5\n1 -0.5 -0.5  0.5  0.5\n2  0.5  0.5  0.5 -0.5\n3 -0.5 -0.5 -0.5  0.5\n""]";"['apply', 'import numpy as np\nimport pandas as pd\n\nnp.random.seed(1)\n\ndf = pd.DataFrame(np.random.randn(4,4)* 4 + 3)\n\n          0         1         2         3\n0  9.497381  0.552974  0.887313 -1.291874\n1  6.461631 -6.206155  9.979247 -0.044828\n2  4.276156  2.002518  8.848432 -5.240563\n3  1.710331  1.463783  7.535078 -1.399565\n\ndf.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n\n          0         1         2         3\n0  0.515087  0.133967 -0.651699  0.135175\n1  0.125241 -0.689446  0.348301  0.375188\n2 -0.155414  0.310554  0.223925 -0.624812\n3 -0.484913  0.244924  0.079473  0.114448\n', 'groupby', ""df['grp'] = ['A', 'A', 'B', 'B']\n\n          0         1         2         3 grp\n0  9.497381  0.552974  0.887313 -1.291874   A\n1  6.461631 -6.206155  9.979247 -0.044828   A\n2  4.276156  2.002518  8.848432 -5.240563   B\n3  1.710331  1.463783  7.535078 -1.399565   B\n\n\ndf.groupby(['grp'])[[0,1,2,3]].apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n\n     0    1    2    3\n0  0.5  0.5 -0.5 -0.5\n1 -0.5 -0.5  0.5  0.5\n2  0.5  0.5  0.5 -0.5\n3 -0.5 -0.5 -0.5  0.5\n""]"
1396;;1;33374834;17063458.0;2;10;;;"<p>Thought i should add here, that if you want to access rows or columns to loop through them, you do this:</p>

<pre><code>import pandas as pd

# open the file
xlsx = pd.ExcelFile(PATH\FileName.xlsx)

# get the first sheet as an object
sheet1 = xlsx.parse(0)

# get the first column as a list you can loop through
# where the is 0 in the code below change to the row or column number you want    
column = sheet1.icol(0).real

# get the first row as a list you can loop through
row = sheet1.irow(0).real
</code></pre>
";;['import pandas as pd\n\n# open the file\nxlsx = pd.ExcelFile(PATH\\FileName.xlsx)\n\n# get the first sheet as an object\nsheet1 = xlsx.parse(0)\n\n# get the first column as a list you can loop through\n# where the is 0 in the code below change to the row or column number you want    \ncolumn = sheet1.icol(0).real\n\n# get the first row as a list you can loop through\nrow = sheet1.irow(0).real\n'];['import pandas as pd\n\n# open the file\nxlsx = pd.ExcelFile(PATH\\FileName.xlsx)\n\n# get the first sheet as an object\nsheet1 = xlsx.parse(0)\n\n# get the first column as a list you can loop through\n# where the is 0 in the code below change to the row or column number you want    \ncolumn = sheet1.icol(0).real\n\n# get the first row as a list you can loop through\nrow = sheet1.irow(0).real\n']
1397;;1;33375383;11707586.0;2;26;;;"<p>If you want to set options temporarily for display one large df, you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.option_context.html"">option_context</a>:</p>

<pre><code>with pd.option_context('display.max_rows', 999, 'display.max_columns', 5):
    print df
</code></pre>

<p>Option values are restored automatically when you exit the <code>with</code> block. </p>
";;"[""with pd.option_context('display.max_rows', 999, 'display.max_columns', 5):\n    print df\n""]";"[""with pd.option_context('display.max_rows', 999, 'display.max_columns', 5):\n    print df\n"", 'with']"
1398;;0;33555435;11067027.0;2;8;;;"<p>If you need an arbitrary sequence instead of sorted sequence, you could do:</p>

<pre><code>sequence = ['Q1.1','Q1.2','Q1.3',.....'Q6.1',......]
your_dataframe = your_dataframe.reindex(columns=sequence)
</code></pre>

<p>I tested this in 2.7.10 and it worked for me.</p>
";;"[""sequence = ['Q1.1','Q1.2','Q1.3',.....'Q6.1',......]\nyour_dataframe = your_dataframe.reindex(columns=sequence)\n""]";"[""sequence = ['Q1.1','Q1.2','Q1.3',.....'Q6.1',......]\nyour_dataframe = your_dataframe.reindex(columns=sequence)\n""]"
1399;;4;33570065;17098654.0;2;22;;;"<p>Although there are already some answers I found a nice comparison in which they tried several ways to serialize Pandas DataFrames: <a href=""http://matthewrocklin.com/blog/work/2015/03/16/Fast-Serialization/"" rel=""noreferrer"">Efficiently Store Pandas DataFrames</a> [Edit: page has been deleted, but still available on <a href=""https://web.archive.org/web/20151120080623/http://matthewrocklin.com/blog/work/2015/03/16/Fast-Serialization/"" rel=""noreferrer"">web.archive.org</a>]. </p>

<p>They compare:</p>

<ul>
<li>pickle: original ASCII data format</li>
<li>cPickle, a C library</li>
<li>pickle-p2: uses the newer binary format</li>
<li>json: standardlib json library</li>
<li>json-no-index: like json, but without index</li>
<li>msgpack: binary JSON alternative</li>
<li>CSV</li>
<li>hdfstore: HDF5 storage format</li>
</ul>

<p>In their experiment they serialize a DataFrame of 1,000,000 rows with the two columns tested separately: one with text data, the other with numbers. Their disclaimer says:</p>

<blockquote>
  <p>You should not trust that what follows generalizes to your data. You should look at your own data and run benchmarks yourself</p>
</blockquote>

<p>The source code for the test which they refer to is available <a href=""https://gist.github.com/mrocklin/4f6d06a2ccc03731dd5f"" rel=""noreferrer"">online</a>. Since this code did not work directly I made some minor changes, which you can get here: <a href=""https://gist.github.com/agoldhoorn/ee3bec427dec5bfabb2c"" rel=""noreferrer"">serialize.py</a>
 I got the following results:</p>

<p><a href=""https://i.stack.imgur.com/T9JEL.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/T9JEL.png"" alt=""time comparison results""></a></p>

<p>They also mention that with the conversion of text data to <a href=""http://pandas.pydata.org/pandas-docs/version/0.15.2/generated/pandas.core.categorical.Categorical.html"" rel=""noreferrer"">categorical</a> data the the serialization is much faster. In their test about 10 times as fast (also see the test code).</p>

<p><strong>Edit</strong>: The higher times for pickle than csv can be explained by the data format used. By default <a href=""https://docs.python.org/2/library/pickle.html#data-stream-format"" rel=""noreferrer""><code>pickle</code></a> uses a printable ASCII representation, which generates larger data sets. As can be seen from the graph however, pickle using the newer binary data format (version 2, <code>pickle-p2</code>) has much lower load times.</p>

<p>Some other references:</p>

<ul>
<li>In the question <a href=""https://softwarerecs.stackexchange.com/questions/7463/fastest-python-library-to-read-a-csv-file"">Fastest Python library to read a CSV file</a> there is a very detailed <a href=""https://softwarerecs.stackexchange.com/a/7510/18147"">answer</a> which compares different libraries to read csv files with a benchmark. The result is that for reading csv files <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.fromfile.html"" rel=""noreferrer""><code>numpy.fromfile</code></a> is the fastest.</li>
<li>Another <a href=""https://gist.github.com/justinfx/3174062"" rel=""noreferrer"">serialization test</a>
shows <a href=""https://pypi.python.org/pypi/msgpack-python"" rel=""noreferrer"">msgpack-python</a>, <a href=""https://pypi.python.org/pypi/ujson"" rel=""noreferrer"">ujson</a>, and cPickle to be the quickest in serializing.</li>
</ul>
";;[];['pickle', 'pickle-p2', 'numpy.fromfile']
1400;;0;33577649;16852911.0;2;11;;;"<p>Now you can do <code>df['column'].dt.date</code></p>

<p>Note that for datetime objects, if you don't see the hour when they're all 00:00:00, that's not pandas. That's iPython notebook trying to make things look pretty.  </p>
";;[];"[""df['column'].dt.date""]"
1401;;1;33687073;16729483.0;2;9;;;"<pre><code>df['MyColumnName'] = df['MyColumnName'].astype('float64') 
</code></pre>
";;"[""df['MyColumnName'] = df['MyColumnName'].astype('float64') \n""]";"[""df['MyColumnName'] = df['MyColumnName'].astype('float64') \n""]"
1402;;2;33768634;12190874.0;2;12;;;"<p>New in version 0.16.1:</p>

<pre><code>sample_dataframe = your_dataframe.sample(n=how_many_rows_you_want)
</code></pre>

<p>doc here: <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.sample.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.sample.html</a></p>
";;['sample_dataframe = your_dataframe.sample(n=how_many_rows_you_want)\n'];['sample_dataframe = your_dataframe.sample(n=how_many_rows_you_want)\n']
1403;;0;33786696;28901683.0;2;6;;;"<p>As already hinted at, isin requires columns and indices to be the same for a match. If match should only be on row contents, one way to get the mask for filtering the rows present is to convert the rows to a (Multi)Index:</p>

<pre><code>In [77]: df1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]})
In [78]: df2 = pandas.DataFrame(data = {'col1' : [1, 3, 4], 'col2' : [10, 12, 13]})
In [79]: df1.loc[~df1.set_index(list(df1.columns)).index.isin(df2.set_index(list(df2.columns)).index)]
Out[79]:
   col1  col2
1     2    11
4     5    14
</code></pre>

<p>If index should be taken into account, set_index has keyword argument append to append columns to existing index. If columns do not line up, list(df.columns) can be replaced with column specifications to align the data.</p>

<pre><code>pandas.MultiIndex.from_tuples(list(df&lt;N&gt;.to_records(index = False)))
</code></pre>

<p>could alternatively be used to create the indices, though I doubt this is more efficient.</p>
";;"[""In [77]: df1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]})\nIn [78]: df2 = pandas.DataFrame(data = {'col1' : [1, 3, 4], 'col2' : [10, 12, 13]})\nIn [79]: df1.loc[~df1.set_index(list(df1.columns)).index.isin(df2.set_index(list(df2.columns)).index)]\nOut[79]:\n   col1  col2\n1     2    11\n4     5    14\n"", 'pandas.MultiIndex.from_tuples(list(df<N>.to_records(index = False)))\n']";"[""In [77]: df1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]})\nIn [78]: df2 = pandas.DataFrame(data = {'col1' : [1, 3, 4], 'col2' : [10, 12, 13]})\nIn [79]: df1.loc[~df1.set_index(list(df1.columns)).index.isin(df2.set_index(list(df2.columns)).index)]\nOut[79]:\n   col1  col2\n1     2    11\n4     5    14\n"", 'pandas.MultiIndex.from_tuples(list(df<N>.to_records(index = False)))\n']"
1404;;0;33798922;11707586.0;2;10;;;"<p>Set column max width using:</p>

<pre><code>pd.set_option('max_colwidth', 800)
</code></pre>

<p>This particular statement sets max width to 800px, per column.</p>
";;"[""pd.set_option('max_colwidth', 800)\n""]";"[""pd.set_option('max_colwidth', 800)\n""]"
1405;;1;33837592;17141558.0;2;16;;;"<p>As of pandas 0.17.0, <code>DataFrame.sort()</code> is deprecated, and set to be removed in a future version of pandas. The way to sort a dataframe by its values is now is <code>DataFrame.sort_values</code></p>

<p>As such, the answer to your question would now be</p>

<pre><code>df.sort_values(['b', 'c'], ascending=[True, False], inplace=True)
</code></pre>
";;"[""df.sort_values(['b', 'c'], ascending=[True, False], inplace=True)\n""]";"['DataFrame.sort()', 'DataFrame.sort_values', ""df.sort_values(['b', 'c'], ascending=[True, False], inplace=True)\n""]"
1406;;1;33872824;11346283.0;2;9;;;"<p>Another way we could replace the original column labels is by stripping the unwanted characters (here '$') from the original column labels.</p>

<p>This could have been done by running a for loop over df.columns and appending the stripped columns to df.columns.</p>

<p>Instead , we can do this neatly in a single statement by using list comprehension like below:</p>

<pre><code>df.columns = [col.strip('$') for col in df.columns]
</code></pre>

<p>(<code>strip</code> method in Python strips the given character from beginning and end of the string.)</p>
";;"[""df.columns = [col.strip('$') for col in df.columns]\n""]";"[""df.columns = [col.strip('$') for col in df.columns]\n"", 'strip']"
1407;;0;33913961;19377969.0;2;9;;;"<p>Although the @silvado answer is good if you change <code>df.map(str)</code> to <code>df.astype(str)</code> it will be faster:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'Year': ['2014', '2015'], 'quarter': ['q1', 'q2']})

In [131]: %timeit df[""Year""].map(str)
10000 loops, best of 3: 132 us per loop

In [132]: %timeit df[""Year""].astype(str)
10000 loops, best of 3: 82.2 us per loop
</code></pre>
";;"['import pandas as pd\ndf = pd.DataFrame({\'Year\': [\'2014\', \'2015\'], \'quarter\': [\'q1\', \'q2\']})\n\nIn [131]: %timeit df[""Year""].map(str)\n10000 loops, best of 3: 132 us per loop\n\nIn [132]: %timeit df[""Year""].astype(str)\n10000 loops, best of 3: 82.2 us per loop\n']";"['df.map(str)', 'df.astype(str)', 'import pandas as pd\ndf = pd.DataFrame({\'Year\': [\'2014\', \'2015\'], \'quarter\': [\'q1\', \'q2\']})\n\nIn [131]: %timeit df[""Year""].map(str)\n10000 loops, best of 3: 132 us per loop\n\nIn [132]: %timeit df[""Year""].astype(str)\n10000 loops, best of 3: 82.2 us per loop\n']"
1408;;0;33986975;11346283.0;2;9;;;"<p>Real simple just use </p>

<pre><code>df.columns = ['Name1', 'Name2', 'Name3'...]
</code></pre>

<p>and it will assign the column names by the order you put them</p>
";;"[""df.columns = ['Name1', 'Name2', 'Name3'...]\n""]";"[""df.columns = ['Name1', 'Name2', 'Name3'...]\n""]"
1409;;0;33997632;10665889.0;2;39;;;"<p>Lets use the titanic dataset from the seaborn package as an example</p>

<pre><code># Load dataset (pip install seaborn)
&gt;&gt; import seaborn.apionly as sns
&gt;&gt; titanic = sns.load_dataset('titanic')
</code></pre>

<blockquote>
  <h1>using the column names</h1>
</blockquote>

<pre><code>&gt;&gt; titanic.loc[:,['sex','age','fare']]
</code></pre>

<blockquote>
  <h1>using the column indices</h1>
</blockquote>

<pre><code>&gt;&gt; titanic.iloc[:,[2,3,6]]
</code></pre>

<blockquote>
  <h1>using ix</h1>
</blockquote>

<pre><code>&gt;&gt; titanic.ix[:,[sex,age,fare]]
</code></pre>

<p>or</p>

<pre><code>&gt;&gt; titanic.ix[:,[2,3,6]]
</code></pre>

<blockquote>
  <h1>using the reindex method</h1>
</blockquote>

<pre><code>&gt;&gt; titanic.reindex(columns=['sex','age','fare'])
</code></pre>
";;"[""# Load dataset (pip install seaborn)\n>> import seaborn.apionly as sns\n>> titanic = sns.load_dataset('titanic')\n"", "">> titanic.loc[:,['sex','age','fare']]\n"", '>> titanic.iloc[:,[2,3,6]]\n', '>> titanic.ix[:,[\x91sex\x92,\x92age\x92,\x92fare\x92]]\n', '>> titanic.ix[:,[2,3,6]]\n', "">> titanic.reindex(columns=['sex','age','fare'])\n""]";"[""# Load dataset (pip install seaborn)\n>> import seaborn.apionly as sns\n>> titanic = sns.load_dataset('titanic')\n"", "">> titanic.loc[:,['sex','age','fare']]\n"", '>> titanic.iloc[:,[2,3,6]]\n', '>> titanic.ix[:,[\x91sex\x92,\x92age\x92,\x92fare\x92]]\n', '>> titanic.ix[:,[2,3,6]]\n', "">> titanic.reindex(columns=['sex','age','fare'])\n""]"
1410;;0;34013098;34001922.0;2;33;;;"<p>The <code>FailedPreconditionError</code> arises because the program is attempting to read a variable (named <code>""Variable_1""</code>) before it has been initialized. In TensorFlow, all variables must be explicitly initialized, by running their ""initializer"" operations. For convenience, you can run all of the variable initializers in the current session by executing the following statement before your training loop:</p>

<pre><code>tf.initialize_all_variables().run()
</code></pre>

<p>Note that this answer assumes that, as in the question, you are using <code>tf.InteractiveSession</code>, which allows you to run operations without specifying a session. For non-interactive uses, it is more common to use <code>tf.Session</code>, and initialize as follows:</p>

<pre><code>init_op = tf.initialize_all_variables()

sess = tf.Session()
sess.run(init_op)
</code></pre>
";;['tf.initialize_all_variables().run()\n', 'init_op = tf.initialize_all_variables()\n\nsess = tf.Session()\nsess.run(init_op)\n'];"['FailedPreconditionError', '""Variable_1""', 'tf.initialize_all_variables().run()\n', 'tf.InteractiveSession', 'tf.Session', 'init_op = tf.initialize_all_variables()\n\nsess = tf.Session()\nsess.run(init_op)\n']"
1411;;1;34082664;13413590.0;2;15;;;"<p>You could use dataframe method <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.notnull.html"" rel=""noreferrer"">notnull</a> or inverse of <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isnull.html"" rel=""noreferrer"">isnull</a>, or <a href=""http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.isnan.html"" rel=""noreferrer"">numpy.isnan</a>:</p>

<pre><code>In [332]: df[df.EPS.notnull()]
Out[332]:
   STK_ID  RPT_Date  STK_ID.1  EPS  cash
2  600016  20111231    600016  4.3   NaN
4  601939  20111231    601939  2.5   NaN


In [334]: df[~df.EPS.isnull()]
Out[334]:
   STK_ID  RPT_Date  STK_ID.1  EPS  cash
2  600016  20111231    600016  4.3   NaN
4  601939  20111231    601939  2.5   NaN


In [347]: df[~np.isnan(df.EPS)]
Out[347]:
   STK_ID  RPT_Date  STK_ID.1  EPS  cash
2  600016  20111231    600016  4.3   NaN
4  601939  20111231    601939  2.5   NaN
</code></pre>
";;['In [332]: df[df.EPS.notnull()]\nOut[332]:\n   STK_ID  RPT_Date  STK_ID.1  EPS  cash\n2  600016  20111231    600016  4.3   NaN\n4  601939  20111231    601939  2.5   NaN\n\n\nIn [334]: df[~df.EPS.isnull()]\nOut[334]:\n   STK_ID  RPT_Date  STK_ID.1  EPS  cash\n2  600016  20111231    600016  4.3   NaN\n4  601939  20111231    601939  2.5   NaN\n\n\nIn [347]: df[~np.isnan(df.EPS)]\nOut[347]:\n   STK_ID  RPT_Date  STK_ID.1  EPS  cash\n2  600016  20111231    600016  4.3   NaN\n4  601939  20111231    601939  2.5   NaN\n'];['In [332]: df[df.EPS.notnull()]\nOut[332]:\n   STK_ID  RPT_Date  STK_ID.1  EPS  cash\n2  600016  20111231    600016  4.3   NaN\n4  601939  20111231    601939  2.5   NaN\n\n\nIn [334]: df[~df.EPS.isnull()]\nOut[334]:\n   STK_ID  RPT_Date  STK_ID.1  EPS  cash\n2  600016  20111231    600016  4.3   NaN\n4  601939  20111231    601939  2.5   NaN\n\n\nIn [347]: df[~np.isnan(df.EPS)]\nOut[347]:\n   STK_ID  RPT_Date  STK_ID.1  EPS  cash\n2  600016  20111231    600016  4.3   NaN\n4  601939  20111231    601939  2.5   NaN\n']
1412;;2;34092032;34091877.0;2;46;;;"<p>You can use <code>names</code> directly in the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html""><code>read_csv</code></a> </p>

<blockquote>
  <p>names : array-like, default None List of column names to use. If file
  contains no header row, then you should explicitly pass header=None</p>
</blockquote>

<pre><code>Cov = pd.read_csv(""path/to/file.txt"", sep='\t', 
                  names = [""Sequence"", ""Start"", ""End"", ""Coverage""])
</code></pre>

<p>The line below will not work as you expect. <code>Cov</code> is already a dataframe, assuming it really has 4 columns when it's being read from the file.</p>

<pre><code>Frame=pd.DataFrame([Cov], columns = [""Sequence"", ""Start"", ""End"", ""Coverage""])
</code></pre>
";;"['Cov = pd.read_csv(""path/to/file.txt"", sep=\'\\t\', \n                  names = [""Sequence"", ""Start"", ""End"", ""Coverage""])\n', 'Frame=pd.DataFrame([Cov], columns = [""Sequence"", ""Start"", ""End"", ""Coverage""])\n']";"['names', 'read_csv', 'Cov = pd.read_csv(""path/to/file.txt"", sep=\'\\t\', \n                  names = [""Sequence"", ""Start"", ""End"", ""Coverage""])\n', 'Cov', 'Frame=pd.DataFrame([Cov], columns = [""Sequence"", ""Start"", ""End"", ""Coverage""])\n']"
1413;;0;34094058;34091877.0;2;25;;;"<p>Alternatively you could read you csv with <code>header=None</code> and then add it with <code>df.columns</code>:</p>

<pre><code>Cov = pd.read_csv(""path/to/file.txt"", sep='\t', header=None)
Cov.columns = [""Sequence"", ""Start"", ""End"", ""Coverage""]
</code></pre>
";;"['Cov = pd.read_csv(""path/to/file.txt"", sep=\'\\t\', header=None)\nCov.columns = [""Sequence"", ""Start"", ""End"", ""Coverage""]\n']";"['header=None', 'df.columns', 'Cov = pd.read_csv(""path/to/file.txt"", sep=\'\\t\', header=None)\nCov.columns = [""Sequence"", ""Start"", ""End"", ""Coverage""]\n']"
1414;;0;34097939;19482970.0;2;7;;;"<p>It's interesting but <code>df.columns.values.tolist()</code> is almost 3 times faster then <code>df.columns.tolist()</code> but I thought that they are the same:</p>

<pre><code>In [97]: %timeit df.columns.values.tolist()
100000 loops, best of 3: 2.97 s per loop

In [98]: %timeit df.columns.tolist()
10000 loops, best of 3: 9.67 s per loop
</code></pre>
";;['In [97]: %timeit df.columns.values.tolist()\n100000 loops, best of 3: 2.97 s per loop\n\nIn [98]: %timeit df.columns.tolist()\n10000 loops, best of 3: 9.67 s per loop\n'];['df.columns.values.tolist()', 'df.columns.tolist()', 'In [97]: %timeit df.columns.values.tolist()\n100000 loops, best of 3: 2.97 s per loop\n\nIn [98]: %timeit df.columns.tolist()\n10000 loops, best of 3: 9.67 s per loop\n']
1415;;1;34192820;20868394.0;2;18;;;"<p>What about?</p>

<pre><code>df.columns.values[2] = ""new_name""
</code></pre>
";;"['df.columns.values[2] = ""new_name""\n']";"['df.columns.values[2] = ""new_name""\n']"
1416;;1;34262133;14507794.0;2;35;;;"<pre><code>pd.DataFrame(df.to_records()) # multiindex become columns and new index is integers only
</code></pre>
";;['pd.DataFrame(df.to_records()) # multiindex become columns and new index is integers only\n'];['pd.DataFrame(df.to_records()) # multiindex become columns and new index is integers only\n']
1417;;6;34277514;16176996.0;2;50;;;"<p>Since version <code>0.15.0</code> this can now be easily done using <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#dt-accessor"" rel=""noreferrer""><code>.dt</code></a> to access just the date component:</p>

<pre><code>df['just_date'] = df['dates'].dt.date
</code></pre>
";;"[""df['just_date'] = df['dates'].dt.date\n""]";"['0.15.0', '.dt', ""df['just_date'] = df['dates'].dt.date\n""]"
1418;;5;34282362;16740887.0;2;7;;;"<p>I would use HDF5/pytables as follows:</p>

<ol>
<li>Keep the data as a python list ""as long as possible"".</li>
<li>Append your results to that list.</li>
<li>When it gets ""big"":

<ul>
<li>push to HDF5 Store using pandas io (and an appendable table).</li>
<li>clear the list.</li>
</ul></li>
<li>Repeat.</li>
</ol>

<p>In fact, the function I define uses a list for each ""key"" so that you can store multiple DataFrames to the HDF5 Store in the same process.</p>

<hr>

<p>We define a function which you call with each row <code>d</code>:</p>

<pre><code>CACHE = {}
STORE = 'store.h5'   # Note: another option is to keep the actual file open

def process_row(d, key, max_len=5000, _cache=CACHE):
    """"""
    Append row d to the store 'key'.

    When the number of items in the key's cache reaches max_len,
    append the list of rows to the HDF5 store and clear the list.

    """"""
    # keep the rows for each key separate.
    lst = _cache.setdefault(key, [])
    if len(lst) &gt;= max_len:
        store_and_clear(lst, key)
    lst.append(d)

def store_and_clear(lst, key):
    """"""
    Convert key's cache list to a DataFrame and append that to HDF5.
    """"""
    df = pd.DataFrame(lst)
    with pd.HDFStore(STORE) as store:
        store.append(key, df)
    lst.clear()
</code></pre>

<p><em>Note: we use the with statement to automatically close the store after each write. It <strong>may</strong> be faster to keep it open, but if so <a href=""http://www.pytables.org/usersguide/tutorials.html#creating-a-new-table"">it's recommended that you flush regularly (closing flushes)</a>. Also note it may be more readable to have used a <a href=""https://docs.python.org/2/library/collections.html#collections.deque"">collections deque</a> rather than a list, but the performance of a list will be slightly better here.</em></p>

<p>To use this you call as:</p>

<pre><code>process_row({'time' :'2013-01-01 00:00:00', 'stock' : 'BLAH', 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0},
            key=""df"")
</code></pre>

<p><em>Note: ""df"" is the stored <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#hierarchical-keys"">key</a> used in the pytables store.</em></p>

<p>Once the job has finished ensure you <code>store_and_clear</code> the remaining cache:</p>

<pre><code>for k, lst in CACHE.items():  # you can instead use .iteritems() in python 2
    store_and_clear(lst, k)
</code></pre>

<p>Now your complete DataFrame is available via:</p>

<pre><code>with pd.HDFStore(STORE) as store:
    df = store[""df""]                    # other keys will be store[key]
</code></pre>

<h3>Some comments:</h3>

<ul>
<li>5000 can be adjusted, try with some smaller/larger numbers to suit your needs.</li>
<li><a href=""https://www.ics.uci.edu/~pattis/ICS-33/lectures/complexitypython.txt"">List append is O(1)</a>, DataFrame append is O(<code>len(df)</code>).</li>
<li>Until you're doing stats or data-munging you don't need pandas, use what's fastest.</li>
<li>This code works with multiple key's (data points) coming in.</li>
<li>This is very little code, and we're staying in vanilla python list and then pandas dataframe...</li>
</ul>

<hr>

<p>Additionally, to get the up to date reads you could define a get method which stores and clears <em>before</em> reading. In this way you would get the most up to date data:</p>

<pre><code>def get_latest(key, _cache=CACHE):
    store_and_clear(_cache[key], key)
    with pd.HDFStore(STORE) as store:
        return store[key]
</code></pre>

<p>Now when you access with:</p>

<pre><code>df = get_latest(""df"")
</code></pre>

<p>you'll get the latest ""df"" available.</p>

<hr>

<p>Another option is <em>slightly</em> more involved: define a custom table in vanilla pytables, see the <a href=""http://www.pytables.org/usersguide/tutorials.html#creating-a-new-table"">tutorial</a>.</p>

<p><em>Note: You need to know the field-names to create the <a href=""http://www.pytables.org/usersguide/tutorials.html#declaring-a-column-descriptor"">column descriptor</a>.</em></p>
";;"['CACHE = {}\nSTORE = \'store.h5\'   # Note: another option is to keep the actual file open\n\ndef process_row(d, key, max_len=5000, _cache=CACHE):\n    """"""\n    Append row d to the store \'key\'.\n\n    When the number of items in the key\'s cache reaches max_len,\n    append the list of rows to the HDF5 store and clear the list.\n\n    """"""\n    # keep the rows for each key separate.\n    lst = _cache.setdefault(key, [])\n    if len(lst) >= max_len:\n        store_and_clear(lst, key)\n    lst.append(d)\n\ndef store_and_clear(lst, key):\n    """"""\n    Convert key\'s cache list to a DataFrame and append that to HDF5.\n    """"""\n    df = pd.DataFrame(lst)\n    with pd.HDFStore(STORE) as store:\n        store.append(key, df)\n    lst.clear()\n', 'process_row({\'time\' :\'2013-01-01 00:00:00\', \'stock\' : \'BLAH\', \'high\' : 4.0, \'low\' : 3.0, \'open\' : 2.0, \'close\' : 1.0},\n            key=""df"")\n', 'for k, lst in CACHE.items():  # you can instead use .iteritems() in python 2\n    store_and_clear(lst, k)\n', 'with pd.HDFStore(STORE) as store:\n    df = store[""df""]                    # other keys will be store[key]\n', 'def get_latest(key, _cache=CACHE):\n    store_and_clear(_cache[key], key)\n    with pd.HDFStore(STORE) as store:\n        return store[key]\n', 'df = get_latest(""df"")\n']";"['d', 'CACHE = {}\nSTORE = \'store.h5\'   # Note: another option is to keep the actual file open\n\ndef process_row(d, key, max_len=5000, _cache=CACHE):\n    """"""\n    Append row d to the store \'key\'.\n\n    When the number of items in the key\'s cache reaches max_len,\n    append the list of rows to the HDF5 store and clear the list.\n\n    """"""\n    # keep the rows for each key separate.\n    lst = _cache.setdefault(key, [])\n    if len(lst) >= max_len:\n        store_and_clear(lst, key)\n    lst.append(d)\n\ndef store_and_clear(lst, key):\n    """"""\n    Convert key\'s cache list to a DataFrame and append that to HDF5.\n    """"""\n    df = pd.DataFrame(lst)\n    with pd.HDFStore(STORE) as store:\n        store.append(key, df)\n    lst.clear()\n', 'process_row({\'time\' :\'2013-01-01 00:00:00\', \'stock\' : \'BLAH\', \'high\' : 4.0, \'low\' : 3.0, \'open\' : 2.0, \'close\' : 1.0},\n            key=""df"")\n', 'store_and_clear', 'for k, lst in CACHE.items():  # you can instead use .iteritems() in python 2\n    store_and_clear(lst, k)\n', 'with pd.HDFStore(STORE) as store:\n    df = store[""df""]                    # other keys will be store[key]\n', 'len(df)', 'def get_latest(key, _cache=CACHE):\n    store_and_clear(_cache[key], key)\n    with pd.HDFStore(STORE) as store:\n        return store[key]\n', 'df = get_latest(""df"")\n']"
1419;;4;34297689;13035764.0;2;117;;;"<p>I would suggest using the <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.Index.duplicated.html"" rel=""noreferrer"">duplicated</a> method on the Pandas Index itself:</p>

<pre><code>df3 = df3[~df3.index.duplicated(keep='first')]
</code></pre>

<p>While all the other methods work, the <a href=""https://stackoverflow.com/a/14900065/3622349"">currently accepted answer</a> is by far the least performant for the provided example. Furthermore, while the groupby method is only slightly less performant, I find the duplicated method to be more readable.</p>

<p>Using the sample data provided:</p>

<pre><code>&gt;&gt;&gt; %timeit df3.reset_index().drop_duplicates(subset='index', keep='first').set_index('index')
1000 loops, best of 3: 1.54 ms per loop

&gt;&gt;&gt; %timeit df3.groupby(df3.index).first()
1000 loops, best of 3: 580 s per loop

&gt;&gt;&gt; %timeit df3[~df3.index.duplicated(keep='first')]
1000 loops, best of 3: 307 s per loop
</code></pre>

<p>Note that you can keep the last element by changing the keep argument.</p>

<p>It should also be noted that this method works with <code>MultiIndex</code> as well (using df1 as specified in Paul's example):</p>

<pre><code>&gt;&gt;&gt; %timeit df1.groupby(level=df1.index.names).last()
1000 loops, best of 3: 771 s per loop

&gt;&gt;&gt; %timeit df1[~df1.index.duplicated(keep='last')]
1000 loops, best of 3: 365 s per loop
</code></pre>
";;"[""df3 = df3[~df3.index.duplicated(keep='first')]\n"", "">>> %timeit df3.reset_index().drop_duplicates(subset='index', keep='first').set_index('index')\n1000 loops, best of 3: 1.54 ms per loop\n\n>>> %timeit df3.groupby(df3.index).first()\n1000 loops, best of 3: 580 s per loop\n\n>>> %timeit df3[~df3.index.duplicated(keep='first')]\n1000 loops, best of 3: 307 s per loop\n"", "">>> %timeit df1.groupby(level=df1.index.names).last()\n1000 loops, best of 3: 771 s per loop\n\n>>> %timeit df1[~df1.index.duplicated(keep='last')]\n1000 loops, best of 3: 365 s per loop\n""]";"[""df3 = df3[~df3.index.duplicated(keep='first')]\n"", "">>> %timeit df3.reset_index().drop_duplicates(subset='index', keep='first').set_index('index')\n1000 loops, best of 3: 1.54 ms per loop\n\n>>> %timeit df3.groupby(df3.index).first()\n1000 loops, best of 3: 580 s per loop\n\n>>> %timeit df3[~df3.index.duplicated(keep='first')]\n1000 loops, best of 3: 307 s per loop\n"", 'MultiIndex', "">>> %timeit df1.groupby(level=df1.index.names).last()\n1000 loops, best of 3: 771 s per loop\n\n>>> %timeit df1[~df1.index.duplicated(keep='last')]\n1000 loops, best of 3: 365 s per loop\n""]"
1420;;3;34311080;7837722.0;2;43;;;"<p>Like what has been mentioned before, pandas object is most efficient when process the whole array at once. However for those who really need to loop through a pandas DataFrame to perform something, like me, I found at least three ways to do it. I have done a short test to see which one of the three is the least time consuming.</p>

<pre><code>t = pd.DataFrame({'a': range(0, 10000), 'b': range(10000, 20000)})
B = []
C = []
A = time.time()
for i,r in t.iterrows():
    C.append((r['a'], r['b']))
B.append(time.time()-A)

C = []
A = time.time()
for ir in t.itertuples():
    C.append((ir[1], ir[2]))    
B.append(time.time()-A)

C = []
A = time.time()
for r in zip(t['a'], t['b']):
    C.append((r[0], r[1]))
B.append(time.time()-A)

print B
</code></pre>

<p>Result:</p>

<pre><code>[0.5639059543609619, 0.017839908599853516, 0.005645036697387695]
</code></pre>

<p>This is probably not the best way to measure the time consumption but it's quick for me.</p>

<p>Here are some pros and cons IMHO:</p>

<ul>
<li>.iterrows(): return index and row items in separate variables, but significantly slower</li>
<li>.itertuples(): faster than .iterrows(), but return index together with row items, ir[0] is the index</li>
<li>zip: quickest, but no access to index of the row</li>
</ul>
";;"[""t = pd.DataFrame({'a': range(0, 10000), 'b': range(10000, 20000)})\nB = []\nC = []\nA = time.time()\nfor i,r in t.iterrows():\n    C.append((r['a'], r['b']))\nB.append(time.time()-A)\n\nC = []\nA = time.time()\nfor ir in t.itertuples():\n    C.append((ir[1], ir[2]))    \nB.append(time.time()-A)\n\nC = []\nA = time.time()\nfor r in zip(t['a'], t['b']):\n    C.append((r[0], r[1]))\nB.append(time.time()-A)\n\nprint B\n"", '[0.5639059543609619, 0.017839908599853516, 0.005645036697387695]\n']";"[""t = pd.DataFrame({'a': range(0, 10000), 'b': range(10000, 20000)})\nB = []\nC = []\nA = time.time()\nfor i,r in t.iterrows():\n    C.append((r['a'], r['b']))\nB.append(time.time()-A)\n\nC = []\nA = time.time()\nfor ir in t.itertuples():\n    C.append((ir[1], ir[2]))    \nB.append(time.time()-A)\n\nC = []\nA = time.time()\nfor r in zip(t['a'], t['b']):\n    C.append((r[0], r[1]))\nB.append(time.time()-A)\n\nprint B\n"", '[0.5639059543609619, 0.017839908599853516, 0.005645036697387695]\n']"
1421;;1;34333886;28901683.0;2;6;;;"<p>Suppose you have two dataframes, df_1 and df_2 having multiple fields(column_names) and you want to find the only those entries in df_1 that are not in df_2 on the basis of some fields(e.g. fields_x, fields_y), follow the following steps.</p>

<p>Step1.Add a column key1 and key2 to df_1 and df_2 respectively.</p>

<p>Step2.Merge the dataframes as shown below. field_x and field_y are our desired columns.</p>

<p>Step3.Select only those rows from df_1 where key1 is not equal to key2.</p>

<p>Step4.Drop key1 and key2.</p>

<p>This method will solve your problem and works fast even with big data sets. I have tried it for dataframes with more than 1,000,000 rows.</p>

<pre><code>df_1['key1'] = 1
df_2['key2'] = 1
df_1 = pd.merge(df_1, df_2, on=['field_x', 'field_y'], how = 'left')
df_1 = df_1[~(df_1.key2 == df_1.key1)]
df_1 = df_1.drop(['key1','key2'], axis=1)
</code></pre>
";;"[""df_1['key1'] = 1\ndf_2['key2'] = 1\ndf_1 = pd.merge(df_1, df_2, on=['field_x', 'field_y'], how = 'left')\ndf_1 = df_1[~(df_1.key2 == df_1.key1)]\ndf_1 = df_1.drop(['key1','key2'], axis=1)\n""]";"[""df_1['key1'] = 1\ndf_2['key2'] = 1\ndf_1 = pd.merge(df_1, df_2, on=['field_x', 'field_y'], how = 'left')\ndf_1 = df_1[~(df_1.key2 == df_1.key1)]\ndf_1 = df_1.drop(['key1','key2'], axis=1)\n""]"
1422;;0;34530065;25039626.0;2;25;;;"<p>You can use the following command to filter only numeric columns</p>

<pre><code>df._get_numeric_data()
</code></pre>

<p>Example</p>

<pre><code>In [32]: data
Out[32]:
   A  B
0  1  s
1  2  s
2  3  s
3  4  s

In [33]: data._get_numeric_data()
Out[33]:
   A
0  1
1  2
2  3
3  4
</code></pre>
";;['df._get_numeric_data()\n', 'In [32]: data\nOut[32]:\n   A  B\n0  1  s\n1  2  s\n2  3  s\n3  4  s\n\nIn [33]: data._get_numeric_data()\nOut[33]:\n   A\n0  1\n1  2\n2  3\n3  4\n'];['df._get_numeric_data()\n', 'In [32]: data\nOut[32]:\n   A  B\n0  1  s\n1  2  s\n2  3  s\n3  4  s\n\nIn [33]: data._get_numeric_data()\nOut[33]:\n   A\n0  1\n1  2\n2  3\n3  4\n']
1423;;0;34548894;9652832.0;2;22;;;"<p>As of 17.0 <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.DataFrame.from_csv.html""><code>from_csv</code></a> is discouraged.</p>

<p>Use <code>pd.read_csv(fpath, sep='\t')</code> or <code>pd.read_table(fpath)</code>.</p>
";;[];"['from_csv', ""pd.read_csv(fpath, sep='\\t')"", 'pd.read_table(fpath)']"
1424;;0;34551914;9758450.0;2;16;;;"<pre><code>list(data_set.itertuples(index=False))
</code></pre>

<p>As of 17.1, the above will return a list of namedtuples.</p>
";;['list(data_set.itertuples(index=False))\n'];['list(data_set.itertuples(index=False))\n']
1425;;0;34576537;13411544.0;2;31;;;"<p>A nice addition is the ability to <strong>drop columns only if they exist</strong>, this way you can cover more use cases, and it will only drop the existing columns from the labels passed to it: </p>

<p>simply add <strong>errors='ignore'</strong> ,e.g:</p>

<pre><code>df.drop(['col_name_1','col_name_2',...,'col_name_N'],inplace=True,axis=1,errors='ignore')
</code></pre>

<ul>
<li>this is new from pandas 0.16.1, docs are <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.drop.html"" rel=""noreferrer"">here</a></li>
</ul>
";;"[""df.drop(['col_name_1','col_name_2',...,'col_name_N'],inplace=True,axis=1,errors='ignore')\n""]";"[""df.drop(['col_name_1','col_name_2',...,'col_name_N'],inplace=True,axis=1,errors='ignore')\n""]"
1426;;0;34614046;14661701.0;2;54;;;"<p>Note that it may be important to use the ""inplace"" command when you want to do the drop in line. </p>

<pre><code>df.drop(df.index[[1,3]], inplace=True)
</code></pre>

<p>Because your original question is not returning anything, this command should be used.
<a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.drop.html"">http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.drop.html</a></p>
";;['df.drop(df.index[[1,3]], inplace=True)\n'];['df.drop(df.index[[1,3]], inplace=True)\n']
1427;;1;34687479;16628819.0;2;18;;;"<p>To answer my own question, this functionality has been added to pandas in the meantime. Starting <strong>from pandas 0.15.0</strong>, you can use <code>tz_localize(None)</code> to remove the timezone resulting in local time.<br>
See the whatsnew entry: <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#timezone-handling-improvements"">http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#timezone-handling-improvements</a></p>

<p>So with my example from above:</p>

<pre><code>In [4]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=2, freq='H',
                          tz= ""Europe/Brussels"")

In [5]: t
Out[5]: DatetimeIndex(['2013-05-18 12:00:00+02:00', '2013-05-18 13:00:00+02:00'],
                       dtype='datetime64[ns, Europe/Brussels]', freq='H')
</code></pre>

<p>using <code>tz_localize(None)</code> removes the timezone information resulting in <strong>naive local time</strong>:</p>

<pre><code>In [6]: t.tz_localize(None)
Out[6]: DatetimeIndex(['2013-05-18 12:00:00', '2013-05-18 13:00:00'], 
                      dtype='datetime64[ns]', freq='H')
</code></pre>

<p>Further, you can also use <code>tz_convert(None)</code> to remove the timezone information but converting to UTC, so yielding <strong>naive UTC time</strong>:</p>

<pre><code>In [7]: t.tz_convert(None)
Out[7]: DatetimeIndex(['2013-05-18 10:00:00', '2013-05-18 11:00:00'], 
                      dtype='datetime64[ns]', freq='H')
</code></pre>

<hr>

<p>This is much <strong>more performant</strong> than the <code>datetime.replace</code> solution:</p>

<pre><code>In [31]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=10000, freq='H',
                           tz=""Europe/Brussels"")

In [32]: %timeit t.tz_localize(None)
1000 loops, best of 3: 233 s per loop

In [33]: %timeit pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])
10 loops, best of 3: 99.7 ms per loop
</code></pre>
";;"['In [4]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=2, freq=\'H\',\n                          tz= ""Europe/Brussels"")\n\nIn [5]: t\nOut[5]: DatetimeIndex([\'2013-05-18 12:00:00+02:00\', \'2013-05-18 13:00:00+02:00\'],\n                       dtype=\'datetime64[ns, Europe/Brussels]\', freq=\'H\')\n', ""In [6]: t.tz_localize(None)\nOut[6]: DatetimeIndex(['2013-05-18 12:00:00', '2013-05-18 13:00:00'], \n                      dtype='datetime64[ns]', freq='H')\n"", ""In [7]: t.tz_convert(None)\nOut[7]: DatetimeIndex(['2013-05-18 10:00:00', '2013-05-18 11:00:00'], \n                      dtype='datetime64[ns]', freq='H')\n"", 'In [31]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=10000, freq=\'H\',\n                           tz=""Europe/Brussels"")\n\nIn [32]: %timeit t.tz_localize(None)\n1000 loops, best of 3: 233 s per loop\n\nIn [33]: %timeit pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])\n10 loops, best of 3: 99.7 ms per loop\n']";"['tz_localize(None)', 'In [4]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=2, freq=\'H\',\n                          tz= ""Europe/Brussels"")\n\nIn [5]: t\nOut[5]: DatetimeIndex([\'2013-05-18 12:00:00+02:00\', \'2013-05-18 13:00:00+02:00\'],\n                       dtype=\'datetime64[ns, Europe/Brussels]\', freq=\'H\')\n', 'tz_localize(None)', ""In [6]: t.tz_localize(None)\nOut[6]: DatetimeIndex(['2013-05-18 12:00:00', '2013-05-18 13:00:00'], \n                      dtype='datetime64[ns]', freq='H')\n"", 'tz_convert(None)', ""In [7]: t.tz_convert(None)\nOut[7]: DatetimeIndex(['2013-05-18 10:00:00', '2013-05-18 11:00:00'], \n                      dtype='datetime64[ns]', freq='H')\n"", 'datetime.replace', 'In [31]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=10000, freq=\'H\',\n                           tz=""Europe/Brussels"")\n\nIn [32]: %timeit t.tz_localize(None)\n1000 loops, best of 3: 233 s per loop\n\nIn [33]: %timeit pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])\n10 loops, best of 3: 99.7 ms per loop\n']"
1428;;2;34756965;17874063.0;2;6;;;"<p>Looks like @CarstenKnig <a href=""https://stackoverflow.com/a/16399202/623735"">found the right way</a>:</p>

<pre><code>df.hist(bins=20, weights=np.ones_like(df[df.columns[0]]) * 100. / len(df))
</code></pre>
";;['df.hist(bins=20, weights=np.ones_like(df[df.columns[0]]) * 100. / len(df))\n'];['df.hist(bins=20, weights=np.ones_like(df[df.columns[0]]) * 100. / len(df))\n']
1429;;2;34790248;17618981.0;2;17;;;"<p>Use of <code>sort</code> can result in warning message. See <a href=""https://github.com/bokeh/bokeh/issues/3014"">github</a> discussion.
So you might wanna use <code>sort_values</code>, docs <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.DataFrame.sort_values.html"">here</a></p>

<p>Then your code can look like this:</p>

<pre><code>df = df.sort_values(by=['c1','c2'], ascending=[False,True])
</code></pre>
";;"[""df = df.sort_values(by=['c1','c2'], ascending=[False,True])\n""]";"['sort', 'sort_values', ""df = df.sort_values(by=['c1','c2'], ascending=[False,True])\n""]"
1430;;0;34879805;29576430.0;2;140;;;"<p>The more idiomatic way to do this with pandas is to use the <code>.sample</code> method of your dataframe, i.e.</p>

<pre><code>df.sample(frac=1)
</code></pre>

<p>The <code>frac</code> keyword argument specifies the fraction of rows to return in the random sample, so <code>frac=1</code> means return all rows (in random order).</p>

<p><strong>Note:</strong>
<em>If you wish to shuffle your dataframe in-place and reset the index, you could do e.g.</em></p>

<pre><code>df = df.sample(frac=1).reset_index(drop=True)
</code></pre>

<p><em>Here, specifying <code>drop=True</code> prevents <code>.reset_index</code> from creating a column containing the old index entries.</em></p>
";;['df.sample(frac=1)\n', 'df = df.sample(frac=1).reset_index(drop=True)\n'];['.sample', 'df.sample(frac=1)\n', 'frac', 'frac=1', 'df = df.sample(frac=1).reset_index(drop=True)\n', 'drop=True', '.reset_index']
1431;;0;34962199;34962104.0;2;67;;;"<p>Given a sample dataframe <code>df</code> as:</p>

<pre><code>a,b
1,2
2,3
3,4
4,5
</code></pre>

<p>what you want is:</p>

<pre><code>df['a'] = df['a'].apply(lambda x: x + 1)
</code></pre>

<p>that returns:</p>

<pre><code>   a  b
0  2  2
1  3  3
2  4  4
3  5  5
</code></pre>
";;"['a,b\n1,2\n2,3\n3,4\n4,5\n', ""df['a'] = df['a'].apply(lambda x: x + 1)\n"", '   a  b\n0  2  2\n1  3  3\n2  4  4\n3  5  5\n']";"['df', 'a,b\n1,2\n2,3\n3,4\n4,5\n', ""df['a'] = df['a'].apply(lambda x: x + 1)\n"", '   a  b\n0  2  2\n1  3  3\n2  4  4\n3  5  5\n']"
1432;;0;34962592;34962104.0;2;9;;;"<p>You don't need a function at all. You can work on a whole column directly.</p>

<p>Example data:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'a': [100, 1000], 'b': [200, 2000], 'c': [300, 3000]})
&gt;&gt;&gt; df

      a     b     c
0   100   200   300
1  1000  2000  3000
</code></pre>

<p>Half all the values in column <code>a</code>:</p>

<pre><code>&gt;&gt;&gt; df.a = df.a / 2
&gt;&gt;&gt; df

     a     b     c
0   50   200   300
1  500  2000  3000
</code></pre>
";;"["">>> df = pd.DataFrame({'a': [100, 1000], 'b': [200, 2000], 'c': [300, 3000]})\n>>> df\n\n      a     b     c\n0   100   200   300\n1  1000  2000  3000\n"", '>>> df.a = df.a / 2\n>>> df\n\n     a     b     c\n0   50   200   300\n1  500  2000  3000\n']";"["">>> df = pd.DataFrame({'a': [100, 1000], 'b': [200, 2000], 'c': [300, 3000]})\n>>> df\n\n      a     b     c\n0   100   200   300\n1  1000  2000  3000\n"", 'a', '>>> df.a = df.a / 2\n>>> df\n\n     a     b     c\n0   50   200   300\n1  500  2000  3000\n']"
1433;;0;34996876;19112398.0;2;34;;;"<p>With approach explained by EdChum above, the values in the list are shown as rows. To show the values of lists as columns in DataFrame instead, simply use transpose() as following:</p>

<pre><code>table = [[1 , 2], [3, 4]]
df = DataFrame(table)
df = df.transpose()
df.columns = ['Heading1', 'Heading2']
</code></pre>

<p>The output then is:</p>

<pre><code>      Heading1  Heading2
0         1        3
1         2        4
</code></pre>
";;"[""table = [[1 , 2], [3, 4]]\ndf = DataFrame(table)\ndf = df.transpose()\ndf.columns = ['Heading1', 'Heading2']\n"", '      Heading1  Heading2\n0         1        3\n1         2        4\n']";"[""table = [[1 , 2], [3, 4]]\ndf = DataFrame(table)\ndf = df.transpose()\ndf.columns = ['Heading1', 'Heading2']\n"", '      Heading1  Heading2\n0         1        3\n1         2        4\n']"
1434;;0;35068123;11346283.0;2;8;;;"<p>You could use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.slice.html"" rel=""noreferrer""><code>str.slice</code></a> for that:</p>

<pre><code>df.columns = df.columns.str.slice(1)
</code></pre>
";;['df.columns = df.columns.str.slice(1)\n'];['str.slice', 'df.columns = df.columns.str.slice(1)\n']
1435;;0;35203149;11285613.0;2;16;;;"<p>I realize this question is quite old, but in the latest version of pandas there is an easy way to do exactly this. Column names (which are strings) <strong>can</strong> be sliced in whatever manner you like.</p>

<pre><code>columns = ['b', 'c']
df1 = pd.DataFrame(df, columns=columns)
</code></pre>
";;"[""columns = ['b', 'c']\ndf1 = pd.DataFrame(df, columns=columns)\n""]";"[""columns = ['b', 'c']\ndf1 = pd.DataFrame(df, columns=columns)\n""]"
1436;;2;35203658;22898824.0;2;21;;;"<p>Previous answer is not correct in my experience, you can't pass it a simple string, needs to be a datetime object. So:</p>

<pre><code>import datetime 
df.ix[datetime.date(year=2014,month=1,day=1):datetime.date(year=2014,month=2,day=1)]
</code></pre>
";;['import datetime \ndf.ix[datetime.date(year=2014,month=1,day=1):datetime.date(year=2014,month=2,day=1)]\n'];['import datetime \ndf.ix[datetime.date(year=2014,month=1,day=1):datetime.date(year=2014,month=2,day=1)]\n']
1437;;0;35212740;18039057.0;2;10;;;"<p>I had this problem as well but perhaps for a different reason. I had some trailing commas in my CSV that were adding an additional column that pandas was attempting to read. Using the following works but it simply ignores the bad lines:</p>

<pre><code>data = pd.read_csv('file1.csv', error_bad_lines=False)
</code></pre>

<p>If you want to keep the lines an ugly kind of hack for handling the errors is to do something like the following:</p>

<pre><code>line     = []
expected = []
saw      = []     
cont     = True 

while cont == True:     
    try:
        data = pd.read_csv('file1.csv',skiprows=line)
        cont = False
    except Exception as e:    
        errortype = e.message.split('.')[0].strip()                                
        if errortype == 'Error tokenizing data':                        
           cerror      = e.message.split(':')[1].strip().replace(',','')
           nums        = [n for n in cerror.split(' ') if str.isdigit(n)]
           expected.append(int(nums[0]))
           saw.append(int(nums[2]))
           line.append(int(nums[1])-1)
         else:
           cerror      = 'Unknown'
           print 'Unknown Error - 222'

if line != []:
    # Handle the errors however you want
</code></pre>

<p>I proceeded to write a script to reinsert the lines into the DataFrame since the bad lines will be given by the variable 'line' in the above code. This can all be avoided by simply using the csv reader. Hopefully the pandas developers can make it easier to deal with this situation in the future.</p>
";;"[""data = pd.read_csv('file1.csv', error_bad_lines=False)\n"", ""line     = []\nexpected = []\nsaw      = []     \ncont     = True \n\nwhile cont == True:     \n    try:\n        data = pd.read_csv('file1.csv',skiprows=line)\n        cont = False\n    except Exception as e:    \n        errortype = e.message.split('.')[0].strip()                                \n        if errortype == 'Error tokenizing data':                        \n           cerror      = e.message.split(':')[1].strip().replace(',','')\n           nums        = [n for n in cerror.split(' ') if str.isdigit(n)]\n           expected.append(int(nums[0]))\n           saw.append(int(nums[2]))\n           line.append(int(nums[1])-1)\n         else:\n           cerror      = 'Unknown'\n           print 'Unknown Error - 222'\n\nif line != []:\n    # Handle the errors however you want\n""]";"[""data = pd.read_csv('file1.csv', error_bad_lines=False)\n"", ""line     = []\nexpected = []\nsaw      = []     \ncont     = True \n\nwhile cont == True:     \n    try:\n        data = pd.read_csv('file1.csv',skiprows=line)\n        cont = False\n    except Exception as e:    \n        errortype = e.message.split('.')[0].strip()                                \n        if errortype == 'Error tokenizing data':                        \n           cerror      = e.message.split(':')[1].strip().replace(',','')\n           nums        = [n for n in cerror.split(' ') if str.isdigit(n)]\n           expected.append(int(nums[0]))\n           saw.append(int(nums[2]))\n           line.append(int(nums[1])-1)\n         else:\n           cerror      = 'Unknown'\n           print 'Unknown Error - 222'\n\nif line != []:\n    # Handle the errors however you want\n""]"
1438;;0;35219658;20230326.0;2;19;;;"<pre><code>df.ix[:, df.columns != col]
</code></pre>

<p>where <code>col</code> is the name of the column to leave out.</p>
";;['df.ix[:, df.columns != col]\n'];['df.ix[:, df.columns != col]\n', 'col']
1439;;2;35240942;17950374.0;2;32;;;"<p>Change data type of DataFrame column:</p>

<p>To int:</p>

<p><code>df.column_name = df.column_name.astype(np.int64)</code></p>

<p>To str:</p>

<p><code>df.column_name = df.column_name.astype(str)</code></p>
";;[];['df.column_name = df.column_name.astype(np.int64)', 'df.column_name = df.column_name.astype(str)']
1440;;0;35245297;20763012.0;2;8;;;"<p>I agree with Joris; it seems like you should be doing this differently, like with <a href=""http://docs.scipy.org/doc/numpy-1.10.1/user/basics.rec.html"" rel=""nofollow noreferrer"">numpy record arrays</a>. Modifying ""option 2"" from <a href=""https://stackoverflow.com/a/21647198/943773"">this great answer</a>, you could do it like this:</p>

<pre><code>import pandas
import numpy

dtype = [('Col1','int32'), ('Col2','float32'), ('Col3','float32')]
values = numpy.zeros(20, dtype=dtype)
index = ['Row'+str(i) for i in range(1, len(values)+1)]

df = pandas.DataFrame(values, index=index)
</code></pre>
";;"[""import pandas\nimport numpy\n\ndtype = [('Col1','int32'), ('Col2','float32'), ('Col3','float32')]\nvalues = numpy.zeros(20, dtype=dtype)\nindex = ['Row'+str(i) for i in range(1, len(values)+1)]\n\ndf = pandas.DataFrame(values, index=index)\n""]";"[""import pandas\nimport numpy\n\ndtype = [('Col1','int32'), ('Col2','float32'), ('Col3','float32')]\nvalues = numpy.zeros(20, dtype=dtype)\nindex = ['Row'+str(i) for i in range(1, len(values)+1)]\n\ndf = pandas.DataFrame(values, index=index)\n""]"
1441;;4;35246041;19611729.0;2;16;;;"<p>Seems to work for me without the <code>StringIO</code>:</p>

<pre><code>test = pd.read_csv('https://docs.google.com/spreadsheets/d/' + 
                   '0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc' +
                   '/export?gid=0&amp;format=csv',
                   # Set first column as rownames in data frame
                   index_col=0,
                   # Parse column values to datetime
                   parse_dates=['Quradate']
                  )
test.head(5)  # Same result as @TomAugspurger
</code></pre>

<p>BTW, including the <code>?gid=</code> enables importing different sheets, find the gid in the URL.</p>
";;"[""test = pd.read_csv('https://docs.google.com/spreadsheets/d/' + \n                   '0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc' +\n                   '/export?gid=0&format=csv',\n                   # Set first column as rownames in data frame\n                   index_col=0,\n                   # Parse column values to datetime\n                   parse_dates=['Quradate']\n                  )\ntest.head(5)  # Same result as @TomAugspurger\n""]";"['StringIO', ""test = pd.read_csv('https://docs.google.com/spreadsheets/d/' + \n                   '0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc' +\n                   '/export?gid=0&format=csv',\n                   # Set first column as rownames in data frame\n                   index_col=0,\n                   # Parse column values to datetime\n                   parse_dates=['Quradate']\n                  )\ntest.head(5)  # Same result as @TomAugspurger\n"", '?gid=']"
1442;;2;35282530;17071871.0;2;24;;;"<p>I find the syntax of the previous answers to be redundant and difficult to remember. Pandas introduced the <code>query()</code> method in v0.13 and I much prefer it. For your question, you could do <code>df.query('col == val')</code></p>

<p>Reproduced from <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/indexing.html#indexing-query"">http://pandas.pydata.org/pandas-docs/version/0.17.0/indexing.html#indexing-query</a></p>

<pre><code>In [167]: n = 10

In [168]: df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))

In [169]: df
Out[169]: 
          a         b         c
0  0.687704  0.582314  0.281645
1  0.250846  0.610021  0.420121
2  0.624328  0.401816  0.932146
3  0.011763  0.022921  0.244186
4  0.590198  0.325680  0.890392
5  0.598892  0.296424  0.007312
6  0.634625  0.803069  0.123872
7  0.924168  0.325076  0.303746
8  0.116822  0.364564  0.454607
9  0.986142  0.751953  0.561512

# pure python
In [170]: df[(df.a &lt; df.b) &amp; (df.b &lt; df.c)]
Out[170]: 
          a         b         c
3  0.011763  0.022921  0.244186
8  0.116822  0.364564  0.454607

# query
In [171]: df.query('(a &lt; b) &amp; (b &lt; c)')
Out[171]: 
          a         b         c
3  0.011763  0.022921  0.244186
8  0.116822  0.364564  0.454607
</code></pre>

<p>You can also access variables in the environment by prepending an <code>@</code>.</p>

<pre><code>exclude = ('red', 'orange')
df.query('color not in @exclude')
</code></pre>
";;"[""In [167]: n = 10\n\nIn [168]: df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))\n\nIn [169]: df\nOut[169]: \n          a         b         c\n0  0.687704  0.582314  0.281645\n1  0.250846  0.610021  0.420121\n2  0.624328  0.401816  0.932146\n3  0.011763  0.022921  0.244186\n4  0.590198  0.325680  0.890392\n5  0.598892  0.296424  0.007312\n6  0.634625  0.803069  0.123872\n7  0.924168  0.325076  0.303746\n8  0.116822  0.364564  0.454607\n9  0.986142  0.751953  0.561512\n\n# pure python\nIn [170]: df[(df.a < df.b) & (df.b < df.c)]\nOut[170]: \n          a         b         c\n3  0.011763  0.022921  0.244186\n8  0.116822  0.364564  0.454607\n\n# query\nIn [171]: df.query('(a < b) & (b < c)')\nOut[171]: \n          a         b         c\n3  0.011763  0.022921  0.244186\n8  0.116822  0.364564  0.454607\n"", ""exclude = ('red', 'orange')\ndf.query('color not in @exclude')\n""]";"['query()', ""df.query('col == val')"", ""In [167]: n = 10\n\nIn [168]: df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))\n\nIn [169]: df\nOut[169]: \n          a         b         c\n0  0.687704  0.582314  0.281645\n1  0.250846  0.610021  0.420121\n2  0.624328  0.401816  0.932146\n3  0.011763  0.022921  0.244186\n4  0.590198  0.325680  0.890392\n5  0.598892  0.296424  0.007312\n6  0.634625  0.803069  0.123872\n7  0.924168  0.325076  0.303746\n8  0.116822  0.364564  0.454607\n9  0.986142  0.751953  0.561512\n\n# pure python\nIn [170]: df[(df.a < df.b) & (df.b < df.c)]\nOut[170]: \n          a         b         c\n3  0.011763  0.022921  0.244186\n8  0.116822  0.364564  0.454607\n\n# query\nIn [171]: df.query('(a < b) & (b < c)')\nOut[171]: \n          a         b         c\n3  0.011763  0.022921  0.244186\n8  0.116822  0.364564  0.454607\n"", '@', ""exclude = ('red', 'orange')\ndf.query('color not in @exclude')\n""]"
1443;;0;35385805;13411544.0;2;19;;;"<p>In pandas 0.16.1+ you can drop columns only if they exist per the solution posted by @eiTanLaVi.  Prior to that version, you can achieve the same result via a conditional list comprehension:</p>

<pre><code>df.drop([col for col in ['col_name_1','col_name_2',...,'col_name_N'] if col in df], 
        axis=1, inplace=True)
</code></pre>
";;"[""df.drop([col for col in ['col_name_1','col_name_2',...,'col_name_N'] if col in df], \n        axis=1, inplace=True)\n""]";"[""df.drop([col for col in ['col_name_1','col_name_2',...,'col_name_N'] if col in df], \n        axis=1, inplace=True)\n""]"
1444;;0;35387028;11346283.0;2;10;;;"<pre><code>df = pd.DataFrame({'$a': [1], '$b': [1], '$c': [1], '$d': [1], '$e': [1]}
</code></pre>

<p>If your new list of columns is in the same order as the existing columns, the assignment is simple:</p>

<pre><code>new_cols = ['a', 'b', 'c', 'd', 'e']
df.columns = new_cols
&gt;&gt;&gt; df
   a  b  c  d  e
0  1  1  1  1  1
</code></pre>

<p>If you had a dictionary keyed on old column names to new column names, you could do the following:</p>

<pre><code>d = {'$a': 'a', '$b': 'b', '$c': 'c', '$d': 'd', '$e': 'e'}
df.columns.map(lambda col: d[col])
&gt;&gt;&gt; df
   a  b  c  d  e
0  1  1  1  1  1
</code></pre>

<p>If you don't have a list or dictionary mapping, you could strip the leading <code>$</code> symbol via a list comprehension:</p>

<pre><code>df.columns = [col[1:] if col[0] == '$' else col for col in df]
</code></pre>
";;"[""df = pd.DataFrame({'$a': [1], '$b': [1], '$c': [1], '$d': [1], '$e': [1]}\n"", ""new_cols = ['a', 'b', 'c', 'd', 'e']\ndf.columns = new_cols\n>>> df\n   a  b  c  d  e\n0  1  1  1  1  1\n"", ""d = {'$a': 'a', '$b': 'b', '$c': 'c', '$d': 'd', '$e': 'e'}\ndf.columns.map(lambda col: d[col])\n>>> df\n   a  b  c  d  e\n0  1  1  1  1  1\n"", ""df.columns = [col[1:] if col[0] == '$' else col for col in df]\n""]";"[""df = pd.DataFrame({'$a': [1], '$b': [1], '$c': [1], '$d': [1], '$e': [1]}\n"", ""new_cols = ['a', 'b', 'c', 'd', 'e']\ndf.columns = new_cols\n>>> df\n   a  b  c  d  e\n0  1  1  1  1  1\n"", ""d = {'$a': 'a', '$b': 'b', '$c': 'c', '$d': 'd', '$e': 'e'}\ndf.columns.map(lambda col: d[col])\n>>> df\n   a  b  c  d  e\n0  1  1  1  1  1\n"", '$', ""df.columns = [col[1:] if col[0] == '$' else col for col in df]\n""]"
1445;;3;35387129;12555323.0;2;50;;;"<blockquote>
  <p>I would like to add a new column, 'e', to the existing data frame and do not change anything in the data frame. (The series always got the same length as a dataframe.) </p>
</blockquote>

<p>I assume that the index values in <code>e</code> match those in <code>df1</code>.</p>

<p>The easiest way to initiate a new column named <code>e</code>, and assign it the values from your series <code>e</code>:</p>

<pre><code>df['e'] = e.values
</code></pre>

<p><strong>assign (Pandas 0.16.0+)</strong></p>

<p>As of Pandas 0.16.0, you can also use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html"" rel=""noreferrer""><code>assign</code></a>, which assigns new columns to a DataFrame and returns a new object (a copy) with all the original columns in addition to the new ones.</p>

<pre><code>df1 = df1.assign(e=e.values)
</code></pre>

<p>As per <a href=""https://stackoverflow.com/questions/42101382/pandas-dataframe-assign-arguments"">this example</a> (which also includes the source code of the <code>assign</code> function), you can also include more than one column:</p>

<pre><code>df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})
&gt;&gt;&gt; df.assign(mean_a=df.a.mean(), mean_b=df.b.mean())
   a  b  mean_a  mean_b
0  1  3     1.5     3.5
1  2  4     1.5     3.5
</code></pre>

<p>In context with your example: </p>

<pre><code>np.random.seed(0)
df1 = pd.DataFrame(np.random.randn(10, 4), columns=['a', 'b', 'c', 'd'])
mask = df1.applymap(lambda x: x &lt;-0.7)
df1 = df1[-mask.any(axis=1)]
sLength = len(df1['a'])
e = pd.Series(np.random.randn(sLength))

&gt;&gt;&gt; df1
          a         b         c         d
0  1.764052  0.400157  0.978738  2.240893
2 -0.103219  0.410599  0.144044  1.454274
3  0.761038  0.121675  0.443863  0.333674
7  1.532779  1.469359  0.154947  0.378163
9  1.230291  1.202380 -0.387327 -0.302303

&gt;&gt;&gt; e
0   -1.048553
1   -1.420018
2   -1.706270
3    1.950775
4   -0.509652
dtype: float64

df1 = df1.assign(e=e.values)

&gt;&gt;&gt; df1
          a         b         c         d         e
0  1.764052  0.400157  0.978738  2.240893 -1.048553
2 -0.103219  0.410599  0.144044  1.454274 -1.420018
3  0.761038  0.121675  0.443863  0.333674 -1.706270
7  1.532779  1.469359  0.154947  0.378163  1.950775
9  1.230291  1.202380 -0.387327 -0.302303 -0.509652
</code></pre>

<p>The description of this new feature when it was first introduced can be found <a href=""http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#dataframe-assign"" rel=""noreferrer"">here</a>.</p>
";;"[""df['e'] = e.values\n"", 'df1 = df1.assign(e=e.values)\n', ""df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\n>>> df.assign(mean_a=df.a.mean(), mean_b=df.b.mean())\n   a  b  mean_a  mean_b\n0  1  3     1.5     3.5\n1  2  4     1.5     3.5\n"", ""np.random.seed(0)\ndf1 = pd.DataFrame(np.random.randn(10, 4), columns=['a', 'b', 'c', 'd'])\nmask = df1.applymap(lambda x: x <-0.7)\ndf1 = df1[-mask.any(axis=1)]\nsLength = len(df1['a'])\ne = pd.Series(np.random.randn(sLength))\n\n>>> df1\n          a         b         c         d\n0  1.764052  0.400157  0.978738  2.240893\n2 -0.103219  0.410599  0.144044  1.454274\n3  0.761038  0.121675  0.443863  0.333674\n7  1.532779  1.469359  0.154947  0.378163\n9  1.230291  1.202380 -0.387327 -0.302303\n\n>>> e\n0   -1.048553\n1   -1.420018\n2   -1.706270\n3    1.950775\n4   -0.509652\ndtype: float64\n\ndf1 = df1.assign(e=e.values)\n\n>>> df1\n          a         b         c         d         e\n0  1.764052  0.400157  0.978738  2.240893 -1.048553\n2 -0.103219  0.410599  0.144044  1.454274 -1.420018\n3  0.761038  0.121675  0.443863  0.333674 -1.706270\n7  1.532779  1.469359  0.154947  0.378163  1.950775\n9  1.230291  1.202380 -0.387327 -0.302303 -0.509652\n""]";"['e', 'df1', 'e', 'e', ""df['e'] = e.values\n"", 'assign', 'df1 = df1.assign(e=e.values)\n', 'assign', ""df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\n>>> df.assign(mean_a=df.a.mean(), mean_b=df.b.mean())\n   a  b  mean_a  mean_b\n0  1  3     1.5     3.5\n1  2  4     1.5     3.5\n"", ""np.random.seed(0)\ndf1 = pd.DataFrame(np.random.randn(10, 4), columns=['a', 'b', 'c', 'd'])\nmask = df1.applymap(lambda x: x <-0.7)\ndf1 = df1[-mask.any(axis=1)]\nsLength = len(df1['a'])\ne = pd.Series(np.random.randn(sLength))\n\n>>> df1\n          a         b         c         d\n0  1.764052  0.400157  0.978738  2.240893\n2 -0.103219  0.410599  0.144044  1.454274\n3  0.761038  0.121675  0.443863  0.333674\n7  1.532779  1.469359  0.154947  0.378163\n9  1.230291  1.202380 -0.387327 -0.302303\n\n>>> e\n0   -1.048553\n1   -1.420018\n2   -1.706270\n3    1.950775\n4   -0.509652\ndtype: float64\n\ndf1 = df1.assign(e=e.values)\n\n>>> df1\n          a         b         c         d         e\n0  1.764052  0.400157  0.978738  2.240893 -1.048553\n2 -0.103219  0.410599  0.144044  1.454274 -1.420018\n3  0.761038  0.121675  0.443863  0.333674 -1.706270\n7  1.532779  1.469359  0.154947  0.378163  1.950775\n9  1.230291  1.202380 -0.387327 -0.302303 -0.509652\n""]"
1446;;3;35446404;31357611.0;2;29;;;"<p><a href=""https://stackoverflow.com/users/5014134/jianxun-li"">Jianxun</a>'s solution did the job for me but broke the y value indicator at the bottom left of the window. </p>

<p>I ended up using <code>FuncFormatter</code>instead (and also stripped the uneccessary trailing zeroes as suggested <a href=""https://stackoverflow.com/questions/14997799/most-pythonic-way-to-print-at-most-some-number-of-decimal-places"">here</a>):</p>

<pre><code>import pandas as pd
import numpy as np
from matplotlib.ticker import FuncFormatter

df = pd.DataFrame(np.random.randn(100,5))

ax = df.plot()
ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y))) 
</code></pre>

<p>Generally speaking I'd recommend using <code>FuncFormatter</code> for label formatting: it's reliable, and versatile. </p>

<p><a href=""https://i.stack.imgur.com/uKf1z.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/uKf1z.png"" alt=""enter image description here""></a></p>
";;"[""import pandas as pd\nimport numpy as np\nfrom matplotlib.ticker import FuncFormatter\n\ndf = pd.DataFrame(np.random.randn(100,5))\n\nax = df.plot()\nax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y))) \n""]";"['FuncFormatter', ""import pandas as pd\nimport numpy as np\nfrom matplotlib.ticker import FuncFormatter\n\ndf = pd.DataFrame(np.random.randn(100,5))\n\nax = df.plot()\nax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y))) \n"", 'FuncFormatter']"
1447;;2;35476539;17709641.0;2;24;;;"<p>I found it to be a simple version being outdated or mismatch and was fixed with: </p>

<pre><code>pip install --upgrade numpy
pip install --upgrade scipy
pip install --upgrade pandas
</code></pre>

<p>Or might work with the one liner:</p>

<pre><code>pip install --upgrade numpy scipy pandas
</code></pre>
";;['pip install --upgrade numpy\npip install --upgrade scipy\npip install --upgrade pandas\n', 'pip install --upgrade numpy scipy pandas\n'];['pip install --upgrade numpy\npip install --upgrade scipy\npip install --upgrade pandas\n', 'pip install --upgrade numpy scipy pandas\n']
1448;;0;35509134;26309962.0;2;17;;;"<p>Sometimes it's easier to do all the appending outside of pandas, then, just create the DataFrame in one shot.</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; list=[['a','b']]
&gt;&gt;&gt; list.append(['e','f'])
&gt;&gt;&gt; df=pd.DataFrame(list,columns=['col1','col2'])
   col1 col2
0    a    b
1    e    f
</code></pre>
";;"["">>> import pandas as pd\n>>> list=[['a','b']]\n>>> list.append(['e','f'])\n>>> df=pd.DataFrame(list,columns=['col1','col2'])\n   col1 col2\n0    a    b\n1    e    f\n""]";"["">>> import pandas as pd\n>>> list=[['a','b']]\n>>> list.append(['e','f'])\n>>> df=pd.DataFrame(list,columns=['col1','col2'])\n   col1 col2\n0    a    b\n1    e    f\n""]"
1449;;0;35523946;15943769.0;2;50;;;"<p>suppose df is your dataframe then:</p>

<pre><code>Count_Row=df.shape[0] #gives number of row count
Count_Col=df.shape[1] #gives number of col count
</code></pre>
";;['Count_Row=df.shape[0] #gives number of row count\nCount_Col=df.shape[1] #gives number of col count\n'];['Count_Row=df.shape[0] #gives number of row count\nCount_Col=df.shape[1] #gives number of col count\n']
1450;;3;35531218;24147278.0;2;87;;;"<p>Pandas random sample will also work </p>

<pre><code>train=df.sample(frac=0.8,random_state=200)
test=df.drop(train.index)
</code></pre>
";;['train=df.sample(frac=0.8,random_state=200)\ntest=df.drop(train.index)\n'];['train=df.sample(frac=0.8,random_state=200)\ntest=df.drop(train.index)\n']
1451;;2;35583219;10715965.0;2;17;;;"<p>You can append a single row as a dictionary using the <code>ignore_index</code> option.</p>

<pre><code>&gt;&gt;&gt; f = pandas.DataFrame(data = {'Animal':['cow','horse'], 'Color':['blue', 'red']})
&gt;&gt;&gt; f
  Animal Color
0    cow  blue
1  horse   red
&gt;&gt;&gt; f.append({'Animal':'mouse', 'Color':'black'}, ignore_index=True)
  Animal  Color
0    cow   blue
1  horse    red
2  mouse  black
</code></pre>
";;"["">>> f = pandas.DataFrame(data = {'Animal':['cow','horse'], 'Color':['blue', 'red']})\n>>> f\n  Animal Color\n0    cow  blue\n1  horse   red\n>>> f.append({'Animal':'mouse', 'Color':'black'}, ignore_index=True)\n  Animal  Color\n0    cow   blue\n1  horse    red\n2  mouse  black\n""]";"['ignore_index', "">>> f = pandas.DataFrame(data = {'Animal':['cow','horse'], 'Color':['blue', 'red']})\n>>> f\n  Animal Color\n0    cow  blue\n1  horse   red\n>>> f.append({'Animal':'mouse', 'Color':'black'}, ignore_index=True)\n  Animal  Color\n0    cow   blue\n1  horse    red\n2  mouse  black\n""]"
1452;;0;35616082;18062135.0;2;9;;;"<p>Why don't you just use .to_frame if both have the same indexes?</p>

<pre><code>a.to_frame().join(b.to_frame())
</code></pre>
";;['a.to_frame().join(b.to_frame())\n'];['a.to_frame().join(b.to_frame())\n']
1453;;0;35768306;13636592.0;2;18;;;"<p>Since pandas 0.17.0, <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort.html""><code>sort</code></a> is deprecated and replaced by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html""><code>sort_values</code></a>:</p>

<pre><code>df.sort_values(['Peak', 'Weeks'], ascending=[True, False], inplace=True)
</code></pre>
";;"[""df.sort_values(['Peak', 'Weeks'], ascending=[True, False], inplace=True)\n""]";"['sort', 'sort_values', ""df.sort_values(['Peak', 'Weeks'], ascending=[True, False], inplace=True)\n""]"
1454;;0;35783766;17477979.0;2;8;;;"<p>Here is another method using <code>.loc</code> to replace inf with nan on a Series:</p>

<pre><code>s.loc[(~np.isfinite(s)) &amp; s.notnull()] = np.nan
</code></pre>

<p>So, in response to the original question:</p>

<pre><code>df = pd.DataFrame(np.ones((3, 3)), columns=list('ABC'))

for i in range(3): 
    df.iat[i, i] = np.inf

df
          A         B         C
0       inf  1.000000  1.000000
1  1.000000       inf  1.000000
2  1.000000  1.000000       inf

df.sum()
A    inf
B    inf
C    inf
dtype: float64

df.apply(lambda s: s[np.isfinite(s)].dropna()).sum()
A    2
B    2
C    2
dtype: float64
</code></pre>
";;"['s.loc[(~np.isfinite(s)) & s.notnull()] = np.nan\n', ""df = pd.DataFrame(np.ones((3, 3)), columns=list('ABC'))\n\nfor i in range(3): \n    df.iat[i, i] = np.inf\n\ndf\n          A         B         C\n0       inf  1.000000  1.000000\n1  1.000000       inf  1.000000\n2  1.000000  1.000000       inf\n\ndf.sum()\nA    inf\nB    inf\nC    inf\ndtype: float64\n\ndf.apply(lambda s: s[np.isfinite(s)].dropna()).sum()\nA    2\nB    2\nC    2\ndtype: float64\n""]";"['.loc', 's.loc[(~np.isfinite(s)) & s.notnull()] = np.nan\n', ""df = pd.DataFrame(np.ones((3, 3)), columns=list('ABC'))\n\nfor i in range(3): \n    df.iat[i, i] = np.inf\n\ndf\n          A         B         C\n0       inf  1.000000  1.000000\n1  1.000000       inf  1.000000\n2  1.000000  1.000000       inf\n\ndf.sum()\nA    inf\nB    inf\nC    inf\ndtype: float64\n\ndf.apply(lambda s: s[np.isfinite(s)].dropna()).sum()\nA    2\nB    2\nC    2\ndtype: float64\n""]"
1455;;0;35784666;15772009.0;2;57;;;"<p>Sampling randomizes, so just sample the entire data frame.</p>

<pre><code>df.sample(frac=1)
</code></pre>
";;['df.sample(frac=1)\n'];['df.sample(frac=1)\n']
1456;;2;35850749;19377969.0;2;47;;;"<p>The method <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.cat.html#pandas.Series.str.cat""><code>cat()</code> of the <code>.str</code> accessor</a> works really well for this:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame([[""2014"", ""q1""], 
...                    [""2015"", ""q3""]],
...                   columns=('Year', 'Quarter'))
&gt;&gt;&gt; print(df)
   Year Quarter
0  2014      q1
1  2015      q3
&gt;&gt;&gt; df['Period'] = df.Year.str.cat(df.Quarter)
&gt;&gt;&gt; print(df)
   Year Quarter  Period
0  2014      q1  2014q1
1  2015      q3  2015q3
</code></pre>

<p><code>cat()</code> even allows you to add a separator so, for example, suppose you only have integers for year and period, you can do this:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame([[2014, 1],
...                    [2015, 3]],
...                   columns=('Year', 'Quarter'))
&gt;&gt;&gt; print(df)
   Year Quarter
0  2014       1
1  2015       3
&gt;&gt;&gt; df['Period'] = df.Year.astype(str).str.cat(df.Quarter.astype(str), sep='q')
&gt;&gt;&gt; print(df)
   Year Quarter  Period
0  2014       1  2014q1
1  2015       3  2015q3
</code></pre>
";;"['>>> import pandas as pd\n>>> df = pd.DataFrame([[""2014"", ""q1""], \n...                    [""2015"", ""q3""]],\n...                   columns=(\'Year\', \'Quarter\'))\n>>> print(df)\n   Year Quarter\n0  2014      q1\n1  2015      q3\n>>> df[\'Period\'] = df.Year.str.cat(df.Quarter)\n>>> print(df)\n   Year Quarter  Period\n0  2014      q1  2014q1\n1  2015      q3  2015q3\n', "">>> import pandas as pd\n>>> df = pd.DataFrame([[2014, 1],\n...                    [2015, 3]],\n...                   columns=('Year', 'Quarter'))\n>>> print(df)\n   Year Quarter\n0  2014       1\n1  2015       3\n>>> df['Period'] = df.Year.astype(str).str.cat(df.Quarter.astype(str), sep='q')\n>>> print(df)\n   Year Quarter  Period\n0  2014       1  2014q1\n1  2015       3  2015q3\n""]";"['cat()', '.str', '>>> import pandas as pd\n>>> df = pd.DataFrame([[""2014"", ""q1""], \n...                    [""2015"", ""q3""]],\n...                   columns=(\'Year\', \'Quarter\'))\n>>> print(df)\n   Year Quarter\n0  2014      q1\n1  2015      q3\n>>> df[\'Period\'] = df.Year.str.cat(df.Quarter)\n>>> print(df)\n   Year Quarter  Period\n0  2014      q1  2014q1\n1  2015      q3  2015q3\n', 'cat()', "">>> import pandas as pd\n>>> df = pd.DataFrame([[2014, 1],\n...                    [2015, 3]],\n...                   columns=('Year', 'Quarter'))\n>>> print(df)\n   Year Quarter\n0  2014       1\n1  2015       3\n>>> df['Period'] = df.Year.astype(str).str.cat(df.Quarter.astype(str), sep='q')\n>>> print(df)\n   Year Quarter  Period\n0  2014       1  2014q1\n1  2015       3  2015q3\n""]"
1457;;0;35902487;19213789.0;2;11;;;"<p>If you have a time-axis, and you have Pandas imported as pd, you can use:</p>

<pre><code>ax.axvline(pd.to_datetime('2015-11-01'), color='r', linestyle='--', lw=2)
</code></pre>

<p>For multiple lines:</p>

<pre><code>xposition = [pd.to_datetime('2010-01-01'), pd.to_datetime('2015-12-31')]
for xc in xposition:
    ax.axvline(x=xc, color='k', linestyle='-')
</code></pre>
";;"[""ax.axvline(pd.to_datetime('2015-11-01'), color='r', linestyle='--', lw=2)\n"", ""xposition = [pd.to_datetime('2010-01-01'), pd.to_datetime('2015-12-31')]\nfor xc in xposition:\n    ax.axvline(x=xc, color='k', linestyle='-')\n""]";"[""ax.axvline(pd.to_datetime('2015-11-01'), color='r', linestyle='--', lw=2)\n"", ""xposition = [pd.to_datetime('2010-01-01'), pd.to_datetime('2015-12-31')]\nfor xc in xposition:\n    ax.axvline(x=xc, color='k', linestyle='-')\n""]"
1458;;3;35970794;17839973.0;2;9;;;"<p>You can also use <code>pd.DataFrame.from_records</code> which is more convenient when you already have the dictionary in hand:</p>

<pre><code>df = pd.DataFrame.from_records([{ 'A':a,'B':b }])
</code></pre>

<p>You can also set index, if you want, by:</p>

<pre><code>df = pd.DataFrame.from_records([{ 'A':a,'B':b }], index='A')
</code></pre>
";;"[""df = pd.DataFrame.from_records([{ 'A':a,'B':b }])\n"", ""df = pd.DataFrame.from_records([{ 'A':a,'B':b }], index='A')\n""]";"['pd.DataFrame.from_records', ""df = pd.DataFrame.from_records([{ 'A':a,'B':b }])\n"", ""df = pd.DataFrame.from_records([{ 'A':a,'B':b }], index='A')\n""]"
1459;;0;36012306;20612645.0;2;7;;;"<p>open the console and type the code below.</p>

<p>Code:</p>

<pre><code>import pandas as pd
pd.__version__
</code></pre>

<blockquote>
  <p>**Its double underscore before and after the word ""version"".</p>
</blockquote>

<p>Output:</p>

<pre><code>'0.14.1'
</code></pre>
";;"['import pandas as pd\npd.__version__\n', ""'0.14.1'\n""]";"['import pandas as pd\npd.__version__\n', ""'0.14.1'\n""]"
1460;;0;36013757;18022845.0;2;21;;;"<p>From version <code>0.18.0</code> you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename_axis.html""><code>rename_axis</code></a>:</p>

<pre><code>print df
             Column 1
Index Title          
Apples            1.0
Oranges           2.0
Puppies           3.0
Ducks             4.0
</code></pre>

<p>The new functionality works well in method chains.</p>

<pre><code>print df.rename_axis('foo')
         Column 1
foo              
Apples        1.0
Oranges       2.0
Puppies       3.0
Ducks         4.0
</code></pre>

<p>You can also rename column names with parameter <code>axis</code>:</p>

<pre><code>print df
Col Name     Column 1
Index Title          
Apples            1.0
Oranges           2.0
Puppies           3.0
Ducks             4.0
</code></pre>



<pre><code>print df.rename_axis('foo').rename_axis(""bar"", axis=""columns"")
bar      Column 1
foo              
Apples        1.0
Oranges       2.0
Puppies       3.0
Ducks         4.0

print df.rename_axis('foo').rename_axis(""bar"", axis=1)
bar      Column 1
foo              
Apples        1.0
Oranges       2.0
Puppies       3.0
Ducks         4.0
</code></pre>
";;"['print df\n             Column 1\nIndex Title          \nApples            1.0\nOranges           2.0\nPuppies           3.0\nDucks             4.0\n', ""print df.rename_axis('foo')\n         Column 1\nfoo              \nApples        1.0\nOranges       2.0\nPuppies       3.0\nDucks         4.0\n"", 'print df\nCol Name     Column 1\nIndex Title          \nApples            1.0\nOranges           2.0\nPuppies           3.0\nDucks             4.0\n', 'print df.rename_axis(\'foo\').rename_axis(""bar"", axis=""columns"")\nbar      Column 1\nfoo              \nApples        1.0\nOranges       2.0\nPuppies       3.0\nDucks         4.0\n\nprint df.rename_axis(\'foo\').rename_axis(""bar"", axis=1)\nbar      Column 1\nfoo              \nApples        1.0\nOranges       2.0\nPuppies       3.0\nDucks         4.0\n']";"['0.18.0', 'rename_axis', 'print df\n             Column 1\nIndex Title          \nApples            1.0\nOranges           2.0\nPuppies           3.0\nDucks             4.0\n', ""print df.rename_axis('foo')\n         Column 1\nfoo              \nApples        1.0\nOranges       2.0\nPuppies       3.0\nDucks         4.0\n"", 'axis', 'print df\nCol Name     Column 1\nIndex Title          \nApples            1.0\nOranges           2.0\nPuppies           3.0\nDucks             4.0\n', 'print df.rename_axis(\'foo\').rename_axis(""bar"", axis=""columns"")\nbar      Column 1\nfoo              \nApples        1.0\nOranges       2.0\nPuppies       3.0\nDucks         4.0\n\nprint df.rename_axis(\'foo\').rename_axis(""bar"", axis=1)\nbar      Column 1\nfoo              \nApples        1.0\nOranges       2.0\nPuppies       3.0\nDucks         4.0\n']"
1461;;0;36041831;19377969.0;2;12;;;"<p>Use of a lamba function this time with string.format().  </p>

<pre><code>import pandas as pd
df = pd.DataFrame({'Year': ['2014', '2015'], 'Quarter': ['q1', 'q2']})
print df
df['YearQuarter'] = df[['Year','Quarter']].apply(lambda x : '{}{}'.format(x[0],x[1]), axis=1)
print df

  Quarter  Year
0      q1  2014
1      q2  2015
  Quarter  Year YearQuarter
0      q1  2014      2014q1
1      q2  2015      2015q2
</code></pre>

<p>This allows you to work with non-strings and reformat values as needed.</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'Year': ['2014', '2015'], 'Quarter': [1, 2]})
print df.dtypes
print df

df['YearQuarter'] = df[['Year','Quarter']].apply(lambda x : '{}q{}'.format(x[0],x[1]), axis=1)
print df

Quarter     int64
Year       object
dtype: object
   Quarter  Year
0        1  2014
1        2  2015
   Quarter  Year YearQuarter
0        1  2014      2014q1
1        2  2015      2015q2
</code></pre>
";;"[""import pandas as pd\ndf = pd.DataFrame({'Year': ['2014', '2015'], 'Quarter': ['q1', 'q2']})\nprint df\ndf['YearQuarter'] = df[['Year','Quarter']].apply(lambda x : '{}{}'.format(x[0],x[1]), axis=1)\nprint df\n\n  Quarter  Year\n0      q1  2014\n1      q2  2015\n  Quarter  Year YearQuarter\n0      q1  2014      2014q1\n1      q2  2015      2015q2\n"", ""import pandas as pd\ndf = pd.DataFrame({'Year': ['2014', '2015'], 'Quarter': [1, 2]})\nprint df.dtypes\nprint df\n\ndf['YearQuarter'] = df[['Year','Quarter']].apply(lambda x : '{}q{}'.format(x[0],x[1]), axis=1)\nprint df\n\nQuarter     int64\nYear       object\ndtype: object\n   Quarter  Year\n0        1  2014\n1        2  2015\n   Quarter  Year YearQuarter\n0        1  2014      2014q1\n1        2  2015      2015q2\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'Year': ['2014', '2015'], 'Quarter': ['q1', 'q2']})\nprint df\ndf['YearQuarter'] = df[['Year','Quarter']].apply(lambda x : '{}{}'.format(x[0],x[1]), axis=1)\nprint df\n\n  Quarter  Year\n0      q1  2014\n1      q2  2015\n  Quarter  Year YearQuarter\n0      q1  2014      2014q1\n1      q2  2015      2015q2\n"", ""import pandas as pd\ndf = pd.DataFrame({'Year': ['2014', '2015'], 'Quarter': [1, 2]})\nprint df.dtypes\nprint df\n\ndf['YearQuarter'] = df[['Year','Quarter']].apply(lambda x : '{}q{}'.format(x[0],x[1]), axis=1)\nprint df\n\nQuarter     int64\nYear       object\ndtype: object\n   Quarter  Year\n0        1  2014\n1        2  2015\n   Quarter  Year YearQuarter\n0        1  2014      2014q1\n1        2  2015      2015q2\n""]"
1462;;1;36059898;21733893.0;2;8;;;"<p>You can also do a nested np.where()</p>

<pre><code>df['Age_group'] = np.where(df.Age&lt;18, 'under 18',
                           np.where(df.Age&lt;40,'under 40', '&gt;40'))
</code></pre>
";;"[""df['Age_group'] = np.where(df.Age<18, 'under 18',\n                           np.where(df.Age<40,'under 40', '>40'))\n""]";"[""df['Age_group'] = np.where(df.Age<18, 'under 18',\n                           np.where(df.Age<40,'under 40', '>40'))\n""]"
1463;;2;36073837;20067636.0;2;13;;;"<p>This will give you the second row of each group (zero indexed, nth(0) is the same as first()):</p>

<pre><code>df.groupby('id').nth(1) 
</code></pre>

<p>Documentation: <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#taking-the-nth-row-of-each-group"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/groupby.html#taking-the-nth-row-of-each-group</a></p>
";;"[""df.groupby('id').nth(1) \n""]";"[""df.groupby('id').nth(1) \n""]"
1464;;2;36074520;27842613.0;2;24;;;"<p>You could also just do it in one go, by doing the sort first and using head to take the first 3 of each group. </p>

<pre><code>In[34]: df.sort_values(['job','count'],ascending=False).groupby('job').head(3)

Out[35]: 
   count     job source
4      7   sales      E
2      6   sales      C
1      4   sales      B
5      5  market      A
8      4  market      D
6      3  market      B
</code></pre>
";;"[""In[34]: df.sort_values(['job','count'],ascending=False).groupby('job').head(3)\n\nOut[35]: \n   count     job source\n4      7   sales      E\n2      6   sales      C\n1      4   sales      B\n5      5  market      A\n8      4  market      D\n6      3  market      B\n""]";"[""In[34]: df.sort_values(['job','count'],ascending=False).groupby('job').head(3)\n\nOut[35]: \n   count     job source\n4      7   sales      E\n2      6   sales      C\n1      4   sales      B\n5      5  market      A\n8      4  market      D\n6      3  market      B\n""]"
1465;;0;36082588;21606987.0;2;13;;;"<p>You can now just call <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.strip.html#pandas.Series.str.strip""><code>.str.strip</code></a> on the columns if you're using a recent version:</p>

<pre><code>In [5]:
df = pd.DataFrame(columns=['Year', 'Month ', 'Value'])
print(df.columns.tolist())
df.columns = df.columns.str.strip()
df.columns.tolist()

['Year', 'Month ', 'Value']
Out[5]:
['Year', 'Month', 'Value']
</code></pre>
";;"[""In [5]:\ndf = pd.DataFrame(columns=['Year', 'Month ', 'Value'])\nprint(df.columns.tolist())\ndf.columns = df.columns.str.strip()\ndf.columns.tolist()\n\n['Year', 'Month ', 'Value']\nOut[5]:\n['Year', 'Month', 'Value']\n""]";"['.str.strip', ""In [5]:\ndf = pd.DataFrame(columns=['Year', 'Month ', 'Value'])\nprint(df.columns.tolist())\ndf.columns = df.columns.str.strip()\ndf.columns.tolist()\n\n['Year', 'Month ', 'Value']\nOut[5]:\n['Year', 'Month', 'Value']\n""]"
1466;;0;36149967;11346283.0;2;41;;;"<pre><code>df.columns = ['a', 'b', 'c', 'd', 'e']
</code></pre>

<p>It will replace the existing names with the names you provide, in the order you provide.</p>

<p>You can also assign them by index like this:</p>

<pre><code>df.columns.values[2] = 'c'    #renames the 2nd column to 'c'
</code></pre>
";;"[""df.columns = ['a', 'b', 'c', 'd', 'e']\n"", ""df.columns.values[2] = 'c'    #renames the 2nd column to 'c'\n""]";"[""df.columns = ['a', 'b', 'c', 'd', 'e']\n"", ""df.columns.values[2] = 'c'    #renames the 2nd column to 'c'\n""]"
1467;;0;36184396;21197774.0;2;35;;;"<p>For those coming from Google (etc.) such as myself:</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.convert_objects.html"" rel=""noreferrer""><code>convert_objects</code></a> has been deprecated - if you use it, you get a warning like this one: </p>

<pre><code>FutureWarning: convert_objects is deprecated.  Use the data-type specific converters 
pd.to_datetime, pd.to_timedelta and pd.to_numeric.
</code></pre>

<p>You should do something like the following:</p>

<ul>
<li><code>df =</code><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.astype.html"" rel=""noreferrer""><code>df.astype(np.float)</code></a> </li>
<li><code>df[""A""] =</code><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html"" rel=""noreferrer""><code>pd.to_numeric(df[""A""])</code></a></li>
</ul>
";;['FutureWarning: convert_objects is deprecated.  Use the data-type specific converters \npd.to_datetime, pd.to_timedelta and pd.to_numeric.\n'];"['convert_objects', 'FutureWarning: convert_objects is deprecated.  Use the data-type specific converters \npd.to_datetime, pd.to_timedelta and pd.to_numeric.\n', 'df =', 'df.astype(np.float)', 'df[""A""] =', 'pd.to_numeric(df[""A""])']"
1468;;2;36188131;14262433.0;2;32;;;"<p>There is now, two years after the question, an 'out-of-core' pandas equivalent: <a href=""http://dask.pydata.org/en/latest/"" rel=""noreferrer"">dask</a>. It is excellent! Though it does not support all of pandas functionality, you can get really far with it.</p>
";;[];[]
1469;;0;36236885;23307301.0;2;12;;;"<pre><code>w.female.replace(to_replace=dict(female=1, male=0), inplace=True)
</code></pre>

<p>See <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html"" rel=""noreferrer"">pandas.DataFrame.replace() docs</a>.</p>
";;['w.female.replace(to_replace=dict(female=1, male=0), inplace=True)\n'];['w.female.replace(to_replace=dict(female=1, male=0), inplace=True)\n']
1470;;0;36257640;29525808.0;2;8;;;"<p>Just to make this more clear for novice pandas programmers, here is a concrete example,</p>

<pre><code>pd.read_sql(session.query(Complaint).filter(Complaint.id == 2).statement,session.bind) 
</code></pre>

<p>Here we select a complaint from complaints table (sqlalchemy model is Complaint) with id = 2</p>
";;['pd.read_sql(session.query(Complaint).filter(Complaint.id == 2).statement,session.bind) \n'];['pd.read_sql(session.query(Complaint).filter(Complaint.id == 2).statement,session.bind) \n']
1471;;4;36319915;31357611.0;2;12;;;"<p>This is a few months late, but I have created <a href=""https://github.com/matplotlib/matplotlib/pull/6251"">PR#6251</a> with matplotlib to add a new <code>PercentFormatter</code> class. With this class you just need one line to reformat your axis (two if you count the import of <code>matplotlib.ticker</code>):</p>

<pre><code>import ...
import matplotlib.ticker as mtick

ax = df['myvar'].plot(kind='bar')
ax.yaxis.set_major_formatter(mtick.PercentFormatter())
</code></pre>

<p><code>PercentFormatter()</code> accepts three arguments, <code>max</code>, <code>decimals</code>, <code>symbol</code>. <code>max</code> allows you to set the value that corresponds to 100% on the axis. This is nice if you have data from 0.0 to 1.0 and you want to display it from 0% to 100%. Just do <code>PercentFormatter(1.0)</code>.</p>

<p>The other two parameters allow you to set the number of digits after the decimal point and the symbol. They default to <code>None</code> and <code>'%'</code>, respectively. <code>decimals=None</code> will automatically set the number of decimal points based on how much of the axes you are showing.</p>
";;"[""import ...\nimport matplotlib.ticker as mtick\n\nax = df['myvar'].plot(kind='bar')\nax.yaxis.set_major_formatter(mtick.PercentFormatter())\n""]";"['PercentFormatter', 'matplotlib.ticker', ""import ...\nimport matplotlib.ticker as mtick\n\nax = df['myvar'].plot(kind='bar')\nax.yaxis.set_major_formatter(mtick.PercentFormatter())\n"", 'PercentFormatter()', 'max', 'decimals', 'symbol', 'max', 'PercentFormatter(1.0)', 'None', ""'%'"", 'decimals=None']"
1472;;0;36372667;28218698.0;2;18;;;"<p>You can use <code>iteritems()</code>:</p>

<pre><code>for name, values in df.iteritems():
    print '{name}: {value}'.format(name=name, value=values[0])
</code></pre>
";;"[""for name, values in df.iteritems():\n    print '{name}: {value}'.format(name=name, value=values[0])\n""]";"['iteritems()', ""for name, values in df.iteritems():\n    print '{name}: {value}'.format(name=name, value=values[0])\n""]"
1473;;0;36373866;11707586.0;2;6;;;"<p>According to the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.html"" rel=""noreferrer"">docs for v0.18.0</a>, if you're running on a terminal (ie not iPython notebook, qtconsole or IDLE), it's a 2-liner to have Pandas auto-detect your screen width and adapt on the fly with how many columns it shows:</p>

<pre><code>pd.set_option('display.large_repr', 'truncate')
pd.set_option('display.max_columns', 0)
</code></pre>
";;"[""pd.set_option('display.large_repr', 'truncate')\npd.set_option('display.max_columns', 0)\n""]";"[""pd.set_option('display.large_repr', 'truncate')\npd.set_option('display.max_columns', 0)\n""]"
1474;;8;36416258;20906474.0;2;97;;;"<p>An alternative to <a href=""https://stackoverflow.com/a/21232849/3888455"">darindaCoder's answer</a>:</p>

<pre><code>path = r'C:\DRO\DCL_rawdata_files'                     # use your path
all_files = glob.glob(os.path.join(path, ""*.csv""))     # advisable to use os.path.join as this makes concatenation OS independent

df_from_each_file = (pd.read_csv(f) for f in all_files)
concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)
# doesn't create a list, nor does it append to one
</code></pre>
";;"['path = r\'C:\\DRO\\DCL_rawdata_files\'                     # use your path\nall_files = glob.glob(os.path.join(path, ""*.csv""))     # advisable to use os.path.join as this makes concatenation OS independent\n\ndf_from_each_file = (pd.read_csv(f) for f in all_files)\nconcatenated_df   = pd.concat(df_from_each_file, ignore_index=True)\n# doesn\'t create a list, nor does it append to one\n']";"['path = r\'C:\\DRO\\DCL_rawdata_files\'                     # use your path\nall_files = glob.glob(os.path.join(path, ""*.csv""))     # advisable to use os.path.join as this makes concatenation OS independent\n\ndf_from_each_file = (pd.read_csv(f) for f in all_files)\nconcatenated_df   = pd.concat(df_from_each_file, ignore_index=True)\n# doesn\'t create a list, nor does it append to one\n']"
1475;;3;36434248;22391433.0;2;27;;;"<p>If you want to apply to all columns you can use:</p>

<pre><code>df.apply(pd.value_counts)
</code></pre>

<p>This will apply a column based aggregation function (in this case value_counts) to each of the columns.</p>
";;['df.apply(pd.value_counts)\n'];['df.apply(pd.value_counts)\n']
1476;;0;36475297;24645153.0;2;24;;;"<p>I am not sure if previous versions of <code>pandas</code> prevented this but now the following snippet works perfectly for me and produces exactly what you want without having to use <code>apply</code></p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler


&gt;&gt;&gt; scaler = MinMaxScaler()

&gt;&gt;&gt; dfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],
                           'B':[103.02,107.26,110.35,114.23,114.68],
                           'C':['big','small','big','small','small']})

&gt;&gt;&gt; dfTest[['A', 'B']] = scaler.fit_transform(dfTest[['A', 'B']])

&gt;&gt;&gt; dfTest
          A         B      C
0  0.000000  0.000000    big
1  0.926219  0.363636  small
2  0.935335  0.628645    big
3  1.000000  0.961407  small
4  0.938495  1.000000  small
</code></pre>
";;"["">>> import pandas as pd\n>>> from sklearn.preprocessing import MinMaxScaler\n\n\n>>> scaler = MinMaxScaler()\n\n>>> dfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],\n                           'B':[103.02,107.26,110.35,114.23,114.68],\n                           'C':['big','small','big','small','small']})\n\n>>> dfTest[['A', 'B']] = scaler.fit_transform(dfTest[['A', 'B']])\n\n>>> dfTest\n          A         B      C\n0  0.000000  0.000000    big\n1  0.926219  0.363636  small\n2  0.935335  0.628645    big\n3  1.000000  0.961407  small\n4  0.938495  1.000000  small\n""]";"['pandas', 'apply', "">>> import pandas as pd\n>>> from sklearn.preprocessing import MinMaxScaler\n\n\n>>> scaler = MinMaxScaler()\n\n>>> dfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],\n                           'B':[103.02,107.26,110.35,114.23,114.68],\n                           'C':['big','small','big','small','small']})\n\n>>> dfTest[['A', 'B']] = scaler.fit_transform(dfTest[['A', 'B']])\n\n>>> dfTest\n          A         B      C\n0  0.000000  0.000000    big\n1  0.926219  0.363636  small\n2  0.935335  0.628645    big\n3  1.000000  0.961407  small\n4  0.938495  1.000000  small\n""]"
1477;;0;36513262;14984119.0;2;9;;;"<p>If I'm not mistaken, the following does what was asked without the memory problems of the transpose solution and with fewer lines than @kalu 's function, keeping the first of any similarly named columns.</p>

<pre><code>Cols = list(df.columns)
for i,item in enumerate(df.columns):
    if item in df.columns[:i]: Cols[i] = ""toDROP""
df.columns = Cols
df = df.drop(""toDROP"",1)
</code></pre>
";;"['Cols = list(df.columns)\nfor i,item in enumerate(df.columns):\n    if item in df.columns[:i]: Cols[i] = ""toDROP""\ndf.columns = Cols\ndf = df.drop(""toDROP"",1)\n']";"['Cols = list(df.columns)\nfor i,item in enumerate(df.columns):\n    if item in df.columns[:i]: Cols[i] = ""toDROP""\ndf.columns = Cols\ndf = df.drop(""toDROP"",1)\n']"
1478;;1;36519122;36519086.0;2;34;;;"<p>It's the index column, pass <code>index=False</code> to not write it out, see the <a href=""http://pandas.pydata.org/pandas-docs/version/0.18.0/generated/pandas.DataFrame.to_csv.html"">docs</a></p>

<p>Example:</p>

<pre><code>In [37]:
df = pd.DataFrame(np.random.randn(5,3), columns=list('abc'))
pd.read_csv(io.StringIO(df.to_csv()))

Out[37]:
   Unnamed: 0         a         b         c
0           0  0.109066 -1.112704 -0.545209
1           1  0.447114  1.525341  0.317252
2           2  0.507495  0.137863  0.886283
3           3  1.452867  1.888363  1.168101
4           4  0.901371 -0.704805  0.088335
</code></pre>

<p>compare with:</p>

<pre><code>In [38]:
pd.read_csv(io.StringIO(df.to_csv(index=False)))

Out[38]:
          a         b         c
0  0.109066 -1.112704 -0.545209
1  0.447114  1.525341  0.317252
2  0.507495  0.137863  0.886283
3  1.452867  1.888363  1.168101
4  0.901371 -0.704805  0.088335
</code></pre>

<p>You could also optionally tell <code>read_csv</code> that the first column is the index column by passing <code>index_col=0</code>:</p>

<pre><code>In [40]:
pd.read_csv(io.StringIO(df.to_csv()), index_col=0)

Out[40]:
          a         b         c
0  0.109066 -1.112704 -0.545209
1  0.447114  1.525341  0.317252
2  0.507495  0.137863  0.886283
3  1.452867  1.888363  1.168101
4  0.901371 -0.704805  0.088335
</code></pre>
";;"[""In [37]:\ndf = pd.DataFrame(np.random.randn(5,3), columns=list('abc'))\npd.read_csv(io.StringIO(df.to_csv()))\n\nOut[37]:\n   Unnamed: 0         a         b         c\n0           0  0.109066 -1.112704 -0.545209\n1           1  0.447114  1.525341  0.317252\n2           2  0.507495  0.137863  0.886283\n3           3  1.452867  1.888363  1.168101\n4           4  0.901371 -0.704805  0.088335\n"", 'In [38]:\npd.read_csv(io.StringIO(df.to_csv(index=False)))\n\nOut[38]:\n          a         b         c\n0  0.109066 -1.112704 -0.545209\n1  0.447114  1.525341  0.317252\n2  0.507495  0.137863  0.886283\n3  1.452867  1.888363  1.168101\n4  0.901371 -0.704805  0.088335\n', 'In [40]:\npd.read_csv(io.StringIO(df.to_csv()), index_col=0)\n\nOut[40]:\n          a         b         c\n0  0.109066 -1.112704 -0.545209\n1  0.447114  1.525341  0.317252\n2  0.507495  0.137863  0.886283\n3  1.452867  1.888363  1.168101\n4  0.901371 -0.704805  0.088335\n']";"['index=False', ""In [37]:\ndf = pd.DataFrame(np.random.randn(5,3), columns=list('abc'))\npd.read_csv(io.StringIO(df.to_csv()))\n\nOut[37]:\n   Unnamed: 0         a         b         c\n0           0  0.109066 -1.112704 -0.545209\n1           1  0.447114  1.525341  0.317252\n2           2  0.507495  0.137863  0.886283\n3           3  1.452867  1.888363  1.168101\n4           4  0.901371 -0.704805  0.088335\n"", 'In [38]:\npd.read_csv(io.StringIO(df.to_csv(index=False)))\n\nOut[38]:\n          a         b         c\n0  0.109066 -1.112704 -0.545209\n1  0.447114  1.525341  0.317252\n2  0.507495  0.137863  0.886283\n3  1.452867  1.888363  1.168101\n4  0.901371 -0.704805  0.088335\n', 'read_csv', 'index_col=0', 'In [40]:\npd.read_csv(io.StringIO(df.to_csv()), index_col=0)\n\nOut[40]:\n          a         b         c\n0  0.109066 -1.112704 -0.545209\n1  0.447114  1.525341  0.317252\n2  0.507495  0.137863  0.886283\n3  1.452867  1.888363  1.168101\n4  0.901371 -0.704805  0.088335\n']"
1479;;1;36572039;20107570.0;2;53;;;"<p>When reading to and from your csv file include the argument index=False so for example</p>

<pre><code> df.to_csv(filename ,  index = False)
</code></pre>

<p>and to read from the csv</p>

<pre><code>df.read_csv(filename ,  index = False)  
</code></pre>

<p>This should prevent the issue so you don't need to fix it later.</p>
";;[' df.to_csv(filename ,  index = False)\n', 'df.read_csv(filename ,  index = False)  \n'];[' df.to_csv(filename ,  index = False)\n', 'df.read_csv(filename ,  index = False)  \n']
1480;;1;36590692;17063458.0;2;34;;;"<p>This is much simple and easy way.</p>

<pre><code>import pandas
df = pandas.read_excel(open('your_xls_xlsx_filename','rb'), sheetname='Sheet 1')
# or using sheet index starting 0
df = pandas.read_excel(open('your_xls_xlsx_filename','rb'), sheetname=2)
</code></pre>

<p>check out documentation full details
<a href=""http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.read_excel.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.read_excel.html</a></p>
";;"[""import pandas\ndf = pandas.read_excel(open('your_xls_xlsx_filename','rb'), sheetname='Sheet 1')\n# or using sheet index starting 0\ndf = pandas.read_excel(open('your_xls_xlsx_filename','rb'), sheetname=2)\n""]";"[""import pandas\ndf = pandas.read_excel(open('your_xls_xlsx_filename','rb'), sheetname='Sheet 1')\n# or using sheet index starting 0\ndf = pandas.read_excel(open('your_xls_xlsx_filename','rb'), sheetname=2)\n""]"
1481;;0;36682678;22233488.0;2;15;;;"<p>Another way to do this is to reassign <code>df</code> based on a cross section of <code>df</code>, using the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.xs.html"">.xs</a> method.</p>

<pre><code>&gt;&gt;&gt; df

    a
    b   c
0   1   2
1   3   4

&gt;&gt;&gt; df = df.xs('a', axis=1, drop_level=True)

    # 'a' : key on which to get cross section
    # axis=1 : get cross section of column
    # drop_level=True : returns cross section without the multilevel index

&gt;&gt;&gt; df

    b   c
0   1   2
1   3   4
</code></pre>
";;"["">>> df\n\n    a\n    b   c\n0   1   2\n1   3   4\n\n>>> df = df.xs('a', axis=1, drop_level=True)\n\n    # 'a' : key on which to get cross section\n    # axis=1 : get cross section of column\n    # drop_level=True : returns cross section without the multilevel index\n\n>>> df\n\n    b   c\n0   1   2\n1   3   4\n""]";"['df', 'df', "">>> df\n\n    a\n    b   c\n0   1   2\n1   3   4\n\n>>> df = df.xs('a', axis=1, drop_level=True)\n\n    # 'a' : key on which to get cross section\n    # axis=1 : get cross section of column\n    # drop_level=True : returns cross section without the multilevel index\n\n>>> df\n\n    b   c\n0   1   2\n1   3   4\n""]"
1482;;0;36710126;26266362.0;2;10;;;"<p>if you are using Jupyter Notebook, How about....</p>

<pre><code> %%timeit
 df.isnull().any().any()
</code></pre>

<p>or</p>

<pre><code> %timeit 
 df.isnull().values.sum()
</code></pre>

<p>or, are there anywhere NaNs in the data, if yes, where?</p>

<pre><code> df.isnull().any()
</code></pre>
";;[' %%timeit\n df.isnull().any().any()\n', ' %timeit \n df.isnull().values.sum()\n', ' df.isnull().any()\n'];[' %%timeit\n df.isnull().any().any()\n', ' %timeit \n df.isnull().values.sum()\n', ' df.isnull().any()\n']
1483;;7;36911306;19377969.0;2;49;;;"<p>yet another ways to do this:</p>

<pre><code>df['period'] = df['Year'].astype(str) + df['quarter']
</code></pre>

<p>or bit slower:</p>

<pre><code>df['period'] = df[['Year','quarter']].astype(str).sum(axis=1)
</code></pre>

<p>Let's test it on 200K rows DF:</p>

<pre><code>In [250]: df
Out[250]:
   Year quarter
0  2014      q1
1  2015      q2

In [251]: df = pd.concat([df] * 10**5)

In [252]: df.shape
Out[252]: (200000, 2)
</code></pre>

<p><strong>UPDATE:</strong> new timings using Pandas 0.19.0</p>

<p><strong>Timing</strong> without CPU/GPU optimization (sorted from fastest to slowest):</p>

<pre><code>In [107]: %timeit df['Year'].astype(str) + df['quarter']
10 loops, best of 3: 131 ms per loop

In [106]: %timeit df['Year'].map(str) + df['quarter']
10 loops, best of 3: 161 ms per loop

In [108]: %timeit df.Year.str.cat(df.quarter)
10 loops, best of 3: 189 ms per loop

In [109]: %timeit df.loc[:, ['Year','quarter']].astype(str).sum(axis=1)
1 loop, best of 3: 567 ms per loop

In [110]: %timeit df[['Year','quarter']].astype(str).sum(axis=1)
1 loop, best of 3: 584 ms per loop

In [111]: %timeit df[['Year','quarter']].apply(lambda x : '{}{}'.format(x[0],x[1]), axis=1)
1 loop, best of 3: 24.7 s per loop
</code></pre>

<p><strong>Timing</strong> using CPU/GPU optimization:</p>

<pre><code>In [113]: %timeit df['Year'].astype(str) + df['quarter']
10 loops, best of 3: 53.3 ms per loop

In [114]: %timeit df['Year'].map(str) + df['quarter']
10 loops, best of 3: 65.5 ms per loop

In [115]: %timeit df.Year.str.cat(df.quarter)
10 loops, best of 3: 79.9 ms per loop

In [116]: %timeit df.loc[:, ['Year','quarter']].astype(str).sum(axis=1)
1 loop, best of 3: 230 ms per loop

In [117]: %timeit df[['Year','quarter']].astype(str).sum(axis=1)
1 loop, best of 3: 230 ms per loop

In [118]: %timeit df[['Year','quarter']].apply(lambda x : '{}{}'.format(x[0],x[1]), axis=1)
1 loop, best of 3: 9.38 s per loop
</code></pre>
";;"[""df['period'] = df['Year'].astype(str) + df['quarter']\n"", ""df['period'] = df[['Year','quarter']].astype(str).sum(axis=1)\n"", 'In [250]: df\nOut[250]:\n   Year quarter\n0  2014      q1\n1  2015      q2\n\nIn [251]: df = pd.concat([df] * 10**5)\n\nIn [252]: df.shape\nOut[252]: (200000, 2)\n', ""In [107]: %timeit df['Year'].astype(str) + df['quarter']\n10 loops, best of 3: 131 ms per loop\n\nIn [106]: %timeit df['Year'].map(str) + df['quarter']\n10 loops, best of 3: 161 ms per loop\n\nIn [108]: %timeit df.Year.str.cat(df.quarter)\n10 loops, best of 3: 189 ms per loop\n\nIn [109]: %timeit df.loc[:, ['Year','quarter']].astype(str).sum(axis=1)\n1 loop, best of 3: 567 ms per loop\n\nIn [110]: %timeit df[['Year','quarter']].astype(str).sum(axis=1)\n1 loop, best of 3: 584 ms per loop\n\nIn [111]: %timeit df[['Year','quarter']].apply(lambda x : '{}{}'.format(x[0],x[1]), axis=1)\n1 loop, best of 3: 24.7 s per loop\n"", ""In [113]: %timeit df['Year'].astype(str) + df['quarter']\n10 loops, best of 3: 53.3 ms per loop\n\nIn [114]: %timeit df['Year'].map(str) + df['quarter']\n10 loops, best of 3: 65.5 ms per loop\n\nIn [115]: %timeit df.Year.str.cat(df.quarter)\n10 loops, best of 3: 79.9 ms per loop\n\nIn [116]: %timeit df.loc[:, ['Year','quarter']].astype(str).sum(axis=1)\n1 loop, best of 3: 230 ms per loop\n\nIn [117]: %timeit df[['Year','quarter']].astype(str).sum(axis=1)\n1 loop, best of 3: 230 ms per loop\n\nIn [118]: %timeit df[['Year','quarter']].apply(lambda x : '{}{}'.format(x[0],x[1]), axis=1)\n1 loop, best of 3: 9.38 s per loop\n""]";"[""df['period'] = df['Year'].astype(str) + df['quarter']\n"", ""df['period'] = df[['Year','quarter']].astype(str).sum(axis=1)\n"", 'In [250]: df\nOut[250]:\n   Year quarter\n0  2014      q1\n1  2015      q2\n\nIn [251]: df = pd.concat([df] * 10**5)\n\nIn [252]: df.shape\nOut[252]: (200000, 2)\n', ""In [107]: %timeit df['Year'].astype(str) + df['quarter']\n10 loops, best of 3: 131 ms per loop\n\nIn [106]: %timeit df['Year'].map(str) + df['quarter']\n10 loops, best of 3: 161 ms per loop\n\nIn [108]: %timeit df.Year.str.cat(df.quarter)\n10 loops, best of 3: 189 ms per loop\n\nIn [109]: %timeit df.loc[:, ['Year','quarter']].astype(str).sum(axis=1)\n1 loop, best of 3: 567 ms per loop\n\nIn [110]: %timeit df[['Year','quarter']].astype(str).sum(axis=1)\n1 loop, best of 3: 584 ms per loop\n\nIn [111]: %timeit df[['Year','quarter']].apply(lambda x : '{}{}'.format(x[0],x[1]), axis=1)\n1 loop, best of 3: 24.7 s per loop\n"", ""In [113]: %timeit df['Year'].astype(str) + df['quarter']\n10 loops, best of 3: 53.3 ms per loop\n\nIn [114]: %timeit df['Year'].map(str) + df['quarter']\n10 loops, best of 3: 65.5 ms per loop\n\nIn [115]: %timeit df.Year.str.cat(df.quarter)\n10 loops, best of 3: 79.9 ms per loop\n\nIn [116]: %timeit df.loc[:, ['Year','quarter']].astype(str).sum(axis=1)\n1 loop, best of 3: 230 ms per loop\n\nIn [117]: %timeit df[['Year','quarter']].astype(str).sum(axis=1)\n1 loop, best of 3: 230 ms per loop\n\nIn [118]: %timeit df[['Year','quarter']].apply(lambda x : '{}{}'.format(x[0],x[1]), axis=1)\n1 loop, best of 3: 9.38 s per loop\n""]"
1484;;0;36922103;36921951.0;2;48;;;"<p>The <code>or</code> and <code>and</code> python statements require <code>truth</code>-values. For <code>pandas</code> these are considered ambiguous so you should use ""bitwise"" <code>|</code> (or) or <code>&amp;</code> (and) operations:</p>

<pre><code>result = result[(result['var']&gt;0.25) | (result['var']&lt;-0.25)]
</code></pre>

<p>These are overloaded for these kind of datastructures to yield the element-wise <code>or</code> (or <code>and</code>).</p>

<hr>

<p>Just to add some more explanation to this statement:</p>

<p>The exception is thrown when you want to get the <code>bool</code> of a <code>pandas.Series</code>:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; x = pd.Series([1])
&gt;&gt;&gt; bool(x)
ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
</code></pre>

<p>What you hit was a place where the operator <strong>implicitly</strong> converted the operands to <code>bool</code> (you used <code>or</code> but it also happens for <code>and</code>, <code>if</code> and <code>while</code>):</p>

<pre><code>&gt;&gt;&gt; x or x
ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
&gt;&gt;&gt; x and x
ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
&gt;&gt;&gt; if x:
...     print('fun')
ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
&gt;&gt;&gt; while x:
...     print('fun')
ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
</code></pre>

<p>Besides these 4 statements there are several python functions that hide some <code>bool</code> calls (like <code>any</code>, <code>all</code>, <code>filter</code>, ...) these are normally not problematic with <code>pandas.Series</code> but for completeness I wanted to mention these.</p>

<hr>

<p>In your case the exception isn't really helpful, because it doesn't mention the <strong>right alternatives</strong>. For <code>and</code> and <code>or</code> you can use (if you want element-wise comparisons):</p>

<ul>
<li><p><a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.logical_or.html""><code>numpy.logical_or</code></a>:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.logical_or(x, y)
</code></pre>

<p>or simply the <code>|</code> operator:</p>

<pre><code>&gt;&gt;&gt; x | y
</code></pre></li>
<li><p><a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.logical_and.html""><code>numpy.logical_and</code></a>:</p>

<pre><code>&gt;&gt;&gt; np.logical_and(x, y)
</code></pre>

<p>or simply the <code>&amp;</code> operator:</p>

<pre><code>&gt;&gt;&gt; x &amp; y
</code></pre></li>
</ul>

<p>If you're using the operators then make sure you set your parenthesis correctly because of <a href=""https://docs.python.org/reference/expressions.html#operator-precedence"">the operator precedence</a>.</p>

<p>There are <a href=""https://docs.scipy.org/doc/numpy/reference/routines.logic.html"">several logical numpy functions</a> which <em>should</em> work on <code>pandas.Series</code>.</p>

<hr>

<p>The alternatives mentioned in the Exception are more suited if you encountered it when doing <code>if</code> or <code>while</code>. I'll shortly explain each of these:</p>

<ul>
<li><p>If you want to check if your Series is <strong>empty</strong>:</p>

<pre><code>&gt;&gt;&gt; x = pd.Series([])
&gt;&gt;&gt; x.empty
True
&gt;&gt;&gt; x = pd.Series([1])
&gt;&gt;&gt; x.empty
False
</code></pre>

<p>Python normally interprets the <code>len</code>gth of containers (like <code>list</code>, <code>tuple</code>, ...) as truth-value if it has no explicit boolean interpretation. So if you want the python-like check, you could do: <code>if x.size</code> or <code>if not x.empty</code> instead of <code>if x</code>.</p></li>
<li><p>If your <code>Series</code> contains <strong>one and only one</strong> boolean value:</p>

<pre><code>&gt;&gt;&gt; x = pd.Series([100])
&gt;&gt;&gt; (x &gt; 50).bool()
True
&gt;&gt;&gt; (x &lt; 50).bool()
False
</code></pre></li>
<li><p>If you want to check the <strong>first and only item</strong> of your Series (like <code>.bool()</code> but works even for not boolean contents):</p>

<pre><code>&gt;&gt;&gt; x = pd.Series([100])
&gt;&gt;&gt; x.item()
100
</code></pre></li>
<li><p>If you want to check if <strong>all</strong> or <strong>any</strong> item is not-zero, not-empty or not-False:</p>

<pre><code>&gt;&gt;&gt; x = pd.Series([0, 1, 2])
&gt;&gt;&gt; x.all()   # because one element is zero
False
&gt;&gt;&gt; x.any()   # because one (or more) elements are non-zero
True
</code></pre></li>
</ul>
";;"[""result = result[(result['var']>0.25) | (result['var']<-0.25)]\n"", '>>> import pandas as pd\n>>> x = pd.Series([1])\n>>> bool(x)\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n', "">>> x or x\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n>>> x and x\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n>>> if x:\n...     print('fun')\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n>>> while x:\n...     print('fun')\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n"", '>>> import numpy as np\n>>> np.logical_or(x, y)\n', '>>> x | y\n', '>>> np.logical_and(x, y)\n', '>>> x & y\n', '>>> x = pd.Series([])\n>>> x.empty\nTrue\n>>> x = pd.Series([1])\n>>> x.empty\nFalse\n', '>>> x = pd.Series([100])\n>>> (x > 50).bool()\nTrue\n>>> (x < 50).bool()\nFalse\n', '>>> x = pd.Series([100])\n>>> x.item()\n100\n', '>>> x = pd.Series([0, 1, 2])\n>>> x.all()   # because one element is zero\nFalse\n>>> x.any()   # because one (or more) elements are non-zero\nTrue\n']";"['or', 'and', 'truth', 'pandas', '|', '&', ""result = result[(result['var']>0.25) | (result['var']<-0.25)]\n"", 'or', 'and', 'bool', 'pandas.Series', '>>> import pandas as pd\n>>> x = pd.Series([1])\n>>> bool(x)\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n', 'bool', 'or', 'and', 'if', 'while', "">>> x or x\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n>>> x and x\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n>>> if x:\n...     print('fun')\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n>>> while x:\n...     print('fun')\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n"", 'bool', 'any', 'all', 'filter', 'pandas.Series', 'and', 'or', 'numpy.logical_or', '>>> import numpy as np\n>>> np.logical_or(x, y)\n', '|', '>>> x | y\n', 'numpy.logical_and', '>>> np.logical_and(x, y)\n', '&', '>>> x & y\n', 'pandas.Series', 'if', 'while', '>>> x = pd.Series([])\n>>> x.empty\nTrue\n>>> x = pd.Series([1])\n>>> x.empty\nFalse\n', 'len', 'list', 'tuple', 'if x.size', 'if not x.empty', 'if x', 'Series', '>>> x = pd.Series([100])\n>>> (x > 50).bool()\nTrue\n>>> (x < 50).bool()\nFalse\n', '.bool()', '>>> x = pd.Series([100])\n>>> x.item()\n100\n', '>>> x = pd.Series([0, 1, 2])\n>>> x.all()   # because one element is zero\nFalse\n>>> x.any()   # because one (or more) elements are non-zero\nTrue\n']"
1485;;0;36922486;36921951.0;2;7;;;"<p>For boolean logic, use <code>&amp;</code> and <code>|</code>.</p>

<pre><code>np.random.seed(0)
df = pd.DataFrame(np.random.randn(5,3), columns=list('ABC'))

&gt;&gt;&gt; df
          A         B         C
0  1.764052  0.400157  0.978738
1  2.240893  1.867558 -0.977278
2  0.950088 -0.151357 -0.103219
3  0.410599  0.144044  1.454274
4  0.761038  0.121675  0.443863

&gt;&gt;&gt; df.loc[(df.C &gt; 0.25) | (df.C &lt; -0.25)]
          A         B         C
0  1.764052  0.400157  0.978738
1  2.240893  1.867558 -0.977278
3  0.410599  0.144044  1.454274
4  0.761038  0.121675  0.443863
</code></pre>

<p>To see what is happening, you get a column of booleans for each comparison, e.g.</p>

<pre><code>df.C &gt; 0.25
0     True
1    False
2    False
3     True
4     True
Name: C, dtype: bool
</code></pre>

<p>When you have multiple criteria, you will get multiple columns returned.  This is why the the join logic is ambiguous.  Using <code>and</code> or <code>or</code> treats each column separately, so you first need to reduce that column to a single boolean value.  For example, to see if any value or all values in each of the columns is True.</p>

<pre><code># Any value in either column is True?
(df.C &gt; 0.25).any() or (df.C &lt; -0.25).any()
True

# All values in either column is True?
(df.C &gt; 0.25).all() or (df.C &lt; -0.25).all()
False
</code></pre>

<p>One convoluted way to achieve the same thing is to zip all of these columns together, and perform the appropriate logic.</p>

<pre><code>&gt;&gt;&gt; df[[any([a, b]) for a, b in zip(df.C &gt; 0.25, df.C &lt; -0.25)]]
          A         B         C
0  1.764052  0.400157  0.978738
1  2.240893  1.867558 -0.977278
3  0.410599  0.144044  1.454274
4  0.761038  0.121675  0.443863
</code></pre>

<p>For more details, refer to <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"">Boolean Indexing</a> in the docs.</p>
";;"[""np.random.seed(0)\ndf = pd.DataFrame(np.random.randn(5,3), columns=list('ABC'))\n\n>>> df\n          A         B         C\n0  1.764052  0.400157  0.978738\n1  2.240893  1.867558 -0.977278\n2  0.950088 -0.151357 -0.103219\n3  0.410599  0.144044  1.454274\n4  0.761038  0.121675  0.443863\n\n>>> df.loc[(df.C > 0.25) | (df.C < -0.25)]\n          A         B         C\n0  1.764052  0.400157  0.978738\n1  2.240893  1.867558 -0.977278\n3  0.410599  0.144044  1.454274\n4  0.761038  0.121675  0.443863\n"", 'df.C > 0.25\n0     True\n1    False\n2    False\n3     True\n4     True\nName: C, dtype: bool\n', '# Any value in either column is True?\n(df.C > 0.25).any() or (df.C < -0.25).any()\nTrue\n\n# All values in either column is True?\n(df.C > 0.25).all() or (df.C < -0.25).all()\nFalse\n', '>>> df[[any([a, b]) for a, b in zip(df.C > 0.25, df.C < -0.25)]]\n          A         B         C\n0  1.764052  0.400157  0.978738\n1  2.240893  1.867558 -0.977278\n3  0.410599  0.144044  1.454274\n4  0.761038  0.121675  0.443863\n']";"['&', '|', ""np.random.seed(0)\ndf = pd.DataFrame(np.random.randn(5,3), columns=list('ABC'))\n\n>>> df\n          A         B         C\n0  1.764052  0.400157  0.978738\n1  2.240893  1.867558 -0.977278\n2  0.950088 -0.151357 -0.103219\n3  0.410599  0.144044  1.454274\n4  0.761038  0.121675  0.443863\n\n>>> df.loc[(df.C > 0.25) | (df.C < -0.25)]\n          A         B         C\n0  1.764052  0.400157  0.978738\n1  2.240893  1.867558 -0.977278\n3  0.410599  0.144044  1.454274\n4  0.761038  0.121675  0.443863\n"", 'df.C > 0.25\n0     True\n1    False\n2    False\n3     True\n4     True\nName: C, dtype: bool\n', 'and', 'or', '# Any value in either column is True?\n(df.C > 0.25).any() or (df.C < -0.25).any()\nTrue\n\n# All values in either column is True?\n(df.C > 0.25).all() or (df.C < -0.25).all()\nFalse\n', '>>> df[[any([a, b]) for a, b in zip(df.C > 0.25, df.C < -0.25)]]\n          A         B         C\n0  1.764052  0.400157  0.978738\n1  2.240893  1.867558 -0.977278\n3  0.410599  0.144044  1.454274\n4  0.761038  0.121675  0.443863\n']"
1486;;2;36951842;22691010.0;2;20;;;"<p>Simply do:</p>

<pre><code>grouped_df = df.groupby('A')

for key, item in grouped_df:
    print grouped_df.get_group(key), ""\n\n""
</code></pre>

<p>This also works,</p>

<pre><code>grouped_df = df.groupby('A')    
gb = grouped_df.groups

for key, values in gb.iteritems():
    print df.ix[values], ""\n\n""
</code></pre>

<p><strong>For selective key grouping:</strong> Insert the keys you want inside the key_list_from_gb, in following, using gb.keys(): For Example,</p>

<pre><code>gb = grouped_df.groups
gb.keys()

key_list_from_gb = [key1, key2, key3]

for key, values in gb.iteritems():
    if key in key_list_from_gb:
        print df.ix[values], ""\n""
</code></pre>
";;"['grouped_df = df.groupby(\'A\')\n\nfor key, item in grouped_df:\n    print grouped_df.get_group(key), ""\\n\\n""\n', 'grouped_df = df.groupby(\'A\')    \ngb = grouped_df.groups\n\nfor key, values in gb.iteritems():\n    print df.ix[values], ""\\n\\n""\n', 'gb = grouped_df.groups\ngb.keys()\n\nkey_list_from_gb = [key1, key2, key3]\n\nfor key, values in gb.iteritems():\n    if key in key_list_from_gb:\n        print df.ix[values], ""\\n""\n']";"['grouped_df = df.groupby(\'A\')\n\nfor key, item in grouped_df:\n    print grouped_df.get_group(key), ""\\n\\n""\n', 'grouped_df = df.groupby(\'A\')    \ngb = grouped_df.groups\n\nfor key, values in gb.iteritems():\n    print df.ix[values], ""\\n\\n""\n', 'gb = grouped_df.groups\ngb.keys()\n\nkey_list_from_gb = [key1, key2, key3]\n\nfor key, values in gb.iteritems():\n    if key in key_list_from_gb:\n        print df.ix[values], ""\\n""\n']"
1487;;0;36955053;11285613.0;2;24;;;"<p>As of version 0.11.0, columns <em>can be</em> sliced in the manner you tried using the <a href=""http://pandas-docs.github.io/pandas-docs-travis/whatsnew.html#selection-choices"" rel=""noreferrer""><code>.loc</code></a> indexer: </p>

<pre><code>df.loc[:, 'C':'E']
</code></pre>

<p>returns columns <code>C</code> through <code>E</code>.</p>

<hr>

<p>A demo on a randomly generated DataFrame:</p>

<pre><code>import pandas as pd
import numpy as np
np.random.seed(5)
df = pd.DataFrame(np.random.randint(100, size=(100, 6)), 
                  columns=list('ABCDEF'), 
                  index=['R{}'.format(i) for i in range(100)])
df.head()

Out: 
     A   B   C   D   E   F
R0  99  78  61  16  73   8
R1  62  27  30  80   7  76
R2  15  53  80  27  44  77
R3  75  65  47  30  84  86
R4  18   9  41  62   1  82
</code></pre>

<p>To get the columns from C to E (note that unlike integer slicing, 'E' is included in the columns):</p>

<pre><code>df.loc[:, 'C':'E']

Out: 
      C   D   E
R0   61  16  73
R1   30  80   7
R2   80  27  44
R3   47  30  84
R4   41  62   1
R5    5  58   0
...
</code></pre>

<p>Same works for selecting rows based on labels. Get the rows 'R6' to 'R10' from those columns:</p>

<pre><code>df.loc['R6':'R10', 'C':'E']

Out: 
      C   D   E
R6   51  27  31
R7   83  19  18
R8   11  67  65
R9   78  27  29
R10   7  16  94
</code></pre>

<p><code>.loc</code> also accepts a boolean array so you can select the columns whose corresponding entry in the array is <code>True</code>. For example, <code>df.columns.isin(list('BCD'))</code> returns <code>array([False,  True,  True,  True, False, False], dtype=bool)</code> - True if the column name is in the list <code>['B', 'C', 'D']</code>; False, otherwise.</p>

<pre><code>df.loc[:, df.columns.isin(list('BCD'))]

Out: 
      B   C   D
R0   78  61  16
R1   27  30  80
R2   53  80  27
R3   65  47  30
R4    9  41  62
R5   78   5  58
...
</code></pre>
";;"[""df.loc[:, 'C':'E']\n"", ""import pandas as pd\nimport numpy as np\nnp.random.seed(5)\ndf = pd.DataFrame(np.random.randint(100, size=(100, 6)), \n                  columns=list('ABCDEF'), \n                  index=['R{}'.format(i) for i in range(100)])\ndf.head()\n\nOut: \n     A   B   C   D   E   F\nR0  99  78  61  16  73   8\nR1  62  27  30  80   7  76\nR2  15  53  80  27  44  77\nR3  75  65  47  30  84  86\nR4  18   9  41  62   1  82\n"", ""df.loc[:, 'C':'E']\n\nOut: \n      C   D   E\nR0   61  16  73\nR1   30  80   7\nR2   80  27  44\nR3   47  30  84\nR4   41  62   1\nR5    5  58   0\n...\n"", ""df.loc['R6':'R10', 'C':'E']\n\nOut: \n      C   D   E\nR6   51  27  31\nR7   83  19  18\nR8   11  67  65\nR9   78  27  29\nR10   7  16  94\n"", ""df.loc[:, df.columns.isin(list('BCD'))]\n\nOut: \n      B   C   D\nR0   78  61  16\nR1   27  30  80\nR2   53  80  27\nR3   65  47  30\nR4    9  41  62\nR5   78   5  58\n...\n""]";"['.loc', ""df.loc[:, 'C':'E']\n"", 'C', 'E', ""import pandas as pd\nimport numpy as np\nnp.random.seed(5)\ndf = pd.DataFrame(np.random.randint(100, size=(100, 6)), \n                  columns=list('ABCDEF'), \n                  index=['R{}'.format(i) for i in range(100)])\ndf.head()\n\nOut: \n     A   B   C   D   E   F\nR0  99  78  61  16  73   8\nR1  62  27  30  80   7  76\nR2  15  53  80  27  44  77\nR3  75  65  47  30  84  86\nR4  18   9  41  62   1  82\n"", ""df.loc[:, 'C':'E']\n\nOut: \n      C   D   E\nR0   61  16  73\nR1   30  80   7\nR2   80  27  44\nR3   47  30  84\nR4   41  62   1\nR5    5  58   0\n...\n"", ""df.loc['R6':'R10', 'C':'E']\n\nOut: \n      C   D   E\nR6   51  27  31\nR7   83  19  18\nR8   11  67  65\nR9   78  27  29\nR10   7  16  94\n"", '.loc', 'True', ""df.columns.isin(list('BCD'))"", 'array([False,  True,  True,  True, False, False], dtype=bool)', ""['B', 'C', 'D']"", ""df.loc[:, df.columns.isin(list('BCD'))]\n\nOut: \n      B   C   D\nR0   78  61  16\nR1   27  30  80\nR2   53  80  27\nR3   65  47  30\nR4    9  41  62\nR5   78   5  58\n...\n""]"
1488;;0;36957431;23307301.0;2;6;;;"<p>Slight variation:</p>

<pre><code>w.female.replace(['male', 'female'], [1, 0], inplace=True)
</code></pre>
";;"[""w.female.replace(['male', 'female'], [1, 0], inplace=True)\n""]";"[""w.female.replace(['male', 'female'], [1, 0], inplace=True)\n""]"
1489;;1;36958937;13411544.0;2;33;;;"<p>from version 0.16.1 you can do </p>

<pre><code>df.drop(['column_name'], axis = 1, inplace = True, errors = 'ignore')
</code></pre>
";;"[""df.drop(['column_name'], axis = 1, inplace = True, errors = 'ignore')\n""]";"[""df.drop(['column_name'], axis = 1, inplace = True, errors = 'ignore')\n""]"
1490;;1;37000877;13411544.0;2;44;;;"<p>The actual question posed, missed by most answers here is:</p>

<h3>Why can't I use <code>del df.column_name</code>?</h3>

<p>At first we need to understand the problem, which requires us to dive into <a href=""http://www.rafekettler.com/magicmethods.html""><em>python magic methods</em></a>.</p>

<p>As Wes points out in his answer <code>del df['column']</code> maps to the python <em>magic method</em> <code>df.__delitem__('column')</code> which is <a href=""https://github.com/pydata/pandas/blob/c6110e25b3eceb2f25022c2aa9ccea03c0b8b359/pandas/core/generic.py#L1580"">implemented in pandas to drop the column</a></p>

<p>However, as pointed out in the link above about <a href=""http://www.rafekettler.com/magicmethods.html""><em>python magic methods</em></a>:</p>

<blockquote>
  <p>In fact, <strong>del</strong> should almost never be used because of the precarious circumstances under which it is called; use it with caution!</p>
</blockquote>

<p>You could argue that <code>del df['column_name']</code> should not be used or encouraged, and thereby <code>del df.column_name</code> should not even be considered.</p>

<p>However, in theory, <code>del df.column_name</code> could be implemeted to work in pandas using <a href=""http://www.rafekettler.com/magicmethods.html#access"">the <em>magic method <code>__delattr__</code></em></a>. This does however introduce certain problems, problems which the <code>del df['column_name']</code> implementation already has, but in lesser degree.</p>

<h2>Example Problem</h2>

<p>What if I define a column in a dataframe called ""dtypes"" or ""columns"".</p>

<p>Then assume I want to delete these columns.</p>

<p><code>del df.dtypes</code> would make the <code>__delattr__</code> method confused as if it should delete the ""dtypes"" attribute or the ""dtypes"" column.</p>

<h2>Architectural questions behind this problem</h2>

<ol>
<li>Is a dataframe a
collection of <em>columns</em>?</li>
<li>Is a dataframe a collection of <em>rows</em>?</li>
<li>Is a column an <em>attribute</em> of a dataframe?</li>
</ol>

<h3>Pandas answers:</h3>

<ol>
<li>Yes, in all ways</li>
<li>No, but if you want it to be, you can use the <code>.ix</code>, <code>.loc</code> or <code>.iloc</code> methods.</li>
<li>Maybe, do you want to <em>read</em> data? Then <strong>yes</strong>, <em>unless</em> the name of the attribute is already taken by another attribute belonging to the dataframe. Do you want to <em>modify</em> data? Then <strong>no</strong>.</li>
</ol>

<h1>TLDR;</h1>

<p>You cannot do <code>del df.column_name</code> because pandas has a quite wildly grown architecture that needs to be reconsidered in order for this kind of <em>cognitive dissonance</em> not to occur to its users.</p>

<h3>Protip:</h3>

<p>Don't use df.column_name, It may be pretty, but it causes <em>cognitive dissonance</em></p>

<h3>Zen of Python quotes that fits in here:</h3>

<p>There are multiple ways of deleting a column.</p>

<blockquote>
  <p>There should be one-- and preferably only one --obvious way to do it.</p>
</blockquote>

<p>Columns are sometimes attributes but sometimes not.</p>

<blockquote>
  <p>Special cases aren't special enough to break the rules.</p>
</blockquote>

<p>Does <code>del df.dtypes</code> delete the dtypes attribute or the dtypes column?</p>

<blockquote>
  <p>In the face of ambiguity, refuse the temptation to guess.</p>
</blockquote>
";;[];"['del df.column_name', ""del df['column']"", ""df.__delitem__('column')"", ""del df['column_name']"", 'del df.column_name', 'del df.column_name', '__delattr__', ""del df['column_name']"", 'del df.dtypes', '__delattr__', '.ix', '.loc', '.iloc', 'del df.column_name', 'del df.dtypes']"
1491;;2;37043071;13187778.0;2;65;;;"<p>To convert a pandas dataframe (df) to a numpy ndarray, use this code:</p>

<p><code>df=df.values</code></p>

<p>df now becomes a numpy ndarray.</p>
";;[];['df=df.values']
1492;;2;37097791;14661701.0;2;21;;;"<p>You can also pass to <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"">DataFrame.drop</a> the <em>label itself</em> (instead of Series of index labels):</p>

<pre><code>In[17]: df
Out[17]: 
            a         b         c         d         e
one  0.456558 -2.536432  0.216279 -1.305855 -0.121635
two -1.015127 -0.445133  1.867681  2.179392  0.518801

In[18]: df.drop('one')
Out[18]: 
            a         b         c         d         e
two -1.015127 -0.445133  1.867681  2.179392  0.518801
</code></pre>

<p>Which is equivalent to:</p>

<pre><code>In[19]: df.drop(df.index[[0]])
Out[19]: 
            a         b         c         d         e
two -1.015127 -0.445133  1.867681  2.179392  0.518801
</code></pre>
";;"[""In[17]: df\nOut[17]: \n            a         b         c         d         e\none  0.456558 -2.536432  0.216279 -1.305855 -0.121635\ntwo -1.015127 -0.445133  1.867681  2.179392  0.518801\n\nIn[18]: df.drop('one')\nOut[18]: \n            a         b         c         d         e\ntwo -1.015127 -0.445133  1.867681  2.179392  0.518801\n"", 'In[19]: df.drop(df.index[[0]])\nOut[19]: \n            a         b         c         d         e\ntwo -1.015127 -0.445133  1.867681  2.179392  0.518801\n']";"[""In[17]: df\nOut[17]: \n            a         b         c         d         e\none  0.456558 -2.536432  0.216279 -1.305855 -0.121635\ntwo -1.015127 -0.445133  1.867681  2.179392  0.518801\n\nIn[18]: df.drop('one')\nOut[18]: \n            a         b         c         d         e\ntwo -1.015127 -0.445133  1.867681  2.179392  0.518801\n"", 'In[19]: df.drop(df.index[[0]])\nOut[19]: \n            a         b         c         d         e\ntwo -1.015127 -0.445133  1.867681  2.179392  0.518801\n']"
1493;;3;37199623;12525722.0;2;34;;;"<p>If you don't mind importing the <code>sklearn</code> library, I would recommend the method talked on <a href=""https://web.archive.org/web/20160520170701/http://chrisalbon.com:80/python/pandas_normalize_column.html"" rel=""noreferrer"">this</a> blog.</p>

<pre><code>import pandas as pd
from sklearn import preprocessing

data = {'score': [234,24,14,27,-74,46,73,-18,59,160]}
df = pd.DataFrame(data)
df

min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(df)
df_normalized = pd.DataFrame(np_scaled)
df_normalized
</code></pre>
";;"[""import pandas as pd\nfrom sklearn import preprocessing\n\ndata = {'score': [234,24,14,27,-74,46,73,-18,59,160]}\ndf = pd.DataFrame(data)\ndf\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(df)\ndf_normalized = pd.DataFrame(np_scaled)\ndf_normalized\n""]";"['sklearn', ""import pandas as pd\nfrom sklearn import preprocessing\n\ndata = {'score': [234,24,14,27,-74,46,73,-18,59,160]}\ndf = pd.DataFrame(data)\ndf\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(df)\ndf_normalized = pd.DataFrame(np_scaled)\ndf_normalized\n""]"
1494;;2;37347783;19124601.0;2;14;;;"<p>Simply use this for printing entire frames:</p>

<pre><code>pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
</code></pre>

<p>Also you can create a pretty-print function with context manager, like in examples above.</p>

<p>For full list of useful options, see:</p>

<pre><code>pd.describe_option('display')
</code></pre>

<p>Few examples of use:
<a href=""http://pandas.pydata.org/pandas-docs/stable/options.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/stable/options.html</a></p>
";;"[""pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n"", ""pd.describe_option('display')\n""]";"[""pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n"", ""pd.describe_option('display')\n""]"
1495;;0;37384347;16096627.0;2;6;;;"<p>To index-based access to the pandas table, one can also consider <em>numpy.as_array</em> option to convert the table to Numpy array as</p>

<pre><code>np_df = df.as_matrix()
</code></pre>

<p>and then</p>

<pre><code>np_df[i] 
</code></pre>

<p>would work.</p>
";;['np_df = df.as_matrix()\n', 'np_df[i] \n'];['np_df = df.as_matrix()\n', 'np_df[i] \n']
1496;;4;37441204;14940743.0;2;45;;;"<p>There is a new index method called <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.difference.html""><code>difference</code></a>. It returns the original columns, with the columns passed as argument removed. </p>

<pre><code>df2 = df[df.columns.difference(['B', 'D'])]
</code></pre>

<p>Here, the output is used to filter out columns <code>B</code> and <code>D</code> from <code>df</code>.</p>
";;"[""df2 = df[df.columns.difference(['B', 'D'])]\n""]";"['difference', ""df2 = df[df.columns.difference(['B', 'D'])]\n"", 'B', 'D', 'df']"
1497;;0;37442692;29530232.0;2;11;;;"<p>If you need to know how many ""1 or more"" rows have NaNs:</p>

<pre><code>df.isnull().T.any().T.sum()
</code></pre>

<p>Or if you need to pull out these rows and examine them:</p>

<pre><code>nan_rows = df[df.isnull().T.any().T]
</code></pre>
";;['df.isnull().T.any().T.sum()\n', 'nan_rows = df[df.isnull().T.any().T]\n'];['df.isnull().T.any().T.sum()\n', 'nan_rows = df[df.isnull().T.any().T]\n']
1498;;4;37447530;10636024.0;2;17;;;"<p>I wasn't fully satisfied with some other GUIs, so I created my own, which I'm now maintaining <a href=""https://github.com/bluenote10/PandasDataFrameGUI"" rel=""noreferrer"">on Github</a>. Example:</p>

<p><a href=""https://i.stack.imgur.com/rtB25.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/rtB25.png"" alt=""enter image description here""></a></p>

<p>Apart from the basic table + plot functionality, I wanted to have a specific way to filter data:</p>

<ul>
<li>select a column to filter from a combo box</li>
<li>write an ""underscore expression"" to filter on that column using arbitrary Python code. For example: <code>_ &gt; 0</code> to filter positive values only, or more complex expressions like <code>(_ &gt;= date(2016, 1, 1)) &amp; (_ &lt;= date(2016, 1, 31))</code> e.g. for datetime columns.</li>
</ul>
";;[];['_ > 0', '(_ >= date(2016, 1, 1)) & (_ <= date(2016, 1, 31))']
1499;;2;37453925;21269399.0;2;60;;;"<h1>Why it does not work</h1>

<p>There is no datetime dtype to be set for read_csv as csv files can only contain strings, integers and floats.</p>

<p>Setting a dtype to datetime will make pandas interpret the datetime as an object, meaning you will end up with a string.</p>

<h1>Pandas way of solving this</h1>

<p>The <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""noreferrer""><code>pandas.read_csv()</code></a> function has a keyword argument called <code>parse_dates</code></p>

<p>Using this you can on the fly convert strings, floats or integers into datetimes using the default <code>date_parser</code> (<code>dateutil.parser.parser</code>)</p>

<pre><code>headers = ['col1', 'col2', 'col3', 'col4']
dtypes = {'col1': 'str', 'col2': 'str', 'col3': 'str', 'col4': 'float'}
parse_dates = ['col1', 'col2']
pd.read_csv(file, sep='\t', header=None, names=headers, dtype=dtypes, parse_dates=parse_dates)
</code></pre>

<p>This will cause pandas to read <code>col1</code> and <code>col2</code> as strings, which they most likely are (""2016-05-05"" etc.) and after having read the string, the date_parser for each column will act upon that string and give back whatever that function returns.</p>

<h1>Defining your own date parsing function:</h1>

<p>The <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""noreferrer""><code>pandas.read_csv()</code></a> function <strong>also</strong> has a keyword argument called <code>date_parser</code></p>

<p>Setting this to a lambda function will make that particular function be used for the parsing of the dates.</p>

<h3>GOTCHA WARNING</h3>

<p>You have to give it the function, not the execution of the function, thus this is <strong>Correct</strong></p>

<pre><code>date_parser = pd.datetools.to_datetime
</code></pre>

<p>This is <strong>incorrect</strong>:</p>

<pre><code>date_parser = pd.datetools.to_datetime()
</code></pre>
";;"[""headers = ['col1', 'col2', 'col3', 'col4']\ndtypes = {'col1': 'str', 'col2': 'str', 'col3': 'str', 'col4': 'float'}\nparse_dates = ['col1', 'col2']\npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes, parse_dates=parse_dates)\n"", 'date_parser = pd.datetools.to_datetime\n', 'date_parser = pd.datetools.to_datetime()\n']";"['pandas.read_csv()', 'parse_dates', 'date_parser', 'dateutil.parser.parser', ""headers = ['col1', 'col2', 'col3', 'col4']\ndtypes = {'col1': 'str', 'col2': 'str', 'col3': 'str', 'col4': 'float'}\nparse_dates = ['col1', 'col2']\npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes, parse_dates=parse_dates)\n"", 'col1', 'col2', 'pandas.read_csv()', 'date_parser', 'date_parser = pd.datetools.to_datetime\n', 'date_parser = pd.datetools.to_datetime()\n']"
1500;;2;37592047;17116814.0;2;8;;;"<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'ItemQty': {0: 3, 1: 25}, 
                   'Seatblocks': {0: '2:218:10:4,6', 1: '1:13:36:1,12 1:13:37:1,13'}, 
                   'ItemExt': {0: 60, 1: 300}, 
                   'CustomerName': {0: 'McCartney, Paul', 1: 'Lennon, John'}, 
                   'CustNum': {0: 32363, 1: 31316}, 
                   'Item': {0: 'F04', 1: 'F01'}}, 
                    columns=['CustNum','CustomerName','ItemQty','Item','Seatblocks','ItemExt'])

print (df)
   CustNum     CustomerName  ItemQty Item                 Seatblocks  ItemExt
0    32363  McCartney, Paul        3  F04               2:218:10:4,6       60
1    31316     Lennon, John       25  F01  1:13:36:1,12 1:13:37:1,13      300
</code></pre>

<p>Another similar solution with chaining is use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html"" rel=""noreferrer""><code>reset_index</code></a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.rename.html"" rel=""noreferrer""><code>rename</code></a>:</p>

<pre><code>print (df.drop('Seatblocks', axis=1)
             .join
             (
             df.Seatblocks
             .str
             .split(expand=True)
             .stack()
             .reset_index(drop=True, level=1)
             .rename('Seatblocks')           
             ))

   CustNum     CustomerName  ItemQty Item  ItemExt    Seatblocks
0    32363  McCartney, Paul        3  F04       60  2:218:10:4,6
1    31316     Lennon, John       25  F01      300  1:13:36:1,12
1    31316     Lennon, John       25  F01      300  1:13:37:1,13
</code></pre>

<hr>

<p>If in column are <strong>NOT</strong> <code>NaN</code> values, the fastest solution is use <code>list</code> comprehension with <code>DataFrame</code> constructor:</p>

<pre><code>df = pd.DataFrame(['a b c']*100000, columns=['col'])

In [141]: %timeit (pd.DataFrame(dict(zip(range(3), [df['col'].apply(lambda x : x.split(' ')[i]) for i in range(3)]))))
1 loop, best of 3: 211 ms per loop

In [142]: %timeit (pd.DataFrame(df.col.str.split().tolist()))
10 loops, best of 3: 87.8 ms per loop

In [143]: %timeit (pd.DataFrame(list(df.col.str.split())))
10 loops, best of 3: 86.1 ms per loop

In [144]: %timeit (df.col.str.split(expand=True))
10 loops, best of 3: 156 ms per loop

In [145]: %timeit (pd.DataFrame([ x.split() for x in df['col'].tolist()]))
10 loops, best of 3: 54.1 ms per loop
</code></pre>

<p>But if column contains <code>NaN</code> only works <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.split.html"" rel=""noreferrer""><code>str.split</code></a> with parameter <code>expand=True</code> which return <code>DataFrame</code> (<a href=""http://pandas.pydata.org/pandas-docs/stable/text.html#splitting-and-replacing-strings"" rel=""noreferrer"">documentation</a>), and it explain why it is slowier:</p>

<pre><code>df = pd.DataFrame(['a b c']*10, columns=['col'])
df.loc[0] = np.nan
print (df.head())
     col
0    NaN
1  a b c
2  a b c
3  a b c
4  a b c

print (df.col.str.split(expand=True))
     0     1     2
0  NaN  None  None
1    a     b     c
2    a     b     c
3    a     b     c
4    a     b     c
5    a     b     c
6    a     b     c
7    a     b     c
8    a     b     c
9    a     b     c
</code></pre>
";;"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'ItemQty': {0: 3, 1: 25}, \n                   'Seatblocks': {0: '2:218:10:4,6', 1: '1:13:36:1,12 1:13:37:1,13'}, \n                   'ItemExt': {0: 60, 1: 300}, \n                   'CustomerName': {0: 'McCartney, Paul', 1: 'Lennon, John'}, \n                   'CustNum': {0: 32363, 1: 31316}, \n                   'Item': {0: 'F04', 1: 'F01'}}, \n                    columns=['CustNum','CustomerName','ItemQty','Item','Seatblocks','ItemExt'])\n\nprint (df)\n   CustNum     CustomerName  ItemQty Item                 Seatblocks  ItemExt\n0    32363  McCartney, Paul        3  F04               2:218:10:4,6       60\n1    31316     Lennon, John       25  F01  1:13:36:1,12 1:13:37:1,13      300\n"", ""print (df.drop('Seatblocks', axis=1)\n             .join\n             (\n             df.Seatblocks\n             .str\n             .split(expand=True)\n             .stack()\n             .reset_index(drop=True, level=1)\n             .rename('Seatblocks')           \n             ))\n\n   CustNum     CustomerName  ItemQty Item  ItemExt    Seatblocks\n0    32363  McCartney, Paul        3  F04       60  2:218:10:4,6\n1    31316     Lennon, John       25  F01      300  1:13:36:1,12\n1    31316     Lennon, John       25  F01      300  1:13:37:1,13\n"", ""df = pd.DataFrame(['a b c']*100000, columns=['col'])\n\nIn [141]: %timeit (pd.DataFrame(dict(zip(range(3), [df['col'].apply(lambda x : x.split(' ')[i]) for i in range(3)]))))\n1 loop, best of 3: 211 ms per loop\n\nIn [142]: %timeit (pd.DataFrame(df.col.str.split().tolist()))\n10 loops, best of 3: 87.8 ms per loop\n\nIn [143]: %timeit (pd.DataFrame(list(df.col.str.split())))\n10 loops, best of 3: 86.1 ms per loop\n\nIn [144]: %timeit (df.col.str.split(expand=True))\n10 loops, best of 3: 156 ms per loop\n\nIn [145]: %timeit (pd.DataFrame([ x.split() for x in df['col'].tolist()]))\n10 loops, best of 3: 54.1 ms per loop\n"", ""df = pd.DataFrame(['a b c']*10, columns=['col'])\ndf.loc[0] = np.nan\nprint (df.head())\n     col\n0    NaN\n1  a b c\n2  a b c\n3  a b c\n4  a b c\n\nprint (df.col.str.split(expand=True))\n     0     1     2\n0  NaN  None  None\n1    a     b     c\n2    a     b     c\n3    a     b     c\n4    a     b     c\n5    a     b     c\n6    a     b     c\n7    a     b     c\n8    a     b     c\n9    a     b     c\n""]";"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'ItemQty': {0: 3, 1: 25}, \n                   'Seatblocks': {0: '2:218:10:4,6', 1: '1:13:36:1,12 1:13:37:1,13'}, \n                   'ItemExt': {0: 60, 1: 300}, \n                   'CustomerName': {0: 'McCartney, Paul', 1: 'Lennon, John'}, \n                   'CustNum': {0: 32363, 1: 31316}, \n                   'Item': {0: 'F04', 1: 'F01'}}, \n                    columns=['CustNum','CustomerName','ItemQty','Item','Seatblocks','ItemExt'])\n\nprint (df)\n   CustNum     CustomerName  ItemQty Item                 Seatblocks  ItemExt\n0    32363  McCartney, Paul        3  F04               2:218:10:4,6       60\n1    31316     Lennon, John       25  F01  1:13:36:1,12 1:13:37:1,13      300\n"", 'reset_index', 'rename', ""print (df.drop('Seatblocks', axis=1)\n             .join\n             (\n             df.Seatblocks\n             .str\n             .split(expand=True)\n             .stack()\n             .reset_index(drop=True, level=1)\n             .rename('Seatblocks')           \n             ))\n\n   CustNum     CustomerName  ItemQty Item  ItemExt    Seatblocks\n0    32363  McCartney, Paul        3  F04       60  2:218:10:4,6\n1    31316     Lennon, John       25  F01      300  1:13:36:1,12\n1    31316     Lennon, John       25  F01      300  1:13:37:1,13\n"", 'NaN', 'list', 'DataFrame', ""df = pd.DataFrame(['a b c']*100000, columns=['col'])\n\nIn [141]: %timeit (pd.DataFrame(dict(zip(range(3), [df['col'].apply(lambda x : x.split(' ')[i]) for i in range(3)]))))\n1 loop, best of 3: 211 ms per loop\n\nIn [142]: %timeit (pd.DataFrame(df.col.str.split().tolist()))\n10 loops, best of 3: 87.8 ms per loop\n\nIn [143]: %timeit (pd.DataFrame(list(df.col.str.split())))\n10 loops, best of 3: 86.1 ms per loop\n\nIn [144]: %timeit (df.col.str.split(expand=True))\n10 loops, best of 3: 156 ms per loop\n\nIn [145]: %timeit (pd.DataFrame([ x.split() for x in df['col'].tolist()]))\n10 loops, best of 3: 54.1 ms per loop\n"", 'NaN', 'str.split', 'expand=True', 'DataFrame', ""df = pd.DataFrame(['a b c']*10, columns=['col'])\ndf.loc[0] = np.nan\nprint (df.head())\n     col\n0    NaN\n1  a b c\n2  a b c\n3  a b c\n4  a b c\n\nprint (df.col.str.split(expand=True))\n     0     1     2\n0  NaN  None  None\n1    a     b     c\n2    a     b     c\n3    a     b     c\n4    a     b     c\n5    a     b     c\n6    a     b     c\n7    a     b     c\n8    a     b     c\n9    a     b     c\n""]"
1501;;0;37647160;17383094.0;2;11;;;"<p>Just multiply your Dataframe by 1 (int)</p>

<pre><code>[1]: data = pd.DataFrame([[True, False, True], [False, False, True]])
[2]: print data
          0      1     2
     0   True  False  True
     1   False False  True

[3]: print data*1
         0  1  2
     0   1  0  1
     1   0  0  1
</code></pre>
";;['[1]: data = pd.DataFrame([[True, False, True], [False, False, True]])\n[2]: print data\n          0      1     2\n     0   True  False  True\n     1   False False  True\n\n[3]: print data*1\n         0  1  2\n     0   1  0  1\n     1   0  0  1\n'];['[1]: data = pd.DataFrame([[True, False, True], [False, False, True]])\n[2]: print data\n          0      1     2\n     0   True  False  True\n     1   False False  True\n\n[3]: print data*1\n         0  1  2\n     0   1  0  1\n     1   0  0  1\n']
1502;;0;37655063;27236275.0;2;36;;;"<p>As others have said, you've probably got duplicate values in your original index. To find them do this:</p>

<p><code>df[df.index.duplicated()]</code></p>
";;[];['df[df.index.duplicated()]']
1503;;1;37717675;29763620.0;2;39;;;"<p>Don't use <code>ix</code>. It's <a href=""https://pandas-docs.github.io/pandas-docs-travis/whatsnew.html#deprecate-ix"" rel=""noreferrer"">deprecated</a>. The most readable and idiomatic way of doing this is <code>df.drop()</code>:</p>

<pre><code>&gt;&gt;&gt; df

          a         b         c         d
0  0.175127  0.191051  0.382122  0.869242
1  0.414376  0.300502  0.554819  0.497524
2  0.142878  0.406830  0.314240  0.093132
3  0.337368  0.851783  0.933441  0.949598

&gt;&gt;&gt; df.drop('b', axis=1)

          a         c         d
0  0.175127  0.382122  0.869242
1  0.414376  0.554819  0.497524
2  0.142878  0.314240  0.093132
3  0.337368  0.933441  0.949598
</code></pre>

<p>Note that by default, <code>.drop()</code> does not operate inplace; despite the ominous name, <code>df</code> is unharmed by this process. If you want to permanently remove <code>b</code> from <code>df</code>, do <code>df.drop('b', inplace=True)</code>.</p>

<p><code>df.drop()</code> also accepts a list of labels, e.g. <code>df.drop(['a', 'b'], axis=1)</code> will drop column <code>a</code> and <code>b</code>.</p>
";;"["">>> df\n\n          a         b         c         d\n0  0.175127  0.191051  0.382122  0.869242\n1  0.414376  0.300502  0.554819  0.497524\n2  0.142878  0.406830  0.314240  0.093132\n3  0.337368  0.851783  0.933441  0.949598\n\n>>> df.drop('b', axis=1)\n\n          a         c         d\n0  0.175127  0.382122  0.869242\n1  0.414376  0.554819  0.497524\n2  0.142878  0.314240  0.093132\n3  0.337368  0.933441  0.949598\n""]";"['ix', 'df.drop()', "">>> df\n\n          a         b         c         d\n0  0.175127  0.191051  0.382122  0.869242\n1  0.414376  0.300502  0.554819  0.497524\n2  0.142878  0.406830  0.314240  0.093132\n3  0.337368  0.851783  0.933441  0.949598\n\n>>> df.drop('b', axis=1)\n\n          a         c         d\n0  0.175127  0.382122  0.869242\n1  0.414376  0.554819  0.497524\n2  0.142878  0.314240  0.093132\n3  0.337368  0.933441  0.949598\n"", '.drop()', 'df', 'b', 'df', ""df.drop('b', inplace=True)"", 'df.drop()', ""df.drop(['a', 'b'], axis=1)"", 'a', 'b']"
1504;;1;37793940;20158597.0;2;8;;;"<p>Another way to do this is to introduce a minimal amount of noise, which will artificially create unique bin edges. Here's an example:</p>

<pre><code>a = pd.Series(range(100) + ([0]*20))

def jitter(a_series, noise_reduction=1000000):
    return (np.random.random(len(a_series))*a_series.std()/noise_reduction)-(a_series.std()/(2*noise_reduction))

# and now this works by adding a little noise
a_deciles = pd.qcut(a + jitter(a), 10, labels=False)
</code></pre>

<p>we can recreate the original error using something like this:</p>

<pre><code>a_deciles = pd.qcut(a, 10, labels=False)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/site-packages/pandas/tools/tile.py"", line 173, in qcut
    precision=precision, include_lowest=True)
  File ""/usr/local/lib/python2.7/site-packages/pandas/tools/tile.py"", line 192, in _bins_to_cuts
    raise ValueError('Bin edges must be unique: %s' % repr(bins))
ValueError: Bin edges must be unique: array([  0.        ,   0.        ,   0.        ,   3.8       ,
        11.73333333,  19.66666667,  27.6       ,  35.53333333,
        43.46666667,  51.4       ,  59.33333333,  67.26666667,
        75.2       ,  83.13333333,  91.06666667,  99.        ])
</code></pre>
";;"['a = pd.Series(range(100) + ([0]*20))\n\ndef jitter(a_series, noise_reduction=1000000):\n    return (np.random.random(len(a_series))*a_series.std()/noise_reduction)-(a_series.std()/(2*noise_reduction))\n\n# and now this works by adding a little noise\na_deciles = pd.qcut(a + jitter(a), 10, labels=False)\n', 'a_deciles = pd.qcut(a, 10, labels=False)\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/usr/local/lib/python2.7/site-packages/pandas/tools/tile.py"", line 173, in qcut\n    precision=precision, include_lowest=True)\n  File ""/usr/local/lib/python2.7/site-packages/pandas/tools/tile.py"", line 192, in _bins_to_cuts\n    raise ValueError(\'Bin edges must be unique: %s\' % repr(bins))\nValueError: Bin edges must be unique: array([  0.        ,   0.        ,   0.        ,   3.8       ,\n        11.73333333,  19.66666667,  27.6       ,  35.53333333,\n        43.46666667,  51.4       ,  59.33333333,  67.26666667,\n        75.2       ,  83.13333333,  91.06666667,  99.        ])\n']";"['a = pd.Series(range(100) + ([0]*20))\n\ndef jitter(a_series, noise_reduction=1000000):\n    return (np.random.random(len(a_series))*a_series.std()/noise_reduction)-(a_series.std()/(2*noise_reduction))\n\n# and now this works by adding a little noise\na_deciles = pd.qcut(a + jitter(a), 10, labels=False)\n', 'a_deciles = pd.qcut(a, 10, labels=False)\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/usr/local/lib/python2.7/site-packages/pandas/tools/tile.py"", line 173, in qcut\n    precision=precision, include_lowest=True)\n  File ""/usr/local/lib/python2.7/site-packages/pandas/tools/tile.py"", line 192, in _bins_to_cuts\n    raise ValueError(\'Bin edges must be unique: %s\' % repr(bins))\nValueError: Bin edges must be unique: array([  0.        ,   0.        ,   0.        ,   3.8       ,\n        11.73333333,  19.66666667,  27.6       ,  35.53333333,\n        43.46666667,  51.4       ,  59.33333333,  67.26666667,\n        75.2       ,  83.13333333,  91.06666667,  99.        ])\n']"
1505;;0;37806819;12433076.0;2;10;;;"<p>Extending <a href=""https://stackoverflow.com/a/12510334/5741205"">@Def_Os's</a> answer with an actual demo...</p>

<p>As @Def_Os has already said - using <a href=""https://pandas-datareader.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">Pandas Datareader</a> makes this task a real fun</p>

<pre><code>In [12]: from pandas_datareader import data
</code></pre>

<p>pulling all available historical data for <code>AAPL</code> starting from <code>1980-01-01</code></p>

<pre><code>#In [13]: aapl = data.DataReader('AAPL', 'yahoo', '1980-01-01')

# yahoo api is inconsistent for getting historical data, please use google instead.
In [13]: aapl = data.DataReader('AAPL', 'google', '1980-01-01')
</code></pre>

<p>first 5 rows</p>

<pre><code>In [14]: aapl.head()
Out[14]:
                 Open       High     Low   Close     Volume  Adj Close
Date
1980-12-12  28.750000  28.875000  28.750  28.750  117258400   0.431358
1980-12-15  27.375001  27.375001  27.250  27.250   43971200   0.408852
1980-12-16  25.375000  25.375000  25.250  25.250   26432000   0.378845
1980-12-17  25.875000  25.999999  25.875  25.875   21610400   0.388222
1980-12-18  26.625000  26.750000  26.625  26.625   18362400   0.399475
</code></pre>

<p>last 5 rows</p>

<pre><code>In [15]: aapl.tail()
Out[15]:
                 Open       High        Low      Close    Volume  Adj Close
Date
2016-06-07  99.250000  99.870003  98.959999  99.029999  22366400  99.029999
2016-06-08  99.019997  99.559998  98.680000  98.940002  20812700  98.940002
2016-06-09  98.500000  99.989998  98.459999  99.650002  26419600  99.650002
2016-06-10  98.529999  99.349998  98.480003  98.830002  31462100  98.830002
2016-06-13  98.690002  99.120003  97.099998  97.339996  37612900  97.339996
</code></pre>

<p>save all data as CSV file</p>

<pre><code>In [16]: aapl.to_csv('d:/temp/aapl_data.csv')
</code></pre>

<p>d:/temp/aapl_data.csv - 5 first rows</p>

<pre><code>Date,Open,High,Low,Close,Volume,Adj Close
1980-12-12,28.75,28.875,28.75,28.75,117258400,0.431358
1980-12-15,27.375001,27.375001,27.25,27.25,43971200,0.408852
1980-12-16,25.375,25.375,25.25,25.25,26432000,0.378845
1980-12-17,25.875,25.999999,25.875,25.875,21610400,0.38822199999999996
1980-12-18,26.625,26.75,26.625,26.625,18362400,0.399475
...
</code></pre>
";;"['In [12]: from pandas_datareader import data\n', ""#In [13]: aapl = data.DataReader('AAPL', 'yahoo', '1980-01-01')\n\n# yahoo api is inconsistent for getting historical data, please use google instead.\nIn [13]: aapl = data.DataReader('AAPL', 'google', '1980-01-01')\n"", 'In [14]: aapl.head()\nOut[14]:\n                 Open       High     Low   Close     Volume  Adj Close\nDate\n1980-12-12  28.750000  28.875000  28.750  28.750  117258400   0.431358\n1980-12-15  27.375001  27.375001  27.250  27.250   43971200   0.408852\n1980-12-16  25.375000  25.375000  25.250  25.250   26432000   0.378845\n1980-12-17  25.875000  25.999999  25.875  25.875   21610400   0.388222\n1980-12-18  26.625000  26.750000  26.625  26.625   18362400   0.399475\n', 'In [15]: aapl.tail()\nOut[15]:\n                 Open       High        Low      Close    Volume  Adj Close\nDate\n2016-06-07  99.250000  99.870003  98.959999  99.029999  22366400  99.029999\n2016-06-08  99.019997  99.559998  98.680000  98.940002  20812700  98.940002\n2016-06-09  98.500000  99.989998  98.459999  99.650002  26419600  99.650002\n2016-06-10  98.529999  99.349998  98.480003  98.830002  31462100  98.830002\n2016-06-13  98.690002  99.120003  97.099998  97.339996  37612900  97.339996\n', ""In [16]: aapl.to_csv('d:/temp/aapl_data.csv')\n"", 'Date,Open,High,Low,Close,Volume,Adj Close\n1980-12-12,28.75,28.875,28.75,28.75,117258400,0.431358\n1980-12-15,27.375001,27.375001,27.25,27.25,43971200,0.408852\n1980-12-16,25.375,25.375,25.25,25.25,26432000,0.378845\n1980-12-17,25.875,25.999999,25.875,25.875,21610400,0.38822199999999996\n1980-12-18,26.625,26.75,26.625,26.625,18362400,0.399475\n...\n']";"['In [12]: from pandas_datareader import data\n', 'AAPL', '1980-01-01', ""#In [13]: aapl = data.DataReader('AAPL', 'yahoo', '1980-01-01')\n\n# yahoo api is inconsistent for getting historical data, please use google instead.\nIn [13]: aapl = data.DataReader('AAPL', 'google', '1980-01-01')\n"", 'In [14]: aapl.head()\nOut[14]:\n                 Open       High     Low   Close     Volume  Adj Close\nDate\n1980-12-12  28.750000  28.875000  28.750  28.750  117258400   0.431358\n1980-12-15  27.375001  27.375001  27.250  27.250   43971200   0.408852\n1980-12-16  25.375000  25.375000  25.250  25.250   26432000   0.378845\n1980-12-17  25.875000  25.999999  25.875  25.875   21610400   0.388222\n1980-12-18  26.625000  26.750000  26.625  26.625   18362400   0.399475\n', 'In [15]: aapl.tail()\nOut[15]:\n                 Open       High        Low      Close    Volume  Adj Close\nDate\n2016-06-07  99.250000  99.870003  98.959999  99.029999  22366400  99.029999\n2016-06-08  99.019997  99.559998  98.680000  98.940002  20812700  98.940002\n2016-06-09  98.500000  99.989998  98.459999  99.650002  26419600  99.650002\n2016-06-10  98.529999  99.349998  98.480003  98.830002  31462100  98.830002\n2016-06-13  98.690002  99.120003  97.099998  97.339996  37612900  97.339996\n', ""In [16]: aapl.to_csv('d:/temp/aapl_data.csv')\n"", 'Date,Open,High,Low,Close,Volume,Adj Close\n1980-12-12,28.75,28.875,28.75,28.75,117258400,0.431358\n1980-12-15,27.375001,27.375001,27.25,27.25,43971200,0.408852\n1980-12-16,25.375,25.375,25.25,25.25,26432000,0.378845\n1980-12-17,25.875,25.999999,25.875,25.875,21610400,0.38822199999999996\n1980-12-18,26.625,26.75,26.625,26.625,18362400,0.399475\n...\n']"
1506;;0;37891437;22676081.0;2;56;;;"<p><code>pandas.merge()</code> is the underlying function used for all merge/join behavior. </p>

<p>DataFrames provide the <code>pandas.DataFrame.merge()</code> and <code>pandas.DataFrame.join()</code> methods as a convenient way to access the capabilities of <code>pandas.merge()</code>. For example, <code>df1.merge(right=df2, ...)</code> is equivalent to <code>pandas.merge(left=df1, right=df2, ...)</code>. </p>

<p>These are the main differences between <code>df.join()</code> and <code>df.merge()</code>:</p>

<ol>
<li>lookup on right table: <code>df1.join(df2)</code> always joins via the index of <code>df2</code>, but <code>df1.merge(df2)</code> can join to one or more columns of <code>df2</code> (default) or to the index of <code>df2</code> (with <code>right_index=True</code>). </li>
<li>lookup on left table: by default, <code>df1.join(df2)</code> uses the index of <code>df1</code> and <code>df1.merge(df2)</code> uses column(s) of <code>df1</code>. That can be overridden by specifying <code>df1.join(df2, on=key_or_keys)</code> or <code>df1.merge(df2, right_index=True)</code>. </li>
<li>left vs inner join: <code>df1.join(df2)</code> does a left join by default (keeps all rows of <code>df1</code>), but <code>df.merge</code> does an inner join by default (returns only matching rows of <code>df1</code> and <code>df2</code>).</li>
</ol>

<p>So, the generic approach is to use <code>pandas.merge(df1, df2)</code> or <code>df1.merge(df2)</code>. But for a number of common situations (keeping all rows of <code>df1</code> and joining to  an index in <code>df2</code>), you can save some typing by using <code>df1.join(df2)</code> instead.</p>

<p>Some notes on these issues from the documentation at <a href=""http://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging"">http://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging</a>:</p>

<blockquote>
  <p><code>merge</code> is a function in the pandas namespace, and it is also
  available as a DataFrame instance method, with the calling DataFrame
  being implicitly considered the left object in the join.</p>
  
  <p>The related <code>DataFrame.join</code> method, uses <code>merge</code> internally for the
  index-on-index and index-on-column(s) joins, but joins on indexes by
  default rather than trying to join on common columns (the default
  behavior for <code>merge</code>). If you are joining on index, you may wish to
  use <code>DataFrame.join</code> to save yourself some typing.</p>
</blockquote>

<p>...</p>

<blockquote>
  <p>These two function calls are completely equivalent:</p>

<pre><code>left.join(right, on=key_or_keys)
pd.merge(left, right, left_on=key_or_keys, right_index=True, how='left', sort=False)
</code></pre>
</blockquote>
";;"[""left.join(right, on=key_or_keys)\npd.merge(left, right, left_on=key_or_keys, right_index=True, how='left', sort=False)\n""]";"['pandas.merge()', 'pandas.DataFrame.merge()', 'pandas.DataFrame.join()', 'pandas.merge()', 'df1.merge(right=df2, ...)', 'pandas.merge(left=df1, right=df2, ...)', 'df.join()', 'df.merge()', 'df1.join(df2)', 'df2', 'df1.merge(df2)', 'df2', 'df2', 'right_index=True', 'df1.join(df2)', 'df1', 'df1.merge(df2)', 'df1', 'df1.join(df2, on=key_or_keys)', 'df1.merge(df2, right_index=True)', 'df1.join(df2)', 'df1', 'df.merge', 'df1', 'df2', 'pandas.merge(df1, df2)', 'df1.merge(df2)', 'df1', 'df2', 'df1.join(df2)', 'merge', 'DataFrame.join', 'merge', 'merge', 'DataFrame.join', ""left.join(right, on=key_or_keys)\npd.merge(left, right, left_on=key_or_keys, right_index=True, how='left', sort=False)\n""]"
1507;;0;37992805;13331518.0;2;15;;;"<p><em>TLDR: do not append items to a series one by one, better extend with an ordered collection</em></p>

<p>I think the question in its current form is a bit tricky. And the accepted answer does answer the question. But the more I use pandas, the more I understand that it's a bad idea to append items to a Series one by one. I'll try to explain why for pandas beginners.</p>

<p>You might think that appending data to a given Series might allow you to reuse some resources, but in reality a Series is just a container that stores a relation between an index and a values array. Each is a numpy.array under the hood, and the index is immutable. When you add to Series an item with a label that is missing in the index, a new index with size n+1 is created, and a new values values array of the same size. That means that when you append items one by one, you create two more arrays of the n+1 size on each step.</p>

<p>By the way, you can not append a new item by position (you will get an IndexError) and the label in an index does not have to be unique, that is when you assign a value with a label, you assign the value to all existing items with the the label, and a new row is not appended in this case. This might lead to subtle bugs.</p>

<p>The moral of the story is that you should not append data one by one, you should better extend with an ordered collection. The problem is that you can not extend a Series inplace. That is why it is better to organize your code so that you don't need to update a specific instance of a Series by reference.</p>

<p>If you create labels yourself and they are increasing, the easiest way is to add new items to a dictionary, then create a new Series from the dictionary (it sorts the keys) and append the Series to an old one. If the keys are not increasing, then you will need to create two separate lists for the new labels and the new values.</p>

<p>Below are some code samples:</p>

<pre><code>In [1]: import pandas as pd
In [2]: import numpy as np

In [3]: s = pd.Series(np.arange(4)**2, index=np.arange(4))

In [4]: s
Out[4]:
0    0
1    1
2    4
3    9
dtype: int64

In [6]: id(s.index), id(s.values)
Out[6]: (4470549648, 4470593296)
</code></pre>

<p>When we update an existing item, the index and the values array stay the same (if you do not change the type of the value)</p>

<pre><code>In [7]: s[2] = 14  

In [8]: id(s.index), id(s.values)
Out[8]: (4470549648, 4470593296)
</code></pre>

<p>But when you add a new item, a new index and a new values array is generated:</p>

<pre><code>In [9]: s[4] = 16

In [10]: s
Out[10]:
0     0
1     1
2    14
3     9
4    16
dtype: int64

In [11]: id(s.index), id(s.values)
Out[11]: (4470548560, 4470595056)
</code></pre>

<p>That is if you are going to append several items, collect them in a dictionary, create a Series, append it to the old one and save the result:</p>

<pre><code>In [13]: new_items = {item: item**2 for item in range(5, 7)}

In [14]: s2 = pd.Series(new_items)

In [15]: s2  # keys are guaranteed to be sorted!
Out[15]:
5    25
6    36
dtype: int64

In [16]: s = s.append(s2); s
Out[16]:
0     0
1     1
2    14
3     9
4    16
5    25
6    36
dtype: int64
</code></pre>
";;"['In [1]: import pandas as pd\nIn [2]: import numpy as np\n\nIn [3]: s = pd.Series(np.arange(4)**2, index=np.arange(4))\n\nIn [4]: s\nOut[4]:\n0    0\n1    1\n2    4\n3    9\ndtype: int64\n\nIn [6]: id(s.index), id(s.values)\nOut[6]: (4470549648, 4470593296)\n', 'In [7]: s[2] = 14  \n\nIn [8]: id(s.index), id(s.values)\nOut[8]: (4470549648, 4470593296)\n', 'In [9]: s[4] = 16\n\nIn [10]: s\nOut[10]:\n0     0\n1     1\n2    14\n3     9\n4    16\ndtype: int64\n\nIn [11]: id(s.index), id(s.values)\nOut[11]: (4470548560, 4470595056)\n', 'In [13]: new_items = {item: item**2 for item in range(5, 7)}\n\nIn [14]: s2 = pd.Series(new_items)\n\nIn [15]: s2  # keys are guaranteed to be sorted!\nOut[15]:\n5    25\n6    36\ndtype: int64\n\nIn [16]: s = s.append(s2); s\nOut[16]:\n0     0\n1     1\n2    14\n3     9\n4    16\n5    25\n6    36\ndtype: int64\n']";"['In [1]: import pandas as pd\nIn [2]: import numpy as np\n\nIn [3]: s = pd.Series(np.arange(4)**2, index=np.arange(4))\n\nIn [4]: s\nOut[4]:\n0    0\n1    1\n2    4\n3    9\ndtype: int64\n\nIn [6]: id(s.index), id(s.values)\nOut[6]: (4470549648, 4470593296)\n', 'In [7]: s[2] = 14  \n\nIn [8]: id(s.index), id(s.values)\nOut[8]: (4470549648, 4470593296)\n', 'In [9]: s[4] = 16\n\nIn [10]: s\nOut[10]:\n0     0\n1     1\n2    14\n3     9\n4    16\ndtype: int64\n\nIn [11]: id(s.index), id(s.values)\nOut[11]: (4470548560, 4470595056)\n', 'In [13]: new_items = {item: item**2 for item in range(5, 7)}\n\nIn [14]: s2 = pd.Series(new_items)\n\nIn [15]: s2  # keys are guaranteed to be sorted!\nOut[15]:\n5    25\n6    36\ndtype: int64\n\nIn [16]: s = s.append(s2); s\nOut[16]:\n0     0\n1     1\n2    14\n3     9\n4    16\n5    25\n6    36\ndtype: int64\n']"
1508;;0;38025280;15943769.0;2;7;;;"<p>For getting rows use</p>

<pre><code>df.index
</code></pre>

<p>and for columns use</p>

<pre><code>df.columns
</code></pre>

<p>You can always use <code>len(func)</code> for getting the count of list, hence you can use
<code>len(df.index)</code> for getting the number of rows.</p>

<p>But keep in mind, as stated by @root, using <code>shape[0] and shape[1]</code> for getting the number of rows and columns, respectively, is a faster option.</p>
";;['df.index\n', 'df.columns\n'];['df.index\n', 'df.columns\n', 'len(func)', 'len(df.index)', 'shape[0] and shape[1]']
1509;;0;38034085;19125091.0;2;15;;;"<p>I use the <code>suffixes</code> option in <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html"" rel=""noreferrer""><code>.merge()</code></a>:</p>

<pre><code>dfNew = df.merge(df2, left_index=True, right_index=True,
                 how='outer', suffixes=('', '_y'))
</code></pre>

<p>You can then filter the columns based on the flag ""_y"", i.e. delete them.</p>
";;"[""dfNew = df.merge(df2, left_index=True, right_index=True,\n                 how='outer', suffixes=('', '_y'))\n""]";"['suffixes', '.merge()', ""dfNew = df.merge(df2, left_index=True, right_index=True,\n                 how='outer', suffixes=('', '_y'))\n""]"
1510;;0;38156594;21197774.0;2;6;;;"<p>Another way to set the column types is to first construct a numpy record array with your desired types, fill it out and then pass it to a DataFrame constructor.</p>

<pre><code>import pandas as pd
import numpy as np    

x = np.empty((10,), dtype=[('x', np.uint8), ('y', np.float64)])
df = pd.DataFrame(x)

df.dtypes -&gt;

x      uint8
y    float64
</code></pre>
";;"[""import pandas as pd\nimport numpy as np    \n\nx = np.empty((10,), dtype=[('x', np.uint8), ('y', np.float64)])\ndf = pd.DataFrame(x)\n\ndf.dtypes ->\n\nx      uint8\ny    float64\n""]";"[""import pandas as pd\nimport numpy as np    \n\nx = np.empty((10,), dtype=[('x', np.uint8), ('y', np.float64)])\ndf = pd.DataFrame(x)\n\ndf.dtypes ->\n\nx      uint8\ny    float64\n""]"
1511;;1;38251063;38250710.0;2;18;;;"<h3>Note:</h3>

<p>Function was written to handle seeding of randomized set creation.  You should not rely on set splitting that doesn't randomize the sets.</p>

<pre><code>import numpy as np
import pandas as pd

def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):
    np.random.seed(seed)
    perm = np.random.permutation(df.index)
    m = len(df)
    train_end = int(train_percent * m)
    validate_end = int(validate_percent * m) + train_end
    train = df.ix[perm[:train_end]]
    validate = df.ix[perm[train_end:validate_end]]
    test = df.ix[perm[validate_end:]]
    return train, validate, test
</code></pre>

<h3>Demonstration</h3>

<pre><code>np.random.seed([3,1415])
df = pd.DataFrame(np.random.rand(10, 5), columns=list('ABCDE'))
df
</code></pre>

<p><a href=""https://i.stack.imgur.com/ThpsQ.png""><img src=""https://i.stack.imgur.com/ThpsQ.png"" alt=""enter image description here""></a></p>

<pre><code>train, validate, test = train_validate_test_split(df)

train
</code></pre>

<p><a href=""https://i.stack.imgur.com/XNRBT.png""><img src=""https://i.stack.imgur.com/XNRBT.png"" alt=""enter image description here""></a></p>

<pre><code>validate
</code></pre>

<p><a href=""https://i.stack.imgur.com/PpyC8.png""><img src=""https://i.stack.imgur.com/PpyC8.png"" alt=""enter image description here""></a></p>

<pre><code>test
</code></pre>

<p><a href=""https://i.stack.imgur.com/U6CaT.png""><img src=""https://i.stack.imgur.com/U6CaT.png"" alt=""enter image description here""></a></p>
";;"['import numpy as np\nimport pandas as pd\n\ndef train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n    np.random.seed(seed)\n    perm = np.random.permutation(df.index)\n    m = len(df)\n    train_end = int(train_percent * m)\n    validate_end = int(validate_percent * m) + train_end\n    train = df.ix[perm[:train_end]]\n    validate = df.ix[perm[train_end:validate_end]]\n    test = df.ix[perm[validate_end:]]\n    return train, validate, test\n', ""np.random.seed([3,1415])\ndf = pd.DataFrame(np.random.rand(10, 5), columns=list('ABCDE'))\ndf\n"", 'train, validate, test = train_validate_test_split(df)\n\ntrain\n', 'validate\n', 'test\n']";"['import numpy as np\nimport pandas as pd\n\ndef train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n    np.random.seed(seed)\n    perm = np.random.permutation(df.index)\n    m = len(df)\n    train_end = int(train_percent * m)\n    validate_end = int(validate_percent * m) + train_end\n    train = df.ix[perm[:train_end]]\n    validate = df.ix[perm[train_end:validate_end]]\n    test = df.ix[perm[validate_end:]]\n    return train, validate, test\n', ""np.random.seed([3,1415])\ndf = pd.DataFrame(np.random.rand(10, 5), columns=list('ABCDE'))\ndf\n"", 'train, validate, test = train_validate_test_split(df)\n\ntrain\n', 'validate\n', 'test\n']"
1512;;9;38251213;38250710.0;2;35;;;"<p>Numpy solution (thanks to <a href=""https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-development-and-test/38251213?noredirect=1#comment63923795_38251213"">root</a> for the randomizing hint) - we will split our data set into the following parts: (60% - train set, 20% - validation set, 20% - test set):</p>

<pre><code>In [305]: train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])

In [306]: train
Out[306]:
          A         B         C         D         E
0  0.046919  0.792216  0.206294  0.440346  0.038960
2  0.301010  0.625697  0.604724  0.936968  0.870064
1  0.642237  0.690403  0.813658  0.525379  0.396053
9  0.488484  0.389640  0.599637  0.122919  0.106505
8  0.842717  0.793315  0.554084  0.100361  0.367465
7  0.185214  0.603661  0.217677  0.281780  0.938540

In [307]: validate
Out[307]:
          A         B         C         D         E
5  0.806176  0.008896  0.362878  0.058903  0.026328
6  0.145777  0.485765  0.589272  0.806329  0.703479

In [308]: test
Out[308]:
          A         B         C         D         E
4  0.521640  0.332210  0.370177  0.859169  0.401087
3  0.333348  0.964011  0.083498  0.670386  0.169619
</code></pre>

<p>PS [int(.6*len(df)), int(.8*len(df))] - is an <code>indices_or_sections</code> array for <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.split.html"" rel=""noreferrer"">numpy.split()</a></p>

<p>Here is a small demo for <code>np.split()</code> usage - let's split 20-elements array into the following parts: 90%, 10%, 10%:</p>

<pre><code>In [45]: a = np.arange(1, 21)

In [46]: a
Out[46]: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])

In [47]: np.split(a, [int(.8 * len(a)), int(.9 * len(a))])
Out[47]:
[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16]),
 array([17, 18]),
 array([19, 20])]
</code></pre>
";;['In [305]: train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n\nIn [306]: train\nOut[306]:\n          A         B         C         D         E\n0  0.046919  0.792216  0.206294  0.440346  0.038960\n2  0.301010  0.625697  0.604724  0.936968  0.870064\n1  0.642237  0.690403  0.813658  0.525379  0.396053\n9  0.488484  0.389640  0.599637  0.122919  0.106505\n8  0.842717  0.793315  0.554084  0.100361  0.367465\n7  0.185214  0.603661  0.217677  0.281780  0.938540\n\nIn [307]: validate\nOut[307]:\n          A         B         C         D         E\n5  0.806176  0.008896  0.362878  0.058903  0.026328\n6  0.145777  0.485765  0.589272  0.806329  0.703479\n\nIn [308]: test\nOut[308]:\n          A         B         C         D         E\n4  0.521640  0.332210  0.370177  0.859169  0.401087\n3  0.333348  0.964011  0.083498  0.670386  0.169619\n', 'In [45]: a = np.arange(1, 21)\n\nIn [46]: a\nOut[46]: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n\nIn [47]: np.split(a, [int(.8 * len(a)), int(.9 * len(a))])\nOut[47]:\n[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16]),\n array([17, 18]),\n array([19, 20])]\n'];['In [305]: train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n\nIn [306]: train\nOut[306]:\n          A         B         C         D         E\n0  0.046919  0.792216  0.206294  0.440346  0.038960\n2  0.301010  0.625697  0.604724  0.936968  0.870064\n1  0.642237  0.690403  0.813658  0.525379  0.396053\n9  0.488484  0.389640  0.599637  0.122919  0.106505\n8  0.842717  0.793315  0.554084  0.100361  0.367465\n7  0.185214  0.603661  0.217677  0.281780  0.938540\n\nIn [307]: validate\nOut[307]:\n          A         B         C         D         E\n5  0.806176  0.008896  0.362878  0.058903  0.026328\n6  0.145777  0.485765  0.589272  0.806329  0.703479\n\nIn [308]: test\nOut[308]:\n          A         B         C         D         E\n4  0.521640  0.332210  0.370177  0.859169  0.401087\n3  0.333348  0.964011  0.083498  0.670386  0.169619\n', 'indices_or_sections', 'np.split()', 'In [45]: a = np.arange(1, 21)\n\nIn [46]: a\nOut[46]: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n\nIn [47]: np.split(a, [int(.8 * len(a)), int(.9 * len(a))])\nOut[47]:\n[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16]),\n array([17, 18]),\n array([19, 20])]\n']
1513;;1;38278787;16729483.0;2;19;;;"<p>In a newer version of pandas (0.17 and up), you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html"" rel=""noreferrer"">to_numeric</a> function. It allows you to convert the whole dataframe or just individual columns. It also gives you an ability to select how to treat stuff that can't be converted to numeric values:</p>

<pre><code>import pandas as pd
s = pd.Series(['1.0', '2', -3])
pd.to_numeric(s)
s = pd.Series(['apple', '1.0', '2', -3])
pd.to_numeric(s, errors='ignore')
pd.to_numeric(s, errors='coerce')
</code></pre>
";;"[""import pandas as pd\ns = pd.Series(['1.0', '2', -3])\npd.to_numeric(s)\ns = pd.Series(['apple', '1.0', '2', -3])\npd.to_numeric(s, errors='ignore')\npd.to_numeric(s, errors='coerce')\n""]";"[""import pandas as pd\ns = pd.Series(['1.0', '2', -3])\npd.to_numeric(s)\ns = pd.Series(['apple', '1.0', '2', -3])\npd.to_numeric(s, errors='ignore')\npd.to_numeric(s, errors='coerce')\n""]"
1514;;0;38341066;29370057.0;2;9;;;"<p>I feel the best option will be to use the direct checks rather than using loc function:</p>

<pre><code>df = df[(df['date'] &gt; '2000-6-1') &amp; (df['date'] &lt;= '2000-6-10')]
</code></pre>

<p>It works for me.</p>

<p>Major issue with loc function with a slice is that the limits should be present in the actual values, if not this will result in KeyError.</p>
";;"[""df = df[(df['date'] > '2000-6-1') & (df['date'] <= '2000-6-10')]\n""]";"[""df = df[(df['date'] > '2000-6-1') & (df['date'] <= '2000-6-10')]\n""]"
1515;;0;38348167;10715965.0;2;8;;;"<p>This is not an answer to the OP question but a toy example to illustrate the answer of @ShikharDua above which I found very useful. </p>

<p>While this fragment is trivial, in the actual data I had 1,000s of rows, and many columns, and I wished to be able to group by different columns and then perform the stats below for more than one taget column. So having a reliable method for building the data frame one row at a time was a great convenience. Thank you @ShikharDua ! </p>

<pre><code>import pandas as pd 

BaseData = pd.DataFrame({ 'Customer' : ['Acme','Mega','Acme','Acme','Mega','Acme'],
                          'Territory'  : ['West','East','South','West','East','South'],
                          'Product'  : ['Econ','Luxe','Econ','Std','Std','Econ']})
BaseData

columns = ['Customer','Num Unique Products', 'List Unique Products']

rows_list=[]
for name, group in BaseData.groupby('Customer'):
    RecordtoAdd={} #initialise an empty dict 
    RecordtoAdd.update({'Customer' : name}) #
    RecordtoAdd.update({'Num Unique Products' : len(pd.unique(group['Product']))})      
    RecordtoAdd.update({'List Unique Products' : pd.unique(group['Product'])})                   

    rows_list.append(RecordtoAdd)

AnalysedData = pd.DataFrame(rows_list)

print('Base Data : \n',BaseData,'\n\n Analysed Data : \n',AnalysedData)
</code></pre>
";;"[""import pandas as pd \n\nBaseData = pd.DataFrame({ 'Customer' : ['Acme','Mega','Acme','Acme','Mega','Acme'],\n                          'Territory'  : ['West','East','South','West','East','South'],\n                          'Product'  : ['Econ','Luxe','Econ','Std','Std','Econ']})\nBaseData\n\ncolumns = ['Customer','Num Unique Products', 'List Unique Products']\n\nrows_list=[]\nfor name, group in BaseData.groupby('Customer'):\n    RecordtoAdd={} #initialise an empty dict \n    RecordtoAdd.update({'Customer' : name}) #\n    RecordtoAdd.update({'Num Unique Products' : len(pd.unique(group['Product']))})      \n    RecordtoAdd.update({'List Unique Products' : pd.unique(group['Product'])})                   \n\n    rows_list.append(RecordtoAdd)\n\nAnalysedData = pd.DataFrame(rows_list)\n\nprint('Base Data : \\n',BaseData,'\\n\\n Analysed Data : \\n',AnalysedData)\n""]";"[""import pandas as pd \n\nBaseData = pd.DataFrame({ 'Customer' : ['Acme','Mega','Acme','Acme','Mega','Acme'],\n                          'Territory'  : ['West','East','South','West','East','South'],\n                          'Product'  : ['Econ','Luxe','Econ','Std','Std','Econ']})\nBaseData\n\ncolumns = ['Customer','Num Unique Products', 'List Unique Products']\n\nrows_list=[]\nfor name, group in BaseData.groupby('Customer'):\n    RecordtoAdd={} #initialise an empty dict \n    RecordtoAdd.update({'Customer' : name}) #\n    RecordtoAdd.update({'Num Unique Products' : len(pd.unique(group['Product']))})      \n    RecordtoAdd.update({'List Unique Products' : pd.unique(group['Product'])})                   \n\n    rows_list.append(RecordtoAdd)\n\nAnalysedData = pd.DataFrame(rows_list)\n\nprint('Base Data : \\n',BaseData,'\\n\\n Analysed Data : \\n',AnalysedData)\n""]"
1516;;0;38421614;17095101.0;2;6;;;"<p>This answer simply extends @Andy Hayden's, making it resilient to when numeric fields are <code>nan</code>, and wrapping it up into a function.</p>

<pre><code>import pandas as pd
import numpy as np


def diff_pd(df1, df2):
    """"""Identify differences between two pandas DataFrames""""""
    assert (df1.columns == df2.columns).all(), \
        ""DataFrame column names are different""
    if df1.equals(df2):
        return None
    else:
        # need to account for np.nan != np.nan returning True
        diff_mask = (df1 != df2) &amp; ~(df1.isnull() &amp; df2.isnull())
        ne_stacked = diff_mask.stack()
        changed = ne_stacked[ne_stacked]
        changed.index.names = ['id', 'col']
        difference_locations = np.where(diff_mask)
        changed_from = df1.values[difference_locations]
        changed_to = df2.values[difference_locations]
        return pd.DataFrame({'from': changed_from, 'to': changed_to},
                            index=changed.index)
</code></pre>

<p>So with your data (slightly edited to have a NaN in the score column):</p>

<pre><code>import sys
if sys.version_info[0] &lt; 3:
    from StringIO import StringIO
else:
    from io import StringIO

DF1 = StringIO(""""""id   Name   score                    isEnrolled           Comment
111  Jack   2.17                     True                 ""He was late to class""
112  Nick   1.11                     False                ""Graduated""
113  Zoe    NaN                     True                  "" ""
"""""")
DF2 = StringIO(""""""id   Name   score                    isEnrolled           Comment
111  Jack   2.17                     True                 ""He was late to class""
112  Nick   1.21                     False                ""Graduated""
113  Zoe    NaN                     False                ""On vacation"" """""")
df1 = pd.read_table(DF1, sep='\s+', index_col='id')
df2 = pd.read_table(DF2, sep='\s+', index_col='id')
diff_pd(df1, df2)
</code></pre>

<p>Output:</p>

<pre><code>                from           to
id  col                          
112 score       1.11         1.21
113 isEnrolled  True        False
    Comment           On vacation
</code></pre>
";;"['import pandas as pd\nimport numpy as np\n\n\ndef diff_pd(df1, df2):\n    """"""Identify differences between two pandas DataFrames""""""\n    assert (df1.columns == df2.columns).all(), \\\n        ""DataFrame column names are different""\n    if df1.equals(df2):\n        return None\n    else:\n        # need to account for np.nan != np.nan returning True\n        diff_mask = (df1 != df2) & ~(df1.isnull() & df2.isnull())\n        ne_stacked = diff_mask.stack()\n        changed = ne_stacked[ne_stacked]\n        changed.index.names = [\'id\', \'col\']\n        difference_locations = np.where(diff_mask)\n        changed_from = df1.values[difference_locations]\n        changed_to = df2.values[difference_locations]\n        return pd.DataFrame({\'from\': changed_from, \'to\': changed_to},\n                            index=changed.index)\n', 'import sys\nif sys.version_info[0] < 3:\n    from StringIO import StringIO\nelse:\n    from io import StringIO\n\nDF1 = StringIO(""""""id   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 ""He was late to class""\n112  Nick   1.11                     False                ""Graduated""\n113  Zoe    NaN                     True                  "" ""\n"""""")\nDF2 = StringIO(""""""id   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 ""He was late to class""\n112  Nick   1.21                     False                ""Graduated""\n113  Zoe    NaN                     False                ""On vacation"" """""")\ndf1 = pd.read_table(DF1, sep=\'\\s+\', index_col=\'id\')\ndf2 = pd.read_table(DF2, sep=\'\\s+\', index_col=\'id\')\ndiff_pd(df1, df2)\n', '                from           to\nid  col                          \n112 score       1.11         1.21\n113 isEnrolled  True        False\n    Comment           On vacation\n']";"['nan', 'import pandas as pd\nimport numpy as np\n\n\ndef diff_pd(df1, df2):\n    """"""Identify differences between two pandas DataFrames""""""\n    assert (df1.columns == df2.columns).all(), \\\n        ""DataFrame column names are different""\n    if df1.equals(df2):\n        return None\n    else:\n        # need to account for np.nan != np.nan returning True\n        diff_mask = (df1 != df2) & ~(df1.isnull() & df2.isnull())\n        ne_stacked = diff_mask.stack()\n        changed = ne_stacked[ne_stacked]\n        changed.index.names = [\'id\', \'col\']\n        difference_locations = np.where(diff_mask)\n        changed_from = df1.values[difference_locations]\n        changed_to = df2.values[difference_locations]\n        return pd.DataFrame({\'from\': changed_from, \'to\': changed_to},\n                            index=changed.index)\n', 'import sys\nif sys.version_info[0] < 3:\n    from StringIO import StringIO\nelse:\n    from io import StringIO\n\nDF1 = StringIO(""""""id   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 ""He was late to class""\n112  Nick   1.11                     False                ""Graduated""\n113  Zoe    NaN                     True                  "" ""\n"""""")\nDF2 = StringIO(""""""id   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 ""He was late to class""\n112  Nick   1.21                     False                ""Graduated""\n113  Zoe    NaN                     False                ""On vacation"" """""")\ndf1 = pd.read_table(DF1, sep=\'\\s+\', index_col=\'id\')\ndf2 = pd.read_table(DF2, sep=\'\\s+\', index_col=\'id\')\ndiff_pd(df1, df2)\n', '                from           to\nid  col                          \n112 score       1.11         1.21\n113 isEnrolled  True        False\n    Comment           On vacation\n']"
1517;;0;38466059;20109391.0;2;12;;;"<h1>Diary of an Answerer</h1>

<p>My best advice for asking questions would be to play on the psychology of the people who answer questions.  Being one of those people, I can give insight into why I answer certain questions and why I don't answer others.</p>

<h3>Motivations</h3>

<p>I'm motivated to answer questions for several reasons</p>

<ol>
<li>Stackoverflow.com has been a tremendously valuable resource to me.  I wanted to give back.</li>
<li>In my efforts to give back, I've found this site to be an even more powerful resource than before.  Answering questions is a learning experience for me and I like to learn.  <a href=""https://stackoverflow.com/a/38444638/2336654"">Read this answer and comment from another vet</a>.  This kind of interaction makes me happy.</li>
<li>I like points!</li>
<li>See #3.</li>
<li>I like interesting problems.</li>
</ol>

<p>All my purest intentions are great and all, but I get that satisfaction if I answer 1 question or 30.  <strong>What drives my choices</strong> for which questions to answer has a huge component of point maximization.</p>

<p>I'll also spend time on interesting problems but that is few and far between and doesn't help an asker who needs a solution to a non-interesting question.  Your best bet to get me to answer a question is to serve that question up on a platter ripe for me to answer it with as little effort as possible.  If I'm looking at two questions and one has code I can copy paste to create all the variables I need... I'm taking that one!  I'll come back to the other one if I have time, maybe.</p>

<h3>Main Advice</h3>

<p>Make it easy for the people answering questions.</p>

<ul>
<li>Provide code that creates variables that are needed.</li>
<li>Minimize that code.  If my eyes glaze over as I look at the post, I'm on to the next question or getting back to whatever else I'm doing.</li>
<li>Think about what your asking and be specific.  We want to see what you've done because natural languages (English) are inexact and confusing.  Code samples of what you've tried help resolve inconsistencies in a natural language description.</li>
<li>PLEASE show what you expect!!!  I have to sit down and try things.  I almost never know the answer to a question without trying some things out.  If I don't see an example of what you're looking for, I might pass on the question because I don't feel like guessing.</li>
</ul>

<h3>Your reputation is more than just your reputation.</h3>

<p>I like points (I mentioned that above).  But those points aren't really really my reputation.  My real reputation is an amalgamation of what others on the site think of me.  I strive to be fair and honest and I hope others can see that.  What that means for an asker is, we remember the behaviors of askers.  If you don't select answers and upvote good answers, I remember.  If you behave in ways I don't like or in ways I do like, I remember.  This also plays into which questions I'll answer.</p>

<hr>

<p>Anyway, I can probably go on, but I'll spare all of you who actually read this.</p>
";;[];[]
1518;;1;38467449;13842088.0;2;17;;;"<p>You can also use a conditional lookup using <code>.loc</code> as seen here:</p>

<pre><code>df.loc[df[&lt;some_column_name&gt;] == &lt;condition&gt;, &lt;another_column_name&gt;] = &lt;value_to_add&gt;
</code></pre>

<p>where <code>&lt;some_column_name</code> is the column you want to check the <code>&lt;condition&gt;</code> variable against and <code>&lt;another_column_name&gt;</code> is the column you want to add to (can be a new column or one that already exists). <code>&lt;value_to_add&gt;</code> is the value you want to add to that column/row.</p>

<p>This example doesn't work precisely with the question at hand, but it might be useful for someone wants to add a specific value based on a condition.</p>
";;['df.loc[df[<some_column_name>] == <condition>, <another_column_name>] = <value_to_add>\n'];['.loc', 'df.loc[df[<some_column_name>] == <condition>, <another_column_name>] = <value_to_add>\n', '<some_column_name', '<condition>', '<another_column_name>', '<value_to_add>']
1519;;0;38503561;17679089.0;2;6;;;"<blockquote>
  <blockquote>
    <p>Inserting <strong>data</strong> into panadas dataframe and <strong>providing coloumn name</strong>.   </p>
  </blockquote>
</blockquote>

<pre><code>import pandas as pd
df = pd.DataFrame([['A','C','A','B','C','A','B','B','A','A'], ['ONE','TWO','ONE','ONE','ONE','TWO','ONE','TWO','ONE','THREE']]).T
df.columns = [['Alphabet','Words']]
print(df)   #printing dataframe.
</code></pre>

<p><strong><em>This is our printed data:</em></strong>
<br><a href=""https://i.stack.imgur.com/BuKLt.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/BuKLt.png"" alt=""enter image description here""></a></p>

<p></p>

<blockquote>
  <blockquote>
    <p>For making a <strong><em>group of dataframe in pandas and counter</em></strong>, <br>
    You need to provide one more coloumn which counts the grouping, <strong>lets call that coloumn as, <em>""COUNTER""</em> in  dataframe</strong>.</p>
    
    <p>Like this:</p>
  </blockquote>
</blockquote>

<pre><code>df['COUNTER'] =1       #initially, set that counter to 1.
group_data = df.groupby(['Alphabet','Words'])['COUNTER'].sum() #sum function
print(group_data)
</code></pre>

<blockquote>
  <p><strong>OUTPUT:</strong></p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/EpdTH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/EpdTH.png"" alt=""enter image description here""></a></p>

<blockquote>
  <blockquote>
    <p><strong><em>HOPE! IT HELPS</em></strong></p>
  </blockquote>
</blockquote>
";;"[""import pandas as pd\ndf = pd.DataFrame([['A','C','A','B','C','A','B','B','A','A'], ['ONE','TWO','ONE','ONE','ONE','TWO','ONE','TWO','ONE','THREE']]).T\ndf.columns = [['Alphabet','Words']]\nprint(df)   #printing dataframe.\n"", ""df['COUNTER'] =1       #initially, set that counter to 1.\ngroup_data = df.groupby(['Alphabet','Words'])['COUNTER'].sum() #sum function\nprint(group_data)\n""]";"[""import pandas as pd\ndf = pd.DataFrame([['A','C','A','B','C','A','B','B','A','A'], ['ONE','TWO','ONE','ONE','ONE','TWO','ONE','TWO','ONE','THREE']]).T\ndf.columns = [['Alphabet','Words']]\nprint(df)   #printing dataframe.\n"", ""df['COUNTER'] =1       #initially, set that counter to 1.\ngroup_data = df.groupby(['Alphabet','Words'])['COUNTER'].sum() #sum function\nprint(group_data)\n""]"
1520;;0;38510820;12555323.0;2;21;;;"<p>It seems that in recent Pandas versions the way to go is to use <a href=""http://pandas.pydata.org/pandas-docs/stable/dsintro.html#assigning-new-columns-in-method-chains"" rel=""nofollow noreferrer"">df.assign</a>:</p>

<p><code>df1 = df1.assign(e=np.random.randn(sLength))</code></p>

<p>It doesn't produce SettingWithCopyWarning.</p>
";;[];['df1 = df1.assign(e=np.random.randn(sLength))']
1521;;1;38544742;26473681.0;2;8;;;"<p>Try updating pip:</p>

<pre><code>pip install -U pip
</code></pre>
";;['pip install -U pip\n'];['pip install -U pip\n']
1522;;0;38709267;11927715.0;2;17;;;"<p>I found the easiest way is to use the <code>colormap</code> parameter in <code>.plot()</code> with one of the preset color gradients:</p>

<pre><code>df.plot(kind='bar', stacked=True, colormap='Paired')
</code></pre>

<p><a href=""https://i.stack.imgur.com/7BOTc.png""><img src=""https://i.stack.imgur.com/7BOTc.png"" alt=""enter image description here""></a></p>

<p>You can find a large <a href=""http://scipy.github.io/old-wiki/pages/Cookbook/Matplotlib/Show_colormaps"">list of preset colormaps here</a>.</p>

<p><a href=""https://i.stack.imgur.com/i5XZo.png""><img src=""https://i.stack.imgur.com/i5XZo.png"" alt=""colormaps""></a></p>
";;"[""df.plot(kind='bar', stacked=True, colormap='Paired')\n""]";"['colormap', '.plot()', ""df.plot(kind='bar', stacked=True, colormap='Paired')\n""]"
1523;;0;38776854;11346283.0;2;7;;;"<p>I know this question and answer has been chewed to death. But I referred to it for inspiration for one of the problem I was having . I was able to solve it using bits and pieces from different answers hence providing my response in case anyone needs it.</p>

<p>My method is generic wherein you can add additional delimiters by comma separating <code>delimiters=</code> variable and future-proof it.</p>

<p><strong>Working Code:</strong></p>

<pre><code>import pandas as pd
import re


df = pd.DataFrame({'$a':[1,2], '$b': [3,4],'$c':[5,6], '$d': [7,8], '$e': [9,10]})

delimiters = '$'
matchPattern = '|'.join(map(re.escape, delimiters))
df.columns = [re.split(matchPattern, i)[1] for i in df.columns ]
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>&gt;&gt;&gt; df
   $a  $b  $c  $d  $e
0   1   3   5   7   9
1   2   4   6   8  10

&gt;&gt;&gt; df
   a  b  c  d   e
0  1  3  5  7   9
1  2  4  6  8  10
</code></pre>
";;"[""import pandas as pd\nimport re\n\n\ndf = pd.DataFrame({'$a':[1,2], '$b': [3,4],'$c':[5,6], '$d': [7,8], '$e': [9,10]})\n\ndelimiters = '$'\nmatchPattern = '|'.join(map(re.escape, delimiters))\ndf.columns = [re.split(matchPattern, i)[1] for i in df.columns ]\n"", '>>> df\n   $a  $b  $c  $d  $e\n0   1   3   5   7   9\n1   2   4   6   8  10\n\n>>> df\n   a  b  c  d   e\n0  1  3  5  7   9\n1  2  4  6  8  10\n']";"['delimiters=', ""import pandas as pd\nimport re\n\n\ndf = pd.DataFrame({'$a':[1,2], '$b': [3,4],'$c':[5,6], '$d': [7,8], '$e': [9,10]})\n\ndelimiters = '$'\nmatchPattern = '|'.join(map(re.escape, delimiters))\ndf.columns = [re.split(matchPattern, i)[1] for i in df.columns ]\n"", '>>> df\n   $a  $b  $c  $d  $e\n0   1   3   5   7   9\n1   2   4   6   8  10\n\n>>> df\n   a  b  c  d   e\n0  1  3  5  7   9\n1  2  4  6  8  10\n']"
1524;;0;38900352;19798153.0;2;7;;;"<p>There's great information in these answers, but I'm adding my own to clearly summarize which methods work array-wise versus element-wise. jeremiahbuddha mostly did this but did not mention Series.apply.  I don't have the rep to comment.</p>

<ul>
<li><p><code>DataFrame.apply</code> operates on entire rows or columns at a time.</p></li>
<li><p><code>DataFrame.applymap</code>, <code>Series.apply</code>, and <code>Series.map</code> operate on one
element at time.</p></li>
</ul>

<p>There is a lot of overlap between the capabilities of <code>Series.apply</code> and <code>Series.map</code>, meaning that either one will work in most cases.  They do have some slight differences though, some of which were discussed in osa's answer.</p>
";;[];['DataFrame.apply', 'DataFrame.applymap', 'Series.apply', 'Series.map', 'Series.apply', 'Series.map']
1525;;0;38902835;15772009.0;2;8;;;"<p>You can use  <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html"" rel=""noreferrer""><code>sklearn.utils.shuffle()</code></a> (<a href=""https://github.com/scikit-learn/scikit-learn/issues/4008"" rel=""noreferrer"">requires</a> sklearn 0.16.1 or higher to support Pandas data frames):</p>

<pre><code># Generate data
import pandas as pd
df = pd.DataFrame({'A':range(5), 'B':range(5)})
print('df: {0}'.format(df))

# Shuffle Pandas data frame
import sklearn.utils
df = sklearn.utils.shuffle(df)
print('\n\ndf: {0}'.format(df))
</code></pre>

<p>outputs:</p>

<pre><code>df:    A  B
0  0  0
1  1  1
2  2  2
3  3  3
4  4  4


df:    A  B
1  1  1
0  0  0
3  3  3
4  4  4
2  2  2
</code></pre>

<p>Then you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html"" rel=""noreferrer""><code>df.reset_index()</code></a> to reset the index column, if needs to be:</p>

<pre><code>df = df.reset_index(drop=True)
print('\n\ndf: {0}'.format(df)
</code></pre>

<p>outputs:</p>

<pre><code>df:    A  B
0  1  1
1  0  0
2  4  4
3  2  2
4  3  3
</code></pre>
";;"[""# Generate data\nimport pandas as pd\ndf = pd.DataFrame({'A':range(5), 'B':range(5)})\nprint('df: {0}'.format(df))\n\n# Shuffle Pandas data frame\nimport sklearn.utils\ndf = sklearn.utils.shuffle(df)\nprint('\\n\\ndf: {0}'.format(df))\n"", 'df:    A  B\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n4  4  4\n\n\ndf:    A  B\n1  1  1\n0  0  0\n3  3  3\n4  4  4\n2  2  2\n', ""df = df.reset_index(drop=True)\nprint('\\n\\ndf: {0}'.format(df)\n"", 'df:    A  B\n0  1  1\n1  0  0\n2  4  4\n3  2  2\n4  3  3\n']";"['sklearn.utils.shuffle()', ""# Generate data\nimport pandas as pd\ndf = pd.DataFrame({'A':range(5), 'B':range(5)})\nprint('df: {0}'.format(df))\n\n# Shuffle Pandas data frame\nimport sklearn.utils\ndf = sklearn.utils.shuffle(df)\nprint('\\n\\ndf: {0}'.format(df))\n"", 'df:    A  B\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n4  4  4\n\n\ndf:    A  B\n1  1  1\n0  0  0\n3  3  3\n4  4  4\n2  2  2\n', 'df.reset_index()', ""df = df.reset_index(drop=True)\nprint('\\n\\ndf: {0}'.format(df)\n"", 'df:    A  B\n0  1  1\n1  0  0\n2  4  4\n3  2  2\n4  3  3\n']"
1526;;0;39104306;17627219.0;2;19;;;"<p>You can compute pairwise cosine similarity on the rows of a sparse matrix directly using sklearn.  As of version 0.17 it also supports sparse output:</p>

<pre><code>from sklearn.metrics.pairwise import cosine_similarity
from scipy import sparse

A =  np.array([[0, 1, 0, 0, 1], [0, 0, 1, 1, 1],[1, 1, 0, 1, 0]])
A_sparse = sparse.csr_matrix(A)

similarities = cosine_similarity(A_sparse)
print('pairwise dense output:\n {}\n'.format(similarities))

#also can output sparse matrices
similarities_sparse = cosine_similarity(A_sparse,dense_output=False)
print('pairwise sparse output:\n {}\n'.format(similarities_sparse))
</code></pre>

<p>Results:</p>

<pre><code>pairwise dense output:
[[ 1.          0.40824829  0.40824829]
[ 0.40824829  1.          0.33333333]
[ 0.40824829  0.33333333  1.        ]]

pairwise sparse output:
(0, 1)  0.408248290464
(0, 2)  0.408248290464
(0, 0)  1.0
(1, 0)  0.408248290464
(1, 2)  0.333333333333
(1, 1)  1.0
(2, 1)  0.333333333333
(2, 0)  0.408248290464
(2, 2)  1.0
</code></pre>

<p>If you want column-wise cosine similarities simply transpose your input matrix beforehand: </p>

<pre><code>A_sparse.transpose()
</code></pre>
";;"[""from sklearn.metrics.pairwise import cosine_similarity\nfrom scipy import sparse\n\nA =  np.array([[0, 1, 0, 0, 1], [0, 0, 1, 1, 1],[1, 1, 0, 1, 0]])\nA_sparse = sparse.csr_matrix(A)\n\nsimilarities = cosine_similarity(A_sparse)\nprint('pairwise dense output:\\n {}\\n'.format(similarities))\n\n#also can output sparse matrices\nsimilarities_sparse = cosine_similarity(A_sparse,dense_output=False)\nprint('pairwise sparse output:\\n {}\\n'.format(similarities_sparse))\n"", 'pairwise dense output:\n[[ 1.          0.40824829  0.40824829]\n[ 0.40824829  1.          0.33333333]\n[ 0.40824829  0.33333333  1.        ]]\n\npairwise sparse output:\n(0, 1)  0.408248290464\n(0, 2)  0.408248290464\n(0, 0)  1.0\n(1, 0)  0.408248290464\n(1, 2)  0.333333333333\n(1, 1)  1.0\n(2, 1)  0.333333333333\n(2, 0)  0.408248290464\n(2, 2)  1.0\n', 'A_sparse.transpose()\n']";"[""from sklearn.metrics.pairwise import cosine_similarity\nfrom scipy import sparse\n\nA =  np.array([[0, 1, 0, 0, 1], [0, 0, 1, 1, 1],[1, 1, 0, 1, 0]])\nA_sparse = sparse.csr_matrix(A)\n\nsimilarities = cosine_similarity(A_sparse)\nprint('pairwise dense output:\\n {}\\n'.format(similarities))\n\n#also can output sparse matrices\nsimilarities_sparse = cosine_similarity(A_sparse,dense_output=False)\nprint('pairwise sparse output:\\n {}\\n'.format(similarities_sparse))\n"", 'pairwise dense output:\n[[ 1.          0.40824829  0.40824829]\n[ 0.40824829  1.          0.33333333]\n[ 0.40824829  0.33333333  1.        ]]\n\npairwise sparse output:\n(0, 1)  0.408248290464\n(0, 2)  0.408248290464\n(0, 0)  1.0\n(1, 0)  0.408248290464\n(1, 2)  0.333333333333\n(1, 1)  1.0\n(2, 1)  0.333333333333\n(2, 0)  0.408248290464\n(2, 2)  1.0\n', 'A_sparse.transpose()\n']"
1527;;4;39116381;24193174.0;2;6;;;"<p>As the answer given by @pelson uses <code>set_color_cycle</code> and this is deprecated in Matplotlib 1.5, I thought it would be useful to have an updated version of his solution using <code>set_prop_cycle</code>:</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np

for i in range(3):
    plt.plot(np.arange(10) + i)

plt.gca().set_prop_cycle(None)

for i in range(3):
    plt.plot(np.arange(10, 0, -1) + i)

plt.show()
</code></pre>

<p>Remark also that I had to change <code>np.arange(10,1,-1)</code> to <code>np.arange(10,0,-1)</code>. The former gave an array of only 9 elements. This probably arises from using different Numpy versions. Mine is 1.10.2.</p>

<p><strong>EDIT</strong>: Removed the need to use <code>rcParams</code>. Thanks to @divenex for pointing that out in a comment.</p>
";;['import matplotlib.pyplot as plt\nimport numpy as np\n\nfor i in range(3):\n    plt.plot(np.arange(10) + i)\n\nplt.gca().set_prop_cycle(None)\n\nfor i in range(3):\n    plt.plot(np.arange(10, 0, -1) + i)\n\nplt.show()\n'];['set_color_cycle', 'set_prop_cycle', 'import matplotlib.pyplot as plt\nimport numpy as np\n\nfor i in range(3):\n    plt.plot(np.arange(10) + i)\n\nplt.gca().set_prop_cycle(None)\n\nfor i in range(3):\n    plt.plot(np.arange(10, 0, -1) + i)\n\nplt.show()\n', 'np.arange(10,1,-1)', 'np.arange(10,0,-1)', 'rcParams']
1528;;1;39192113;29763620.0;2;9;;;"<pre><code>df[df.columns.difference(['b'])]

Out: 
          a         c         d
0  0.427809  0.459807  0.333869
1  0.678031  0.668346  0.645951
2  0.996573  0.673730  0.314911
3  0.786942  0.719665  0.330833
</code></pre>
";;"[""df[df.columns.difference(['b'])]\n\nOut: \n          a         c         d\n0  0.427809  0.459807  0.333869\n1  0.678031  0.668346  0.645951\n2  0.996573  0.673730  0.314911\n3  0.786942  0.719665  0.330833\n""]";"[""df[df.columns.difference(['b'])]\n\nOut: \n          a         c         d\n0  0.427809  0.459807  0.333869\n1  0.678031  0.668346  0.645951\n2  0.996573  0.673730  0.314911\n3  0.786942  0.719665  0.330833\n""]"
1529;;0;39206377;25146121.0;2;13;;;"<p><strong>update</strong> - Look for another answer above from me titled <strong>Best way Found</strong>. It's really the best way :)</p>

<p>if you want the month year unique pair, using apply is pretty sleek.</p>

<pre><code>    df['mnth_yr'] = df['date_column'].apply(lambda x: x.strftime('%B-%Y'))     
</code></pre>

<p>outputs month-year in one column.</p>

<p>don't forget to first change the format to date-time before, I generally forget :|</p>

<pre><code>    df['date_column'] = pd.to_datetime(df['date_column'])
</code></pre>
";;"[""    df['mnth_yr'] = df['date_column'].apply(lambda x: x.strftime('%B-%Y'))     \n"", ""    df['date_column'] = pd.to_datetime(df['date_column'])\n""]";"[""    df['mnth_yr'] = df['date_column'].apply(lambda x: x.strftime('%B-%Y'))     \n"", ""    df['date_column'] = pd.to_datetime(df['date_column'])\n""]"
1530;;0;39237712;13148429.0;2;19;;;"<p>In your case,</p>

<pre><code>df = df.reindex_axis(['mean',0,1,2,3,4], axis=1)
</code></pre>

<p>will do exactly what you what.</p>

<p>In my case (general form):</p>

<pre><code>df = df.reindex_axis(sorted(df.columns), axis=1)
df = df.reindex_axis(['opened'] + list([a for a in df.columns if a != 'opened']), axis=1)
</code></pre>
";;"[""df = df.reindex_axis(['mean',0,1,2,3,4], axis=1)\n"", ""df = df.reindex_axis(sorted(df.columns), axis=1)\ndf = df.reindex_axis(['opened'] + list([a for a in df.columns if a != 'opened']), axis=1)\n""]";"[""df = df.reindex_axis(['mean',0,1,2,3,4], axis=1)\n"", ""df = df.reindex_axis(sorted(df.columns), axis=1)\ndf = df.reindex_axis(['opened'] + list([a for a in df.columns if a != 'opened']), axis=1)\n""]"
1531;;0;39246607;17618981.0;2;7;;;"<p>The dataframe.sort() method is - so my understanding - deprecated in pandas > 0.18. In order to solve your problem you should use dataframe.sort_values() instead:</p>

<pre><code>f.sort_values(by=[""c1"",""c2""], ascending=[False, True])
</code></pre>

<p>The output looks like this: </p>

<pre><code>    c1  c2
    3   10
    2   15
    2   30
    2   100
    1   20
</code></pre>
";;"['f.sort_values(by=[""c1"",""c2""], ascending=[False, True])\n', '    c1  c2\n    3   10\n    2   15\n    2   30\n    2   100\n    1   20\n']";"['f.sort_values(by=[""c1"",""c2""], ascending=[False, True])\n', '    c1  c2\n    3   10\n    2   15\n    2   30\n    2   100\n    1   20\n']"
1532;;0;39251401;21654635.0;2;13;;;"<p>This is simple to do with <a href=""https://stanford.edu/~mwaskom/software/seaborn/index.html"" rel=""noreferrer"">Seaborn</a> (<code>pip install seaborn</code>) as a oneliner </p>

<p><code>sns.pairplot(x_vars=[""one""], y_vars=[""two""], data=df, hue=""key1"", size=5)</code>
:</p>

<pre><code>import seaborn as sns
import pandas as pd
import numpy as np
np.random.seed(1974)

df = pd.DataFrame(
    np.random.normal(10, 1, 30).reshape(10, 3),
    index=pd.date_range('2010-01-01', freq='M', periods=10),
    columns=('one', 'two', 'three'))
df['key1'] = (4, 4, 4, 6, 6, 6, 8, 8, 8, 8)

sns.pairplot(x_vars=[""one""], y_vars=[""two""], data=df, hue=""key1"", size=5)
</code></pre>

<p><a href=""https://i.stack.imgur.com/sDrhW.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/sDrhW.png"" alt=""enter image description here""></a></p>

<p>Here is the dataframe for reference:</p>

<p><a href=""https://i.stack.imgur.com/V7XYk.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/V7XYk.png"" alt=""enter image description here""></a></p>

<p>Since you have three variable columns in your data, you may want to plot all pairwise dimensions with:</p>

<pre><code>sns.pairplot(vars=[""one"",""two"",""three""], data=df, hue=""key1"", size=5)
</code></pre>

<p><a href=""https://i.stack.imgur.com/zOWFk.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zOWFk.png"" alt=""enter image description here""></a></p>
";;"['import seaborn as sns\nimport pandas as pd\nimport numpy as np\nnp.random.seed(1974)\n\ndf = pd.DataFrame(\n    np.random.normal(10, 1, 30).reshape(10, 3),\n    index=pd.date_range(\'2010-01-01\', freq=\'M\', periods=10),\n    columns=(\'one\', \'two\', \'three\'))\ndf[\'key1\'] = (4, 4, 4, 6, 6, 6, 8, 8, 8, 8)\n\nsns.pairplot(x_vars=[""one""], y_vars=[""two""], data=df, hue=""key1"", size=5)\n', 'sns.pairplot(vars=[""one"",""two"",""three""], data=df, hue=""key1"", size=5)\n']";"['pip install seaborn', 'sns.pairplot(x_vars=[""one""], y_vars=[""two""], data=df, hue=""key1"", size=5)', 'import seaborn as sns\nimport pandas as pd\nimport numpy as np\nnp.random.seed(1974)\n\ndf = pd.DataFrame(\n    np.random.normal(10, 1, 30).reshape(10, 3),\n    index=pd.date_range(\'2010-01-01\', freq=\'M\', periods=10),\n    columns=(\'one\', \'two\', \'three\'))\ndf[\'key1\'] = (4, 4, 4, 6, 6, 6, 8, 8, 8, 8)\n\nsns.pairplot(x_vars=[""one""], y_vars=[""two""], data=df, hue=""key1"", size=5)\n', 'sns.pairplot(vars=[""one"",""two"",""three""], data=df, hue=""key1"", size=5)\n']"
1533;;1;39259437;13331698.0;2;12;;;"<p>A simple solution is:</p>

<pre><code>df['col_3'] = df[['col_1','col_2']].apply(lambda x: f(*x), axis=1)
</code></pre>
";;"[""df['col_3'] = df[['col_1','col_2']].apply(lambda x: f(*x), axis=1)\n""]";"[""df['col_3'] = df[['col_1','col_2']].apply(lambda x: f(*x), axis=1)\n""]"
1534;;2;39358752;19726663.0;2;8;;;"<p>I'd use matplotlib table functionality, with some additional styling:</p>

<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import six

df = pd.DataFrame()
df['date'] = ['2016-04-01', '2016-04-02', '2016-04-03']
df['calories'] = [2200, 2100, 1500]
df['sleep hours'] = [2200, 2100, 1500]
df['gym'] = [True, False, False]

def render_mpl_table(data, col_width=3.0, row_height=0.625, font_size=14,
                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',
                     bbox=[0, 0, 1, 1], header_columns=0,
                     ax=None, **kwargs):
    if ax is None:
        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])
        fig, ax = plt.subplots(figsize=size)
        ax.axis('off')

    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)

    mpl_table.auto_set_font_size(False)
    mpl_table.set_fontsize(font_size)

    for k, cell in six.iteritems(mpl_table._cells):
        cell.set_edgecolor(edge_color)
        if k[0] == 0 or k[1] &lt; header_columns:
            cell.set_text_props(weight='bold', color='w')
            cell.set_facecolor(header_color)
        else:
            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])
    return ax

render_mpl_table(df, header_columns=0, col_width=2.0)
</code></pre>

<p><a href=""https://i.stack.imgur.com/F5xoF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F5xoF.png"" alt=""enter image description here""></a></p>
";;"[""import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport six\n\ndf = pd.DataFrame()\ndf['date'] = ['2016-04-01', '2016-04-02', '2016-04-03']\ndf['calories'] = [2200, 2100, 1500]\ndf['sleep hours'] = [2200, 2100, 1500]\ndf['gym'] = [True, False, False]\n\ndef render_mpl_table(data, col_width=3.0, row_height=0.625, font_size=14,\n                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n                     bbox=[0, 0, 1, 1], header_columns=0,\n                     ax=None, **kwargs):\n    if ax is None:\n        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n        fig, ax = plt.subplots(figsize=size)\n        ax.axis('off')\n\n    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n\n    mpl_table.auto_set_font_size(False)\n    mpl_table.set_fontsize(font_size)\n\n    for k, cell in six.iteritems(mpl_table._cells):\n        cell.set_edgecolor(edge_color)\n        if k[0] == 0 or k[1] < header_columns:\n            cell.set_text_props(weight='bold', color='w')\n            cell.set_facecolor(header_color)\n        else:\n            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n    return ax\n\nrender_mpl_table(df, header_columns=0, col_width=2.0)\n""]";"[""import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport six\n\ndf = pd.DataFrame()\ndf['date'] = ['2016-04-01', '2016-04-02', '2016-04-03']\ndf['calories'] = [2200, 2100, 1500]\ndf['sleep hours'] = [2200, 2100, 1500]\ndf['gym'] = [True, False, False]\n\ndef render_mpl_table(data, col_width=3.0, row_height=0.625, font_size=14,\n                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n                     bbox=[0, 0, 1, 1], header_columns=0,\n                     ax=None, **kwargs):\n    if ax is None:\n        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n        fig, ax = plt.subplots(figsize=size)\n        ax.axis('off')\n\n    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n\n    mpl_table.auto_set_font_size(False)\n    mpl_table.set_fontsize(font_size)\n\n    for k, cell in six.iteritems(mpl_table._cells):\n        cell.set_edgecolor(edge_color)\n        if k[0] == 0 or k[1] < header_columns:\n            cell.set_text_props(weight='bold', color='w')\n            cell.set_facecolor(header_color)\n        else:\n            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n    return ax\n\nrender_mpl_table(df, header_columns=0, col_width=2.0)\n""]"
1535;;4;39358924;14745022.0;2;81;;;"<h1>TL;DR version:</h1>

<p>For the simple case of:</p>

<ul>
<li>I have a text column with a delimiter and I want two columns</li>
</ul>

<p>The simplest solution is:</p>

<pre><code>df['A'], df['B'] = df['AB'].str.split(' ', 1).str
</code></pre>

<p>Or you can create create a DataFrame with one column for each entry of the split automatically with:</p>

<pre><code>df['AB'].str.split(' ', 1, expand=True)
</code></pre>

<p>Notice how, in either case, the <code>.tolist()</code> method is not necessary. Neither is <code>zip()</code>.</p>

<h1>In detail:</h1>

<p><a href=""https://stackoverflow.com/a/21296915/1273938"">Andy Hayden's solution</a> is most excellent in demonstrating the power of the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.extract.html"" rel=""noreferrer""><code>str.extract()</code></a> method.</p>

<p>But for a simple split over a known separator (like, splitting by dashes, or splitting by whitespace), the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.split.html"" rel=""noreferrer""><code>.str.split()</code></a> method is enough<sup>1</sup>. It operates on a column (Series) of strings, and returns a column (Series) of lists:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame({'AB': ['A1-B1', 'A2-B2']})
&gt;&gt;&gt; df

      AB
0  A1-B1
1  A2-B2
&gt;&gt;&gt; df['AB_split'] = df['AB'].str.split('-')
&gt;&gt;&gt; df

      AB  AB_split
0  A1-B1  [A1, B1]
1  A2-B2  [A2, B2]
</code></pre>

<p><sub>1: If you're unsure what the first two parameters of <code>.str.split()</code> do,
 I recommend the docs for the <a href=""https://docs.python.org/3.6/library/stdtypes.html#str.split"" rel=""noreferrer"">plain Python version of the method</a>.</sub></p>

<p>But how do you go from:</p>

<ul>
<li>a column containing two-element lists</li>
</ul>

<p>to:</p>

<ul>
<li>two columns, each containing the respective element of the lists?</li>
</ul>

<p>Well, we need to take a closer look at the <code>.str</code> attribute of a column.</p>

<p>It's a magical object that is used to collect methods that treat each element in a column as a string, and then apply the respective method in each element as efficient as possible:</p>

<pre><code>&gt;&gt;&gt; upper_lower_df = pd.DataFrame({""U"": [""A"", ""B"", ""C""]})
&gt;&gt;&gt; upper_lower_df

   U
0  A
1  B
2  C
&gt;&gt;&gt; upper_lower_df[""L""] = upper_lower_df[""U""].str.lower()
&gt;&gt;&gt; upper_lower_df

   U  L
0  A  a
1  B  b
2  C  c
</code></pre>

<p>But it also has an ""indexing"" interface for getting each element of a string by its index:</p>

<pre><code>&gt;&gt;&gt; df['AB'].str[0]

0    A
1    A
Name: AB, dtype: object

&gt;&gt;&gt; df['AB'].str[1]

0    1
1    2
Name: AB, dtype: object
</code></pre>

<p>Of course, this indexing interface of <code>.str</code> doesn't really care if each element it's indexing is actually a string, as long as it can be indexed, so:</p>

<pre><code>&gt;&gt;&gt; df['AB'].str.split('-', 1).str[0]

0    A1
1    A2
Name: AB, dtype: object

&gt;&gt;&gt; df['AB'].str.split('-', 1).str[1]

0    B1
1    B2
Name: AB, dtype: object
</code></pre>

<p>Then, it's a simple matter of taking advantage of the Python tuple unpacking of iterables to do</p>

<pre><code>&gt;&gt;&gt; df['A'], df['B'] = df['AB'].str.split('-', 1).str
&gt;&gt;&gt; df

      AB  AB_split   A   B
0  A1-B1  [A1, B1]  A1  B1
1  A2-B2  [A2, B2]  A2  B2
</code></pre>

<p>Of course, getting a DataFrame out of splitting a column of strings is so useful that the <code>.str.split()</code> method can do it for you with the <code>expand=True</code> parameter:</p>

<pre><code>&gt;&gt;&gt; df['AB'].str.split('-', 1, expand=True)

    0   1
0  A1  B1
1  A2  B2
</code></pre>

<p>So, another way of accomplishing what we wanted is to do:</p>

<pre><code>&gt;&gt;&gt; df = df[['AB']]
&gt;&gt;&gt; df

      AB
0  A1-B1
1  A2-B2

&gt;&gt;&gt; df.join(df['AB'].str.split('-', 1, expand=True).rename(columns={0:'A', 1:'B'}))

      AB   A   B
0  A1-B1  A1  B1
1  A2-B2  A2  B2
</code></pre>
";;"[""df['A'], df['B'] = df['AB'].str.split(' ', 1).str\n"", ""df['AB'].str.split(' ', 1, expand=True)\n"", "">>> import pandas as pd\n>>> df = pd.DataFrame({'AB': ['A1-B1', 'A2-B2']})\n>>> df\n\n      AB\n0  A1-B1\n1  A2-B2\n>>> df['AB_split'] = df['AB'].str.split('-')\n>>> df\n\n      AB  AB_split\n0  A1-B1  [A1, B1]\n1  A2-B2  [A2, B2]\n"", '>>> upper_lower_df = pd.DataFrame({""U"": [""A"", ""B"", ""C""]})\n>>> upper_lower_df\n\n   U\n0  A\n1  B\n2  C\n>>> upper_lower_df[""L""] = upper_lower_df[""U""].str.lower()\n>>> upper_lower_df\n\n   U  L\n0  A  a\n1  B  b\n2  C  c\n', "">>> df['AB'].str[0]\n\n0    A\n1    A\nName: AB, dtype: object\n\n>>> df['AB'].str[1]\n\n0    1\n1    2\nName: AB, dtype: object\n"", "">>> df['AB'].str.split('-', 1).str[0]\n\n0    A1\n1    A2\nName: AB, dtype: object\n\n>>> df['AB'].str.split('-', 1).str[1]\n\n0    B1\n1    B2\nName: AB, dtype: object\n"", "">>> df['A'], df['B'] = df['AB'].str.split('-', 1).str\n>>> df\n\n      AB  AB_split   A   B\n0  A1-B1  [A1, B1]  A1  B1\n1  A2-B2  [A2, B2]  A2  B2\n"", "">>> df['AB'].str.split('-', 1, expand=True)\n\n    0   1\n0  A1  B1\n1  A2  B2\n"", "">>> df = df[['AB']]\n>>> df\n\n      AB\n0  A1-B1\n1  A2-B2\n\n>>> df.join(df['AB'].str.split('-', 1, expand=True).rename(columns={0:'A', 1:'B'}))\n\n      AB   A   B\n0  A1-B1  A1  B1\n1  A2-B2  A2  B2\n""]";"[""df['A'], df['B'] = df['AB'].str.split(' ', 1).str\n"", ""df['AB'].str.split(' ', 1, expand=True)\n"", '.tolist()', 'zip()', 'str.extract()', '.str.split()', "">>> import pandas as pd\n>>> df = pd.DataFrame({'AB': ['A1-B1', 'A2-B2']})\n>>> df\n\n      AB\n0  A1-B1\n1  A2-B2\n>>> df['AB_split'] = df['AB'].str.split('-')\n>>> df\n\n      AB  AB_split\n0  A1-B1  [A1, B1]\n1  A2-B2  [A2, B2]\n"", '.str.split()', '.str', '>>> upper_lower_df = pd.DataFrame({""U"": [""A"", ""B"", ""C""]})\n>>> upper_lower_df\n\n   U\n0  A\n1  B\n2  C\n>>> upper_lower_df[""L""] = upper_lower_df[""U""].str.lower()\n>>> upper_lower_df\n\n   U  L\n0  A  a\n1  B  b\n2  C  c\n', "">>> df['AB'].str[0]\n\n0    A\n1    A\nName: AB, dtype: object\n\n>>> df['AB'].str[1]\n\n0    1\n1    2\nName: AB, dtype: object\n"", '.str', "">>> df['AB'].str.split('-', 1).str[0]\n\n0    A1\n1    A2\nName: AB, dtype: object\n\n>>> df['AB'].str.split('-', 1).str[1]\n\n0    B1\n1    B2\nName: AB, dtype: object\n"", "">>> df['A'], df['B'] = df['AB'].str.split('-', 1).str\n>>> df\n\n      AB  AB_split   A   B\n0  A1-B1  [A1, B1]  A1  B1\n1  A2-B2  [A2, B2]  A2  B2\n"", '.str.split()', 'expand=True', "">>> df['AB'].str.split('-', 1, expand=True)\n\n    0   1\n0  A1  B1\n1  A2  B2\n"", "">>> df = df[['AB']]\n>>> df\n\n      AB\n0  A1-B1\n1  A2-B2\n\n>>> df.join(df['AB'].str.split('-', 1, expand=True).rename(columns={0:'A', 1:'B'}))\n\n      AB   A   B\n0  A1-B1  A1  B1\n1  A2-B2  A2  B2\n""]"
1536;;1;39370553;16476924.0;2;23;;;"<p>You can use the df.iloc function as follows:</p>

<pre><code>for i in range(0, len(df)):
    print df.iloc[i]['c1'], df.iloc[i]['c2']
</code></pre>
";;"[""for i in range(0, len(df)):\n    print df.iloc[i]['c1'], df.iloc[i]['c2']\n""]";"[""for i in range(0, len(df)):\n    print df.iloc[i]['c1'], df.iloc[i]['c2']\n""]"
1537;;2;39371897;24870306.0;2;18;;;"<p>To check if one <em>or more</em> columns all exist, you can use <a href=""https://docs.python.org/3.5/library/stdtypes.html?highlight=issubset#set.issubset"" rel=""nofollow noreferrer""><code>set.issubset</code></a>, as in:</p>

<pre><code>if set(['A','C']).issubset(df.columns):
   df['sum'] = df['A'] + df['C']                
</code></pre>

<p><strong>edit:</strong> As @brianpck points out in a comment, <code>set([])</code> can alternatively be constructed with curly braces:</p>

<pre><code>if {'A', 'C'}.issubset(df.columns):
</code></pre>

<p>See <a href=""https://stackoverflow.com/questions/17373161/using-curly-braces-to-initialize-set"">this question</a> for a discussion of the curly-braces syntax.</p>
";;"[""if set(['A','C']).issubset(df.columns):\n   df['sum'] = df['A'] + df['C']                \n"", ""if {'A', 'C'}.issubset(df.columns):\n""]";"['set.issubset', ""if set(['A','C']).issubset(df.columns):\n   df['sum'] = df['A'] + df['C']                \n"", 'set([])', ""if {'A', 'C'}.issubset(df.columns):\n""]"
1538;;0;39405540;16327055.0;2;10;;;"<p>To add to DSM's answer and building on <a href=""https://stackoverflow.com/questions/30926670/pandas-add-multiple-empty-columns-to-dataframe"">this associated question</a>, I'd split the approach into two cases:</p>

<ul>
<li><p>Adding a single column: Just assign empty values to the new columns, e.g. <code>df['C'] = np.nan</code></p></li>
<li><p>Adding multiple columns: I'd suggest using the <code>.reindex(columns=[...])</code> <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer"">method of pandas</a> to add the new columns to the dataframe's column index. This also works for adding multiple new rows.</p></li>
</ul>

<p>Here is an example adding multiple columns:</p>

<pre><code>mydf = mydf.reindex( mydf.columns.tolist() + ['newcol1','newcol2'])  # version &gt;= 0.20.0
</code></pre>

<p>or </p>

<pre><code>mydf = mydf.reindex( mydf.columns.values + ['newcol1','newcol2'])  # version &lt; 0.20.0
</code></pre>

<p>You can also always concatenate a new (empty) dataframe to the existing dataframe, but that doesn't feel as pythonic to me :)</p>
";;"[""mydf = mydf.reindex( mydf.columns.tolist() + ['newcol1','newcol2'])  # version >= 0.20.0\n"", ""mydf = mydf.reindex( mydf.columns.values + ['newcol1','newcol2'])  # version < 0.20.0\n""]";"[""df['C'] = np.nan"", '.reindex(columns=[...])', ""mydf = mydf.reindex( mydf.columns.tolist() + ['newcol1','newcol2'])  # version >= 0.20.0\n"", ""mydf = mydf.reindex( mydf.columns.values + ['newcol1','newcol2'])  # version < 0.20.0\n""]"
1539;;0;39474812;17978092.0;2;9;;;"<p>The accepted answer works for columns that are of datatype <code>string</code>. For completeness: I come across this question when searching how to do this when the columns are of datatypes: date and time. </p>

<pre><code>df.apply(lambda r : pd.datetime.combine(r['date_column_name'],r['time_column_name']),1)
</code></pre>
";;"[""df.apply(lambda r : pd.datetime.combine(r['date_column_name'],r['time_column_name']),1)\n""]";"['string', ""df.apply(lambda r : pd.datetime.combine(r['date_column_name'],r['time_column_name']),1)\n""]"
1540;;0;39478896;13295735.0;2;6;;;"<p>The below code worked for me.</p>

<pre><code>import pandas

df = pandas.read_csv('somefile.txt')

df = df.fillna(0)
</code></pre>
";;"[""import pandas\n\ndf = pandas.read_csv('somefile.txt')\n\ndf = df.fillna(0)\n""]";"[""import pandas\n\ndf = pandas.read_csv('somefile.txt')\n\ndf = df.fillna(0)\n""]"
1541;;2;39482402;32244753.0;2;27;;;"<p>The suggested solutions are incompatible with Seaborn 0.7.1</p>

<p>giving the following errors because the Seaborn interface has changed:</p>

<pre><code>AttributeError: 'AxesSubplot' object has no attribute 'fig'
When trying to access the figure

AttributeError: 'AxesSubplot' object has no attribute 'savefig'
when trying to use the savefig directly as a function
</code></pre>

<p>The following calls allow you to access the figure (Seaborn 0.7.1 compatible):</p>

<pre><code>swarm_plot = sns.swarmplot(...)
fig = swarm_plot.get_figure()
fig.savefig(...) 
</code></pre>

<p>as seen previously in <a href=""https://stackoverflow.com/a/35670528/1079075"">this answer</a>.</p>

<p><strong>UPDATE:</strong>
I have recently used PairGrid object from seaborn to generate a plot similar to the one in <a href=""http://seaborn.pydata.org/examples/pairgrid_dotplot.html"" rel=""noreferrer"">this example</a>.
In this case, since GridPlot is not a plot object like, for example, sns.swarmplot, it has no get_figure() function.
It is possible to directly access the matplotlib figure by</p>

<pre><code>fig = myGridPlotObject.fig
</code></pre>

<p>Like previously suggested in other posts in this thread.</p>
";;"[""AttributeError: 'AxesSubplot' object has no attribute 'fig'\nWhen trying to access the figure\n\nAttributeError: 'AxesSubplot' object has no attribute 'savefig'\nwhen trying to use the savefig directly as a function\n"", 'swarm_plot = sns.swarmplot(...)\nfig = swarm_plot.get_figure()\nfig.savefig(...) \n', 'fig = myGridPlotObject.fig\n']";"[""AttributeError: 'AxesSubplot' object has no attribute 'fig'\nWhen trying to access the figure\n\nAttributeError: 'AxesSubplot' object has no attribute 'savefig'\nwhen trying to use the savefig directly as a function\n"", 'swarm_plot = sns.swarmplot(...)\nfig = swarm_plot.get_figure()\nfig.savefig(...) \n', 'fig = myGridPlotObject.fig\n']"
1542;;0;39628860;27673231.0;2;11;;;"<p>Because if you don't make a copy then the indices can still be manipulated elsewhere even if you assign the dataFrame to a different name.</p>

<p>For example:</p>

<pre><code>df2 = df
func1(df2)
func2(df)
</code></pre>

<p>func1 can modify df by modifying df2, so to avoid that:</p>

<pre><code>df2 = df.copy()
func1(df2)
func2(df)
</code></pre>
";;['df2 = df\nfunc1(df2)\nfunc2(df)\n', 'df2 = df.copy()\nfunc1(df2)\nfunc2(df)\n'];['df2 = df\nfunc1(df2)\nfunc2(df)\n', 'df2 = df.copy()\nfunc1(df2)\nfunc2(df)\n']
1543;;0;39657077;19758364.0;2;10;;;"<p>A much faster implementation would be to use <code>list-comprehension</code> if you need to rename a single column.</p>

<pre><code>df.columns = ['log(gdp)' if x=='gdp' else x for x in df.columns]
</code></pre>

<p>If the need arises to rename multiple columns, either use conditional expressions like:</p>

<pre><code>df.columns = ['log(gdp)' if x=='gdp' else 'cap_mod' if x=='cap' else x for x in df.columns]
</code></pre>

<p>Or, construct a mapping using a <code>dictionary</code> and perform the <code>list-comprehension</code> with it's <code>get</code> operation by setting default value as the old name:</p>

<pre><code>col_dict = {'gdp': 'log(gdp)', 'cap': 'cap_mod'}   ## key?old name, value?new name

df.columns = [col_dict.get(x, x) for x in df.columns]
</code></pre>

<p><strong>Timings:</strong></p>

<pre><code>%%timeit
df.rename(columns={'gdp':'log(gdp)'}, inplace=True)
10000 loops, best of 3: 168 s per loop

%%timeit
df.columns = ['log(gdp)' if x=='gdp' else x for x in df.columns]
10000 loops, best of 3: 58.5 s per loop
</code></pre>
";;"[""df.columns = ['log(gdp)' if x=='gdp' else x for x in df.columns]\n"", ""df.columns = ['log(gdp)' if x=='gdp' else 'cap_mod' if x=='cap' else x for x in df.columns]\n"", ""col_dict = {'gdp': 'log(gdp)', 'cap': 'cap_mod'}   ## key?old name, value?new name\n\ndf.columns = [col_dict.get(x, x) for x in df.columns]\n"", ""%%timeit\ndf.rename(columns={'gdp':'log(gdp)'}, inplace=True)\n10000 loops, best of 3: 168 s per loop\n\n%%timeit\ndf.columns = ['log(gdp)' if x=='gdp' else x for x in df.columns]\n10000 loops, best of 3: 58.5 s per loop\n""]";"['list-comprehension', ""df.columns = ['log(gdp)' if x=='gdp' else x for x in df.columns]\n"", ""df.columns = ['log(gdp)' if x=='gdp' else 'cap_mod' if x=='cap' else x for x in df.columns]\n"", 'dictionary', 'list-comprehension', 'get', ""col_dict = {'gdp': 'log(gdp)', 'cap': 'cap_mod'}   ## key?old name, value?new name\n\ndf.columns = [col_dict.get(x, x) for x in df.columns]\n"", ""%%timeit\ndf.rename(columns={'gdp':'log(gdp)'}, inplace=True)\n10000 loops, best of 3: 168 s per loop\n\n%%timeit\ndf.columns = ['log(gdp)' if x=='gdp' else x for x in df.columns]\n10000 loops, best of 3: 58.5 s per loop\n""]"
1544;;0;39673666;29576430.0;2;27;;;"<p>You can simply use sklearn for this</p>

<pre><code>from sklearn.utils import shuffle
df = shuffle(df)
</code></pre>
";;['from sklearn.utils import shuffle\ndf = shuffle(df)\n'];['from sklearn.utils import shuffle\ndf = shuffle(df)\n']
1545;;1;39762850;26277757.0;2;7;;;"<p>it seems that <code>pd.set_option('display.max_colwidth', -1)</code> is indeed the only option. To prevent irreversible global changes of how dataframes are presented in the console, you may save the previous setting in a variable and restore it immediately after the usage, as follows:</p>

<pre><code>    old_width = pd.get_option('display.max_colwidth')
    pd.set_option('display.max_colwidth', -1)
    open('some_file.html', 'w').write(some_data.to_html())
    pd.set_option('display.max_colwidth', old_width)
</code></pre>
";;"[""    old_width = pd.get_option('display.max_colwidth')\n    pd.set_option('display.max_colwidth', -1)\n    open('some_file.html', 'w').write(some_data.to_html())\n    pd.set_option('display.max_colwidth', old_width)\n""]";"[""pd.set_option('display.max_colwidth', -1)"", ""    old_width = pd.get_option('display.max_colwidth')\n    pd.set_option('display.max_colwidth', -1)\n    open('some_file.html', 'w').write(some_data.to_html())\n    pd.set_option('display.max_colwidth', old_width)\n""]"
1546;;0;39770407;11346283.0;2;19;;;"<h1>Column names vs Names of Series</h1>

<p>I would like to explain a bit what happens behind the scenes.</p>

<p>Dataframes are a set of Series.</p>

<p>Series in turn are an extension of a <code>numpy.array</code></p>

<p><code>numpy.array</code>s have a property <code>.name</code></p>

<p>This is the name of the series. It is seldom that pandas respects this attribute, but it lingers in places and can be used to hack some pandas behaviors.</p>

<h1>Naming the list of columns</h1>

<p>A lot of answers here talks about the <code>df.columns</code> attribute being a <code>list</code> when in fact it is a <code>Series</code>. This means it has a <code>.name</code> attribute.</p>

<p>This is what happens if you decide to fill in the name of the columns <code>Series</code>:</p>

<pre><code>df.columns = ['column_one', 'column_two']
df.columns.names = ['name of the list of columns']
df.index.names = ['name of the index']

name of the list of columns     column_one  column_two
name of the index       
0                                    4           1
1                                    5           2
2                                    6           3
</code></pre>

<p>Note that the name of the index always comes one column lower.</p>

<h2>Artifacts that linger</h2>

<p>The <code>.name</code> attribute lingers on sometimes. If you set <code>df.columns = ['one', 'two']</code> then the <code>df.one.name</code> will be <code>'one'</code>.</p>

<p>If you set <code>df.one.name = 'three'</code> then <code>df.columns</code> will still give you <code>['one', 'two']</code>, and <code>df.one.name</code> will give you <code>'three'</code></p>

<h3>BUT</h3>

<p><code>pd.DataFrame(df.one)</code> will return</p>

<pre><code>    three
0       1
1       2
2       3
</code></pre>

<p>Because pandas reuses the <code>.name</code> of the already defined <code>Series</code>.</p>

<h1>Multi level column names</h1>

<p>Pandas has ways of doing multi layered column names. There is not so much magic involved but I wanted to cover this in my answer too since I don't see anyone picking up on this here.</p>

<pre><code>    |one            |
    |one      |two  |
0   |  4      |  1  |
1   |  5      |  2  |
2   |  6      |  3  |
</code></pre>

<p>This is easily achievable by setting columns to lists, like this:</p>

<pre><code>df.columns = [['one', 'one'], ['one', 'two']]
</code></pre>
";;"[""df.columns = ['column_one', 'column_two']\ndf.columns.names = ['name of the list of columns']\ndf.index.names = ['name of the index']\n\nname of the list of columns     column_one  column_two\nname of the index       \n0                                    4           1\n1                                    5           2\n2                                    6           3\n"", '    three\n0       1\n1       2\n2       3\n', '    |one            |\n    |one      |two  |\n0   |  4      |  1  |\n1   |  5      |  2  |\n2   |  6      |  3  |\n', ""df.columns = [['one', 'one'], ['one', 'two']]\n""]";"['numpy.array', 'numpy.array', '.name', 'df.columns', 'list', 'Series', '.name', 'Series', ""df.columns = ['column_one', 'column_two']\ndf.columns.names = ['name of the list of columns']\ndf.index.names = ['name of the index']\n\nname of the list of columns     column_one  column_two\nname of the index       \n0                                    4           1\n1                                    5           2\n2                                    6           3\n"", '.name', ""df.columns = ['one', 'two']"", 'df.one.name', ""'one'"", ""df.one.name = 'three'"", 'df.columns', ""['one', 'two']"", 'df.one.name', ""'three'"", 'pd.DataFrame(df.one)', '    three\n0       1\n1       2\n2       3\n', '.name', 'Series', '    |one            |\n    |one      |two  |\n0   |  4      |  1  |\n1   |  5      |  2  |\n2   |  6      |  3  |\n', ""df.columns = [['one', 'one'], ['one', 'two']]\n""]"
1547;;0;39820329;22086116.0;2;8;;;"<p>For more general boolean functions that you would like to use as a filter and that depend on more than one column, you can use: </p>

<pre><code>df = df[df[['col_1','col_2']].apply(lambda x: f(*x), axis=1)]
</code></pre>

<p>where f is a function that is applied to every pair of elements (x1, x2) from col_1 and col_2 and returns True or False depending on any condition you want on (x1, x2).</p>
";;"[""df = df[df[['col_1','col_2']].apply(lambda x: f(*x), axis=1)]\n""]";"[""df = df[df[['col_1','col_2']].apply(lambda x: f(*x), axis=1)]\n""]"
1548;;0;39837358;18695605.0;2;19;;;"<pre><code>mydict = dict(zip(df.id, df.value))
</code></pre>
";;['mydict = dict(zip(df.id, df.value))\n'];['mydict = dict(zip(df.id, df.value))\n']
1549;;1;39891994;13295735.0;2;19;;;"<p>It is not guaranteed that the slicing returns a view or a copy. You can do</p>

<pre><code>df['column']=df['column'].fillna(value)
</code></pre>
";;"[""df['column']=df['column'].fillna(value)\n""]";"[""df['column']=df['column'].fillna(value)\n""]"
1550;;6;39923958;19124601.0;2;88;;;"<p>No need to hack settings. There is a simple way:</p>

<pre><code>print(df.to_string())
</code></pre>
";;['print(df.to_string())\n'];['print(df.to_string())\n']
1551;;1;39946744;12680754.0;2;8;;;"<p>Here's a <a href=""https://github.com/cognoma/genes/blob/721204091a96e55de6dcad165d6d8265e67e2a48/2.process.py#L61-L95"" rel=""nofollow noreferrer"" title=""Source on GitHub: cognoma/genes repository"">function I wrote</a> for this common task. It's more efficient than the <code>Series</code>/<code>stack</code> methods. Column order and names are retained.</p>

<pre><code>def tidy_split(df, column, sep='|', keep=False):
    """"""
    Split the values of a column and expand so the new DataFrame has one split
    value per row. Filters rows where the column is missing.

    Params
    ------
    df : pandas.DataFrame
        dataframe with the column to split and expand
    column : str
        the column to split and expand
    sep : str
        the string used to split the column's values
    keep : bool
        whether to retain the presplit value as it's own row

    Returns
    -------
    pandas.DataFrame
        Returns a dataframe with the same columns as `df`.
    """"""
    indexes = list()
    new_values = list()
    df = df.dropna(subset=[column])
    for i, presplit in enumerate(df[column].astype(str)):
        values = presplit.split(sep)
        if keep and len(values) &gt; 1:
            indexes.append(i)
            new_values.append(presplit)
        for value in values:
            indexes.append(i)
            new_values.append(value)
    new_df = df.iloc[indexes, :].copy()
    new_df[column] = new_values
    return new_df
</code></pre>

<p>With this function, the <a href=""https://stackoverflow.com/q/12680754/4651668"">original question</a> is as simple as:</p>

<pre><code>tidy_split(a, 'var1', sep=',')
</code></pre>
";;"['def tidy_split(df, column, sep=\'|\', keep=False):\n    """"""\n    Split the values of a column and expand so the new DataFrame has one split\n    value per row. Filters rows where the column is missing.\n\n    Params\n    ------\n    df : pandas.DataFrame\n        dataframe with the column to split and expand\n    column : str\n        the column to split and expand\n    sep : str\n        the string used to split the column\'s values\n    keep : bool\n        whether to retain the presplit value as it\'s own row\n\n    Returns\n    -------\n    pandas.DataFrame\n        Returns a dataframe with the same columns as `df`.\n    """"""\n    indexes = list()\n    new_values = list()\n    df = df.dropna(subset=[column])\n    for i, presplit in enumerate(df[column].astype(str)):\n        values = presplit.split(sep)\n        if keep and len(values) > 1:\n            indexes.append(i)\n            new_values.append(presplit)\n        for value in values:\n            indexes.append(i)\n            new_values.append(value)\n    new_df = df.iloc[indexes, :].copy()\n    new_df[column] = new_values\n    return new_df\n', ""tidy_split(a, 'var1', sep=',')\n""]";"['Series', 'stack', 'def tidy_split(df, column, sep=\'|\', keep=False):\n    """"""\n    Split the values of a column and expand so the new DataFrame has one split\n    value per row. Filters rows where the column is missing.\n\n    Params\n    ------\n    df : pandas.DataFrame\n        dataframe with the column to split and expand\n    column : str\n        the column to split and expand\n    sep : str\n        the string used to split the column\'s values\n    keep : bool\n        whether to retain the presplit value as it\'s own row\n\n    Returns\n    -------\n    pandas.DataFrame\n        Returns a dataframe with the same columns as `df`.\n    """"""\n    indexes = list()\n    new_values = list()\n    df = df.dropna(subset=[column])\n    for i, presplit in enumerate(df[column].astype(str)):\n        values = presplit.split(sep)\n        if keep and len(values) > 1:\n            indexes.append(i)\n            new_values.append(presplit)\n        for value in values:\n            indexes.append(i)\n            new_values.append(value)\n    new_df = df.iloc[indexes, :].copy()\n    new_df[column] = new_values\n    return new_df\n', ""tidy_split(a, 'var1', sep=',')\n""]"
1552;;1;40008322;11350770.0;2;7;;;"<p>If anyone wonders how to perform a related problem: <strong><em>""Select column by partial string""</em></strong> </p>

<p>Use:</p>

<pre><code>df.loc[:, df.columns.to_series().str.contains('a').tolist()]
</code></pre>
";;"[""df.loc[:, df.columns.to_series().str.contains('a').tolist()]\n""]";"[""df.loc[:, df.columns.to_series().str.contains('a').tolist()]\n""]"
1553;;0;40110335;13851535.0;2;20;;;"<p>I was looking for a solution to this and I stumbled on an obvious approach which is to just filter the data frame and assign back to the original data frame so</p>

<pre><code>df= df[df[""score""] &gt; 50]
</code></pre>
";;"['df= df[df[""score""] > 50]\n']";"['df= df[df[""score""] > 50]\n']"
1554;;5;40214434;20625582.0;2;19;;;"<h1>Pandas dataframe copy warning</h1>

<p>When you go and do something like this:</p>

<pre><code>quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]
</code></pre>

<p><code>pandas.ix</code> <em>in this case</em> returns a new, stand alone dataframe.</p>

<p>Any values you decide to change in this dataframe, will not change the original dataframe.</p>

<p>This is what pandas tries to warn you about.</p>

<h1>Why <code>.ix</code> is a bad idea</h1>

<p>The <code>.ix</code> object tries to do more than one thing, and for anyone who has read anything about clean code, this is a strong smell.</p>

<p>Given this dataframe:</p>

<pre><code>df = pd.DataFrame({""a"": [1,2,3,4], ""b"": [1,1,2,2]})
</code></pre>

<p>Two behaviors:</p>

<pre><code>dfcopy = df.ix[:,[""a""]]
dfcopy.a.ix[0] = 2
</code></pre>

<p>Behavior one: <code>dfcopy</code> is now a stand alone dataframe. Changing it will not change <code>df</code></p>

<pre><code>df.ix[0, ""a""] = 3
</code></pre>

<p>Behavior two: This changes the original dataframe.</p>

<h1>Use <code>.loc</code> instead</h1>

<p>The pandas developers recognized that the <code>.ix</code> object was quite smelly[speculatively] and thus created two new objects which helps in the accession and assignment of data. (The other being <code>.iloc</code>)</p>

<p><code>.loc</code> is faster, because it does not try to create a copy of the data.</p>

<p><code>.loc</code> is meant to modify your existing dataframe inplace, which is more memory efficient.</p>

<p><code>.loc</code> is predictable, it has one behavior.</p>

<h1>The solution</h1>

<p>What you are doing in your code example is loading a big file with lots of columns, then modifying it to be smaller.</p>

<p>The <code>pd.read_csv</code> function can help you out with a lot of this and also make the loading of the file a lot faster.</p>

<p>So instead of doing this</p>

<pre><code>quote_df = pd.read_csv(StringIO(str_of_all), sep=',', names=list('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg')) #dtype={'A': object, 'B': object, 'C': np.float64}
quote_df.rename(columns={'A':'STK', 'B':'TOpen', 'C':'TPCLOSE', 'D':'TPrice', 'E':'THigh', 'F':'TLow', 'I':'TVol', 'J':'TAmt', 'e':'TDate', 'f':'TTime'}, inplace=True)
quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]
</code></pre>

<p>Do this</p>

<pre><code>columns = ['STK', 'TPrice', 'TPCLOSE', 'TOpen', 'THigh', 'TLow', 'TVol', 'TAmt', 'TDate', 'TTime']
df = pd.read_csv(StringIO(str_of_all), sep=',', usecols=[0,3,2,1,4,5,8,9,30,31])
df.columns = columns
</code></pre>

<p>This will only read the columns you are interested in, and name them properly. No need for using the evil <code>.ix</code> object to do magical stuff.</p>
";;"['quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\n', 'df = pd.DataFrame({""a"": [1,2,3,4], ""b"": [1,1,2,2]})\n', 'dfcopy = df.ix[:,[""a""]]\ndfcopy.a.ix[0] = 2\n', 'df.ix[0, ""a""] = 3\n', ""quote_df = pd.read_csv(StringIO(str_of_all), sep=',', names=list('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg')) #dtype={'A': object, 'B': object, 'C': np.float64}\nquote_df.rename(columns={'A':'STK', 'B':'TOpen', 'C':'TPCLOSE', 'D':'TPrice', 'E':'THigh', 'F':'TLow', 'I':'TVol', 'J':'TAmt', 'e':'TDate', 'f':'TTime'}, inplace=True)\nquote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\n"", ""columns = ['STK', 'TPrice', 'TPCLOSE', 'TOpen', 'THigh', 'TLow', 'TVol', 'TAmt', 'TDate', 'TTime']\ndf = pd.read_csv(StringIO(str_of_all), sep=',', usecols=[0,3,2,1,4,5,8,9,30,31])\ndf.columns = columns\n""]";"['quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\n', 'pandas.ix', '.ix', '.ix', 'df = pd.DataFrame({""a"": [1,2,3,4], ""b"": [1,1,2,2]})\n', 'dfcopy = df.ix[:,[""a""]]\ndfcopy.a.ix[0] = 2\n', 'dfcopy', 'df', 'df.ix[0, ""a""] = 3\n', '.loc', '.ix', '.iloc', '.loc', '.loc', '.loc', 'pd.read_csv', ""quote_df = pd.read_csv(StringIO(str_of_all), sep=',', names=list('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg')) #dtype={'A': object, 'B': object, 'C': np.float64}\nquote_df.rename(columns={'A':'STK', 'B':'TOpen', 'C':'TPCLOSE', 'D':'TPrice', 'E':'THigh', 'F':'TLow', 'I':'TVol', 'J':'TAmt', 'e':'TDate', 'f':'TTime'}, inplace=True)\nquote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\n"", ""columns = ['STK', 'TPrice', 'TPCLOSE', 'TOpen', 'THigh', 'TLow', 'TVol', 'TAmt', 'TDate', 'TTime']\ndf = pd.read_csv(StringIO(str_of_all), sep=',', usecols=[0,3,2,1,4,5,8,9,30,31])\ndf.columns = columns\n"", '.ix']"
1555;;1;40228738;29432629.0;2;22;;;"<p>Seaborn's heatmap version:</p>

<pre><code>import seaborn as sns
corr = dataframe.corr()
sns.heatmap(corr, 
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values)
</code></pre>
";;['import seaborn as sns\ncorr = dataframe.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)\n'];['import seaborn as sns\ncorr = dataframe.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)\n']
1556;;0;40429755;21319929.0;2;6;;;"<p>You can also use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.isin.html"" rel=""noreferrer"">pandas.Series.isin</a> although it's a little bit longer than <code>'a' in s.values</code>:</p>

<pre><code>In [2]: s = pd.Series(list('abc'))

In [3]: s
Out[3]: 
0    a
1    b
2    c
dtype: object

In [3]: s.isin(['a'])
Out[3]: 
0    True
1    False
2    False
dtype: bool

In [4]: s[s.isin(['a'])].empty
Out[4]: False

In [5]: s[s.isin(['z'])].empty
Out[5]: True
</code></pre>

<p>But this approach can be more flexible if you need to match multiple values at once for a DataFrame (see <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isin.html"" rel=""noreferrer"">DataFrame.isin</a>)</p>

<pre><code>&gt;&gt;&gt; df = DataFrame({'A': [1, 2, 3], 'B': [1, 4, 7]})
&gt;&gt;&gt; df.isin({'A': [1, 3], 'B': [4, 7, 12]})
       A      B
0   True  False  # Note that B didn't match 1 here.
1  False   True
2   True   True
</code></pre>
";;"[""In [2]: s = pd.Series(list('abc'))\n\nIn [3]: s\nOut[3]: \n0    a\n1    b\n2    c\ndtype: object\n\nIn [3]: s.isin(['a'])\nOut[3]: \n0    True\n1    False\n2    False\ndtype: bool\n\nIn [4]: s[s.isin(['a'])].empty\nOut[4]: False\n\nIn [5]: s[s.isin(['z'])].empty\nOut[5]: True\n"", "">>> df = DataFrame({'A': [1, 2, 3], 'B': [1, 4, 7]})\n>>> df.isin({'A': [1, 3], 'B': [4, 7, 12]})\n       A      B\n0   True  False  # Note that B didn't match 1 here.\n1  False   True\n2   True   True\n""]";"[""'a' in s.values"", ""In [2]: s = pd.Series(list('abc'))\n\nIn [3]: s\nOut[3]: \n0    a\n1    b\n2    c\ndtype: object\n\nIn [3]: s.isin(['a'])\nOut[3]: \n0    True\n1    False\n2    False\ndtype: bool\n\nIn [4]: s[s.isin(['a'])].empty\nOut[4]: False\n\nIn [5]: s[s.isin(['z'])].empty\nOut[5]: True\n"", "">>> df = DataFrame({'A': [1, 2, 3], 'B': [1, 4, 7]})\n>>> df.isin({'A': [1, 3], 'B': [4, 7, 12]})\n       A      B\n0   True  False  # Note that B didn't match 1 here.\n1  False   True\n2   True   True\n""]"
1557;;3;40435354;14984119.0;2;48;;;"<p>All of the above seem unnecessarily heavy and tedious methods --there's a one line solution to the problem. This applies if some column names are duplicated and you wish to remove them:</p>

<pre><code>df = df.loc[:,~df.columns.duplicated()]
</code></pre>

<h3>[update] How it works:</h3>

<p>Suppose the columns of the data frame are <code>['alpha','beta','alpha']</code></p>

<p><code>df.columns.duplicated()</code> returns a boolean array: a <code>True</code> or <code>False</code> for each column. If it is <code>False</code> then the column name is unique up to that point, if it is <code>True</code> then the column name is duplicated earlier. For example, using the given example, the returned value would be <code>[False,False,True]</code>. </p>

<p><code>Pandas</code> allows one to index using boolean values whereby it selects only the <code>True</code> values. Since we want to keep the unduplicated columns, we need the above boolean array to be flipped (ie <code>[True, True, False] = ~[False,False,True]</code>)</p>

<p>Finally, <code>df.loc[:,[True,True,False]]</code> selects only the non-duplicated columns using the aforementioned indexing capability. </p>

<p><strong>Note</strong>: the above only checks columns names, <em>not</em> column values.</p>
";;['df = df.loc[:,~df.columns.duplicated()]\n'];"['df = df.loc[:,~df.columns.duplicated()]\n', ""['alpha','beta','alpha']"", 'df.columns.duplicated()', 'True', 'False', 'False', 'True', '[False,False,True]', 'Pandas', 'True', '[True, True, False] = ~[False,False,True]', 'df.loc[:,[True,True,False]]']"
1558;;2;40449726;12680754.0;2;22;;;"<p><strong>UPDATE2:</strong> more generic vectorized function, which will work for multiple <code>normal</code> and multiple <code>list</code> columns</p>

<pre><code>def explode(df, lst_cols, fill_value=''):
    # make sure `lst_cols` is a list
    if lst_cols and not isinstance(lst_cols, list):
        lst_cols = [lst_cols]
    # all columns except `lst_cols`
    idx_cols = df.columns.difference(lst_cols)

    # calculate lengths of lists
    lens = df[lst_cols[0]].str.len()

    if (lens &gt; 0).all():
        # ALL lists in cells aren't empty
        return pd.DataFrame({
            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())
            for col in idx_cols
        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \
          .loc[:, df.columns]
    else:
        # at least one list in cells is empty
        return pd.DataFrame({
            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())
            for col in idx_cols
        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \
          .append(df.loc[lens==0, idx_cols]).fillna(fill_value) \
          .loc[:, df.columns]
</code></pre>

<p>Demo:</p>

<p>Multiple <code>list</code> columns - all <code>list</code> columns must have the same # of elements in each row:</p>

<pre><code>In [36]: df
Out[36]:
   aaa  myid        num          text
0   10     1  [1, 2, 3]  [aa, bb, cc]
1   11     2     [1, 2]      [cc, dd]
2   12     3         []            []
3   13     4         []            []

In [37]: explode(df, ['num','text'], fill_value='')
Out[37]:
   aaa  myid num text
0   10     1   1   aa
1   10     1   2   bb
2   10     1   3   cc
3   11     2   1   cc
4   11     2   2   dd
2   12     3
3   13     4
</code></pre>

<p>Setup:</p>

<pre><code>df = pd.DataFrame({
 'aaa': {0: 10, 1: 11, 2: 12, 3: 13},
 'myid': {0: 1, 1: 2, 2: 3, 3: 4},
 'num': {0: [1, 2, 3], 1: [1, 2], 2: [], 3: []},
 'text': {0: ['aa', 'bb', 'cc'], 1: ['cc', 'dd'], 2: [], 3: []}
})
</code></pre>

<p>CSV column:</p>

<pre><code>In [46]: df
Out[46]:
        var1  var2 var3
0      a,b,c     1   XX
1  d,e,f,x,y     2   ZZ

In [47]: explode(df.assign(var1=df.var1.str.split(',')), 'var1')
Out[47]:
  var1  var2 var3
0    a     1   XX
1    b     1   XX
2    c     1   XX
3    d     2   ZZ
4    e     2   ZZ
5    f     2   ZZ
6    x     2   ZZ
7    y     2   ZZ
</code></pre>

<p>using this little trick we can convert CSV-like column to <code>list</code> column:</p>

<pre><code>In [48]: df.assign(var1=df.var1.str.split(','))
Out[48]:
              var1  var2 var3
0        [a, b, c]     1   XX
1  [d, e, f, x, y]     2   ZZ
</code></pre>

<hr>

<p><strong>UPDATE:</strong> <strong>generic vectorized approach (will work also for multiple columns):</strong></p>

<p>Original DF:</p>

<pre><code>In [177]: df
Out[177]:
        var1  var2 var3
0      a,b,c     1   XX
1  d,e,f,x,y     2   ZZ
</code></pre>

<p><strong>Solution:</strong></p>

<p>first let's convert CSV strings to lists:</p>

<pre><code>In [178]: lst_col = 'var1' 

In [179]: x = df.assign(**{lst_col:df[lst_col].str.split(',')})

In [180]: x
Out[180]:
              var1  var2 var3
0        [a, b, c]     1   XX
1  [d, e, f, x, y]     2   ZZ
</code></pre>

<p>Now we can do this:</p>

<pre><code>In [181]: pd.DataFrame({
     ...:     col:np.repeat(x[col].values, x[lst_col].str.len())
     ...:     for col in x.columns.difference([lst_col])
     ...: }).assign(**{lst_col:np.concatenate(x[lst_col].values)})[x.columns.tolist()]
     ...:
Out[181]:
  var1  var2 var3
0    a     1   XX
1    b     1   XX
2    c     1   XX
3    d     2   ZZ
4    e     2   ZZ
5    f     2   ZZ
6    x     2   ZZ
7    y     2   ZZ
</code></pre>

<hr>

<p><strong>OLD answer:</strong></p>

<p>Inspired by <a href=""https://stackoverflow.com/a/28182629/5741205"">@AFinkelstein solution</a>, i wanted to make it bit more generalized which could be applied to DF with more than two columns and as fast, well almost, as fast as AFinkelstein's solution):</p>

<pre><code>In [2]: df = pd.DataFrame(
   ...:    [{'var1': 'a,b,c', 'var2': 1, 'var3': 'XX'},
   ...:     {'var1': 'd,e,f,x,y', 'var2': 2, 'var3': 'ZZ'}]
   ...: )

In [3]: df
Out[3]:
        var1  var2 var3
0      a,b,c     1   XX
1  d,e,f,x,y     2   ZZ

In [4]: (df.set_index(df.columns.drop('var1',1).tolist())
   ...:    .var1.str.split(',', expand=True)
   ...:    .stack()
   ...:    .reset_index()
   ...:    .rename(columns={0:'var1'})
   ...:    .loc[:, df.columns]
   ...: )
Out[4]:
  var1  var2 var3
0    a     1   XX
1    b     1   XX
2    c     1   XX
3    d     2   ZZ
4    e     2   ZZ
5    f     2   ZZ
6    x     2   ZZ
7    y     2   ZZ
</code></pre>
";;"[""def explode(df, lst_cols, fill_value=''):\n    # make sure `lst_cols` is a list\n    if lst_cols and not isinstance(lst_cols, list):\n        lst_cols = [lst_cols]\n    # all columns except `lst_cols`\n    idx_cols = df.columns.difference(lst_cols)\n\n    # calculate lengths of lists\n    lens = df[lst_cols[0]].str.len()\n\n    if (lens > 0).all():\n        # ALL lists in cells aren't empty\n        return pd.DataFrame({\n            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())\n            for col in idx_cols\n        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n          .loc[:, df.columns]\n    else:\n        # at least one list in cells is empty\n        return pd.DataFrame({\n            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())\n            for col in idx_cols\n        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n          .append(df.loc[lens==0, idx_cols]).fillna(fill_value) \\\n          .loc[:, df.columns]\n"", ""In [36]: df\nOut[36]:\n   aaa  myid        num          text\n0   10     1  [1, 2, 3]  [aa, bb, cc]\n1   11     2     [1, 2]      [cc, dd]\n2   12     3         []            []\n3   13     4         []            []\n\nIn [37]: explode(df, ['num','text'], fill_value='')\nOut[37]:\n   aaa  myid num text\n0   10     1   1   aa\n1   10     1   2   bb\n2   10     1   3   cc\n3   11     2   1   cc\n4   11     2   2   dd\n2   12     3\n3   13     4\n"", ""df = pd.DataFrame({\n 'aaa': {0: 10, 1: 11, 2: 12, 3: 13},\n 'myid': {0: 1, 1: 2, 2: 3, 3: 4},\n 'num': {0: [1, 2, 3], 1: [1, 2], 2: [], 3: []},\n 'text': {0: ['aa', 'bb', 'cc'], 1: ['cc', 'dd'], 2: [], 3: []}\n})\n"", ""In [46]: df\nOut[46]:\n        var1  var2 var3\n0      a,b,c     1   XX\n1  d,e,f,x,y     2   ZZ\n\nIn [47]: explode(df.assign(var1=df.var1.str.split(',')), 'var1')\nOut[47]:\n  var1  var2 var3\n0    a     1   XX\n1    b     1   XX\n2    c     1   XX\n3    d     2   ZZ\n4    e     2   ZZ\n5    f     2   ZZ\n6    x     2   ZZ\n7    y     2   ZZ\n"", ""In [48]: df.assign(var1=df.var1.str.split(','))\nOut[48]:\n              var1  var2 var3\n0        [a, b, c]     1   XX\n1  [d, e, f, x, y]     2   ZZ\n"", 'In [177]: df\nOut[177]:\n        var1  var2 var3\n0      a,b,c     1   XX\n1  d,e,f,x,y     2   ZZ\n', ""In [178]: lst_col = 'var1' \n\nIn [179]: x = df.assign(**{lst_col:df[lst_col].str.split(',')})\n\nIn [180]: x\nOut[180]:\n              var1  var2 var3\n0        [a, b, c]     1   XX\n1  [d, e, f, x, y]     2   ZZ\n"", 'In [181]: pd.DataFrame({\n     ...:     col:np.repeat(x[col].values, x[lst_col].str.len())\n     ...:     for col in x.columns.difference([lst_col])\n     ...: }).assign(**{lst_col:np.concatenate(x[lst_col].values)})[x.columns.tolist()]\n     ...:\nOut[181]:\n  var1  var2 var3\n0    a     1   XX\n1    b     1   XX\n2    c     1   XX\n3    d     2   ZZ\n4    e     2   ZZ\n5    f     2   ZZ\n6    x     2   ZZ\n7    y     2   ZZ\n', ""In [2]: df = pd.DataFrame(\n   ...:    [{'var1': 'a,b,c', 'var2': 1, 'var3': 'XX'},\n   ...:     {'var1': 'd,e,f,x,y', 'var2': 2, 'var3': 'ZZ'}]\n   ...: )\n\nIn [3]: df\nOut[3]:\n        var1  var2 var3\n0      a,b,c     1   XX\n1  d,e,f,x,y     2   ZZ\n\nIn [4]: (df.set_index(df.columns.drop('var1',1).tolist())\n   ...:    .var1.str.split(',', expand=True)\n   ...:    .stack()\n   ...:    .reset_index()\n   ...:    .rename(columns={0:'var1'})\n   ...:    .loc[:, df.columns]\n   ...: )\nOut[4]:\n  var1  var2 var3\n0    a     1   XX\n1    b     1   XX\n2    c     1   XX\n3    d     2   ZZ\n4    e     2   ZZ\n5    f     2   ZZ\n6    x     2   ZZ\n7    y     2   ZZ\n""]";"['normal', 'list', ""def explode(df, lst_cols, fill_value=''):\n    # make sure `lst_cols` is a list\n    if lst_cols and not isinstance(lst_cols, list):\n        lst_cols = [lst_cols]\n    # all columns except `lst_cols`\n    idx_cols = df.columns.difference(lst_cols)\n\n    # calculate lengths of lists\n    lens = df[lst_cols[0]].str.len()\n\n    if (lens > 0).all():\n        # ALL lists in cells aren't empty\n        return pd.DataFrame({\n            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())\n            for col in idx_cols\n        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n          .loc[:, df.columns]\n    else:\n        # at least one list in cells is empty\n        return pd.DataFrame({\n            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())\n            for col in idx_cols\n        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n          .append(df.loc[lens==0, idx_cols]).fillna(fill_value) \\\n          .loc[:, df.columns]\n"", 'list', 'list', ""In [36]: df\nOut[36]:\n   aaa  myid        num          text\n0   10     1  [1, 2, 3]  [aa, bb, cc]\n1   11     2     [1, 2]      [cc, dd]\n2   12     3         []            []\n3   13     4         []            []\n\nIn [37]: explode(df, ['num','text'], fill_value='')\nOut[37]:\n   aaa  myid num text\n0   10     1   1   aa\n1   10     1   2   bb\n2   10     1   3   cc\n3   11     2   1   cc\n4   11     2   2   dd\n2   12     3\n3   13     4\n"", ""df = pd.DataFrame({\n 'aaa': {0: 10, 1: 11, 2: 12, 3: 13},\n 'myid': {0: 1, 1: 2, 2: 3, 3: 4},\n 'num': {0: [1, 2, 3], 1: [1, 2], 2: [], 3: []},\n 'text': {0: ['aa', 'bb', 'cc'], 1: ['cc', 'dd'], 2: [], 3: []}\n})\n"", ""In [46]: df\nOut[46]:\n        var1  var2 var3\n0      a,b,c     1   XX\n1  d,e,f,x,y     2   ZZ\n\nIn [47]: explode(df.assign(var1=df.var1.str.split(',')), 'var1')\nOut[47]:\n  var1  var2 var3\n0    a     1   XX\n1    b     1   XX\n2    c     1   XX\n3    d     2   ZZ\n4    e     2   ZZ\n5    f     2   ZZ\n6    x     2   ZZ\n7    y     2   ZZ\n"", 'list', ""In [48]: df.assign(var1=df.var1.str.split(','))\nOut[48]:\n              var1  var2 var3\n0        [a, b, c]     1   XX\n1  [d, e, f, x, y]     2   ZZ\n"", 'In [177]: df\nOut[177]:\n        var1  var2 var3\n0      a,b,c     1   XX\n1  d,e,f,x,y     2   ZZ\n', ""In [178]: lst_col = 'var1' \n\nIn [179]: x = df.assign(**{lst_col:df[lst_col].str.split(',')})\n\nIn [180]: x\nOut[180]:\n              var1  var2 var3\n0        [a, b, c]     1   XX\n1  [d, e, f, x, y]     2   ZZ\n"", 'In [181]: pd.DataFrame({\n     ...:     col:np.repeat(x[col].values, x[lst_col].str.len())\n     ...:     for col in x.columns.difference([lst_col])\n     ...: }).assign(**{lst_col:np.concatenate(x[lst_col].values)})[x.columns.tolist()]\n     ...:\nOut[181]:\n  var1  var2 var3\n0    a     1   XX\n1    b     1   XX\n2    c     1   XX\n3    d     2   ZZ\n4    e     2   ZZ\n5    f     2   ZZ\n6    x     2   ZZ\n7    y     2   ZZ\n', ""In [2]: df = pd.DataFrame(\n   ...:    [{'var1': 'a,b,c', 'var2': 1, 'var3': 'XX'},\n   ...:     {'var1': 'd,e,f,x,y', 'var2': 2, 'var3': 'ZZ'}]\n   ...: )\n\nIn [3]: df\nOut[3]:\n        var1  var2 var3\n0      a,b,c     1   XX\n1  d,e,f,x,y     2   ZZ\n\nIn [4]: (df.set_index(df.columns.drop('var1',1).tolist())\n   ...:    .var1.str.split(',', expand=True)\n   ...:    .stack()\n   ...:    .reset_index()\n   ...:    .rename(columns={0:'var1'})\n   ...:    .loc[:, df.columns]\n   ...: )\nOut[4]:\n  var1  var2 var3\n0    a     1   XX\n1    b     1   XX\n2    c     1   XX\n3    d     2   ZZ\n4    e     2   ZZ\n5    f     2   ZZ\n6    x     2   ZZ\n7    y     2   ZZ\n""]"
1559;;1;40501868;17091769.0;2;6;;;"<p>More simpler version</p>

<p><code>df = DataFrame(columns=('col1', 'col2', 'col3'))
for i in range(5):
   df.loc[i] = ['&lt;some value for first&gt;','&lt;some value for second&gt;','&lt;some value for third&gt;']</code></p>
";;[];"[""df = DataFrame(columns=('col1', 'col2', 'col3'))\nfor i in range(5):\n   df.loc[i] = ['<some value for first>','<some value for second>','<some value for third>']""]"
1560;;1;40535454;17326973.0;2;6;;;"<p>Inspired by user6178746's answer above, I have the following:</p>

<pre><code># Given a dict of dataframes, for example:
# dfs = {'gadgets': df_gadgets, 'widgets': df_widgets}

writer = pd.ExcelWriter(filename, engine='xlsxwriter')
for sheetname, df in dfs.items():  # loop through `dict` of dataframes
    df.to_excel(writer, sheet_name=sheetname)  # send df to writer
    worksheet = writer.sheets[sheetname]  # pull worksheet object
    for idx, col in enumerate(df):  # loop through all columns
        series = df[col]
        max_len = max((
            series.astype(str).map(len).max(),  # len of largest item
            len(str(series.name))  # len of column name/header
            )) + 1  # adding a little extra space
        worksheet.set_column(idx, idx, max_len)  # set column width
writer.save()
</code></pre>
";;"[""# Given a dict of dataframes, for example:\n# dfs = {'gadgets': df_gadgets, 'widgets': df_widgets}\n\nwriter = pd.ExcelWriter(filename, engine='xlsxwriter')\nfor sheetname, df in dfs.items():  # loop through `dict` of dataframes\n    df.to_excel(writer, sheet_name=sheetname)  # send df to writer\n    worksheet = writer.sheets[sheetname]  # pull worksheet object\n    for idx, col in enumerate(df):  # loop through all columns\n        series = df[col]\n        max_len = max((\n            series.astype(str).map(len).max(),  # len of largest item\n            len(str(series.name))  # len of column name/header\n            )) + 1  # adding a little extra space\n        worksheet.set_column(idx, idx, max_len)  # set column width\nwriter.save()\n""]";"[""# Given a dict of dataframes, for example:\n# dfs = {'gadgets': df_gadgets, 'widgets': df_widgets}\n\nwriter = pd.ExcelWriter(filename, engine='xlsxwriter')\nfor sheetname, df in dfs.items():  # loop through `dict` of dataframes\n    df.to_excel(writer, sheet_name=sheetname)  # send df to writer\n    worksheet = writer.sheets[sheetname]  # pull worksheet object\n    for idx, col in enumerate(df):  # loop through all columns\n        series = df[col]\n        max_len = max((\n            series.astype(str).map(len).max(),  # len of largest item\n            len(str(series.name))  # len of column name/header\n            )) + 1  # adding a little extra space\n        worksheet.set_column(idx, idx, max_len)  # set column width\nwriter.save()\n""]"
1561;;2;40629420;15705630.0;2;14;;;"<p>You can sort the dataFrame by count and then remove duplicates. I think it's easier:</p>

<pre><code>df.sort_values('count', ascending=False).drop_duplicates(['Sp','Mt'])
</code></pre>
";;"[""df.sort_values('count', ascending=False).drop_duplicates(['Sp','Mt'])\n""]";"[""df.sort_values('count', ascending=False).drop_duplicates(['Sp','Mt'])\n""]"
1562;;0;40834052;22591174.0;2;15;;;"<p>Late answer but you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html"" rel=""noreferrer"">query()</a>, i.e:</p>

<pre><code>df_filtered = df.query('a == 4 &amp; b != 2')
</code></pre>
";;"[""df_filtered = df.query('a == 4 & b != 2')\n""]";"[""df_filtered = df.query('a == 4 & b != 2')\n""]"
1563;;0;41022840;16476924.0;2;38;;;"<p>To iterate through DataFrame's row in pandas way one can use:</p>

<ul>
<li><p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html"" rel=""noreferrer"">DataFrame.iterrows()</a></p>

<pre><code>for index, row in df.iterrows():
    print row[""c1""], row[""c2""]
</code></pre></li>
<li><p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.itertuples.html"" rel=""noreferrer"">DataFrame.itertuples()</a></p>

<pre><code>for row in df.itertuples(index=True, name='Pandas'):
    print getattr(row, ""c1""), getattr(row, ""c2"")
</code></pre></li>
</ul>

<p><code>itertuples()</code> is supposed to be faster than <code>iterrows()</code></p>

<p>But be aware, according to the docs (pandas 0.19.1 at the moment):</p>

<ul>
<li>iterrows: data's <code>dtype</code> might not match from row to row

<blockquote>
  <p>Because iterrows returns a Series for each row, it <strong>does not preserve</strong> dtypes across the rows (dtypes are preserved across columns for DataFrames)
  *</p>
</blockquote></li>
<li><p>iterrows: Do not modify rows</p>

<blockquote>
  <p>You should <strong>never modify</strong> something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect.</p>
</blockquote>

<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html"" rel=""noreferrer"">DataFrame.apply()</a> instead:</p>

<pre><code>new_df = df.apply(lambda x: x * 2)
</code></pre></li>
<li><p>itertuples: </p>

<blockquote>
  <p>The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. With a large number of columns (>255), regular tuples are returned.</p>
</blockquote></li>
</ul>
";;"['for index, row in df.iterrows():\n    print row[""c1""], row[""c2""]\n', 'for row in df.itertuples(index=True, name=\'Pandas\'):\n    print getattr(row, ""c1""), getattr(row, ""c2"")\n', 'new_df = df.apply(lambda x: x * 2)\n']";"['for index, row in df.iterrows():\n    print row[""c1""], row[""c2""]\n', 'for row in df.itertuples(index=True, name=\'Pandas\'):\n    print getattr(row, ""c1""), getattr(row, ""c2"")\n', 'itertuples()', 'iterrows()', 'dtype', 'new_df = df.apply(lambda x: x * 2)\n']"
1564;;2;41173392;17097643.0;2;9;;;"<p>I was having trouble with the not (~) symbol as well, so here's another way from another <a href=""https://stackoverflow.com/questions/11350770/pandas-dataframe-select-by-partial-string"">StackOverflow thread</a>:</p>

<pre><code>    df[df[""col""].str.contains('this'|'that')==False]
</code></pre>
";;"['    df[df[""col""].str.contains(\'this\'|\'that\')==False]\n']";"['    df[df[""col""].str.contains(\'this\'|\'that\')==False]\n']"
1565;;0;41210491;13129618.0;2;7;;;"<p>Inorder to get the frequency counts of the values in a given interval binned range, we could make use of <a href=""http://pandas.pydata.org/pandas-docs/version/0.19.0/generated/pandas.cut.html"" rel=""noreferrer""><code>pd.cut</code></a> which returns indices of half open bins for each element along with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html"" rel=""noreferrer""><code>value_counts</code></a> for computing their respective counts. </p>

<p>To plot their counts, a bar plot can be then made.</p>

<pre><code>step = 50
bin_range = np.arange(-200, 1000+step, step)
out, bins  = pd.cut(s, bins=bin_range, include_lowest=True, right=False, retbins=True)
out.value_counts(sort=False).plot.bar()
</code></pre>

<p><a href=""https://i.stack.imgur.com/sZBWd.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/sZBWd.png"" alt=""enter image description here""></a></p>

<p>Frequency for each interval sorted in descending order of their counts:</p>

<pre><code>out.value_counts().head()
[-100, -50)    18
[0, 50)        16
[800, 850)      2
[-50, 0)        2
[950, 1000)     1
dtype: int64
</code></pre>

<hr>

<p>To modify the plot to include just the lower closed interval of the range for aesthetic purpose, you could do:</p>

<pre><code>out.cat.categories = bins[:-1]
out.value_counts(sort=False).plot.bar()
</code></pre>

<p><a href=""https://i.stack.imgur.com/Wu0XU.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Wu0XU.png"" alt=""enter image description here""></a></p>
";;['step = 50\nbin_range = np.arange(-200, 1000+step, step)\nout, bins  = pd.cut(s, bins=bin_range, include_lowest=True, right=False, retbins=True)\nout.value_counts(sort=False).plot.bar()\n', 'out.value_counts().head()\n[-100, -50)    18\n[0, 50)        16\n[800, 850)      2\n[-50, 0)        2\n[950, 1000)     1\ndtype: int64\n', 'out.cat.categories = bins[:-1]\nout.value_counts(sort=False).plot.bar()\n'];['pd.cut', 'value_counts', 'step = 50\nbin_range = np.arange(-200, 1000+step, step)\nout, bins  = pd.cut(s, bins=bin_range, include_lowest=True, right=False, retbins=True)\nout.value_counts(sort=False).plot.bar()\n', 'out.value_counts().head()\n[-100, -50)    18\n[0, 50)        16\n[800, 850)      2\n[-50, 0)        2\n[950, 1000)     1\ndtype: int64\n', 'out.cat.categories = bins[:-1]\nout.value_counts(sort=False).plot.bar()\n']
1566;;3;41218519;21291259.0;2;8;;;"<p>Using a list of column names, change the type for multiple columns with .applymap() or for a single column with .apply().</p>

<pre><code>    df = pd.DataFrame(10*np.random.rand(3, 4), columns=list(""ABCD""))

              A         B         C         D
    0  8.362940  0.354027  1.916283  6.226750
    1  1.988232  9.003545  9.277504  8.522808
    2  1.141432  4.935593  2.700118  7.739108

    cols = ['A', 'B']
    df[cols] = df[cols].applymap(np.int64)

       A  B         C         D
    0  8  0  1.916283  6.226750
    1  1  9  9.277504  8.522808
    2  1  4  2.700118  7.739108

    df['C'] = df['C'].apply(np.int64)
       A  B  C         D
    0  8  0  1  6.226750
    1  1  9  9  8.522808
    2  1  4  2  7.739108
</code></pre>
";;"['    df = pd.DataFrame(10*np.random.rand(3, 4), columns=list(""ABCD""))\n\n              A         B         C         D\n    0  8.362940  0.354027  1.916283  6.226750\n    1  1.988232  9.003545  9.277504  8.522808\n    2  1.141432  4.935593  2.700118  7.739108\n\n    cols = [\'A\', \'B\']\n    df[cols] = df[cols].applymap(np.int64)\n\n       A  B         C         D\n    0  8  0  1.916283  6.226750\n    1  1  9  9.277504  8.522808\n    2  1  4  2.700118  7.739108\n\n    df[\'C\'] = df[\'C\'].apply(np.int64)\n       A  B  C         D\n    0  8  0  1  6.226750\n    1  1  9  9  8.522808\n    2  1  4  2  7.739108\n']";"['    df = pd.DataFrame(10*np.random.rand(3, 4), columns=list(""ABCD""))\n\n              A         B         C         D\n    0  8.362940  0.354027  1.916283  6.226750\n    1  1.988232  9.003545  9.277504  8.522808\n    2  1.141432  4.935593  2.700118  7.739108\n\n    cols = [\'A\', \'B\']\n    df[cols] = df[cols].applymap(np.int64)\n\n       A  B         C         D\n    0  8  0  1.916283  6.226750\n    1  1  9  9.277504  8.522808\n    2  1  4  2.700118  7.739108\n\n    df[\'C\'] = df[\'C\'].apply(np.int64)\n       A  B  C         D\n    0  8  0  1  6.226750\n    1  1  9  9  8.522808\n    2  1  4  2  7.739108\n']"
1567;;2;41228272;16777570.0;2;8;;;"<p>We can create a series with both index and values equal to the index keys using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.to_series.html"" rel=""noreferrer""><code>to_series</code></a> and then compute the differences between successive rows which would result in <code>timedelta64[ns]</code> dtype. After obtaining this, via the <code>.dt</code> property, we could access the seconds attribute of the time portion and finally divide each element by 60 to get it outputted in minutes(optionally filling the first value with 0).</p>

<pre><code>In [13]: df['deltaT'] = df.index.to_series().diff().dt.seconds.div(60, fill_value=0)
    ...: df                                 # use .astype(int) to obtain integer values
Out[13]: 
                     value  deltaT
time                              
2012-03-16 23:50:00      1     0.0
2012-03-16 23:56:00      2     6.0
2012-03-17 00:08:00      3    12.0
2012-03-17 00:10:00      4     2.0
2012-03-17 00:12:00      5     2.0
2012-03-17 00:20:00      6     8.0
2012-03-20 00:43:00      7    23.0
</code></pre>

<hr>

<p><strong><em>simplification:</em></strong></p>

<p>When we perform <code>diff</code>:</p>

<pre><code>In [8]: ser_diff = df.index.to_series().diff()

In [9]: ser_diff
Out[9]: 
time
2012-03-16 23:50:00               NaT
2012-03-16 23:56:00   0 days 00:06:00
2012-03-17 00:08:00   0 days 00:12:00
2012-03-17 00:10:00   0 days 00:02:00
2012-03-17 00:12:00   0 days 00:02:00
2012-03-17 00:20:00   0 days 00:08:00
2012-03-20 00:43:00   3 days 00:23:00
Name: time, dtype: timedelta64[ns]
</code></pre>

<p>Seconds to minutes conversion:</p>

<pre><code>In [10]: ser_diff.dt.seconds.div(60, fill_value=0)
Out[10]: 
time
2012-03-16 23:50:00     0.0
2012-03-16 23:56:00     6.0
2012-03-17 00:08:00    12.0
2012-03-17 00:10:00     2.0
2012-03-17 00:12:00     2.0
2012-03-17 00:20:00     8.0
2012-03-20 00:43:00    23.0
Name: time, dtype: float64
</code></pre>

<hr>

<p>If suppose you want to include even the <code>date</code> portion as it was excluded previously(only time portion was considered), <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.total_seconds.html"" rel=""noreferrer""><code>dt.total_seconds</code></a> would give you the elapsed duration in seconds with which minutes could then be calculated again by division.</p>

<pre><code>In [12]: ser_diff.dt.total_seconds().div(60, fill_value=0)
Out[12]: 
time
2012-03-16 23:50:00       0.0
2012-03-16 23:56:00       6.0
2012-03-17 00:08:00      12.0
2012-03-17 00:10:00       2.0
2012-03-17 00:12:00       2.0
2012-03-17 00:20:00       8.0
2012-03-20 00:43:00    4343.0    # &lt;-- number of minutes in 3 days 23 minutes
Name: time, dtype: float64
</code></pre>
";;"[""In [13]: df['deltaT'] = df.index.to_series().diff().dt.seconds.div(60, fill_value=0)\n    ...: df                                 # use .astype(int) to obtain integer values\nOut[13]: \n                     value  deltaT\ntime                              \n2012-03-16 23:50:00      1     0.0\n2012-03-16 23:56:00      2     6.0\n2012-03-17 00:08:00      3    12.0\n2012-03-17 00:10:00      4     2.0\n2012-03-17 00:12:00      5     2.0\n2012-03-17 00:20:00      6     8.0\n2012-03-20 00:43:00      7    23.0\n"", 'In [8]: ser_diff = df.index.to_series().diff()\n\nIn [9]: ser_diff\nOut[9]: \ntime\n2012-03-16 23:50:00               NaT\n2012-03-16 23:56:00   0 days 00:06:00\n2012-03-17 00:08:00   0 days 00:12:00\n2012-03-17 00:10:00   0 days 00:02:00\n2012-03-17 00:12:00   0 days 00:02:00\n2012-03-17 00:20:00   0 days 00:08:00\n2012-03-20 00:43:00   3 days 00:23:00\nName: time, dtype: timedelta64[ns]\n', 'In [10]: ser_diff.dt.seconds.div(60, fill_value=0)\nOut[10]: \ntime\n2012-03-16 23:50:00     0.0\n2012-03-16 23:56:00     6.0\n2012-03-17 00:08:00    12.0\n2012-03-17 00:10:00     2.0\n2012-03-17 00:12:00     2.0\n2012-03-17 00:20:00     8.0\n2012-03-20 00:43:00    23.0\nName: time, dtype: float64\n', 'In [12]: ser_diff.dt.total_seconds().div(60, fill_value=0)\nOut[12]: \ntime\n2012-03-16 23:50:00       0.0\n2012-03-16 23:56:00       6.0\n2012-03-17 00:08:00      12.0\n2012-03-17 00:10:00       2.0\n2012-03-17 00:12:00       2.0\n2012-03-17 00:20:00       8.0\n2012-03-20 00:43:00    4343.0    # <-- number of minutes in 3 days 23 minutes\nName: time, dtype: float64\n']";"['to_series', 'timedelta64[ns]', '.dt', ""In [13]: df['deltaT'] = df.index.to_series().diff().dt.seconds.div(60, fill_value=0)\n    ...: df                                 # use .astype(int) to obtain integer values\nOut[13]: \n                     value  deltaT\ntime                              \n2012-03-16 23:50:00      1     0.0\n2012-03-16 23:56:00      2     6.0\n2012-03-17 00:08:00      3    12.0\n2012-03-17 00:10:00      4     2.0\n2012-03-17 00:12:00      5     2.0\n2012-03-17 00:20:00      6     8.0\n2012-03-20 00:43:00      7    23.0\n"", 'diff', 'In [8]: ser_diff = df.index.to_series().diff()\n\nIn [9]: ser_diff\nOut[9]: \ntime\n2012-03-16 23:50:00               NaT\n2012-03-16 23:56:00   0 days 00:06:00\n2012-03-17 00:08:00   0 days 00:12:00\n2012-03-17 00:10:00   0 days 00:02:00\n2012-03-17 00:12:00   0 days 00:02:00\n2012-03-17 00:20:00   0 days 00:08:00\n2012-03-20 00:43:00   3 days 00:23:00\nName: time, dtype: timedelta64[ns]\n', 'In [10]: ser_diff.dt.seconds.div(60, fill_value=0)\nOut[10]: \ntime\n2012-03-16 23:50:00     0.0\n2012-03-16 23:56:00     6.0\n2012-03-17 00:08:00    12.0\n2012-03-17 00:10:00     2.0\n2012-03-17 00:12:00     2.0\n2012-03-17 00:20:00     8.0\n2012-03-20 00:43:00    23.0\nName: time, dtype: float64\n', 'date', 'dt.total_seconds', 'In [12]: ser_diff.dt.total_seconds().div(60, fill_value=0)\nOut[12]: \ntime\n2012-03-16 23:50:00       0.0\n2012-03-16 23:56:00       6.0\n2012-03-17 00:08:00      12.0\n2012-03-17 00:10:00       2.0\n2012-03-17 00:12:00       2.0\n2012-03-17 00:20:00       8.0\n2012-03-20 00:43:00    4343.0    # <-- number of minutes in 3 days 23 minutes\nName: time, dtype: float64\n']"
1568;;0;41228807;18215317.0;2;7;;;"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.days.html"" rel=""noreferrer""><code>dt.days</code></a> to obtain the days attribute as integers.</p>

<p>For eg:</p>

<pre><code>In [14]: s = pd.Series(pd.timedelta_range(start='1 days', end='12 days', freq='3000T'))

In [15]: s
Out[15]: 
0    1 days 00:00:00
1    3 days 02:00:00
2    5 days 04:00:00
3    7 days 06:00:00
4    9 days 08:00:00
5   11 days 10:00:00
dtype: timedelta64[ns]

In [16]: s.dt.days
Out[16]: 
0     1
1     3
2     5
3     7
4     9
5    11
dtype: int64
</code></pre>

<p>More generally - You can use the <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.Series.dt.components.html"" rel=""noreferrer""><code>.components</code></a> property to access a reduced form of <code>timedelta</code>.</p>

<pre><code>In [17]: s.dt.components
Out[17]: 
   days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds
0     1      0        0        0             0             0            0
1     3      2        0        0             0             0            0
2     5      4        0        0             0             0            0
3     7      6        0        0             0             0            0
4     9      8        0        0             0             0            0
5    11     10        0        0             0             0            0
</code></pre>

<p>Now, to get the <code>hours</code> attribute:</p>

<pre><code>In [23]: s.dt.components.hours
Out[23]: 
0     0
1     2
2     4
3     6
4     8
5    10
Name: hours, dtype: int64
</code></pre>
";;"[""In [14]: s = pd.Series(pd.timedelta_range(start='1 days', end='12 days', freq='3000T'))\n\nIn [15]: s\nOut[15]: \n0    1 days 00:00:00\n1    3 days 02:00:00\n2    5 days 04:00:00\n3    7 days 06:00:00\n4    9 days 08:00:00\n5   11 days 10:00:00\ndtype: timedelta64[ns]\n\nIn [16]: s.dt.days\nOut[16]: \n0     1\n1     3\n2     5\n3     7\n4     9\n5    11\ndtype: int64\n"", 'In [17]: s.dt.components\nOut[17]: \n   days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds\n0     1      0        0        0             0             0            0\n1     3      2        0        0             0             0            0\n2     5      4        0        0             0             0            0\n3     7      6        0        0             0             0            0\n4     9      8        0        0             0             0            0\n5    11     10        0        0             0             0            0\n', 'In [23]: s.dt.components.hours\nOut[23]: \n0     0\n1     2\n2     4\n3     6\n4     8\n5    10\nName: hours, dtype: int64\n']";"['dt.days', ""In [14]: s = pd.Series(pd.timedelta_range(start='1 days', end='12 days', freq='3000T'))\n\nIn [15]: s\nOut[15]: \n0    1 days 00:00:00\n1    3 days 02:00:00\n2    5 days 04:00:00\n3    7 days 06:00:00\n4    9 days 08:00:00\n5   11 days 10:00:00\ndtype: timedelta64[ns]\n\nIn [16]: s.dt.days\nOut[16]: \n0     1\n1     3\n2     5\n3     7\n4     9\n5    11\ndtype: int64\n"", '.components', 'timedelta', 'In [17]: s.dt.components\nOut[17]: \n   days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds\n0     1      0        0        0             0             0            0\n1     3      2        0        0             0             0            0\n2     5      4        0        0             0             0            0\n3     7      6        0        0             0             0            0\n4     9      8        0        0             0             0            0\n5    11     10        0        0             0             0            0\n', 'hours', 'In [23]: s.dt.components.hours\nOut[23]: \n0     0\n1     2\n2     4\n3     6\n4     8\n5    10\nName: hours, dtype: int64\n']"
1569;;0;41452422;32244753.0;2;7;;;"<p>Some of the above solutions did not work for me. The <code>.fig</code> attribute was not found when I tried that and I was unable to use <code>.savefig()</code> directly. However, what did work was:</p>

<pre><code>sns_plot.figure.savefig(""output.png"")
</code></pre>

<p>I am a newer Python user, so I do not know if this is due to an update. I wanted to mention it in case anybody else runs into the same issues as I did.</p>
";;"['sns_plot.figure.savefig(""output.png"")\n']";"['.fig', '.savefig()', 'sns_plot.figure.savefig(""output.png"")\n']"
1570;;0;41517319;19991445.0;2;8;;;"<p>I don't know if this is new in <code>sklearn</code> or <code>pandas</code>, but I'm able to pass the data frame directly to <code>sklearn</code> without converting the data frame to a numpy array or any other data types.</p>

<pre><code>from sklearn import linear_model

reg = linear_model.LinearRegression()
reg.fit(df[['B', 'C']], df['A'])

&gt;&gt;&gt; reg.coef_
array([  4.01182386e-01,   3.51587361e-04])
</code></pre>
";;"[""from sklearn import linear_model\n\nreg = linear_model.LinearRegression()\nreg.fit(df[['B', 'C']], df['A'])\n\n>>> reg.coef_\narray([  4.01182386e-01,   3.51587361e-04])\n""]";"['sklearn', 'pandas', 'sklearn', ""from sklearn import linear_model\n\nreg = linear_model.LinearRegression()\nreg.fit(df[['B', 'C']], df['A'])\n\n>>> reg.coef_\narray([  4.01182386e-01,   3.51587361e-04])\n""]"
1571;;1;41529411;13784192.0;2;28;;;"<p><strong><em>If you simply want to create an empty data frame and fill it with some incoming data frames later, try this:</em></strong> </p>

<p>In this example I am using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html"" rel=""noreferrer"">this pandas doc</a> to create a new data frame and then using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.append.html"" rel=""noreferrer"">append</a> to write to the newDF with data from oldDF. </p>

<hr>

<p><strong>Have a look at this</strong></p>

<pre><code>newDF = pd.DataFrame() #creates a new dataframe that's empty
newDF = newDF.append(oldDF, ignore_index = True) # ignoring index is optional
# try printing some data from newDF
print newDF.head() #again optional 
</code></pre>

<ul>
<li>if I have to keep appending new data into this newDF from more than
one oldDFs, I just use a for loop to iterate over
<a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.append.html"" rel=""noreferrer"">pandas.DataFrame.append()</a></li>
</ul>
";;"[""newDF = pd.DataFrame() #creates a new dataframe that's empty\nnewDF = newDF.append(oldDF, ignore_index = True) # ignoring index is optional\n# try printing some data from newDF\nprint newDF.head() #again optional \n""]";"[""newDF = pd.DataFrame() #creates a new dataframe that's empty\nnewDF = newDF.append(oldDF, ignore_index = True) # ignoring index is optional\n# try printing some data from newDF\nprint newDF.head() #again optional \n""]"
1572;;0;41554866;26266362.0;2;6;;;"<p>if its just counting nan values in a pandas column here is a quick way</p>

<pre><code>import pandas as pd
## df1 as an example data frame 
## col1 name of column for which you want to calculate the nan values
sum(pd.isnull(df1['col1']))
</code></pre>
";;"[""import pandas as pd\n## df1 as an example data frame \n## col1 name of column for which you want to calculate the nan values\nsum(pd.isnull(df1['col1']))\n""]";"[""import pandas as pd\n## df1 as an example data frame \n## col1 name of column for which you want to calculate the nan values\nsum(pd.isnull(df1['col1']))\n""]"
1573;;3;41607207;34001922.0;2;13;;;"<p>tf.initialize_all_variables() is deprecated. Instead initialize tensorflow variables with: </p>

<pre><code>tf.global_variables_initializer()
</code></pre>

<p>A common example usage is:</p>

<pre><code>with tf.Session() as sess:
     sess.run(tf.global_variables_initializer())
</code></pre>
";;['tf.global_variables_initializer()\n', 'with tf.Session() as sess:\n     sess.run(tf.global_variables_initializer())\n'];['tf.global_variables_initializer()\n', 'with tf.Session() as sess:\n     sess.run(tf.global_variables_initializer())\n']
1574;;2;41678874;20250771.0;2;16;;;"<p>This is an alternative answer that can be much faster when your dictionary has more than a couple of keys.  If your dictionary exhaustively maps all possible values, this takes a very simple form:</p>

<pre><code>df['col1'].map(di)
</code></pre>

<p>Although <code>map</code> most commonly takes a function as its argument, it can alternatively take a dictionary or series:  <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html"" rel=""noreferrer"">Documentation for Pandas.series.map</a></p>

<p>Without an exhaustive mapping, you need to add <code>update</code>:</p>

<pre><code>df['col1'].update( df['col1'].map(di) )   # note: series update is an inplace operation
</code></pre>

<p>Here are some timings for the exhaustive case on a dataframe with 100,000 rows and 8 dictionary keys (it's about 10x faster).</p>

<pre><code>di = {1: ""A"", 2: ""B"", 3: ""C"", 4: ""D"", 5: ""E"", 6: ""F"", 7: ""G"", 8: ""H"" }
df = pd.DataFrame({ 'col1': np.random.choice( range(1,9), 100000 ) })

%timeit df.replace({""col1"": di})
10 loops, best of 3: 55.6 ms per loop

%timeit df['col1'].map(di)
100 loops, best of 3: 4.16 ms per loop
</code></pre>
";;"[""df['col1'].map(di)\n"", ""df['col1'].update( df['col1'].map(di) )   # note: series update is an inplace operation\n"", 'di = {1: ""A"", 2: ""B"", 3: ""C"", 4: ""D"", 5: ""E"", 6: ""F"", 7: ""G"", 8: ""H"" }\ndf = pd.DataFrame({ \'col1\': np.random.choice( range(1,9), 100000 ) })\n\n%timeit df.replace({""col1"": di})\n10 loops, best of 3: 55.6 ms per loop\n\n%timeit df[\'col1\'].map(di)\n100 loops, best of 3: 4.16 ms per loop\n']";"[""df['col1'].map(di)\n"", 'map', 'update', ""df['col1'].update( df['col1'].map(di) )   # note: series update is an inplace operation\n"", 'di = {1: ""A"", 2: ""B"", 3: ""C"", 4: ""D"", 5: ""E"", 6: ""F"", 7: ""G"", 8: ""H"" }\ndf = pd.DataFrame({ \'col1\': np.random.choice( range(1,9), 100000 ) })\n\n%timeit df.replace({""col1"": di})\n10 loops, best of 3: 55.6 ms per loop\n\n%timeit df[\'col1\'].map(di)\n100 loops, best of 3: 4.16 ms per loop\n']"
1575;;0;41802199;29370057.0;2;7;;;"<p>You can use the <code>isin</code> method on the <code>date</code> column like so
<code>df[df[""date""].isin(pd.date_range(start_date, end_date))]</code></p>

<p><strong>Example:</strong>   </p>

<pre><code>import numpy as np   
import pandas as pd

# Make a DataFrame with dates and random numbers
df = pd.DataFrame(np.random.random((30, 3)))
df['date'] = pd.date_range('2017-1-1', periods=30, freq='D')

# Select the rows between two dates
in_range_df = df[df[""date""].isin(pd.date_range(""2017-01-15"", ""2017-01-20""))]

print(in_range_df)  # print result
</code></pre>

<p>which gives</p>

<pre><code>           0         1         2       date
14  0.960974  0.144271  0.839593 2017-01-15
15  0.814376  0.723757  0.047840 2017-01-16
16  0.911854  0.123130  0.120995 2017-01-17
17  0.505804  0.416935  0.928514 2017-01-18
18  0.204869  0.708258  0.170792 2017-01-19
19  0.014389  0.214510  0.045201 2017-01-20
</code></pre>
";;"['import numpy as np   \nimport pandas as pd\n\n# Make a DataFrame with dates and random numbers\ndf = pd.DataFrame(np.random.random((30, 3)))\ndf[\'date\'] = pd.date_range(\'2017-1-1\', periods=30, freq=\'D\')\n\n# Select the rows between two dates\nin_range_df = df[df[""date""].isin(pd.date_range(""2017-01-15"", ""2017-01-20""))]\n\nprint(in_range_df)  # print result\n', '           0         1         2       date\n14  0.960974  0.144271  0.839593 2017-01-15\n15  0.814376  0.723757  0.047840 2017-01-16\n16  0.911854  0.123130  0.120995 2017-01-17\n17  0.505804  0.416935  0.928514 2017-01-18\n18  0.204869  0.708258  0.170792 2017-01-19\n19  0.014389  0.214510  0.045201 2017-01-20\n']";"['isin', 'date', 'df[df[""date""].isin(pd.date_range(start_date, end_date))]', 'import numpy as np   \nimport pandas as pd\n\n# Make a DataFrame with dates and random numbers\ndf = pd.DataFrame(np.random.random((30, 3)))\ndf[\'date\'] = pd.date_range(\'2017-1-1\', periods=30, freq=\'D\')\n\n# Select the rows between two dates\nin_range_df = df[df[""date""].isin(pd.date_range(""2017-01-15"", ""2017-01-20""))]\n\nprint(in_range_df)  # print result\n', '           0         1         2       date\n14  0.960974  0.144271  0.839593 2017-01-15\n15  0.814376  0.723757  0.047840 2017-01-16\n16  0.911854  0.123130  0.120995 2017-01-17\n17  0.505804  0.416935  0.928514 2017-01-18\n18  0.204869  0.708258  0.170792 2017-01-19\n19  0.014389  0.214510  0.045201 2017-01-20\n']"
1576;;1;41816244;22697773.0;2;15;;;"<p>I know this is a bit of an old thread but with pandas 19.02, you can do:</p>

<pre><code>df.select_dtypes(include=['float64']).apply(your_function)
df.select_dtypes(exclude=['string','object']).apply(your_other_function)
</code></pre>

<p><a href=""http://pandas.pydata.org/pandas-docs/version/0.19.2/generated/pandas.DataFrame.select_dtypes.html"" rel=""noreferrer"">http://pandas.pydata.org/pandas-docs/version/0.19.2/generated/pandas.DataFrame.select_dtypes.html</a></p>
";;"[""df.select_dtypes(include=['float64']).apply(your_function)\ndf.select_dtypes(exclude=['string','object']).apply(your_other_function)\n""]";"[""df.select_dtypes(include=['float64']).apply(your_function)\ndf.select_dtypes(exclude=['string','object']).apply(your_other_function)\n""]"
1577;;0;41845355;22898824.0;2;7;;;"<p>And if your dates are standardized by importing datetime package, you can simply use:</p>

<pre><code>df[(df['date']&gt;datetime.date(2016,1,1)) &amp; (df['date']&lt;datetime.date(2016,3,1))]  
</code></pre>

<p>For standarding your date string using datetime package, you can use this function:</p>

<pre><code>import datetime
datetime.datetime.strptime
</code></pre>
";;"[""df[(df['date']>datetime.date(2016,1,1)) & (df['date']<datetime.date(2016,3,1))]  \n"", 'import datetime\ndatetime.datetime.strptime\n']";"[""df[(df['date']>datetime.date(2016,1,1)) & (df['date']<datetime.date(2016,3,1))]  \n"", 'import datetime\ndatetime.datetime.strptime\n']"
1578;;0;41876593;21285380.0;2;9;;;"<p><a href=""https://stackoverflow.com/a/20903553"">This answer</a> uses the DataFrame.filter method to do this without list comprehension:</p>

<pre><code>import pandas as pd

data = {'spike-2': [1,2,3], 'hey spke': [4,5,6]}
df = pd.DataFrame(data)

print(df.filter(like='spike').columns)
</code></pre>

<p>Will output just 'spike-2'.  You can also use regex, as some people suggested in comments above:</p>

<pre><code>print(df.filter(regex='spike|spke').columns)
</code></pre>

<p>Will output both columns: ['spike-2', 'hey spke']</p>
";;"[""import pandas as pd\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6]}\ndf = pd.DataFrame(data)\n\nprint(df.filter(like='spike').columns)\n"", ""print(df.filter(regex='spike|spke').columns)\n""]";"[""import pandas as pd\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6]}\ndf = pd.DataFrame(data)\n\nprint(df.filter(like='spike').columns)\n"", ""print(df.filter(regex='spike|spke').columns)\n""]"
1579;;2;41880513;32400867.0;2;35;;;"<p>In the latest version of pandas (<code>0.19.2</code>) you can directly pass the url</p>

<pre><code>import pandas as pd

url=""https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv""
c=pd.read_csv(url)
</code></pre>
";;"['import pandas as pd\n\nurl=""https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv""\nc=pd.read_csv(url)\n']";"['0.19.2', 'import pandas as pd\n\nurl=""https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv""\nc=pd.read_csv(url)\n']"
1580;;1;42094658;14744068.0;2;10;;;"<p>A nice way to do this in one line using <code>pandas.concat()</code>:</p>

<pre><code>import pandas as pd

pd.concat([df], keys=['Foo'], names=['Firstlevel'])
</code></pre>

<p>This can be generalized to many data frames, see the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html"" rel=""noreferrer"">docs</a>.</p>
";;"[""import pandas as pd\n\npd.concat([df], keys=['Foo'], names=['Firstlevel'])\n""]";"['pandas.concat()', ""import pandas as pd\n\npd.concat([df], keys=['Foo'], names=['Firstlevel'])\n""]"
1581;;0;42133330;14057007.0;2;11;;;"<p>You have many options. Collating some of the answers above and the <a href=""https://stackoverflow.com/questions/19960077/how-to-implement-in-and-not-in-for-pandas-dataframe"">accepted answer from this post</a> you can do:<br>
1. <code>df[-df[""column""].isin([""value""])]</code><br>
2. <code>df[~df[""column""].isin([""value""])]</code><br>
3. <code>df[df[""column""].isin([""value""]) == False]</code><br>
4. <code>df[np.logical_not(df[""column""].isin([""value""]))]</code></p>

<p>Note: for option 4 for you'll need to <code>import numpy as np</code></p>
";;[];"['df[-df[""column""].isin([""value""])]', 'df[~df[""column""].isin([""value""])]', 'df[df[""column""].isin([""value""]) == False]', 'df[np.logical_not(df[""column""].isin([""value""]))]', 'import numpy as np']"
1582;;1;42247228;25646200.0;2;8;;;"<p>Use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.days.html"" rel=""noreferrer""><code>dt.days</code></a> attribute. Supposing <code>td</code> is the name of your timedelta Series, access this attribute via:</p>

<pre><code>td.dt.days
</code></pre>
";;['td.dt.days\n'];['dt.days', 'td', 'td.dt.days\n']
1583;;0;42293737;25351968.0;2;7;;;"<pre><code>pd.set_option('display.max_columns', None)  
</code></pre>

<p><code>id</code> can fully show the columns. </p>
";;"[""pd.set_option('display.max_columns', None)  \n""]";"[""pd.set_option('display.max_columns', None)  \n"", 'id']"
1584;;1;42335527;42157944.0;2;11;;;"<p>Before pulling out the multiprocessing hammer, your first step should be to do some profiling. Use cProfile to quickly look through to identify which functions are taking a long time. Unfortunately if your lines are all in a single function call, they'll show up as library calls. line_profiler is better but takes a little more setup time. </p>

<p>NOTE. If using ipython, you can use %timeit (magic command for the timeit module) and %prun (magic command for the profile module) both to time your statements as well as functions. A google search will show some guides.</p>

<p>Pandas is a wonderful library, but I've been an occasional victim of using it poorly with atrocious results. In particular, be wary of  append()/concat() operations. That might be your bottleneck but you should profile to be sure.  Usually, the numpy.vstack() and numpy.hstack() operations are faster if you don't need to perform index/column alignment. In your case it looks like you might be able to get by with Series or 1-D numpy ndarrays which can save time.</p>

<p>BTW, a <code>try</code> block in python is much slower often 10x or more than checking for an invalid condition, so be sure you absolutely need it when sticking it into a loop for every single line. This is probably the other hogger of time; I imagine you stuck the try block to check for AttributeError in case of a match.group(1) failure. I would check for a valid match first. </p>

<p>Even these small modifications should be enough for your program to run significantly faster before trying anything drastic like multiprocessing. Those Python libraries are awesome but bring a fresh set of challenges to deal with.</p>
";;[];['try']
1585;;1;42392805;26873127.0;2;10;;;"<p>This answer is based on the 2nd tip from this blog post: <a href=""https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/"" rel=""nofollow noreferrer"">28 Jupyter Notebook tips, tricks and shortcuts</a></p>

<p>You can add the following code to the top of your notebook</p>

<pre><code>from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = ""all""
</code></pre>

<p>This tells Jupyter to print the results for any variable or statement on its own line. So you can then execute a cell solely containing   </p>

<pre><code>df1
df2
</code></pre>

<p>and it will ""print out the beautiful tables for both datasets"".</p>
";;"['from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = ""all""\n', 'df1\ndf2\n']";"['from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = ""all""\n', 'df1\ndf2\n']"
1586;;1;42401977;42157944.0;2;7;;;"<p>I've used this many times as it's a particular easy implementation of multiprocessing. </p>

<pre><code>import pandas as pd
from multiprocessing import Pool

def reader(filename):
    return pd.read_excel(filename)

def main():
    pool = Pool(4) # number of cores you want to use
    file_list = [file1.xlsx, file2.xlsx, file3.xlsx, ...]
    df_list = pool.map(reader, file_list) #creates a list of the loaded df's
    df = pd.concat(df_list) # concatenates all the df's into a single df

if __name__ == '__main__':
    main()
</code></pre>

<p>Using this you should be able to substantially increase the speed of your program without too much work at all. If you don't know how many processors you have, you can check by pulling up your shell and typing</p>

<pre><code>echo %NUMBER_OF_PROCESSORS%
</code></pre>

<p>EDIT: To make this run even faster, consider changing your files to csvs and using pandas function <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""nofollow noreferrer"">pandas.read_csv</a></p>
";;"[""import pandas as pd\nfrom multiprocessing import Pool\n\ndef reader(filename):\n    return pd.read_excel(filename)\n\ndef main():\n    pool = Pool(4) # number of cores you want to use\n    file_list = [file1.xlsx, file2.xlsx, file3.xlsx, ...]\n    df_list = pool.map(reader, file_list) #creates a list of the loaded df's\n    df = pd.concat(df_list) # concatenates all the df's into a single df\n\nif __name__ == '__main__':\n    main()\n"", 'echo %NUMBER_OF_PROCESSORS%\n']";"[""import pandas as pd\nfrom multiprocessing import Pool\n\ndef reader(filename):\n    return pd.read_excel(filename)\n\ndef main():\n    pool = Pool(4) # number of cores you want to use\n    file_list = [file1.xlsx, file2.xlsx, file3.xlsx, ...]\n    df_list = pool.map(reader, file_list) #creates a list of the loaded df's\n    df = pd.concat(df_list) # concatenates all the df's into a single df\n\nif __name__ == '__main__':\n    main()\n"", 'echo %NUMBER_OF_PROCESSORS%\n']"
1587;;4;42426751;42347868.0;2;18;;;"<p>Turns out this is incredibly easy once you realize how the function actually works... </p>

<pre><code>print(
    df
    .head(10)
    .to_string(
        formatters={""total_bill"": ""${:,.2f}"".format, 
                    ""tip"": ""${:,.2f}"".format,
                    ""date"": lambda x: ""{:%m/%d/%Y}"".format(pd.to_datetime(x, unit=""D""))
        }
    )
)

  total_bill   tip     sex smoker  day    time  size       date
0     $16.99 $1.01  Female     No  Sun  Dinner     2 02/08/2017
1     $10.34 $1.66    Male     No  Sun  Dinner     3 02/09/2017
2     $21.01 $3.50    Male     No  Sun  Dinner     3 02/10/2017
3     $23.68 $3.31    Male     No  Sun  Dinner     2 02/11/2017
4     $24.59 $3.61  Female     No  Sun  Dinner     4 02/12/2017
5     $25.29 $4.71    Male     No  Sun  Dinner     4 02/13/2017
6      $8.77 $2.00    Male     No  Sun  Dinner     2 02/14/2017
7     $26.88 $3.12    Male     No  Sun  Dinner     4 02/15/2017
8     $15.04 $1.96    Male     No  Sun  Dinner     2 02/16/2017
9     $14.78 $3.23    Male     No  Sun  Dinner     2 02/17/2017
</code></pre>
";;"['print(\n    df\n    .head(10)\n    .to_string(\n        formatters={""total_bill"": ""${:,.2f}"".format, \n                    ""tip"": ""${:,.2f}"".format,\n                    ""date"": lambda x: ""{:%m/%d/%Y}"".format(pd.to_datetime(x, unit=""D""))\n        }\n    )\n)\n\n  total_bill   tip     sex smoker  day    time  size       date\n0     $16.99 $1.01  Female     No  Sun  Dinner     2 02/08/2017\n1     $10.34 $1.66    Male     No  Sun  Dinner     3 02/09/2017\n2     $21.01 $3.50    Male     No  Sun  Dinner     3 02/10/2017\n3     $23.68 $3.31    Male     No  Sun  Dinner     2 02/11/2017\n4     $24.59 $3.61  Female     No  Sun  Dinner     4 02/12/2017\n5     $25.29 $4.71    Male     No  Sun  Dinner     4 02/13/2017\n6      $8.77 $2.00    Male     No  Sun  Dinner     2 02/14/2017\n7     $26.88 $3.12    Male     No  Sun  Dinner     4 02/15/2017\n8     $15.04 $1.96    Male     No  Sun  Dinner     2 02/16/2017\n9     $14.78 $3.23    Male     No  Sun  Dinner     2 02/17/2017\n']";"['print(\n    df\n    .head(10)\n    .to_string(\n        formatters={""total_bill"": ""${:,.2f}"".format, \n                    ""tip"": ""${:,.2f}"".format,\n                    ""date"": lambda x: ""{:%m/%d/%Y}"".format(pd.to_datetime(x, unit=""D""))\n        }\n    )\n)\n\n  total_bill   tip     sex smoker  day    time  size       date\n0     $16.99 $1.01  Female     No  Sun  Dinner     2 02/08/2017\n1     $10.34 $1.66    Male     No  Sun  Dinner     3 02/09/2017\n2     $21.01 $3.50    Male     No  Sun  Dinner     3 02/10/2017\n3     $23.68 $3.31    Male     No  Sun  Dinner     2 02/11/2017\n4     $24.59 $3.61  Female     No  Sun  Dinner     4 02/12/2017\n5     $25.29 $4.71    Male     No  Sun  Dinner     4 02/13/2017\n6      $8.77 $2.00    Male     No  Sun  Dinner     2 02/14/2017\n7     $26.88 $3.12    Male     No  Sun  Dinner     4 02/15/2017\n8     $15.04 $1.96    Male     No  Sun  Dinner     2 02/16/2017\n9     $14.78 $3.23    Male     No  Sun  Dinner     2 02/17/2017\n']"
1588;;0;42545576;22149584.0;2;6;;;"<p><code>axis</code> refers to the dimension of the array, in the case of <code>pd.DataFrame</code>s <code>axis=0</code> is the dimension that points downwards and <code>axis=1</code> the one that points to the right. </p>

<p><strong>Example:</strong> Think of an <code>ndarray</code> with shape <code>(3,5,7)</code>.   </p>

<pre><code>a = np.ones((3,5,7))
</code></pre>

<p><code>a</code> is a 3 dimensional <code>ndarray</code>, i.e. it has <strong>3 axes</strong> (""axes"" is plural of ""axis""). The configuration of <code>a</code> will look like 3 slices of bread where each slice is of dimension 5-by-7. <code>a[0,:,:]</code> will refer to the 0-th slice, <code>a[1,:,:]</code> will refer to the 1-st slice etc.</p>

<p><code>a.sum(axis=0)</code> will apply <code>sum()</code> along the 0-th axis of <code>a</code>. You will add all the slices and end up with one slice of shape <code>(5,7)</code>.</p>

<p><code>a.sum(axis=0)</code> is equivalent to </p>

<pre><code>b = np.zeros((5,7))
for i in range(5):
    for j in range(7):
        b[i,j] += a[:,i,j].sum()
</code></pre>

<p><code>b</code> and <code>a.sum(axis=0)</code> will both look like this</p>

<pre><code>array([[ 3.,  3.,  3.,  3.,  3.,  3.,  3.],
       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.],
       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.],
       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.],
       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.]])
</code></pre>

<p>In a <code>pd.DataFrame</code>, axes work the same way as in <code>numpy.array</code>s: <code>axis=0</code> will apply <code>sum()</code> or any other reduction function for each column. </p>

<p><strong>N.B.</strong> In @zhangxaochen's answer, I find the phrases ""along the rows"" and ""along the columns"" slightly confusing. <code>axis=0</code> should refer to ""along each column"", and <code>axis=1</code> ""along each row"".</p>
";;['a = np.ones((3,5,7))\n', 'b = np.zeros((5,7))\nfor i in range(5):\n    for j in range(7):\n        b[i,j] += a[:,i,j].sum()\n', 'array([[ 3.,  3.,  3.,  3.,  3.,  3.,  3.],\n       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.],\n       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.],\n       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.],\n       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.]])\n'];['axis', 'pd.DataFrame', 'axis=0', 'axis=1', 'ndarray', '(3,5,7)', 'a = np.ones((3,5,7))\n', 'a', 'ndarray', 'a', 'a[0,:,:]', 'a[1,:,:]', 'a.sum(axis=0)', 'sum()', 'a', '(5,7)', 'a.sum(axis=0)', 'b = np.zeros((5,7))\nfor i in range(5):\n    for j in range(7):\n        b[i,j] += a[:,i,j].sum()\n', 'b', 'a.sum(axis=0)', 'array([[ 3.,  3.,  3.,  3.,  3.,  3.,  3.],\n       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.],\n       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.],\n       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.],\n       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.]])\n', 'pd.DataFrame', 'numpy.array', 'axis=0', 'sum()', 'axis=0', 'axis=1']
1589;;0;42550516;22219004.0;2;6;;;"<p>If performance is important go down to numpy level:</p>

<pre><code>import numpy as np

df = pd.DataFrame( {'a':np.random.randint(0,60,600), 'b':[1,2,5,5,4,6]*100})

def f(df):
         keys,values=df.sort_values('a').values.T
         ukeys,index=np.unique(keys,True)
         arrays=np.split(values,index[1:])
         df2=pd.DataFrame({'a':ukeys,'b':[list(a) for a in arrays]})
         return df2
</code></pre>

<p>Tests:</p>

<pre><code>In [301]: %timeit f(df)
1000 loops, best of 3: 1.64 ms per loop

In [302]: %timeit df.groupby('a')['b'].apply(list)
100 loops, best of 3: 5.26 ms per loop
</code></pre>
";;"[""import numpy as np\n\ndf = pd.DataFrame( {'a':np.random.randint(0,60,600), 'b':[1,2,5,5,4,6]*100})\n\ndef f(df):\n         keys,values=df.sort_values('a').values.T\n         ukeys,index=np.unique(keys,True)\n         arrays=np.split(values,index[1:])\n         df2=pd.DataFrame({'a':ukeys,'b':[list(a) for a in arrays]})\n         return df2\n"", ""In [301]: %timeit f(df)\n1000 loops, best of 3: 1.64 ms per loop\n\nIn [302]: %timeit df.groupby('a')['b'].apply(list)\n100 loops, best of 3: 5.26 ms per loop\n""]";"[""import numpy as np\n\ndf = pd.DataFrame( {'a':np.random.randint(0,60,600), 'b':[1,2,5,5,4,6]*100})\n\ndef f(df):\n         keys,values=df.sort_values('a').values.T\n         ukeys,index=np.unique(keys,True)\n         arrays=np.split(values,index[1:])\n         df2=pd.DataFrame({'a':ukeys,'b':[list(a) for a in arrays]})\n         return df2\n"", ""In [301]: %timeit f(df)\n1000 loops, best of 3: 1.64 ms per loop\n\nIn [302]: %timeit df.groupby('a')['b'].apply(list)\n100 loops, best of 3: 5.26 ms per loop\n""]"
1590;;1;42837693;17091769.0;2;10;;;"<p>My approach was, but I can't guarantee that this is the fastest solution.</p>

<pre><code>df = pd.Dataframe(columns=[""firstname"", ""lastname""])
df = df.append({
     ""firstname"": ""John"",
     ""lastname"":  ""Johny""
      }, ignore_index=True)
</code></pre>
";;"['df = pd.Dataframe(columns=[""firstname"", ""lastname""])\ndf = df.append({\n     ""firstname"": ""John"",\n     ""lastname"":  ""Johny""\n      }, ignore_index=True)\n']";"['df = pd.Dataframe(columns=[""firstname"", ""lastname""])\ndf = df.append({\n     ""firstname"": ""John"",\n     ""lastname"":  ""Johny""\n      }, ignore_index=True)\n']"
1591;;4;42932524;38250710.0;2;8;;;"<p>However, one approach to dividing the dataset into <code>train</code>, <code>test</code>, <code>cv</code> with <code>0.6</code>, <code>0.2</code>, <code>0.2</code> would be to use the <code>train_test_split</code> method twice.</p>

<pre><code>x, x_test, y, y_test = train_test_split(xtrain,labels,test_size=0.2,train_size=0.8)
x_train, x_cv, y_train, y_cv = train_test_split(x,y,test_size = 0.25,train_size =0.75)
</code></pre>
";;['x, x_test, y, y_test = train_test_split(xtrain,labels,test_size=0.2,train_size=0.8)\nx_train, x_cv, y_train, y_cv = train_test_split(x,y,test_size = 0.25,train_size =0.75)\n'];['train', 'test', 'cv', '0.6', '0.2', '0.2', 'train_test_split', 'x, x_test, y, y_test = train_test_split(xtrain,labels,test_size=0.2,train_size=0.8)\nx_train, x_cv, y_train, y_cv = train_test_split(x,y,test_size = 0.25,train_size =0.75)\n']
1592;;2;42965916;26139423.0;2;7;;;"<p>Here's a succinct and generic solution to use a seaborn color palette.</p>

<p>First <a href=""http://seaborn.pydata.org/tutorial/color_palettes.html"" rel=""noreferrer"">find a color palette</a> you like and optionally visualize it:</p>

<pre><code>sns.palplot(sns.color_palette(""Set2"", 8))
</code></pre>

<p>Then you can use it with <code>matplotlib</code> doing this:</p>

<pre><code># Unique category labels: 'D', 'F', 'G', ...
color_labels = df['color'].unique()

# List of RGB triplets
rgb_values = sns.color_palette(""Set2"", 8)

# Map label to RGB
color_map = dict(zip(color_labels, rgb_values))

# Finally use the mapped values
plt.scatter(df['carat'], df['price'], c=df['color'].map(color_map))
</code></pre>
";;"['sns.palplot(sns.color_palette(""Set2"", 8))\n', '# Unique category labels: \'D\', \'F\', \'G\', ...\ncolor_labels = df[\'color\'].unique()\n\n# List of RGB triplets\nrgb_values = sns.color_palette(""Set2"", 8)\n\n# Map label to RGB\ncolor_map = dict(zip(color_labels, rgb_values))\n\n# Finally use the mapped values\nplt.scatter(df[\'carat\'], df[\'price\'], c=df[\'color\'].map(color_map))\n']";"['sns.palplot(sns.color_palette(""Set2"", 8))\n', 'matplotlib', '# Unique category labels: \'D\', \'F\', \'G\', ...\ncolor_labels = df[\'color\'].unique()\n\n# List of RGB triplets\nrgb_values = sns.color_palette(""Set2"", 8)\n\n# Map label to RGB\ncolor_map = dict(zip(color_labels, rgb_values))\n\n# Finally use the mapped values\nplt.scatter(df[\'carat\'], df[\'price\'], c=df[\'color\'].map(color_map))\n']"
1593;;1;42977946;29432629.0;2;10;;;"<p>You can observe the relation between features either by drawing a heat map from seaborn or scatter matrix from pandas. </p>

<p>Scatter Matrix:</p>

<pre><code>pd.scatter_matrix(dataframe, alpha = 0.3, figsize = (14,8), diagonal = 'kde');
</code></pre>

<p>If you want to visualize each feature's skewness as well - use seaborn pairplots. </p>

<pre><code>sns.pairplot(dataframe)
</code></pre>

<p>Sns Heatmap:</p>

<pre><code>import seaborn as sns

f, ax = pl.subplots(figsize=(10, 8))
corr = dataframe.corr()
sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),
            square=True, ax=ax)
</code></pre>

<p>The output will be a correlation map of the features. i.e. see the below example. </p>

<p><a href=""https://i.stack.imgur.com/DSvaM.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/DSvaM.png"" alt=""enter image description here""></a></p>

<p>The correlation between grocery and detergents is high. Similarly:</p>

Pdoducts With High Correlation:

<ol>
<li>Grocery and Detergents. </li>
</ol>

Products With Medium Correlation:

<ol>
<li>Milk and Grocery</li>
<li>Milk and Detergents_Paper</li>
</ol>

Products With Low Correlation:

<ol>
<li>Milk and Deli</li>
<li>Frozen and Fresh.</li>
<li>Frozen and Deli. </li>
</ol>

<p>From Pairplots: You can observe same set of relations from pairplots or scatter matrix. But from these we can say that whether the data is normally distributed or not. </p>

<p><a href=""https://i.stack.imgur.com/W7toh.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/W7toh.png"" alt=""enter image description here""></a></p>

<p>Note: The above is same graph taken from the data, which is used to draw heatmap.</p>
";;"[""pd.scatter_matrix(dataframe, alpha = 0.3, figsize = (14,8), diagonal = 'kde');\n"", 'sns.pairplot(dataframe)\n', 'import seaborn as sns\n\nf, ax = pl.subplots(figsize=(10, 8))\ncorr = dataframe.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\n']";"[""pd.scatter_matrix(dataframe, alpha = 0.3, figsize = (14,8), diagonal = 'kde');\n"", 'sns.pairplot(dataframe)\n', 'import seaborn as sns\n\nf, ax = pl.subplots(figsize=(10, 8))\ncorr = dataframe.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\n']"
1594;;1;42978156;20845213.0;2;7;;;"<p>There are two ways to handle the situation where we do not want the index to be stored in csv file.</p>

<ol>
<li><p>As others have stated you can use <strong><em>index=False</em></strong> while saving your<br>
dataframe to csv file.</p>

<blockquote>
  <p><code>df.to_csv('file_name.csv',index=False)</code></p>
</blockquote></li>
<li>You can also save your dataframe into file as it is with an index and then while reading this file, you can drop the column <strong>unnamed 0</strong> containing your previous index.

<blockquote>
  <p><code>df.to_csv(' file_name.csv ')</code><br>
  <code>df_new = pd.read_csv('file_name.csv').drop(['unnamed 0'],axis=1)</code></p>
</blockquote></li>
</ol>
";;[];"[""df.to_csv('file_name.csv',index=False)"", ""df.to_csv(' file_name.csv ')"", ""df_new = pd.read_csv('file_name.csv').drop(['unnamed 0'],axis=1)""]"
1595;;0;43093390;23199796.0;2;14;;;"<p>For each of your dataframe column, you could get quantile with:</p>

<pre><code>q = df[""col""].quantile(0.99)
</code></pre>

<p>and then filter with:</p>

<pre><code>df[df[""col""] &lt; q]
</code></pre>
";;"['q = df[""col""].quantile(0.99)\n', 'df[df[""col""] < q]\n']";"['q = df[""col""].quantile(0.99)\n', 'df[df[""col""] < q]\n']"
1596;;1;43180437;12555323.0;2;6;;;"<h1>Super simple column assignment</h1>

<p>A pandas dataframe is implemented as an ordered dict of columns.</p>

<p>This means that the <code>__getitem__</code> <code>[]</code> can not only be used to get a certain column, but <code>__setitem__</code> <code>[] =</code> can be used to assign a new column.</p>

<p>For example, this dataframe can have a column added to it by simply using the <code>[]</code> accessor</p>

<pre><code>    size      name color
0    big      rose   red
1  small    violet  blue
2  small     tulip   red
3  small  harebell  blue

df['protected'] = ['no', 'no', 'no', 'yes']

    size      name color protected
0    big      rose   red        no
1  small    violet  blue        no
2  small     tulip   red        no
3  small  harebell  blue       yes
</code></pre>

<p>Note that this works even if the index of the dataframe is off.</p>

<pre><code>df.index = [3,2,1,0]
df['protected'] = ['no', 'no', 'no', 'yes']
    size      name color protected
3    big      rose   red        no
2  small    violet  blue        no
1  small     tulip   red        no
0  small  harebell  blue       yes
</code></pre>

<h3>[]= is the way to go, but watch out!</h3>

<p>However, if you have a <code>pd.Series</code> and try to assign it to a dataframe where the indexes are off, you will run in to trouble. See example:</p>

<pre><code>df['protected'] = pd.Series(['no', 'no', 'no', 'yes'])
    size      name color protected
3    big      rose   red       yes
2  small    violet  blue        no
1  small     tulip   red        no
0  small  harebell  blue        no
</code></pre>

<p>This is because a <code>pd.Series</code> by default has an index enumerated from 0 to n. And the pandas <code>[] =</code> method <strong>tries</strong> <em>to be ""smart""</em></p>

<h2>What actually is going on.</h2>

<p>When you use the <code>[] =</code> method pandas is quietly performing an outer join or outer merge using the index of the left hand dataframe and the index of the right hand series. <code>df['column'] = series</code></p>

<h3>Side note</h3>

<p>This quickly causes cognitive dissonance, since the <code>[]=</code> method is trying to do a lot of different things depending on the input, and the outcome cannot be predicted unless you <em>just know</em> how pandas works. I would therefore advice against the <code>[]=</code> in code bases, but when exploring data in a notebook, it is fine.</p>

<h2>Going around the problem</h2>

<p>If you have a <code>pd.Series</code> and want it assigned from top to bottom, or if you are coding productive code and you are not sure of the index order, it is worth it to safeguard for this kind of issue.</p>

<p>You could downcast the <code>pd.Series</code> to a <code>np.ndarray</code> or a <code>list</code>, this will do the trick.</p>

<pre><code>df['protected'] = pd.Series(['no', 'no', 'no', 'yes']).values
</code></pre>

<p>or</p>

<pre><code>df['protected'] = list(pd.Series(['no', 'no', 'no', 'yes']))
</code></pre>

<p><strong>But this is not very explicit.</strong></p>

<p>Some coder may come along and say ""Hey, this looks redundant, I'll just optimize this away"".</p>

<h3>Explicit way</h3>

<p>Setting the index of the <code>pd.Series</code> to be the index of the <code>df</code> is explicit.</p>

<pre><code>df['protected'] = pd.Series(['no', 'no', 'no', 'yes'], index=df.index)
</code></pre>

<p>Or more realistically, you probably have a <code>pd.Series</code> already available.</p>

<pre><code>protected_series = pd.Series(['no', 'no', 'no', 'yes'])
protected_series.index = df.index

3     no
2     no
1     no
0    yes
</code></pre>

<p>Can now be assigned</p>

<pre><code>df['protected'] = protected_series

    size      name color protected
3    big      rose   red        no
2  small    violet  blue        no
1  small     tulip   red        no
0  small  harebell  blue       yes
</code></pre>

<h2>Alternative way with <code>df.reset_index()</code></h2>

<p>Since the index dissonance is the problem, if you feel that the index of the dataframe <em>should</em> not dictate things, you can simply drop the index, this should be faster, but it is not very clean, since your function now <em>probably</em> does two things.</p>

<pre><code>df.reset_index(drop=True)
protected_series.reset_index(drop=True)
df['protected'] = protected_series

    size      name color protected
0    big      rose   red        no
1  small    violet  blue        no
2  small     tulip   red        no
3  small  harebell  blue       yes
</code></pre>

<h2>Note on <code>df.assign</code></h2>

<p>While <code>df.assign</code> make it more explicit what you are doing, it actually has all the same problems as the above <code>[]=</code></p>

<pre><code>df.assign(protected=pd.Series(['no', 'no', 'no', 'yes']))
    size      name color protected
3    big      rose   red       yes
2  small    violet  blue        no
1  small     tulip   red        no
0  small  harebell  blue        no
</code></pre>

<p>Just watch out with <code>df.assign</code> that your column is not called <code>self</code>. It will cause errors. This makes <code>df.assign</code> <strong>smelly</strong>, since there are these kind of artifacts in the function.</p>

<pre><code>df.assign(self=pd.Series(['no', 'no', 'no', 'yes'])
TypeError: assign() got multiple values for keyword argument 'self'
</code></pre>

<p>You may say, ""Well, I'll just not use <code>self</code> then"". But who knows how this function changes in the future to support new arguments. Maybe your column name will be an argument in a new update of pandas, causing problems with upgrading.</p>
";;"[""    size      name color\n0    big      rose   red\n1  small    violet  blue\n2  small     tulip   red\n3  small  harebell  blue\n\ndf['protected'] = ['no', 'no', 'no', 'yes']\n\n    size      name color protected\n0    big      rose   red        no\n1  small    violet  blue        no\n2  small     tulip   red        no\n3  small  harebell  blue       yes\n"", ""df.index = [3,2,1,0]\ndf['protected'] = ['no', 'no', 'no', 'yes']\n    size      name color protected\n3    big      rose   red        no\n2  small    violet  blue        no\n1  small     tulip   red        no\n0  small  harebell  blue       yes\n"", ""df['protected'] = pd.Series(['no', 'no', 'no', 'yes'])\n    size      name color protected\n3    big      rose   red       yes\n2  small    violet  blue        no\n1  small     tulip   red        no\n0  small  harebell  blue        no\n"", ""df['protected'] = pd.Series(['no', 'no', 'no', 'yes']).values\n"", ""df['protected'] = list(pd.Series(['no', 'no', 'no', 'yes']))\n"", ""df['protected'] = pd.Series(['no', 'no', 'no', 'yes'], index=df.index)\n"", ""protected_series = pd.Series(['no', 'no', 'no', 'yes'])\nprotected_series.index = df.index\n\n3     no\n2     no\n1     no\n0    yes\n"", ""df['protected'] = protected_series\n\n    size      name color protected\n3    big      rose   red        no\n2  small    violet  blue        no\n1  small     tulip   red        no\n0  small  harebell  blue       yes\n"", ""df.reset_index(drop=True)\nprotected_series.reset_index(drop=True)\ndf['protected'] = protected_series\n\n    size      name color protected\n0    big      rose   red        no\n1  small    violet  blue        no\n2  small     tulip   red        no\n3  small  harebell  blue       yes\n"", ""df.assign(protected=pd.Series(['no', 'no', 'no', 'yes']))\n    size      name color protected\n3    big      rose   red       yes\n2  small    violet  blue        no\n1  small     tulip   red        no\n0  small  harebell  blue        no\n"", ""df.assign(self=pd.Series(['no', 'no', 'no', 'yes'])\nTypeError: assign() got multiple values for keyword argument 'self'\n""]";"['__getitem__', '[]', '__setitem__', '[] =', '[]', ""    size      name color\n0    big      rose   red\n1  small    violet  blue\n2  small     tulip   red\n3  small  harebell  blue\n\ndf['protected'] = ['no', 'no', 'no', 'yes']\n\n    size      name color protected\n0    big      rose   red        no\n1  small    violet  blue        no\n2  small     tulip   red        no\n3  small  harebell  blue       yes\n"", ""df.index = [3,2,1,0]\ndf['protected'] = ['no', 'no', 'no', 'yes']\n    size      name color protected\n3    big      rose   red        no\n2  small    violet  blue        no\n1  small     tulip   red        no\n0  small  harebell  blue       yes\n"", 'pd.Series', ""df['protected'] = pd.Series(['no', 'no', 'no', 'yes'])\n    size      name color protected\n3    big      rose   red       yes\n2  small    violet  blue        no\n1  small     tulip   red        no\n0  small  harebell  blue        no\n"", 'pd.Series', '[] =', '[] =', ""df['column'] = series"", '[]=', '[]=', 'pd.Series', 'pd.Series', 'np.ndarray', 'list', ""df['protected'] = pd.Series(['no', 'no', 'no', 'yes']).values\n"", ""df['protected'] = list(pd.Series(['no', 'no', 'no', 'yes']))\n"", 'pd.Series', 'df', ""df['protected'] = pd.Series(['no', 'no', 'no', 'yes'], index=df.index)\n"", 'pd.Series', ""protected_series = pd.Series(['no', 'no', 'no', 'yes'])\nprotected_series.index = df.index\n\n3     no\n2     no\n1     no\n0    yes\n"", ""df['protected'] = protected_series\n\n    size      name color protected\n3    big      rose   red        no\n2  small    violet  blue        no\n1  small     tulip   red        no\n0  small  harebell  blue       yes\n"", 'df.reset_index()', ""df.reset_index(drop=True)\nprotected_series.reset_index(drop=True)\ndf['protected'] = protected_series\n\n    size      name color protected\n0    big      rose   red        no\n1  small    violet  blue        no\n2  small     tulip   red        no\n3  small  harebell  blue       yes\n"", 'df.assign', 'df.assign', '[]=', ""df.assign(protected=pd.Series(['no', 'no', 'no', 'yes']))\n    size      name color protected\n3    big      rose   red       yes\n2  small    violet  blue        no\n1  small     tulip   red        no\n0  small  harebell  blue        no\n"", 'df.assign', 'self', 'df.assign', ""df.assign(self=pd.Series(['no', 'no', 'no', 'yes'])\nTypeError: assign() got multiple values for keyword argument 'self'\n"", 'self']"
1597;;1;43190411;19377969.0;2;6;;;"<p>Here is an implementation that I find very versatile: </p>

<pre><code>In [1]: import pandas as pd 

In [2]: df = pd.DataFrame([[0, 'the', 'quick', 'brown'],
   ...:                    [1, 'fox', 'jumps', 'over'], 
   ...:                    [2, 'the', 'lazy', 'dog']],
   ...:                   columns=['c0', 'c1', 'c2', 'c3'])

In [3]: def str_join(df, sep, *cols):
   ...:     from functools import reduce
   ...:     return reduce(lambda x, y: x.astype(str).str.cat(y.astype(str), sep=sep), 
   ...:                   [df[col] for col in cols])
   ...: 

In [4]: df['cat'] = str_join(df, '-', 'c0', 'c1', 'c2', 'c3')

In [5]: df
Out[5]: 
   c0   c1     c2     c3                cat
0   0  the  quick  brown  0-the-quick-brown
1   1  fox  jumps   over   1-fox-jumps-over
2   2  the   lazy    dog     2-the-lazy-dog
</code></pre>
";;"[""In [1]: import pandas as pd \n\nIn [2]: df = pd.DataFrame([[0, 'the', 'quick', 'brown'],\n   ...:                    [1, 'fox', 'jumps', 'over'], \n   ...:                    [2, 'the', 'lazy', 'dog']],\n   ...:                   columns=['c0', 'c1', 'c2', 'c3'])\n\nIn [3]: def str_join(df, sep, *cols):\n   ...:     from functools import reduce\n   ...:     return reduce(lambda x, y: x.astype(str).str.cat(y.astype(str), sep=sep), \n   ...:                   [df[col] for col in cols])\n   ...: \n\nIn [4]: df['cat'] = str_join(df, '-', 'c0', 'c1', 'c2', 'c3')\n\nIn [5]: df\nOut[5]: \n   c0   c1     c2     c3                cat\n0   0  the  quick  brown  0-the-quick-brown\n1   1  fox  jumps   over   1-fox-jumps-over\n2   2  the   lazy    dog     2-the-lazy-dog\n""]";"[""In [1]: import pandas as pd \n\nIn [2]: df = pd.DataFrame([[0, 'the', 'quick', 'brown'],\n   ...:                    [1, 'fox', 'jumps', 'over'], \n   ...:                    [2, 'the', 'lazy', 'dog']],\n   ...:                   columns=['c0', 'c1', 'c2', 'c3'])\n\nIn [3]: def str_join(df, sep, *cols):\n   ...:     from functools import reduce\n   ...:     return reduce(lambda x, y: x.astype(str).str.cat(y.astype(str), sep=sep), \n   ...:                   [df[col] for col in cols])\n   ...: \n\nIn [4]: df['cat'] = str_join(df, '-', 'c0', 'c1', 'c2', 'c3')\n\nIn [5]: df\nOut[5]: \n   c0   c1     c2     c3                cat\n0   0  the  quick  brown  0-the-quick-brown\n1   1  fox  jumps   over   1-fox-jumps-over\n2   2  the   lazy    dog     2-the-lazy-dog\n""]"
1598;;0;43289220;21197774.0;2;8;;;"<p>you can set the types explicitly with pandas <code>DataFrame.astype(dtype, copy=True, raise_on_error=True, **kwargs)</code> and pass in a dictionary with the dtypes you want to <code>dtype</code></p>

<p>here's an example:</p>

<pre><code>import pandas as pd
wheel_number = 5
car_name = 'jeep'
minutes_spent = 4.5

# set the columns
data_columns = ['wheel_number', 'car_name', 'minutes_spent']

# create an empty dataframe
data_df = pd.DataFrame(columns = data_columns)
df_temp = pd.DataFrame([[wheel_number, car_name, minutes_spent]],columns = data_columns)
data_df = data_df.append(df_temp, ignore_index=True) 

In [11]: data_df.dtypes
Out[11]:
wheel_number     float64
car_name          object
minutes_spent    float64
dtype: object

data_df = data_df.astype(dtype= {""wheel_number"":""int64"",
        ""car_name"":""object"",""minutes_spent"":""float64""})
</code></pre>

<p>now you can see that it's changed</p>

<pre><code>In [18]: data_df.dtypes
Out[18]:
wheel_number       int64
car_name          object
minutes_spent    float64
</code></pre>
";;"['import pandas as pd\nwheel_number = 5\ncar_name = \'jeep\'\nminutes_spent = 4.5\n\n# set the columns\ndata_columns = [\'wheel_number\', \'car_name\', \'minutes_spent\']\n\n# create an empty dataframe\ndata_df = pd.DataFrame(columns = data_columns)\ndf_temp = pd.DataFrame([[wheel_number, car_name, minutes_spent]],columns = data_columns)\ndata_df = data_df.append(df_temp, ignore_index=True) \n\nIn [11]: data_df.dtypes\nOut[11]:\nwheel_number     float64\ncar_name          object\nminutes_spent    float64\ndtype: object\n\ndata_df = data_df.astype(dtype= {""wheel_number"":""int64"",\n        ""car_name"":""object"",""minutes_spent"":""float64""})\n', 'In [18]: data_df.dtypes\nOut[18]:\nwheel_number       int64\ncar_name          object\nminutes_spent    float64\n']";"['DataFrame.astype(dtype, copy=True, raise_on_error=True, **kwargs)', 'dtype', 'import pandas as pd\nwheel_number = 5\ncar_name = \'jeep\'\nminutes_spent = 4.5\n\n# set the columns\ndata_columns = [\'wheel_number\', \'car_name\', \'minutes_spent\']\n\n# create an empty dataframe\ndata_df = pd.DataFrame(columns = data_columns)\ndf_temp = pd.DataFrame([[wheel_number, car_name, minutes_spent]],columns = data_columns)\ndata_df = data_df.append(df_temp, ignore_index=True) \n\nIn [11]: data_df.dtypes\nOut[11]:\nwheel_number     float64\ncar_name          object\nminutes_spent    float64\ndtype: object\n\ndata_df = data_df.astype(dtype= {""wheel_number"":""int64"",\n        ""car_name"":""object"",""minutes_spent"":""float64""})\n', 'In [18]: data_df.dtypes\nOut[18]:\nwheel_number       int64\ncar_name          object\nminutes_spent    float64\n']"
1599;;0;43421391;14661701.0;2;12;;;"<p>If the DataFrame is huge, and the number of rows to drop is large as well, then simple drop by index <code>df.drop(df.index[])</code> takes too much time. </p>

<p>In my case, I have a multi-indexed DataFrame of floats with <code>100M rows x 3 cols</code>, and I need to remove <code>10k</code> rows from it. The fastest method I found is, quite counterintuitively, to <code>take</code> the remaining rows.</p>

<p>Let <code>indexes_to_drop</code> be an array of positional indexes to drop (<code>[1, 2, 4]</code> in the question).</p>

<pre><code>indexes_to_keep = set(range(df.shape[0])) - set(indexes_to_drop)
df_sliced = df.take(list(indexes_to_keep))
</code></pre>

<p>In my case this took <code>20.5s</code>, while the simple <code>df.drop</code> took <code>5min 27s</code> and consumed a lot of memory. The resulting DataFrame is the same.</p>
";;['indexes_to_keep = set(range(df.shape[0])) - set(indexes_to_drop)\ndf_sliced = df.take(list(indexes_to_keep))\n'];['df.drop(df.index[])', '100M rows x 3 cols', '10k', 'take', 'indexes_to_drop', '[1, 2, 4]', 'indexes_to_keep = set(range(df.shape[0])) - set(indexes_to_drop)\ndf_sliced = df.take(list(indexes_to_keep))\n', '20.5s', 'df.drop', '5min 27s']
1600;;1;43559496;43423347.0;2;10;;;"<p>here is my 2 cent on this with a very simple example why the warning is important.</p>

<p>so assuming that I am creating a df such has</p>

<pre><code>x = pd.DataFrame(list(zip(range(4), range(4))), columns=['a', 'b'])
print(x)
   a  b
0  0  0
1  1  1
2  2  2
3  3  3
</code></pre>

<p>now I want to create a new dataframe based on a subset of the original and modify it such has:</p>

<pre><code> q = x.loc[:, 'a']
</code></pre>

<p>now <strong>this is a slice of the original</strong> and whatever I do on it will affect x:</p>

<pre><code>q += 2
print(x)  # checking x again, wow! it changed!
   a  b
0  2  0
1  3  1
2  4  2
3  5  3
</code></pre>

<p><em>this is what the warning is telling you. you are working on a slice, so everything you do on it will be reflected on the original DataFrame</em></p>

<p>now <strong>using <code>.copy()</code>, it won't be a slice of the original</strong>, so doing an operation on q wont affect x :</p>

<pre><code>x = pd.DataFrame(list(zip(range(4), range(4))), columns=['a', 'b'])
print(x)
   a  b
0  0  0
1  1  1
2  2  2
3  3  3

q = x.loc[:, 'a'].copy()
q += 2
print(x)  # oh, x did not change because q is a copy now
   a  b
0  0  0
1  1  1
2  2  2
3  3  3
</code></pre>

<p>and btw, a copy just mean that <code>q</code> will be a new object in memory. where a slice share the same original object in memory</p>

<p>imo, using <code>.copy()</code>is very safe. as an example <code>df.loc[:, 'a']</code> return a slice but <code>df.loc[df.index, 'a']</code> return a copy. Jeff told me that this was an unexpected behavior and <code>:</code> or <code>df.index</code> should have the same behavior as an indexer in .loc[], but using <code>.copy()</code> on both will return a copy, better be safe. so use <code>.copy()</code> if you don't want to affect the original dataframe.</p>

<p>now using <strong><code>.copy()</code> return a deepcopy of the DataFrame</strong>, which is a very safe approach not to get the phone call you are talking about.</p>

<p>but using <code>df.is_copy = None</code>, is just a trick that does not copy anything which is a very bad idea, <strong>you will still be working on a slice of the original DataFrame</strong></p>

<p><em>one more thing that people tend not to know:</em></p>

<p><code>df[columns]</code> <strong>may return</strong> a view.</p>

<p><code>df.loc[indexer, columns]</code> also <strong>may return</strong> a view, <strong>but almost always does not in practice.</strong>
emphasis on the <strong>may</strong> here</p>
";;"[""x = pd.DataFrame(list(zip(range(4), range(4))), columns=['a', 'b'])\nprint(x)\n   a  b\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n"", "" q = x.loc[:, 'a']\n"", 'q += 2\nprint(x)  # checking x again, wow! it changed!\n   a  b\n0  2  0\n1  3  1\n2  4  2\n3  5  3\n', ""x = pd.DataFrame(list(zip(range(4), range(4))), columns=['a', 'b'])\nprint(x)\n   a  b\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n\nq = x.loc[:, 'a'].copy()\nq += 2\nprint(x)  # oh, x did not change because q is a copy now\n   a  b\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n""]";"[""x = pd.DataFrame(list(zip(range(4), range(4))), columns=['a', 'b'])\nprint(x)\n   a  b\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n"", "" q = x.loc[:, 'a']\n"", 'q += 2\nprint(x)  # checking x again, wow! it changed!\n   a  b\n0  2  0\n1  3  1\n2  4  2\n3  5  3\n', '.copy()', ""x = pd.DataFrame(list(zip(range(4), range(4))), columns=['a', 'b'])\nprint(x)\n   a  b\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n\nq = x.loc[:, 'a'].copy()\nq += 2\nprint(x)  # oh, x did not change because q is a copy now\n   a  b\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n"", 'q', '.copy()', ""df.loc[:, 'a']"", ""df.loc[df.index, 'a']"", ':', 'df.index', '.copy()', '.copy()', '.copy()', 'df.is_copy = None', 'df[columns]', 'df.loc[indexer, columns]']"
1601;;2;43665978;43423347.0;2;6;;;"<p>While the other answers provide good information about why one shouldn't simply ignore the warning, I think your original question has not been answered, yet.</p>

<p>@thn points out that using <code>copy()</code> completely depends on the scenario at hand. When you want that the original data is preserved, you use <code>.copy()</code>, otherwise you don't. If you are using <code>copy()</code> to circumvent the <code>SettingWithCopyWarning</code> you are ignoring the fact that you may introduce a logical bug into your software. As long as you are absolutely certain that this is what you want to do, you are fine.</p>

<p>However, when using <code>.copy()</code> blindly you may run into another issue, which is no longer really pandas specific, but occurs every time you are copying data.</p>

<p>I slightly modified your example code to make the problem more apparent:</p>

<pre><code>@profile
def foo():
    df = pd.DataFrame(np.random.randn(2 * 10 ** 7))

    d1 = df[:]
    d1 = d1.copy()

if __name__ == '__main__':
    foo()
</code></pre>

<p>When using <a href=""https://pypi.python.org/pypi/memory_profiler"" rel=""nofollow noreferrer"">memory_profile</a> one can clearly see that <code>.copy()</code> doubles our memory consumption:</p>

<pre><code>&gt; python -m memory_profiler demo.py 
Filename: demo.py

Line #    Mem usage    Increment   Line Contents
================================================
     4   61.195 MiB    0.000 MiB   @profile
     5                             def foo():
     6  213.828 MiB  152.633 MiB    df = pd.DataFrame(np.random.randn(2 * 10 ** 7))
     7                             
     8  213.863 MiB    0.035 MiB    d1 = df[:]
     9  366.457 MiB  152.594 MiB    d1 = d1.copy()
</code></pre>

<p>This relates to the fact, that there is still a reference (<code>df</code>) which points to the original data frame. Thus, <code>df</code> is not cleaned up by the garbage collector and is kept in memory. </p>

<p>When you are using this code in a production system, you may or may not get a <code>MemoryError</code> depending on the size of the data you are dealing with and your available memory.</p>

<p>To conclude, it is not a wise idea to use <code>.copy()</code> <em>blindly</em>. Not just because you may introduce a logical bug in your software, but also because it may expose runtime dangers such as a <code>MemoryError</code>. </p>

<hr>

<p><strong>Edit:</strong>
Even if you are doing <code>df = df.copy()</code>, and you can ensure that there are no other references to the original <code>df</code>, still <code>copy()</code> is evaluated before the assignment. Meaning that for a short time both data frames will be in memory. </p>

<p><em>Example (notice that you cannot see this behavior in the memory summary)</em>:</p>

<pre><code>&gt; mprof run -T 0.001 demo.py
Line #    Mem usage    Increment   Line Contents
================================================
     7     62.9 MiB      0.0 MiB   @profile
     8                             def foo():
     9    215.5 MiB    152.6 MiB    df = pd.DataFrame(np.random.randn(2 * 10 ** 7))
    10    215.5 MiB      0.0 MiB    df = df.copy()
</code></pre>

<p><em>But if you visualise memory consumption over time, at 1.6s both data frames are in memory:</em></p>

<p><a href=""https://i.stack.imgur.com/qUl5a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qUl5a.png"" alt=""enter image description here""></a></p>
";;"[""@profile\ndef foo():\n    df = pd.DataFrame(np.random.randn(2 * 10 ** 7))\n\n    d1 = df[:]\n    d1 = d1.copy()\n\nif __name__ == '__main__':\n    foo()\n"", '> python -m memory_profiler demo.py \nFilename: demo.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     4   61.195 MiB    0.000 MiB   @profile\n     5                             def foo():\n     6  213.828 MiB  152.633 MiB    df = pd.DataFrame(np.random.randn(2 * 10 ** 7))\n     7                             \n     8  213.863 MiB    0.035 MiB    d1 = df[:]\n     9  366.457 MiB  152.594 MiB    d1 = d1.copy()\n', '> mprof run -T 0.001 demo.py\nLine #    Mem usage    Increment   Line Contents\n================================================\n     7     62.9 MiB      0.0 MiB   @profile\n     8                             def foo():\n     9    215.5 MiB    152.6 MiB    df = pd.DataFrame(np.random.randn(2 * 10 ** 7))\n    10    215.5 MiB      0.0 MiB    df = df.copy()\n']";"['copy()', '.copy()', 'copy()', 'SettingWithCopyWarning', '.copy()', ""@profile\ndef foo():\n    df = pd.DataFrame(np.random.randn(2 * 10 ** 7))\n\n    d1 = df[:]\n    d1 = d1.copy()\n\nif __name__ == '__main__':\n    foo()\n"", '.copy()', '> python -m memory_profiler demo.py \nFilename: demo.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     4   61.195 MiB    0.000 MiB   @profile\n     5                             def foo():\n     6  213.828 MiB  152.633 MiB    df = pd.DataFrame(np.random.randn(2 * 10 ** 7))\n     7                             \n     8  213.863 MiB    0.035 MiB    d1 = df[:]\n     9  366.457 MiB  152.594 MiB    d1 = d1.copy()\n', 'df', 'df', 'MemoryError', '.copy()', 'MemoryError', 'df = df.copy()', 'df', 'copy()', '> mprof run -T 0.001 demo.py\nLine #    Mem usage    Increment   Line Contents\n================================================\n     7     62.9 MiB      0.0 MiB   @profile\n     8                             def foo():\n     9    215.5 MiB    152.6 MiB    df = pd.DataFrame(np.random.randn(2 * 10 ** 7))\n    10    215.5 MiB      0.0 MiB    df = df.copy()\n']"
1602;;2;43896119;25146121.0;2;11;;;"<p><strong>Best way found</strong>!!</p>

<p>the <code>date_column</code> has to be in date time format.</p>

<pre><code>import datetime as dt
df['month_year'] = df.date_column.dt.to_period('M')
</code></pre>
";;"[""import datetime as dt\ndf['month_year'] = df.date_column.dt.to_period('M')\n""]";"['date_column', ""import datetime as dt\ndf['month_year'] = df.date_column.dt.to_period('M')\n""]"
1603;;4;43897124;19078325.0;2;17;;;"<p>The currently accepted answer by unutbu describes are great way of doing this in pandas versions &lt;= 0.20. However, as of pandas 0.20, using this method raises a warning indicating that the syntax will not be available in future versions of pandas.</p>

<p>Series:</p>

<blockquote>
  <p>FutureWarning: using a dict on a Series for aggregation is deprecated and will be removed in a future version</p>
</blockquote>

<p>DataFrames:</p>

<blockquote>
  <p>FutureWarning: using a dict with renaming is deprecated and will be removed in a future version</p>
</blockquote>

<p>According to the <a href=""http://pandas.pydata.org/pandas-docs/version/0.20/whatsnew.html#deprecate-groupby-agg-with-a-dictionary-when-renaming"" rel=""noreferrer"">pandas 0.20 changelog</a>, the recommended way of renaming columns while aggregating is as follows.</p>

<pre><code># Create a sample data frame
df = pd.DataFrame({'A': [1, 1, 1, 2, 2],
                   'B': range(5),
                   'C': range(5)})

# ==== SINGLE COLUMN (SERIES) ====
# Syntax soon to be deprecated
df.groupby('A').B.agg({'foo': 'count'})
# Recommended replacement syntax
df.groupby('A').B.agg(['count']).rename(columns={'count': 'foo'})

# ==== MULTI COLUMN ====
# Syntax soon to be deprecated
df.groupby('A').agg({'B': {'foo': 'sum'}, 'C': {'bar': 'min'}})
# Recommended replacement syntax
df.groupby('A').agg({'B': 'sum', 'C': 'min'}).rename(columns={'B': 'foo', 'C': 'bar'})
# As the recommended syntax is more verbose, parentheses can
# be used to introduce line breaks and increase readability
(df.groupby('A')
    .agg({'B': 'sum', 'C': 'min'})
    .rename(columns={'B': 'foo', 'C': 'bar'})
)
</code></pre>

<p>Please see the <a href=""http://pandas.pydata.org/pandas-docs/version/0.20/whatsnew.html#deprecate-groupby-agg-with-a-dictionary-when-renaming"" rel=""noreferrer"">0.20 changelog</a> for additional details.</p>

<hr>

<h3>Update 2017-01-03 in response to @JunkMechanic's comment.</h3>

<p>With the old style dictionary syntax, it was possible to pass multiple <code>lambda</code> functions to <code>.agg</code>, since these would be renamed with the key in the passed dictionary:</p>

<pre><code>&gt;&gt;&gt; df.groupby('A').agg({'B': {'min': lambda x: x.min(), 'max': lambda x: x.max()}})

    B    
  max min
A        
1   2   0
2   4   3
</code></pre>

<p>Multiple functions can also be passed to a single column as a list:</p>

<pre><code>&gt;&gt;&gt; df.groupby('A').agg({'B': [np.min, np.max]})

     B     
  amin amax
A          
1    0    2
2    3    4
</code></pre>

<p>However, this does not work with lambda functions, since they are anonymous and all return <code>&lt;lambda&gt;</code>, which causes a name collision:</p>

<pre><code>&gt;&gt;&gt; df.groupby('A').agg({'B': [lambda x: x.min(), lambda x: x.max]})
SpecificationError: Function names must be unique, found multiple named &lt;lambda&gt;
</code></pre>

<p>To avoid the <code>SpecificationError</code>, named functions can be defined a priori instead of using <code>lambda</code>. Suitable function names also avoid calling <code>.rename</code> on the data frame afterwards. These functions can be passed with the same list syntax as above:</p>

<pre><code>&gt;&gt;&gt; def my_min(x):
&gt;&gt;&gt;     return x.min()

&gt;&gt;&gt; def my_max(x):
&gt;&gt;&gt;     return x.max()

&gt;&gt;&gt; df.groupby('A').agg({'B': [my_min, my_max]})

       B       
  my_min my_max
A              
1      0      2
2      3      4
</code></pre>
";;"[""# Create a sample data frame\ndf = pd.DataFrame({'A': [1, 1, 1, 2, 2],\n                   'B': range(5),\n                   'C': range(5)})\n\n# ==== SINGLE COLUMN (SERIES) ====\n# Syntax soon to be deprecated\ndf.groupby('A').B.agg({'foo': 'count'})\n# Recommended replacement syntax\ndf.groupby('A').B.agg(['count']).rename(columns={'count': 'foo'})\n\n# ==== MULTI COLUMN ====\n# Syntax soon to be deprecated\ndf.groupby('A').agg({'B': {'foo': 'sum'}, 'C': {'bar': 'min'}})\n# Recommended replacement syntax\ndf.groupby('A').agg({'B': 'sum', 'C': 'min'}).rename(columns={'B': 'foo', 'C': 'bar'})\n# As the recommended syntax is more verbose, parentheses can\n# be used to introduce line breaks and increase readability\n(df.groupby('A')\n    .agg({'B': 'sum', 'C': 'min'})\n    .rename(columns={'B': 'foo', 'C': 'bar'})\n)\n"", "">>> df.groupby('A').agg({'B': {'min': lambda x: x.min(), 'max': lambda x: x.max()}})\n\n    B    \n  max min\nA        \n1   2   0\n2   4   3\n"", "">>> df.groupby('A').agg({'B': [np.min, np.max]})\n\n     B     \n  amin amax\nA          \n1    0    2\n2    3    4\n"", "">>> df.groupby('A').agg({'B': [lambda x: x.min(), lambda x: x.max]})\nSpecificationError: Function names must be unique, found multiple named <lambda>\n"", "">>> def my_min(x):\n>>>     return x.min()\n\n>>> def my_max(x):\n>>>     return x.max()\n\n>>> df.groupby('A').agg({'B': [my_min, my_max]})\n\n       B       \n  my_min my_max\nA              \n1      0      2\n2      3      4\n""]";"[""# Create a sample data frame\ndf = pd.DataFrame({'A': [1, 1, 1, 2, 2],\n                   'B': range(5),\n                   'C': range(5)})\n\n# ==== SINGLE COLUMN (SERIES) ====\n# Syntax soon to be deprecated\ndf.groupby('A').B.agg({'foo': 'count'})\n# Recommended replacement syntax\ndf.groupby('A').B.agg(['count']).rename(columns={'count': 'foo'})\n\n# ==== MULTI COLUMN ====\n# Syntax soon to be deprecated\ndf.groupby('A').agg({'B': {'foo': 'sum'}, 'C': {'bar': 'min'}})\n# Recommended replacement syntax\ndf.groupby('A').agg({'B': 'sum', 'C': 'min'}).rename(columns={'B': 'foo', 'C': 'bar'})\n# As the recommended syntax is more verbose, parentheses can\n# be used to introduce line breaks and increase readability\n(df.groupby('A')\n    .agg({'B': 'sum', 'C': 'min'})\n    .rename(columns={'B': 'foo', 'C': 'bar'})\n)\n"", 'lambda', '.agg', "">>> df.groupby('A').agg({'B': {'min': lambda x: x.min(), 'max': lambda x: x.max()}})\n\n    B    \n  max min\nA        \n1   2   0\n2   4   3\n"", "">>> df.groupby('A').agg({'B': [np.min, np.max]})\n\n     B     \n  amin amax\nA          \n1    0    2\n2    3    4\n"", '<lambda>', "">>> df.groupby('A').agg({'B': [lambda x: x.min(), lambda x: x.max]})\nSpecificationError: Function names must be unique, found multiple named <lambda>\n"", 'SpecificationError', 'lambda', '.rename', "">>> def my_min(x):\n>>>     return x.min()\n\n>>> def my_max(x):\n>>>     return x.max()\n\n>>> df.groupby('A').agg({'B': [my_min, my_max]})\n\n       B       \n  my_min my_max\nA              \n1      0      2\n2      3      4\n""]"
1604;;2;43968774;28757389.0;2;14;;;"<p>Updated for <code>pandas</code> <code>0.20</code> given that <code>ix</code> is deprecated.  This demonstrates not only how to use <code>loc</code>, <code>iloc</code>, <code>at</code>, <code>iat</code>, <code>set_value</code>, but how to accomplish, mixed positional/label based indexing.</p>

<hr>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html"" rel=""noreferrer""><strong><code>loc</code></strong></a> - <strong>label based</strong><br>
Allows you to pass 1-D arrays as indexers.  Arrays can be either slices (subsets) of the index or column, or they can be boolean arrays which are equal in length to the index or columns.  </p>

<p><strong>Special Note:</strong> when a scalar indexer is passed, <code>loc</code> can assign a new index or column value that didn't exist before.</p>

<pre><code># label based, but we can use position values
# to get the labels from the index object
df.loc[df.index[2], 'ColName'] = 3
</code></pre>

<hr>

<pre><code>df.loc[df.index[1:3], 'ColName'] = 3
</code></pre>

<hr>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iloc.html"" rel=""noreferrer""><strong><code>iloc</code></strong></a> - <strong>position based</strong><br>
Similar to <code>loc</code> except with positions rather that index values.  However, you <strong>cannot</strong> assign new columns or indices.</p>

<pre><code># position based, but we can get the position
# from the columns object via the `get_loc` method
df.iloc[2, df.columns.get_loc('ColName')] = 3
</code></pre>

<hr>

<pre><code>df.iloc[2, 4] = 3
</code></pre>

<hr>

<pre><code>df.iloc[:3, 2:4] = 3
</code></pre>

<hr>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.at.html"" rel=""noreferrer""><strong><code>at</code></strong></a> - <strong>label based</strong><br>
Works very similar to <code>loc</code> for scalar indexers.  <strong>Cannot</strong> operate on array indexers.  <strong>Can!</strong> assign new indices and columns.  </p>

<p><strong>Advantage</strong> over <code>loc</code> is that this is faster.<br>
<strong>Disadvantage</strong> is that you can't use arrays for indexers.</p>

<pre><code># label based, but we can use position values
# to get the labels from the index object
df.at[df.index[2], 'ColName'] = 3
</code></pre>

<hr>

<pre><code>df.at['C', 'ColName'] = 3
</code></pre>

<hr>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iat.html"" rel=""noreferrer""><strong><code>iat</code></strong></a> - <strong>position based</strong><br>
Works similarly to <code>iloc</code>.  <strong>Cannot</strong> work in array indexers.  <strong>Cannot!</strong> assign new indices and columns.</p>

<p><strong>Advantage</strong> over <code>iloc</code> is that this is faster.<br>
<strong>Disadvantage</strong> is that you can't use arrays for indexers.</p>

<pre><code># position based, but we can get the position
# from the columns object via the `get_loc` method
IBM.iat[2, IBM.columns.get_loc('PNL')] = 3
</code></pre>

<hr>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_value.html"" rel=""noreferrer""><strong><code>set_value</code></strong></a> - <strong>label based</strong><br>
Works very similar to <code>loc</code> for scalar indexers.  <strong>Cannot</strong> operate on array indexers.  <strong>Can!</strong> assign new indices and columns</p>

<p><strong>Advantage</strong> Super fast, because there is very little overhead!<br>
<strong>Disadvantage</strong> There is very little overhead because <code>pandas</code> is not doing a bunch of safety checks.  <strong><em>Use at your own risk</em></strong>.  Also, this is not intended for public use.</p>

<pre><code># label based, but we can use position values
# to get the labels from the index object
df.set_value(df.index[2], 'ColName', 3)
</code></pre>

<hr>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_value.html"" rel=""noreferrer""><strong><code>set_value</code> with <code>takable=True</code></strong></a> - <strong>position based</strong><br>
Works similarly to <code>iloc</code>.  <strong>Cannot</strong> work in array indexers.  <strong>Cannot!</strong> assign new indices and columns.</p>

<p><strong>Advantage</strong> Super fast, because there is very little overhead!<br>
<strong>Disadvantage</strong> There is very little overhead because <code>pandas</code> is not doing a bunch of safety checks.  <strong><em>Use at your own risk</em></strong>.  Also, this is not intended for public use.</p>

<pre><code># position based, but we can get the position
# from the columns object via the `get_loc` method
df.set_value(2, df.columns.get_loc('ColName'), 3, takable=True)
</code></pre>
";;"[""# label based, but we can use position values\n# to get the labels from the index object\ndf.loc[df.index[2], 'ColName'] = 3\n"", ""df.loc[df.index[1:3], 'ColName'] = 3\n"", ""# position based, but we can get the position\n# from the columns object via the `get_loc` method\ndf.iloc[2, df.columns.get_loc('ColName')] = 3\n"", 'df.iloc[2, 4] = 3\n', 'df.iloc[:3, 2:4] = 3\n', ""# label based, but we can use position values\n# to get the labels from the index object\ndf.at[df.index[2], 'ColName'] = 3\n"", ""df.at['C', 'ColName'] = 3\n"", ""# position based, but we can get the position\n# from the columns object via the `get_loc` method\nIBM.iat[2, IBM.columns.get_loc('PNL')] = 3\n"", ""# label based, but we can use position values\n# to get the labels from the index object\ndf.set_value(df.index[2], 'ColName', 3)\n"", ""# position based, but we can get the position\n# from the columns object via the `get_loc` method\ndf.set_value(2, df.columns.get_loc('ColName'), 3, takable=True)\n""]";"['pandas', '0.20', 'ix', 'loc', 'iloc', 'at', 'iat', 'set_value', 'loc', 'loc', ""# label based, but we can use position values\n# to get the labels from the index object\ndf.loc[df.index[2], 'ColName'] = 3\n"", ""df.loc[df.index[1:3], 'ColName'] = 3\n"", 'iloc', 'loc', ""# position based, but we can get the position\n# from the columns object via the `get_loc` method\ndf.iloc[2, df.columns.get_loc('ColName')] = 3\n"", 'df.iloc[2, 4] = 3\n', 'df.iloc[:3, 2:4] = 3\n', 'at', 'loc', 'loc', ""# label based, but we can use position values\n# to get the labels from the index object\ndf.at[df.index[2], 'ColName'] = 3\n"", ""df.at['C', 'ColName'] = 3\n"", 'iat', 'iloc', 'iloc', ""# position based, but we can get the position\n# from the columns object via the `get_loc` method\nIBM.iat[2, IBM.columns.get_loc('PNL')] = 3\n"", 'set_value', 'loc', 'pandas', ""# label based, but we can use position values\n# to get the labels from the index object\ndf.set_value(df.index[2], 'ColName', 3)\n"", 'set_value', 'takable=True', 'iloc', 'pandas', ""# position based, but we can get the position\n# from the columns object via the `get_loc` method\ndf.set_value(2, df.columns.get_loc('ColName'), 3, takable=True)\n""]"
1605;;3;44311454;21608228.0;2;19;;;"<p><code>.ix</code> indexer works okay for pandas version prior to 0.20.0, but since pandas 0.20.0, the <code>.ix</code> indexer is <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated"" rel=""noreferrer"">deprecated</a>, so you should avoid using it. Instead, you can use <code>.loc</code> or <code>iloc</code> indexers. You can solve this problem by:</p>

<pre><code>mask = df.my_channel &gt; 20000
column_name = 'my_channel'
df.loc[mask, column_name] = 0
</code></pre>

<p><code>mask</code> helps you to select the rows in which <code>df.my_channel &gt; 20000</code> is <code>True</code>, while <code>df.loc[mask, column_name] = 0</code> sets the value 0 to the selected rows where <code>mask</code>holds in the column which name is <code>column_name</code>.</p>

<p><strong>Update:</strong>
In this case, you should use <code>loc</code> because if you use <code>iloc</code>, you will get a <code>NotImplementedError</code> telling you that <em>iLocation based boolean indexing on an integer type is not available</em>.</p>
";;"[""mask = df.my_channel > 20000\ncolumn_name = 'my_channel'\ndf.loc[mask, column_name] = 0\n""]";"['.ix', '.ix', '.loc', 'iloc', ""mask = df.my_channel > 20000\ncolumn_name = 'my_channel'\ndf.loc[mask, column_name] = 0\n"", 'mask', 'df.my_channel > 20000', 'True', 'df.loc[mask, column_name] = 0', 'mask', 'column_name', 'loc', 'iloc', 'NotImplementedError']"
1606;;0;44736467;10665889.0;2;18;;;"<h1>2017 Answer - pandas 0.20: .ix is deprecated. Use .loc</h1>

<p>See the <a href=""https://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated"" rel=""noreferrer"">deprecation in the docs</a></p>

<p><code>.loc</code> uses label based indexing to select both rows and columns. The labels being the values of the index or the columns. Slicing with <code>.loc</code> includes the last element. </p>

<blockquote>
  <p>Let's assume we have a DataFrame with the following columns:<br>
  <code>foo</code>, <code>bar</code>, <code>quz</code>, <code>ant</code>, <code>cat</code>, <code>sat</code>, <code>dat</code>.</p>
</blockquote>

<pre><code># selects all rows and all columns beginning at 'foo' up to and including 'ant'
df.loc[:, 'foo':'sat']
# foo bar quz ant cat sat
</code></pre>

<p><code>.loc</code> accepts the same slice notation that Python lists do for both row and columns. Slice notation being <code>start:stop:step</code></p>

<pre><code># slice from 'foo' to 'cat' by every 2nd column
df.loc[:, 'foo':'cat':2]
# foo quz cat

# slice from the beginning to 'bar'
df.loc[:, :'bar']
# foo bar

# slice from 'quz' to the end by 3
df.loc[:, 'quz'::3]
# quz sat

# attempt from 'sat' to 'bar'
df.loc[:, 'sat':'bar']
# no columns returned

# slice from 'sat' to 'bar'
df.loc[:, 'sat':'bar':-1]
sat cat ant quz bar

# slice notation is syntatic sugar for the slice function
# slice from 'quz' to the end by 2 with slice function
df.loc[:, slice('quz',None, 2)]
# quz cat dat

# select specific columns with a list
# select columns foo, bar and dat
df.loc[:, ['foo','bar','dat']]
# foo bar dat
</code></pre>

<p>You can slice by rows and columns. For instance if you have 5 rows with labels <code>v</code>, <code>w</code>, <code>x</code>, <code>y</code>, <code>z</code></p>

<pre><code># slice from 'w' to 'y' and 'foo' to 'ant' by 3
df.loc['w':'y', 'foo':'ant':3]
#    foo ant
# w
# x
# y
</code></pre>
";;"[""# selects all rows and all columns beginning at 'foo' up to and including 'ant'\ndf.loc[:, 'foo':'sat']\n# foo bar quz ant cat sat\n"", ""# slice from 'foo' to 'cat' by every 2nd column\ndf.loc[:, 'foo':'cat':2]\n# foo quz cat\n\n# slice from the beginning to 'bar'\ndf.loc[:, :'bar']\n# foo bar\n\n# slice from 'quz' to the end by 3\ndf.loc[:, 'quz'::3]\n# quz sat\n\n# attempt from 'sat' to 'bar'\ndf.loc[:, 'sat':'bar']\n# no columns returned\n\n# slice from 'sat' to 'bar'\ndf.loc[:, 'sat':'bar':-1]\nsat cat ant quz bar\n\n# slice notation is syntatic sugar for the slice function\n# slice from 'quz' to the end by 2 with slice function\ndf.loc[:, slice('quz',None, 2)]\n# quz cat dat\n\n# select specific columns with a list\n# select columns foo, bar and dat\ndf.loc[:, ['foo','bar','dat']]\n# foo bar dat\n"", ""# slice from 'w' to 'y' and 'foo' to 'ant' by 3\ndf.loc['w':'y', 'foo':'ant':3]\n#    foo ant\n# w\n# x\n# y\n""]";"['.loc', '.loc', 'foo', 'bar', 'quz', 'ant', 'cat', 'sat', 'dat', ""# selects all rows and all columns beginning at 'foo' up to and including 'ant'\ndf.loc[:, 'foo':'sat']\n# foo bar quz ant cat sat\n"", '.loc', 'start:stop:step', ""# slice from 'foo' to 'cat' by every 2nd column\ndf.loc[:, 'foo':'cat':2]\n# foo quz cat\n\n# slice from the beginning to 'bar'\ndf.loc[:, :'bar']\n# foo bar\n\n# slice from 'quz' to the end by 3\ndf.loc[:, 'quz'::3]\n# quz sat\n\n# attempt from 'sat' to 'bar'\ndf.loc[:, 'sat':'bar']\n# no columns returned\n\n# slice from 'sat' to 'bar'\ndf.loc[:, 'sat':'bar':-1]\nsat cat ant quz bar\n\n# slice notation is syntatic sugar for the slice function\n# slice from 'quz' to the end by 2 with slice function\ndf.loc[:, slice('quz',None, 2)]\n# quz cat dat\n\n# select specific columns with a list\n# select columns foo, bar and dat\ndf.loc[:, ['foo','bar','dat']]\n# foo bar dat\n"", 'v', 'w', 'x', 'y', 'z', ""# slice from 'w' to 'y' and 'foo' to 'ant' by 3\ndf.loc['w':'y', 'foo':'ant':3]\n#    foo ant\n# w\n# x\n# y\n""]"
1607;;0;44913631;12307099.0;2;11;;;"<p>Starting from pandas 0.20 <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated"" rel=""noreferrer"">ix is deprecated</a>. The right way to perform this task is using <code>loc</code></p>

<p>here is a working example </p>

<pre><code>&gt;&gt;&gt; import pandas as pd 
&gt;&gt;&gt; import numpy as np 
&gt;&gt;&gt; df = pd.DataFrame({""A"":[0,1,0], ""B"":[2,0,5]}, columns=list('AB'))
&gt;&gt;&gt; df.loc[df.A == 0, 'B'] = np.nan
&gt;&gt;&gt; df
   A   B
0  0 NaN
1  1   0
2  0 NaN
&gt;&gt;&gt; 
</code></pre>

<p>For more information check the advanced indexing documentation <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-advanced"" rel=""noreferrer"">here</a>. </p>
";;"['>>> import pandas as pd \n>>> import numpy as np \n>>> df = pd.DataFrame({""A"":[0,1,0], ""B"":[2,0,5]}, columns=list(\'AB\'))\n>>> df.loc[df.A == 0, \'B\'] = np.nan\n>>> df\n   A   B\n0  0 NaN\n1  1   0\n2  0 NaN\n>>> \n']";"['loc', '>>> import pandas as pd \n>>> import numpy as np \n>>> df = pd.DataFrame({""A"":[0,1,0], ""B"":[2,0,5]}, columns=list(\'AB\'))\n>>> df.loc[df.A == 0, \'B\'] = np.nan\n>>> df\n   A   B\n0  0 NaN\n1  1   0\n2  0 NaN\n>>> \n']"
1608;;0;45357725;13411544.0;2;10;;;"<p>You should use <em>drop()</em>. Suppose your dataframe name is <em>df</em>. </p>

<pre><code>#for dropping single column 
df = df.drop('your_column', axis=1)

#for dropping multiple columns
df = df.drop(['col_1','col_2','col_3'], axis=1)
</code></pre>
";;"[""#for dropping single column \ndf = df.drop('your_column', axis=1)\n\n#for dropping multiple columns\ndf = df.drop(['col_1','col_2','col_3'], axis=1)\n""]";"[""#for dropping single column \ndf = df.drop('your_column', axis=1)\n\n#for dropping multiple columns\ndf = df.drop(['col_1','col_2','col_3'], axis=1)\n""]"
1609;;0;45568211;22697773.0;2;6;;;"<p>In <code>pandas 0.20.2</code> you can do:</p>

<pre><code>from pandas.api.types import is_string_dtype
from pandas.api.types import is_numeric_dtype

is_string_dtype(df['A'])
&gt;&gt;&gt;&gt; True

is_numeric_dtype(df['B'])
&gt;&gt;&gt;&gt; True
</code></pre>

<p>So your code becomes:</p>

<pre><code>for y in agg.columns:
    if (is_string_dtype(agg[y])):
        treat_str(agg[y])
    elif (is_numeric_dtype(agg[y])):
        treat_numeric(agg[y])
</code></pre>
";;"[""from pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\n\nis_string_dtype(df['A'])\n>>>> True\n\nis_numeric_dtype(df['B'])\n>>>> True\n"", 'for y in agg.columns:\n    if (is_string_dtype(agg[y])):\n        treat_str(agg[y])\n    elif (is_numeric_dtype(agg[y])):\n        treat_numeric(agg[y])\n']";"['pandas 0.20.2', ""from pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\n\nis_string_dtype(df['A'])\n>>>> True\n\nis_numeric_dtype(df['B'])\n>>>> True\n"", 'for y in agg.columns:\n    if (is_string_dtype(agg[y])):\n        treat_str(agg[y])\n    elif (is_numeric_dtype(agg[y])):\n        treat_numeric(agg[y])\n']"
