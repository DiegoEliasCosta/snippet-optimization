;AnswerCount;CommentCount;Id;ParentId;PostTypeId;Score;Tags;Title;ViewCount;Code
0;2.0;7;7776679;;1;25;<python><pandas>;append two data frame with pandas;25479.0;['bigdata = data1.append(data2)\n', 'Exception: Index cannot contain duplicate values!\n', 'data1', '    meta  particle  ratio   area    type    \n0   2     part10    1.348   0.8365  touching\n1   2     part18    1.558   0.8244  single  \n2   2     part2     1.893   0.894   single  \n3   2     part37    0.6695  1.005   single  \n....clip...\n36  2     part23    1.051   0.8781  single  \n37  2     part3     80.54   0.9714  nuclei  \n38  2     part34    1.071   0.9337  single  \n', 'data2', '    meta  particle  ratio    area    type    \n0   3     part10    0.4756   1.025   single  \n1   3     part18    0.04387  1.232   dusts   \n2   3     part2     1.132    0.8927  single  \n...clip...\n46  3     part46    13.71    1.001   nuclei  \n47  3     part3     0.7439   0.9038  single  \n48  3     part34    0.4349   0.9956  single \n']
1;7.0;3;7837722;;1;187;<python><performance><for-loop><pandas>;What is the most efficient way to loop through dataframes with pandas?;195364.0;"['Date,Open,High,Low,Close,Volume,Adj Close\n2011-10-19,27.37,27.47,27.01,27.13,42880000,27.13\n2011-10-18,26.94,27.40,26.80,27.31,52487900,27.31\n2011-10-17,27.11,27.42,26.85,26.98,39433400,26.98\n2011-10-14,27.31,27.50,27.02,27.27,50947700,27.27\n\n....\n', ""#!/usr/bin/env python\nfrom pandas import *\n\ndf = read_csv('table.csv')\n\nfor i, row in enumerate(df.values):\n    date = df.index[i]\n    open, high, low, close, adjclose = row\n    #now perform analysis on open/close based on date, etc..\n"", 'df.iteritems']"
2;2.0;0;8916302;;1;26;<python><csv><numpy><tab-delimited><pandas>;selecting across multiple columns with python pandas?;19805.0;"['df', 'pandas.read_table', 'colA', 'df_greater_than10 = df[df[""colA""] > 10]\n', 'df', 'colA', 'colB']"
3;3.0;16;8991709;;1;134;<python><r><join><data.table><pandas>;Why are pandas merges in python faster than data.table merges in R?;16173.0;['pandas', 'data.table', 'data.table', 'merge(X, Y, all=FALSE)', 'merge(X, Y, all=TRUE)']
4;2.0;2;9588331;;1;21;<python><pandas>;Simple cross-tabulation in pandas;9493.0;['pivot/crosstab/indexing', 'Panel', 'DataFrames', 'AB,100.00\nAB,200.00\nAC,150.00\nAD,500.00\n', 'AB,500.00\nAC,250.00\nAX,900.00\n', 'AB,300.00,2\nAC,150.00,1\nAD,500.00,1\n', 'AB,500.00,1\nAC,250.00,1\nAX,900.00,1\n', 'pivot/crosstab/groupby/an index']
5;3.0;0;9652832;;1;38;<python><pandas><tsv>;How to I load a tsv file into a Pandas DataFrame?;32040.0;"['tsv', 'DataFrame', '>>> df1 = DataFrame(csv.reader(open(\'c:/~/trainSetRel3.txt\'), delimiter=\'\\t\'))\n\nTraceback (most recent call last):\n  File ""<pyshell#28>"", line 1, in <module>\n    df1 = DataFrame(csv.reader(open(\'c:/~/trainSetRel3.txt\'), delimiter=\'\\t\'))\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 318, in __init__\n    raise PandasError(\'DataFrame constructor not properly called!\')\nPandasError: DataFrame constructor not properly called!\n']"
6;6.0;0;9758450;;1;34;<python><pandas>;Pandas convert dataframe to array of tuples;36433.0;['In [182]: data_set\nOut[182]: \n  index data_date   data_1  data_2\n0  14303 2012-02-17  24.75   25.03 \n1  12009 2012-02-16  25.00   25.07 \n2  11830 2012-02-15  24.99   25.15 \n3  6274  2012-02-14  24.68   25.05 \n4  2302  2012-02-13  24.62   24.77 \n5  14085 2012-02-10  24.38   24.61 \n', '[(datetime.date(2012,2,17),24.75,25.03),\n(datetime.date(2012,2,16),25.00,25.07),\n...etc. ]\n']
7;11.0;4;10065051;;1;77;<python><pandas>;python-pandas and databases like mysql;72432.0;[]
8;4.0;2;10202570;;1;84;<python><pandas>;Pandas DataFrame - Find row where values for column is maximal;75652.0;['df.max()']
9;6.0;0;10373660;;1;173;<python><pandas><dataframe><group-by><multi-index>;Converting a Pandas GroupBy object to DataFrame;167415.0;"['df1 = pandas.DataFrame( { \n    ""Name"" : [""Alice"", ""Bob"", ""Mallory"", ""Mallory"", ""Bob"" , ""Mallory""] , \n    ""City"" : [""Seattle"", ""Seattle"", ""Portland"", ""Seattle"", ""Seattle"", ""Portland""] } )\n', '   City     Name\n0   Seattle    Alice\n1   Seattle      Bob\n2  Portland  Mallory\n3   Seattle  Mallory\n4   Seattle      Bob\n5  Portland  Mallory\n', 'g1 = df1.groupby( [ ""Name"", ""City""] ).count()\n', 'GroupBy', '                  City  Name\nName    City\nAlice   Seattle      1     1\nBob     Seattle      2     2\nMallory Portland     2     2\n        Seattle      1     1\n', '                  City  Name\nName    City\nAlice   Seattle      1     1\nBob     Seattle      2     2\nMallory Portland     2     2\nMallory Seattle      1     1\n']"
10;2.0;1;10457584;;1;68;<python><pandas>;Redefining the Index in a Pandas DataFrame object;107691.0;"['DataFrame', 'From:\n            a   b   c\n        0   1   2   3\n        1  10  11  12\n        2  20  21  22\n\nTo :\n           b   c\n       1   2   3\n      10  11  12\n      20  21  22\n', "">>> col = ['a','b','c']\n>>> data = DataFrame([[1,2,3],[10,11,12],[20,21,22]],columns=col)\n>>> data\n    a   b   c\n0   1   2   3\n1  10  11  12\n2  20  21  22\n>>> idx2 = data.a.values\n>>> idx2\narray([ 1, 10, 20], dtype=int64)\n>>> data2 = DataFrame(data,index=idx2,columns=col[1:])\n>>> data2\n     b   c\n1   11  12\n10 NaN NaN\n20 NaN NaN\n""]"
11;2.0;0;10464738;;1;23;<python><pandas>;Interpolation on DataFrame in pandas;15524.0;"['reindex', 'NaN', ""fillna(method='pad')""]"
12;6.0;3;10511024;;1;64;<python><ipython><pandas>;in Ipython notebook, Pandas is not displying the graph I try to plot;44369.0;['In [7]:\n\npledge.Amount.plot()\n\nOut[7]:\n\n<matplotlib.axes.AxesSubplot at 0x9397c6c>\n']
13;2.0;2;10591000;;1;25;<python><pandas>;Specifying data type in Pandas csv reader;17261.0;['read_csv()']
14;14.0;5;10636024;;1;33;<python><user-interface><pandas><dataframe>;Python / Pandas - GUI for viewing a DataFrame or Matrix;26216.0;[]
15;7.0;3;10665889;;1;126;<python><numpy><pandas><slice>;How to take column-slices of dataframe in pandas;154109.0;"[""data = pandas.read_csv('mydata.csv')\n"", ""data = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))\n"", 'a', 'b', 'c', 'd', 'e', ""observations = data[:'c']\nfeatures = data['c':]\n"", 'pd.Panel', ""data['a']"", 'data[0]', ""data['a':]"", 'data[0:]', 'data[0] != data[0:1]']"
16;13.0;3;10715965;;1;312;<python><pandas>;add one row in a pandas.DataFrame;349935.0;"['DataFrame', ""res = DataFrame(columns=('lib', 'qty1', 'qty2'))\n"", ""res = res.set_value(len(res), 'qty1', 10.0)\n""]"
17;1.0;2;10729210;;1;122;<python><pandas>;iterating row by row through a pandas dataframe;174898.0;['DataFrame', 'for i in df.index:\n    do_something(df.ix[i])\n']
18;1.0;2;10751127;;1;41;<python><pandas>;Returning multiple values from pandas apply on a DataFrame;19415.0;"['DataFrame', 'import numpy\nimport pandas\n\ndf = pandas.DataFrame(numpy.log2(numpy.randn(1000, 4), \n                      columns=[""a"", ""b"", ""c"", ""d""])\n\ndf = df.dropna()\n', 'apply', 'from scipy.stats import ttest_ind\n\ndef t_test_and_mean(series, first, second):\n    first_group = series[first]\n    second_group = series[second]\n    _, pvalue = ttest_ind(first_group, second_group)\n\n    mean_ratio = second_group.mean() / first_group.mean()\n\n    return (pvalue, mean_ratio)\n', 'df.apply(t_test_and_mean, first=[""a"", ""b""], second=[""c"", ""d""], axis=1)\n']"
19;1.0;1;10857924;;1;39;<python><pandas>;Remove NULL columns in a dataframe Pandas?;33392.0;['dataFrame']
20;3.0;0;10867028;;1;23;<python><csv><pandas>;Get pandas.read_csv to read empty values as empty string instead of nan;21259.0;"['""nan""', 'One,Two,Three\na,1,one\nb,2,two\n,3,three\nd,4,nan\ne,5,five\nnan,6,\ng,7,seven\n\n>>> pandas.read_csv(\'test.csv\', na_values={\'One\': [], ""Three"": []})\n    One  Two  Three\n0    a    1    one\n1    b    2    two\n2  NaN    3  three\n3    d    4    nan\n4    e    5   five\n5  nan    6    NaN\n6    g    7  seven\n', 'str', 'converters', ""converters={'One': str})""]"
21;5.0;0;10951341;;1;45;<python><pandas>;Pandas DataFrame aggregate function using multiple columns;17889.0;"['DataFrame.agg', 'def wAvg(c, w):\n    return ((c * w).sum() / w.sum())\n\ndf = DataFrame(....) # df has columns c and w, i want weighted average\n                     # of c using w as weight.\ndf.aggregate ({""c"": wAvg}) # and somehow tell it to use w column as weights ...\n']"
22;3.0;1;10982089;;1;38;<python><pandas><dataframe>;How to shift a column in Pandas DataFrame;31697.0;['DataFrame', '##    x1   x2\n##0  206  214\n##1  226  234\n##2  245  253\n##3  265  272\n##4  283  291\n', '##    x1   x2\n##0  206  nan\n##1  226  214\n##2  245  234\n##3  265  253\n##4  283  272\n##5  nan  291\n']
23;11.0;1;11067027;;1;102;<python><pandas><order>;Python Pandas - Re-ordering columns in a dataframe based on column name;74414.0;"['dataframe', ""['Q1.3','Q6.1','Q1.2','Q1.1',......]\n"", ""['Q1.1','Q1.2','Q1.3',.....'Q6.1',......]\n""]"
24;1.0;0;11073609;;1;28;<python><pandas>;How to group DataFrame by a period of time?;20945.0;"[' def gen(date, count=10):\n     while count > 0:\n         yield date, ""event{}"".format(randint(1,9)), ""source{}"".format(randint(1,3))\n         count -= 1\n         date += DateOffset(seconds=randint(40))\n\n df = DataFrame.from_records(list(gen(datetime(2012,1,1,12, 30))), index=\'Time\', columns=[\'Time\', \'Event\', \'Source\'])\n', ' Event  Source\n 2012-01-01 12:30:00     event3  source1\n 2012-01-01 12:30:12     event2  source2\n 2012-01-01 12:30:12     event2  source2\n 2012-01-01 12:30:29     event6  source1\n 2012-01-01 12:30:38     event1  source1\n 2012-01-01 12:31:05     event4  source2\n 2012-01-01 12:31:38     event4  source1\n 2012-01-01 12:31:44     event5  source1\n 2012-01-01 12:31:48     event5  source2\n 2012-01-01 12:32:23     event6  source1\n', ""df.resample('Min')"", ""df.groupby(date_range(datetime(2012,1,1,12, 30), freq='Min',\nperiods=4))"", ""df.groupby(TimeGrouper(freq='Min'))"", 'DataFrameGroupBy', ""grouped = df.groupby(TimeGrouper(freq='Min'))\ngrouped.Source.value_counts()\n2012-01-01 12:30:00  source1    1\n2012-01-01 12:31:00  source2    2\n                     source1    2\n2012-01-01 12:32:00  source2    2\n                     source1    2\n2012-01-01 12:33:00  source1    1\n"", 'TimeGrouper', ""groupby([TimeGrouper(freq='Min'), df.Source])""]"
25;3.0;3;11077023;;1;106;<python><numpy><scipy><pandas>;What are the differences between Pandas and NumPy+SciPy in Python?;54531.0;[]
26;4.0;1;11106823;;1;26;<python><pandas>;Adding two pandas dataframes;19395.0;['dataframes', 'timeseries', 'dataframe', 'dataframe', '.add', 'combined_data = dataframe1 + dataframe2', 'NaN']
27;4.0;0;11232275;;1;28;<python><pandas>;Pandas pivot warning about repeated entries on index;12221.0;"['pivot', ""Examples\n--------\n>>> df\n    foo   bar  baz\n0   one   A    1.\n1   one   B    2.\n2   one   C    3.\n3   two   A    4.\n4   two   B    5.\n5   two   C    6.\n\n>>> df.pivot('foo', 'bar', 'baz')\n     A   B   C\none  1   2   3\ntwo  4   5   6\n"", 'DataFrame', '   name   id     x\n----------------------\n0  john   1      0\n1  john   2      0\n2  mike   1      1\n3  mike   2      0\n', ""      1    2   # (this is the id as columns)\n----------------------\nmike  0    0   # (and this is the 'x' as values)\njohn  1    0\n"", 'pivot', '*** ReshapeError: Index contains duplicate entries, cannot reshape\n', 'foo', 'name', 'pivot']"
28;7.0;4;11285613;;1;280;<python><pandas>;Selecting columns;413954.0;"['index  a   b   c\n1      2   3   4\n2      3   4   5\n', ""'b'"", ""'c'"", ""df1 = df['a':'b']\ndf1 = df.ix[:, 'a':'b']\n""]"
29;16.0;0;11346283;;1;750;<python><pandas><replace><dataframe><rename>;Renaming columns in pandas;767316.0;"['A', ""['$a', '$b', '$c', '$d', '$e'] \n"", ""['a', 'b', 'c', 'd', 'e'].\n""]"
30;6.0;0;11350770;;1;132;<python><pandas>;pandas + dataframe - select by partial string;130134.0;"['DataFrame', 're.search(pattern, cell_in_question) \n', 'df[df[\'A\'] == ""hello world""]', ""'hello'""]"
31;6.0;1;11361985;;1;59;<python><numpy><pandas>;Output data from all columns in a dataframe in pandas;106412.0;"['params.csv', 'ipython qtconsole', 'dataframe', ""import pandas\nparamdata = pandas.read_csv('params.csv', names=paramnames)\n"", 'paramnames', 'paramnames', 'paramnames = [""id"",\n""fc"",\n""mc"",\n""markup"",\n""asplevel"",\n""aspreview"",\n""reviewpd""]\n', 'paramdata', ""In[35]: paramdata\nOut[35]: \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 59 entries, 0 to 58\nData columns:\nid                    59  non-null values\nfc                    59  non-null values\nmc                    59  non-null values\nmarkup                59  non-null values\nasplevel              59  non-null values\naspreview             59  non-null values\nreviewpd              59  non-null values\n"", ""paramdata['mc']"", 'mc', 'df', ""paramdata[['id','fc','mc']]""]"
32;3.0;0;11391969;;1;39;<python><pandas>;How to group pandas DataFrame entries by date in a non-unique column;24050.0;"['DataFrame', '""date""', 'datetime', ""data.groupby(data['date'])\n"", 'datetime']"
33;3.0;0;11418192;;1;36;<pandas>;pandas: complex filter on rows of DataFrame;20153.0;"[""def f(row):\n  return sin(row['velocity'])/np.prod(['masses']) > 5\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, f)]\n"", ""def g(row):\n  if row['col1'].method1() == 1:\n    val = row['col1'].method2() / row['col1'].method3(row['col3'], row['col4'])\n  else:\n    val = row['col2'].method5(row['col6'])\n  return np.sin(val)\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, g)]\n""]"
34;2.0;0;11495051;;1;21;<python><r><pandas><rpy2><statsmodels>;Difference in Python statsmodels OLS and R's lm;4994.0;"['R', 'import pandas\nfrom rpy2.robjects import r\n\nfrom functools import partial\n\nloadcsv = partial(pandas.DataFrame.from_csv,\n                  index_col=""seqn"", parse_dates=False)\n\ndemoq = loadcsv(""csv/DEMO.csv"")\nrxq = loadcsv(""csv/quest/RXQ_RX.csv"")\n\nnum_rx = {}\nfor seqn, num in rxq.rxd295.iteritems():\n    try:\n        val = int(num)\n    except ValueError:\n        val = 0\n    num_rx[seqn] = val\n\nseries = pandas.Series(num_rx, name=""num_rx"")\ndemoq = demoq.join(series)\n\nimport pandas.rpy.common as com\ndf = com.convert_to_r_dataframe(demoq)\nr.assign(""demoq"", df)\nr(\'lmout <- lm(demoq$num_rx ~ demoq$ridageyr)\')  # run the regression\nr(\'print(summary(lmout))\')  # print from R\n', 'R', 'Call:\nlm(formula = demoq$num_rx ~ demoq$ridageyr)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9086 -0.6908 -0.2940  0.1358 15.7003 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -0.1358216  0.0241399  -5.626 1.89e-08 ***\ndemoq$ridageyr  0.0358161  0.0006232  57.469  < 2e-16 ***\n---\nSignif. codes:  0 \x91***\x92 0.001 \x91**\x92 0.01 \x91*\x92 0.05 \x91.\x92 0.1 \x91 \x92 1 \n\nResidual standard error: 1.545 on 9963 degrees of freedom\nMultiple R-squared: 0.249,  Adjusted R-squared: 0.2489 \nF-statistic:  3303 on 1 and 9963 DF,  p-value: < 2.2e-16\n', 'statsmodels.api', 'import statsmodels.api as sm\nresults = sm.OLS(demoq.num_rx, demoq.ridageyr).fit()\nresults.summary()\n', 'OLS Regression Results\nAdj. R-squared:  0.247\nLog-Likelihood:  -18488.\nNo. Observations:    9965    AIC:   3.698e+04\nDf Residuals:    9964    BIC:   3.698e+04\n             coef   std err  t     P>|t|    [95.0% Conf. Int.]\nridageyr     0.0331  0.000   82.787    0.000        0.032 0.034\n']"
35;2.0;3;11548005;;1;54;<python><numpy><int><pandas><data-type-conversion>;NumPy or Pandas: Keeping array type as integer while having a NaN value;16797.0;['numpy', 'int', 'int64', 'numpy.NaN', 'int', 'from_records()', 'coerce_float=False']
36;2.0;0;11615504;;1;21;<python><pandas>;Parse dates when YYYYMMDD and HH are in separate columns using pandas in Python;15326.0;"['YYYYMMDD, HH,    X\n20110101,  1,   10\n20110101,  2,   20\n20110101,  3,   30\n', 'import pandas as pnd\npnd.read_csv(""..\\\\file.csv"",  parse_dates = True, index_col = [0,1])\n', '                         X\nYYYYMMDD    HH            \n2011-01-01 2012-07-01   10\n           2012-07-02   20\n           2012-07-03   30\n', '                      X\nDatetime              \n2011-01-01 01:00:00  10\n2011-01-01 02:00:00  20\n2011-01-01 03:00:00  30\n']"
37;6.0;1;11622652;;1;74;<python><pandas><sas>;Large, persistent DataFrame in pandas;45446.0;['pandas.read_csv()', 'pandas']
38;1.0;0;11640243;;1;33;<python><pandas>;PANDAS plot multiple Y axes;15502.0;['pandas']
39;4.0;6;11697887;;1;43;<python><django><pandas>;Converting Django QuerySet to pandas DataFrame;10165.0;"['DataFrame', ""qs = SomeModel.objects.select_related().filter(date__year=2012)\nq = qs.values('date', 'OtherField')\ndf = pd.DataFrame.from_records(q)\n""]"
40;8.0;0;11707586;;1;157;<python><pandas><options><display><column-width>;Python pandas, how to widen output display to see more columns?;96153.0;['dataframe', 'dataframe', 'dataframe', '>Index: 8 entries, count to max  \n>Data columns:  \n>x1          8  non-null values  \n>x2          8  non-null values  \n>x3          8  non-null values  \n>x4          8  non-null values  \n>x5          8  non-null values  \n>x6          8  non-null values  \n>x7          8  non-null values  \n']
41;1.0;7;11728836;;1;87;<python><pandas><multiprocessing><shared-memory>;Efficiently applying a function to a grouped pandas DataFrame in parallel;5986.0;['DataFrame', 'DataFrame', 'numpy']
42;4.0;0;11858472;;1;41;<python><numpy><dataframe><pandas>;Pandas: Combine string and int columns;30814.0;"['DataFrame', ""from pandas import *\ndf = DataFrame({'foo':['a','b','c'], 'bar':[1, 2, 3]})\n"", '    bar foo\n0    1   a\n1    2   b\n2    3   c\n', '     bar\n0    1 is a\n1    2 is b\n2    3 is c\n', ""df['foo'] = '%s is %s' % (df['bar'], df['foo'])\n"", '>>>print df.ix[0]\n\nbar                                                    a\nfoo    0    a\n1    b\n2    c\nName: bar is 0    1\n1    2\n2\nName: 0\n']"
43;8.0;0;11869910;;1;141;<pandas>;pandas: filter rows of DataFrame with operator chaining;187196.0;"['pandas', 'groupby', 'aggregate', 'apply', ""df_filtered = df[df['column'] == value]\n"", 'df', ""df_filtered = df.mask(lambda x: x['column'] == value)\n""]"
44;2.0;0;11927715;;1;38;<python><matplotlib><pandas>;How to give a pandas/matplotlib bar graph custom colors;29388.0;"[""  4 from matplotlib import pyplot\n  5 from pandas import *\n  6 import random\n  7 \n  8 x = [{i:random.randint(1,5)} for i in range(10)]\n  9 df = DataFrame(x)\n 10 \n 11 df.plot(kind='bar', stacked=True)\n""]"
45;3.0;0;11941492;;1;30;<python><ipython><pandas>;Selecting rows from a Pandas dataframe with a compound (hierarchical) index;25839.0;"['dataframe', 'dataframe', ""import pandas\ndf = pandas.DataFrame({'group1': ['a','a','a','b','b','b'],\n                       'group2': ['c','c','d','d','d','e'],\n                       'value1': [1.1,2,3,4,5,6],\n                       'value2': [7.1,8,9,10,11,12]\n})\ndf = df.set_index(['group1', 'group2'])\n"", ""df['group1' == 'a']\n"", ""df['a','c']\n""]"
46;1.0;1;11976503;;1;38;<python><pandas>;How to keep index when using pandas merge;12679.0;"['DataFrames', 'In [441]: a=DataFrame(data={""col1"": [1,2,3], \'to_merge_on\' : [1,3,4]}, index=[""a"",""b"",""c""])\n\nIn [442]: b=DataFrame(data={""col2"": [1,2,3], \'to_merge_on\' : [1,3,5]})\nIn [443]: a\nOut[443]: \n   col1  to_merge_on\na     1            1\nb     2            3\nc     3            4\n\nIn [444]: b\nOut[444]: \n   col2  to_merge_on\n0     1            1\n1     2            3\n2     3            5\n\n\nIn [445]: a.merge(b, how=""left"")\nOut[445]: \n   col1  to_merge_on  col2\n0     1            1     1\n1     2            3     2\n2     3            4   NaN\n\nIn [446]: _.index\nOut[447]: Int64Index([0, 1, 2])\n']"
47;3.0;7;12021730;;1;21;<python><pandas>;Can pandas handle variable-length whitespace as column delimiters;8587.0;"[""'s*'"", '## sample data\nhead sample.txt\n\n#                                                                            --- full sequence --- -------------- this domain -------------   hmm coord   ali coord   env coord\n# target name        accession   tlen query name           accession   qlen   E-value  score  bias   #  of  c-Evalue  i-Evalue  score  bias  from    to  from    to  from    to  acc description of target\n#------------------- ---------- ----- -------------------- ---------- ----- --------- ------ ----- --- --- --------- --------- ------ ----- ----- ----- ----- ----- ----- ----- ---- ---------------------\nABC_membrane         PF00664.18   275 AAF67494.2_AF170880  -            615     8e-29  100.7  11.4   1   1     3e-32     1e-28  100.4   7.9     3   273    42   313    40   315 0.95 ABC transporter transmembrane region\nABC_tran             PF00005.22   118 AAF67494.2_AF170880  -            615   2.6e-20   72.8   0.0   1   1   1.9e-23   6.4e-20   71.5   0.0     1   118   402   527   402   527 0.93 ABC transporter\nSMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   1   2    0.0036        12    4.9   0.0    27    40   391   404   383   408 0.86 RecF/RecN/SMC N terminal domain\nSMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   2   2   1.8e-09   6.1e-06   25.4   0.0   116   210   461   568   428   575 0.85 RecF/RecN/SMC N terminal domain\nAAA_16               PF13191.1    166 AAF67494.2_AF170880  -            615   3.1e-06   27.5   0.3   1   1     2e-09     7e-06   26.4   0.2    20   158   386   544   376   556 0.72 AAA ATPase domain\nYceG                 PF02618.11   297 AAF67495.1_AF170880  -            284   3.4e-64  216.6   0.0   1   1   2.9e-68     4e-64  216.3   0.0    68   296    53   274    29   275 0.85 YceG-like family\nPyr_redox_3          PF13738.1    203 AAF67496.2_AF170880  -            352   2.9e-28   99.1   0.0   1   2   2.8e-30   4.8e-27   95.2   0.0     1   201     4   198     4   200 0.85 Pyridine nucleotide-disulphide oxidoreductase\n\n#load data\nfrom pandas import *\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep="" "")\n\nValueError: Expecting 83 columns, got 91 in row 4\n\n#load data part 2\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep=""\'s*\' "")\n#this mushes some of the columns into the first column and drops the rest.\n    X.1\n1    ABC_tran PF00005.22 118 AAF67494.2_\n2    SMC_N PF02463.14 220 AAF67494.2_\n3    SMC_N PF02463.14 220 AAF67494.2_\n4    AAA_16 PF13191.1 166 AAF67494.2_\n5    YceG PF02618.11 297 AAF67495.1_\n6    Pyr_redox_3 PF13738.1 203 AAF67496.2_\n7    Pyr_redox_3 PF13738.1 203 AAF67496.2_\n8    FMO-like PF00743.14 532 AAF67496.2_\n9    FMO-like PF00743.14 532 AAF67496.2_\n']"
48;12.0;3;12047193;;1;43;<python><mysql><data-structures><pandas>;How to convert SQL Query result to PANDAS Data Structure?;69160.0;"['from sqlalchemy import create_engine\n\n\nengine2 = create_engine(\'mysql://THE DATABASE I AM ACCESSING\')\nconnection2 = engine2.connect()\ndataid = 1022\nresoverall = connection2.execute(""SELECT sum(BLABLA) AS BLA, sum(BLABLABLA2) AS BLABLABLA2, sum(SOME_INT) AS SOME_INT, sum(SOME_INT2) AS SOME_INT2, 100*sum(SOME_INT2)/sum(SOME_INT) AS ctr, sum(SOME_INT2)/sum(SOME_INT) AS cpc FROM daily_report_cooked WHERE campaign_id = \'%s\'""%dataid)\n']"
49;6.0;0;12065885;;1;223;<python><pandas><dataframe>;Filter dataframe rows if value in column is in a set list of values;129843.0;"['rpt', ""rpt\n<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 47518 entries, ('000002', '20120331') to ('603366', '20091231')\nData columns:\nSTK_ID                    47518  non-null values\nSTK_Name                  47518  non-null values\nRPT_Date                  47518  non-null values\nsales                     47518  non-null values\n"", ""'600809'"", ""rpt[rpt['STK_ID'] == '600809']"", ""<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 25 entries, ('600809', '20120331') to ('600809', '20060331')\nData columns:\nSTK_ID                    25  non-null values\nSTK_Name                  25  non-null values\nRPT_Date                  25  non-null values\nsales                     25  non-null values\n"", ""['600809','600141','600329']"", ""stk_list = ['600809','600141','600329']\n\nrst = rpt[rpt['STK_ID'] in stk_list] # this does not works in pandas \n""]"
50;1.0;1;12096252;;1;241;<python><pandas>;use a list of values to select rows from a pandas dataframe;178009.0;"[""df = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})\ndf\n\n     A   B\n0    5   1\n1    6   2\n2    3   3\n3    4   5\n"", ""x = df[df['A'] == 3]\nx\n\n     A   B\n2    3   3\n"", ""list_of_values = [3,6]\n\ny = df[df['A'] in list_of_values]\n""]"
51;3.0;1;12182744;;1;49;<python><pandas><apply>;python pandas: apply a function with arguments to a series;45364.0;['x = my_series.apply(my_function, more_arguments_1)\ny = my_series.apply(my_function, more_arguments_2)\n...\n']
52;5.0;1;12190874;;1;59;<python><partitioning><pandas>;Pandas: Sampling a DataFrame;57789.0;['rows = data.index\nrow_count = len(rows)\nrandom.shuffle(list(rows))\n\ndata.reindex(rows)\n\ntraining_data = data[row_count // 10:]\ntesting_data = data[:row_count // 10]\n', 'sklearn', 'IndexError: each subindex must be either a slice, an integer, Ellipsis, or newaxis\n']
53;5.0;3;12200693;;1;34;<python><group-by><dataframe><pandas>;Python Pandas How to assign groupby operation results back to columns in parent dataframe?;28220.0;"[""In [261]: bdata\nOut[261]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 21210 entries, 0 to 21209\nData columns:\nBloombergTicker      21206  non-null values\nCompany              21210  non-null values\nCountry              21210  non-null values\nMarketCap            21210  non-null values\nPriceReturn          21210  non-null values\nSEDOL                21210  non-null values\nyearmonth            21210  non-null values\ndtypes: float64(2), int64(1), object(4)\n"", 'In [262]: bdata.groupby(""yearmonth"").apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\nOut[262]:\nyearmonth\n201204      -0.109444\n201205      -0.290546\n', 'In [263]: dateGrps = bdata.groupby(""yearmonth"")\n\nIn [264]: dateGrps[""MarketReturn""] = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/mnt/bos-devrnd04/usr6/home/espears/ws/Research/Projects/python-util/src/util/<ipython-input-264-4a68c8782426> in <module>()\n----> 1 dateGrps[""MarketReturn""] = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n\nTypeError: \'DataFrameGroupBy\' object does not support item assignment\n', 'marketRetsByDate  = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n\nbdata[""MarketReturn""] = np.repeat(np.NaN, len(bdata))\n\nfor elem in marketRetsByDate.index.values:\n    bdata[""MarketReturn""][bdata[""yearmonth""]==elem] = marketRetsByDate.ix[elem]\n']"
54;2.0;0;12207326;;1;60;<python><statistics><pandas><frequency>;Frequency table for a single variable;54196.0;['my_series = pandas.Series([1,2,2,3,3,3])\npandas.magical_frequency_function( my_series )\n\n>> {\n     1 : 1,\n     2 : 2, \n     3 : 3\n   }\n']
55;3.0;1;12286607;;1;44;<python><pandas><heatmap>;python Making heatmap from DataFrame;34778.0;"[""import numpy as np \nfrom pandas import *\n\nIndex= ['aaa','bbb','ccc','ddd','eee']\nCols = ['A', 'B', 'C','D']\ndf = DataFrame(abs(np.random.randn(5, 4)), index= Index, columns=Cols)\n\n>>> df\n          A         B         C         D\naaa  2.431645  1.248688  0.267648  0.613826\nbbb  0.809296  1.671020  1.564420  0.347662\nccc  1.501939  1.126518  0.702019  1.596048\nddd  0.137160  0.147368  1.504663  0.202822\neee  0.134540  3.708104  0.309097  1.641090\n>>> \n""]"
56;3.0;0;12307099;;1;82;<python><pandas>;Modifying a subset of rows in a pandas dataframe;48481.0;"[""df['A'==0]['B'] = np.nan\n"", ""df['A'==0]['B'].values.fill(np.nan)\n""]"
57;2.0;0;12322779;;1;22;<pandas>;Pandas: unique dataframe;26767.0;['groupby']
58;1.0;0;12356501;;1;68;<python><pandas>;Pandas: create two new columns in a dataframe with values calculated from a pre-existing column;45846.0;"['df', 'def calculate(x):\n    ...operate...\n    return z, y\n', ""df['new_col']) = df['column_A'].map(a_function)\n"", ""(df['new_col_zetas'], df['new_col_ys']) = df['column_A'].map(calculate)\n"", ""df['column_A'].map(calculate)""]"
59;4.0;0;12376863;;1;51;<python><pandas>;Adding calculated column(s) to a dataframe in pandas;70320.0;"[""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 500047 entries, 1998-05-04 04:45:00 to 2012-08-07 00:15:00\nFreq: 15T\nData columns:\nClose    363152  non-null values\nHigh     363152  non-null values\nLow      363152  non-null values\nOpen     363152  non-null values\ndtypes: float64(4)\n"", 'def closed_in_top_half_of_range(h,l,c):\n    return c > l + (h-1)/2\n\ndef lower_wick(o,l,c):\n    return min(o,c)-l\n\ndef real_body(o,c):\n    return abs(c-o)\n\ndef lower_wick_at_least_twice_real_body(o,l,c):\n    return lower_wick(o,l,c) >= 2 * real_body(o,c)\n\ndef is_hammer(row):\n    return lower_wick_at_least_twice_real_body(row[""Open""],row[""Low""],row[""Close""]) \\\n    and closed_in_top_half_of_range(row[""High""],row[""Low""],row[""Close""])\n']"
60;1.0;3;12389898;;1;22;<python><group-by><transform><dataframe><pandas>;Python Pandas: how to add a totally new column to a data frame inside of a groupby/transform operation;18845.0;"[""import pandas, numpy as np\ndfrm = pandas.DataFrame({'A':np.random.rand(100), \n                         'B':(50+np.random.randn(100)), \n                         'C':np.random.randint(low=0, high=3, size=(100,))})\n"", 'import scipy.stats as st\ndef mark_quintiles(x, breakpoints):\n    # Assume this is filled in, using st.mstats.mquantiles.\n    # This returns an array the same shape as x, with an integer for which\n    # breakpoint-bucket that entry of x falls into.\n', 'transform', 'def transformXtiles(dataFrame, inputColumnName, newColumnName, breaks):\n    dataFrame[newColumnName] = mark_quintiles(dataFrame[inputColumnName].values, \n                                              breaks)\n    return dataFrame\n', 'dfrm.groupby(""C"").transform(lambda x: transformXtiles(x, ""A"", ""A_xtile"", [0.2, 0.4, 0.6, 0.8, 1.0]))\n', 'apply']"
61;4.0;1;12433076;;1;34;<pandas><finance><yahoo-finance><google-finance><stockquotes>;Download history stock prices automatically from yahoo finance in python;81673.0;[]
62;6.0;1;12497402;;1;43;<python><duplicates><pandas>;python pandas: Remove duplicates by columns A, keeping the row with the highest value in column B;38793.0;['A B\n1 10\n1 20\n2 30\n2 40\n3 10\n', 'A B\n1 20\n2 40\n3 10\n']
63;3.0;0;12504976;;1;26;<python><string><pandas><split>;"Get last ""column"" after .str.split() operation on column in pandas DataFrame";14984.0;"[""DataFrame.str.split(' ')"", '.str.split()', ""import pandas as pd\ntemp = pd.DataFrame({'ticker' : ['spx 5/25/2001 p500', 'spx 5/25/2001 p600', 'spx 5/25/2001 p700']})\ntemp2 = temp.ticker.str.split(' ')\n"", ""0    ['spx', '5/25/2001', 'p500']\n1    ['spx', '5/25/2001', 'p600']\n2    ['spx', '5/25/2001', 'p700']\n"", 'temp2[0]', 'temp2[:][-1]']"
64;4.0;0;12525722;;1;64;<python><pandas><numpy>;Normalize data in pandas;84877.0;['df.apply(average) \n', 'df.apply(max) - df.apply(min)\n']
65;18.0;0;12555323;;1;350;<python><pandas><dataframe>;Adding new column to existing DataFrame in Python pandas;578606.0;"['          a         b         c         d\n2  0.671399  0.101208 -0.181532  0.241273\n3  0.446172 -0.243316  0.051767  1.577318\n5  0.614758  0.075793 -0.451460 -0.012493\n', ""'e'"", '0   -0.335485\n1   -1.166658\n2   -0.385571\ndtype: float64\n', 'join', 'append', 'merge', 'e']"
66;2.0;4;12569730;;1;31;<python><wildcard><pandas>;Sum all columns with a wildcard name search using Python Pandas;6662.0;['Day P1S1 P1S2 P1S3 P2S1 P2S2 P2S3\n1   1    2    2    3    1    2\n2   2    2    3    5    4    2\n']
67;2.0;0;12589481;;1;30;<python><aggregate><pandas>;Python Pandas: Multiple aggregations of the same column;11477.0;"['df = pandas.DataFrame({\n                       ""date"":[datetime.date(2012,x,1) for x in range(1,11)], \n                       ""returns"":0.05*np.random.randn(10), \n                       ""dummy"":np.repeat(1,10) \n                      })\n', 'agg', '# Assume `function1` and `function2` are defined for aggregating.\ndf.groupby(""dummy"").agg({""returns"":function1, ""returns"":function2})\n', 'agg', '[(column, function)]']"
68;1.0;2;12604909;;1;28;<python><database><pandas>;Pandas: how to change all the values of a column?;53708.0;"['""Date""', 'City     Date\nParis    01/04/2004\nLisbon   01/09/2004\nMadrid   2004\nPekin    31/2004\n', 'City     Date\nParis    2004\nLisbon   2004\nMadrid   2004\nPekin    2004\n', ""fr61_70xls = pd.ExcelFile('AMADEUS FRANCE 1961-1970.xlsx')\n\n#Here we import the individual sheets and clean the sheets    \nyears=(['1961','1962','1963','1964','1965','1966','1967','1968','1969','1970'])\n\nfr={}\n\nheader=(['City','Country','NACE','Cons','Last_year','Op_Rev_EUR_Last_avail_yr','BvD_Indep_Indic','GUO_Name','Legal_status','Date_of_incorporation','Legal_status_date'])\n\nfor year in years:\n    # save every sheet in variable fr['1961'], fr['1962'] and so on\n    fr[year]=fr61_70xls.parse(year,header=0,parse_cols=10)\n    fr[year].columns=header\n    # drop the entire Legal status date column\n    fr[year]=fr[year].drop(['Legal_status_date','Date_of_incorporation'],axis=1)\n    # drop every row where GUO Name is empty\n    fr[year]=fr[year].dropna(axis=0,how='all',subset=[['GUO_Name']])\n    fr[year]=fr[year].set_index(['GUO_Name','Date_of_incorporation'])\n"", ""fr['1961']"", 'Date_of_incorporation']"
69;9.0;0;12680754;;1;38;<python><pandas><numpy><dataframe>;Split pandas dataframe string entry to separate rows;23089.0;"['pandas dataframe', 'a', 'b', 'In [7]: a\nOut[7]: \n    var1  var2\n0  a,b,c     1\n1  d,e,f     2\n\nIn [8]: b\nOut[8]: \n  var1  var2\n0    a     1\n1    b     1\n2    c     1\n3    d     2\n4    e     2\n5    f     2\n', '.apply', '.transform', ""from pandas import DataFrame\nimport numpy as np\na = DataFrame([{'var1': 'a,b,c', 'var2': 1},\n               {'var1': 'd,e,f', 'var2': 2}])\nb = DataFrame([{'var1': 'a', 'var2': 1},\n               {'var1': 'b', 'var2': 1},\n               {'var1': 'c', 'var2': 1},\n               {'var1': 'd', 'var2': 2},\n               {'var1': 'e', 'var2': 2},\n               {'var1': 'f', 'var2': 2}])\n"", ""def fun(row):\n    letters = row['var1']\n    letters = letters.split(',')\n    out = np.array([row] * len(letters))\n    out['var1'] = letters\na['idx'] = range(a.shape[0])\nz = a.groupby('idx')\nz.transform(fun)\n""]"
70;2.0;2;12725417;;1;21;<python><pandas>;Drop non-numeric columns from a pandas DataFrame;9057.0;['source = pandas.read_table(inputfile, index_col=0)\n']
71;1.0;3;12741092;;1;22;<python><dataframe><pandas>;Pandas DataFrame: apply function to all columns;13636.0;"['.map(func)', ""df=DataFrame({'a':[1,2,3,4,5,6],'b':[2,3,4,5,6,7]})\n\ndf['a']=df['a'].map(lambda x: x > 1)\n"", ""df['a'],df['b']=df['a'].map(lambda x: x > 1),df['b'].map(lambda x: x > 1)\n""]"
72;3.0;1;12860421;;1;24;<python><pandas><pivot-table>;Python Pandas : pivot table with aggfunc = count unique distinct;31611.0;"[""df2 = pd.DataFrame({'X' : ['X1', 'X1', 'X1', 'X1'], 'Y' : ['Y2','Y1','Y1','Y1'], 'Z' : ['Z3','Z1','Z1','Z2']})\n\n    X   Y   Z\n0  X1  Y2  Z3\n1  X1  Y1  Z1\n2  X1  Y1  Z1\n3  X1  Y1  Z2\n\ng=df2.groupby('X')\n\npd.pivot_table(g, values='X', rows='Y', cols='Z', margins=False, aggfunc='count')\n"", 'aggfunc', 'np.bincount()', 'values_counts()', 'Z   Z1  Z2  Z3\nY             \nY1   1   1 NaN\nY2 NaN NaN   1\n']"
73;5.0;4;12867178;;1;26;<python><pandas>;pandas: count things;42829.0;"[""mc = [ sum( male_trips['start_station_id'] == id ) for id in stations['id'] ]\n"", 'groupby()', 'size()', '.value_counts()', 'timeit', 'groupby', 'from timeit import Timer\nsetup = ""import pandas; male_trips=pandas.load(\'maletrips\')""\na  = ""male_trips.start_station_id.value_counts()""\nb = ""male_trips.groupby(\'start_station_id\').size()""\nTimer(a,setup).timeit(100)\nTimer(b,setup).timeit(100)\n', 'In [4]: Timer(a,setup).timeit(100) # <- this is value_counts\nOut[4]: 9.709594964981079\n\nIn [5]: Timer(b,setup).timeit(100) # <- this is groupby / size\nOut[5]: 1.5574288368225098\n']"
74;2.0;3;12877189;;1;21;<python><numpy><pandas>;float64 with pandas to_csv;18798.0;['Bob,0.085\nAlice,0.005\n', 'df = pd.read_csv(orig)\ndf.to_csv(pandasfile)\n', 'pandasfile', 'Bob,0.085000000000000006\nAlice,0.0050000000000000001\n']
75;1.0;3;12945971;;1;70;<python><matplotlib><pandas>;Pandas timeseries plot setting x-axis major and minor ticks and labels;63942.0;"['ax.xaxis.set_major_locator', 'ax.xaxis.set_major_formatter', 'import pandas\nprint \'pandas.__version__ is \', pandas.__version__\nprint \'matplotlib.__version__ is \', matplotlib.__version__    \n\ndStart = datetime.datetime(2011,5,1) # 1 May\ndEnd = datetime.datetime(2011,7,1) # 1 July    \n\ndateIndex = pandas.date_range(start=dStart, end=dEnd, freq=\'D\')\nprint ""1 May to 1 July 2011"", dateIndex      \n\ntestSeries = pandas.Series(data=np.random.randn(len(dateIndex)),\n                           index=dateIndex)    \n\nax = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\ntestSeries.plot(ax=ax, style=\'v-\', label=\'first line\')    \n\n# using MatPlotLib date time locators and formatters doesn\'t work with new\n# pandas datetime index\nax.xaxis.set_minor_locator(matplotlib.dates.WeekdayLocator(byweekday=(1),\n                                                           interval=1))\nax.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter(\'%d\\n%a\'))\nax.xaxis.grid(True, which=""minor"")\nax.xaxis.grid(False, which=""major"")\nax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\'\\n\\n\\n%b%Y\'))\nplt.show()    \n\n# set the major xticks and labels through pandas\nax2 = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\nxticks = pandas.date_range(start=dStart, end=dEnd, freq=\'W-Tue\')\nprint ""xticks: "", xticks\ntestSeries.plot(ax=ax2, style=\'-v\', label=\'second line\',\n                xticks=xticks.to_pydatetime())\nax2.set_xticklabels([x.strftime(\'%a\\n%d\\n%h\\n%Y\') for x in xticks]);\n# set the text of the first few minor ticks created by pandas.plot\n#    ax2.set_xticklabels([\'a\',\'b\',\'c\',\'d\',\'e\'], minor=True)\n# remove the minor xtick labels set by pandas.plot \nax2.set_xticklabels([], minor=True)\n# turn the minor ticks created by pandas.plot off \n# plt.minorticks_off()\nplt.show()\nprint testSeries[\'6/4/2011\':\'6/7/2011\']\n', ""pandas.__version__ is  0.9.1.dev-3de54ae\nmatplotlib.__version__ is  1.1.1\n1 May to 1 July 2011 <class 'pandas.tseries.index.DatetimeIndex'>\n[2011-05-01 00:00:00, ..., 2011-07-01 00:00:00]\nLength: 62, Freq: D, Timezone: None\n"", ""xticks:  <class 'pandas.tseries.index.DatetimeIndex'>\n[2011-05-03 00:00:00, ..., 2011-06-28 00:00:00]\nLength: 9, Freq: W-TUE, Timezone: None\n"", '2011-06-04   -0.199393\n2011-06-05   -0.043118\n2011-06-06    0.477771\n2011-06-07   -0.033207\nFreq: D\n', ""# only show month for first label in month\nmonth = dStart.month - 1\nxticklabels = []\nfor x in xticks:\n    if  month != x.month :\n        xticklabels.append(x.strftime('%d\\n%a\\n%h'))\n        month = x.month\n    else:\n        xticklabels.append(x.strftime('%d\\n%a'))\n"", 'ax.annotate']"
76;6.0;0;13003051;;1;21;<pandas>;How do I exclude a few columns from a DataFrame plot?;15239.0;[]
77;3.0;0;13021654;;1;54;<python><dataframe><pandas>;Retrieving column index from column name in python pandas;40205.0;['idx <- which(names(my_data)==my_colum_name)\n']
78;4.0;0;13035764;;1;90;<python><pandas>;Remove rows with duplicate indices (Pandas DataFrame and TimeSeries);67548.0;"['                      Sta  Precip1hr  Precip5min  Temp  DewPnt  WindSpd  WindDir  AtmPress\nDate                                                                                      \n2001-01-01 00:00:00  KPDX          0           0     4       3        0        0     30.31\n2001-01-01 00:05:00  KPDX          0           0     4       3        0        0     30.30\n2001-01-01 00:10:00  KPDX          0           0     4       3        4       80     30.30\n2001-01-01 00:15:00  KPDX          0           0     3       2        5       90     30.30\n2001-01-01 00:20:00  KPDX          0           0     3       2       10      110     30.28\n', ""import pandas \nimport datetime\nstartdate = datetime.datetime(2001, 1, 1, 0, 0)\nenddate = datetime.datetime(2001, 1, 1, 5, 0)\nindex = pandas.DatetimeIndex(start=startdate, end=enddate, freq='H')\ndata = {'A' : range(6), 'B' : range(6)}\ndata1 = {'A' : [20, -30, 40], 'B' : [-50, 60, -70]}\ndf1 = pandas.DataFrame(data=data, index=index)\ndf2 = pandas.DataFrame(data=data1, index=index[:3])\ndf3 = df1.append(df2)\ndf3\n                       A   B\n2001-01-01 00:00:00   20 -50\n2001-01-01 01:00:00  -30  60\n2001-01-01 02:00:00   40 -70\n2001-01-01 03:00:00    3   3\n2001-01-01 04:00:00    4   4\n2001-01-01 05:00:00    5   5\n2001-01-01 00:00:00    0   0\n2001-01-01 01:00:00    1   1\n2001-01-01 02:00:00    2   2\n"", 'df3', '                       A   B\n2001-01-01 00:00:00    0   0\n2001-01-01 01:00:00    1   1\n2001-01-01 02:00:00    2   2\n2001-01-01 03:00:00    3   3\n2001-01-01 04:00:00    4   4\n2001-01-01 05:00:00    5   5\n', ""df3['rownum'] = range(df3.shape[0])"", 'DatetimeIndex', 'group_by', 'pivot']"
79;2.0;4;13114512;;1;24;<python><pandas>;Calculating difference between two rows in Python / Pandas;31121.0;"['dataframes', 'pandas', '           Date   Close  Adj Close\n251  2011-01-03  147.48     143.25\n250  2011-01-04  147.64     143.41\n249  2011-01-05  147.05     142.83\n248  2011-01-06  148.66     144.40\n247  2011-01-07  147.93     143.69\n', ""import pandas\n\nurl = 'http://ichart.finance.yahoo.com/table.csv?s=IBM&a=00&b=1&c=2011&d=11&e=31&f=2011&g=d&ignore=.csv'\ndata = data = pandas.read_csv(url)\n\n## now I sorted the data frame ascending by date \ndata = data.sort(columns='Date')\n"", 'pandas', 'apply']"
80;3.0;1;13129618;;1;33;<python><pandas><numpy><matplotlib>;Histogram values of a Pandas Series;23683.0;['In [1]: series = pd.Series([0.0,950.0,-70.0,812.0,0.0,-90.0,0.0,0.0,-90.0,0.0,-64.0,208.0,0.0,-90.0,0.0,-80.0,0.0,0.0,-80.0,-48.0,840.0,-100.0,190.0,130.0,-100.0,-100.0,0.0,-50.0,0.0,-100.0,-100.0,0.0,-90.0,0.0,-90.0,-90.0,63.0,-90.0,0.0,0.0,-90.0,-80.0,0.0,])\n\nIn [2]: series.min()\nOut[2]: -100.0\n\nIn [3]: series.max()\nOut[3]: 950.0\n', 'lwb = range(-200,1000,50)\n', 'upb = range(-150,1050,50)\n', 'cut']
81;18.0;2;13148429;;1;286;<python><pandas>;How to change the order of DataFrame columns?;187524.0;"['DataFrame', 'df', 'import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.rand(10, 5))\n', ""df['mean'] = df.mean(1)\n"", 'mean']"
82;2.0;5;13167391;;1;25;<python><pandas>;filtering grouped df in pandas;14412.0;"['groupby', 'DataFrame', 'grouped[grouped.size > 1 ]\n', 'DataFrame', 'grouped', ""'name'"", ""'foo'"", ""'bar'"", ""df = pandas.DataFrame({'A': ['foo','bar','foo','foo'],\n                       'B': range(4)})\ngrouped = df.groupby('A')\n"", 'groupby', 'grouped[grouped.size() > 1]\n', 'A\nfoo 0\n    2\n    3\n', 'grouped']"
83;7.0;0;13187778;;1;92;<python><arrays><numpy><pandas><type-conversion>;Convert pandas dataframe to numpy array, preserving index;161037.0;"['label   A    B    C\nID                                 \n1   NaN  0.2  NaN\n2   NaN  NaN  0.5\n3   NaN  0.2  0.5\n4   0.1  0.2  NaN\n5   0.1  0.2  0.5\n6   0.1  NaN  0.5\n7   0.1  NaN  NaN\n', 'array([[ nan,  0.2,  nan],\n       [ nan,  nan,  0.5],\n       [ nan,  0.2,  0.5],\n       [ 0.1,  0.2,  nan],\n       [ 0.1,  0.2,  0.5],\n       [ 0.1,  nan,  0.5],\n       [ 0.1,  nan,  nan]])\n', ""array([[ 1, nan,  0.2,  nan],\n       [ 2, nan,  nan,  0.5],\n       [ 3, nan,  0.2,  0.5],\n       [ 4, 0.1,  0.2,  nan],\n       [ 5, 0.1,  0.2,  0.5],\n       [ 6, 0.1,  nan,  0.5],\n       [ 7, 0.1,  nan,  nan]],\n     dtype=[('ID', '<i4'), ('A', '<f8'), ('B', '<f8'), ('B', '<f8')])\n""]"
84;1.0;0;13226029;;1;29;<python><pandas><multi-index>;Benefits of panda's multiindex?;15935.0;[]
85;4.0;3;13269890;;1;30;<python><pandas>;cartesian product in pandas;18300.0;"[""from pandas import DataFrame\ndf1 = DataFrame({'col1':[1,2],'col2':[3,4]})\ndf2 = DataFrame({'col3':[5,6]})     \n"", ""#df1, df2 cartesian product\ndf_cartesian = DataFrame({'col1':[1,2,1,2],'col2':[3,4,3,4],'col3':[5,5,6,6]})\n""]"
86;2.0;2;13293810;;1;32;<pandas>;Import pandas dataframe column as string not int;28863.0;"[""ID\n00013007854817840016671868\n00013007854817840016749251\n00013007854817840016754630\n00013007854817840016781876\n00013007854817840017028824\n00013007854817840017963235\n00013007854817840018860166\n\n\ndf = read_csv('sample.csv')\n\ndf.ID\n>>\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\nName: ID\n"", ""df = read_csv('sample.csv', converters={'ID': str})\ndf.ID\n>>\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\nName: ID\n""]"
87;7.0;1;13295735;;1;145;<python><pandas>;How can I replace all the NaN values with Zero's in a column of a pandas dataframe;174458.0;['      itm Date                  Amount \n67    420 2012-09-30 00:00:00   65211\n68    421 2012-09-09 00:00:00   29424\n69    421 2012-09-16 00:00:00   29877\n70    421 2012-09-23 00:00:00   30990\n71    421 2012-09-30 00:00:00   61303\n72    485 2012-09-09 00:00:00   71781\n73    485 2012-09-16 00:00:00     NaN\n74    485 2012-09-23 00:00:00   11072\n75    485 2012-09-30 00:00:00  113702\n76    489 2012-09-09 00:00:00   64731\n77    489 2012-09-16 00:00:00     NaN\n', 'ValueError: cannot convert float NaN to integer\n']
88;5.0;0;13331518;;1;26;<python><pandas>;How to add a single item to a Pandas Series;38577.0;['>> x = Series()\n>> N = 4\n>> for i in xrange(N):\n>>     x.some_appending_function(i**2)    \n>> print x\n\n0 | 0\n1 | 1\n2 | 4\n3 | 9\n']
89;8.0;4;13331698;;1;137;<python><pandas>;How to apply a function to two columns of Pandas dataframe;154207.0;"['df', ""'ID', 'col_1', 'col_2'"", 'f = lambda x, y : my_function_expression', 'f', 'df', ""'col_1', 'col_2'"", ""'col_3'"", ""df['col_3'] = df[['col_1','col_2']].apply(f)  \n# Pandas gives : TypeError: ('<lambda>() takes exactly 2 arguments (1 given)'\n"", ""import pandas as pd\n\ndf = pd.DataFrame({'ID':['1','2','3'], 'col_1': [0,2,3], 'col_2':[1,4,5]})\nmylist = ['a','b','c','d','e','f']\n\ndef get_sublist(sta,end):\n    return mylist[sta:end+1]\n\n#df['col_3'] = df[['col_1','col_2']].apply(get_sublist,axis=1)\n# expect above to output df as below \n\n  ID  col_1  col_2            col_3\n0  1      0      1       ['a', 'b']\n1  2      2      4  ['c', 'd', 'e']\n2  3      3      5  ['d', 'e', 'f']\n""]"
90;6.0;0;13385860;;1;28;<python><parsing><pandas>;How can I remove extra whitespace from strings when parsing a csv file in Pandas?;28532.0;"['    1997,Ford,E350\n    1997, Ford , E350\n    1997,Ford,E350,""Super, luxurious truck""\n    1997,Ford,E350,""Super """"luxurious"""" truck""\n    1997,Ford,E350,"" Super luxurious truck ""\n    ""1997"",Ford,E350\n    1997,Ford,E350\n    2000,Mercury,Cougar\n', '       Year     Make   Model              Description\n    0  1997     Ford    E350                     None\n    1  1997     Ford    E350                     None\n    2  1997     Ford    E350   Super, luxurious truck\n    3  1997     Ford    E350  Super ""luxurious"" truck\n    4  1997     Ford    E350    Super luxurious truck\n    5  1997     Ford    E350                     None\n    6  1997     Ford    E350                     None\n    7  2000  Mercury  Cougar                     None\n', '    pd.read_table(""data.csv"", sep=r\',\', names=[""Year"", ""Make"", ""Model"", ""Description""])\n', '    Year     Make   Model              Description\n 0  1997     Ford    E350                     None\n 1  1997    Ford     E350                     None\n 2  1997     Ford    E350   Super, luxurious truck\n 3  1997     Ford    E350  Super ""luxurious"" truck\n 4  1997     Ford    E350   Super luxurious truck \n 5  1997     Ford    E350                     None\n 6  1997     Ford    E350                     None\n 7  2000  Mercury  Cougar                     None\n']"
91;2.0;0;13404468;;1;32;<python><pandas><scipy><statistics><hypothesis-test>;T-test in Pandas (Python);27308.0;"[""data = {'Category': ['cat2','cat1','cat2','cat1','cat2','cat1','cat2','cat1','cat1','cat1','cat2'],\n        'values': [1,2,3,1,2,3,1,2,3,5,1]}\nmy_data = DataFrame(data)\nmy_data.groupby('Category').mean()\n\nCategory:     values:   \ncat1     2.666667\ncat2     1.600000\n""]"
92;11.0;0;13411544;;1;643;<python><pandas><design><dataframe><magic-methods>;Delete column from pandas DataFrame;684200.0;"[""del df['column_name']\n"", 'del df.column_name\n', 'df.column_name']"
93;8.0;2;13413590;;1;272;<python><pandas><dataframe>;How to drop rows of Pandas DataFrame whose value in certain columns is NaN;280310.0;['DataFrame', '>>> df\n                 STK_ID  EPS  cash\nSTK_ID RPT_Date                   \n601166 20111231  601166  NaN   NaN\n600036 20111231  600036  NaN    12\n600016 20111231  600016  4.3   NaN\n601009 20111231  601009  NaN   NaN\n601939 20111231  601939  2.5   NaN\n000001 20111231  000001  NaN   NaN\n', 'EPS', 'NaN', 'df.drop(....)', '                  STK_ID  EPS  cash\nSTK_ID RPT_Date                   \n600016 20111231  600016  4.3   NaN\n601939 20111231  601939  2.5   NaN\n']
94;3.0;2;13445241;;1;57;<python><pandas>;Replacing blank values (white space) with NaN in pandas;50114.0;"['                   A    B    C\n2000-01-01 -0.532681  foo    0\n2000-01-02  1.490752  bar    1\n2000-01-03 -1.387326  foo    2\n2000-01-04  0.814772  baz     \n2000-01-05 -0.222552         4\n2000-01-06 -1.176781  qux     \n', '                   A     B     C\n2000-01-01 -0.532681   foo     0\n2000-01-02  1.490752   bar     1\n2000-01-03 -1.387326   foo     2\n2000-01-04  0.814772   baz   NaN\n2000-01-05 -0.222552   NaN     4\n2000-01-06 -1.176781   qux   NaN\n', ""for i in df.columns:\n    df[i][df[i].apply(lambda i: True if re.search('^\\s*$', str(i)) else False)]=None\n"", ""if df[i].dtype == np.dtype('object')\n""]"
95;2.0;0;13575090;;1;29;<python><dataframe><pandas>;Construct pandas DataFrame from items in nested dictionary;27582.0;"['user_dict[12] = {\n    ""Category 1"": {""att_1"": 1, \n                   ""att_2"": ""whatever""},\n    ""Category 2"": {""att_1"": 23, \n                   ""att_2"": ""another""}}\n', 'df = pandas.DataFrame(users_summary)\n']"
96;1.0;0;13582449;;1;21;<python><pandas>;Convert DataFrameGroupBy object to DataFrame pandas;15555.0;"[""kl = ks.groupby('FIPS')\n\nkl.aggregate(np.sum)\n"", 'pandas.core.groupby.DataFrameGroupBy']"
97;3.0;1;13611065;;1;46;<python><algorithm><pandas>;Efficient way to apply multiple filters to pandas DataFrame or Series;52152.0;"['reindex()', 'apply()', 'map()', ""relops = {'>=': [1], '<=': [1]}\n"", '   def apply_relops(series, relops):\n        """"""\n        Pass dictionary of relational operators to perform on given series object\n        """"""\n        for op, vals in relops.iteritems():\n            op_func = ops[op]\n            for val in vals:\n                filtered = op_func(series, val)\n                series = series.reindex(series[filtered])\n        return series\n', "">>> df = pandas.DataFrame({'col1': [0, 1, 2], 'col2': [10, 11, 12]})\n>>> print df\n>>> print df\n   col1  col2\n0     0    10\n1     1    11\n2     2    12\n\n>>> from operator import le, ge\n>>> ops ={'>=': ge, '<=': le}\n>>> apply_relops(df['col1'], {'>=': [1]})\ncol1\n1       1\n2       2\nName: col1\n>>> apply_relops(df['col1'], relops = {'>=': [1], '<=': [1]})\ncol1\n1       1\nName: col1\n""]"
98;5.0;0;13636592;;1;23;<python><pandas>;How to sort a Pandas DataFrame according to multiple criteria?;50220.0;"[""                                          Song            Peak            Weeks\n76                            Paperback Writer               1               16\n117                               Lady Madonna               1                9\n118                                   Hey Jude               1               27\n22                           Can't Buy Me Love               1               17\n29                          A Hard Day's Night               1               14\n48                              Ticket To Ride               1               14\n56                                       Help!               1               17\n109                       All You Need Is Love               1               16\n173                The Ballad Of John And Yoko               1               13\n85                               Eleanor Rigby               1               14\n87                            Yellow Submarine               1               14\n20                    I Want To Hold Your Hand               1               24\n45                                 I Feel Fine               1               15\n60                                 Day Tripper               1               12\n61                          We Can Work It Out               1               12\n10                               She Loves You               1               36\n155                                   Get Back               1                6\n8                               From Me To You               1                7\n115                              Hello Goodbye               1                7\n2                             Please Please Me               2               20\n92                   Strawberry Fields Forever               2               12\n93                                  Penny Lane               2               13\n107                       Magical Mystery Tour               2               16\n176                                  Let It Be               2               14\n0                                   Love Me Do               4               26\n157                                  Something               4                9\n166                              Come Together               4               10\n58                                   Yesterday               8               21\n135                       Back In The U.S.S.R.              19                3\n164                         Here Comes The Sun              58               19\n96       Sgt. Pepper's Lonely Hearts Club Band              63               12\n105         With A Little Help From My Friends              63                7\n""]"
99;4.0;2;13636848;;1;24;<python><pandas>;is it possible to do fuzzy match merge with python pandas?;10322.0;"[""df1 = DataFrame([[1],[2],[3],[4],[5]], index=['one','two','three','four','five'], columns=['number'])\n\n       number\none         1\ntwo         2\nthree       3\nfour        4\nfive        5\n\ndf2 = DataFrame([['a'],['b'],['c'],['d'],['e']], index=['one','too','three','fours','five'], columns=['letter'])\n\n      letter\none        a\ntoo        b\nthree      c\nfours      d\nfive       e\n"", '       number letter\none         1      a\ntwo         2      b\nthree       3      c\nfour        4      d\nfive        5      e\n']"
100;2.0;0;13651117;;1;39;<pandas>;pandas: filter lines on load in read_csv;17401.0;['read_csv']
101;1.0;0;13654699;;1;27;<python><datetime><python-2.7><pandas>;Reindexing pandas timeseries from object dtype to datetime dtype;23824.0;"[""In [1]: df = pd.read_csv('data.csv',index_col=0)\nIn [2]: print df['2008-02-27':'2008-03-02']\nOut[2]: \n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-02-30   0\n2008-02-31   0\n2008-03-01   0\n2008-03-02  17\n\nIn [3]: def clean_timestamps(df):\n    # remove invalid dates like '2008-02-30' and '2009-04-31'\n    to_drop = list()\n    for d in df.index:\n        try:\n            datetime.date(int(d[0:4]),int(d[5:7]),int(d[8:10]))\n        except ValueError:\n            to_drop.append(d)\n    df2 = df.drop(to_drop,axis=0)\n    return df2\n\nIn [4]: df2 = clean_timestamps(df)\nIn [5] :print df2['2008-02-27':'2008-03-02']\nOut[5]:\n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-03-01   0\n2008-03-02  17\n"", 'In [6]: df2.index\nOut[6]: Index([2008-01-01, 2008-01-02, 2008-01-03, ..., 2012-11-27, 2012-11-28,\n   2012-11-29], dtype=object)\n', ""In [7]: i = pd.date_range(start=min(df2.index),end=max(df2.index))\nIn [8]: df3 = df2.reindex(index=i,columns=['count'])\nIn [9]: df3['2008-02-27':'2008-03-02']\nOut[9]: \n            count\n2008-02-27 NaN\n2008-02-28 NaN\n2008-02-29 NaN\n2008-03-01 NaN\n2008-03-02 NaN\n"", ""In [10]: df3 = pd.DataFrame(columns=['count'],index=i)\nIn [11]: values = dict(df2['count'])\nIn [12]: for d in i:\n    try:\n        df3.set_value(index=d,col='count',value=values[d.isoformat()[0:10]])\n    except KeyError:\n        pass\nIn [13]: print df3['2008-02-27':'2008-03-02']\nOut[13]: \n\n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-03-01   0\n2008-03-02  17\n\nIn [14]: df3.index\nOut[14];\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2008-01-01 00:00:00, ..., 2012-11-29 00:00:00]\nLength: 1795, Freq: D, Timezone: None\n""]"
102;1.0;0;13659881;;1;23;<pandas>;Count by unique pair of columns in pandas;12957.0;"[""d = pd.DataFrame({'ip': ['192.168.0.1', '192.168.0.1', '192.168.0.1', '192.168.0.2'], 'useragent': ['a', 'a', 'b', 'b']})\n\n     ip              useragent\n0    192.168.0.1     a\n1    192.168.0.1     a\n2    192.168.0.1     b\n3    192.168.0.2     b\n"", 'ip           useragent  \n192.168.0.1  a           2\n192.168.0.1  b           1\n192.168.0.2  b           1\n']"
103;6.0;0;13682044;;1;48;<python><dataframe><pandas>;Pandas DataFrame: remove unwanted parts from strings in a column;66744.0;"['    time    result\n1    09:00   +52A\n2    10:00   +62B\n3    11:00   +44a\n4    12:00   +30b\n5    13:00   -110a\n', '    time    result\n1    09:00   52\n2    10:00   62\n3    11:00   44\n4    12:00   30\n5    13:00   110\n', "".str.lstrip('+-')"", ""str.rstrip('aAbBcC')"", 'TypeError: wrapper() takes exactly 1 argument (2 given)\n']"
104;10.0;3;13703720;;1;140;<python><datetime><numpy><pandas>;Converting between datetime, Timestamp and datetime64;163489.0;"['numpy.datetime64', 'datetime.datetime', 'Timestamp', ""import datetime\nimport numpy as np\nimport pandas as pd\ndt = datetime.datetime(2012, 5, 1)\n# A strange way to extract a Timestamp object, there's surely a better way?\nts = pd.DatetimeIndex([dt])[0]\ndt64 = np.datetime64(dt)\n\nIn [7]: dt\nOut[7]: datetime.datetime(2012, 5, 1, 0, 0)\n\nIn [8]: ts\nOut[8]: <Timestamp: 2012-05-01 00:00:00>\n\nIn [9]: dt64\nOut[9]: numpy.datetime64('2012-05-01T01:00:00.000000+0100')\n"", 'In [10]: ts.to_datetime()\nOut[10]: datetime.datetime(2012, 5, 1, 0, 0)\n', 'datetime', 'Timestamp', 'numpy.datetime64', 'dt64', ""dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100')\n"", 'datetime.datetime(2002, 6, 28, 1, 0)', '1025222400000000000L']"
105;2.0;0;13784192;;1;131;<python><dataframe><pandas>;Creating an empty Pandas DataFrame, then filling it?;326681.0;"[""import datetime as dt\nimport pandas as pd\nimport scipy as s\n\nif __name__ == '__main__':\n    base = dt.datetime.today().date()\n    dates = [ base - dt.timedelta(days=x) for x in range(0,10) ]\n    dates.sort()\n\n    valdict = {}\n    symbols = ['A','B', 'C']\n    for symb in symbols:\n        valdict[symb] = pd.Series( s.zeros( len(dates)), dates )\n\n    for thedate in dates:\n        if thedate > dates[0]:\n            for symb in valdict:\n                valdict[symb][thedate] = 1+valdict[symb][thedate - dt.timedelta(days=1)]\n\n    print valdict\n""]"
106;3.0;1;13838405;;1;25;<python><pandas>;Custom sorting in pandas dataframe;17015.0;"[""custom_dict = {'March':0, 'April':1, 'Dec':3}  \n""]"
107;8.0;3;13842088;;1;136;<python><pandas>;Set value for particular cell in pandas DataFrame;182280.0;"[""df=DataFrame(index=['A','B','C'], columns=['x','y'])\n"", ""df.xs('C')['x']=10\n""]"
108;3.0;0;13851535;;1;48;<python><pandas>;How to delete rows from a pandas DataFrame based on a conditional expression;79345.0;"['df.dropna()', 'NaN', ""df[(len(df['column name']) < 2)]\n"", ""KeyError: u'no item named False'\n""]"
109;3.0;0;13872533;;1;31;<python><matplotlib><pandas>;Plot different DataFrames in the same figure;31002.0;['2012-04-12,16:13:09,20.6\n2012-04-12,17:13:09,20.9\n2012-04-12,18:13:09,20.6\n2007-05-12,19:13:09,5.4\n2007-05-12,20:13:09,20.6\n2007-05-12,20:13:09,20.6\n2005-08-11,11:13:09,20.6\n2005-08-11,11:13:09,17.5\n2005-08-13,07:13:09,20.6\n2006-04-13,01:13:09,20.6\n']
110;2.0;3;13888468;;1;30;<pandas>;Get unique values from index column in MultiIndex;21242.0;"['DataFrame', '        C\n A B     \n 0 one  3\n 1 one  2\n 2 two  1\n', ""df = df.reset_index()\nuniq_b = df.B.unique()\ndf = df.set_index(['A','B'])\n""]"
111;2.0;0;13921647;;1;43;<python><r><pandas>;Python - Dimension of Data Frame;25371.0;[]
112;2.0;0;13999850;;1;28;<python><pandas>;How to specify date format when using pandas.to_csv?;23394.0;['to_csv()', '12/14/2012  12:00:00 AM\n', '20121214\n', '20121214,  084530\n']
113;2.0;2;14016247;;1;49;<python><pandas>;Python - find integer index of rows with NaN in pandas;59715.0;['                    a         b\n2011-01-01 00:00:00 1.883381  -0.416629\n2011-01-01 01:00:00 0.149948  -1.782170\n2011-01-01 02:00:00 -0.407604 0.314168\n2011-01-01 03:00:00 1.452354  NaN\n2011-01-01 04:00:00 -1.224869 -0.947457\n2011-01-01 05:00:00 0.498326  0.070416\n2011-01-01 06:00:00 0.401665  NaN\n2011-01-01 07:00:00 -0.019766 0.533641\n2011-01-01 08:00:00 -1.101303 -1.408561\n2011-01-01 09:00:00 1.671795  -0.764629\n', '[3, 6]']
114;4.0;0;14057007;;1;21;<python><filtering><pandas>;Remove rows not .isin('X');20970.0;"[""isin('X')"", 'X', '!which(a %in% b)']"
115;5.0;0;14059094;;1;24;<python><python-2.7><pandas>;I want to multiply two columns in a pandas DataFrame and add the result into a new column;39480.0;"[""for i in orders_df.Action:\n if i  == 'Sell':\n  orders_df['Value'] = orders_df.Prices*orders_df.Amount\n elif i == 'Buy':\n  orders_df['Value'] = -orders_df.Prices*orders_df.Amount)\n""]"
116;4.0;1;14162723;;1;36;<numpy><pandas><mysql-python>;Replacing Pandas or Numpy Nan with a None to use with MysqlDB;22191.0;[]
117;1.0;0;14225676;;1;43;<python><pandas><openpyxl>;Save list of DataFrames to multisheet Excel spreadsheet;23952.0;"['to_excel', ""writer = ExcelWriter('output.xlsx')"", ""df1.to_excel(writer, 'sheet1')"", ""df2.to_excel(writer, 'sheet2')"", 'writer.save()', ""from openpyxl.writer.excel import ExcelWriter\ndef save_xls(list_dfs, xls_path):\n    writer = ExcelWriter(xls_path)\n    for n, df in enumerate(list_dfs):\n        df.to_excel(writer,'sheet%s' % n)\n    writer.save()\n"", 'to_excel', ""AttributeError: 'str' object has no attribute 'worksheets'\n"", 'ExcelWriter']"
118;2.0;0;14247586;;1;60;<python><pandas><null><nan>;Python Pandas How to select rows with one or more nulls from a DataFrame without listing columns explicitly?;66974.0;['mask=False\nfor col in df.columns: mask = mask | df[col].isnull()\ndfnulls = df[mask]\n', 'df.ix[df.index[(df.T == np.nan).sum() > 1]]\n']
119;11.0;4;14262433;;1;577;<python><mongodb><pandas><large-data><hdf5>;"""Large data"" work flows using pandas";161470.0;"['HDFStore', ""if var1 > 2 then newvar = 'A' elif var2 = 4 then newvar = 'B'""]"
120;1.0;2;14300137;;1;52;<python><matplotlib><plot><dataframe><pandas>;making matplotlib scatter plots from dataframes in Python's pandas;43938.0;"['matplotlib', 'pandas', 'df', 'import matplotlib.pylab as plt\n# df is a DataFrame: fetch col1 and col2 \n# and drop na rows if any of the columns are NA\nmydata = df[[""col1"", ""col2""]].dropna(how=""any"")\n# Now plot with matplotlib\nvals = mydata.values\nplt.scatter(vals[:, 0], vals[:, 1])\n', 'col3', 'scatter', 'col1,col2', 'mydata = df.dropna(how=""any"", subset=[""col1"", ""col2""])\n# plot a scatter of col1 by col2, with sizes according to col3\nscatter(mydata([""col1"", ""col2""]), s=mydata[""col3""])\n', 'col1, col2', 'mydata = df.dropna(how=""any"", subset=[""col1"", ""col2""]) \nmyscatter = scatter(mydata[[""col1"", ""col2""]], s=1)\n# Plot in red, with smaller size, all the points that \n# have a col2 value greater than 0.5\nmyscatter.replot(mydata[""col2""] > 0.5, color=""red"", s=0.5)\n', 'subset_a', 'subset_b', 'col1,col2', 'col3', 'col1,col2,col3', 'col2', 'col3', 'col1', 'col3', 'dropna', 'mydata = df.dropna(how=""any"", subset=[""col1"", ""col2"", ""col3"")\n', 'mydata', 'col1,col2', 'col3', 'mydata', 'col1,col2', 'col3', 'mydata']"
121;7.0;5;14349055;;1;53;<python><r><matplotlib><plot><pandas>;making matplotlib graphs look like R by default?;21800.0;['matplotlib', 'matplotlib', 'matplotlib', 'matplotlib', 'matplotlib']
122;4.0;3;14365542;;1;35;<python><r><csv><pandas>;read csv file and return data.frame in Python;64795.0;"['""value.txt""', 'Date,""price"",""factor_1"",""factor_2""\n2012-06-11,1600.20,1.255,1.548\n2012-06-12,1610.02,1.258,1.554\n2012-06-13,1618.07,1.249,1.552\n2012-06-14,1624.40,1.253,1.556\n2012-06-15,1626.15,1.258,1.552\n2012-06-16,1626.15,1.263,1.558\n2012-06-17,1626.15,1.264,1.572\n', 'price <- read.csv(""value.txt"")  \n', '> price <- read.csv(""value.txt"")\n> price\n     Date   price factor_1 factor_2\n1  2012-06-11 1600.20    1.255    1.548\n2  2012-06-12 1610.02    1.258    1.554\n3  2012-06-13 1618.07    1.249    1.552\n4  2012-06-14 1624.40    1.253    1.556\n5  2012-06-15 1626.15    1.258    1.552\n6  2012-06-16 1626.15    1.263    1.558\n7  2012-06-17 1626.15    1.264    1.572\n']"
123;2.0;1;14380371;;1;30;<python><latex><dataframe><pandas>;Export a LaTeX table from pandas DataFrame;10522.0;[]
124;10.0;2;14507794;;1;89;<python><pandas>;Python Pandas - How to flatten a hierarchical index in columns;47981.0;"['     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf       \n                                     sum   sum   sum    sum   amax   amin\n0  702730  26451  1993      1    1     1     0    12     13  30.92  24.98\n1  702730  26451  1993      1    2     0     0    13     13  32.00  24.98\n2  702730  26451  1993      1    3     1    10     2     13  23.00   6.98\n3  702730  26451  1993      1    4     1     0    12     13  10.04   3.92\n4  702730  26451  1993      1    5     3     0    10     13  19.94  10.94\n', '     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf_amax  tmpf_amin   \n0  702730  26451  1993      1    1     1     0    12     13  30.92          24.98\n1  702730  26451  1993      1    2     0     0    13     13  32.00          24.98\n2  702730  26451  1993      1    3     1    10     2     13  23.00          6.98\n3  702730  26451  1993      1    4     1     0    12     13  10.04          3.92\n4  702730  26451  1993      1    5     3     0    10     13  19.94          10.94\n', ""{('USAF', ''): {0: '702730',\n  1: '702730',\n  2: '702730',\n  3: '702730',\n  4: '702730'},\n ('WBAN', ''): {0: '26451', 1: '26451', 2: '26451', 3: '26451', 4: '26451'},\n ('day', ''): {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n ('month', ''): {0: 1, 1: 1, 2: 1, 3: 1, 4: 1},\n ('s_CD', 'sum'): {0: 12.0, 1: 13.0, 2: 2.0, 3: 12.0, 4: 10.0},\n ('s_CL', 'sum'): {0: 0.0, 1: 0.0, 2: 10.0, 3: 0.0, 4: 0.0},\n ('s_CNT', 'sum'): {0: 13.0, 1: 13.0, 2: 13.0, 3: 13.0, 4: 13.0},\n ('s_PC', 'sum'): {0: 1.0, 1: 0.0, 2: 1.0, 3: 1.0, 4: 3.0},\n ('tempf', 'amax'): {0: 30.920000000000002,\n  1: 32.0,\n  2: 23.0,\n  3: 10.039999999999999,\n  4: 19.939999999999998},\n ('tempf', 'amin'): {0: 24.98,\n  1: 24.98,\n  2: 6.9799999999999969,\n  3: 3.9199999999999982,\n  4: 10.940000000000001},\n ('year', ''): {0: 1993, 1: 1993, 2: 1993, 3: 1993, 4: 1993}}\n""]"
125;1.0;0;14529838;;1;59;<python><group-by><aggregate-functions><pandas>;Apply multiple functions to multiple groupby columns;34437.0;"[""In [563]: grouped['D'].agg({'result1' : np.sum,\n   .....:                   'result2' : np.mean})\n   .....:\nOut[563]: \n      result2   result1\nA                      \nbar -0.579846 -1.739537\nfoo -0.280588 -1.402938\n"", ""grouped.agg({'C_sum' : lambda x: x['C'].sum(),\n             'C_std': lambda x: x['C'].std(),\n             'D_sum' : lambda x: x['D'].sum()},\n             'D_sumifC3': lambda x: x['D'][x['C'] == 3].sum(), ...)\n"", 'agg']"
126;1.0;1;14627380;;1;21;<python><html><css><dataframe><pandas>;pandas: HTML output with conditional formatting;9977.0;"['    correlation  p-value\n0   0.5          0.1\n1   0.1          0.8\n2   0.9          *0.01*\n', 'import pandas as pd\nfrom pandas.core import format\nfrom StringIO import StringIO\nbuf = StringIO()\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\nfmt = format.DataFrameFormatter(df, \n          formatters={\'p_value\':lambda x: ""*%f*"" % x if x<0.05 else str(x)})\nformat.HTMLFormatter(fmt).write_result(buf)\n', '<td>', '<table border=""1"" class=""dataframe"">\n  <thead>\n    <tr style=""text-align: right;"">\n      <th></th>\n      <th>correlation</th>\n      <th>p_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td> 0.5</td>\n      <td> 0.10</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td> 0.1</td>\n      <td> 0.80</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td> 0.9</td>\n      <td class=\'significant\'> 0.01</td>\n    </tr>\n  </tbody>\n</table>\n', '<span class=""signifcant"">...</span>', 'import pandas as pd\nfrom StringIO import StringIO\nbuf = StringIO()\nsignificant = lambda x: \'<span class=""significant"">%f</span>\' % x if x<0.05 else str(x)\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\ndf.to_html(buf, formatters={\'p_value\': significant})\n', ""df.to_html(buf, formatters={'p_value': significant}, escape=False)\n""]"
127;5.0;0;14661701;;1;133;<python><pandas>;How to drop a list of rows from Pandas dataframe?;202455.0;['>>> df\n                  sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                     \n600141 20060331   2.709       NaN      2.709   2.245\n       20060630   6.590       NaN      6.590   5.291\n       20060930  10.103       NaN     10.103   7.981\n       20061231  15.915       NaN     15.915  12.686\n       20070331   3.196       NaN      3.196   2.710\n       20070630   7.907       NaN      7.907   6.459\n', '[1,2,4],', '                  sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                     \n600141 20060331   2.709       NaN      2.709   2.245\n       20061231  15.915       NaN     15.915  12.686\n       20070630   7.907       NaN      7.907   6.459\n']
128;2.0;4;14663004;;1;53;<pandas>;How to get the last n row of pandas dataframe?;73792.0;['df1', 'df2', '>>> df1\n    STK_ID  RPT_Date  TClose   sales  discount\n0   000568  20060331    3.69   5.975       NaN\n1   000568  20060630    9.14  10.143       NaN\n2   000568  20060930    9.49  13.854       NaN\n3   000568  20061231   15.84  19.262       NaN\n4   000568  20070331   17.00   6.803       NaN\n5   000568  20070630   26.31  12.940       NaN\n6   000568  20070930   39.12  19.977       NaN\n7   000568  20071231   45.94  29.269       NaN\n8   000568  20080331   38.75  12.668       NaN\n9   000568  20080630   30.09  21.102       NaN\n10  000568  20080930   26.00  30.769       NaN\n\n>>> df2\n                 TClose   sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                             \n000568 20060331    3.69   5.975       NaN      5.975   2.591\n       20060630    9.14  10.143       NaN     10.143   4.363\n       20060930    9.49  13.854       NaN     13.854   5.901\n       20061231   15.84  19.262       NaN     19.262   8.407\n       20070331   17.00   6.803       NaN      6.803   2.815\n       20070630   26.31  12.940       NaN     12.940   5.418\n       20070930   39.12  19.977       NaN     19.977   8.452\n       20071231   45.94  29.269       NaN     29.269  12.606\n       20080331   38.75  12.668       NaN     12.668   3.958\n       20080630   30.09  21.102       NaN     21.102   7.431\n', '>>> df2.ix[-3:]\n                 TClose   sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                             \n000568 20071231   45.94  29.269       NaN     29.269  12.606\n       20080331   38.75  12.668       NaN     12.668   3.958\n       20080630   30.09  21.102       NaN     21.102   7.431\n', 'df1.ix[-3:]', '>>> df1.ix[-3:]\n    STK_ID  RPT_Date  TClose   sales  discount\n0   000568  20060331    3.69   5.975       NaN\n1   000568  20060630    9.14  10.143       NaN\n2   000568  20060930    9.49  13.854       NaN\n3   000568  20061231   15.84  19.262       NaN\n4   000568  20070331   17.00   6.803       NaN\n5   000568  20070630   26.31  12.940       NaN\n6   000568  20070930   39.12  19.977       NaN\n7   000568  20071231   45.94  29.269       NaN\n8   000568  20080331   38.75  12.668       NaN\n9   000568  20080630   30.09  21.102       NaN\n10  000568  20080930   26.00  30.769       NaN\n', 'df1']
129;5.0;0;14688306;;1;35;<python><pandas>;Adding meta-information/metadata to pandas DataFrame;9816.0;[]
130;1.0;0;14733871;;1;42;<python><sorting><pandas><multi-index>;Multi Index Sorting in Pandas;17418.0;['    Group1    Group2\n    A B C     A B C\n1   1 0 3     2 5 7\n2   5 6 9     1 0 0\n3   7 0 2     0 3 5 \n', '    Group1    Group2\n    A B C     A B C\n 2  5 6 9     1 0 0\n 1  1 0 3     2 5 7\n 3  7 0 2     0 3 5 \n']
131;5.0;0;14734533;;1;71;<python><group-by><dataframe><pandas>;How to access pandas groupby dataframe by key;77177.0;"[""rand = np.random.RandomState(1)\ndf = pd.DataFrame({'A': ['foo', 'bar'] * 3,\n                   'B': rand.randn(6),\n                   'C': rand.randint(0, 20, 6)})\ngb = df.groupby(['A'])\n"", ""In [11]: for k, gp in gb:\n             print 'key=' + str(k)\n             print gp\nkey=bar\n     A         B   C\n1  bar -0.611756  18\n3  bar -1.072969  10\n5  bar -2.301539  18\nkey=foo\n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n"", ""In [12]: gb['foo']\nOut[12]:  \n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n"", ""gb[('foo',)]"", 'pandas.core.groupby.DataFrameGroupBy', ""In [13]: def gb_df_key(gb, key, orig_df):\n             ix = gb.indices[key]\n             return orig_df.ix[ix]\n\n         gb_df_key(gb, 'foo', df)\nOut[13]:\n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14  \n""]"
132;2.0;0;14744068;;1;22;<python><pandas>;Prepend a level to a pandas MultiIndex;16301.0;"[""import numpy as np\nimport pandas as p\nfrom numpy.random import randn\n\ndf = p.DataFrame({\n    'A' : ['a1', 'a1', 'a2', 'a3']\n  , 'B' : ['b1', 'b2', 'b3', 'b4']\n  , 'Vals' : randn(4)\n}).groupby(['A', 'B']).sum()\n\ndf\n\nOutput>            Vals\nOutput> A  B           \nOutput> a1 b1 -1.632460\nOutput>    b2  0.596027\nOutput> a2 b3 -0.619130\nOutput> a3 b4 -0.002009\n"", 'Output>                       Vals\nOutput> FirstLevel A  B           \nOutput> Foo        a1 b1 -1.632460\nOutput>               b2  0.596027\nOutput>            a2 b3 -0.619130\nOutput>            a3 b4 -0.002009\n']"
133;4.0;1;14745022;;1;42;<python><dataframe><pandas>;Pandas DataFrame, how do i split a column into two;56622.0;"[""fips'"", ""'row'"", '          row\n0    00000 UNITED STATES\n1    01000 ALABAMA\n2    01001 Autauga County, AL\n3    01003 Baldwin County, AL\n4    01005 Barbour County, AL\n', 'df.row.str[:]', ""df['fips'] = hello"", '         fips       row\n0    00000 UNITED STATES\n1    01000 ALABAMA \n2    01001 Autauga County, AL\n3    01003 Baldwin County, AL\n4    01005 Barbour County, AL\n']"
134;2.0;1;14808945;;1;34;<python><pandas>;check if variable is dataframe;21988.0;"['def f(var):\nif var == pd.DataFrame():\n    print ""do stuff""\n', 'def f(var):\nif var.values != None:\n    print ""do stuff""\n']"
135;6.0;1;14940743;;1;84;<python><pandas>;Selecting/Excluding sets of columns in Pandas;77656.0;"['import numpy as np\nimport pandas as pd\n\n# Create a dataframe with columns A,B,C and D\ndf = pd.DataFrame(np.random.randn(100, 4), columns=list(\'ABCD\'))\n\n# Try to create a second dataframe df2 from df with all columns except \'B\' and D\nmy_cols = set(df.columns)\nmy_cols.remove(\'B\').remove(\'D\')\n\n# This returns an error (""unhashable type: set"")\ndf2 = df[my_cols]\n']"
136;3.0;0;14941366;;1;43;<python><sorting><group-by><dataframe><pandas>;Pandas sort by group aggregate and column;70881.0;"[""In [31]: rand = np.random.RandomState(1)\n         df = pd.DataFrame({'A': ['foo', 'bar', 'baz'] * 2,\n                            'B': rand.randn(6),\n                            'C': rand.rand(6) > .5})\n\nIn [32]: df\nOut[32]:      A         B      C\n         0  foo  1.624345  False\n         1  bar -0.611756   True\n         2  baz -0.528172  False\n         3  foo -1.072969   True\n         4  bar  0.865408  False\n         5  baz -2.301539   True \n"", 'A', 'B', 'C', 'A', ""In [28]: df.groupby('A').sum().sort('B')\nOut[28]:             B  C\n         A               \n         baz -2.829710  1\n         bar  0.253651  1\n         foo  0.551377  1\n"", 'In [30]: df.ix[[5, 2, 1, 4, 3, 0]]\nOut[30]: A         B      C\n    5  baz -2.301539   True\n    2  baz -0.528172  False\n    1  bar -0.611756   True\n    4  bar  0.865408  False\n    3  foo -1.072969   True\n    0  foo  1.624345  False\n']"
137;2.0;0;14964493;;1;21;<pandas>;MultiIndex-based indexing in pandas;10792.0;"[""import itertools\nimport pandas as pd\nimport numpy as np\na = ('A', 'B')\ni = (0, 1, 2)\nb = (True, False)\nidx = pd.MultiIndex.from_tuples(list(itertools.product(a, i, b)),\n                                names=('Alpha', 'Int', 'Bool'))\ndf = pd.DataFrame(np.random.randn(len(idx), 7), index=idx,\n                  columns=('I', 'II', 'III', 'IV', 'V', 'VI', 'VII'))\n"", 'In [19]: df\nOut[19]: \n                        I        II       III        IV         V        VI       VII\nAlpha Int Bool                                                                       \nA     0   True  -0.462924  1.210442  0.306737  0.325116 -1.320084 -0.831699  0.892865\n          False -0.850570 -0.949779  0.022074 -0.205575 -0.684794 -0.214307 -1.133833\n      1   True   0.603602  1.387020 -0.830780 -1.242000 -0.321938  0.484271  0.171738\n          False -1.591730  1.282136  0.095159 -1.239882  0.760880 -0.606444 -0.485957\n      2   True  -1.346883  1.650247 -1.476443  2.092067  1.344689  0.177083  0.100844\n          False  0.001407 -1.127299 -0.417828  0.143595 -0.277838 -0.478262 -0.350906\nB     0   True   0.722781 -1.093182  0.237536  0.457614 -2.500885  0.338257  0.009128\n          False  0.321022  0.419357  1.161140 -1.371035  1.093696  0.250517 -1.125612\n      1   True   0.237441  1.739933  0.029653  0.327823 -0.384647  1.523628 -0.009053\n          False -0.459148 -0.598577 -0.593486 -0.607447  1.478399  0.504028 -0.329555\n      2   True  -0.583052 -0.986493 -0.057788 -0.639798  1.400311  0.076471 -0.212513\n          False  0.896755  2.583520  1.520151  2.367336 -1.084994 -1.233548 -2.414215\n', ""'VII'"", ""In [20]: df['VII']\nOut[20]: \nAlpha  Int  Bool \nA      0    True     0.892865\n            False   -1.133833\n       1    True     0.171738\n            False   -0.485957\n       2    True     0.100844\n            False   -0.350906\nB      0    True     0.009128\n            False   -1.125612\n       1    True    -0.009053\n            False   -0.329555\n       2    True    -0.212513\n            False   -2.414215\nName: VII\n"", ""Alpha=='B'"", ""Alpha=='B'"", 'Bool==False', ""Alpha=='B'"", 'Bool==False', ""'I'"", ""Alpha=='B'"", 'Bool==False', ""'I'"", ""'III'"", ""Alpha=='B'"", 'Bool==False', ""'I'"", ""'III'"", ""'V'"", 'Int']"
138;4.0;3;14984119;;1;24;<python><pandas>;python pandas remove duplicate columns;23542.0;['import pandas as pd\n\ndf=pd.read_table(fname)\n', 'Time, Time Relative, N2, Time, Time Relative, H2, etc...\n', 'Time, Time Relative, N2, H2\n', 'df=df.T.drop_duplicates().T\n', 'Reindexing only valid with uniquely valued index objects\n', 'Time    Time Relative [s]    N2[%]    Time    Time Relative [s]    H2[ppm]\n2/12/2013 9:20:55 AM    6.177    9.99268e+001    2/12/2013 9:20:55 AM    6.177    3.216293e-005    \n2/12/2013 9:21:06 AM    17.689    9.99296e+001    2/12/2013 9:21:06 AM    17.689    3.841667e-005    \n2/12/2013 9:21:18 AM    29.186    9.992954e+001    2/12/2013 9:21:18 AM    29.186    3.880365e-005    \n... etc ...\n2/12/2013 2:12:44 PM    17515.269    9.991756+001    2/12/2013 2:12:44 PM    17515.269    2.800279e-005    \n2/12/2013 2:12:55 PM    17526.769    9.991754e+001    2/12/2013 2:12:55 PM    17526.769    2.880386e-005\n2/12/2013 2:13:07 PM    17538.273    9.991797e+001    2/12/2013 2:13:07 PM    17538.273    3.131447e-005\n']
139;2.0;2;14988480;;1;26;<python><r><dataframe><pandas>;Pandas version of rbind;17614.0;['        0         1       2        3          4          5        6                    7\n0   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43\n4   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44\n5  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44\n6   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45\n', '        0         1        2        3          4         5        6                    7       0         1       2        3          4          5        6                    7\n0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43\n4     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44\n5     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44\n6     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45\n0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  \n']
140;1.0;0;14991195;;1;23;<python><pandas>;How to remove rows with null values from kth column onward in python;33921.0;"[""df = DataFrame(np.random.randn(6, 5), index=['a', 'c', 'e', 'f', 'g','h'], columns=['one', 'two', 'three', 'four', 'five'])\n\ndf2 = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\ndf2.ix[1][0] = 111\ndf2.ix[1][1] = 222\n"", 'df.dropna()']"
141;7.0;4;15006298;;1;23;<python><pandas><dataframe><ipython><ipython-notebook>;How to preview a part of a large pandas DataFrame?;40523.0;"['DataFrame', 'DataFrame', 'In [27]:\n\nevaluation = readCSV(""evaluation_MO_without_VNS_quality.csv"").filter([""solver"", ""instance"", ""runtime"", ""objective""])\n\nIn [37]:\n\nevaluation\n\nOut[37]:\n\n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 333 entries, 0 to 332\nData columns:\nsolver       333  non-null values\ninstance     333  non-null values\nruntime      333  non-null values\nobjective    333  non-null values\ndtypes: int64(1), object(3)\n']"
142;2.0;1;15008970;;1;24;<csv><dataframe><pandas>;Way to read first few lines for pandas dataframe;24174.0;"['read_csv', 'n', 'footer_lines = total_lines - n', 'skipfooter', 'n', ""import pandas as pd\nfrom StringIO import StringIO\n\nn = 20\nwith open('big_file.csv', 'r') as f:\n    head = ''.join(f.readlines(n))\n\ndf = pd.read_csv(StringIO(head))\n""]"
143;4.0;4;15017072;;1;37;<python><pandas>;pandas read_csv and filter columns with usecols;74330.0;"['pandas.read_csv', 'usecols', 'import pandas as pd\ncsv = r""""""dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nbar,20090103,b,5""""""\nf = open(\'foo.csv\', \'w\')\nf.write(csv)\nf.close()\n\ndf1 = pd.read_csv(\'foo.csv\', \n        index_col=[""date"", ""loc""], \n        usecols=[""dummy"", ""date"", ""loc"", ""x""],\n        parse_dates=[""date""],\n        header=0,\n        names=[""dummy"", ""date"", ""loc"", ""x""])\nprint df1\n\n# Ignore the dummy columns\ndf2 = pd.read_csv(\'foo.csv\', \n        index_col=[""date"", ""loc""], \n        usecols=[""date"", ""loc"", ""x""], # <----------- Changed\n        parse_dates=[""date""],\n        header=0,\n        names=[""dummy"", ""date"", ""loc"", ""x""])\nprint df2\n', 'In [118]: %run test.py\n               dummy  x\ndate       loc\n2009-01-01 a     bar  1\n2009-01-02 a     bar  3\n2009-01-03 a     bar  5\n2009-01-01 b     bar  1\n2009-01-02 b     bar  3\n2009-01-03 b     bar  5\n              date\ndate loc\na    1    20090101\n     3    20090102\n     5    20090103\nb    1    20090101\n     3    20090102\n     5    20090103\n']"
144;2.0;0;15026698;;1;30;<python><csv><pandas><dataframe><whitespace>;How to make separator in read_csv more flexible wrt whitespace?;25215.0;['read_csv', '\\t', 'for line in file(file_name):\n   fld = line.split()\n']
145;4.0;0;15118111;;1;24;<python><pandas>;Apply function to each row of pandas dataframe to create two new columns;61486.0;"['st', ""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 53732 entries, 1993-01-07 12:23:58 to 2012-12-02 20:06:23\nData columns:\nDate(dd-mm-yy)_Time(hh-mm-ss)       53732  non-null values\nJulian_Day                          53732  non-null values\nAOT_1020                            53716  non-null values\nAOT_870                             53732  non-null values\nAOT_675                             53188  non-null values\nAOT_500                             51687  non-null values\nAOT_440                             53727  non-null values\nAOT_380                             51864  non-null values\nAOT_340                             52852  non-null values\nWater(cm)                           51687  non-null values\n%TripletVar_1020                    53710  non-null values\n%TripletVar_870                     53726  non-null values\n%TripletVar_675                     53182  non-null values\n%TripletVar_500                     51683  non-null values\n%TripletVar_440                     53721  non-null values\n%TripletVar_380                     51860  non-null values\n%TripletVar_340                     52846  non-null values\n440-870Angstrom                     53732  non-null values\n380-500Angstrom                     52253  non-null values\n440-675Angstrom                     53732  non-null values\n500-870Angstrom                     53732  non-null values\n340-440Angstrom                     53277  non-null values\nLast_Processing_Date(dd/mm/yyyy)    53732  non-null values\nSolar_Zenith_Angle                  53732  non-null values\ndtypes: datetime64[ns](1), float64(22), object(1)\n"", 'apply', 'apply', 'Series', ""def calculate(s):\n    a = s['path'] + 2*s['row'] # Simple calc for example\n    b = s['path'] * 0.153\n    return (a, b)\n"", 'st.apply(calculate, axis=1)\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n<ipython-input-248-acb7a44054a7> in <module>()\n----> 1 st.apply(calculate, axis=1)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in apply(self, func, axis, broadcast, raw, args, **kwds)\n   4191                     return self._apply_raw(f, axis)\n   4192                 else:\n-> 4193                     return self._apply_standard(f, axis)\n   4194             else:\n   4195                 return self._apply_broadcast(f, axis)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _apply_standard(self, func, axis, ignore_failures)\n   4274                 index = None\n   4275 \n-> 4276             result = self._constructor(data=results, index=index)\n   4277             result.rename(columns=dict(zip(range(len(res_index)), res_index)),\n   4278                           inplace=True)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in __init__(self, data, index, columns, dtype, copy)\n    390             mgr = self._init_mgr(data, index, columns, dtype=dtype, copy=copy)\n    391         elif isinstance(data, dict):\n--> 392             mgr = self._init_dict(data, index, columns, dtype=dtype)\n    393         elif isinstance(data, ma.MaskedArray):\n    394             mask = ma.getmaskarray(data)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _init_dict(self, data, index, columns, dtype)\n    521 \n    522         return _arrays_to_mgr(arrays, data_names, index, columns,\n--> 523                               dtype=dtype)\n    524 \n    525     def _init_ndarray(self, values, index, columns, dtype=None,\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _arrays_to_mgr(arrays, arr_names, index, columns, dtype)\n   5411 \n   5412     # consolidate for now\n-> 5413     mgr = BlockManager(blocks, axes)\n   5414     return mgr.consolidate()\n   5415 \n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in __init__(self, blocks, axes, do_integrity_check)\n    802 \n    803         if do_integrity_check:\n--> 804             self._verify_integrity()\n    805 \n    806         self._consolidate_check()\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in _verify_integrity(self)\n    892                                      ""items"")\n    893             if block.values.shape[1:] != mgr_shape[1:]:\n--> 894                 raise AssertionError(\'Block shape incompatible with manager\')\n    895         tot_items = sum(len(x.items) for x in self.blocks)\n    896         if len(self.items) != tot_items:\n\nAssertionError: Block shape incompatible with manager\n', 'apply', 'None', ""st['a'] = None\nst['b'] = None\n"", 'None', ""for i in st.index:\n    # do calc here\n    st.ix[i]['a'] = a\n    st.ix[i]['b'] = b\n""]"
146;2.0;0;15203623;;1;30;<python><pandas>;Convert pandas DateTimeIndex to Unix Time?;20077.0;['[time.mktime(t.timetuple()) for t in my_data_frame.index.to_pydatetime()]\n']
147;2.0;2;15210962;;1;34;<numpy><pandas>;Specifying dtype float32 with pandas.read_csv on pandas 0.10.1;52865.0;"['read_csv', 'dtype', 'read_csv', 'converters', "">>> cat test.out\na b\n0.76398 0.81394\n0.32136 0.91063\n>>> import pandas\n>>> import numpy\n>>> x = pandas.read_csv('test.out', dtype={'a': numpy.float32}, delim_whitespace=True)\n>>> x\n         a        b\n0  0.76398  0.81394\n1  0.32136  0.91063\n>>> x.a.dtype\ndtype('float64')\n"", 'dtype', 'numpy.int32', 'numpy.int64', ""AttributeError: 'NoneType' object has no attribute 'dtype'\n"", 'AttributeError', "">>> !uname -a\nLinux ubuntu 3.0.0-13-generic #22-Ubuntu SMP Wed Nov 2 13:25:36 UTC 2011 i686 i686 i386 GNU/Linux\n>>> import platform\n>>> platform.architecture()\n('32bit', 'ELF')\n>>> pandas.__version__\n'0.10.1'\n""]"
148;3.0;0;15222754;;1;23;<python><pandas>;Group by pandas dataframe and select most common string factor;15902.0;"[""import pandas as pd\nfrom scipy import stats\n\nsource = pd.DataFrame({'Country' : ['USA', 'USA', 'Russia','USA'], \n                  'City' : ['New-York', 'New-York', 'Sankt-Petersburg', 'New-York'],\n                  'Short name' : ['NY','New','Spb','NY']})\n\nprint source.groupby(['Country','City']).agg(lambda x: stats.mode(x['Short name'])[0])\n""]"
149;3.0;0;15242746;;1;38;<python><pandas>;Handling Variable Number of Columns with Pandas - Python;15050.0;"['1,2,3\n1,2,3,4\n1,2,3,4,5\n1,2\n1,2,3,4\n....\n', ""import pandas as pd\nmy_cols=['A','B','C','D','E']\nmy_df=pd.read_table(path,sep=',',header=None,names=my_cols)\n""]"
150;1.0;0;15315452;;1;77;<python><pandas>;Selecting with complex criteria from pandas.DataFrame;153283.0;"[""import pandas as pd\nfrom random import randint\n\ndf = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],\n                   'B': [randint(1, 9)*10 for x in xrange(10)],\n                   'C': [randint(1, 9)*100 for x in xrange(10)]})\n""]"
151;2.0;0;15322632;;1;37;<python><group-by><pandas>;python pandas, DF.groupby().agg(), column reference in agg();68108.0;"['     word  tag count\n0    a     S    30\n1    the   S    20\n2    a     T    60\n3    an    T    5\n4    the   T    10 \n', '     word  tag count\n1    the   S    20\n2    a     T    60\n3    an    T    5\n', ""DF.groupby(['word']).agg(lambda x: x['tag'][ x['count'].argmax() ] )\n""]"
152;4.0;2;15325182;;1;56;<python><regex><pandas>;How to filter rows in pandas by regex;38880.0;"[""In [210]: foo = pd.DataFrame({'a' : [1,2,3,4], 'b' : ['hi', 'foo', 'fat', 'cat']})\nIn [211]: foo\nOut[211]: \n   a    b\n0  1   hi\n1  2  foo\n2  3  fat\n3  4  cat\n"", 'f', ""In [213]: foo.b.str.match('f.*')\nOut[213]: \n0    []\n1    ()\n2    ()\n3    []\n"", ""In [226]: foo.b.str.match('(f.*)').str.len() > 0\nOut[226]: \n0    False\n1     True\n2     True\n3    False\nName: b\n"", ""In [229]: foo[foo.b.str.match('(f.*)').str.len() > 0]\nOut[229]: \n   a    b\n1  2  foo\n2  3  fat\n""]"
153;4.0;0;15360925;;1;55;<python><dataframe><pandas><series>;How to get the first column of a pandas DataFrame as a Series?;81214.0;['x=pandas.DataFrame(...)\ns = x.take([0], axis=1)\n', 's']
154;4.0;2;15411158;;1;87;<python><pandas><count><group-by><distinct>;Pandas count(distinct) equivalent;100056.0;"['YEARMONTH, CLIENTCODE, SIZE, .... etc etc\n', 'SELECT count(distinct CLIENTCODE) FROM table GROUP BY YEARMONTH;\n', '201301    5000\n201302    13245\n']"
155;1.0;1;15465645;;1;35;<python><matplotlib><group-by><pandas><data-analysis>;Plotting results of Pandas GroupBy;31249.0;['A=True', 'time=t', 't-5', 't', 'A=True']
156;4.0;0;15570099;;1;26;<python><pandas><pivot-table>;Pandas Pivot tables row subtotals;17499.0;"['Date       State   City    SalesToday  SalesMTD  SalesYTD\n20130320     stA    ctA            20       400      1000\n20130320     stA    ctB            30       500      1100\n20130320     stB    ctC            10       500       900\n20130320     stB    ctD            40       200      1300\n20130320     stC    ctF            30       300       800\n', 'State   City  SalesToday  SalesMTD  SalesYTD\n  stA    ALL          50       900      2100\n  stA    ctA          20       400      1000\n  stA    ctB          30       500      1100\n', ""table = pivot_table(df, values=['SalesToday', 'SalesMTD','SalesYTD'],\\\n                     rows=['State','City'], aggfunc=np.sum, margins=True)\n""]"
157;5.0;5;15705630;;1;40;<python><pandas>;Python : Getting the Row which has the max value in groups using groupby;46906.0;[' Sp  Mt Value  count\n0  MM1  S1   a      **3**\n1  MM1  S1   n      2\n2  MM1  S3   cb     5\n3  MM2  S3   mk      **8**\n4  MM2  S4   bg     **10**\n5  MM2  S4   dgd      1\n6  MM4  S2  rd     2\n7  MM4  S2   cb      2\n8  MM4  S2   uyi      **7**\n', '0  MM1  S1   a      **3**\n1 3  MM2  S3   mk      **8**\n4  MM2  S4   bg     **10** \n8  MM4  S2   uyi      **7**\n', '   Sp   Mt   Value  count\n4  MM2  S4   bg     10\n5  MM2  S4   dgd    1\n6  MM4  S2   rd     2\n7  MM4  S2   cb     8\n8  MM4  S2   uyi    8\n', 'MM2  S4   bg     10\nMM4  S2   cb     8\nMM4  S2   uyi    8\n']
158;3.0;0;15723628;;1;30;<python><pandas>;Pandas - make a column dtype object or Factor;21102.0;['as.factor()', 'pandas.Factor', 'pandas.Categorical']
159;3.0;0;15741759;;1;26;<python><pandas>;Find maximum value of a column and return the corresponding row values using Pandas;51057.0;"[""data.groupby(['Country','Place'])['Value'].max()\n""]"
160;7.0;3;15771472;;1;41;<python><pandas><time-series>;Pandas: rolling mean by time interval;47293.0;['polls_subset.tail(20)\nOut[185]: \n            favorable  unfavorable  other\n\nenddate                                  \n2012-10-25       0.48         0.49   0.03\n2012-10-25       0.51         0.48   0.02\n2012-10-27       0.51         0.47   0.02\n2012-10-26       0.56         0.40   0.04\n2012-10-28       0.48         0.49   0.04\n2012-10-28       0.46         0.46   0.09\n2012-10-28       0.48         0.49   0.03\n2012-10-28       0.49         0.48   0.03\n2012-10-30       0.53         0.45   0.02\n2012-11-01       0.49         0.49   0.03\n2012-11-01       0.47         0.47   0.05\n2012-11-01       0.51         0.45   0.04\n2012-11-03       0.49         0.45   0.06\n2012-11-04       0.53         0.39   0.00\n2012-11-04       0.47         0.44   0.08\n2012-11-04       0.49         0.48   0.03\n2012-11-04       0.52         0.46   0.01\n2012-11-04       0.50         0.47   0.03\n2012-11-05       0.51         0.46   0.02\n2012-11-07       0.51         0.41   0.00\n']
161;9.0;0;15772009;;1;45;<python><numpy><pandas>;shuffling/permutating a DataFrame in pandas;40329.0;"['shuffle(df, n, axis=0)', 'n', 'axis=0', 'axis=1', 'n', 'df.index', 'df', 'a', 'b', 'a', 'b', 'for 1...n:\n  for each col in df: shuffle column\nreturn new_df\n', ""def shuffle(df, n, axis=0):\n        shuffled_df = df.copy()\n        for k in range(n):\n            shuffled_df.apply(np.random.shuffle(shuffled_df.values),axis=axis)\n        return shuffled_df\n\ndf = pandas.DataFrame({'A':range(10), 'B':range(10)})\nshuffle(df, 5)\n""]"
162;1.0;0;15777951;;1;25;<pandas><suppress-warnings>;How to suppress Pandas Future warning ?;7622.0;"['D:\\Python\\lib\\site-packages\\pandas\\core\\frame.py:3581: FutureWarning: rename with inplace=True  will return None from pandas 0.11 onward\n  "" from pandas 0.11 onward"", FutureWarning) \n']"
163;1.0;0;15819050;;1;22;<python><pandas>;Pandas DataFrame concat vs append;33712.0;"[""data\n\n[<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 35228 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-03-28 18:59:20.357000+02:00\nData columns:\nPrice       4040  non-null values\nVolume      4040  non-null values\nBidQty      35228  non-null values\nBidPrice    35228  non-null values\nAskPrice    35228  non-null values\nAskQty      35228  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 33088 entries, 2013-04-01 00:03:17.047000+02:00 to 2013-04-01 18:59:58.175000+02:00\nData columns:\nPrice       3969  non-null values\nVolume      3969  non-null values\nBidQty      33088  non-null values\nBidPrice    33088  non-null values\nAskPrice    33088  non-null values\nAskQty      33088  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 50740 entries, 2013-04-02 00:03:27.470000+02:00 to 2013-04-02 18:59:58.172000+02:00\nData columns:\nPrice       7326  non-null values\nVolume      7326  non-null values\nBidQty      50740  non-null values\nBidPrice    50740  non-null values\nAskPrice    50740  non-null values\nAskQty      50740  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 60799 entries, 2013-04-03 00:03:06.994000+02:00 to 2013-04-03 18:59:58.180000+02:00\nData columns:\nPrice       8258  non-null values\nVolume      8258  non-null values\nBidQty      60799  non-null values\nBidPrice    60799  non-null values\nAskPrice    60799  non-null values\nAskQty      60799  non-null values\ndtypes: float64(6)]\n"", 'append', ""pd.DataFrame().append(data)\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 179855 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-04-03 18:59:58.180000+02:00\nData columns:\nAskPrice    179855  non-null values\nAskQty      179855  non-null values\nBidPrice    179855  non-null values\nBidQty      179855  non-null values\nPrice       23593  non-null values\nVolume      23593  non-null values\ndtypes: float64(6)\n"", 'concat', ""pd.concat(data)\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 179855 entries, 2013-03-27 22:00:07.089000+02:00 to 2013-04-03 16:59:58.180000+02:00\nData columns:\nPrice       23593  non-null values\nVolume      23593  non-null values\nBidQty      179855  non-null values\nBidPrice    179855  non-null values\nAskPrice    179855  non-null values\nAskQty      179855  non-null values\ndtypes: float64(6)\n"", 'concat', 'concat', 'append', 'concat']"
164;1.0;0;15854878;;1;28;<python><pandas>;Correlation between columns in DataFrame;45152.0;"['     a     b\n0  0.5  0.75\n1  0.5  0.75\n2  0.5  0.75\n3  0.5  0.75\n4  0.5  0.75\n', 'df.corr()', '    a   b\na NaN NaN\nb NaN NaN\n', 'np.correlate(df[""a""], df[""b""])', '1.875', 'corr()', 'NaN']"
165;2.0;0;15862034;;1;21;<python><pandas>;Access index of last element in data frame;31919.0;"[""df.ix[0]['date']\n"", 'datetime.datetime(2011, 1, 10, 16, 0)\n', ""df[-1:]['date']\n"", 'myIndex\n13         2011-12-20 16:00:00\nName: mydate\n', ""df.ix[df.tail(1)['IndexCopy']]['mydate']\n""]"
166;4.0;5;15891038;;1;257;<python><pandas><dataframe><types><casting>;Pandas: change data type of columns;406291.0;"[""a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\ndf = pd.DataFrame(a)\n""]"
167;5.0;0;15923826;;1;33;<python><pandas>;Random row selection in Pandas dataframe;26646.0;['some(x, n)', 'df.sample(n)']
168;7.0;3;15943769;;1;260;<python><pandas><dataframe>;How do I get the row count of a Pandas dataframe?;373069.0;"['total_rows = df.count\nprint total_rows +1\n', ""total_rows = df['First_columnn_label'].count\nprint total_rows +1\n"", 'len(df.index)\n']"
169;4.0;0;15998188;;1;80;<python><pandas><boolean-logic>;How can I obtain the element-wise logical NOT of a pandas Series?;30530.0;['Series', 'NOT', 'True\nTrue\nTrue\nFalse\n', 'False\nFalse\nFalse\nTrue\n']
170;2.0;0;16031056;;1;38;<python><dataframe><pandas><tuples>;How to form tuple column from two columns in Pandas;23199.0;"[""<class 'pandas.core.frame.DataFrame'>\nInt64Index: 205482 entries, 0 to 209018\nData columns:\nMonth           205482  non-null values\nReported by     205482  non-null values\nFalls within    205482  non-null values\nEasting         205482  non-null values\nNorthing        205482  non-null values\nLocation        205482  non-null values\nCrime type      205482  non-null values\nlong            205482  non-null values\nlat             205482  non-null values\ndtypes: float64(4), object(5)\n"", ""def merge_two_cols(series): \n    return (series['lat'], series['long'])\n\nsample['lat_long'] = sample.apply(merge_two_cols, axis=1)\n"", ""---------------------------------------------------------------------------\n AssertionError                            Traceback (most recent call last)\n<ipython-input-261-e752e52a96e6> in <module>()\n      2     return (series['lat'], series['long'])\n      3 \n----> 4 sample['lat_long'] = sample.apply(merge_two_cols, axis=1)\n      5\n"", 'AssertionError: Block shape incompatible with manager \n']"
171;3.0;1;16074392;;1;38;<python><matplotlib><pandas>;Getting vertical gridlines to appear in line plot in matplotlib;50935.0;"['pandas.DataFrame', ""data.plot()\ngrid('on')\n"", 'ax = plt.axes()        \nax.yaxis.grid() # horizontal lines\nax.xaxis.grid() # vertical lines\n']"
172;2.0;0;16088741;;1;22;<pandas><multi-index>;Pandas: add a column to a multiindex column dataframe;10377.0;"['In [151]: df\nOut[151]: \nfirst        bar                 baz           \nsecond       one       two       one       two \nA       0.487880 -0.487661 -1.030176  0.100813 \nB       0.267913  1.918923  0.132791  0.178503\nC       1.550526 -0.312235 -1.177689 -0.081596 \n', ""In [152]: df['bar']['three'] = [0, 1, 2]\n\nIn [153]: df\nOut[153]: \nfirst        bar                 baz           \nsecond       one       two       one       two \nA       0.487880 -0.487661 -1.030176  0.100813\nB       0.267913  1.918923  0.132791  0.178503\nC       1.550526 -0.312235 -1.177689 -0.081596\n""]"
173;5.0;1;16096627;;1;166;<pandas>;Pandas select row of data frame by integer index;227892.0;['df[2]', 'df.ix[2]', 'df[2:3]', 'In [26]: df.ix[2]\nOut[26]: \nA    1.027680\nB    1.514210\nC   -1.466963\nD   -0.162339\nName: 2000-01-03 00:00:00\n\nIn [27]: df[2:3]\nOut[27]: \n                  A        B         C         D\n2000-01-03  1.02768  1.51421 -1.466963 -0.162339\n', 'df[2]', 'df[2:3]']
174;3.0;0;16175874;;1;29;<python><dataframe><pandas>;python pandas dataframe slicing by date conditions;43773.0;"["">>> data\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 252 entries, 2010-12-31 00:00:00 to 2010-04-01 00:00:00\nData columns:\nAdj Close    252  non-null values\ndtypes: float64(1)\n\n>>> st = datetime.datetime(2010, 12, 31, 0, 0)\n>>> en = datetime.datetime(2010, 12, 28, 0, 0)\n\n>>> data[st:en]\n            Adj Close\nDate                 \n2010-12-31     593.97\n2010-12-30     598.86\n2010-12-29     601.00\n2010-12-28     598.92\n""]"
175;4.0;6;16176996;;1;33;<python><datetime><pandas>;Keep only date part when using pandas.to_datetime;31231.0;['pandas.to_datetime', 'datetime64[ns]', 'datetime.date', 'datetime64[D]', '00:00:00', '[dt.to_datetime().date() for dt in df.dates]\n', 'pandas.to_datetime', 'dtype', 'pandas.to_datetime']
176;5.0;4;16236684;;1;75;<merge><pandas><multiple-columns><return-type>;Apply pandas function to column to create multiple new columns?;33681.0;['extract_text_features', 'df.ix[: ,10:16] = df.textcol.map(extract_text_features)', 'df.iterrows()', 'df.iterrows()', '.map(lambda ...)']
177;8.0;1;16249736;;1;38;<python><mongodb><pandas><pymongo>;How to import data from mongodb to pandas?;24496.0;"['{\n""_cls"" : ""SensorReport"",\n""_id"" : ObjectId(""515a963b78f6a035d9fa531b""),\n""_types"" : [\n    ""SensorReport""\n],\n""Readings"" : [\n    {\n        ""a"" : 0.958069536790466,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:26:35.297Z""),\n        ""b"" : 6.296118156595,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.95574014778624,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:27:09.963Z""),\n        ""b"" : 6.29651468650064,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.953648289182713,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:27:37.545Z""),\n        ""b"" : 7.29679823731148,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.955931884300997,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:28:21.369Z""),\n        ""b"" : 6.29642922525632,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.95821381,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:41:20.801Z""),\n        ""b"" : 7.28956613,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 4.95821335,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:41:36.931Z""),\n        ""b"" : 6.28956574,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 9.95821341,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:42:09.971Z""),\n        ""b"" : 0.28956488,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 1.95667927,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:43:55.463Z""),\n        ""b"" : 0.29115237,\n        ""_cls"" : ""Reading""\n    }\n],\n""latestReportTime"" : ISODate(""2013-04-02T08:43:55.463Z""),\n""sensorName"" : ""56847890-0"",\n""reportCount"" : 8\n}\n']"
178;3.0;2;16266019;;1;24;<python><date><pandas>;Python Pandas: Group datetime column into hour and minute aggregations;20761.0;[]
179;5.0;1;16327055;;1;73;<python><pandas>;How to add an empty column to a dataframe?;65645.0;"['DataFrame', ""df['foo'] = df.apply(lambda _: '', axis=1)\n""]"
180;2.0;0;16353729;;1;84;<python><python-2.7><pandas><dataframe><apply>;Pandas: How to use apply function to multiple columns;136478.0;"[""df = DataFrame ({'a' : np.random.randn(6),\n             'b' : ['foo', 'bar'] * 3,\n             'c' : np.random.randn(6)})\n"", 'def my_test(a, b):\n    return a % b\n', ""df['Value'] = df.apply(lambda row: my_test(row[a], row[c]), axis=1)\n"", 'NameError: (""global name \'a\' is not defined"", u\'occurred at index 0\')\n', ""def my_test(a):\n    cum_diff = 0\n    for ix in df.index():\n        cum_diff = cum_diff + (a - df['a'][ix])\n    return cum_diff \n""]"
181;4.0;0;16392921;;1;47;<python><pandas><ipython><ipython-notebook>;Make more than one chart in same IPython Notebook cell;38376.0;"['ipython notebook --pylab inline\n', ""df['korisnika'].plot()\ndf['osiguranika'].plot()\n""]"
182;5.0;0;16396903;;1;58;<pandas>;Delete the first three rows of a dataframe in pandas;65041.0;['df.ix[:-1]']
183;3.0;8;16424493;;1;44;<python><formatting><pandas><ipython-notebook>;Pandas: Setting no. of max rows;28777.0;"['DataFrame', ""n = 100\nfoo = DataFrame(index=range(n))\nfoo['floats'] = np.random.randn(n)\nfoo\n"", ""pd.set_option('display.max_rows', 500)\n""]"
184;8.0;2;16476924;;1;379;<python><pandas><rows><dataframe>;How to iterate over rows in a DataFrame in Pandas?;384371.0;"[""import pandas as pd\ninp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]\ndf = pd.DataFrame(inp)\nprint df\n"", '   c1   c2\n0  10  100\n1  11  110\n2  12  120\n', ""for row in df.rows:\n   print row['c1'], row['c2']\n"", 'for date, row in df.T.iteritems():\n', 'for row in df.iterrows():\n', 'row']"
185;2.0;2;16522380;;1;52;<python><matplotlib><pandas>;Matplotlib plot is a no-show;30493.0;"[""import pandas as pd\nimport numpy as np\ndef add_prop(group):\n    births = group.births.astype(float)\n    group['prop'] = births/births.sum()\n    return group\n\npieces = []\ncolumns = ['name', 'sex', 'births']\n\nfor year in range(1880, 2012):\n    path = 'yob%d.txt' % year\n    frame = pd.read_csv(path, names = columns)\n    frame['year'] = year\n    pieces.append(frame)\n    names = pd.concat(pieces, ignore_index = True)\n\ntotal_births = names.pivot_table('births', rows = 'year', cols = 'sex', aggfunc = sum)\ntotal_births.plot(title = 'Total Births by sex and year')\n""]"
186;3.0;1;16597265;;1;84;<python><pandas>;Appending to an empty data frame in Pandas?;97800.0;"[""df = pd.DataFrame()\ndata = ['some kind of data here' --> I have checked the type already, and it is a dataframe]\ndf.append(data)\n"", 'Empty DataFrame\nColumns: []\nIndex: []\n']"
187;2.0;2;16628329;;1;39;<python><sqlite><pandas><hdf5>;HDF5 - concurrency, compression & I/O performance;17032.0;[]
188;4.0;4;16628819;;1;22;<python><pandas>;Convert pandas timezone-aware DateTimeIndex to naive timestamp, but in certain timezone;16527.0;"['tz_localize', 'In [82]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=10, freq=\'s\', tz=""Europe/Brussels"")\n\nIn [83]: t\nOut[83]: \n<class \'pandas.tseries.index.DatetimeIndex\'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: S, Timezone: Europe/Brussels\n', ""In [86]: t.tz = None\n\nIn [87]: t\nOut[87]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-05-18 10:00:00, ..., 2013-05-18 10:00:09]\nLength: 10, Freq: S, Timezone: None\n"", 'In [119]: d = pd.Timestamp(""2013-05-18 12:00:00"", tz=""Europe/Brussels"")\n\nIn [120]: d\nOut[120]: <Timestamp: 2013-05-18 12:00:00+0200 CEST, tz=Europe/Brussels>\n\nIn [121]: d.replace(tzinfo=None)\nOut[121]: <Timestamp: 2013-05-18 12:00:00> \n', ""In [124]: t\nOut[124]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: S, Timezone: Europe/Brussels\n\nIn [125]: pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])\nOut[125]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: None, Timezone: None\n""]"
189;1.0;2;16689514;;1;21;<python><pandas><dataframe>;how to get the average of dataframe column values;52385.0;['                    A        B\nDATE                 \n2013-05-01        473077    71333\n2013-05-02         35131    62441\n2013-05-03           727    27381\n2013-05-04           481     1206\n2013-05-05           226     1733\n2013-05-06           NaN     4064\n2013-05-07           NaN    41151\n2013-05-08           NaN     8144\n2013-05-09           NaN       23\n2013-05-10           NaN       10\n', 'df.sum(axis=1) / len(df.columns)\n']
190;4.0;0;16729483;;1;39;<python><pandas>;Converting strings to floats in a DataFrame;88771.0;['NaN']
191;5.0;0;16729574;;1;86;<python><pandas><dataframe>;How to get a value from a cell of a data frame?;161920.0;"[""d2 = df[(df['l_ext']==l_ext) & (df['item']==item) & (df['wn']==wn) & (df['wd']==1)]\n"", ""val = d2['col_name']\n""]"
192;2.0;6;16740887;;1;46;<python><pandas>;How to handle incoming real time data with python pandas;7890.0;"[""{'time' :'2013-01-01 00:00:00', 'stock' : 'BLAH',\n 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}\n"", ""In [1]: index = pd.DatetimeIndex(start='2013-01-01 00:00:00', freq='S', periods=5)\n\nIn [2]: columns = ['high', 'low', 'open', 'close']\n\nIn [3]: df = pd.DataFrame(index=t, columns=columns)\n\nIn [4]: df\nOut[4]: \n                    high  low open close\n2013-01-01 00:00:00  NaN  NaN  NaN   NaN\n2013-01-01 00:00:01  NaN  NaN  NaN   NaN\n2013-01-01 00:00:02  NaN  NaN  NaN   NaN\n2013-01-01 00:00:03  NaN  NaN  NaN   NaN\n2013-01-01 00:00:04  NaN  NaN  NaN   NaN\n\nIn [5]: data = {'time' :'2013-01-01 00:00:02', 'stock' : 'BLAH', 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}\n\nIn [6]: data_ = pd.Series(data)\n\nIn [7]: df.loc[data['time']] = data_\n\nIn [8]: df\nOut[8]: \n                    high  low open close\n2013-01-01 00:00:00  NaN  NaN  NaN   NaN\n2013-01-01 00:00:01  NaN  NaN  NaN   NaN\n2013-01-01 00:00:02    4    3    2     1\n2013-01-01 00:00:03  NaN  NaN  NaN   NaN\n2013-01-01 00:00:04  NaN  NaN  NaN   NaN\n"", ""In [9]: ls = []\n\nIn [10]: for n in range(5):\n   .....:     # Naive stuff ahead =)\n   .....:     time = '2013-01-01 00:00:0' + str(n)\n   .....:     d = {'time' : time, 'stock' : 'BLAH', 'high' : np.random.rand()*10, 'low' : np.random.rand()*10, 'open' : np.random.rand()*10, 'close' : np.random.rand()*10}\n   .....:     ls.append(d)\n\nIn [11]: df = pd.DataFrame(ls[1:3]).set_index('time')\n\nIn [12]: df\nOut[12]: \n                        close      high       low      open stock\ntime                                                             \n2013-01-01 00:00:01  3.270078  1.008289  7.486118  2.180683  BLAH\n2013-01-01 00:00:02  3.883586  2.215645  0.051799  2.310823  BLAH\n""]"
193;2.0;1;16777570;;1;22;<python><dataframe><pandas>;Calculate time difference between Pandas Dataframe indices;19835.0;['time                 value\n\n2012-03-16 23:50:00      1\n2012-03-16 23:56:00      2\n2012-03-17 00:08:00      3\n2012-03-17 00:10:00      4\n2012-03-17 00:12:00      5\n2012-03-17 00:20:00      6\n2012-03-20 00:43:00      7\n', 'time                 value  deltaT\n\n2012-03-16 23:50:00      1       0\n2012-03-16 23:56:00      2       6\n2012-03-17 00:08:00      3      12\n2012-03-17 00:10:00      4       2\n2012-03-17 00:12:00      5       2\n2012-03-17 00:20:00      6       8\n2012-03-20 00:43:00      7      23\n']
194;1.0;1;16782323;;1;33;<python><pandas>;Python pandas: Keep selected column as DataFrame instead of Series;15889.0;"['df.iloc[:, 0]', ""df['A']"", 'df.A', 'pd.DataFrame(df.iloc[:, 0])']"
195;4.0;0;16852911;;1;42;<python><date><pandas>;How do I convert dates in a Pandas data frame to a 'date' data type?;54111.0;[]
196;1.0;1;16888888;;1;28;<python><pandas><ipython><ipython-notebook><dataframe>;How to read a .xlsx file using the pandas Library in iPython?;25755.0;"['import pandas as pd\ndata = pd.ExcelFile(""*File Name*"")\n']"
197;5.0;0;16923281;;1;214;<python><csv><pandas><dataframe>;Pandas writing dataframe to CSV file;296632.0;"[""df.to_csv('out.csv')\n"", ""UnicodeEncodeError: 'ascii' codec can't encode character u'\\u03b1' in position 20: ordinal not in range(128)\n""]"
198;3.0;0;16947336;;1;31;<python><numpy><pandas>;binning a dataframe in pandas in Python;34913.0;"['import numpy as np\ndf = pandas.DataFrame({""a"": np.random.random(100), ""b"": np.random.random(100), ""id"": np.arange(100)})\n', 'id', 'a', 'b', 'a', 'b', 'a', 'b', 'df', 'NaN', 'a', 'b', 'df', 'a = np.random.random(20)\ndf = pandas.DataFrame({""a"": a, ""b"": a + 10})\n# bins for df.a\nbins = np.linspace(0, 1, 10)\n# bin df according to a\ngroups = df.groupby(np.digitize(df.a,bins))\n# Get the mean of a in each group\nprint groups.mean()\n## But how to get the mean of b for each group of a?\n# ...\n']"
199;3.0;2;17001389;;1;113;<python><documentation><pandas>;pandas resample documentation;59119.0;"['resample', ""'D'"", ""'xMin'"", ""'xL'"", ""'first'"", 'np.max', ""'last'"", ""'mean'"", ""'n1n2n3n4...nx'"", 'pandas.resample']"
200;4.0;0;17063458;;1;56;<python><python-2.7><pandas>;Reading an Excel file in python using pandas;114839.0;['newFile = pd.ExcelFile(PATH\\FileName.xlsx)\nParsedData = pd.io.parsers.ExcelFile.parse(newFile)\n']
201;9.0;0;17071871;;1;375;<python><pandas><dataframe>;Select rows from a DataFrame based on values in a column in pandas;420943.0;['select * from table where colume_name = some_value. \n']
202;4.0;0;17091769;;1;54;<python><dataframe><row><pandas>;Python pandas: fill a dataframe row by row;53561.0;"['pandas.DataFrame', "">>> df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'])\n>>> df\n     a    b    c    d\nx  NaN  NaN  NaN  NaN\ny  NaN  NaN  NaN  NaN\nz  NaN  NaN  NaN  NaN\n"", 'pandas.Series', "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df['y'] = y\nAssertionError: Length of values does not match length of index\n"", "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.join(y)\nAttributeError: 'builtin_function_or_method' object has no attribute 'is_unique'\n"", "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.set_value(index='y', value=y)\nTypeError: set_value() takes exactly 4 arguments (3 given)\n"", "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.append(y)\nException: Can only append a Series if ignore_index=True\n"", '>>> df.append(y, ignore_index=True)\n     a    b    c    d\n0  NaN  NaN  NaN  NaN\n1  NaN  NaN  NaN  NaN\n2  NaN  NaN  NaN  NaN\n3    1    5    2    3\n', "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.ix['y'] = y\n>>> df\n                                  a                                 b  \\\nx                               NaN                               NaN\ny  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}\nz                               NaN                               NaN\n\n                                  c                                 d\nx                               NaN                               NaN\ny  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}\nz                               NaN                               NaN\n""]"
203;8.0;1;17095101;;1;48;<python><html><dataframe><pandas>;Outputting difference in two Pandas dataframes side by side - highlighting the difference;51910.0;"['""StudentRoster Jan-1"":\nid   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.11                     False                Graduated\n113  Zoe    4.12                     True       \n\n""StudentRoster Jan-2"":\nid   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.21                     False                Graduated\n113  Zoe    4.12                     False                On vacation\n', '""StudentRoster Difference Jan-1 - Jan-2"":  \nid   Name   score                    isEnrolled           Comment\n112  Nick   was 1.11| now 1.21       False                Graduated\n113  Zoe    4.12                     was True | now False was """" | now   ""On   vacation""\n']"
204;2.0;5;17097236;;1;29;<python><replace><pandas><nan><nonetype>;How to replace values with None in Pandas data frame in Python?;53211.0;"['None', ""df.replace('pre', 'post')"", 'None', ""df = DataFrame(['-',3,2,5,1,-5,-1,'-',9])\ndf.replace('-', 0)\n"", ""df.replace('-', None)\n"", ""0\n0   - // this isn't replaced\n1   3\n2   2\n3   5\n4   1\n5  -5\n6  -1\n7  -1 // this is changed to `-1`...\n8   9\n"", 'NaN', 'None', ""'-'"", 'NaN', 'NaN', 'None']"
205;3.0;0;17097643;;1;23;<python><pandas><contains>;"search for ""does-not-contain"" on a dataframe in pandas";13033.0;"['df[""col""].str.contains(word)', '!(df[""col""].str.contains(word))', 'DataFrame']"
206;5.0;0;17098654;;1;117;<python><pandas><dataframe>;How to store a dataframe using Pandas;78448.0;['CSV']
207;3.0;0;17116814;;1;95;<python><pandas><dataframe>;pandas: How do I split text in a column into multiple rows?;70528.0;"['CustNum  CustomerName     ItemQty  Item   Seatblocks                 ItemExt\n32363    McCartney, Paul      3     F04    2:218:10:4,6                   60\n31316    Lennon, John        25     F01    1:13:36:1,12 1:13:37:1,13     300\n', ""(' ')"", ""(':')"", 'Seatblocks', 'Seatblocks', 'text-to-columns']"
208;2.0;0;17134716;;1;92;<python><pandas><dataframe>;Convert DataFrame column type from string to datetime;85223.0;[]
209;2.0;1;17141558;;1;65;<python><python-2.7><pandas><data-analysis>;How to sort a dataFrame in python pandas by two or more columns?;73105.0;[]
210;2.0;0;17142304;;1;33;<python><replace><dataframe><pandas>;replace string/value in entire dataframe;35525.0;['data\n   resp          A          B          C\n0     1       poor       poor       good\n1     2       good       poor       good\n2     3  very good  very good  very good\n3     4       bad        poor       bad \n4     5   very bad   very bad   very bad\n5     6       poor       good   very bad\n6     7       good       good       good\n7     8  very good  very good  very good\n8     9       bad        bad    very bad\n9    10   very bad   very bad   very bad\n', ' data\n   resp  A  B  C\n0      1  3  3  4\n1     2  4  3  4\n2     3  5  5  5\n3     4  2  3  2\n4     5  1  1  1\n5     6  3  4  1\n6     7  4  4  4\n7     8  5  5  5\n8     9  2  2  1\n9    10  1  1  1\n']
211;4.0;0;17241004;;1;105;<python><pandas>;Pandas - how to get the data frame index as an array;158874.0;[]
212;4.0;4;17326973;;1;21;<python><excel><pandas><openpyxl>;Is there a way to auto-adjust Excel column widths with pandas.ExcelWriter?;11512.0;"['writer = pd.ExcelWriter(excel_file_path)\ndf.to_excel(writer, sheet_name=""Summary"")\n']"
213;4.0;1;17383094;;1;30;<python><numpy><pandas>;python pandas/numpy True/False to 1/0 mapping;27133.0;['dtype=object', 'np_values    = np.array(df.values, dtype = np.float64)']
214;2.0;1;17438906;;1;30;<python><pandas>;Combining rows in pandas;26878.0;['city_id', '[city],[state]', 'new york,ny', 'city_id', 'groupby()', 'city_id    val1 val2 val3\nhouston,tx    1    2    0\nhouston,tx    0    0    1\nhouston,tx    2    1    1\n', 'city_id    val1 val2 val3\nhouston,tx    3    3    2\n']
215;4.0;0;17465045;;1;40;<python><date><types><dataframe><pandas>;Can pandas automatically recognize dates?;49798.0;"['df = pandas.read_csv(\'test.dat\', delimiter=r""\\s+"", names=[\'col1\',\'col2\',\'col3\'])\n', ""for i, r in df.iterrows():\n    print type(r['col1']), type(r['col2']), type(r['col3'])\n"", '2013-6-4']"
216;4.0;0;17477979;;1;60;<python><numpy><scipy><pandas>;dropping infinite values from dataframes in pandas?;45786.0;"['mode.use_inf_as_null', 'subset', 'how', 'dropna', 'inf', 'df.dropna(subset=[""col1"", ""col2""], how=""all"", with_inf=True)\n', 'dropna', 'inf']"
217;4.0;1;17530542;;1;55;<csv><pandas>;How to add pandas data to an existing csv file?;48770.0;['to_csv()']
218;4.0;3;17534106;;1;43;<python><numpy><pandas><nan>;What is the difference between NaN and None?;23404.0;['readcsv()', 'None', 'nan', 'None', 'nan', 'None', 'nan', 'nan', 'None', 'numpy.isnan()', 'for k, v in my_dict.iteritems():\n    if np.isnan(v):\n', 'v', 'v', 'nan']
219;5.0;17;17557074;;1;61;<python><windows><pandas>;Memory error when using pandas read_csv;17699.0;"['data = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2)\n', 'MemoryError', 'Traceback (most recent call last):\n  File ""F:\\QA ALM\\Python\\new WIM data\\new WIM data\\new_WIM_data.py"", line 25, in\n <module>\n    wimdata = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2\n)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 401, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 216, in _read\n    return parser.read()\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 643, in read\n    df = DataFrame(col_dict, columns=columns, index=index)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 394, in __init__\n    mgr = self._init_dict(data, index, columns, dtype=dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 525, in _init_dict\n    dtype=dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 5338, in _arrays_to_mgr\n    return create_block_manager_from_arrays(arrays, arr_names, axes)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1820, in create_block_manager_from_arrays\n    blocks = form_blocks(arrays, names, axes)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1872, in form_blocks\n    float_blocks = _multi_blockify(float_items, items)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1930, in _multi_blockify\n    block_items, values = _stack_arrays(list(tup_block), ref_items, dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1962, in _stack_arrays\n    stacked = np.empty(shape, dtype=dtype)\nMemoryError\nPress any key to continue . . .\n', 'data <- read.table(paste(INPUTDIR,config[i,]$TOEXTRACT,sep=""""), HASHEADER, DELIMITER,skip=2,fill=TRUE)\n']"
220;6.0;1;17618981;;1;35;<python><sorting><dataframe><pandas>;How to sort pandas data frame using values from several columns?;82354.0;"[""df = pandas.DataFrame([{'c1':3,'c2':10},{'c1':2, 'c2':30},{'c1':1,'c2':20},{'c1':2,'c2':15},{'c1':2,'c2':100}])\n"", '   c1   c2\n0   3   10\n1   2   30\n2   1   20\n3   2   15\n4   2  100\n', ""df.sort(['c1','c2'], ascending=False)\n"", '   c1   c2\n0   3   10\n4   2  100\n1   2   30\n3   2   15\n2   1   20\n', ""df.sort(['c1','c2'], ascending=[False,True])\n"", '   c1   c2\n2   1   20\n3   2   15\n1   2   30\n4   2  100\n0   3   10\n', "">>> df.sort(['c1','c2'], ascending=[False,True])\n   c1   c2\n2   1   20\n3   2   15\n1   2   30\n4   2  100\n0   3   10\n""]"
221;5.0;3;17627219;;1;29;<python><numpy><pandas><similarity><cosine-similarity>;What's the fastest way in Python to calculate cosine similarity given sparse matrix data?;33178.0;"['A= \n[0 1 0 0 1\n 0 0 1 1 1\n 1 1 0 1 0]\n', 'A = \n0, 1\n0, 4\n1, 2\n1, 3\n1, 4\n2, 0\n2, 1\n2, 3\n', 'import numpy as np\nfrom sklearn.metrics import pairwise_distances\nfrom scipy.spatial.distance import cosine\n\nA = np.array(\n[[0, 1, 0, 0, 1],\n[0, 0, 1, 1, 1],\n[1, 1, 0, 1, 0]])\n\ndist_out = 1-pairwise_distances(A, metric=""cosine"")\ndist_out\n', 'array([[ 1.        ,  0.40824829,  0.40824829],\n       [ 0.40824829,  1.        ,  0.33333333],\n       [ 0.40824829,  0.33333333,  1.        ]])\n']"
222;5.0;1;17679089;;1;41;<python><pandas><dataframe>;Pandas DataFrame Groupby two columns and get counts;65459.0;"[""df = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T\ndf.columns = ['col1','col2','col3','col4','col5']\n"", '   col1 col2 col3     col4 col5\n0   1.1    A  1.1    x/y/z    1\n1   1.1    A  1.7      x/y    3\n2   1.1    A  2.5  x/y/z/n    3\n3   2.6    B  2.6      x/u    2\n4   2.5    B  3.3        x    4\n5   3.4    B  3.8    x/u/v    2\n6   2.6    B    4    x/y/z    5\n7   2.6    A  4.2        x    3\n8   3.4    B  4.3  x/u/v/b    6\n9   3.4    C  4.5        -    3\n10  2.6    B  4.6      x/y    5\n11  1.1    D  4.7    x/y/z    1\n12  1.1    D  4.7        x    1\n13  3.3    D  4.8  x/u/v/w    1\n', ""df.groupby(['col5','col2']).reset_index()\n"", '             index col1 col2 col3     col4 col5\ncol5 col2                                      \n1    A    0      0  1.1    A  1.1    x/y/z    1\n     D    0     11  1.1    D  4.7    x/y/z    1\n          1     12  1.1    D  4.7        x    1\n          2     13  3.3    D  4.8  x/u/v/w    1\n2    B    0      3  2.6    B  2.6      x/u    2\n          1      5  3.4    B  3.8    x/u/v    2\n3    A    0      1  1.1    A  1.7      x/y    3\n          1      2  1.1    A  2.5  x/y/z/n    3\n          2      7  2.6    A  4.2        x    3\n     C    0      9  3.4    C  4.5        -    3\n4    B    0      4  2.5    B  3.3        x    4\n5    B    0      6  2.6    B    4    x/y/z    5\n          1     10  2.6    B  4.6      x/y    5\n6    B    0      8  3.4    B  4.3  x/u/v/b    6\n', 'col5 col2 count\n1    A      1\n     D      3\n2    B      2\netc...\n']"
223;3.0;0;17682613;;1;41;<python><arrays><numpy><pandas><scikit-learn>;How to convert a pandas DataFrame subset of columns AND rows into a numpy array?;99966.0;[]
224;2.0;0;17690738;;1;41;<python><datetime><pandas>;In Pandas how do I convert a string of date strings to datetime objects and put them in a DataFrame?;50026.0;"[""import pandas as pd\ndate_stngs = ('2008-12-20','2008-12-21','2008-12-22','2008-12-23')\n\na = pd.Series(range(4),index = (range(4)))\n\nfor idx, date in enumerate(date_stngs):\n    a[idx]= pd.to_datetime(date)\n"", 'DateTime']"
225;2.0;0;17691447;;1;22;<python><pandas><dataframe>;Get count of values across columns-Pandas DataFrame;21387.0;['               A              B              C\n0   192.168.2.85   192.168.2.85  124.43.113.22\n1  192.248.8.183  192.248.8.183   192.168.2.85\n2  192.168.2.161            NaN  192.248.8.183\n3   66.249.74.52            NaN  192.168.2.161\n4            NaN            NaN   66.249.74.52\n', 'IP          Count\n192.168.2.85 3 #Since this value is there in all coulmns\n192.248.8.183 3\n192.168.2.161 2\n66.249.74.52 2\n124.43.113.22 1\n']
226;10.0;7;17709641;;1;79;<python><numpy><install><pandas><statsmodels>;ValueError: numpy.dtype has the wrong size, try recompiling;64914.0;"['numpy.dtype has the wrong size, try recompiling\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\formula\\__init__.py"",\nline 4, in <module>\n    from formulatools import handle_formula_data\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\formula\\formulatools.p\ny"", line 1, in <module>\n    import statsmodels.tools.data as data_util\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\tools\\__init__.py"", li\nne 1, in <module>\n    from tools import add_constant, categorical\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\tools\\tools.py"", line\n14, in <module>\n    from pandas import DataFrame\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\pandas\\__init__.py"", line 6, in <module>\n    from . import hashtable, tslib, lib\n  File ""numpy.pxd"", line 157, in init pandas.tslib (pandas\\tslib.c:49133)\nValueError: numpy.dtype has the wrong size, try recompiling\n']"
227;2.0;0;17812978;;1;30;<python><matplotlib><plot><pandas><dataframe>;How to plot two columns of a pandas data frame using points?;74379.0;"['plot', ""df.plot(x='col_name_1', y='col_name_2')\n"", 'kind']"
228;3.0;1;17818783;;1;25;<python><numpy><scipy><pandas><sparse-matrix>;Populate a Pandas SparseDataFrame from a SciPy Sparse Matrix;9661.0;['DataFrame()', 'return DataFrame(matrix.toarray(), columns=features, index=observations)\n', 'SparseDataFrame()', 'scipy.sparse.csc_matrix()', 'csr_matrix()']
229;6.0;0;17839973;;1;64;<python><pandas><dataframe>;construct pandas DataFrame from values in variables;53479.0;"['a = 2\nb = 3\n', ""df2 = pd.DataFrame({'A':a,'B':b})\n"", ""df2 = (pd.DataFrame({'a':a,'b':b})).reset_index()\n""]"
230;4.0;0;17841149;;1;37;<python><pandas>;Pandas groupby: How to get a union of strings;24290.0;"['   A         B       C\n0  1  0.749065    This\n1  2  0.301084      is\n2  3  0.463468       a\n3  4  0.643961  random\n4  1  0.866521  string\n5  2  0.120737       !\n', 'In [10]: print df.groupby(""A"")[""B""].sum()\n', 'A\n1    1.615586\n2    0.421821\n3    0.463468\n4    0.643961\n', 'A\n1    {This, string}\n2    {is, !}\n3    {a}\n4    {random}\n', 'df.groupby(""A"")[""B""]\n', 'pandas.core.groupby.SeriesGroupBy object\n']"
231;4.0;3;17874063;;1;22;<python><pandas><matplotlib>;Is there a parameter in matplotlib/pandas to have the Y axis of a histogram as percentage?;15779.0;[]
232;3.0;1;17950374;;1;38;<string><int><pandas>;Converting a column within pandas dataframe from int to string;71902.0;"['int', 'str', ""mtrx['X.3'] = mtrx.to_string(columns = ['X.3'])\n"", ""mtrx['X.3'] = mtrx['X.3'].astype(str)\n"", 'str']"
233;2.0;0;17972938;;1;21;<python><python-2.7><pandas>;check if string in pandas dataframe column is in list;27027.0;"[""frame = pd.DataFrame({'a' : ['the cat is blue', 'the sky is green', 'the dog is black']})\n"", 'frame[\'b\'] = frame.a.str.contains(""dog"") | frame.a.str.contains(""cat"") | frame.a.str.contains(""fish"")\n', ""frame['b']"", 'True\nFalse\nTrue\n', ""mylist =['dog', 'cat', 'fish']\n""]"
234;2.0;0;17977540;;1;32;<python><pandas>;Pandas: Looking up the list of sheets in an excel file;16531.0;"[""read_excel('path_to_file.xls', 'Sheet1', index_col=None, na_values=['NA'])\n"", 'N']"
235;4.0;1;17978092;;1;30;<python><pandas>;Combine Date and Time columns using python pandas;16717.0;['Date              Time\n01-06-2013      23:00:00\n02-06-2013      01:00:00\n02-06-2013      21:00:00\n02-06-2013      22:00:00\n02-06-2013      23:00:00\n03-06-2013      01:00:00\n03-06-2013      21:00:00\n03-06-2013      22:00:00\n03-06-2013      23:00:00\n04-06-2013      01:00:00\n', 'pd.to_datetime', 'Date\n01-06-2013 23:00:00\n02-06-2013 01:00:00\n02-06-2013 21:00:00\n02-06-2013 22:00:00\n02-06-2013 23:00:00\n03-06-2013 01:00:00\n03-06-2013 21:00:00\n03-06-2013 22:00:00\n03-06-2013 23:00:00\n04-06-2013 01:00:00\n']
236;4.0;1;17978133;;1;24;<python><merge><pandas>;Python Pandas merge only certain columns;15297.0;[]
237;6.0;0;18022845;;1;100;<python><pandas><dataframe><columnname>;Pandas index column title or name;114634.0;"['             Column 1\nIndex Title          \nApples              1\nOranges             2\nPuppies             3\nDucks               4  \n', 'import pandas as pd\ndata = {\'Column 1\'     : [1., 2., 3., 4.],\n        \'Index Title\'  : [""Apples"", ""Oranges"", ""Puppies"", ""Ducks""]}\ndf = pd.DataFrame(data)\ndf.index = df[""Index Title""]\ndel df[""Index Title""]\nprint df\n']"
238;11.0;1;18039057;;1;84;<python><csv><pandas>;Python Pandas Error tokenizing data;84465.0;"['pandas.parser.CParserError: Error tokenizing data. C error: Expected 2 fields in line 3,  saw 12\n', ""path = 'GOOG Key Ratios.csv'\n#print(open(path).read())\ndata = pd.read_csv(path)\n"", 'csv']"
239;5.0;0;18062135;;1;113;<python><pandas><series><dataframe>;Combining two Series into a DataFrame in pandas;90070.0;['s1', 's2', 's1', 's2']
240;5.0;0;18079563;;1;22;<python><pandas><series>;Finding the intersection between two series in Pandas;23715.0;[]
241;6.0;4;18089667;;1;42;<python><pandas>;How to estimate how much memory a Pandas' DataFrame will need?;17587.0;[]
242;1.0;0;18171739;;1;82;<python><csv><pandas><unicode>;UnicodeDecodeError when reading CSV file in Pandas with Python;46551.0;"['   File ""C:\\Importer\\src\\dfman\\importer.py"", line 26, in import_chr\n     data = pd.read_csv(filepath, names=fields)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 400, in parser_f\n     return _read(filepath_or_buffer, kwds)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 205, in _read\n     return parser.read()\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 608, in read\n     ret = self._engine.read(nrows)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 1028, in read\n     data = self._reader.read(nrows)\n   File ""parser.pyx"", line 706, in pandas.parser.TextReader.read (pandas\\parser.c:6745)\n   File ""parser.pyx"", line 728, in pandas.parser.TextReader._read_low_memory (pandas\\parser.c:6964)\n   File ""parser.pyx"", line 804, in pandas.parser.TextReader._read_rows (pandas\\parser.c:7780)\n   File ""parser.pyx"", line 890, in pandas.parser.TextReader._convert_column_data (pandas\\parser.c:8793)\n   File ""parser.pyx"", line 950, in pandas.parser.TextReader._convert_tokens (pandas\\parser.c:9484)\n   File ""parser.pyx"", line 1026, in pandas.parser.TextReader._convert_with_dtype (pandas\\parser.c:10642)\n   File ""parser.pyx"", line 1046, in pandas.parser.TextReader._string_convert (pandas\\parser.c:10853)\n   File ""parser.pyx"", line 1278, in pandas.parser._string_box_utf8 (pandas\\parser.c:15657)\n UnicodeDecodeError: \'utf-8\' codec can\'t decode byte 0xda in position 6: invalid    continuation byte\n']"
243;4.0;1;18172851;;1;194;<python><pandas>;Deleting DataFrame row in Pandas based on column value;264894.0;['             daysago  line_race rating        rw    wrating\n line_date                                                 \n 2007-03-31       62         11     56  1.000000  56.000000\n 2007-03-10       83         11     67  1.000000  67.000000\n 2007-02-10      111          9     66  1.000000  66.000000\n 2007-01-13      139         10     83  0.880678  73.096278\n 2006-12-23      160         10     88  0.793033  69.786942\n 2006-11-09      204          9     52  0.636655  33.106077\n 2006-10-22      222          8     66  0.581946  38.408408\n 2006-09-29      245          9     70  0.518825  36.317752\n 2006-09-16      258         11     68  0.486226  33.063381\n 2006-08-30      275          8     72  0.446667  32.160051\n 2006-02-11      475          5     65  0.164591  10.698423\n 2006-01-13      504          0     70  0.142409   9.968634\n 2006-01-02      515          0     64  0.134800   8.627219\n 2005-12-06      542          0     70  0.117803   8.246238\n 2005-11-29      549          0     70  0.113758   7.963072\n 2005-11-22      556          0     -1  0.109852  -0.109852\n 2005-11-01      577          0     -1  0.098919  -0.098919\n 2005-10-20      589          0     -1  0.093168  -0.093168\n 2005-09-27      612          0     -1  0.083063  -0.083063\n 2005-09-07      632          0     -1  0.075171  -0.075171\n 2005-06-12      719          0     69  0.048690   3.359623\n 2005-05-29      733          0     -1  0.045404  -0.045404\n 2005-05-02      760          0     -1  0.039679  -0.039679\n 2005-04-02      790          0     -1  0.034160  -0.034160\n 2005-03-13      810          0     -1  0.030915  -0.030915\n 2004-11-09      934          0     -1  0.016647  -0.016647\n', 'line_race', '0']
244;1.0;0;18196203;;1;22;<python><pandas>;How to conditionally update DataFrame column in Pandas;20823.0;['rating', 'line_race', '    line_track  line_race  rating foreign\n 25        MTH         10     84    False\n 26        MTH          6     88    False\n 27        TAM          5     87    False\n 28         GP          2     86    False\n 29         GP          7     59    False\n 30        LCH          0    103     True\n 31        LEO          0    125     True\n 32        YOR          0    126     True\n 33        ASC          0    124     True\n']
245;3.0;1;18215317;;1;36;<python><numpy><pandas>;extracting days from a numpy.timedelta64 value;32906.0;['0    385 days, 04:10:36\n1     57 days, 22:54:00\n2    642 days, 21:15:23\n3    615 days, 00:55:44\n4    160 days, 22:13:35\n5    196 days, 23:06:49\n6     23 days, 22:57:17\n7      2 days, 22:17:31\n8    622 days, 01:29:25\n9     79 days, 20:15:14\n10    23 days, 22:46:51\n11   268 days, 19:23:04\n12                  NaT\n13                  NaT\n14   583 days, 03:40:39\n']
246;4.0;1;18316211;;1;33;<python><pandas>;Access index in pandas.Series.apply;16141.0;['s', '>>> s\n     values\na b\n1 2  0.1 \n3 6  0.3\n4 4  0.7\n', 'def f(x):\n   # conditions or computations using the indexes\n   if x.index[0] and ...: \n   other = sum(x.index) + ...\n   return something\n', 's.apply(f)']
247;6.0;0;18327624;;1;43;<python><pandas>;Find element's index in pandas Series;81639.0;['import pandas as pd\nmyseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])\nprint myseries.find(7) # should output 3\n', 'def find(s, el):\n    for i in s.index:\n        if s[i] == el: \n            return i\n    return None\n\nprint find(myseries, 7)\n']
248;2.0;4;18358938;;1;25;<python><list><pandas><indexing>;Get index values of Pandas DataFrame as list?;35190.0;"["" list = list(df['column']) \n"", 'set_index']"
249;6.0;4;18429491;;1;40;<pandas><grouping><nan>;groupby columns with NaN (missing) values;20547.0;"[""import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'a': ['1', '2', '3'], 'b': ['4', np.NaN, '6']})\n\nIn [4]: df.groupby('b').groups\nOut[4]: {'4': [0], '6': [2]}\n""]"
250;2.0;0;18504967;;1;21;<python><pandas><calculated-columns>;pandas dataframe create new columns and fill with calculated values from same df;37638.0;"[""ds = pd.DataFrame(np.abs(randn(3, 4)), index=[1,2,3], columns=['A','B','C','D'])\nds\n      A         B         C         D\n1  1.099679  0.042043  0.083903  0.410128\n2  0.268205  0.718933  1.459374  0.758887\n3  0.680566  0.538655  0.038236  1.169403\n"", ""ds['sum']=ds.sum(axis=1)\nds\n      A         B         C         D       sum\n1  0.095389  0.556978  1.646888  1.959295  4.258550\n2  1.076190  2.668270  0.825116  1.477040  6.046616\n3  0.245034  1.066285  0.967124  0.791606  3.070049\n""]"
251;2.0;0;18554920;;1;39;<python><pandas>;Pandas aggregate count distinct;33368.0;"[""import numpy as np\nimport pandas as pd\ndf = pd.DataFrame({'date': ['2013-04-01','2013-04-01','2013-04-01','2013-04-02', '2013-04-02'],\n    'user_id': ['0001', '0001', '0002', '0002', '0002'],\n    'duration': [30, 15, 20, 15, 30]})\n"", ""group = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg\n            duration\ndate\n2013-04-01        65\n2013-04-02        45\n"", ""agg = group.aggregate({ 'duration': np.sum, 'user_id': count_distinct})\n"", ""group = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg['uv'] = df.groupby('date').user_id.nunique()\nagg\n            duration  uv\ndate\n2013-04-01        65   2\n2013-04-02        45   1\n""]"
252;2.0;0;18594469;;1;27;<python><pandas><normalization><dataframe>;Normalizing a pandas DataFrame by row;11082.0;['(df.T / df.T.sum()).T\n', 'df / df.sum(axis=1)']
253;2.0;0;18674064;;1;61;<python><indexing><pandas>;how do I insert a column at a specific column index in pandas?;47504.0;"[""import pandas as pd\ndf = pd.DataFrame({'l':['a','b','c','d'], 'v':[1,2,1,2]})\ndf['n'] = 0\n"", 'n', 'df', 'df', 'n']"
254;2.0;2;18689512;;1;55;<python><numpy><pandas>;Efficiently checking if arbitrary object is NaN in Python / numpy / pandas?;68431.0;"['np.nan', 'numpy.isnan(val)', 'val', 'numpy.isnan()', '>>> np.isnan(\'some_string\')\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nTypeError: Not implemented for this type\n', 'False']"
255;6.0;0;18689823;;1;29;<python><pandas><nan>;pandas DataFrame: replace nan values with average of columns;36285.0;['nan', 'nan']
256;4.0;4;18695605;;1;42;<python><dictionary><pandas>;python pandas dataframe to dictionary;63301.0;['    id    value\n0    0     10.2\n1    1      5.7\n2    2      7.4\n']
257;3.0;0;18792918;;1;28;<python><pandas><left-join><dataframe>;Pandas Combining 2 Data Frames (join on a common column);63764.0;"['Data columns (total 13 columns):\nbusiness_id      4503  non-null values\ncategories       4503  non-null values\ncity             4503  non-null values\nfull_address     4503  non-null values\nlatitude         4503  non-null values\nlongitude        4503  non-null values\nname             4503  non-null values\nneighborhoods    4503  non-null values\nopen             4503  non-null values\nreview_count     4503  non-null values\nstars            4503  non-null values\nstate            4503  non-null values\ntype             4503  non-null values\ndtypes: bool(1), float64(3), int64(1), object(8)`\n', 'Int64Index: 158430 entries, 0 to 229905\nData columns (total 8 columns):\nbusiness_id    158430  non-null values\ndate           158430  non-null values\nreview_id      158430  non-null values\nstars          158430  non-null values\ntext           158430  non-null values\ntype           158430  non-null values\nuser_id        158430  non-null values\nvotes          158430  non-null values\ndtypes: int64(1), object(7)\n', ""#the following line of code creates a left join of restaurant_ids_frame and   restaurant_review_frame on the column 'business_id'\nrestaurant_review_frame.join(other=restaurant_ids_dataframe,on='business_id',how='left')\n"", 'Exception: columns overlap: Index([business_id, stars, type], dtype=object)\n']"
258;2.0;2;18835077;;1;34;<python><pandas>;selecting from multi-index pandas;34779.0;['# has multi-index (A,B)\ndf\n#can i do this? I know this doesnt work because index is multi-index so I need to     specify a tuple\n\ndf.ix[df.A ==1]\n']
259;8.0;4;18837262;;1;78;<python><pandas><dataframe>;Convert Python dict into a dataframe;151737.0;"[""{u'2012-06-08': 388,\n u'2012-06-09': 388,\n u'2012-06-10': 388,\n u'2012-06-11': 389,\n u'2012-06-12': 389,\n u'2012-06-13': 389,\n u'2012-06-14': 389,\n u'2012-06-15': 389,\n u'2012-06-16': 389,\n u'2012-06-17': 389,\n u'2012-06-18': 390,\n u'2012-06-19': 390,\n u'2012-06-20': 390,\n u'2012-06-21': 390,\n u'2012-06-22': 390,\n u'2012-06-23': 390,\n u'2012-06-24': 390,\n u'2012-06-25': 391,\n u'2012-06-26': 391,\n u'2012-06-27': 391,\n u'2012-06-28': 391,\n u'2012-06-29': 391,\n u'2012-06-30': 391,\n u'2012-07-01': 391,\n u'2012-07-02': 392,\n u'2012-07-03': 392,\n u'2012-07-04': 392,\n u'2012-07-05': 392,\n u'2012-07-06': 392}\n"", '     Date         DateValue\n0    2012-07-01    391\n1    2012-07-02    392\n2    2012-07-03    392\n.    2012-07-04    392\n.    ...           ...\n.    ...           ...\n', 's  = Series(my_dict,index=my_dict.keys())\n']"
260;3.0;1;18876022;;1;25;<python><html><pandas><ipython>;How to format IPython html display of Pandas dataframe?;11860.0;"['numpy', 'set_printoptions', ""int_frmt:lambda x : '{:,}'.format(x)\nnp.set_printoptions(formatter={'int_kind':int_frmt})\n"", ""pd.set_option('display.notebook_repr_html', True)\n"", ""from IPython.display import HTML\nint_frmt = lambda x: '{:,}'.format(x)\nfloat_frmt = lambda x: '{:,.0f}'.format(x) if x > 1e3 else '{:,.2f}'.format(x)\nfrmt_map = {np.dtype('int64'):int_frmt, np.dtype('float64'):float_frmt}\nfrmt = {col:frmt_map[df.dtypes[col]] for col in df.columns if df.dtypes[col] in frmt_map.keys()}\nHTML(df.to_html(formatters=frmt))\n""]"
261;3.0;0;18885175;;1;29;<python><zip><pandas>;Read a zipped file as a pandas DataFrame;18856.0;"[""import requests, zipfile, StringIO\nr = requests.get('http://data.octo.dc.gov/feeds/crime_incidents/archive/crime_incidents_2013_CSV.zip')\nz = zipfile.ZipFile(StringIO.StringIO(r.content))\ncrime2013 = pandas.read_csv(z.read('crime_incidents_2013_CSV.csv'))\n""]"
262;3.0;3;18889588;;1;21;<python><pandas><dummy-data><categorical-data>;Create dummies from column with multiple values in pandas;14589.0;"['pandas.get_dummies()', ""['A', 'B']"", 'get_dummies()', ""['A', 'B', 'C', 'D', 'A*C', 'C*D']"", 'get_dummies()']"
263;1.0;1;18942506;;1;41;<python><pandas><dataframe>;Add new column in Pandas DataFrame Python;83369.0;['Col1 Col2\nA     1 \nB     2\nC     3\n', 'Col1 Col2 Col3\nA    1    1\nB    2    0\nC    3    0\n']
264;3.0;1;18973404;;1;34;<python><matplotlib><pandas><bar-chart>;Setting Different Bar color in matplotlib Python;46910.0;"["">>> f=plt.figure()\n>>> ax=f.add_subplot(1,1,1)\n>>> ax.bar([1,2,3,4], [1,2,3,4])\n<Container object of 4 artists>\n>>> ax.get_children()\n[<matplotlib.axis.XAxis object at 0x6529850>, <matplotlib.axis.YAxis object at 0x78460d0>,  <matplotlib.patches.Rectangle object at 0x733cc50>, <matplotlib.patches.Rectangle object at 0x733cdd0>, <matplotlib.patches.Rectangle object at 0x777f290>, <matplotlib.patches.Rectangle object at 0x777f710>, <matplotlib.text.Text object at 0x7836450>, <matplotlib.patches.Rectangle object at 0x7836390>, <matplotlib.spines.Spine object at 0x6529950>, <matplotlib.spines.Spine object at 0x69aef50>, <matplotlib.spines.Spine object at 0x69ae310>, <matplotlib.spines.Spine object at 0x69aea50>]\n>>> ax.get_children()[2].set_color('r') #You can also try to locate the first patches.Rectangle object instead of direct calling the index.\n""]"
265;1.0;0;18992086;;1;31;<python><pandas><histogram>;save a pandas.Series histogram plot to file;26802.0;[]
266;3.0;1;19078325;;1;22;<python><group-by><pandas><aggregate-functions>;Naming returned columns in Pandas aggregate function?;15533.0;"['data.groupby(""Country"").agg(\n        {""column1"": {""foo"": sum()}, ""column2"": {""mean"": np.mean, ""std"": np.std}})\n']"
267;2.0;0;19112398;;1;65;<python><pandas><datanitro>;Getting list of lists into pandas DataFrame;72639.0;"['table = Cell(""A1"").table\n', ""table = [['Heading1', 'Heading2'], [1 , 2], [3, 4]]\n\nheaders = table.pop(0) # gives the headers as list and leaves data\n""]"
268;4.0;4;19124601;;1;138;<python><pandas><dataframe>;Is there a way to (pretty) print the entire Pandas Series / DataFrame?;94738.0;['__repr__']
269;2.0;4;19125091;;1;22;<python><pandas>;Pandas Merge - How to avoid duplicating columns;21524.0;"['df:                 currency  adj_date   data_col1 ...\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45\n...\n\ndf2:                currency  adj_date   data_col2 ...\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45\n...\n', ""dfNew = merge(df, df2, left_index=True, right_index=True, how='outer')\n"", 'dfNew:              currency_x  adj_date_x   data_col2 ... currency_y adj_date_y\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45             USD         2012-01-03\n']"
270;2.0;0;19155718;;1;23;<python><pandas>;Select Pandas rows based on list index;38383.0;['   20060930  10.103       NaN     10.103   7.981\n   20061231  15.915       NaN     15.915  12.686\n   20070331   3.196       NaN      3.196   2.710\n   20070630   7.907       NaN      7.907   6.459\n', '   20061231  15.915       NaN     15.915  12.686\n   20070630   7.907       NaN      7.907   6.459\n']
271;2.0;0;19213789;;1;44;<python><matplotlib><plot><pandas>;How do you plot a vertical line on a time series plot in Pandas?;39922.0;[]
272;2.0;0;19231871;;1;30;<python><pandas><unix-timestamp><dataframe>;Convert unix time to readable date in pandas DataFrame;24317.0;"['import json\nimport urllib2\nfrom datetime import datetime\nresponse = urllib2.urlopen(\'http://blockchain.info/charts/market-price?&format=json\')\ndata = json.load(response)   \ndf = DataFrame(data[\'values\'])\ndf.columns = [""date"",""price""]\n#convert dates \ndf.date = df.date.apply(lambda d: datetime.strptime(d, ""%Y-%m-%d""))\ndf.index = df.date   \ndf\n', 'df.date = df.date.apply(lambda d: datetime.strptime(d, ""%Y-%m-%d""))', 'datetime.date.fromtimestamp']"
273;3.0;0;19237878;;1;30;<pandas><subset>;subsetting a Python DataFrame;71327.0;"['k1 <- subset(data, Product = p.id & Month < mn & Year == yr, select = c(Time, Product))\n', 'import pandas as pd\ndata = pd.read_csv(""../data/monthly_prod_sales.csv"")\n\n\n#first, index the dataset by Product. And, get all that matches a given \'p.id\' and time.\n data.set_index(\'Product\')\n k = data.ix[[p.id, \'Time\']]\n\n# then, index this subset with Time and do more subsetting..\n', 'k1 <- subset(data, Product = p.id & Time >= start_time & Time < end_time, select = c(Time, Product))\n']"
274;5.0;0;19324453;;1;32;<python><date><plot><pandas><dataframe>;Add missing dates to pandas dataframe;17676.0;"[""idx = pd.date_range(df['simpleDate'].min(), df['simpleDate'].max())\ns = df.groupby(['simpleDate']).size()\n"", 'reindex', ""df.groupby(['simpleDate']).size()"", '09-02-2013     2\n09-03-2013    10\n09-06-2013     5\n09-07-2013     1\n']"
275;3.0;0;19365513;;1;22;<python><pandas>;How to add an extra row to a pandas dataframe;80190.0;"[""columns = ['Date', 'Name', 'Action','ID']\ndf = pd.DataFrame(columns=columns) \n""]"
276;8.0;1;19377969;;1;103;<python><pandas><dataframe>;Combine two columns of text in dataframe in pandas/python;112188.0;[]
277;2.0;0;19384532;;1;93;<python><group-by><pandas><distinct>;How to count number of rows in a group in pandas group by object?;144711.0;"['df', 'groupby', ""df['col1','col2','col3','col4'].groupby(['col1','col2']).mean()\n""]"
278;12.0;1;19482970;;1;304;<python><pandas><dataframe>;Get list from pandas DataFrame column headers;438665.0;['>>> my_dataframe\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n5   4    8    3\n6   8    2    8\n7   9    9   10\n8   6    6    4\n9  10   10    7\n', '>>> header_list\n[y, gdp, cap]\n']
279;5.0;0;19530568;;1;25;<python><pandas>;Can pandas groupby aggregate into a list, rather than sum, mean, etc?;16908.0;['    A    B    C  \n    1    10   22\n    1    12   20\n    1    11   8\n    1    10   10\n    2    11   13\n    2    12   10 \n    3    14   0\n', '     A    B    C  New1  New2  New3  New4  New5  New6\n    1    10   22  12    20    11    8     10    10\n    2    11   13  12    10 \n    3    14   0\n']
280;3.0;2;19555525;;1;28;<python><matplotlib><pandas>;Saving plots (AxesSubPlot) generated from python pandas with matplotlib's savefig;25060.0;"[""dtf = pd.DataFrame.from_records(d,columns=h)\nfig = plt.figure()\nax = dtf2.plot()\nax = fig.add_subplot(ax)\nfig.savefig('~/Documents/output.png')\n"", 'Traceback (most recent call last):\n  File ""./testgraph.py"", line 76, in <module>\n    ax = fig.add_subplot(ax)\n  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/figure.py"", line 890, in add_subplot\n    assert(a.get_figure() is self)\nAssertionError\n', 'dtf2.plot().savefig(\'~/Documents/output.png\')\n\n\n  File ""./testgraph.py"", line 79, in <module>\n    dtf2.plot().savefig(\'~/Documents/output.png\')\nAttributeError: \'AxesSubplot\' object has no attribute \'savefig\'\n', ""fig = plt.figure()\ndtf2.plot()\nfig.savefig('output.png')\n""]"
281;3.0;0;19584029;;1;27;<python><pandas><histogram>;Plotting histograms from grouped data in a pandas DataFrame;47761.0;"[""from pandas import DataFrame\nimport numpy as np\nx = ['A']*300 + ['B']*400 + ['C']*300\ny = np.random.randn(1000)\ndf = DataFrame({'Letter':x, 'N':y})\ngrouped = df.groupby('Letter')\n"", ""df.groupby('Letter').hist()\n""]"
282;3.0;0;19585280;;1;21;<python><pandas>;Convert a row in pandas into list;21327.0;['admit   gpa  gre  rank   \n0  3.61  380     3  \n1  3.67  660     3  \n1  3.19  640     4  \n0  2.93  520     4\n', '[[0,3.61,380,3], [1,3.67,660,3], [1,3.19,640,4], [0,2.93,520,4]]   \n']
283;2.0;0;19611729;;1;23;<python><pandas><google-spreadsheet><google-apps>;Getting Google Spreadsheet CSV into A Pandas Dataframe;7482.0;"[""import requests\nr = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&output=csv')\ndata = r.content\n"", ""',City,region,Res_Comm,mkt_type,Quradate,National_exp,Alabama_exp,Sales_exp,Inventory_exp,Price_exp,Credit_exp\\n0,Dothan,South_Central-Montgomery-Auburn-Wiregrass-Dothan,Residential,Rural,1/15/2010,2,2,3,2,3,3\\n10,Foley,South_Mobile-Baldwin,Residential,Suburban_Urban,1/15/2010,4,4,4,4,4,3\\n12,Birmingham,North_Central-Birmingham-Tuscaloosa-Anniston,Commercial,Suburban_Urban,1/15/2010,2,2,3,2,2,3\\n\n"", ""df = pd.io.parsers.read_csv('/home/tom/Dropbox/Projects/annonallanswerswithmaster1012013.csv',index_col=0,parse_dates=['Quradate'])\n"", 'https://docs.google.com/spreadsheets/d/177_dFZ0i-duGxLiyg6tnwNDKruAYE-_Dd8vAQziipJQ/export?format=csv&id\n']"
284;3.0;0;19618912;;1;21;<python><python-2.7><pandas><dataframe><intersect>;Finding common rows (intersection) in two Pandas dataframes;23593.0;['df1', 'df2', '+------------------------+------------------------+--------+\n|        user_id         |      business_id       | rating |\n+------------------------+------------------------+--------+\n| rLtl8ZkDX5vH5nAx9C3q5Q | eIxSLxzIlfExI6vgAbn2JA |      4 |\n| C6IOtaaYdLIT5fWd7ZYIuA | eIxSLxzIlfExI6vgAbn2JA |      5 |\n| mlBC3pN9GXlUUfQi1qBBZA | KoIRdcIfh3XWxiCeV1BDmA |      3 |\n+------------------------+------------------------+--------+\n', 'user_id', 'df1', 'df2', 'user_id', 'df1', 'df2', 'user_id', 'merge']
285;2.0;1;19632075;;1;25;<python><pandas>;how to read file with space separated values;17660.0;"[""pd.read_csv('file.csv', delimiter=' ')\n""]"
286;4.0;4;19726663;;1;27;<python><matplotlib><pandas>;How to save the Pandas dataframe/series data as a figure?;22214.0;"['>>> df\n                   sales  net_pft     ROE    ROIC\nSTK_ID RPT_Date                                  \n600809 20120331  22.1401   4.9253  0.1651  0.6656\n       20120630  38.1565   7.8684  0.2567  1.0385\n       20120930  52.5098  12.4338  0.3587  1.2867\n       20121231  64.7876  13.2731  0.3736  1.2205\n       20130331  27.9517   7.5182  0.1745  0.3723\n       20130630  40.6460   9.8572  0.2560  0.4290\n       20130930  53.0501  11.8605  0.2927  0.4369 \n', ""df.output_as_png(filename='df_data.png')""]"
287;3.0;0;19736080;;1;23;<python><pandas>;Creating dataframe from a dictionary where entries have different lengths;17482.0;['pd.DataFrame(my_dict)\n', 'ValueError: arrays must all be the same length\n', 'NaN']
288;2.0;2;19758364;;1;80;<python><pandas><dataframe>;python: rename single column header in pandas dataframe;50105.0;['data', 'gdp', 'log(gdp)', 'data =\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n5   4    8    3\n6   8    2    8\n7   9    9   10\n8   6    6    4\n9  10   10    7\n']
289;6.0;3;19790790;;1;33;<python><split><pandas><dataframe>;Splitting dataframe into multiple dataframes;55592.0;"[""import pandas as pd\n\ndef splitframe(data, name='name'):\n\n    n = data[name][0]\n\n    df = pd.DataFrame(columns=data.columns)\n\n    datalist = []\n\n    for i in range(len(data)):\n        if data[name][i] == n:\n            df = df.append(data.iloc[i])\n        else:\n            datalist.append(df)\n            df = pd.DataFrame(columns=data.columns)\n            n = data[name][i]\n            df = df.append(data.iloc[i])\n\n    return datalist\n""]"
290;6.0;0;19798153;;1;189;<python><numpy><pandas><vectorization>;Difference between map, applymap and apply methods in Pandas;98032.0;['map', 'Series', 'DataFrame', 'apply', 'applymap']
291;3.0;1;19828822;;1;111;<python><pandas>;How to check whether a pandas DataFrame is empty?;70044.0;['DataFrame', 'DataFrame']
292;4.0;0;19851005;;1;43;<python><pandas><dataframe>;Rename Pandas DataFrame Index;76528.0;"[""In [2]: df = pd.read_csv(r'D:\\Data\\DataTimeSeries_csv//seriesSM.csv', header=None, parse_dates=[[0]], index_col=[0] )\n\nIn [3]: df.head()\nOut[3]: \n                   1\n0                   \n2002-06-18  0.112000\n2002-06-22  0.190333\n2002-06-26  0.134000\n2002-06-30  0.093000\n2002-07-04  0.098667\n\nIn [4]: df.rename(index={0:'Date'}, columns={1:'SM'}, inplace=True)\n\nIn [5]: df.head()\nOut[5]: \n                  SM\n0                   \n2002-06-18  0.112000\n2002-06-22  0.190333\n2002-06-26  0.134000\n2002-06-30  0.093000\n2002-07-04  0.098667\n""]"
293;5.0;1;19913659;;1;85;<python><pandas>;Pandas conditional creation of a series/dataframe column;70108.0;['    Type       Set\n1    A          Z\n2    B          Z           \n3    B          X\n4    C          Y\n']
294;4.0;0;19914937;;1;52;<python><pandas>;Applying function with multiple arguments to create a new pandas column;45281.0;"['pandas', 'import pandas as pd\ndf = pd.DataFrame({""A"": [10,20,30], ""B"": [20, 30, 10]})\n\ndef fx(x):\n    return x * x\n\nprint(df)\ndf[\'newcolumn\'] = df.A.apply(fx)\nprint(df)\n', 'def fxy(x, y):\n    return x * y\n']"
295;4.0;0;19928284;;1;23;<pandas>;Pandas dataframe values equality test;7575.0;"[""dates = pd.date_range('20130101', periods=6)\n\ndf1 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\ndf2 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n\nprint df1\nprint df2\nself.assertItemsEqual(df1, df2)\n""]"
296;4.0;2;19960077;;1;84;<python><pandas><dataframe><sql-function>;How to implement 'in' and 'not in' for Pandas dataframe;72686.0;"['IN', 'NOT IN', ""df = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = ['UK','China']\n\n# pseudo-code:\ndf[df['countries'] not in countries]\n"", ""df = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = pd.DataFrame({'countries':['UK','China'], 'matched':True})\n\n# IN\ndf.merge(countries,how='inner',on='countries')\n\n# NOT IN\nnot_in = df.merge(countries,how='left',on='countries')\nnot_in = not_in[pd.isnull(not_in['matched'])]\n""]"
297;2.0;3;19961490;;1;43;<python><python-2.7><pandas>;Construct pandas DataFrame from list of tuples;96105.0;"[""data = [\n('r1', 'c1', avg11, stdev11),\n('r1', 'c2', avg12, stdev12),\n('r2', 'c1', avg21, stdev21),\n('r2', 'c2', avg22, stdev22)\n]\n"", 'pandas.DataFrame([x[1:] for x in data], index = [x[0] for x in data])']"
298;4.0;0;19991445;;1;60;<python><pandas><scikit-learn><regression><statsmodels>;Run an OLS regression with Pandas Data Frame;82103.0;"['pandas', 'import pandas as pd\ndf = pd.DataFrame({""A"": [10,20,30,40,50], \n                   ""B"": [20, 30, 10, 40, 50], \n                   ""C"": [32, 234, 23, 23, 42523]})\n', 'ols(A ~ B + C, data = df)', 'scikit-learn']"
299;5.0;2;20003290;;1;22;<python><csv><numpy><floating-point><pandas>;Print different precision by column with pandas.DataFrame.to_csv()?;16557.0;"['pandas', 'pandas', 'In [53]: df_data[:5]\nOut[53]: \n    year  month  day       lats       lons  vals\n0   2012      6   16  81.862745 -29.834254   0.0\n1   2012      6   16  81.862745 -29.502762   0.1\n2   2012      6   16  81.862745 -29.171271   0.0\n3   2012      6   16  81.862745 -28.839779   0.2\n4   2012      6   16  81.862745 -28.508287   0.0\n', 'float_format', ""df_data.to_csv(outfile, index=False,\n                   header=False, float_format='%11.6f')\n"", 'vals', '2012,6,16,  81.862745, -29.834254,   0.000000\n2012,6,16,  81.862745, -29.502762,   0.100000\n2012,6,16,  81.862745, -29.171270,   0.000000\n2012,6,16,  81.862745, -28.839779,   0.200000\n2012,6,16,  81.862745, -28.508287,   0.000000\n']"
300;3.0;5;20025325;;1;29;<python><pandas><indexing><dataframe>;Apply Function on DataFrame Index;15303.0;"['DataFrame', 'pd.DataFrame({""Month"": df.reset_index().Date.apply(foo)})\n', 'Date', 'foo']"
301;2.0;0;20033111;;1;37;<python><python-2.7><pandas><max>;Python Pandas max value of selected columns;43953.0;"[""data = {'name' : ['bill', 'joe', 'steve'],\n    'test1' : [85, 75, 85],\n    'test2' : [35, 45, 83],\n     'test3' : [51, 61, 45]}\nframe = pd.DataFrame(data)\n"", ' name test1 test2 test3 HighScore\n bill  75    75    85    85\n joe   35    45    83    83 \n steve  51   61    45    61 \n', ""frame['HighScore'] = max(data['test1'], data['test2'], data['test3'])\n""]"
302;3.0;0;20067636;;1;38;<python><pandas><dataframe>;Pandas dataframe get first row of each group;35554.0;"['DataFrame', 'df = pd.DataFrame({\'id\' : [1,1,1,2,2,3,3,3,3,4,4,5,6,6,6,7,7],\n                \'value\'  : [""first"",""second"",""second"",""first"",\n                            ""second"",""first"",""third"",""fourth"",\n                            ""fifth"",""second"",""fifth"",""first"",\n                            ""first"",""second"",""third"",""fourth"",""fifth""]})\n', '        id   value\n0        1   first\n1        1  second\n2        1  second\n3        2   first\n4        2  second\n5        3   first\n6        3   third\n7        3  fourth\n8        3   fifth\n9        4  second\n10       4   fifth\n11       5   first\n12       6   first\n13       6  second\n14       6   third\n15       7  fourth\n16       7   fifth\n', '    id   value\n     1   first\n     2   first\n     3   first\n     4  second\n     5  first\n     6  first\n     7  fourth\n', 'DataFrame', ""In [25]: for index, row in df.iterrows():\n   ....:     df2 = pd.DataFrame(df.groupby(['id','value']).reset_index().ix[0])\n""]"
303;2.0;1;20069009;;1;54;<python><pandas><greatest-n-per-group><window-functions><top-n>;Pandas good approach to get top-n records within each group;41787.0;"["">>> df = pd.DataFrame({'id':[1,1,1,2,2,2,2,3,4],'value':[1,2,3,1,2,3,4,1,1]})\n>>> df\n   id  value\n0   1      1\n1   1      2\n2   1      3\n3   2      1\n4   2      2\n5   2      3\n6   2      4\n7   3      1\n8   4      1\n"", '   id  value\n0   1      1\n1   1      2\n3   2      1\n4   2      2\n7   3      1\n8   4      1\n', "">>> dfN = df.groupby('id').apply(lambda x:x['value'].reset_index()).reset_index()\n>>> dfN\n   id  level_1  index  value\n0   1        0      0      1\n1   1        1      1      2\n2   1        2      2      3\n3   2        0      3      1\n4   2        1      4      2\n5   2        2      5      3\n6   2        3      6      4\n7   3        0      7      1\n8   4        0      8      1\n>>> dfN[dfN['level_1'] <= 1][['id', 'value']]\n   id  value\n0   1      1\n1   1      2\n3   2      1\n4   2      2\n7   3      1\n8   4      1\n""]"
304;2.0;0;20076195;;1;27;<pandas>;what is the most efficient way of counting occurrences in pandas?;34623.0;"[""df.columns = ['word','documents','frequency']\n"", ""word_grouping = df[['word','frequency']].groupby('word')\nMaxFrequency_perWord = word_grouping[['frequency']].max().reset_index()\nMaxFrequency_perWord.columns = ['word','MaxFrequency']\n"", ""Occurrences_of_Words = word_grouping[['word']].count().reset_index()\n"", 'df.word.describe()\n']"
305;2.0;1;20083098;;1;29;<python><performance><pandas><hdf5><pytables>;Improve pandas (PyTables?) HDF5 table write performance;10552.0;"[""with Timer() as t:\n    store = pd.HDFStore('test_storer.h5', 'w')\n    store.put('events', events_dataset, table=False, append=False)\nprint('Fixed format write took ' + str(t.interval))\nwith Timer() as t:\n    store = pd.HDFStore('test_table.h5', 'w')\n    store.put('events', events_dataset, table=True, append=False)\nprint('Table format write took ' + str(t.interval))\n"", 'Fixed format write took 7.1\nTable format write took 178.7\n', 'node_id           int64\nthread_id         int64\nhandle_id         int64\ntype              int64\nbegin             int64\nend               int64\nduration          int64\nflags             int64\nunique_id         int64\nid                int64\nDSTL_LS_FULL    float64\nL2_DMISS        float64\nL3_MISS         float64\nkernel_type     float64\ndtype: object\n', ""%prun -l 20 profile.events.to_hdf('test.h5', 'events', table=False, append=False)\n\n3223 function calls (3222 primitive calls) in 7.385 seconds\n\nOrdered by: internal time\nList reduced from 208 to 20 due to restriction <20>\n\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n    6    7.127    1.188    7.128    1.188 {method '_createArray' of 'tables.hdf5Extension.Array' objects}\n    1    0.242    0.242    0.242    0.242 {method '_closeFile' of 'tables.hdf5Extension.File' objects}\n    1    0.003    0.003    0.003    0.003 {method '_g_new' of 'tables.hdf5Extension.File' objects}\n   46    0.001    0.000    0.001    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n"", ""   %prun -l 40 profile.events.to_hdf('test.h5', 'events', table=True, append=False, chunksize=1000000)\n\n   499082 function calls (499040 primitive calls) in 188.981 seconds\n\n   Ordered by: internal time\n   List reduced from 526 to 40 due to restriction <40>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n       29   92.018    3.173   92.018    3.173 {pandas.lib.create_hdf_rows_2d}\n      640   20.987    0.033   20.987    0.033 {method '_append' of 'tables.hdf5Extension.Array' objects}\n       29   19.256    0.664   19.256    0.664 {method '_append_records' of 'tables.tableExtension.Table' objects}\n      406   19.182    0.047   19.182    0.047 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n    14244   10.646    0.001   10.646    0.001 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n      472   10.359    0.022   10.359    0.022 {method 'copy' of 'numpy.ndarray' objects}\n       80    3.409    0.043    3.409    0.043 {tables.indexesExtension.keysort}\n        2    3.023    1.512    3.023    1.512 common.py:134(_isnull_ndarraylike)\n       41    2.489    0.061    2.533    0.062 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       87    2.401    0.028    2.401    0.028 {method 'astype' of 'numpy.ndarray' objects}\n       30    1.880    0.063    1.880    0.063 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n      282    0.824    0.003    0.824    0.003 {method 'reduce' of 'numpy.ufunc' objects}\n       41    0.537    0.013    0.668    0.016 index.py:607(final_idx32)\n    14490    0.385    0.000    0.712    0.000 array.py:342(_interpret_indexing)\n       39    0.279    0.007   19.635    0.503 index.py:1219(reorder_slice)\n        2    0.256    0.128   10.063    5.031 index.py:1099(get_neworder)\n        1    0.090    0.090  119.392  119.392 pytables.py:3016(write_data)\n    57842    0.087    0.000    0.087    0.000 {numpy.core.multiarray.empty}\n    28570    0.062    0.000    0.107    0.000 utils.py:42(is_idx)\n    14164    0.062    0.000    7.181    0.001 array.py:711(_readSlice)\n"", ""%prun -l 40 profile.events.to_hdf('test.h5', 'events', table=True, append=False, chunksize=1000000)\n\n         499748 function calls (499720 primitive calls) in 117.187 seconds\n\n   Ordered by: internal time\n   List reduced from 539 to 20 due to restriction <20>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      640   22.010    0.034   22.010    0.034 {method '_append' of 'tables.hdf5Extension.Array' objects}\n       29   20.782    0.717   20.782    0.717 {method '_append_records' of 'tables.tableExtension.Table' objects}\n      406   19.248    0.047   19.248    0.047 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n    14244   10.685    0.001   10.685    0.001 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n      472   10.439    0.022   10.439    0.022 {method 'copy' of 'numpy.ndarray' objects}\n       30    7.356    0.245    7.356    0.245 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n       29    7.161    0.247   37.609    1.297 pytables.py:3498(write_data_chunk)\n        2    3.888    1.944    3.888    1.944 common.py:197(_isnull_ndarraylike)\n       80    3.581    0.045    3.581    0.045 {tables.indexesExtension.keysort}\n       41    3.248    0.079    3.294    0.080 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       34    2.744    0.081    2.744    0.081 {method 'ravel' of 'numpy.ndarray' objects}\n      115    2.591    0.023    2.591    0.023 {method 'astype' of 'numpy.ndarray' objects}\n      270    0.875    0.003    0.875    0.003 {method 'reduce' of 'numpy.ufunc' objects}\n       41    0.560    0.014    0.732    0.018 index.py:607(final_idx32)\n    14490    0.387    0.000    0.712    0.000 array.py:342(_interpret_indexing)\n       39    0.303    0.008   19.617    0.503 index.py:1219(reorder_slice)\n        2    0.288    0.144   10.299    5.149 index.py:1099(get_neworder)\n    57871    0.087    0.000    0.087    0.000 {numpy.core.multiarray.empty}\n        1    0.084    0.084   45.266   45.266 pytables.py:3424(write_data)\n        1    0.080    0.080   55.542   55.542 pytables.py:3385(write)\n"", 'In [7]: %timeit f(df)\n1 loops, best of 3: 3.7 s per loop\n\nIn [8]: %timeit f2(df) # where chunksize= 2 000 000\n1 loops, best of 3: 13.8 s per loop\n\nIn [9]: %timeit f3(df) # where chunksize= 2 000 000\n1 loops, best of 3: 43.4 s per loop\n', 'top', 'ls', 'In [28]: %time f(profile.events)\nCPU times: user 0 ns, sys: 7.16 s, total: 7.16 s\nWall time: 7.51 s\n\nIn [29]: %time f2(profile.events)\nCPU times: user 18.7 s, sys: 14 s, total: 32.7 s\nWall time: 47.2 s\n\nIn [31]: %time f3(profile.events)\nCPU times: user 1min 18s, sys: 14.4 s, total: 1min 32s\nWall time: 2min 5s\n', 'ptdump -av test.h5\n/ (RootGroup) \'\'\n  /._v_attrs (AttributeSet), 4 attributes:\n   [CLASS := \'GROUP\',\n    PYTABLES_FORMAT_VERSION := \'2.1\',\n    TITLE := \'\',\n    VERSION := \'1.0\']\n/df (Group) \'\'\n  /df._v_attrs (AttributeSet), 14 attributes:\n   [CLASS := \'GROUP\',\n    TITLE := \'\',\n    VERSION := \'1.0\',\n    data_columns := [],\n    encoding := None,\n    index_cols := [(0, \'index\')],\n    info := {1: {\'type\': \'Index\', \'names\': [None]}, \'index\': {}},\n    levels := 1,\n    nan_rep := \'nan\',\n    non_index_axes := \n    [(1, [\'node_id\', \'thread_id\', \'handle_id\', \'type\', \'begin\', \'end\', \'duration\', \'flags\', \'unique_id\', \'id\', \'DSTL_LS_FULL\', \'L2_DMISS\', \'L3_MISS\', \'kernel_type\'])],\n    pandas_type := \'frame_table\',\n    pandas_version := \'0.10.1\',\n    table_type := \'appendable_frame\',\n    values_cols := [\'values_block_0\', \'values_block_1\']]\n/df/table (Table(28880943,)) \'\'\n  description := {\n  ""index"": Int64Col(shape=(), dflt=0, pos=0),\n  ""values_block_0"": Int64Col(shape=(10,), dflt=0, pos=1),\n  ""values_block_1"": Float64Col(shape=(4,), dflt=0.0, pos=2)}\n  byteorder := \'little\'\n  chunkshape := (4369,)\n  autoindex := True\n  colindexes := {\n    ""index"": Index(6, medium, shuffle, zlib(1)).is_csi=False}\n  /df/table._v_attrs (AttributeSet), 15 attributes:\n   [CLASS := \'TABLE\',\n    FIELD_0_FILL := 0,\n    FIELD_0_NAME := \'index\',\n    FIELD_1_FILL := 0,\n    FIELD_1_NAME := \'values_block_0\',\n    FIELD_2_FILL := 0.0,\n    FIELD_2_NAME := \'values_block_1\',\n    NROWS := 28880943,\n    TITLE := \'\',\n    VERSION := \'2.7\',\n    index_kind := \'integer\',\n    values_block_0_dtype := \'int64\',\n    values_block_0_kind := [\'node_id\', \'thread_id\', \'handle_id\', \'type\', \'begin\', \'end\', \'duration\', \'flags\', \'unique_id\', \'id\'],\n    values_block_1_dtype := \'float64\',\n    values_block_1_kind := [\'DSTL_LS_FULL\', \'L2_DMISS\', \'L3_MISS\', \'kernel_type\']]\n', ""%prun -l 25  %time f3(profile.events)\nCPU times: user 1min 14s, sys: 16.2 s, total: 1min 30s\nWall time: 1min 48s\n\n        542678 function calls (542650 primitive calls) in 108.678 seconds\n\n   Ordered by: internal time\n   List reduced from 629 to 25 due to restriction <25>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      640   23.633    0.037   23.633    0.037 {method '_append' of 'tables.hdf5extension.Array' objects}\n       15   20.852    1.390   20.852    1.390 {method '_append_records' of 'tables.tableextension.Table' objects}\n      406   19.584    0.048   19.584    0.048 {method '_g_write_slice' of 'tables.hdf5extension.Array' objects}\n    14244   10.591    0.001   10.591    0.001 {method '_g_read_slice' of 'tables.hdf5extension.Array' objects}\n      458    9.693    0.021    9.693    0.021 {method 'copy' of 'numpy.ndarray' objects}\n       15    6.350    0.423   30.989    2.066 pytables.py:3498(write_data_chunk)\n       80    3.496    0.044    3.496    0.044 {tables.indexesextension.keysort}\n       41    3.335    0.081    3.376    0.082 {method '_fill_col' of 'tables.tableextension.Row' objects}\n       20    2.551    0.128    2.551    0.128 {method 'ravel' of 'numpy.ndarray' objects}\n      101    2.449    0.024    2.449    0.024 {method 'astype' of 'numpy.ndarray' objects}\n       16    1.789    0.112    1.789    0.112 {method '_g_flush' of 'tables.hdf5extension.Leaf' objects}\n        2    1.728    0.864    1.728    0.864 common.py:197(_isnull_ndarraylike)\n       41    0.586    0.014    0.842    0.021 index.py:637(final_idx32)\n    14490    0.292    0.000    0.616    0.000 array.py:368(_interpret_indexing)\n        2    0.283    0.142   10.267    5.134 index.py:1158(get_neworder)\n      274    0.251    0.001    0.251    0.001 {method 'reduce' of 'numpy.ufunc' objects}\n       39    0.174    0.004   19.373    0.497 index.py:1280(reorder_slice)\n    57857    0.085    0.000    0.085    0.000 {numpy.core.multiarray.empty}\n        1    0.083    0.083   35.657   35.657 pytables.py:3424(write_data)\n        1    0.065    0.065   45.338   45.338 pytables.py:3385(write)\n    14164    0.065    0.000    7.831    0.001 array.py:615(__getitem__)\n    28570    0.062    0.000    0.108    0.000 utils.py:47(is_idx)\n       47    0.055    0.001    0.055    0.001 {numpy.core.multiarray.arange}\n    28570    0.050    0.000    0.090    0.000 leaf.py:397(_process_range)\n    87797    0.048    0.000    0.048    0.000 {isinstance}\n""]"
306;2.0;0;20084382;;1;48;<python><pandas><dataframe>;Find unique values in a Pandas dataframe, irrespective of row or column location;81292.0;[]
307;3.0;0;20095673;;1;26;<python><pandas><dataframe>;python: shift column in pandas dataframe up by one;24501.0;['df =\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n\ndf_lag =\n    y  gdp  cap\n0   1    3    5\n1   2    7    9\n2   8    4    2\n3   3    7    7\n']
308;5.0;0;20107570;;1;27;<python><pandas>;Removing index column in pandas;70802.0;"[""df = pd.DataFrame.from_csv('Efficiency_Data.csv', header=0, parse_dates=False)\nenergy = df.index\nefficiency = df.Efficiency\nprint efficiency\n"", ""del df['index']\n"", 'energy = df.index\n']"
309;5.0;3;20109391;;1;105;<python><pandas>;How to make good reproducible pandas examples;3594.0;"['pandas', 'pandas', ""import pandas as pd\ndf = pd.DataFrame({'user': ['Bob', 'Jane', 'Alice'], \n                   'income': [40000, 50000, 42000]})\n"", 'datetime', 'expand.grid()', 'dput()']"
310;1.0;3;20110170;;1;63;<python><pandas><dataframe><flatten><multi-index>;Turn Pandas Multi-Index into column;31159.0;['                         value\nTrial    measurement\n    1              0        13\n                   1         3\n                   2         4\n    2              0       NaN\n                   1        12\n    3              0        34 \n', 'Trial    measurement       value\n\n    1              0        13\n    1              1         3\n    1              2         4\n    2              0       NaN\n    2              1        12\n    3              0        34 \n']
311;4.0;5;20158597;;1;29;<python><pandas>;How to qcut with non unique bin edges?;8987.0;['def fractile_cut(ser, num_fractiles):\n    num_valid = ser.valid().shape[0]\n    remain_fractiles = num_fractiles\n    vcounts = ser.value_counts()\n    high_freq = []\n    i = 0\n    while vcounts.iloc[i] > num_valid/ float(remain_fractiles):\n        curr_val = vcounts.index[i]\n        high_freq.append(curr_val)\n        remain_fractiles -= 1\n        num_valid = num_valid - vcounts[i]\n        i += 1\n    curr_ser = ser.copy()\n    curr_ser = curr_ser[~curr_ser.isin(high_freq)]\n    qcut = pd.qcut(curr_ser, remain_fractiles, retbins=True)\n    qcut_bins = qcut[1]\n    all_bins = list(qcut_bins)\n    for val in high_freq:\n        bisect.insort(all_bins, val)\n    cut = pd.cut(ser, bins=all_bins)\n    ser_fractiles = pd.Series(cut.labels + 1, index=ser.index)\n    return ser_fractiles\n']
312;5.0;5;20219254;;1;46;<python><excel><python-2.7><pandas>;How to write to an existing excel file without overwriting data (using pandas)?;29344.0;"['import pandas\n\nwriter = pandas.ExcelWriter(\'Masterfile.xlsx\') \n\ndata_filtered.to_excel(writer, ""Main"", cols=[\'Diff1\', \'Diff2\'])\n\nwriter.save()\n']"
313;4.0;0;20230326;;1;29;<python><pandas><dataframe>;Retrieve DataFrame of all but one specified column;17402.0;[]
314;1.0;4;20233071;;1;21;<python><pandas>;Filter Pandas DataFrame by time index;15944.0;"[""df = df[df.index < '2013-10-16 08:00:00']\n""]"
315;1.0;0;20235401;;1;34;<python><pandas><series>;Remove NaN from pandas series;43902.0;[]
316;3.0;0;20250771;;1;69;<python><dictionary><pandas><remap>;Remap values in pandas column with a dict;48507.0;"['di = {1: ""A"", 2: ""B""}', '     col1   col2\n0       w      a\n1       1      2\n2       2    NaN\n', '     col1   col2\n0       w      a\n1       A      2\n2       B    NaN\n']"
317;3.0;1;20297317;;1;37;<python><pandas><dataframe>;python dataframe pandas drop column using int;28907.0;[]
318;3.0;1;20297332;;1;52;<python><pandas><dataframe>;Python pandas dataframe: retrieve number of columns;66459.0;['df.num_columns\n']
319;2.0;0;20375561;;1;28;<python><pandas><dataframe>;Joining pandas dataframes by column names;16792.0;['frame_1:\nevent_id, date, time, county_ID\n\nframe_2:\ncountyid, state\n', 'county_ID = countyid', 'joined_dataframe\nevent_id, date, time, county, state\n']
320;3.0;0;20383647;;1;26;<python><pandas>;Pandas selecting by label sometimes return series, sometimes returns dataframe;7456.0;['In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame(data=range(5), index=[1, 2, 3, 3, 3])\n\nIn [3]: type(df.loc[3])\nOut[3]: pandas.core.frame.DataFrame\n\nIn [4]: type(df.loc[1])\nOut[4]: pandas.core.series.Series\n']
321;1.0;5;20444087;;1;28;<python><pandas><reverse>;Right way to reverse pandas.DataFrame?;31880.0;"[""import pandas as pd\n\ndata = pd.DataFrame({'Odd':[1,3,5,6,7,9], 'Even':[0,2,4,6,8,10]})\n\nfor i in reversed(data):\n    print(data['Odd'], data['Even'])\n"", 'Traceback (most recent call last):\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\generic.py"", line 665, in _get_item_cache\n    return cache[item]\nKeyError: 5\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""C:\\Users\\*****\\Documents\\******\\********\\****.py"", line 5, in <module>\n    for i in reversed(data):\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\frame.py"", line 2003, in __getitem__\n    return self._get_item_cache(key)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\generic.py"", line 667, in _get_item_cache\n    values = self._data.get(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1656, in get\n    _, block = self._find_block(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1936, in _find_block\n    self._check_have(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1943, in _check_have\n    raise KeyError(\'no item named %s\' % com.pprint_thing(item))\nKeyError: \'no item named 5\'\n', 'pandas.DataFrame']"
322;2.0;0;20461165;;1;132;<python><pandas>;How to convert pandas index in a dataframe to a column?;106030.0;['df=\n           gi  ptt_loc\n 0  384444683      593  \n 1  384444684      594 \n 2  384444686      596  \n', 'df=\n    index1       gi    ptt_loc\n 0  0     384444683      593  \n 1  1     384444684      594 \n 2  2     384444686      596  \n']
323;2.0;0;20490274;;1;113;<python><indexing><pandas><dataframe>;How to reset index in a pandas data frame?;101981.0;"['[1,5,6,10,11]', '[0,1,2,3,4]', ""df = df.reset_index()\ndel df['index']\n"", 'df = df.reindex()\n']"
324;3.0;0;20574257;;1;24;<python><pandas><statistics>;Constructing a co-occurrence matrix in python pandas;8487.0;"[""import pandas as pd\n\ndf = pd.DataFrame({'TFD' : ['AA', 'SL', 'BB', 'D0', 'Dk', 'FF'],\n                    'Snack' : ['1', '0', '1', '1', '0', '0'],\n                    'Trans' : ['1', '1', '1', '0', '0', '1'],\n                    'Dop' : ['1', '0', '1', '0', '1', '1']}).set_index('TFD')\n\nprint df\n\n>>> \n    Dop Snack Trans\nTFD                \nAA    1     1     1\nSL    0     0     1\nBB    1     1     1\nD0    0     1     0\nDk    1     0     0\nFF    1     0     1\n\n[6 rows x 3 columns]\n"", '    Dop Snack Trans\n\nDop   0     2     3\nSnack 2     0     2\nTrans 3     2     0\n']"
325;3.0;0;20612645;;1;103;<python><pandas>;How to find the installed pandas version;58878.0;[]
326;5.0;1;20625582;;1;170;<python><parsing><pandas><dataframe>;How to deal with SettingWithCopyWarning in Pandas?;145172.0;"[""E:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE\n"", ""quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE"", 'def _decode_stock_quote(list_of_150_stk_str):\n    """"""decode the webpage and return dataframe""""""\n\n    from cStringIO import StringIO\n\n    str_of_all = """".join(list_of_150_stk_str)\n\n    quote_df = pd.read_csv(StringIO(str_of_all), sep=\',\', names=list(\'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg\')) #dtype={\'A\': object, \'B\': object, \'C\': np.float64}\n    quote_df.rename(columns={\'A\':\'STK\', \'B\':\'TOpen\', \'C\':\'TPCLOSE\', \'D\':\'TPrice\', \'E\':\'THigh\', \'F\':\'TLow\', \'I\':\'TVol\', \'J\':\'TAmt\', \'e\':\'TDate\', \'f\':\'TTime\'}, inplace=True)\n    quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\n    quote_df[\'TClose\'] = quote_df[\'TPrice\']\n    quote_df[\'RT\']     = 100 * (quote_df[\'TPrice\']/quote_df[\'TPCLOSE\'] - 1)\n    quote_df[\'TVol\']   = quote_df[\'TVol\']/TVOL_SCALE\n    quote_df[\'TAmt\']   = quote_df[\'TAmt\']/TAMT_SCALE\n    quote_df[\'STK_ID\'] = quote_df[\'STK\'].str.slice(13,19)\n    quote_df[\'STK_Name\'] = quote_df[\'STK\'].str.slice(21,30)#.decode(\'gb2312\')\n    quote_df[\'TDate\']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])\n\n    return quote_df\n', ""E:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE\nE:\\FinReporter\\FM_EXT.py:450: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TAmt']   = quote_df['TAmt']/TAMT_SCALE\nE:\\FinReporter\\FM_EXT.py:453: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TDate']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])\n""]"
327;3.0;2;20637439;;1;29;<python><csv><pandas>;Skip rows during csv import pandas;35315.0;['pandas.read_csv()', 'skiprows=1']
328;3.0;0;20638006;;1;221;<python><dictionary><pandas><dataframe>;Convert list of dictionaries to Dataframe;57095.0;"['[{\'points\': 50, \'time\': \'5:00\', \'year\': 2010}, \n{\'points\': 25, \'time\': \'6:00\', \'month\': ""february""}, \n{\'points\':90, \'time\': \'9:00\', \'month\': \'january\'}, \n{\'points_h1\':20, \'month\': \'june\'}]\n', 'DataFrame', '      month  points  points_h1  time  year\n0       NaN      50        NaN  5:00  2010\n1  february      25        NaN  6:00   NaN\n2   january      90        NaN  9:00   NaN\n3      june     NaN         20   NaN   NaN\n']"
329;1.0;3;20656663;;1;36;<python><matplotlib><pandas><histogram>;Matplotlib/Pandas error using histogram;24955.0;"[""type(dfj2_MARKET1['VSPD2_perc'])\n"", 'pandas.core.series.Series', ""fig, axes = plt.subplots(1, 7, figsize=(30,4))\naxes[0].hist(dfj2_MARKET1['VSPD1_perc'],alpha=0.9, color='blue')\naxes[0].grid(True)\naxes[0].set_title(MARKET1 + '  5-40 km / h')\n"", ""    AttributeError                            Traceback (most recent call last)\n    <ipython-input-75-3810c361db30> in <module>()\n      1 fig, axes = plt.subplots(1, 7, figsize=(30,4))\n      2 \n    ----> 3 axes[1].hist(dfj2_MARKET1['VSPD2_perc'],alpha=0.9, color='blue')\n      4 axes[1].grid(True)\n      5 axes[1].set_xlabel('Time spent [%]')\n\n    C:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in hist(self, x, bins, range, normed,          weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label,    stacked, **kwargs)\n   8322             # this will automatically overwrite bins,\n   8323             # so that each histogram uses the same bins\n-> 8324             m, bins = np.histogram(x[i], bins, weights=w[i], **hist_kwargs)\n   8325             m = m.astype(float) # causes problems later if it's an int\n   8326             if mlast is None:\n\n    C:\\Python27\\lib\\site-packages\\numpy\\lib\\function_base.pyc in histogram(a, bins, range,     normed, weights, density)\n    158         if (mn > mx):\n    159             raise AttributeError(\n--> 160                 'max must be larger than min in range parameter.')\n    161 \n    162     if not iterable(bins):\n\nAttributeError: max must be larger than min in range parameter.\n""]"
330;2.0;1;20763012;;1;62;<numpy><pandas>;Creating a Pandas DataFrame from a Numpy array: How do I specify the index column and column headers?;128433.0;"[""data = array([['','Col1','Col2'],['Row1',1,2],['Row2',3,4]])\n"", 'df = pd.DataFrame(data,index=data[:,0]),\n']"
331;3.0;2;20845213;;1;100;<python><csv><indexing><pandas>;How to avoid Python/Pandas creating an index in a saved csv?;47670.0;"[""pd.to_csv('C:/Path of file.csv')"", ""pd.read_csv('C:/Path to file to edit.csv', index_col = False)\n"", ""pd.to_csv('C:/Path to save edited file.csv', index_col = False)\n""]"
332;4.0;6;20853474;;1;59;<python><pandas><pip>;ImportError: No module named dateutil.parser;72499.0;"['pandas', 'Python', 'monas-mbp:book mona$ sudo pip install python-dateutil\nRequirement already satisfied (use --upgrade to upgrade): python-dateutil in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python\nCleaning up...\nmonas-mbp:book mona$ python t1.py\nNo module named dateutil.parser\nTraceback (most recent call last):\n  File ""t1.py"", line 4, in <module>\n    import pandas as pd\n  File ""/Library/Python/2.7/site-packages/pandas/__init__.py"", line 6, in <module>\n    from . import hashtable, tslib, lib\n  File ""tslib.pyx"", line 31, in init pandas.tslib (pandas/tslib.c:48782)\nImportError: No module named dateutil.parser\n', 'import codecs \nfrom math import sqrt\nimport numpy as np\nimport pandas as pd\n\nusers = {""Angelica"": {""Blues Traveler"": 3.5, ""Broken Bells"": 2.0,\n                      ""Norah Jones"": 4.5, ""Phoenix"": 5.0,\n                      ""Slightly Stoopid"": 1.5,\n                      ""The Strokes"": 2.5, ""Vampire Weekend"": 2.0},\n\n         ""Bill"":{""Blues Traveler"": 2.0, ""Broken Bells"": 3.5,\n                 ""Deadmau5"": 4.0, ""Phoenix"": 2.0,\n                 ""Slightly Stoopid"": 3.5, ""Vampire Weekend"": 3.0},\n\n         ""Chan"": {""Blues Traveler"": 5.0, ""Broken Bells"": 1.0,\n                  ""Deadmau5"": 1.0, ""Norah Jones"": 3.0, ""Phoenix"": 5,\n                  ""Slightly Stoopid"": 1.0},\n\n         ""Dan"": {""Blues Traveler"": 3.0, ""Broken Bells"": 4.0,\n                 ""Deadmau5"": 4.5, ""Phoenix"": 3.0,\n                 ""Slightly Stoopid"": 4.5, ""The Strokes"": 4.0,\n                 ""Vampire Weekend"": 2.0},\n\n         ""Hailey"": {""Broken Bells"": 4.0, ""Deadmau5"": 1.0,\n                    ""Norah Jones"": 4.0, ""The Strokes"": 4.0,\n                    ""Vampire Weekend"": 1.0},\n\n         ""Jordyn"":  {""Broken Bells"": 4.5, ""Deadmau5"": 4.0,\n                     ""Norah Jones"": 5.0, ""Phoenix"": 5.0,\n                     ""Slightly Stoopid"": 4.5, ""The Strokes"": 4.0,\n                     ""Vampire Weekend"": 4.0},\n\n         ""Sam"": {""Blues Traveler"": 5.0, ""Broken Bells"": 2.0,\n                 ""Norah Jones"": 3.0, ""Phoenix"": 5.0,\n                 ""Slightly Stoopid"": 4.0, ""The Strokes"": 5.0},\n\n         ""Veronica"": {""Blues Traveler"": 3.0, ""Norah Jones"": 5.0,\n                      ""Phoenix"": 4.0, ""Slightly Stoopid"": 2.5,\n                      ""The Strokes"": 3.0}\n        }\n\n\n\nclass recommender:\n\n    def __init__(self, data, k=1, metric=\'pearson\', n=5):\n        """""" initialize recommender\n        currently, if data is dictionary the recommender is initialized\n        to it.\n        For all other data types of data, no initialization occurs\n        k is the k value for k nearest neighbor\n        metric is which distance formula to use\n        n is the maximum number of recommendations to make""""""\n        self.k = k\n        self.n = n\n        self.username2id = {}\n        self.userid2name = {}\n        self.productid2name = {}\n        # for some reason I want to save the name of the metric\n        self.metric = metric\n        if self.metric == \'pearson\':\n            self.fn = self.pearson\n        #\n        # if data is dictionary set recommender data to it\n        #\n        if type(data).__name__ == \'dict\':\n            self.data = data\n\n    def convertProductID2name(self, id):\n        """"""Given product id number return product name""""""\n        if id in self.productid2name:\n            return self.productid2name[id]\n        else:\n            return id\n\n\n    def userRatings(self, id, n):\n        """"""Return n top ratings for user with id""""""\n        print (""Ratings for "" + self.userid2name[id])\n        ratings = self.data[id]\n        print(len(ratings))\n        ratings = list(ratings.items())\n        ratings = [(self.convertProductID2name(k), v)\n                   for (k, v) in ratings]\n        # finally sort and return\n        ratings.sort(key=lambda artistTuple: artistTuple[1],\n                     reverse = True)\n        ratings = ratings[:n]\n        for rating in ratings:\n            print(""%s\\t%i"" % (rating[0], rating[1]))\n\n\n\n\n    def loadBookDB(self, path=\'\'):\n        """"""loads the BX book dataset. Path is where the BX files are\n        located""""""\n        self.data = {}\n        i = 0\n        #\n        # First load book ratings into self.data\n        #\n        f = codecs.open(path + ""BX-Book-Ratings.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #separate line into fields\n            fields = line.split(\';\')\n            user = fields[0].strip(\'""\')\n            book = fields[1].strip(\'""\')\n            rating = int(fields[2].strip().strip(\'""\'))\n            if user in self.data:\n                currentRatings = self.data[user]\n            else:\n                currentRatings = {}\n            currentRatings[book] = rating\n            self.data[user] = currentRatings\n        f.close()\n        #\n        # Now load books into self.productid2name\n        # Books contains isbn, title, and author among other fields\n        #\n        f = codecs.open(path + ""BX-Books.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #separate line into fields\n            fields = line.split(\';\')\n            isbn = fields[0].strip(\'""\')\n            title = fields[1].strip(\'""\')\n            author = fields[2].strip().strip(\'""\')\n            title = title + \' by \' + author\n            self.productid2name[isbn] = title\n        f.close()\n        #\n        #  Now load user info into both self.userid2name and\n        #  self.username2id\n        #\n        f = codecs.open(path + ""BX-Users.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #print(line)\n            #separate line into fields\n            fields = line.split(\';\')\n            userid = fields[0].strip(\'""\')\n            location = fields[1].strip(\'""\')\n            if len(fields) > 3:\n                age = fields[2].strip().strip(\'""\')\n            else:\n                age = \'NULL\'\n            if age != \'NULL\':\n                value = location + \'  (age: \' + age + \')\'\n            else:\n                value = location\n            self.userid2name[userid] = value\n            self.username2id[location] = userid\n        f.close()\n        print(i)\n\n\n    def pearson(self, rating1, rating2):\n        sum_xy = 0\n        sum_x = 0\n        sum_y = 0\n        sum_x2 = 0\n        sum_y2 = 0\n        n = 0\n        for key in rating1:\n            if key in rating2:\n                n += 1\n                x = rating1[key]\n                y = rating2[key]\n                sum_xy += x * y\n                sum_x += x\n                sum_y += y\n                sum_x2 += pow(x, 2)\n                sum_y2 += pow(y, 2)\n        if n == 0:\n            return 0\n        # now compute denominator\n        denominator = (sqrt(sum_x2 - pow(sum_x, 2) / n)\n                       * sqrt(sum_y2 - pow(sum_y, 2) / n))\n        if denominator == 0:\n            return 0\n        else:\n            return (sum_xy - (sum_x * sum_y) / n) / denominator\n\n\n    def computeNearestNeighbor(self, username):\n        """"""creates a sorted list of users based on their distance to\n        username""""""\n        distances = []\n        for instance in self.data:\n            if instance != username:\n                distance = self.fn(self.data[username],\n                                   self.data[instance])\n                distances.append((instance, distance))\n        # sort based on distance -- closest first\n        distances.sort(key=lambda artistTuple: artistTuple[1],\n                       reverse=True)\n        return distances\n\n    def recommend(self, user):\n       """"""Give list of recommendations""""""\n       recommendations = {}\n       # first get list of users  ordered by nearness\n       nearest = self.computeNearestNeighbor(user)\n       #\n       # now get the ratings for the user\n       #\n       userRatings = self.data[user]\n       #\n       # determine the total distance\n       totalDistance = 0.0\n       for i in range(self.k):\n          totalDistance += nearest[i][1]\n       # now iterate through the k nearest neighbors\n       # accumulating their ratings\n       for i in range(self.k):\n          # compute slice of pie \n          weight = nearest[i][1] / totalDistance\n          # get the name of the person\n          name = nearest[i][0]\n          # get the ratings for this person\n          neighborRatings = self.data[name]\n          # get the name of the person\n          # now find bands neighbor rated that user didn\'t\n          for artist in neighborRatings:\n             if not artist in userRatings:\n                if artist not in recommendations:\n                   recommendations[artist] = (neighborRatings[artist]\n                                              * weight)\n                else:\n                   recommendations[artist] = (recommendations[artist]\n                                              + neighborRatings[artist]\n                                              * weight)\n       # now make list from dictionary\n       recommendations = list(recommendations.items())\n       recommendations = [(self.convertProductID2name(k), v)\n                          for (k, v) in recommendations]\n       # finally sort and return\n       recommendations.sort(key=lambda artistTuple: artistTuple[1],\n                            reverse = True)\n       # Return the first n items\n       return recommendations[:self.n]\n\nr = recommender(users)\n# The author implementation\nr.loadBookDB(\'/Users/mona/Downloads/BX-Dump/\')\n\nratings = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Book-Ratings.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nbooks = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Books.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nusers = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Users.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\n\n\n\npivot_rating = ratings.pivot(index=\'User-ID\', columns=\'ISBN\', values=\'Book-Rating\')\n']"
333;3.0;0;20868394;;1;99;<python><pandas>;Changing a specific column name in pandas DataFrame;115075.0;"['DataFrame', ""import pandas as pd\nd = {\n         'one': [1, 2, 3, 4, 5],\n         'two': [9, 8, 7, 6, 5],\n         'three': ['a', 'b', 'c', 'd', 'e']\n    }\ndf = pd.DataFrame(d)\n"", ""names = df.columns.tolist()\nnames[names.index('two')] = 'new_name'\ndf.columns = names\n"", ""df.columns[df.columns.tolist().index('one')] = 'another_name'\n""]"
334;6.0;14;20906474;;1;104;<python><csv><pandas><concatenation>;Import multiple csv files into pandas and concatenate into one DataFrame;80289.0;"['import glob\nimport pandas as pd\n\n# get data file names\npath =r\'C:\\DRO\\DCL_rawdata_files\'\nfilenames = glob.glob(path + ""/*.csv"")\n\ndfs = []\nfor filename in filenames:\n    dfs.append(pd.read_csv(filename))\n\n# Concatenate all data into one DataFrame\nbig_frame = pd.concat(dfs, ignore_index=True)\n']"
335;3.0;2;20937538;;1;60;<python><python-2.7><pandas><ipython><dataframe>;How to display pandas DataFrame of floats using a format string for columns?;56902.0;"['print()', 'display()', ""df = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\nprint df\n\n         cost\nfoo   123.4567\nbar   234.5678\nbaz   345.6789\nquux  456.7890\n"", '         cost\nfoo   $123.46\nbar   $234.57\nbaz   $345.68\nquux  $456.79\n']"
336;1.0;0;20965046;;1;23;<python><pandas><dataframe><cumulative-sum>;Cumulative sum and percentage on column?;27193.0;['DataFrame', 'df', ' fruit    val1 val2\n0 orange    15    3\n1 apple     10   13\n2 mango     5    5 \n', 'val1', 'df_with_cumsum', ' fruit    val1 val2   cum_sum    cum_perc\n0 orange    15    3    15          50.00\n1 apple     10   13    25          83.33\n2 mango     5    5     30          100.00\n', 'df.cumsum()']
337;1.0;0;20970279;;1;24;<python><pandas>;how to do a left,right and mid of a string in a pandas dataframe;23529.0;"[""data = {'state': ['Auckland', 'Otago', 'Wellington', 'Dunedin', 'Hamilton'],\n'year': [2000, 2001, 2002, 2001, 2002],\n'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\ndf = pd.DataFrame(data)\n\nprint df\n\n     pop       state  year\n 0  1.5    Auckland  2000\n 1  1.7       Otago  2001\n 2  3.6  Wellington  2002\n 3  2.4     Dunedin  2001\n 4  2.9    Hamilton  2002\n"", '    pop       state     year  StateInitial\n 0  1.5       Auckland    2000     Au\n 1  1.7       Otago       2001     Ot\n 2  3.6       Wellington  2002     We\n 3  2.4       Dunedin     2001     Du\n 4  2.9       Hamilton    2002     Ha\n']"
338;2.0;0;20995196;;1;23;<python><pandas><sum>;Python Pandas counting and summing specific conditions;38942.0;['sumif', '(df.map(lambda x: condition), or df.size())', '.sum()', 'countif', '(groupby functions', '.count())']
339;1.0;0;21018654;;1;51;<python><types><pandas>;Strings in a DataFrame, but dtype is object;21985.0;"[""<class 'pandas.core.frame.DataFrame'>\nInt64Index: 56992 entries, 0 to 56991\nData columns (total 7 columns):\nid            56992  non-null values\nattr1         56992  non-null values\nattr2         56992  non-null values\nattr3         56992  non-null values\nattr4         56992  non-null values\nattr5         56992  non-null values\nattr6         56992  non-null values\ndtypes: int64(2), object(5)\n"", 'dtype object', 'for c in df.columns:\n    if df[c].dtype == object:\n        print ""convert "", df[c].name, "" to string""\n        df[c] = df[c].astype(str)\n', 'df[""attr2""]', 'dtype object', 'type(df[""attr2""].ix[0]', 'str', 'int64', 'float64', 'object', 'dtype str', 'str', 'object']"
340;4.0;0;21104592;;1;44;<python><json><google-maps><pandas>;JSON to pandas DataFrame;64504.0;"[""from urllib2 import Request, urlopen\nimport json\n\npath1 = '42.974049,-81.205203|42.974298,-81.195755'\nrequest=Request('http://maps.googleapis.com/maps/api/elevation/json?locations='+path1+'&sensor=false')\nresponse = urlopen(request)\nelevations = response.read()\n"", 'elevations.splitlines()\n\n[\'{\',\n \'   ""results"" : [\',\n \'      {\',\n \'         ""elevation"" : 243.3462677001953,\',\n \'         ""location"" : {\',\n \'            ""lat"" : 42.974049,\',\n \'            ""lng"" : -81.205203\',\n \'         },\',\n \'         ""resolution"" : 19.08790397644043\',\n \'      },\',\n \'      {\',\n \'         ""elevation"" : 244.1318664550781,\',\n \'         ""location"" : {\',\n \'            ""lat"" : 42.974298,\',\n \'            ""lng"" : -81.19575500000001\',\n \'         },\',\n \'         ""resolution"" : 19.08790397644043\',\n \'      }\',\n \'   ],\',\n \'   ""status"" : ""OK""\',\n \'}\']\n', 'pd.read_json(elevations)\n', ""data = json.loads(elevations)\nlat,lng,el = [],[],[]\nfor result in data['results']:\n    lat.append(result[u'location'][u'lat'])\n    lng.append(result[u'location'][u'lng'])\n    el.append(result[u'elevation'])\ndf = pd.DataFrame([lat,lng,el]).T\n""]"
341;1.0;3;21137150;;1;36;<python><pandas><floating-point><scientific-notation><numeric-format>;Format / Suppress Scientific Notation from Python Pandas Aggregation Results;25062.0;"[""df1.groupby('dept')['data1'].sum()\n\ndept\nvalue1       1.192433e+08\nvalue2       1.293066e+08\nvalue3       1.077142e+08\n"", 'sum_sales_dept.astype(str)\n']"
342;5.0;4;21197774;;1;48;<python><pandas>;Assign pandas dataframe column dtypes;50855.0;"['dtype', 'pd.Dataframe', 'pd.read_csv', ""import pandas as pd\nprint pd.DataFrame([['a','1'],['b','2']],\n                   dtype={'x':'object','y':'int'},\n                   columns=['x','y'])\n"", 'ValueError: entry not a 2- or 3- tuple\n', 'astype', ""dtypes = {'x':'object','y':'int'}\nmydata = pd.DataFrame([['a','1'],['b','2']],\n                      columns=['x','y'])\nfor c in mydata.columns:\n    mydata[c] = mydata[c].astype(dtypes[c])\nprint mydata['y'].dtype   #=> int64\n""]"
343;3.0;0;21269399;;1;24;<python><csv><datetime><pandas><dataframe>;datetime dtypes in pandas read_csv;28062.0;"[""headers = ['col1', 'col2', 'col3', 'col4']\ndtypes = ['datetime', 'datetime', 'str', 'float']\npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes)\n"", 'TypeError: data type ""datetime"" not understood\n']"
344;2.0;0;21285380;;1;31;<string><python-3.x><pandas><find>;Pandas: find column whose name contains a specific string;28697.0;"[""'spike'"", ""'spike-2'"", ""'hey spike'"", ""'spiked-in'"", ""'spike'"", ""df['name']"", 'df[name]']"
345;3.0;7;21287624;;1;46;<python><pandas><na>;Convert Pandas column containing NaNs to dtype `int`;42262.0;"['id', 'int', 'id', 'id', 'df= pd.read_csv(""data.csv"", dtype={\'id\': int}) \nerror: Integer column has NA values\n', 'df= pd.read_csv(""data.csv"") \ndf[[\'id\']] = df[[\'id\']].astype(int)\nerror: Cannot convert NA to integer\n']"
346;5.0;2;21291259;;1;58;<python><pandas>;Convert floats to ints in Pandas?;84803.0;[]
347;2.0;0;21319929;;1;26;<python><pandas>;How to determine whether a Pandas Column contains a particular value;42502.0;"[""if x in df['id']"", ""43 in df['id']"", 'True', ""df[df['id'] == 43]""]"
348;4.0;0;21360361;;1;43;<python><matplotlib><pandas><ipython><ipython-notebook>;how to dynamically update a plot in a loop in ipython notebook (within one cell);30112.0;"['--pylab=inline', ""i = pd.date_range('2013-1-1',periods=100,freq='s')\nwhile True:\n    plot(pd.Series(data=np.random.randn(100), index=i))\n    #pd.Series(data=np.random.randn(100), index=i).plot() also tried this one\n    time.sleep(5)\n"", 'plot()', 'plot()']"
349;1.0;3;21415661;;1;32;<python><pandas><boolean-logic>;Logic operator for boolean indexing in Pandas;38894.0;"[""a[(a['some_column']==some_number) & (a['some_other_column']==some_other_number)]\n"", ""a[(a['some_column']==some_number) and (a['some_other_column']==some_other_number)]\n"", ""a=pd.DataFrame({'x':[1,1],'y':[10,20]})\n\nIn: a[(a['x']==1)&(a['y']==10)]\nOut:    x   y\n     0  1  10\n\nIn: a[(a['x']==1) and (a['y']==10)]\nOut: ValueError: The truth value of an array with more than one element is ambiguous.     Use a.any() or a.all()\n""]"
350;2.0;0;21441259;;1;33;<python><group-by><pandas>;Pandas Groupby Range of Values;13283.0;"['groupby', 'B', '0.155', 'B', '0, -0.155, 0.155 - 0.31 ...', ""import numpy as np\nimport pandas as pd\ndf=pd.DataFrame({'A':np.random.random(20),'B':np.random.random(20)})\n\n     A         B\n0  0.383493  0.250785\n1  0.572949  0.139555\n2  0.652391  0.401983\n3  0.214145  0.696935\n4  0.848551  0.516692\n"", 'groupby', 'A']"
351;5.0;0;21487329;;1;62;<python><matplotlib><pandas>;Add x and y labels to a pandas plot;62710.0;"[""import pandas as pd\nvalues = [[1,2], [2,5]]\ndf2 = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])\ndf2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')\n""]"
352;2.0;0;21606987;;1;26;<pandas>;How can I strip the whitespace from Pandas DataFrame headers?;14508.0;"['df.columns', ""Index(['Year', 'Month ', 'Value'])"", 'df[""Month""]']"
353;2.0;1;21608228;;1;26;<python><replace><pandas><conditional>;Conditional Replace Pandas;30186.0;['df[df.my_channel > 20000].my_channel = 0\n', 'df2 = df.my_channel \n\ndf2[df2 > 20000] = 0\n']
354;4.0;0;21654635;;1;42;<python><matplotlib><pandas>;Scatter plots in Pandas/Pyplot: How to plot by category;44949.0;"[""import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range('2010-01-01', freq = 'M', periods = 10), columns = ('one', 'two', 'three'))\ndf['key1'] = (4,4,4,6,6,6,8,8,8,8)\nfig1 = plt.figure(1)\nax1 = fig1.add_subplot(111)\nax1.scatter(df['one'], df['two'], marker = 'o', c = df['key1'], alpha = 0.8)\nplt.show()\n""]"
355;2.0;0;21733893;;1;22;<python><lambda>;Pandas dataframe add a field based on multiple if statements;32357.0;"[""if age < 18 then 'under 18' elif age < 40 then 'under 40' else '>40'"", ""import pandas as pd\nimport numpy as n\n\nd = {'Age' : pd.Series([36., 42., 6., 66., 38.]) }\n\ndf = pd.DataFrame(d)\n\ndf['Age_Group'] =  df['Age'].map(lambda x: '<18' if x < 19 else '>18')\n\nprint(df)\n""]"
356;1.0;0;21786490;;1;30;<python><sql><merge><pandas>;Pandas left outer join multiple dataframes on multiple columns;45960.0;['df1: \nYear    Week    Colour    Val1 \n2014       A       Red      50\n2014       B       Red      60\n2014       B     Black      70\n2014       C       Red      10\n2014       D     Green      20\n\ndf2:\nYear    Week    Colour    Val2\n2014       A     Black      30\n2014       B     Black     100\n2014       C     Green      50\n2014       C       Red      20\n2014       D       Red      40\n\ndf3:\nYear    Week    Colour    Val3\n2013       B       Red      60\n2013       C     Black      80\n2013       B     Black      10\n2013       D     Green      20\n2013       D       Red      50\n', 'SELECT df1.*, df2.Val2, df3.Val3\nFROM df1\n  LEFT OUTER JOIN df2\n    ON df1.Year = df2.Year\n    AND df1.Week = df2.Week\n    AND df1.Colour = df2.Colour\n  LEFT OUTER JOIN df3\n    ON df1.Week = df3.Week\n    AND df1.Colour = df3.Colour\n', 'Year    Week    Colour    Val1    Val2    Val3\n2014       A       Red      50    Null    Null\n2014       B       Red      60    Null      60\n2014       B     Black      70     100    Null\n2014       C       Red      10      20    Null\n2014       D     Green      20    Null    Null\n']
357;2.0;0;21800169;;1;66;<python><indexing><pandas>;Python Pandas: Get index of rows which column matches certain value;149908.0;"[""for i in range(100,3000):\n    if df.iloc[i]['BoolCol']== True:\n         print i,df.iloc[i]['BoolCol']\n"", ""df[df['BoolCol'] == True].index.tolist()\n"", ""df.iloc[i]['BoolCol']\n""]"
358;2.0;0;22005911;;1;42;<python><numpy><pandas>;Convert Columns to String in Pandas;74333.0;"['(Pdb) pp total_rows\n     ColumnID  RespondentCount\n0          -1                2\n1  3030096843                1\n2  3030096845                1\n', ""total_data = total_rows.pivot_table(cols=['ColumnID'])\n\n(Pdb) pp total_data\nColumnID         -1            3030096843   3030096845\nRespondentCount            2            1            1\n\n[1 rows x 3 columns]\n\n\ntotal_rows.pivot_table(cols=['ColumnID']).to_dict('records')[0]\n\n{3030096843: 1, 3030096845: 1, -1: 2}\n"", ""{'3030096843': 1, '3030096845': 1, -1: 2}\n""]"
359;5.0;0;22084338;;1;28;<python><dictionary><pandas>;Pandas DataFrame performance;13142.0;"[""import timeit\n\nsetup = '''\nimport numpy, pandas\ndf = pandas.DataFrame(numpy.zeros(shape=[10, 10]))\ndictionary = df.to_dict()\n'''\n\nf = ['value = dictionary[5][5]', 'value = df.loc[5, 5]', 'value = df.iloc[5, 5]']\n\nfor func in f:\n    print func\n    print min(timeit.Timer(func, setup).repeat(3, 100000))\n""]"
360;2.0;1;22086116;;1;31;<python><filter><pandas>;how do you filter pandas dataframes by multiple columns;29798.0;"[""males = df[df[Gender]=='Male']\n"", 'if A = ""Male"" and if B = ""2014"" then \n', 'for y in year:\n\nfor g in gender:\n\ndf = .....\n']"
361;1.0;2;22127569;;1;24;<python><pandas>;Opposite of melt in python pandas;3241.0;"[""import pandas as pd\n\nfrom StringIO import StringIO\n\norigin = pd.read_table(StringIO('''label    type    value\nx   a   1\nx   b   2\nx   c   3\ny   a   4\ny   b   5\ny   c   6\nz   a   7\nz   b   8\nz   c   9'''))\n\norigin\nOut[5]: \n  label type  value\n0     x    a      1\n1     x    b      2\n2     x    c      3\n3     y    a      4\n4     y    b      5\n5     y    c      6\n6     z    a      7\n7     z    b      8\n8     z    c      9\n"", '    label   a   b   c\n        x   1   2   3\n        y   4   5   6\n        z   7   8   9\n']"
362;1.0;0;22137723;;1;23;<python><pandas>;Convert number strings with commas in pandas DataFrame to float;15553.0;"[""a = [['1,200', '4,200'], ['7,000', '-0.03'], [ '5', '0']]\ndf=pandas.DataFrame(a)\n"", 'df[0].apply(locale.atof)\n', 'df.apply(locale.atof)\n', 'df[0:1].apply(locale.atof)\n']"
363;6.0;0;22149584;;1;86;<python><pandas>;What does axis in pandas mean?;39852.0;"[""import pandas as pd\nimport numpy as np\n\ndff = pd.DataFrame(np.random.randn(1,2),columns=list('AB'))\n"", '+------------+---------+--------+\n|            |  A      |  B     |\n+------------+---------+---------\n|      0     | 0.626386| 1.52325|\n+------------+---------+--------+\n', 'dff.mean(axis=1)\n', '0    1.074821\ndtype: float64\n', 'A    0.626386\nB    1.523255\ndtype: float64\n']"
364;2.0;4;22180993;;1;22;<python><pandas><flask>;Pandas Dataframe display on a webpage;13560.0;"['@app.route(\'/analysis/<filename>\')\ndef analysis(filename):\n    x = pd.DataFrame(np.random.randn(20, 5))\n    return render_template(""analysis.html"", name=filename, data=x)\n', '{% extends ""base.html"" %}\n{% block content %}\n<h1>{{name}}</h1>\n{{data}}\n{% endblock %}\n', 'data.to_html()', 'data.to_string()']"
365;2.0;0;22211737;;1;27;<python><pandas>;Python, pandas: how to sort dataframe by index;32238.0;"[""import pandas as pd\ndf = pd.DataFrame([1, 1, 1, 1, 1], index=[100, 29, 234, 1, 150], columns=['A'])\n""]"
366;3.0;3;22219004;;1;42;<python><pandas>;grouping rows in list in pandas groupby;24488.0;['A 1\nA 2\nB 5\nB 5\nB 4\nC 6\n', 'A [1,2]\nB [5,5,4]\nC [6]\n']
367;4.0;0;22233488;;1;67;<python><pandas>;Pandas: drop a level from a multi-level column index?;36471.0;"['>>> cols = pd.MultiIndex.from_tuples([(""a"", ""b""), (""a"", ""c"")])\n>>> pd.DataFrame([[1,2], [3,4]], columns=cols)\n']"
368;1.0;1;22235245;;1;21;<python><csv><pandas><dataframe>;Calculate summary statistics of columns in dataframe;35787.0;['shopper_num,is_martian,number_of_items,count_pineapples,birth_country,tranpsortation_method\n1,FALSE,0,0,MX,\n2,FALSE,1,0,MX,\n3,FALSE,0,0,MX,\n4,FALSE,22,0,MX,\n5,FALSE,0,0,MX,\n6,FALSE,0,0,MX,\n7,FALSE,5,0,MX,\n8,FALSE,0,0,MX,\n9,FALSE,4,0,MX,\n10,FALSE,2,0,MX,\n11,FALSE,0,0,MX,\n12,FALSE,13,0,MX,\n13,FALSE,0,0,CA,\n14,FALSE,0,0,US,\n', 'columnname, max, min, median,\n\nis_martian, NA, NA, FALSE\n']
369;2.0;7;22341271;;1;60;<python><list><pandas>;get list from pandas dataframe column;117410.0;['cluster load_date   budget  actual  fixed_price\nA   1/1/2014    1000    4000    Y\nA   2/1/2014    12000   10000   Y\nA   3/1/2014    36000   2000    Y\nB   4/1/2014    15000   10000   N\nB   4/1/2014    12000   11500   N\nB   4/1/2014    90000   11000   N\nC   7/1/2014    22000   18000   N\nC   8/1/2014    30000   28960   N\nC   9/1/2014    53000   51200   N\n', 'list = [], list[column1] or list[df.ix(row1)]\n']
370;9.0;7;22391433;;1;51;<python><pandas>;count the frequency that a value occurs in a dataframe column;98963.0;['|category|\ncat a\ncat b\ncat a\n', 'category | freq |\ncat a       2\ncat b       1\n']
371;1.0;3;22403469;;1;21;<python><datetime><pandas>;Locate first and last non NaN values in a Pandas DataFrame;9357.0;['DataFrame', 'NaN']
372;5.0;0;22470690;;1;65;<python><pandas>;get list of pandas dataframe columns based on data type;133397.0;"['1. NAME                                     object\n2. On_Time                                      object\n3. On_Budget                                    object\n4. %actual_hr                                  float64\n5. Baseline Start Date                  datetime64[ns]\n6. Forecast Start Date                  datetime64[ns] \n', 'For c in col_list: if c.dtype = ""Something""\nlist[]\nList.append(c)?\n']"
373;4.0;0;22483588;;1;38;<python><matplotlib><pandas>;How can I plot separate Pandas DataFrames as subplots?;34397.0;['df.plot()']
374;4.0;3;22543208;;1;21;<python><matplotlib><pandas><ipython>;ggplot styles in Python;12579.0;[]
375;2.0;0;22546425;;1;28;<python><pandas>;using pandas to select rows conditional on multiple equivalencies;35625.0;"[""SELECT * FROM df WHERE column1 = 'a' OR column2 = 'b' OR column3 = 'c' etc...\n"", ""foo = df.ix[df['column']==value]\n""]"
376;1.0;0;22551403;;1;38;<python><pandas><dataframe>;Python pandas Filtering out nan from a data selection of a column of strings;55836.0;"['groupby', 'NaN', ""import pandas as pd\nimport numpy as np\n\n\ndf = pd.DataFrame({'movie': ['thg', 'thg', 'mol', 'mol', 'lob', 'lob'],\n                  'rating': [3., 4., 5., np.nan, np.nan, np.nan],\n                  'name': ['John', np.nan, 'N/A', 'Graham', np.nan, np.nan]})\n\nnbs = df['name'].str.extract('^(N/A|NA|na|n/a)')\nnms=df[(df['name'] != nbs) ]\n"", '>>> nms\n  movie    name  rating\n0   thg    John       3\n1   thg     NaN       4\n3   mol  Graham     NaN\n4   lob     NaN     NaN\n5   lob     NaN     NaN\n', '  movie    name  rating\n0   thg    John       3\n3   mol  Graham     NaN\n', '~np.isnan']"
377;4.0;0;22588316;;1;25;<python><regex><pandas>;pandas applying regex to replace values;22689.0;['$40,000*\n$40000 conditions attached\n', '[0-9]+\n']
378;2.0;1;22591174;;1;26;<python><pandas><boolean-logic>;pandas: multiple conditions while indexing data frame - unexpected behavior;47480.0;"[""import pandas as pd\n\ndf = pd.DataFrame({'a': range(5), 'b': range(5) })\n\n# let's insert some -1 values\ndf['a'][1] = -1\ndf['b'][1] = -1\ndf['a'][3] = -1\ndf['b'][4] = -1\n\ndf1 = df[(df.a != -1) & (df.b != -1)]\ndf2 = df[(df.a != -1) | (df.b != -1)]\n\nprint pd.concat([df, df1, df2], axis=1,\n                keys = [ 'original df', 'using AND (&)', 'using OR (|)',])\n"", '      original df      using AND (&)      using OR (|)    \n             a  b              a   b             a   b\n0            0  0              0   0             0   0\n1           -1 -1            NaN NaN           NaN NaN\n2            2  2              2   2             2   2\n3           -1  3            NaN NaN            -1   3\n4            4 -1            NaN NaN             4  -1\n\n[5 rows x 6 columns]\n', 'AND', '-1', 'OR', '-1']"
379;1.0;0;22604564;;1;76;<python><csv><pandas>;How to create a Pandas DataFrame from String;32697.0;"['DataFrame', 'TESTDATA=""""""col1;col2;col3\n1;4.4;99\n2;4.5;200\n3;4.7;65\n4;3.2;140\n""""""\n', 'DataFrame']"
380;7.0;0;22649693;;1;22;<python><pandas>;Drop rows with all zeros in pandas data frame;25347.0;['pandas', 'dropna()', 'NA', 'P   kt  b   tt  mky depth\n1   0   0   0   0   0\n2   0   0   0   0   0\n3   0   0   0   0   0\n4   0   0   0   0   0\n5   1.1 3   4.5 2.3 9.0\n']
381;4.0;1;22676081;;1;45;<python><pandas>;Pandas - The difference between join and merge;22067.0;"[""left = pd.DataFrame({'key1': ['foo', 'bar'], 'lval': [1, 2]})\n\nright = pd.DataFrame({'key2': ['foo', 'bar'], 'rval': [4, 5]})\n"", ""pd.merge(left, right, left_on='key1', right_on='key2')\n"", '    key1    lval    key2    rval\n0   foo     1       foo     4\n1   bar     2       bar     5\n', ""left.join(right, on=['key1', 'key2'])\n"", '//anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _validate_specification(self)\n    406             if self.right_index:\n    407                 if not ((len(self.left_on) == self.right.index.nlevels)):\n--> 408                     raise AssertionError()\n    409                 self.right_on = [None] * n\n    410         elif self.right_on is not None:\n\nAssertionError: \n']"
382;4.0;3;22691010;;1;25;<python><pandas>;How to print a groupby object;15872.0;"[""import pandas as pd\ndf = pd.DataFrame({'A': ['one', 'one', 'two', 'three', 'three', 'one'], 'B': range(6)})\nprint df\n\n       A  B\n0    one  0\n1    one  1\n2    two  2\n3  three  3\n4  three  4\n5    one  5\n"", ""print df.groupby('A')\n\n<pandas.core.groupby.DataFrameGroupBy object at 0x05416E90>\n"", ""print df.groupby('A').head()\n"", '             A  B\nA                \none   0    one  0\n      1    one  1\ntwo   2    two  2\nthree 3  three  3\n      4  three  4\none   5    one  5\n', '             A  B\nA                \none   0    one  0\n      1    one  1\n      5    one  5\ntwo   2    two  2\nthree 3  three  3\n      4  three  4\n']"
383;3.0;1;22697773;;1;31;<python><pandas>;how to check the dtype of a column in python pandas;62516.0;"['allc = list((agg.loc[:, (agg.dtypes==np.float64)|(agg.dtypes==np.int)]).columns)\nfor y in allc:\n    treat_numeric(agg[y])    \n\nallc = list((agg.loc[:, (agg.dtypes!=np.float64)&(agg.dtypes!=np.int)]).columns)\nfor y in allc:\n    treat_str(agg[y])    \n', ""for y in agg.columns:\n    if(dtype(agg[y]) == 'string'):\n          treat_str(agg[y])\n    elif(dtype(agg[y]) != 'string'):\n          treat_numeric(agg[y])\n""]"
384;4.0;0;22787209;;1;24;<python><pandas><matplotlib><plot><seaborn>;How to have clusters of stacked bars with python (Pandas);7938.0;"['In [1]: df1=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])\n\nIn [2]: df2=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])\n\nIn [3]: df1\nOut[3]: \n          I         J\nA  0.675616  0.177597\nB  0.675693  0.598682\nC  0.631376  0.598966\nD  0.229858  0.378817\n\nIn [4]: df2\nOut[4]: \n          I         J\nA  0.939620  0.984616\nB  0.314818  0.456252\nC  0.630907  0.656341\nD  0.020994  0.538303\n', 'In [5]: ax = df1.plot(kind=""bar"", stacked=True)\n\nIn [5]: ax2 = df2.plot(kind=""bar"", stacked=True, ax = ax)\n', 'pd.concat(dict(df1 = df1, df2 = df2),axis = 1).plot(kind=""bar"", stacked=True)\n', ' pd.concat(dict(df1 = df1, df2 = df2),axis = 0).plot(kind=""bar"", stacked=True)\n', 'df1 df2    df1 df2\n_______    _______ ...\n   A          B\n']"
385;4.0;0;22898824;;1;23;<python><datetime><pandas><filtering><dataframe>;filtering pandas dataframes on dates;50394.0;[]
386;2.0;0;22923775;;1;21;<python><datetime><pandas>;Calculate Pandas DataFrame Time Difference Between Two Columns in Hours and Minutes;31742.0;"[""df['diff'] = df['todate'] - df['fromdate']\n"", '2014-01-24 13:03:12.050000,2014-01-26 23:41:21.870000,""2 days, 10:38:09.820000""\n2014-01-27 11:57:18.240000,2014-01-27 15:38:22.540000,03:41:04.300000\n2014-01-23 10:07:47.660000,2014-01-23 18:50:41.420000,08:42:53.760000\n']"
387;4.0;3;22963263;;1;22;<python><pandas><dataframe>;Creating a zero-filled pandas data frame;29486.0;['zero_data = np.zeros(shape=(len(data),len(feature_list)))\nd = pd.DataFrame(zero_data, columns=feature_list)\n']
388;2.0;0;23142967;;1;26;<pandas><dataframe><series>;Adding a column thats result of difference in consecutive rows in pandas;15505.0;['    A   B\n0   a   b\n1   c   d\n2   e   f \n3   g   h\n', '    A   B   dA\n0   a   b  (a-c)\n1   c   d  (c-e)\n2   e   f  (e-g)\n3   g   h   Nan\n']
389;6.0;1;23199796;;1;51;<python><pandas><filtering><dataframe><outliers>;Detect and exclude outliers in Pandas dataframe;40223.0;[]
390;1.0;1;23282130;;1;25;<python><pandas><pca><scientific-computing><principal-components>;Principal components analysis using pandas dataframe;13077.0;[]
391;1.0;0;23296282;;1;39;<python><pandas><indexing><dataframe><slice>;What rules does Pandas use to generate a view vs a copy?;8967.0;"[""df = pd.DataFrame(np.random.randn(8,8), columns=list('ABCDEFGH'), index=[1, 2, 3, 4, 5, 6, 7, 8])\n"", 'query', ""foo = df.query('2 < index <= 5')\nfoo.loc[:,'E'] = 40\n"", 'df', 'df.iloc[3] = 70\n', ""df.ix[1,'B':'E'] = 222\n"", 'df', 'df[df.C <= df.B]  = 7654321\n', 'df', ""df[df.C <= df.B].ix[:,'B':'E']\n""]"
392;6.0;1;23307301;;1;33;<python><pandas>;Pandas: Replacing column values in dataframe;84170.0;"[""w['female']['female']='1'\nw['female']['male']='0' \n"", ""if w['female'] =='female':\n    w['female'] = '1';\nelse:\n    w['female'] = '0';\n""]"
393;2.0;0;23317342;;1;26;<python><split><pandas>;Pandas Dataframe: split column into multiple columns, right-align inconsistent cell entries;31355.0;"['0                 HUN\n1                 ESP\n2                 GBR\n3                 ESP\n4                 FRA\n5             ID, USA\n6             GA, USA\n7    Hoboken, NJ, USA\n8             NJ, USA\n9                 AUS\n', ""location_df = df['City, State, Country'].apply(lambda x: pd.Series(x.split(',')))\n"", '     0       1       2\n0    HUN     NaN     NaN\n1    ESP     NaN     NaN\n2    GBR     NaN     NaN\n3    ESP     NaN     NaN\n4    FRA     NaN     NaN\n5    ID      USA     NaN\n6    GA      USA     NaN\n7    Hoboken  NJ     USA\n8    NJ      USA     NaN\n9    AUS     NaN     NaN\n']"
394;2.0;3;23330654;;1;38;<python><pandas><updates><dataframe>;Update a dataframe in pandas while iterating row by row;31600.0;"['           date      exer exp     ifor         mat  \n1092  2014-03-17  American   M  528.205  2014-04-19 \n1093  2014-03-17  American   M  528.205  2014-04-19 \n1094  2014-03-17  American   M  528.205  2014-04-19 \n1095  2014-03-17  American   M  528.205  2014-04-19    \n1096  2014-03-17  American   M  528.205  2014-05-17 \n', 'ifor', ""for i, row in df.iterrows():\n    if <something>:\n        row['ifor'] = x\n    else:\n        row['ifor'] = y\n\n    df.ix[i]['ifor'] = x\n""]"
395;3.0;0;23377108;;1;29;<python><pandas>;Pandas percentage of total with groupby;34307.0;"[""df = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,\n                   'office_id': range(1, 7) * 2,\n                   'sales': [np.random.randint(100000, 999999)\n                             for _ in range(12)]})\n\ndf.groupby(['state', 'office_id']).agg({'sales': 'sum'})\n"", '                  sales\nstate office_id        \nAZ    2          839507\n      4          373917\n      6          347225\nCA    1          798585\n      3          890850\n      5          454423\nCO    1          819975\n      3          202969\n      5          614011\nWA    2          163942\n      4          369858\n      6          959285\n', 'state', 'groupby', 'sales', 'state']"
396;4.0;1;23549231;;1;41;<python><pandas><ipython>;Check if a value exists in pandas dataframe index;45304.0;"['True', 'False', 'df', ""df = pandas.DataFrame({'test':[1,2,3,4]}, index=['a','b','c','d'])\n\ndf.loc['g']  # (should give False)\n"", ""sum(df.index == 'g')\n""]"
397;5.0;3;23668427;;1;34;<python><pandas>;pandas joining multiple dataframes on columns;46068.0;['join()']
398;4.0;1;23748995;;1;43;<python><pandas><tolist>;Pandas DataFrame to list;103159.0;"['import pandas as pd\n\ntst = pd.read_csv(\'C:\\\\SomeCSV.csv\')\n\nlookupValue = tst[\'SomeCol\'] == ""SomeValue""\nID = tst[lookupValue][[\'SomeCol\']]\n#How To convert ID to a list\n']"
399;1.0;0;23853553;;1;23;<python><pandas>;Python Pandas: How to read only first n rows of CSV files in?;18338.0;[]
400;1.0;5;24039023;;1;32;<python><pandas>;add column with constant value to pandas dataframe;32111.0;"[""df['new'] = pd.Series([0 for x in range(len(df.index))])\n"", ""df['new'] = 0 \n""]"
401;1.0;2;24041436;;1;21;<python><pandas>;set multi index of an existing data frame in pandas;21648.0;"['DataFrame', '  Emp1    Empl2           date       Company\n0    0        0     2012-05-01         apple\n1    0        1     2012-05-29         apple\n2    0        1     2013-05-02         apple\n3    0        1     2013-11-22         apple\n18   1        0     2011-09-09        google\n19   1        0     2012-02-02        google\n20   1        0     2012-11-26        google\n21   1        0     2013-05-11        google\n', 'MultiIndex', 'DataFrame', ""df.set_index(['Company', 'date'], inplace=True)"", 'df = pd.DataFrame()\nfor c in company_list:\n        row = pd.DataFrame([dict(company = \'%s\' %s, date = datetime.date(2012, 05, 01))])\n        df = df.append(row, ignore_index = True)\n        for e in emp_list:\n            dataset  = pd.read_sql(""select company, emp_name, date(date), count(*) from company_table where  = \'""+s+""\' and emp_name = \'""+b+""\' group by company, date, name LIMIT 5 "", con)\n                if len(dataset) == 0:\n                row = pd.DataFrame([dict(sitename=\'%s\' %s, name = \'%s\' %b, date = datetime.date(2012, 05, 01), count = np.nan)])\n                dataset = dataset.append(row, ignore_index=True)\n            dataset = dataset.rename(columns = {\'count\': \'%s\' %b})\n            dataset = dataset.groupby([\'company\', \'date\', \'emp_name\'], as_index = False).sum()\n\n            dataset = dataset.drop(\'emp_name\', 1)\n            df = pd.merge(df, dataset, how = \'\')\n            df = df.sort(\'date\', ascending = True)\n            df.fillna(0, inplace = True)\n\ndf.set_index([\'Company\', \'date\'], inplace=True)            \nprint df\n', 'DataFrame', 'None']"
402;1.0;0;24082784;;1;27;<python><datetime><pandas>;pandas dataframe groupby datetime month;26219.0;"['string,date,number\na string,2/5/11 9:16am,1.0\na string,3/5/11 10:44pm,2.0\na string,4/22/11 12:07pm,3.0\na string,4/22/11 12:10pm,4.0\na string,4/29/11 11:59am,1.0\na string,5/2/11 1:41pm,2.0\na string,5/2/11 2:02pm,3.0\na string,5/2/11 2:56pm,4.0\na string,5/2/11 3:00pm,5.0\na string,5/2/14 3:02pm,6.0\na string,5/2/14 3:18pm,7.0\n', ""b=pd.read_csv('b.dat')\nb['date']=pd.to_datetime(b['date'],format='%m/%d/%y %I:%M%p')\n"", ""b.index=b['date']\n"", 'b.index.month\n']"
403;12.0;0;24147278;;1;97;<python><python-2.7><pandas><dataframe>;How do I create test and train samples from one dataframe with pandas?;81694.0;[]
404;5.0;0;24193174;;1;26;<python><matplotlib><pandas>;Reset color cycle in Matplotlib;10416.0;['alpha=1', 'linewidth=1', 'alpha=0.25', 'linewidth=5', 'fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\nfor c in with_transaction_frame.columns:\n    ax.plot(with_transaction_frame[c], label=c, alpha=1, linewidth=1)\n\n****SOME MAGIC GOES HERE TO RESET THE COLOR CYCLE\n\nfor c in no_transaction_frame.columns:\n    ax.plot(no_transaction_frame[c], label=c, alpha=0.25, linewidth=5)\n\nax.legend()\n']
405;1.0;0;24216425;;1;28;<python><pandas>;Adding a new pandas column with mapped value from a dictionary;15904.0;"['import pandas as pd\nequiv = {7001:1, 8001:2, 9001:3}\ndf = pd.DataFrame( {""A"": [7001, 8001, 9001]} )\ndf[""B""] = equiv(df[""A""])\nprint(df)\n', '      A   B\n0  7001   1\n1  8001   2\n2  9001   3\n', 'df[""B""] = df[""A""].map(lambda x:equiv[x])\n']"
406;4.0;2;24251219;;1;81;<python><parsing><numpy><pandas><dataframe>;Pandas read_csv low_memory and dtype options;53257.0;"[""df = pd.read_csv('somefile.csv')\n"", 'dtype', 'low_memory', 'False']"
407;3.0;2;24284342;;1;27;<python><pandas>;Insert a row to pandas dataframe;87343.0;"['s1 = pd.Series([5, 6, 7])\ns2 = pd.Series([7, 8, 9])\n\ndf = pd.DataFrame([list(s1), list(s2)],  columns =  [""A"", ""B"", ""C""])\n\n   A  B  C\n0  5  6  7\n1  7  8  9\n\n[2 rows x 3 columns]\n', '   A  B  C\n0  2  3  4\n1  5  6  7\n2  7  8  9\n']"
408;9.0;2;24458645;;1;67;<python><pandas><scikit-learn>;Label encoding across multiple columns in scikit-learn;27993.0;"['LabelEncoder', 'DataFrame', 'LabelEncoder', 'LabelEncoder', 'DataFrame', 'LabelEncoder', 'import pandas\nfrom sklearn import preprocessing \n\ndf = pandas.DataFrame({\'pets\':[\'cat\', \'dog\', \'cat\', \'monkey\', \'dog\', \'dog\'], \'owner\':[\'Champ\', \'Ron\', \'Brick\', \'Champ\', \'Veronica\', \'Ron\'], \'location\':[\'San_Diego\', \'New_York\', \'New_York\', \'San_Diego\', \'San_Diego\', \'New_York\']})\nle = preprocessing.LabelEncoder()\n\nle.fit(df)\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/label.py"", line 103, in fit\n    y = column_or_1d(y, warn=True)\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py"", line 306, in column_or_1d\n    raise ValueError(""bad input shape {0}"".format(shape))\nValueError: bad input shape (6, 3)\n']"
409;3.0;1;24495695;;1;30;<python><pandas>;Pandas: Get unique MultiIndex level values by label;18489.0;"[""df = pd.DataFrame({'co':['DE','DE','FR','FR'],\n                   'tp':['Lake','Forest','Lake','Forest'],\n                   'area':[10,20,30,40],\n                   'count':[7,5,2,3]})\ndf = df.set_index(['co','tp'])\n"", '           area  count\nco tp\nDE Lake      10      7\n   Forest    20      5\nFR Lake      30      2\n   Forest    40      3\n', ""df.index.levels[0]  # returns ['DE', 'FR]\ndf.index.levels[1]  # returns ['Lake', 'Forest']\n"", ""'co'"", ""'tp'"", ""list(set(df.index.get_level_values('co')))  # returns ['DE', 'FR']\ndf.index.levels[df.index.names.index('co')]  # returns ['DE', 'FR']\n""]"
410;3.0;4;24644656;;1;32;<python><datetime><pandas><dataframe>;How to print dataframe without index;21964.0;['   User ID           Enter Time   Activity Number\n0      123  2014-07-08 00:09:00              1411\n1      123  2014-07-08 00:18:00               893\n2      123  2014-07-08 00:49:00              1041\n', 'User ID   Enter Time   Activity Number\n123         00:09:00              1411\n123         00:18:00               893\n123         00:49:00              1041\n']
411;4.0;3;24645153;;1;25;<python><pandas><scikit-learn><dataframe>;pandas dataframe columns scaling with sklearn;19762.0;"[""import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\n\ndfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],'B':[103.02,107.26,110.35,114.23,114.68], 'C':['big','small','big','small','small']})\nmin_max_scaler = preprocessing.MinMaxScaler()\n\ndef scaleColumns(df, cols_to_scale):\n    for col in cols_to_scale:\n        df[col] = pd.DataFrame(min_max_scaler.fit_transform(pd.DataFrame(dfTest[col])),columns=[col])\n    return df\n\ndfTest\n\n    A   B   C\n0    14.00   103.02  big\n1    90.20   107.26  small\n2    90.95   110.35  big\n3    96.27   114.23  small\n4    91.21   114.68  small\n\nscaled_df = scaleColumns(dfTest,['A','B'])\nscaled_df\n\nA   B   C\n0    0.000000    0.000000    big\n1    0.926219    0.363636    small\n2    0.935335    0.628645    big\n3    1.000000    0.961407    small\n4    0.938495    1.000000    small\n"", ""bad_output = min_max_scaler.fit_transform(dfTest['A'])"", ""dfTest2 = dfTest.drop('C', axis = 1)\ngood_output = min_max_scaler.fit_transform(dfTest2)\ngood_output""]"
412;2.0;0;24775648;;1;32;<python><pandas><boolean-logic><logical-operators><boolean-operations>;Element-wise logical OR in Pandas;17268.0;[]
413;3.0;0;24870306;;1;71;<python><pandas><dataframe>;How to check if a column exists in Pandas;39441.0;"["">>> import pandas as pd\n>>> from random import randint\n>>> df = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],\n                       'B': [randint(1, 9)*10 for x in xrange(10)],\n                       'C': [randint(1, 9)*100 for x in xrange(10)]})\n>>> df\n   A   B    C\n0  3  40  100\n1  6  30  200\n2  7  70  800\n3  3  50  200\n4  7  50  400\n5  4  10  400\n6  3  70  500\n7  8  30  200\n8  3  40  800\n9  6  60  200\n"", ""df['sum'] = df['A'] + df['C']"", ""df['A']"", ""df['sum'] = df['B'] + df['C']""]"
414;4.0;6;24870953;;1;22;<python><performance><pandas><iteration>;Does iterrows have performance issues?;7339.0;"[""import pandas as pd\nimport numpy as np\nimport time\n\ns1 = np.random.randn(2000000)\ns2 = np.random.randn(2000000)\ndfa = pd.DataFrame({'s1': s1, 's2': s2})\n\nstart = time.time()\ni=0\nfor rowindex, row in dfa.iterrows():\n    i+=1\nend = time.time()\nprint end - start\n"", ""import pandas as pd\nimport numpy as np\n\n#%% Create the original tables\nt1 = {'letter':['a','b'],\n      'number1':[50,-10]}\n\nt2 = {'letter':['a','a','b','b'],\n      'number2':[0.2,0.5,0.1,0.4]}\n\ntable1 = pd.DataFrame(t1)\ntable2 = pd.DataFrame(t2)\n\n#%% Create the body of the new table\ntable3 = pd.DataFrame(np.nan, columns=['letter','number2'], index=[0])\n\n#%% Iterate through filtering relevant data, optimizing, returning info\nfor row_index, row in table1.iterrows():   \n    t2info = table2[table2.letter == row['letter']].reset_index()\n    table3.ix[row_index,] = optimize(t2info,row['number1'])\n\n#%% Define optimization\ndef optimize(t2info, t1info):\n    calculation = []\n    for index, r in t2info.iterrows():\n        calculation.append(r['number2']*t1info)\n    maxrow = calculation.index(max(calculation))\n    return t2info.ix[maxrow]\n""]"
415;5.0;1;25039626;;1;37;<python><types><pandas>;find numeric columns in pandas (python);15852.0;['isNumeric = is_numeric(df)\n']
416;1.0;0;25055712;;1;23;<pandas><resampling>;Pandas every nth row;10589.0;[]
417;7.0;1;25146121;;1;53;<python><pandas>;Extracting just Month and Year from Pandas Datetime column (Python);78275.0;"[""df['ArrivalDate'] =\n...\n936   2012-12-31\n938   2012-12-29\n965   2012-12-31\n966   2012-12-31\n967   2012-12-31\n968   2012-12-31\n969   2012-12-31\n970   2012-12-29\n971   2012-12-31\n972   2012-12-29\n973   2012-12-29\n...\n"", ""df['ArrivalDate'].resample('M', how = 'mean')\n"", 'Only valid with DatetimeIndex or PeriodIndex \n', ""df['ArrivalDate'].apply(lambda(x):x[:-2])\n"", ""'Timestamp' object has no attribute '__getitem__' \n"", ""df.index = df['ArrivalDate']\n""]"
418;2.0;2;25212986;;1;30;<python><pandas><seaborn>;How to set some xlim and ylim in Seaborn lmplot facetgrid;25892.0;"[""import pandas as pd\nimport seaborn as sns\nimport random\n\nn = 200\nrandom.seed(2014)\nbase_x = [random.random() for i in range(n)]\nbase_y = [2*i for i in base_x]\nerrors = [random.uniform(0,1) for i in range(n)]\ny = [i+j for i,j in zip(base_y,errors)]\n\ndf = pd.DataFrame({'X': base_x,\n                   'Y': y,\n                   'Z': ['A','B']*(n/2)})\n\nmask_for_b = df.Z == 'B'\ndf.loc[mask_for_b,['X','Y']] = df.loc[mask_for_b,] *2\n\nsns.lmplot('X','Y',df,col='Z',sharex=False,sharey=False)\n"", 'sns.plt.ylim(0,)\nsns.plt.xlim(0,)\n']"
419;4.0;3;25239958;;1;23;<python><pandas><scikit-learn>;Impute categorical missing values in scikit-learn;10847.0;"[""from sklearn.preprocessing import Imputer\nimp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\nimp.fit(df) \n""]"
420;3.0;1;25254016;;1;62;<python><pandas>;Pandas - Get first row value of a given column;90823.0;['  ATime   X   Y   Z   Btime  C   D   E\n0    1.2  2  15   2    1.2  12  25  12\n1    1.4  3  12   1    1.3  13  22  11\n2    1.5  1  10   6    1.4  11  20  16\n3    1.6  2   9  10    1.7  12  29  12\n4    1.9  1   1   9    1.9  11  21  19\n5    2.0  0   0   0    2.0   8  10  11\n6    2.4  0   0   0    2.4  10  12  15\n']
421;2.0;0;25351968;;1;21;<python><html><pandas>;How to display full (non-truncated) dataframe information in html when converting from pandas dataframe to html?;13484.0;['DataFrame.to_html', 'df.head(1)', 'DataFrame.to_html']
422;2.0;0;25386870;;1;25;<python><matplotlib><pandas><multi-index>;Pandas Plotting with Multi-Index;12929.0;[]
423;2.0;0;25493625;;1;22;<python><join><pandas><vlookup>;vlookup in Pandas using join;21751.0;"['Example1\nsku loc flag  \n122  61 True \n123  61 True\n113  62 True \n122  62 True \n123  62 False\n122  63 False\n301  63 True \n\nExample2 \nsku dept \n113 a\n122 b\n123 b\n301 c \n', ""Example3\nsku loc flag   dept  \n122  61 True   b\n123  61 True   b\n113  62 True   a\n122  62 True   b\n123  62 False  b\n122  63 False  b\n301  63 True   c\n\nBoth \ndf_Example1.join(df_Example2,lsuffix='_ProdHier')\ndf_Example1.join(df_Example2,how='outer',lsuffix='_ProdHier')\n""]"
424;1.0;6;25537399;;1;21;<python><pandas><scipy><statsmodels><anova>;ANOVA in python using pandas dataframe with statsmodels or scipy?;10835.0;[]
425;1.0;16;25631076;;1;46;<python><performance><numpy><pandas>;Is this the fastest way to group in Pandas?;2357.0;"['$ python3\nPython 3.4.0 (default, Apr 11 2014, 13:05:11) \n[GCC 4.8.2] on linux\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n>>> import pandas as pd\n>>> import numpy as np\n>>> import timeit\n>>> pd.__version__\n\'0.14.1\'\n\ndef randChar(f, numGrp, N) :\n   things = [f%x for x in range(numGrp)]\n   return [things[x] for x in np.random.choice(numGrp, N)]\n\ndef randFloat(numGrp, N) :\n   things = [round(100*np.random.random(),4) for x in range(numGrp)]\n   return [things[x] for x in np.random.choice(numGrp, N)]\n\nN=int(1e8)\nK=100\nDF = pd.DataFrame({\n  \'id1\' : randChar(""id%03d"", K, N),       # large groups (char)\n  \'id2\' : randChar(""id%03d"", K, N),       # large groups (char)\n  \'id3\' : randChar(""id%010d"", N//K, N),   # small groups (char)\n  \'id4\' : np.random.choice(K, N),         # large groups (int)\n  \'id5\' : np.random.choice(K, N),         # large groups (int)\n  \'id6\' : np.random.choice(N//K, N),      # small groups (int)            \n  \'v1\' :  np.random.choice(5, N),         # int in range [1,5]\n  \'v2\' :  np.random.choice(5, N),         # int in range [1,5]\n  \'v3\' :  randFloat(100,N)                # numeric e.g. 23.5749\n})\n', 'timeit(2)', 'htop', '>>> timeit.Timer(""DF.groupby([\'id1\']).agg({\'v1\':\'sum\'})""                            ,""from __main__ import DF"").timeit(1)\n5.604133386000285\n>>> timeit.Timer(""DF.groupby([\'id1\']).agg({\'v1\':\'sum\'})""                            ,""from __main__ import DF"").timeit(1)\n5.505057081000359\n\n>>> timeit.Timer(""DF.groupby([\'id1\',\'id2\']).agg({\'v1\':\'sum\'})""                      ,""from __main__ import DF"").timeit(1)\n14.232032927000091\n>>> timeit.Timer(""DF.groupby([\'id1\',\'id2\']).agg({\'v1\':\'sum\'})""                      ,""from __main__ import DF"").timeit(1)\n14.242601240999647\n\n>>> timeit.Timer(""DF.groupby([\'id3\']).agg({\'v1\':\'sum\', \'v3\':\'mean\'})""               ,""from __main__ import DF"").timeit(1)\n22.87025260900009\n>>> timeit.Timer(""DF.groupby([\'id3\']).agg({\'v1\':\'sum\', \'v3\':\'mean\'})""               ,""from __main__ import DF"").timeit(1)\n22.393589012999655\n\n>>> timeit.Timer(""DF.groupby([\'id4\']).agg({\'v1\':\'mean\', \'v2\':\'mean\', \'v3\':\'mean\'})"" ,""from __main__ import DF"").timeit(1)\n2.9725865330001398\n>>> timeit.Timer(""DF.groupby([\'id4\']).agg({\'v1\':\'mean\', \'v2\':\'mean\', \'v3\':\'mean\'})"" ,""from __main__ import DF"").timeit(1)\n2.9683854739996605\n\n>>> timeit.Timer(""DF.groupby([\'id6\']).agg({\'v1\':\'sum\', \'v2\':\'sum\', \'v3\':\'sum\'})""    ,""from __main__ import DF"").timeit(1)\n12.776488024999708\n>>> timeit.Timer(""DF.groupby([\'id6\']).agg({\'v1\':\'sum\', \'v2\':\'sum\', \'v3\':\'sum\'})""    ,""from __main__ import DF"").timeit(1)\n13.558292575999076\n', '$ lscpu\nArchitecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                32\nOn-line CPU(s) list:   0-31\nThread(s) per core:    2\nCore(s) per socket:    8\nSocket(s):             2\nNUMA node(s):          2\nVendor ID:             GenuineIntel\nCPU family:            6\nModel:                 62\nStepping:              4\nCPU MHz:               2500.048\nBogoMIPS:              5066.38\nHypervisor vendor:     Xen\nVirtualization type:   full\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              256K\nL3 cache:              25600K\nNUMA node0 CPU(s):     0-7,16-23\nNUMA node1 CPU(s):     8-15,24-31\n\n$ free -h\n             total       used       free     shared    buffers     cached\nMem:          240G        74G       166G       372K        33M       550M\n-/+ buffers/cache:        73G       166G\nSwap:           0B         0B         0B\n', 'randChar', 'mtrand.RandomState.choice']"
426;2.0;1;25646200;;1;27;<python><pandas><timedelta>;Python: Convert timedelta to int in a dataframe;31951.0;[]
427;2.0;0;25748683;;1;35;<python><pandas><dataframe><sum>;Pandas: sum DataFrame rows for given columns;88000.0;"[""import pandas as pd\ndf = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})\n"", ""df['e'] = df[['a','b','d']].map(sum)\n"", ""['a','b','d']"", 'df']"
428;2.0;0;25773245;;1;58;<python><arrays><pandas><numpy><dataframe>;"Ambiguity in Pandas Dataframe / Numpy Array ""axis"" definition";11210.0;"['>>> df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]], columns=[""col1"", ""col2"", ""col3"", ""col4""])\n>>> df\n   col1  col2  col3  col4\n0     1     1     1     1\n1     2     2     2     2\n2     3     3     3     3\n', 'df.mean(axis=1)', '>>> df.mean(axis=1)\n0    1\n1    2\n2    3\n', 'df.drop(name, axis=1)', '>>> df.drop(""col4"", axis=1)\n   col1  col2  col3\n0     1     1     1\n1     2     2     2\n2     3     3     3\n', 'DataFrame.mean', 'DataFrame.mean', 'axis=1']"
429;5.0;1;25962114;;1;37;<python><memory><numpy><pandas>;How to read a 6 GB csv file with pandas;40201.0;"[""MemoryError                               Traceback (most recent call last)\n<ipython-input-58-67a72687871b> in <module>()\n----> 1 data=pd.read_csv('aphro.csv',sep=';')\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format)\n    450                     infer_datetime_format=infer_datetime_format)\n    451 \n--> 452         return _read(filepath_or_buffer, kwds)\n    453 \n    454     parser_f.__name__ = name\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in _read(filepath_or_buffer, kwds)\n    242         return parser\n    243 \n--> 244     return parser.read()\n    245 \n    246 _parser_defaults = {\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in read(self, nrows)\n    693                 raise ValueError('skip_footer not supported for iteration')\n    694 \n--> 695         ret = self._engine.read(nrows)\n    696 \n    697         if self.options.get('as_recarray'):\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in read(self, nrows)\n   1137 \n   1138         try:\n-> 1139             data = self._reader.read(nrows)\n   1140         except StopIteration:\n   1141             if nrows is None:\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader.read (pandas\\parser.c:7145)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._read_low_memory (pandas\\parser.c:7369)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._read_rows (pandas\\parser.c:8194)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_column_data (pandas\\parser.c:9402)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_tokens (pandas\\parser.c:10057)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_with_dtype (pandas\\parser.c:10361)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser._try_int64 (pandas\\parser.c:17806)()\n\nMemoryError: \n""]"
430;2.0;4;26047209;;1;43;<python><pandas>;What is the difference between a pandas Series and a single-column DataFrame?;19282.0;['Series', 'DataFrame', 'Series']
431;4.0;0;26139423;;1;35;<matplotlib><pandas><visualization>;plot different color for different categorical levels using matplotlib;28342.0;"['diamonds', '(carat, price, color)', 'price', 'carat', 'color', 'color', 'R', 'ggplot', ""ggplot(aes(x=carat, y=price, color=color),  #by setting color=color, ggplot automatically draw in different colors\n       data=diamonds) + geom_point(stat='summary', fun.y=median)\n"", 'matplotlib', 'seaborn', 'ggplot for python', 'matplotlib']"
432;3.0;0;26187759;;1;22;<python><pandas><parallel-processing><rosetta>;Parallelize apply after pandas groupby;11709.0;"[""from rosetta.parallel.pandas_easy import groupby_to_series_to_frame\ndf = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])\ngroupby_to_series_to_frame(df, np.mean, n_jobs=8, use_apply=True, by=df.index)\n"", ""def tmpFunc(df):\n    df['c'] = df.a + df.b\n    return df\n\ndf.groupby(df.index).apply(tmpFunc)\ngroupby_to_series_to_frame(df, tmpFunc, n_jobs=1, use_apply=True, by=df.index)\n""]"
433;9.0;0;26266362;;1;110;<python><pandas>;How to count the Nan values in the column in Panda Data frame;83763.0;[]
434;2.0;5;26277757;;1;32;<python><html><pandas>;Pandas to_html() truncates string contents;7560.0;"['DataFrame', 'to_html()', ""import pandas\ndf = pandas.DataFrame({'text': ['Lorem ipsum dolor sit amet, consectetur adipiscing elit.']})\nprint (df.to_html())\n"", 'adapis...', '<table border=""1"" class=""dataframe"">\n  <thead>\n    <tr style=""text-align: right;"">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td> Lorem ipsum dolor sit amet, consectetur adipis...</td>\n    </tr>\n  </tbody>\n</table>\n']"
435;5.0;1;26309962;;1;21;<python><pandas><append><dataframe>;Appending a list or series to a pandas DataFrame as a row?;40866.0;[]
436;1.0;0;26347412;;1;35;<python><pandas>;Drop multiple columns in pandas;66476.0;"['df.drop([df.columns[[1, 69]]], axis=1, inplace=True)\n', ""TypeError: unhashable type: 'Index'\n"", ""Expected type 'Integral', got 'list[int]' instead\n"", 'df.drop([df.columns[69]], axis=1, inplace=True)\ndf.drop([df.columns[1]], axis=1, inplace=True)\n']"
437;14.0;4;26473681;;1;59;<python><numpy><pandas><pip>;"PIP Install Numpy throws an error ""ascii codec can't decode byte 0xe2""";46327.0;"['Traceback (most recent call last):\n  File ""/usr/bin/pip"", line 9, in <module>\n    load_entry_point(\'pip==1.5.4\', \'console_scripts\', \'pip\')()\n  File ""/usr/lib/python2.7/dist-packages/pip/__init__.py"", line 185, in main\n    return command.main(cmd_args)\n  File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 161, in main\n    text = \'\\n\'.join(complete_log)\nUnicodeDecodeError: \'ascii\' codec can\'t decode byte 0xe2 in position 72: ordinal not in range(128)\n']"
438;1.0;3;26483254;;1;22;<python><list><pandas><insert><dataframe>;Python pandas insert list into a cell;17970.0;"[""abc = ['foo', 'bar']\ndf =\n    A  B\n0  12  NaN\n1  23  NaN\n"", ""    A  B\n0  12  NaN\n1  23  ['foo', 'bar']\n"", ""df.ix[1,'B'] = abc\n"", 'ValueError: Must have equal len keys and value when setting with an iterable\n', ""df.ix[1,'B'] = [abc]\n"", ""[['foo', 'bar']]"", ""df.ix[1,'B'] = ', '.join(abc)\n"", 'foo, bar', ""df.ix[1,'B'] = [', '.join(abc)]\n"", ""['foo, bar']"", ""['foo', 'bar']"", ""abc = ['foo', 'bar']\ndf2 =\n    A    B         C\n0  12  NaN      'bla'\n1  23  NaN  'bla bla'\n"", ""df3 =\n    A    B         C                    D\n0  12  NaN      'bla'  ['item1', 'item2']\n1  23  NaN  'bla bla'        [11, 12, 13]\n"", ""df2.loc[1,'B']"", ""df3.loc[1,'B']"", ""df2.loc[1,'B'] = abc"", ""df3.loc[1,'B'] = abc"", ""df4 =\n          A     B\n0      'bla'  NaN\n1  'bla bla'  NaN\n"", ""df.loc[1,'B'] = abc"", ""df4.loc[1,'B'] = abc""]"
439;4.0;0;26640145;;1;24;<python><pandas><dataframe>;Python Pandas: How to get the row names from index of a dataframe?;41938.0;['        X  Y\n Row 1  0  5\n Row 2  8  1\n Row 3  3  0\n']
440;3.0;5;26645515;;1;36;<python><join><pandas>;Pandas join issue: columns overlap but no suffix specified;30832.0;"['df_a =\n\n     mukey  DI  PI\n0   100000  35  14\n1  1000005  44  14\n2  1000006  44  14\n3  1000007  43  13\n4  1000008  43  13\n\ndf_b = \n    mukey  niccdcd\n0  190236        4\n1  190237        6\n2  190238        7\n3  190239        4\n4  190240        7\n', ""join_df = df_a.join(df_b,on='mukey',how='left')\n"", ""*** ValueError: columns overlap but no suffix specified: Index([u'mukey'], dtype='object')\n""]"
441;1.0;2;26658240;;1;26;<python-2.7><pandas><dataframe>;getting the index of a row in a pandas apply function;9080.0;"['DataFrame', ""df = pandas.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])\n>>> df\n   a  b  c\n0  1  2  3\n1  4  5  6\n"", ""def rowFunc(row):\n    return row['a'] + row['b'] * row['c']\n"", ""df['d'] = df.apply(rowFunc, axis=1)\n>>> df\n   a  b  c   d\n0  1  2  3   7\n1  4  5  6  34\n"", 'DataFrame', 'd', ""Index([u'a', u'b', u'c', u'd'], dtype='object')"", 'row.index']"
442;4.0;2;26716616;;1;28;<python><pandas><dictionary><dataframe>;Convert a Pandas DataFrame to a dictionary;34674.0;"['keys', 'values', '    ID   A   B   C\n0   p    1   3   2\n1   q    4   3   2\n2   r    4   0   9  \n', ""{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}\n""]"
443;3.0;0;26763344;;1;43;<python><datetime><pandas>;Convert Pandas Column to DateTime;63841.0;[]
444;2.0;0;26786960;;1;28;<python><csv><pandas>;pandas to_csv first extra column remove, how to?;14508.0;"[""d = {'one' : pd.Series([1., 2., 3.]),'two' : pd.Series([1., 2., 3., 4.])}\ndf0_fa = pd.DataFrame(d)\ndf_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w')\n"", ',one,two\n0,1.0,1.0\n1,2.0,2.0\n2,3.0,3.0\n3,,4.0\n', 'one,two\n1.0,1.0\n2.0,2.0\n3.0,3.0\n,4.0\n']"
445;3.0;1;26837998;;1;40;<python><pandas><nan>;Pandas Replace NaN with blank/empty string;36883.0;"['    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n', '    1    2       3\n 0  a   """"    read\n 1  b    l  unread\n 2  c   """"    read\n']"
446;4.0;2;26873127;;1;78;<pandas><printing><ipython-notebook><jupyter-notebook><display>;Show DataFrame as table in iPython Notebook;52192.0;['df\n', 'df1\ndf2 \n', 'print df1\nprint df2\n']
447;1.0;0;26886653;;1;52;<python><numpy><pandas>;pandas create new column based on values from other columns;68245.0;['IF [ERI_Hispanic] = 1 THEN RETURN \x93Hispanic\x94\nELSE IF SUM([ERI_AmerInd_AKNatv] + [ERI_Asian] + [ERI_Black_Afr.Amer] + [ERI_HI_PacIsl] + [ERI_White]) > 1 THEN RETURN \x93Two or More\x94\nELSE IF [ERI_AmerInd_AKNatv] = 1 THEN RETURN \x93A/I AK Native\x94\nELSE IF [ERI_Asian] = 1 THEN RETURN \x93Asian\x94\nELSE IF [ERI_Black_Afr.Amer] = 1 THEN RETURN \x93Black/AA\x94\nELSE IF [ERI_HI_PacIsl] = 1 THEN RETURN \x93Haw/Pac Isl.\x94\nELSE IF [ERI_White] = 1 THEN RETURN \x93White\x94\n', '     lname          fname       rno_cd  eri_afr_amer    eri_asian   eri_hawaiian    eri_hispanic    eri_nat_amer    eri_white   rno_defined\n0    MOST           JEFF        E       0               0           0               0               0               1           White\n1    CRUISE         TOM         E       0               0           0               1               0               0           White\n2    DEPP           JOHNNY              0               0           0               0               0               1           Unknown\n3    DICAP          LEO                 0               0           0               0               0               1           Unknown\n4    BRANDO         MARLON      E       0               0           0               0               0               0           White\n5    HANKS          TOM         0                       0           0               0               0               1           Unknown\n6    DENIRO         ROBERT      E       0               1           0               0               0               1           White\n7    PACINO         AL          E       0               0           0               0               0               1           White\n8    WILLIAMS       ROBIN       E       0               0           1               0               0               0           White\n9    EASTWOOD       CLINT       E       0               0           0               0               0               1           White\n']
448;5.0;0;26977076;;1;35;<python><pandas><dataframe><unique>;pandas unique values multiple columns;43537.0;"[""df = pd.DataFrame({'Col1': ['Bob', 'Joe', 'Bill', 'Mary', 'Joe'],\n                   'Col2': ['Joe', 'Steve', 'Bob', 'Bob', 'Steve'],\n                   'Col3': np.random.random(5)})\n"", ""'Bob', 'Joe', 'Bill', 'Mary', 'Steve'\n""]"
449;2.0;4;27236275;;1;56;<python><pandas>;What does `ValueError: cannot reindex from a duplicate axis` mean?;48018.0;"['ValueError: cannot reindex from a duplicate axis', 'ipdb', 'sum', 'ValueError: cannot reindex from a duplicate axis', 'ValueError: cannot reindex from a duplicate axis', ""ipdb> type(affinity_matrix)\n<class 'pandas.core.frame.DataFrame'>\nipdb> affinity_matrix.shape\n(333, 10)\nipdb> affinity_matrix.columns\nInt64Index([9315684, 9315597, 9316591, 9320520, 9321163, 9320615, 9321187, 9319487, 9319467, 9320484], dtype='int64')\nipdb> affinity_matrix.index\nIndex([u'001', u'002', u'003', u'004', u'005', u'008', u'009', u'010', u'011', u'014', u'015', u'016', u'018', u'020', u'021', u'022', u'024', u'025', u'026', u'027', u'028', u'029', u'030', u'032', u'033', u'034', u'035', u'036', u'039', u'040', u'041', u'042', u'043', u'044', u'045', u'047', u'047', u'048', u'050', u'053', u'054', u'055', u'056', u'057', u'058', u'059', u'060', u'061', u'062', u'063', u'065', u'067', u'068', u'069', u'070', u'071', u'072', u'073', u'074', u'075', u'076', u'077', u'078', u'080', u'082', u'083', u'084', u'085', u'086', u'089', u'090', u'091', u'092', u'093', u'094', u'095', u'096', u'097', u'098', u'100', u'101', u'103', u'104', u'105', u'106', u'107', u'108', u'109', u'110', u'111', u'112', u'113', u'114', u'115', u'116', u'117', u'118', u'119', u'121', u'122', ...], dtype='object')\n\nipdb> affinity_matrix.values.dtype\ndtype('float64')\nipdb> 'sums' in affinity_matrix.index\nFalse\n"", ""ipdb> affinity_matrix.loc['sums'] = affinity_matrix.sum(axis=0)\n*** ValueError: cannot reindex from a duplicate axis\n"", ""In [32]: import pandas as pd\n\nIn [33]: import numpy as np\n\nIn [34]: a = np.arange(35).reshape(5,7)\n\nIn [35]: df = pd.DataFrame(a, ['x', 'y', 'u', 'z', 'w'], range(10, 17))\n\nIn [36]: df.values.dtype\nOut[36]: dtype('int64')\n\nIn [37]: df.loc['sums'] = df.sum(axis=0)\n\nIn [38]: df\nOut[38]: \n      10  11  12  13  14  15   16\nx      0   1   2   3   4   5    6\ny      7   8   9  10  11  12   13\nu     14  15  16  17  18  19   20\nz     21  22  23  24  25  26   27\nw     28  29  30  31  32  33   34\nsums  70  75  80  85  90  95  100\n""]"
450;3.0;0;27263805;;1;31;<python><pandas>;pandas: When cell contents are lists, create a row for each element in the list;8222.0;"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {'trial_num': [1, 2, 3, 1, 2, 3],\n     'subject': [1, 1, 1, 2, 2, 2],\n     'samples': [list(np.random.randn(3).round(2)) for i in range(6)]\n    }\n)\n\ndf\nOut[10]: \n                 samples  subject  trial_num\n0    [0.57, -0.83, 1.44]        1          1\n1    [-0.01, 1.13, 0.36]        1          2\n2   [1.18, -1.46, -0.94]        1          3\n3  [-0.08, -4.22, -2.05]        2          1\n4     [0.72, 0.79, 0.53]        2          2\n5    [0.4, -0.32, -0.13]        2          3\n"", '   subject  trial_num  sample  sample_num\n0        1          1    0.57           0\n1        1          1   -0.83           1\n2        1          1    1.44           2\n3        1          2   -0.01           0\n4        1          2    1.13           1\n5        1          2    0.36           2\n6        1          3    1.18           0\n# etc.\n']"
451;4.0;1;27365467;;1;32;<python><pandas><matplotlib><time-series>;python pandas: plot histogram of dates?;17415.0;"['datetime64[ns]', ""import pandas as pd\ndf = pd.read_csv('somefile.csv')\ncolumn = df['date']\ncolumn = pd.to_datetime(column, coerce=True)\n"", ""ipdb> column.plot(kind='hist')\n*** TypeError: ufunc add cannot use operands with types dtype('<M8[ns]') and dtype('float64')\n"", 'pandas']"
452;2.0;7;27405483;;1;29;<python><pandas>;How to loop over grouped Pandas dataframe?;27652.0;"['  c_os_family_ss c_os_major_is l_customer_id_i\n0      Windows 7                         90418\n1      Windows 7                         90418\n2      Windows 7                         90418\n', ""print df\nfor name, group in df.groupby('l_customer_id_i').agg(lambda x: ','.join(x)):\n    print name\n    print group\n"", '                                                    c_os_family_ss  \\\nl_customer_id_i\n131572           Windows 7,Windows 7,Windows 7,Windows 7,Window...\n135467           Windows 7,Windows 7,Windows 7,Windows 7,Window...\n\n                                                     c_os_major_is\nl_customer_id_i\n131572           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...\n135467           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...\n']"
453;1.0;2;27517425;;1;38;<python><pandas>;Apply vs transform on a group object;19045.0;"['     A      B         C         D\n0  foo    one  0.162003  0.087469\n1  bar    one -1.156319 -1.526272\n2  foo    two  0.833892 -1.666304\n3  bar  three -2.026673 -0.322057\n4  foo    two  0.411452 -0.954371\n5  bar    two  0.765878 -0.095968\n6  foo    one -0.654890  0.678091\n7  foo  three -1.789842 -1.130922\n', ""> df.groupby('A').apply(lambda x: (x['C'] - x['D']))\n> df.groupby('A').apply(lambda x: (x['C'] - x['D']).mean())\n"", ""> df.groupby('A').transform(lambda x: (x['C'] - x['D']))\nValueError: could not broadcast input array from shape (5) into shape (5,3)\n\n> df.groupby('A').transform(lambda x: (x['C'] - x['D']).mean())\n TypeError: cannot concatenate a non-NDFrame object\n"", 'transform', '# Note that the following suggests row-wise operation (x.mean is the column mean)\nzscore = lambda x: (x - x.mean()) / x.std()\ntransformed = ts.groupby(key).transform(zscore)\n', ""df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n                          'foo', 'bar', 'foo', 'foo'],\n                   'B' : ['one', 'one', 'two', 'three',\n                         'two', 'two', 'one', 'three'],\n                   'C' : randn(8), 'D' : randn(8)})\n""]"
454;1.0;1;27667759;;1;53;<python><r><pandas>;Is .ix() always better than .loc() and .iloc() since it is faster and supports integer and label access?;38433.0;[]
455;3.0;2;27673231;;1;36;<pandas>;why should I make a copy of a data frame in pandas;28042.0;[]
456;3.0;0;27842613;;1;43;<python><sorting><pandas><group-by>;pandas groupby sort within groups;37771.0;"[""In [167]:\ndf\n\nOut[167]:\ncount   job source\n0   2   sales   A\n1   4   sales   B\n2   6   sales   C\n3   3   sales   D\n4   7   sales   E\n5   5   market  A\n6   3   market  B\n7   2   market  C\n8   4   market  D\n9   1   market  E\n\nIn [168]:\ndf.groupby(['job','source']).agg({'count':sum})\n\nOut[168]:\n            count\njob     source  \nmarket  A   5\n        B   3\n        C   2\n        D   4\n        E   1\nsales   A   2\n        B   4\n        C   6\n        D   3\n        E   7\n"", '            count\njob     source  \nmarket  A   5\n        D   4\n        B   3\nsales   E   7\n        C   6\n        B   4\n']"
457;5.0;0;27905295;;1;23;<python><python-3.x><pandas><dataframe><nan>;How to replace NaNs by preceding values in pandas DataFrame?;14351.0;['NaN', '>>> import pandas as pd\n>>> df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])\n>>> df\n    0   1   2\n0   1   2   3\n1   4 NaN NaN\n2 NaN NaN   9\n', 'NaN', 'NaN', 'NaN', '   0  1  2\n0  1  2  3\n1  4  2  3\n2  4  2  9\n']
458;4.0;1;27975069;;1;26;<python><pandas>;How to filter rows containing a string pattern from a Pandas dataframe;35532.0;"[""df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': [u'aball', u'bball', u'cnut', u'fball']})\n"", 'ids    vals\naball   1\nbball   2\ncnut    3\nfball   4\n', 'ids    vals\naball   1\nbball   2\nfball   4\n']"
459;2.0;1;28006793;;1;27;<python><pandas>;Pandas DataFrame to List of Lists;27158.0;['import pandas as pd\ndf = pd.DataFrame([[1,2,3],[3,4,5]])\n', 'lol = df.what_to_do_now?\nprint lol\n# [[1,2,3],[3,4,5]]\n']
460;7.0;1;28218698;;1;42;<python><pandas><statsmodels>;How to iterate over columns of pandas dataframe to run regression;71906.0;"['pandas', ""all_data = {}\nfor ticker in ['FIUIX', 'FSAIX', 'FSAVX', 'FSTMX']:\n    all_data[ticker] = web.get_data_yahoo(ticker, '1/1/2010', '1/1/2015')\n\nprices = DataFrame({tic: data['Adj Close'] for tic, data in all_data.iteritems()})  \nreturns = prices.pct_change()\n"", 'regs = sm.OLS(returns.FIUIX,returns.FSTMX).fit()\n', 'resids = {}\nfor k in returns.keys():\n    reg = sm.OLS(returns[k],returns.FSTMX).fit()\n    resids[k] = reg.resid\n', 'returns[k]']"
461;1.0;1;28236305;;1;21;<python><pandas><dataframe><data-analysis>;How do I sum values in a column that match a given condition using pandas?;27037.0;['a   b  \n1   5   \n1   7\n2   3\n1   3\n2   5\n', 'b', 'a = 1', '5 + 7 + 3 = 15']
462;1.0;0;28311655;;1;24;<pandas>;Ignoring NaNs with str.contains;6219.0;"['DF[DF.col.str.contains(""foo"")]\n', 'DF[DF.col.notnull()][DF.col.dropna().str.contains(""foo"")]\n']"
463;3.0;0;28679930;;1;23;<python><pandas>;How to drop rows from pandas data frame that contains a particular string in a particular column?;20370.0;[]
464;3.0;5;28757389;;1;71;<python><pandas>;Loc vs. iloc vs. ix vs. at vs. iat?;31094.0;['Pandas', '.loc', '.iloc', '.ix', '.loc', 'iloc', 'at', 'iat', '.ix', '.ix', '.ix']
465;8.0;0;28901683;;1;44;<python><pandas>;pandas get rows which are NOT in other dataframe;26226.0;"[""df1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]}) \ndf2 = pandas.DataFrame(data = {'col1' : [1, 2, 3], 'col2' : [10, 11, 12]})\n""]"
466;1.0;0;28931224;;1;28;<python><python-2.7><pandas><matplotlib><data-visualization>;Adding value labels on a matplotlib bar chart;35505.0;"['import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option(\'display.mpl_style\', \'default\') \n%matplotlib inline\n\n\nfrequencies = [6, 16, 75, 160, 244, 260, 145, 73, 16, 4, 1]   # bring some raw data\n\nfreq_series = pd.Series.from_array(frequencies)   # in my original code I create a series and run on that, so for consistency I create a series from the list.\n\nx_labels = [108300.0, 110540.0, 112780.0, 115020.0, 117260.0, 119500.0, 121740.0, 123980.0, 126220.0, 128460.0, 130700.0]\n\n# now to plot the figure...\nplt.figure(figsize=(12, 8))\nfig = freq_series.plot(kind=\'bar\')\nfig.set_title(""Amount Frequency"")\nfig.set_xlabel(""Amount ($)"")\nfig.set_ylabel(""Frequency"")\nfig.set_xticklabels(x_labels)\n']"
467;2.0;0;29177498;;1;28;<python-2.7><pandas><dataframe><nan>;Python Pandas replace NaN in one column with value from corresponding row of second column;14402.0;['File    heat    Farheit Temp_Rating\n   1    YesQ    75      N/A\n   1    NoR     115     N/A\n   1    YesA    63      N/A\n   1    NoT     83      41\n   1    NoY     100     80\n   1    YesZ    56      12\n   2    YesQ    111     N/A\n   2    NoR     60      N/A\n   2    YesA    19      N/A\n   2    NoT     106     77\n   2    NoY     45      21\n   2    YesZ    40      54\n   3    YesQ    84      N/A\n   3    NoR     67      N/A\n   3    YesA    94      N/A\n   3    NoT     68      39\n   3    NoY     63      46\n   3    YesZ    34      81\n', 'Temp_Rating', 'Farheit', 'File        heat    Observation\n   1        YesQ    75\n   1        NoR     115\n   1        YesA    63\n   1        YesQ    41\n   1        NoR     80\n   1        YesA    12\n   2        YesQ    111\n   2        NoR     60\n   2        YesA    19\n   2        NoT     77\n   2        NoY     21\n   2        YesZ    54\n   3        YesQ    84\n   3        NoR     67\n   3        YesA    94\n   3        NoT     39\n   3        NoY     46\n   3        YesZ    81\n', 'Temp_Rating', 'NaN', 'Farheit']
468;1.0;0;29226210;;1;21;<python><pandas><apache-spark><pyspark>;What is the Spark DataFrame method `toPandas` actually doing?;16117.0;"[""lines = sc.textFile('tail5.csv')\nparts = lines.map(lambda l : l.strip().split('\\t'))\nfnames = *some name list*\nschemaData = StructType([StructField(fname, StringType(), True) for fname in fnames])\nddf = sqlContext.createDataFrame(parts,schemaData)\n""]"
469;2.0;4;29287224;;1;31;<python><pandas>;Pandas read in table without headers;32704.0;['usecols']
470;3.0;0;29370057;;1;42;<python><pandas>;Select dataframe rows between two dates;55170.0;"[""stock = pd.read_csv('data_in/' + filename + '.csv', skipinitialspace=True)\n""]"
471;4.0;0;29432629;;1;33;<python><pandas><matplotlib><data-visualization><information-visualization>;Correlation matrix using pandas;39252.0;['dataframe.corr()']
472;3.0;0;29517072;;1;28;<python><pandas>;Add column to dataframe with default value;24041.0;['Date, Open, High, Low, Close\n01-01-2015, 565, 600, 400, 450\n', 'Name, Date, Open, High, Low, Close\nabc, 01-01-2015, 565, 600, 400, 450\n']
473;4.0;0;29525808;;1;37;<python><pandas><sqlalchemy><flask-sqlalchemy>;SQLAlchemy ORM conversion to pandas DataFrame;9483.0;['<Query object>', 'pandas.read_sql', '.db.session.query(Item).filter(Item.symbol.in_(add_symbols)', 'Item', 'add_symbols', 'SELECT ... from ... WHERE ... IN']
474;8.0;1;29530232;;1;113;<python><pandas><nan>;Python pandas: check if any value is NaN in DataFrame;136260.0;['pd.isnan']
475;3.0;0;29576430;;1;60;<python><pandas><dataframe><permutation><shuffle>;Shuffle DataFrame rows;36846.0;['    Col1  Col2  Col3  Type\n0      1     2     3     1\n1      4     5     6     1\n...\n20     7     8     9     2\n21    10    11    12     2\n...\n45    13    14    15     3\n46    16    17    18     3\n...\n', 'Type', 'Type', 'Type', 'Type', '    Col1  Col2  Col3  Type\n0      7     8     9     2\n1     13    14    15     3\n...\n20     1     2     3     1\n21    10    11    12     2\n...\n45     4     5     6     1\n46    16    17    18     3\n...\n']
476;4.0;0;29763620;;1;29;<python><pandas>;How to select all columns, except one column in pandas using .ix;26374.0;"[""    import pandas\n    import numpy as np\n    df = DataFrame(np.random.rand(4,4), columns = list('abcd'))\n    df\n          a         b         c         d\n    0  0.418762  0.042369  0.869203  0.972314\n    1  0.991058  0.510228  0.594784  0.534366\n    2  0.407472  0.259811  0.396664  0.894202\n    3  0.726168  0.139531  0.324932  0.906575\n"", 'column b', 'df.ix']"
477;3.0;1;29815129;;1;38;<python><list><dictionary><pandas><dataframe>;Pandas DataFrame to List of Dictionaries;15409.0;"[""rows = [{'customer': 1, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},\n    {'customer': 2, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},\n    {'customer': 3, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]\n""]"
478;3.0;0;29919306;;1;30;<python><pandas><dataframe><max>;Find the column name which has the maximum value for each row;10758.0;['In [7]:\nframe.head()\nOut[7]:\nCommunications and Search   Business    General Lifestyle\n0   0.745763    0.050847    0.118644    0.084746\n0   0.333333    0.000000    0.583333    0.083333\n0   0.617021    0.042553    0.297872    0.042553\n0   0.435897    0.000000    0.410256    0.153846\n0   0.358974    0.076923    0.410256    0.153846\n', 'In [7]:\n    frame.head()\n    Out[7]:\n    Communications and Search   Business    General Lifestyle   Max\n    0   0.745763    0.050847    0.118644    0.084746           Communications \n    0   0.333333    0.000000    0.583333    0.083333           Business  \n    0   0.617021    0.042553    0.297872    0.042553           Communications \n    0   0.435897    0.000000    0.410256    0.153846           Communications \n    0   0.358974    0.076923    0.410256    0.153846           Business \n']
479;3.0;0;30522724;;1;30;<python><numpy><pandas>;Take multiple lists into dataframe;38497.0;"[""percentile_list = pd.DataFrame({'lst1Tite' : [lst1],\n 'lst2Tite' : [lst2],\n 'lst3Tite':[lst3]\n  }, columns=['lst1Tite','lst1Tite', 'lst1Tite'])\n""]"
480;6.0;3;30522982;;1;28;<python><pandas><dataset>;List with many dictionaries VS dictionary with few lists?;1050.0;"['users = [\n    {""id"": 0, ""name"": ""Ashley""},\n    {""id"": 1, ""name"": ""Ben""},\n    {""id"": 2, ""name"": ""Conrad""},\n    {""id"": 3, ""name"": ""Doug""},\n    {""id"": 4, ""name"": ""Evin""},\n    {""id"": 5, ""name"": ""Florian""},\n    {""id"": 6, ""name"": ""Gerald""}\n]\n', 'users2 = {\n    ""id"": [0, 1, 2, 3, 4, 5, 6],\n    ""name"": [""Ashley"", ""Ben"", ""Conrad"", ""Doug"",""Evin"", ""Florian"", ""Gerald""]\n}\n', 'import pandas as pd\npd_users = pd.DataFrame(users)\npd_users2 = pd.DataFrame(users2)\nprint pd_users == pd_users2\n']"
481;2.0;2;30530663;;1;24;<python><pandas>;"How to ""select distinct"" across multiple data frame columns in pandas?";25025.0;[]
482;2.0;7;30631325;;1;27;<python><mysql><pandas><sqlalchemy><mysql-connector>;Writing to MySQL database with pandas using SQLAlchemy, to_sql;22941.0;"[""import pandas as pd\nimport mysql.connector\nfrom sqlalchemy import create_engine\n\nengine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)\ncnx = engine.raw_connection()\ndata = pd.read_sql('SELECT * FROM sample_table', cnx)\ndata.to_sql(name='sample_table2', con=cnx, if_exists = 'append', index=False)\n"", ""data.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)\n>>AttributeError: 'Engine' object has no attribute 'cursor'\n""]"
483;3.0;0;30926670;;1;26;<python><pandas>;Pandas: Add multiple empty columns to DataFrame;14668.0;"['df[""B""] = None\ndf[""C""] = None\ndf[""D""] = None\n', 'df[[""B"", ""C"", ""D""]] = None\n\nKeyError: ""[\'B\' \'C\' \'D\'] not in index""\n']"
484;3.0;0;31029560;;1;25;<python><pandas>;Plotting categorical data with pandas and matplotlib;20647.0;"['     colour  direction\n1    red     up\n2    blue    up\n3    green   down\n4    red     left\n5    red     right\n6    yellow  down\n7    blue    down\n', ""df.plot(kind='hist')\n""]"
485;4.0;0;31357611;;1;21;<python><pandas><matplotlib><plot>;Format y axis as percent;18791.0;"[""df['myvar'].plot(kind='bar')\n"", ""import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.ticker as mtick\n\ndata = [8,12,15,17,18,18.5]\nperc = np.linspace(0,100,len(data))\n\nfig = plt.figure(1, (7,4))\nax = fig.add_subplot(1,1,1)\n\nax.plot(perc, data)\n\nfmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\nxticks = mtick.FormatStrFormatter(fmt)\nax.xaxis.set_major_formatter(xticks)\n\nplt.show()\n""]"
486;2.0;3;31361721;;1;24;<python><pandas><parallel-processing><dask>;python dask DataFrame, support for (trivially parallelizable) row apply?;4089.0;['ts.apply(func) # for pandas series\ndf.apply(func, axis = 1) # for pandas DF row apply\n', 'ddf.assign(A=lambda df: df.apply(func, axis=1)).compute() # dask DataFrame\n', 'df.apply(func, axis = 1) # for pandas DF row apply\n', 'import dask.dataframe as dd\ns = pd.Series([10000]*120)\nds = dd.from_pandas(s, npartitions = 3)\n\ndef slow_func(k):\n    A = np.random.normal(size = k) # k = 10000\n    s = 0\n    for a in A:\n        if a > 0:\n            s += 1\n        else:\n            s -= 1\n    return s\n\ns.apply(slow_func) # 0.43 sec\nds.map(slow_func).compute() # 2.04 sec\n']
487;2.0;2;31593201;;1;218;<python><pandas><indexing><dataframe>;pandas iloc vs ix vs loc explanation?;118214.0;['DataFrame', 'df.loc[:5]\ndf.ix[:5]\ndf.iloc[:5]\n']
488;1.0;2;31609600;;1;30;<python><pandas><ipython>;Jupyter (IPython) notebook not plotting;22330.0;"[' ipython notebook --pylab==inline\n', '""Support for specifying --pylab on the command line has been removed. Please use \'%pylab = inline\' or \'%matplotlib =inline\' in the notebook itself""\n', '""matplotlib.axes._subplots.AxesSubplot at 0xebf8b70"".\n']"
489;4.0;0;32011359;;1;24;<python><pandas>;Convert categorical data in pandas dataframe;29142.0;"['col1        int64\ncol2        int64\ncol3        category\ncol4        category\ncol5        category\n', 'Name: col3, dtype: category\nCategories (8, object): [B, C, E, G, H, N, S, W]\n', '[1, 2, 3, 4, 5, 6, 7, 8]\n', ""dataframe['c'] = pandas.Categorical.from_array(dataframe.col3).codes\n""]"
490;2.0;0;32244019;;1;22;<python><pandas><matplotlib>;How to rotate x-axis tick labels in Pandas barplot;14916.0;"['import matplotlib\nmatplotlib.style.use(\'ggplot\')\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.DataFrame({ \'celltype\':[""foo"",""bar"",""qux"",""woz""], \'s1\':[5,9,1,7], \'s2\':[12,90,13,87]})\ndf = df[[""celltype"",""s1"",""s2""]]\ndf.set_index([""celltype""],inplace=True)\ndf.plot(kind=\'bar\',alpha=0.75)\nplt.xlabel("""")\n', 'plt.set_xticklabels(df.index,rotation=90)\n']"
491;4.0;0;32244753;;1;35;<python><pandas><matplotlib><seaborn>;How to save a Seaborn plot into a file;27148.0;"['test_seaborn.py', 'import matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nmatplotlib.style.use(\'ggplot\')\nimport seaborn as sns\nsns.set()\ndf = sns.load_dataset(\'iris\')\nsns_plot = sns.pairplot(df, hue=\'species\', size=2.5)\nfig = sns_plot.get_figure()\nfig.savefig(""output.png"")\n#sns.plt.show()\n', '  Traceback (most recent call last):\n  File ""test_searborn.py"", line 11, in <module>\n    fig = sns_plot.get_figure()\nAttributeError: \'PairGrid\' object has no attribute \'get_figure\'\n', 'output.png']"
492;4.0;2;32400867;;1;22;<python><csv><pandas><request>;Pandas read_csv from url;17280.0;"['import pandas as pd\nimport requests\n\nurl=""https://github.com/cs109/2014_data/blob/master/countries.csv""\ns=requests.get(url).content\nc=pd.read_csv(s)\n']"
493;4.0;5;32468402;;1;22;<python><pandas><dataframe>;How to explode a list inside a Dataframe cell into separate rows;8761.0;[]
494;1.0;6;33530753;;1;45;<python><arrays><numpy><pandas>;numpy: Reliable (non-conservative) indicator if numpy array is view;839.0;"['pandas', 'my_array.base is not None', 'numpy.may_share_memory()', ""flags['OWNDATA'])""]"
495;4.0;0;34001922;;1;21;<python><pandas><classification><tensorflow>;FailedPreconditionError: Attempting to use uninitialized in Tensorflow;20199.0;"['# Stuff from tensorflow tutorial \nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\nx = tf.placeholder(""float"", shape=[None, 784])\ny_ = tf.placeholder(""float"", shape=[None, 10])\n\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n\ny = tf.nn.softmax(tf.matmul(x,W) + b)\n\ncross_entropy = -tf.reduce_sum(y_*tf.log(y))\n\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n', '# Read dataframe from training data\ncsvfile=\'train.csv\'\nfrom pandas import DataFrame, read_csv\ndf = read_csv(csvfile)\n\n# Strip off the target data and make it a separate dataframe.\nTarget=df.label\ndel df[""label""]\n\n# Split data into training and testing sets\nmsk = np.random.rand(len(df)) < 0.8\ndfTest = df[~msk]\nTargetTest = Target[~msk]\ndf = df[msk]\nTarget = Target[msk]\n\n# One hot encode the target\nOHTarget=pd.get_dummies(Target)\nOHTargetTest=pd.get_dummies(TargetTest)\n', 'for i in range(100):\n    batch = np.array(df[i*50:i*50+50].values)\n    batch = np.multiply(batch, 1.0 / 255.0)\n    Target_batch = np.array(OHTarget[i*50:i*50+50].values)\n    Target_batch = np.multiply(Target_batch, 1.0 / 255.0)\n    train_step.run(feed_dict={x: batch, y_: Target_batch})\n', '---------------------------------------------------------------------------\nFailedPreconditionError                   Traceback (most recent call last)\n<ipython-input-82-967faab7d494> in <module>()\n      4     Target_batch = np.array(OHTarget[i*50:i*50+50].values)\n      5     Target_batch = np.multiply(Target_batch, 1.0 / 255.0)\n----> 6     train_step.run(feed_dict={x: batch, y_: Target_batch})\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in run(self, feed_dict, session)\n   1265         none, the default session will be used.\n   1266     """"""\n-> 1267     _run_using_default_session(self, feed_dict, self.graph, session)\n   1268\n   1269\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in _run_using_default_session(operation, feed_dict, graph, session)\n   2761                        ""the operation\'s graph is different from the session\'s ""\n   2762                        ""graph."")\n-> 2763   session.run(operation, feed_dict)\n   2764\n   2765\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict)\n    343\n    344     # Run request and get response.\n--> 345     results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n    346\n    347     # User may have fetched the same tensor multiple times, but we\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, target_list, fetch_list, feed_dict)\n    417         # pylint: disable=protected-access\n    418         raise errors._make_specific_exception(node_def, op, e.error_message,\n--> 419                                               e.code)\n    420         # pylint: enable=protected-access\n    421       raise e_type, e_value, e_traceback\n\nFailedPreconditionError: Attempting to use uninitialized value Variable_1\n     [[Node: gradients/add_grad/Shape_1 = Shape[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_1)]]\nCaused by op u\'gradients/add_grad/Shape_1\', defined at:\n  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main\n    ...........\n\n...which was originally created as op u\'add\', defined at:\n  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main\n    ""__main__"", fname, loader, pkg_name)\n[elided 17 identical lines from previous traceback]\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File ""<ipython-input-45-59183d86e462>"", line 1, in <module>\n    y = tf.nn.softmax(tf.matmul(x,W) + b)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 403, in binary_op_wrapper\n    return func(x, y, name=name)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 44, in add\n    return _op_def_lib.apply_op(""Add"", x=x, y=y, name=name)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 633, in apply_op\n    op_def=op_def)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1710, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 988, in __init__\n    self._traceback = _extract_stack()\n']"
496;2.0;0;34091877;;1;30;<python><csv><pandas><header>;How to add header row to a pandas DataFrame;55423.0;"['pandas', 'Cov = pd.read_csv(""path/to/file.txt"", sep=\'\\t\')\nFrame=pd.DataFrame([Cov], columns = [""Sequence"", ""Start"", ""End"", ""Coverage""])\nFrame.to_csv(""path/to/file.txt"", sep=\'\\t\')\n', 'ValueError: Shape of passed values is (1, 1), indices imply (4, 1)\n']"
497;3.0;1;34962104;;1;32;<python><pandas><dataframe><python-3.5>;Pandas: How can I use the apply() function for a single column?;36873.0;[]
498;1.0;0;36519086;;1;29;<python><pandas><ipython>;Pandas: how to get rid of `Unnamed:` column in a dataframe;12496.0;"['csv', 'df', 'unnamed:0', ""merge.to_csv('xy.df', mode = 'w', inplace=False)\n"", 'unnamed:0']"
499;3.0;2;36921951;;1;26;<python><pandas><dataframe><boolean><filtering>;Truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all();44469.0;"["" result = result[(result['var']>0.25) or (result['var']<-0.25)]\n""]"
500;0.0;11;37078880;;1;29;<python><r><pandas><parallel-processing><mclapply>;Status of parallelization of pandas.apply();4502.0;['parallelization', 'pandas.apply()', 'pandas.apply()', 'parallelization', 'R', 'mclapply']
501;3.0;7;38250710;;1;34;<pandas><numpy><dataframe><machine-learning><scikit-learn>;How to split data into 3 sets (train, validation and test)?;12872.0;['sklearn.cross_validation', 'train_test_split']
502;9.0;3;42157944;;1;34;<python><regex><performance><parsing><pandas>;How can I speed up reading multiple files and putting the data into a dataframe?;1476.0;"['import re\nimport pandas as pd\n\ndf = pd.DataFrame()\npaths = [""../gitignore/test1.txt"", ""../gitignore/test2.txt""]\nreg_ex = re.compile(\'^(.+) (.+)\\n\')\n# read all files to determine what indices are available\nfor path in paths:\n    file_obj = open(path, \'r\')\n    print file_obj.readlines()\n\n[\'a 1\\n\', \'b 2\\n\', \'end\']\n[\'c 3\\n\', \'d 4\\n\', \'end\']\n\nindices = []\nfor path in paths:\n    index = []\n    with open(path, \'r\') as file_obj:\n        line = True\n        while line:\n            try:\n                line = file_obj.readline()\n                match = reg_ex.match(line)\n                index += match.group(1)\n            except AttributeError:\n                pass\n    indices.append(index)\n# read files again and put data into a master dataframe\nfor path, index in zip(paths, indices):\n    subset_df = pd.DataFrame(index=index, columns=[""Number""])\n    with open(path, \'r\') as file_obj:\n        line = True\n        while line:\n            try:\n                line = file_obj.readline()\n                match = reg_ex.match(line)\n                subset_df.loc[[match.group(1)]] = match.group(2)\n            except AttributeError:\n                pass\n    df = pd.concat([df, subset_df]).sort_index()\nprint df\n\n  Number\na      1\nb      2\nc      3\nd      4\n', 'a 1\nb 2\nend\n', 'c 3\nd 4\nend\n']"
503;1.0;6;42347868;;1;24;<python>;Convert to date using formatters parameter in pandas to_string;839.0;"['import pandas as pd\n\nurl = ""https://raw.github.com/pandas-dev/pandas/master/pandas/tests/data/tips.csv""\ndf = pd.read_csv(url)\ndf[""date""] = list(range(42005, 42005+len(df)))\n', 'print(\n  df\n  .head(10)\n  .to_string(\n    formatters={""total_bill"": ""${:,.2f}"".format, \n                ""tip"": ""${:,.2f}"".format\n    }\n  )\n)\n']"
504;4.0;9;43423347;;1;23;<python><pandas>;why is blindly using df.copy() a bad idea to fix the SettingWithCopyWarning;697.0;"['SettingWithCopyWarning', 'df', 'is_copy', ""df = pd.DataFrame([[1]])\n\nd1 = df[:]\n\nd1.is_copy\n\n<weakref at 0x1115a4188; to 'DataFrame' at 0x1119bb0f0>\n"", 'None', 'd1 = d1.copy()\n', 'SettingWithCopyWarning', 'copy', 'df = df.copy()', 'SettingWithCopyWarning']"
505;1.0;4;44380068;;1;21;<python><pandas><numpy><linear-regression><statsmodels>;Pandas rolling regression: alternatives to looping;700.0;"['MovingOLS', 'stats/ols', 'MovingOLS', 'model = pd.MovingOLS(y, x)', '.t_stat', '.rmse', '.std_err', 'rolling.apply', '.rolling', 'func', '.apply', 'from datetime import date\nfrom pandas_datareader.data import DataReader\nimport statsmodels.formula.api as smf\n\nsyms = {\'TWEXBMTH\' : \'usd\', \n        \'T10Y2YM\' : \'term_spread\', \n        \'PCOPPUSDM\' : \'copper\'\n       }\n\nstart = date(2000, 1, 1)\ndata = (DataReader(syms.keys(), \'fred\', start)\n        .pct_change()\n        .dropna())\ndata = data.rename(columns = syms)\ndata = data.assign(intercept = 1.) # required by statsmodels OLS\n\ndef sliding_windows(x, window):\n    """"""Create rolling/sliding windows of length ~window~.\n\n    Given an array of shape (y, z), it will return ""blocks"" of shape\n    (x - window + 1, window, z).""""""\n\n    return np.array([x[i:i + window] for i \n                    in range(0, x.shape[0] - window + 1)])\n\ndata.head(3)\nOut[33]: \n                 usd  term_spread    copper  intercept\nDATE                                                  \n2000-02-01  0.012573    -1.409091 -0.019972        1.0\n2000-03-01 -0.000079     2.000000 -0.037202        1.0\n2000-04-01  0.005642     0.518519 -0.033275        1.0\n\nwindow = 36\nwins = sliding_windows(data.values, window=window)\ny, x = wins[:, :, 0], wins[:, :, 1:]\n\ncoefs = []\n\nfor endog, exog in zip(y, x):\n    model = smf.OLS(endog, exog).fit()\n        # The full set of model attributes gets lost with each loop\n    coefs.append(model.params)\n\ndf = pd.DataFrame(coefs, columns=data.iloc[:, 1:].columns,\n                  index=data.index[window - 1:])\n\ndf.head(3) # rolling 36m coefficients\nOut[70]: \n            term_spread    copper  intercept\nDATE                                        \n2003-01-01    -0.000122 -0.018426   0.001937\n2003-02-01     0.000391 -0.015740   0.001597\n2003-03-01     0.000655 -0.016811   0.001546\n']"
506;;2;7779260;7776679.0;2;38;;;;['append', 'ignore_index']
507;;6;7837947;7837722.0;2;111;;;;['close', 'pct_change = close[1:]/close[:-1]\n', 'pct_change = []\nfor row in close:\n    pct_change.append(...)\n', 'for i, row in enumerate(...)']
508;;4;7849789;7837722.0;2;63;;;;['for date, row in df.T.iteritems():\n   # do some logic here\n', 'def my_algo(ndarray[object] dates, ndarray[float64_t] open,\n            ndarray[float64_t] low, ndarray[float64_t] high,\n            ndarray[float64_t] close, ndarray[float64_t] volume):\n    cdef:\n        Py_ssize_t i, n\n        float64_t foo\n    n = len(dates)\n\n    for i from 0 <= i < n:\n        foo = close[i] - open[i] # will be extremely fast\n']
509;;2;8916746;8916302.0;2;34;;;;"['In [11]: df\nOut[11]: \n            A        B        C        D      \n2000-01-03 -0.59885 -0.18141 -0.68828 -0.77572\n2000-01-04  0.83935  0.15993  0.95911 -1.12959\n2000-01-05  2.80215 -0.10858 -1.62114 -0.20170\n2000-01-06  0.71670 -0.26707  1.36029  1.74254\n2000-01-07 -0.45749  0.22750  0.46291 -0.58431\n2000-01-10 -0.78702  0.44006 -0.36881 -0.13884\n2000-01-11  0.79577 -0.09198  0.14119  0.02668\n2000-01-12 -0.32297  0.62332  1.93595  0.78024\n2000-01-13  1.74683 -1.57738 -0.02134  0.11596\n2000-01-14 -0.55613  0.92145 -0.22832  1.56631\n2000-01-17 -0.55233 -0.28859 -1.18190 -0.80723\n2000-01-18  0.73274  0.24387  0.88146 -0.94490\n2000-01-19  0.56644 -0.49321  1.17584 -0.17585\n2000-01-20  1.56441  0.62331 -0.26904  0.11952\n2000-01-21  0.61834  0.17463 -1.62439  0.99103\n2000-01-24  0.86378 -0.68111 -0.15788 -0.16670\n2000-01-25 -1.12230 -0.16128  1.20401  1.08945\n2000-01-26 -0.63115  0.76077 -0.92795 -2.17118\n2000-01-27  1.37620 -1.10618 -0.37411  0.73780\n2000-01-28 -1.40276  1.98372  1.47096 -1.38043\n2000-01-31  0.54769  0.44100 -0.52775  0.84497\n2000-02-01  0.12443  0.32880 -0.71361  1.31778\n2000-02-02 -0.28986 -0.63931  0.88333 -2.58943\n2000-02-03  0.54408  1.17928 -0.26795 -0.51681\n2000-02-04 -0.07068 -1.29168 -0.59877 -1.45639\n2000-02-07 -0.65483 -0.29584 -0.02722  0.31270\n2000-02-08 -0.18529 -0.18701 -0.59132 -1.15239\n2000-02-09 -2.28496  0.36352  1.11596  0.02293\n2000-02-10  0.51054  0.97249  1.74501  0.20525\n2000-02-11  0.10100  0.27722  0.65843  1.73591\n\nIn [12]: df[(df.values > 1.5).any(1)]\nOut[12]: \n            A       B       C        D     \n2000-01-05  2.8021 -0.1086 -1.62114 -0.2017\n2000-01-06  0.7167 -0.2671  1.36029  1.7425\n2000-01-12 -0.3230  0.6233  1.93595  0.7802\n2000-01-13  1.7468 -1.5774 -0.02134  0.1160\n2000-01-14 -0.5561  0.9215 -0.22832  1.5663\n2000-01-20  1.5644  0.6233 -0.26904  0.1195\n2000-01-28 -1.4028  1.9837  1.47096 -1.3804\n2000-02-10  0.5105  0.9725  1.74501  0.2052\n2000-02-11  0.1010  0.2772  0.65843  1.7359\n', '&', '|', ""In [13]: df[(df['A'] > 1) | (df['B'] < -1)]\nOut[13]: \n            A        B       C        D     \n2000-01-05  2.80215 -0.1086 -1.62114 -0.2017\n2000-01-13  1.74683 -1.5774 -0.02134  0.1160\n2000-01-20  1.56441  0.6233 -0.26904  0.1195\n2000-01-27  1.37620 -1.1062 -0.37411  0.7378\n2000-02-04 -0.07068 -1.2917 -0.59877 -1.4564\n""]"
510;;8;8992714;8991709.0;2;185;;;;['data.table', 'data.table']
511;;6;8997908;8991709.0;2;93;;;;['data.table', 'Rprof()', 'sortedmatch(levels(i[[lc]]), levels(x[[rc]])', 'test.data.table()', 'data.table', 'data.table', 'roll=TRUE', 'data.table', 'data.table', 'roll=TRUE']
512;;1;9620832;9588331.0;2;12;;;;"['category,value\nAB,100.00\nAB,200.00\nAC,150.00\nAD,500.00\n', 'import pandas\ndata_2010 = pandas.read_csv(""/path/to/2010.csv"")\ndata_2010.groupby(""category"").agg([len, sum])\n', '          value     \n            len  sum\ncategory            \nAB            2  300\nAC            1  150\nAD            1  500\n']"
513;;0;9623878;9588331.0;2;14;;;;"['pivot_table', ""In [8]: df\nOut[8]: \n  category  value\n0       AB    100\n1       AB    200\n2       AC    150\n3       AD    500\n\nIn [9]: df.pivot_table(rows='category', aggfunc=[len, np.sum])\nOut[9]: \n            len    sum\n          value  value\ncategory              \nAB            2    300\nAC            1    150\nAD            1    500\n"", ""In [12]: df\nOut[12]: \n  category  value  value2\n0       AB    100       5\n1       AB    200       5\n2       AC    150       5\n3       AD    500       5\n\nIn [13]: df.pivot_table(rows='category', aggfunc=[len, np.sum])\nOut[13]: \n            len            sum        \n          value  value2  value  value2\ncategory                              \nAB            2       2    300      10\nAC            1       1    150       5\nAD            1       1    500       5\n"", '__builtin__.sum', 'np.sum']"
514;;6;9652858;9652832.0;2;49;;;;"[""DataFrame.from_csv('c:/~/trainSetRel3.txt', sep='\\t')\n"", 'header=0', ""DataFrame.from_csv('c:/~/trainSetRel3.txt', sep='\\t', header=0)\n""]"
515;;1;9656288;9652832.0;2;35;;;;['read_table(filepath)']
516;;1;9762084;9758450.0;2;71;;;;"[""subset = data_set[['data_date', 'data_1', 'data_2']]\ntuples = [tuple(x) for x in subset.values]\n""]"
517;;3;10202789;10202570.0;2;97;;;;"['argmax()', 'idxmax', "">>> import pandas\n>>> import numpy as np\n>>> df = pandas.DataFrame(np.random.randn(5,3),columns=['A','B','C'])\n>>> df\n          A         B         C\n0  1.232853 -1.979459 -0.573626\n1  0.140767  0.394940  1.068890\n2  0.742023  1.343977 -0.579745\n3  2.125299 -0.649328 -0.211692\n4 -0.187253  1.908618 -1.862934\n>>> df['A'].argmax()\n3\n>>> df['B'].argmax()\n4\n>>> df['C'].argmax()\n1\n"", 'idxmax', 'argmax', 'idxmax', 'numpy.argmax', ""numpy.argmax(df['A'])"", 'pandas', 'idxmax', 'argmax', 'Index', 'Index', 'idxmax', 'argmax', 'idxmax', 'numpy.argmax', 'DataFrame', ""In [19]: dfrm\nOut[19]: \n          A         B         C\na  0.143693  0.653810  0.586007\nb  0.623582  0.312903  0.919076\nc  0.165438  0.889809  0.000967\nd  0.308245  0.787776  0.571195\ne  0.870068  0.935626  0.606911\nf  0.037602  0.855193  0.728495\ng  0.605366  0.338105  0.696460\nh  0.000000  0.090814  0.963927\ni  0.688343  0.188468  0.352213\ni  0.879000  0.105039  0.900260\n\nIn [20]: dfrm['A'].idxmax()\nOut[20]: 'i'\n\nIn [21]: dfrm.ix[dfrm['A'].idxmax()]\nOut[21]: \n          A         B         C\ni  0.688343  0.188468  0.352213\ni  0.879000  0.105039  0.900260\n"", 'idxmax', 'argmax', 'idxmax']"
518;;3;10213167;10202570.0;2;52;;;;"['idxmax', ""In [5]: df = pandas.DataFrame(np.random.randn(10,3),columns=['A','B','C'])\n\nIn [6]: df\nOut[6]: \n          A         B         C\n0  2.001289  0.482561  1.579985\n1 -0.991646 -0.387835  1.320236\n2  0.143826 -1.096889  1.486508\n3 -0.193056 -0.499020  1.536540\n4 -2.083647 -3.074591  0.175772\n5 -0.186138 -1.949731  0.287432\n6 -0.480790 -1.771560 -0.930234\n7  0.227383 -0.278253  2.102004\n8 -0.002592  1.434192 -1.624915\n9  0.404911 -2.167599 -0.452900\n\nIn [7]: df.idxmax()\nOut[7]: \nA    0\nB    8\nC    7\n"", ""In [8]: df.ix[df['A'].idxmax()]\nOut[8]: \nA    2.001289\nB    0.482561\nC    1.579985\n""]"
519;;4;10374456;10373660.0;2;241;;;;"['g1', ""In [19]: type(g1)\nOut[19]: pandas.core.frame.DataFrame\n\nIn [20]: g1.index\nOut[20]: \nMultiIndex([('Alice', 'Seattle'), ('Bob', 'Seattle'), ('Mallory', 'Portland'),\n       ('Mallory', 'Seattle')], dtype=object)\n"", ""In [21]: g1.add_suffix('_Count').reset_index()\nOut[21]: \n      Name      City  City_Count  Name_Count\n0    Alice   Seattle           1           1\n1      Bob   Seattle           2           2\n2  Mallory  Portland           2           2\n3  Mallory   Seattle           1           1\n"", 'In [36]: DataFrame({\'count\' : df1.groupby( [ ""Name"", ""City""] ).size()}).reset_index()\nOut[36]: \n      Name      City  count\n0    Alice   Seattle      1\n1      Bob   Seattle      2\n2  Mallory  Portland      2\n3  Mallory   Seattle      1\n']"
520;;1;10458386;10457584.0;2;168;;;;"['set_index', ""In : col = ['a','b','c']\n\nIn : data = DataFrame([[1,2,3],[10,11,12],[20,21,22]],columns=col)\n\nIn : data\nOut:\n    a   b   c\n0   1   2   3\n1  10  11  12\n2  20  21  22\n\nIn : data2 = data.set_index('a')\n\nIn : data2\nOut:\n     b   c\na\n1    2   3\n10  11  12\n20  21  22\n""]"
521;;5;10465162;10464738.0;2;30;;;;"['DataFrame.apply', 'Series.interpolate', ""In : df = pandas.DataFrame(numpy.random.randn(5,3), index=['a','c','d','e','g'])\n\nIn : df\nOut:\n          0         1         2\na -1.987879 -2.028572  0.024493\nc  2.092605 -1.429537  0.204811\nd  0.767215  1.077814  0.565666\ne -1.027733  1.330702 -0.490780\ng -1.632493  0.938456  0.492695\n\nIn : df2 = df.reindex(['a','b','c','d','e','f','g'])\n\nIn : df2\nOut:\n          0         1         2\na -1.987879 -2.028572  0.024493\nb       NaN       NaN       NaN\nc  2.092605 -1.429537  0.204811\nd  0.767215  1.077814  0.565666\ne -1.027733  1.330702 -0.490780\nf       NaN       NaN       NaN\ng -1.632493  0.938456  0.492695\n\nIn : df2.apply(pandas.Series.interpolate)\nOut:\n          0         1         2\na -1.987879 -2.028572  0.024493\nb  0.052363 -1.729055  0.114652\nc  2.092605 -1.429537  0.204811\nd  0.767215  1.077814  0.565666\ne -1.027733  1.330702 -0.490780\nf -1.330113  1.134579  0.000958\ng -1.632493  0.938456  0.492695\n"", 'Series', 'NaN', 'Series']"
522;;1;10511230;10511024.0;2;19;;;;['import matplotlib.pyplot as plt', 'plt.show()\n']
523;;2;10511545;10511024.0;2;49;;;;[]
524;;4;10666301;10665889.0;2;23;;;;"['DataFrame', ""data = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))\ndata_ab = data[list('ab')]\ndata_cde = data[list('cde')]\n""]"
525;;5;10677896;10665889.0;2;137;;;;"["">>> df = DataFrame(np.random.rand(4,5), columns = list('abcde'))\n>>> df.ix[:,'b':]\n      b         c         d         e\n0  0.418762  0.042369  0.869203  0.972314\n1  0.991058  0.510228  0.594784  0.534366\n2  0.407472  0.259811  0.396664  0.894202\n3  0.726168  0.139531  0.324932  0.906575\n""]"
526;;8;10716007;10715965.0;2;185;;;;['pandas.concat()', 'DataFrame.append()']
527;;4;10739432;10729210.0;2;167;;;;['df.iterrows()']
528;;5;10762516;10751127.0;2;55;;;;"[""return pandas.Series({'pvalue': pvalue, 'mean_ratio': mean_ratio})\n""]"
529;;1;10781413;10591000.0;2;12;;;;['_convert_to_ndarrays', '_convert_types', 'df.phone = df.phone.astype(str)\n']
530;;2;10859883;10857924.0;2;73;;;;"['dropna', 'DataFrame.dropna', ""Definition: DataFrame.dropna(self, axis=0, how='any', thresh=None, subset=None)\nDocstring:\nReturn object with labels on given axis omitted where alternately any\nor all of the data are missing\n\nParameters\n----------\naxis : {0, 1}\nhow : {'any', 'all'}\n    any : if any NA values are present, drop that label\n    all : if all values are NA, drop that label\nthresh : int, default None\n    int value : require that many non-NA values\nsubset : array-like\n    Labels along other axis to consider, e.g. if you are dropping rows\n    these would be a list of columns to include\n\nReturns\n-------\ndropped : DataFrame\n"", ""df=df.dropna(axis=1,how='all')\n""]"
531;;4;10964938;10951341.0;2;63;;;;"['.apply(...)', 'DataFrame', ""grouped = df.groupby(keys)\n\ndef wavg(group):\n    d = group['data']\n    w = group['weights']\n    return (d * w).sum() / w.sum()\n\ngrouped.apply(wavg)\n""]"
532;;1;10982198;10982089.0;2;70;;;;['In [18]: a\nOut[18]: \n   x1  x2\n0   0   5\n1   1   6\n2   2   7\n3   3   8\n4   4   9\n\nIn [19]: a.x2 = a.x2.shift(1)\n\nIn [20]: a\nOut[20]: \n   x1  x2\n0   0 NaN\n1   1   5\n2   2   6\n3   3   7\n4   4   8\n']
533;;4;11005208;10867028.0;2;23;;;;"[""result.fillna('')"", 'na_values']"
534;;1;11067072;11067027.0;2;138;;;;['df.reindex_axis(sorted(df.columns), axis=1)\n']
535;;3;11073962;11073609.0;2;33;;;;"['df.groupby(df.index.map(lambda t: t.minute))\n', ""df.groupby([df.index.map(lambda t: t.minute), 'Source'])\n"", ""df.groupby([df['Source'],pd.TimeGrouper(freq='Min')])\n""]"
536;;0;11077060;11077023.0;2;36;;;;[]
537;;7;11077215;11077023.0;2;160;;;;[]
538;;2;11107627;11106823.0;2;6;;;;['(x.reindex_like(y).fillna(0) + y.fillna(0)).fillna(0)\n', '>>> x\n   A   B   C\n0  1   2 NaN\n1  3 NaN   4\n>>> y\n    A   B   C\n0   8 NaN  88\n1   2 NaN   5\n2  10  11  12\n>>> (x.reindex_like(y).fillna(0) + y.fillna(0)).fillna(0)\n    A   B   C\n0   9   2  88\n1   5   0   9\n2  10  11  12\n']
539;;1;11112419;11106823.0;2;45;;;;"['x.add(y, fill_value=0)', ""import pandas as pd\n\ndf1 = pd.DataFrame([(1,2),(3,4),(5,6)], columns=['a','b'])\nOut: \n   a  b\n0  1  2\n1  3  4\n2  5  6\n\ndf2 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['a','b'])\nOut: \n     a    b\n0  100  200\n1  300  400\n2  500  600\n\ndf_add = df1.add(df2, fill_value=0)\nOut: \n     a    b\n0  101  202\n1  303  404\n2  505  606\n""]"
540;;0;11138275;10065051.0;2;76;;;;"['MySQLdb', 'cx_Oracle', 'cx_Oracle', ""import pandas as pd\nimport cx_Oracle\n\nora_conn = cx_Oracle.connect('your_connection_string')\ndf_ora = pd.read_sql('select * from user_objects', con=ora_conn)    \nprint 'loaded dataframe from Oracle. # Records: ', len(df_ora)\nora_conn.close()\n"", 'MySQLdb', ""import MySQLdb\nmysql_cn= MySQLdb.connect(host='myhost', \n                port=3306,user='myusername', passwd='mypassword', \n                db='information_schema')\ndf_mysql = pd.read_sql('select * from VIEWS;', con=mysql_cn)    \nprint 'loaded dataframe from MySQL. records:', len(df_mysql)\nmysql_cn.close()\n""]"
541;;12;11287278;11285613.0;2;490;;;;"['__getitem__', ""df1 = df[['a','b']]\n"", 'df1 = df.ix[:,0:2] # Remember that Python does not slice inclusive of the ending index.\n', 'copy()', 'df1 = df.ix[0,0:2].copy() # To avoid the case where changing df1 also changes df\n']"
542;;9;11346337;11346283.0;2;768;;;;"['.columns', "">>> df = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\n>>> df.columns = ['a', 'b']\n>>> df\n   a   b\n0  1  10\n1  2  20\n""]"
543;;6;11354850;11346283.0;2;1304;;;;"[""df = df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'})\n# OR\ndf.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n""]"
544;;1;11362056;11361985.0;2;36;;;;['print paramdata.values\n', 'paramdata.columns\n', 'paramdata.index\n']
545;;0;11366429;11361985.0;2;9;;;;['DataFrame.head(x)', '.tail(x)']
546;;1;11366706;11350770.0;2;7;;;;['def stringSearchColumn_DataFrame(df, colName, regex):\n    newdf = DataFrame()\n    for idx, record in df[colName].iteritems():\n\n        if re.search(regex, record):\n            newdf = concat([df[df[colName] == record], newdf], ignore_index=True)\n\n    return newdf\n']
547;;2;11385335;11285613.0;2;35;;;;"[""In [39]: df\nOut[39]: \n   index  a  b  c\n0      1  2  3  4\n1      2  3  4  5\n\nIn [40]: df1 = df[['b', 'c']]\n\nIn [41]: df1\nOut[41]: \n   b  c\n0  3  4\n1  4  5\n""]"
548;;3;11385780;11067027.0;2;147;;;;['df.sort_index(axis=1)', 'df = df.sort_index(axis=1)', 'df.sort_index(axis=1, inplace=True)']
549;;0;11395193;11391969.0;2;10;;;;"[""data.groupby(lambda x: data['date'][x].year)\n""]"
550;;2;11397052;11391969.0;2;57;;;;"[""data.groupby(data['date'].map(lambda x: x.year))\n""]"
551;;1;11420594;11418192.0;2;9;;;;['In [39]: df\nOut[39]: \n      mass1     mass2  velocity\n0  1.461711 -0.404452  0.722502\n1 -2.169377  1.131037  0.232047\n2  0.009450 -0.868753  0.598470\n3  0.602463  0.299249  0.474564\n4 -0.675339 -0.816702  0.799289\n', 'In [40]: mask = (np.sin(df.velocity) / df.ix[:, 0:2].prod(axis=1)) > 0\n\nIn [41]: mask\nOut[41]: \n0    False\n1    False\n2    False\n3     True\n4     True\n', 'In [42]: df[mask]\nOut[42]: \n      mass1     mass2  velocity\n3  0.602463  0.299249  0.474564\n4 -0.675339 -0.816702  0.799289\n']
552;;0;11475486;11418192.0;2;56;;;;"['DataFrame.apply', ""In [3]: df = pandas.DataFrame(np.random.randn(5, 3), columns=['a', 'b', 'c'])\n\nIn [4]: df\nOut[4]: \n          a         b         c\n0 -0.001968 -1.877945 -1.515674\n1 -0.540628  0.793913 -0.983315\n2 -1.313574  1.946410  0.826350\n3  0.015763 -0.267860 -2.228350\n4  0.563111  1.195459  0.343168\n\nIn [6]: df[df.apply(lambda x: x['b'] > x['c'], axis=1)]\nOut[6]: \n          a         b         c\n1 -0.540628  0.793913 -0.983315\n2 -1.313574  1.946410  0.826350\n3  0.015763 -0.267860 -2.228350\n4  0.563111  1.195459  0.343168\n""]"
553;;3;11495086;11495051.0;2;15;;;;['lm( y ~ x - 1, data)\n', 'lm(num_rx ~ ridageyr - 1, data=demoq)\n']
554;;5;11531402;11350770.0;2;267;;;;"['df[df[\'A\'].str.contains(""hello"")]\n']"
555;;7;11548224;11548005.0;2;49;;;;['NaN']
556;;4;11617194;7837722.0;2;209;;;;['for index, row in df.iterrows():\n\n    # do some logic here\n', 'itertuples()']
557;;3;11617682;11615504.0;2;27;;;;"['index_col', 'parse_dates', 'from datetime import datetime\nimport pandas as pd\nparse = lambda x: datetime.strptime(x, \'%Y%m%d %H\')\npd.read_csv(""..\\\\file.csv"",  parse_dates = [[\'YYYYMMDD\', \'HH\']], \n            index_col = 0, \n            date_parser=parse)\n']"
558;;3;11622769;11622652.0;2;64;;;;['read_csv', 'np.mmap', 'iterator=True, chunksize=1000', 'pd.concat']
559;;2;11643893;11640243.0;2;51;;;;"[""import matplotlib.pyplot as plt\nimport numpy as np\nfrom pandas import DataFrame\ndf = DataFrame(np.random.randn(5, 3), columns=['A', 'B', 'C'])\n\nfig, ax = plt.subplots()\nax2, ax3 = ax.twinx(), ax.twinx()\nrspine = ax3.spines['right']\nrspine.set_position(('axes', 1.25))\nax3.set_frame_on(True)\nax3.patch.set_visible(False)\nfig.subplots_adjust(right=0.75)\n\ndf.A.plot(ax=ax, style='b-')\ndf.B.plot(ax=ax2, style='r-', secondary_y=True)\ndf.C.plot(ax=ax3, style='g-')\n""]"
560;;4;11706782;7837722.0;2;20;;;;['iterrows', 'itertuples', 'iterkv']
561;;1;11707706;11707586.0;2;15;;;;['print df.describe().to_string()', 'to_string()', 'describe', 'describe']
562;;2;11708664;11707586.0;2;21;;;;"['set_printoptions', ""In [3]: df.describe()\nOut[3]: \n<class 'pandas.core.frame.DataFrame'>\nIndex: 8 entries, count to max\nData columns:\nx1    8  non-null values\nx2    8  non-null values\nx3    8  non-null values\nx4    8  non-null values\nx5    8  non-null values\nx6    8  non-null values\nx7    8  non-null values\ndtypes: float64(7)\n\nIn [4]: pd.set_printoptions(precision=2)\n\nIn [5]: df.describe()\nOut[5]: \n            x1       x2       x3       x4       x5       x6       x7\ncount      8.0      8.0      8.0      8.0      8.0      8.0      8.0\nmean   69024.5  69025.5  69026.5  69027.5  69028.5  69029.5  69030.5\nstd       17.1     17.1     17.1     17.1     17.1     17.1     17.1\nmin    69000.0  69001.0  69002.0  69003.0  69004.0  69005.0  69006.0\n25%    69012.2  69013.2  69014.2  69015.2  69016.2  69017.2  69018.2\n50%    69024.5  69025.5  69026.5  69027.5  69028.5  69029.5  69030.5\n75%    69036.8  69037.8  69038.8  69039.8  69040.8  69041.8  69042.8\nmax    69049.0  69050.0  69051.0  69052.0  69053.0  69054.0  69055.0\n"", 'to_string', 'set_printoptions', 'to_string', 'In [3]: df.describe()\nOut[3]: \n                 x1            x2            x3            x4            x5  \\\ncount      8.000000      8.000000      8.000000      8.000000      8.000000   \nmean   59832.361578  27356.711336  49317.281222  51214.837838  51254.839690   \nstd    22600.723536  26867.192716  28071.737509  21012.422793  33831.515761   \nmin    31906.695474   1648.359160     56.378115  16278.322271     43.745574   \n25%    45264.625201  12799.540572  41429.628749  40374.273582  29789.643875   \n50%    56340.214856  18666.456293  51995.661512  54894.562656  47667.684422   \n75%    75587.003417  31375.610322  61069.190523  67811.893435  76014.884048   \nmax    98136.474782  84544.484627  91743.983895  75154.587156  99012.695717   \n\n                 x6            x7  \ncount      8.000000      8.000000  \nmean   41863.000717  33950.235126  \nstd    38709.468281  29075.745673  \nmin     3590.990740   1833.464154  \n25%    15145.759625   6879.523949  \n50%    22139.243042  33706.029946  \n75%    72038.983496  51449.893980  \nmax    98601.190488  83309.051963  \n', ""In [4]: pd.set_option('display.precision', 2)\n\nIn [5]: df.describe()\nOut[5]: \n            x1       x2       x3       x4       x5       x6       x7\ncount      8.0      8.0      8.0      8.0      8.0      8.0      8.0\nmean   59832.4  27356.7  49317.3  51214.8  51254.8  41863.0  33950.2\nstd    22600.7  26867.2  28071.7  21012.4  33831.5  38709.5  29075.7\nmin    31906.7   1648.4     56.4  16278.3     43.7   3591.0   1833.5\n25%    45264.6  12799.5  41429.6  40374.3  29789.6  15145.8   6879.5\n50%    56340.2  18666.5  51995.7  54894.6  47667.7  22139.2  33706.0\n75%    75587.0  31375.6  61069.2  67811.9  76014.9  72039.0  51449.9\nmax    98136.5  84544.5  91744.0  75154.6  99012.7  98601.2  83309.1\n""]"
563;;8;11711637;11707586.0;2;228;;;;"['print df.describe().to_string()', 'pandas.util.terminal.get_terminal_size()', '(width, height)', 'pandas.set_printoptions(max_rows=200, max_columns=10)', 'pandas.set_printoptions(...)', 'pandas.set_option', ""import pandas as pd\npd.set_option('display.height', 1000)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n""]"
564;;0;11858532;11858472.0;2;74;;;;"['df[\'bar\'] = df.bar.map(str) + "" is "" + df.foo']"
565;;3;11872393;11869910.0;2;186;;;;"['In [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [99]: df[(df.A == 1) & (df.D == 6)]\nOut[99]:\n   A  B  C  D\nd  1  3  9  6\n', ""In [90]: def mask(df, key, value):\n   ....:     return df[df[key] == value]\n   ....:\n\nIn [92]: pandas.DataFrame.mask = mask\n\nIn [93]: df = pandas.DataFrame(np.random.randint(0, 10, (4,4)), index=list('abcd'), columns=list('ABCD'))\n\nIn [95]: df.ix['d','A'] = df.ix['a', 'A']\n\nIn [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [97]: df.mask('A', 1)\nOut[97]:\n   A  B  C  D\na  1  4  9  1\nd  1  3  9  6\n\nIn [98]: df.mask('A', 1).mask('D', 6)\nOut[98]:\n   A  B  C  D\nd  1  3  9  6\n""]"
566;;0;11874590;11858472.0;2;30;;;;"[""df.apply(lambda x:'%s is %s' % (x['bar'],x['foo']),axis=1)\n""]"
567;;2;11893375;11869910.0;2;49;;;;['def mask(df, f):\n  return df[f(df)]\n', 'df.mask(lambda x: x[0] < 0).mask(lambda x: x[1] > 0)\n']
568;;8;11927922;11927715.0;2;61;;;;"['color', 'plot', ""from matplotlib import pyplot as plt\nfrom itertools import cycle, islice\nimport pandas, numpy as np  # I find np.random.randint to be better\n\n# Make the data\nx = [{i:np.random.randint(1,5)} for i in range(10)]\ndf = pandas.DataFrame(x)\n\n# Make a list by cycling through the colors you care about\n# to match the length of your data.\nmy_colors = list(islice(cycle(['b', 'r', 'g', 'y', 'k']), None, len(df)))\n\n# Specify this list of colors as the `color` option to `plot`.\ndf.plot(kind='bar', stacked=True, color=my_colors)\n"", ""my_colors = ['g', 'b']*5 # <-- this concatenates the list to itself 5 times.\nmy_colors = [(0.5,0.4,0.5), (0.75, 0.75, 0.25)]*5 # <-- make two custom RGBs and repeat/alternate them over all the bar elements.\nmy_colors = [(x/10.0, x/20.0, 0.75) for x in range(len(df))] # <-- Quick gradient example along the Red/Green dimensions.\n""]"
569;;10;11941772;11941492.0;2;9;;;;"[""df.ix['a']\ndf.ix['a'].ix['c']\n"", 'group1', 'group2', ""df.swaplevel(0,1).ix['c']\n""]"
570;;9;11942697;11941492.0;2;47;;;;"['xs', ""In [5]: df.xs('a', level=0)\nOut[5]: \n        value1  value2\ngroup2                \nc          1.1     7.1\nc          2.0     8.0\nd          3.0     9.0\n\nIn [6]: df.xs('c', level='group2')\nOut[6]: \n        value1  value2\ngroup1                \na          1.1     7.1\na          2.0     8.0\n""]"
571;;5;11982843;11976503.0;2;50;;;;"['In [5]: a.reset_index().merge(b, how=""left"").set_index(\'index\')\nOut[5]:\n       col1  to_merge_on  col2\nindex\na         1            1     1\nb         2            3     2\nc         3            4   NaN\n']"
572;;0;12022047;12021730.0;2;18;;;;"['\\', 'In [68]: data = read_table(\'sample.txt\', skiprows=3, header=None, sep=r""\\s*"")\n\nIn [69]: data\nOut[69]: \n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 7 entries, 0 to 6\nData columns:\nX.1     7  non-null values\nX.2     7  non-null values\nX.3     7  non-null values\nX.4     7  non-null values\nX.5     7  non-null values\nX.6     7  non-null values\n[...]\nX.23    7  non-null values\nX.24    7  non-null values\nX.25    5  non-null values\nX.26    3  non-null values\ndtypes: float64(8), int64(10), object(8)\n', 'In [73]: data.ix[:,20:]\nOut[73]: \n   X.21  X.22           X.23                   X.24            X.25    X.26\n0   315  0.95            ABC            transporter   transmembrane  region\n1   527  0.93            ABC            transporter            None    None\n2   408  0.86  RecF/RecN/SMC                      N        terminal  domain\n3   575  0.85  RecF/RecN/SMC                      N        terminal  domain\n4   556  0.72            AAA                 ATPase          domain    None\n5   275  0.85      YceG-like                 family            None    None\n6   200  0.85       Pyridine  nucleotide-disulphide  oxidoreductase    None\n']"
573;;0;12036847;10636024.0;2;11;;;;['QTableWidget', 'DataFrame', 'QTableWidgetObject', 'QTableWidgetItems', 'DataFrame', 'DataFrame', 'df  = read_csv(filename, index_col = 0,header = 0)\nself.datatable = QtGui.QTableWidget(parent=self)\nself.datatable.setColumnCount(len(df.columns))\nself.datatable.setRowCount(len(df.index))\nfor i in range(len(df.index)):\n    for j in range(len(df.columns)):\n        self.datatable.setItem(i,j,QtGui.QTableWidgetItem(str(df.iget_value(i, j))))\n']
574;;3;12056933;12047193.0;2;16;;;;"['read_sql', 'import decimal\n\nimport pydobc\nimport numpy as np\nimport pandas\n\ncnn, cur = myConnectToDBfunction()\ncmd = ""SELECT * FROM myTable""\ncur.execute(cmd)\ndataframe = __processCursor(cur, dataframe=True)\n\ndef __processCursor(cur, dataframe=False, index=None):\n    \'\'\'\n    Processes a database cursor with data on it into either\n    a structured numpy array or a pandas dataframe.\n\n    input:\n    cur - a pyodbc cursor that has just received data\n    dataframe - bool. if false, a numpy record array is returned\n                if true, return a pandas dataframe\n    index - list of column(s) to use as index in a pandas dataframe\n    \'\'\'\n    datatypes = []\n    colinfo = cur.description\n    for col in colinfo:\n        if col[1] == unicode:\n            datatypes.append((col[0], \'U%d\' % col[3]))\n        elif col[1] == str:\n            datatypes.append((col[0], \'S%d\' % col[3]))\n        elif col[1] in [float, decimal.Decimal]:\n            datatypes.append((col[0], \'f4\'))\n        elif col[1] == datetime.datetime:\n            datatypes.append((col[0], \'O4\'))\n        elif col[1] == int:\n            datatypes.append((col[0], \'i4\'))\n\n    data = []\n    for row in cur:\n        data.append(tuple(row))\n\n    array = np.array(data, dtype=datatypes)\n    if dataframe:\n        output = pandas.DataFrame.from_records(array)\n\n        if index is not None:\n            output = output.set_index(index)\n\n    else:\n        output = array\n\n    return output\n']"
575;;1;12060886;12047193.0;2;46;;;;['from pandas import DataFrame\ndf = DataFrame(resoverall.fetchall())\ndf.columns = resoverall.keys()\n']
576;;12;12065904;12065885.0;2;326;;;;"['isin', ""rpt[rpt['STK_ID'].isin(stk_list)]""]"
577;;6;12098586;12096252.0;2;363;;;;"[""In [5]: df = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})\n\nIn [6]: df\nOut[6]:\n   A  B\n0  5  1\n1  6  2\n2  3  3\n3  4  5\n\nIn [7]: df[df['A'].isin([3, 6])]\nOut[7]:\n   A  B\n1  6  2\n2  3  3\n""]"
578;;3;12183507;12182744.0;2;65;;;;['functools.partial', '>>> import functools\n>>> import operator\n>>> add_3 = functools.partial(operator.add,3)\n>>> add_3(2)\n5\n>>> add_3(7)\n10\n', 'partial', 'my_series.apply((lambda x: your_func(a,b,c,d,...,x)))\n', 'partial', 'my_series.apply(your_function, args=(2,3,4), extra_kw=1)\n']
579;;5;12192021;12190874.0;2;74;;;;"[""In [117]: import pandas\n\nIn [118]: import random\n\nIn [119]: df = pandas.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))\n\nIn [120]: rows = random.sample(df.index, 10)\n\nIn [121]: df_10 = df.ix[rows]\n\nIn [122]: df_90 = df.drop(rows)\n""]"
580;;11;12193309;11622652.0;2;66;;;;"[""from pandas import *\n\ntp = read_csv('large_dataset.csv', iterator=True, chunksize=1000)  # gives TextFileReader, which is iterable with chunks of 1000 rows.\ndf = concat(tp, ignore_index=True)  # df is DataFrame. If errors, do `list(tp)` instead of `tp`\n""]"
581;;7;12201723;12200693.0;2;30;;;;"[""In [97]: df = pandas.DataFrame({'month': np.random.randint(0,11, 100), 'A': np.random.randn(100), 'B': np.random.randn(100)})\n\nIn [98]: df.join(df.groupby('month')['A'].sum(), on='month', rsuffix='_r')\nOut[98]:\n           A         B  month       A_r\n0  -0.040710  0.182269      0 -0.331816\n1  -0.004867  0.642243      1  2.448232\n2  -0.162191  0.442338      4  2.045909\n3  -0.979875  1.367018      5 -2.736399\n4  -1.126198  0.338946      5 -2.736399\n5  -0.992209 -1.343258      1  2.448232\n6  -1.450310  0.021290      0 -0.331816\n7  -0.675345 -1.359915      9  2.722156\n""]"
582;;10;12207352;12207326.0;2;102;;;;"['.value_counts()', '>>> import pandas\n>>> my_series = pandas.Series([1,2,2,3,3,3, ""fred"", 1.8, 1.8])\n>>> my_series\n0       1\n1       2\n2       2\n3       3\n4       3\n5       3\n6    fred\n7     1.8\n8     1.8\n>>> counts = my_series.value_counts()\n>>> counts\n3       3\n2       2\n1.8     2\nfred    1\n1       1\n>>> len(counts)\n5\n>>> sum(counts)\n9\n>>> counts[""fred""]\n1\n>>> dict(counts)\n{1.8: 2, 2: 2, 3: 3, 1: 1, \'fred\': 1}\n']"
583;;5;12286958;12286607.0;2;37;;;;"['matplotlib.pcolor', ""import numpy as np \nfrom pandas import DataFrame\nimport matplotlib.pyplot as plt\n\nIndex= ['aaa', 'bbb', 'ccc', 'ddd', 'eee']\nCols = ['A', 'B', 'C', 'D']\ndf = DataFrame(abs(np.random.randn(5, 4)), index=Index, columns=Cols)\n\nplt.pcolor(df)\nplt.yticks(np.arange(0.5, len(df.index), 1), df.index)\nplt.xticks(np.arange(0.5, len(df.columns), 1), df.columns)\nplt.show()\n""]"
584;;11;12307162;12307099.0;2;148;;;;"[""df.ix[df.A==0, 'B'] = np.nan\n"", 'df.A==0', ""'B'"", ""df.ix[df.A==0, 'B'] = df.ix[df.A==0, 'B'] / 2\n""]"
585;;3;12322877;12322779.0;2;9;;;;"[""df = pandas.DataFrame({'b':[2,2,4,5], 'c': [3,3,0,9]}, index=[1,1,3,7])\ndf_unique = df.groupby(level=0).first()\n\ndf\n   b  c\n1  2  3\n1  2  3\n3  4  0\n7  5  9\n\ndf_unique\n   b  c\n1  2  3\n3  4  0\n7  5  9\n""]"
586;;4;12323599;12322779.0;2;48;;;;['In [29]: df.drop_duplicates()\nOut[29]: \n   b  c\n1  2  3\n3  4  0\n7  5  9\n']
587;;0;12332974;12200693.0;2;20;;;;"['apply', ""In [236]: df\nOut[236]: \n  yearmonth    return\n0    201202  0.922132\n1    201202  0.220270\n2    201202  0.228856\n3    201203  0.277170\n4    201203  0.747347\n\nIn [237]: def add_mkt_return(grp):\n   .....:     grp['mkt_return'] = grp['return'].sum()\n   .....:     return grp\n   .....: \n\nIn [238]: df.groupby('yearmonth').apply(add_mkt_return)\nOut[238]: \n  yearmonth    return  mkt_return\n0    201202  0.922132    1.371258\n1    201202  0.220270    1.371258\n2    201202  0.228856    1.371258\n3    201203  0.277170    1.024516\n4    201203  0.747347    1.024516\n""]"
588;;4;12335016;12200693.0;2;8;;;;['transform']
589;;3;12356541;12356501.0;2;92;;;;"['zip', 'In [1]: from pandas import *\n\nIn [2]: def calculate(x):\n   ...:     return x*2, x*3\n   ...: \n\nIn [3]: df = DataFrame({\'a\': [1,2,3], \'b\': [2,3,4]})\n\nIn [4]: df\nOut[4]: \n   a  b\n0  1  2\n1  2  3\n2  3  4\n\nIn [5]: df[""A1""], df[""A2""] = zip(*df[""a""].map(calculate))\n\nIn [6]: df\nOut[6]: \n   a  b  A1  A2\n0  1  2   2   3\n1  2  3   4   6\n2  3  4   6   9\n']"
590;;1;12377080;12376863.0;2;27;;;;"['is_hammer', 'row[""Open""]', 'def is_hammer(rOpen,rLow,rClose,rHigh):\n    return lower_wick_at_least_twice_real_body(rOpen,rLow,rClose) \\\n       and closed_in_top_half_of_range(rHigh,rLow,rClose)\n', 'df[""isHammer""] = map(is_hammer, df[""Open""], df[""Low""], df[""Close""], df[""High""])\n']"
591;;3;12377083;12376863.0;2;51;;;;"['map', 'apply', '>>> d\n    A   B  C\n0  11  13  5\n1   6   7  4\n2   8   3  6\n3   4   8  7\n4   0   1  7\n>>> (d.A + d.B) / d.C\n0    4.800000\n1    3.250000\n2    1.833333\n3    1.714286\n4    0.142857\n>>> d.A > d.C\n0     True\n1     True\n2     True\n3    False\n4    False\n', 'apply', 'axis=1', 'min(A, B)-C', "">>> d.apply(lambda row: min([row['A'], row['B']])-row['C'], axis=1)\n0    6\n1    2\n2   -3\n3   -3\n4   -7\n"", ""d['A'][:-1] < d['C'][1:]\n"", ""d['A'][1:] < d['C'][:-1]\n"", '[\'A""][:-1]', ""['C'][1:]""]"
592;;5;12394122;12389898.0;2;30;;;;"['apply', ""In [82]: df\nOut[82]: \n   X         Y\n0  0 -0.631214\n1  0  0.783142\n2  0  0.526045\n3  1 -1.750058\n4  1  1.163868\n5  1  1.625538\n6  1  0.076105\n7  2  0.183492\n8  2  0.541400\n9  2 -0.672809\n\nIn [83]: def func(x):\n   ....:     x['NewCol'] = np.nan\n   ....:     return x\n   ....: \n\nIn [84]: df.groupby('X').apply(func)\nOut[84]: \n   X         Y  NewCol\n0  0 -0.631214     NaN\n1  0  0.783142     NaN\n2  0  0.526045     NaN\n3  1 -1.750058     NaN\n4  1  1.163868     NaN\n5  1  1.625538     NaN\n6  1  0.076105     NaN\n7  2  0.183492     NaN\n8  2  0.541400     NaN\n9  2 -0.672809     NaN\n""]"
593;;0;12433236;12433076.0;2;32;;;;"['import urllib\n\nbase_url = ""http://ichart.finance.yahoo.com/table.csv?s=""\ndef make_url(ticker_symbol):\n    return base_url + ticker_symbol\n\noutput_path = ""C:/path/to/output/directory""\ndef make_filename(ticker_symbol, directory=""S&P""):\n    return output_path + ""/"" + directory + ""/"" + ticker_symbol + "".csv""\n\ndef pull_historical_data(ticker_symbol, directory=""S&P""):\n    try:\n        urllib.urlretrieve(make_url(ticker_symbol), make_filename(ticker_symbol, directory))\n    except urllib.ContentTooShortError as e:\n        outfile = open(make_filename(ticker_symbol, directory), ""w"")\n        outfile.write(e.content)\n        outfile.close()\n']"
594;;3;12497577;12497402.0;2;7;;;;"[""df.groupby(['A']).max()\n""]"
595;;1;12505089;12504976.0;2;28;;;;"['tolist', ""In [99]: import pandas as pd\n\nIn [100]: d1 = pd.DataFrame({'ticker' : ['spx 5/25/2001 p500', 'spx 5/25/2001 p600', 'spx 5/25/2001 p700']})\n\nIn [101]: d1.ticker.str.split().tolist()\nOut[101]: \n[['spx', '5/25/2001', 'p500'],\n ['spx', '5/25/2001', 'p600'],\n ['spx', '5/25/2001', 'p700']]\n"", 'In [102]: d2 = pd.DataFrame(d1.ticker.str.split().tolist(), \n   .....:                   columns=""symbol date price"".split())\n\nIn [103]: d2\nOut[103]: \n  symbol       date price\n0    spx  5/25/2001  p500\n1    spx  5/25/2001  p600\n2    spx  5/25/2001  p700\n', 'In [104]: d2[""price""] = d2[""price""].str.replace(""p"","""").astype(float)\n\nIn [105]: d2\nOut[105]: \n  symbol       date  price\n0    spx  5/25/2001    500\n1    spx  5/25/2001    600\n2    spx  5/25/2001    700\n', 'apply', 'In [113]: temp2.apply(lambda x: x[2])\nOut[113]: \n0    p500\n1    p600\n2    p700\nName: ticker\n']"
596;;3;12510334;12433076.0;2;85;;;;"['pandas', 'pandas.io.data.DataReader', ""from pandas.io.data import DataReader\nfrom datetime import datetime\n\nibm = DataReader('IBM',  'yahoo', datetime(2000, 1, 1), datetime(2012, 1, 1))\nprint(ibm['Adj Close'])\n"", 'pandas', 'pandas.io.data', 'pandas>=0.19', 'pandas-datareader', 'pip install pandas-datareader\n', ""import pandas_datareader as pdr\nfrom datetime import datetime\n\nibm = pdr.get_data_yahoo(symbols='IBM', start=datetime(2000, 1, 1), end=datetime(2012, 1, 1))\nprint(ibm['Adj Close'])\n"", 'pandas-datareader']"
597;;7;12525836;12525722.0;2;124;;;;['In [92]: df\nOut[92]:\n           a         b          c         d\nA  -0.488816  0.863769   4.325608 -4.721202\nB -11.937097  2.993993 -12.916784 -1.086236\nC  -5.569493  4.672679  -2.168464 -9.315900\nD   8.892368  0.932785   4.535396  0.598124\n\nIn [93]: df_norm = (df - df.mean()) / (df.max() - df.min())\n\nIn [94]: df_norm\nOut[94]:\n          a         b         c         d\nA  0.085789 -0.394348  0.337016 -0.109935\nB -0.463830  0.164926 -0.650963  0.256714\nC -0.158129  0.605652 -0.035090 -0.573389\nD  0.536170 -0.376229  0.349037  0.426611\n\nIn [95]: df_norm.mean()\nOut[95]:\na   -2.081668e-17\nb    4.857226e-17\nc    1.734723e-17\nd   -1.040834e-17\n\nIn [96]: df_norm.max() - df_norm.min()\nOut[96]:\na    1\nb    1\nc    1\nd    1\n']
598;;2;12555491;12555323.0;2;29;;;;"[""df1['e'] = np.random.randn(sLength)\n"", 'map', ""df1['e'] = df1['a'].map(lambda x: np.random.random())\n""]"
599;;22;12555510;12555323.0;2;411;;;;"[""df1['e'] = Series(np.random.randn(sLength), index=df1.index)\n"", 'SettingWithCopyWarning', "">>> sLength = len(df1['a'])\n>>> df1\n          a         b         c         d\n6 -0.269221 -0.026476  0.997517  1.294385\n8  0.917438  0.847941  0.034235 -0.448948\n\n>>> df1['e'] = p.Series(np.random.randn(sLength), index=df1.index)\n>>> df1\n          a         b         c         d         e\n6 -0.269221 -0.026476  0.997517  1.294385  1.757167\n8  0.917438  0.847941  0.034235 -0.448948  2.228131\n\n>>> p.version.short_version\n'0.16.1'\n"", 'SettingWithCopyWarning', "">>> df1.loc[:,'f'] = p.Series(np.random.randn(sLength), index=df1.index)\n>>> df1\n          a         b         c         d         e         f\n6 -0.269221 -0.026476  0.997517  1.294385  1.757167 -0.050927\n8  0.917438  0.847941  0.034235 -0.448948  2.228131  0.006109\n>>> \n"", 'assign', 'df1 = df1.assign(e=p.Series(np.random.randn(sLength)).values)\n']"
600;;0;12570410;12569730.0;2;43;;;;"['from pandas import *\n\nP1Channels = data.filter(regex=""P1"")\nP1Sum = P1Channels.sum(axis=1)\n']"
601;;11;12605055;12604909.0;2;54;;;;"[""df['Date'].str[-4:].astype(int)\n"", ""df['Date'].str.extract('(?P<year>\\d{4})').astype(int)\n"", 'apply', 'map', ""df['Date'] = df['Date'].apply(lambda x: int(str(x)[-4:]))\n             #  converts the last 4 characters of the string to an integer\n"", 'Date', ""def convert_to_year(date_in_some_format);\n    date_as_string = str(date_in_some_format)\n    year_as_string = date_in_some_format[-4:] # last four characters\n    return int(year_as_string)\n\ndf['Date'] = df['Date'].apply(convert_to_year)\n""]"
602;;3;12607018;12307099.0;2;55;;;;['df.ix', 'df.ix[selection criteria, columns I want] = value\n', 'df.A == 0', 'B', 'np.nan']
603;;8;12681217;12680754.0;2;22;;;;"[""In [55]: pd.concat([Series(row['var2'], row['var1'].split(','))              \n                    for _, row in a.iterrows()]).reset_index()\nOut[55]: \n  index  0\n0     a  1\n1     b  1\n2     c  1\n3     d  2\n4     e  2\n5     f  2\n""]"
604;;3;12726468;12725417.0;2;25;;;;"[""In [2]: import pandas as pd\n\nIn [3]: source = pd.DataFrame({'A': ['foo', 'bar'], 'B': [1, 2], 'C': [(1,2), (3,4)]})\n\nIn [4]: source\nOut[4]:\n     A  B       C\n0  foo  1  (1, 2)\n1  bar  2  (3, 4)\n\nIn [5]: source._get_numeric_data()\nOut[5]:\n   B\n0  1\n1  2\n""]"
605;;1;12741168;12741092.0;2;33;;;;['applymap', '>>> print df\n   A  B  C\n0 -1  0  0\n1 -4  3 -1\n2 -1  0  2\n3  0  3  2\n4  1 -1  0\n>>> print df.applymap(lambda x: x>1)\n       A      B      C\n0  False  False  False\n1  False   True  False\n2  False  False   True\n3  False   True   True\n4  False  False  False\n']
606;;0;12834193;10065051.0;2;8;;;;"['import pyodbc\nimport pandas.io.sql as psql\n\ncnxn = pyodbc.connect(\'DRIVER={SQL Server};SERVER=servername;DATABASE=mydb;UID=username;PWD=password\') \ncursor = cnxn.cursor()\nsql = (""""""select * from mytable"""""")\n\ndf = psql.frame_query(sql, cnxn)\ncnxn.close()\n']"
607;;2;12862196;12860421.0;2;37;;;;"[""In [39]: df2.pivot_table(values='X', rows='Y', cols='Z', \n                         aggfunc=lambda x: len(x.unique()))\nOut[39]: \nZ   Z1  Z2  Z3\nY             \nY1   1   1 NaN\nY2 NaN NaN   1\n"", 'len', 'NA', 'x.value_counts().count()', 'len(x.dropna().unique())']"
608;;1;12874054;12867178.0;2;46;;;;"[""df = male_trips.groupby('start_station_id').size()\n""]"
609;;0;12874135;12867178.0;2;18;;;;"['pandas.Series.value_counts', 'count_series = male_trips.start_station_id.value_counts()\n', 'count_series', ""stations['id']"", 'count_series = (\n                male_trips[male_trips.start_station_id.isin(stations.id.values)]\n                    .start_station_id\n                    .value_counts()\n               )\n', 'stations.id']"
610;;0;12882439;12877189.0;2;47;;;;"['float_format', 'to_csv', ""df.to_csv('pandasfile.csv', float_format='%.3f')\n"", ""df.to_csv('pandasfile.csv', float_format='%g')\n"", 'Bob,0.085\nAlice,0.005\n', '%g']"
611;;2;13003524;13003051.0;2;8;;;;['df.ix[:, df.columns - to_excl].hist()\n']
612;;0;13021797;13021654.0;2;82;;;;"['.get_loc()', 'In [45]: df = DataFrame({""pear"": [1,2,3], ""apple"": [2,3,4], ""orange"": [3,4,5]})\n\nIn [46]: df.columns\nOut[46]: Index([apple, orange, pear], dtype=object)\n\nIn [47]: df.columns.get_loc(""pear"")\nOut[47]: 2\n', 'df[""pear""]', 'df[[""apple"", ""orange""]]', 'df.columns.isin([""orange"", ""pear""])']"
613;;1;13036844;13021654.0;2;7;;;;['which', '(df.columns == name).nonzero()']
614;;2;13036848;13035764.0;2;53;;;;"['grouped = df3.groupby(level=0)\ndf4 = grouped.last()\ndf4\n                      A   B  rownum\n\n2001-01-01 00:00:00   0   0       6\n2001-01-01 01:00:00   1   1       7\n2001-01-01 02:00:00   2   2       8\n2001-01-01 03:00:00   3   3       3\n2001-01-01 04:00:00   4   4       4\n2001-01-01 05:00:00   5   5       5\n', 'MultiIndex', 'groupby', ""import numpy as np\nimport pandas\n\n# fake index\nidx = pandas.MultiIndex.from_tuples([('a', letter) for letter in list('abcde')])\n\n# random data + naming the index levels\ndf1 = pandas.DataFrame(np.random.normal(size=(5,2)), index=idx, columns=['colA', 'colB'])\ndf1.index.names = ['iA', 'iB']\n\n# artificially append some duplicate data\ndf1 = df1.append(df1.select(lambda idx: idx[1] in ['c', 'e']))\ndf1\n#           colA      colB\n#iA iB                    \n#a  a  -1.297535  0.691787\n#   b  -1.688411  0.404430\n#   c   0.275806 -0.078871\n#   d  -0.509815 -0.220326\n#   e  -0.066680  0.607233\n#   c   0.275806 -0.078871  # <--- dup 1\n#   e  -0.066680  0.607233  # <--- dup 2\n"", '# group the data, using df1.index.names tells pandas to look at the entire index\ngroups = df1.groupby(level=df1.index.names)  \ngroups.last() # or .first()\n#           colA      colB\n#iA iB                    \n#a  a  -1.297535  0.691787\n#   b  -1.688411  0.404430\n#   c   0.275806 -0.078871\n#   d  -0.509815 -0.220326\n#   e  -0.066680  0.607233\n']"
615;;6;13053267;12504976.0;2;41;;;;['In [43]: temp2.str[-1]\nOut[43]: \n0    p500\n1    p600\n2    p700\nName: ticker\n']
616;;6;13059751;12497402.0;2;64;;;;"['In [10]: df.drop_duplicates(subset=\'A\', keep=""last"")\nOut[10]: \n   A   B\n1  1  20\n3  2  40\n4  3  10\n', ""In [12]: df.groupby('A', group_keys=False).apply(lambda x: x.ix[x.B.idxmax()])\nOut[12]: \n   A   B\nA       \n1  1  20\n2  2  40\n3  3  10\n""]"
617;;4;13115473;13114512.0;2;50;;;;"[""In [26]: data\nOut[26]: \n           Date   Close  Adj Close\n251  2011-01-03  147.48     143.25\n250  2011-01-04  147.64     143.41\n249  2011-01-05  147.05     142.83\n248  2011-01-06  148.66     144.40\n247  2011-01-07  147.93     143.69\n\nIn [27]: data.set_index('Date').diff()\nOut[27]: \n            Close  Adj Close\nDate                        \n2011-01-03    NaN        NaN\n2011-01-04   0.16       0.16\n2011-01-05  -0.59      -0.58\n2011-01-06   1.61       1.57\n2011-01-07  -0.73      -0.71\n""]"
618;;1;13130357;13129618.0;2;45;;;;['import numpy as np\ncount, division = np.histogram(series)\n', 'count, division = np.histogram(series, bins = [-201,-149,949,1001])\n', 'series.hist(bins=division)\n']
619;;6;13148611;13148429.0;2;374;;;;"[""In [6]: df\nOut[6]:\n          0         1         2         3         4      mean\n0  0.445598  0.173835  0.343415  0.682252  0.582616  0.445543\n1  0.881592  0.696942  0.702232  0.696724  0.373551  0.670208\n2  0.662527  0.955193  0.131016  0.609548  0.804694  0.632596\n3  0.260919  0.783467  0.593433  0.033426  0.512019  0.436653\n4  0.131842  0.799367  0.182828  0.683330  0.019485  0.363371\n5  0.498784  0.873495  0.383811  0.699289  0.480447  0.587165\n6  0.388771  0.395757  0.745237  0.628406  0.784473  0.588529\n7  0.147986  0.459451  0.310961  0.706435  0.100914  0.345149\n8  0.394947  0.863494  0.585030  0.565944  0.356561  0.553195\n9  0.689260  0.865243  0.136481  0.386582  0.730399  0.561593\n\nIn [7]: cols = df.columns.tolist()\n\nIn [8]: cols\nOut[8]: [0L, 1L, 2L, 3L, 4L, 'mean']\n"", 'cols', ""In [12]: cols = cols[-1:] + cols[:-1]\n\nIn [13]: cols\nOut[13]: ['mean', 0L, 1L, 2L, 3L, 4L]\n"", 'In [16]: df = df[cols]  #    OR    df = df.ix[:, cols]\n\nIn [17]: df\nOut[17]:\n       mean         0         1         2         3         4\n0  0.445543  0.445598  0.173835  0.343415  0.682252  0.582616\n1  0.670208  0.881592  0.696942  0.702232  0.696724  0.373551\n2  0.632596  0.662527  0.955193  0.131016  0.609548  0.804694\n3  0.436653  0.260919  0.783467  0.593433  0.033426  0.512019\n4  0.363371  0.131842  0.799367  0.182828  0.683330  0.019485\n5  0.587165  0.498784  0.873495  0.383811  0.699289  0.480447\n6  0.588529  0.388771  0.395757  0.745237  0.628406  0.784473\n7  0.345149  0.147986  0.459451  0.310961  0.706435  0.100914\n8  0.553195  0.394947  0.863494  0.585030  0.565944  0.356561\n9  0.561593  0.689260  0.865243  0.136481  0.386582  0.730399\n']"
620;;6;13165753;11285613.0;2;44;;;;"['df.columns', ""['index','a','b','c']"", 'newdf = df[df.columns[2:4]] # Remember, Python is 0-offset! The ""3rd"" entry is at slot 2.\n', 'df.ix', '.columns', ""'index'"", 'DataFrame', 'df.index', 'Index', ""df['index']"", 'df.index', 'Index', 'Series', 'df.columns', 'pd.Index']"
621;;2;13181960;13167391.0;2;6;;;;['In [49]: pd.concat([group for _, group in grouped if len(group) > 1])\nOut[49]: \n     A  B\n0  foo  0\n2  foo  2\n3  foo  3\n']
622;;3;13193256;13187778.0;2;19;;;;"['to_records', 'object', ""In [102]: df\nOut[102]: \nlabel    A    B    C\nID                  \n1      NaN  0.2  NaN\n2      NaN  NaN  0.5\n3      NaN  0.2  0.5\n4      0.1  0.2  NaN\n5      0.1  0.2  0.5\n6      0.1  NaN  0.5\n7      0.1  NaN  NaN\n\nIn [103]: df.index.dtype\nOut[103]: dtype('object')\nIn [104]: df.to_records()\nOut[104]: \nrec.array([(1, nan, 0.2, nan), (2, nan, nan, 0.5), (3, nan, 0.2, 0.5),\n       (4, 0.1, 0.2, nan), (5, 0.1, 0.2, 0.5), (6, 0.1, nan, 0.5),\n       (7, 0.1, nan, nan)], \n      dtype=[('index', '|O8'), ('A', '<f8'), ('B', '<f8'), ('C', '<f8')])\nIn [106]: df.to_records().dtype\nOut[106]: dtype([('index', '|O8'), ('A', '<f8'), ('B', '<f8'), ('C', '<f8')])\n"", ""In [109]: df.index = df.index.astype('i8')\nIn [111]: df.to_records().view([('ID', '<i8'), ('A', '<f8'), ('B', '<f8'), ('C', '<f8')])\nOut[111]:\nrec.array([(1, nan, 0.2, nan), (2, nan, nan, 0.5), (3, nan, 0.2, 0.5),\n       (4, 0.1, 0.2, nan), (5, 0.1, 0.2, 0.5), (6, 0.1, nan, 0.5),\n       (7, 0.1, nan, nan)], \n      dtype=[('ID', '<i8'), ('A', '<f8'), ('B', '<f8'), ('C', '<f8')])\n"", 'ID', 'i8', 'f8']"
623;;2;13226352;13226029.0;2;54;;;;"['MultiIndex', ""import pandas as pd\nimport numpy as np\n\nnp.arrays = [['one','one','one','two','two','two'],[1,2,3,1,2,3]]\n\ndf = pd.DataFrame(np.random.randn(6,2),index=pd.MultiIndex.from_tuples(list(zip(*np.arrays))),columns=['A','B'])\n\ndf  # This is the dataframe we have generated\n\n          A         B\none 1 -0.732470 -0.313871\n    2 -0.031109 -2.068794\n    3  1.520652  0.471764\ntwo 1 -0.101713 -1.204458\n    2  0.958008 -0.455419\n    3 -0.191702 -0.915983\n"", 'df', 'df.ndim\n\n2\n', 'one', '1', '-0.732470 -0.313871', 'one', '2', '-0.031109 -2.068794', 'one', '3', '1.520652  0.471764', 'In [44]: df.ix[""one""]\nOut[44]: \n          A         B\n1 -0.732470 -0.313871\n2 -0.031109 -2.068794\n3  1.520652  0.471764\n', 'In [45]: df.ix[""one""].ix[1]\nOut[45]: \nA   -0.732470\nB   -0.313871\nName: 1\n', 'In [46]: df.ix[""one""].ix[1][""A""]\nOut[46]: -0.73247029752040727\n', 'groupby', ""In [47]: df.xs('one')\nOut[47]: \n          A         B\n1 -0.732470 -0.313871\n2 -0.031109 -2.068794\n3  1.520652  0.471764\n"", ""In [48]: df.xs('B', axis=1)\nOut[48]: \none  1   -0.313871\n     2   -2.068794\n     3    0.471764\ntwo  1   -1.204458\n     2   -0.455419\n     3   -0.915983\nName: B\n""]"
624;;2;13237914;11361985.0;2;125;;;;"[""pandas.set_option('display.max_columns', 7)\n"", ""pandas.set_option('display.max_columns', None)\n"", 'max_columns', '0']"
625;;1;13270110;13269890.0;2;31;;;;"[""from pandas import DataFrame, merge\ndf1 = DataFrame({'key':[1,1], 'col1':[1,2],'col2':[3,4]})\ndf2 = DataFrame({'key':[1,1], 'col3':[5,6]})\n\nmerge(df1, df2,on='key')[['col1', 'col2', 'col3']]\n""]"
626;;7;13295801;13295735.0;2;265;;;;['DataFrame.fillna()', 'In [7]: df\nOut[7]: \n          0         1\n0       NaN       NaN\n1 -0.494375  0.570994\n2       NaN       NaN\n3  1.876360 -0.229738\n4       NaN       NaN\n\nIn [8]: df.fillna(0)\nOut[8]: \n          0         1\n0  0.000000  0.000000\n1 -0.494375  0.570994\n2  0.000000  0.000000\n3  1.876360 -0.229738\n4  0.000000  0.000000\n', 'In [12]: df[1].fillna(0, inplace=True)\nOut[12]: \n0    0.000000\n1    0.570994\n2    0.000000\n3   -0.229738\n4    0.000000\nName: 1\n\nIn [13]: df\nOut[13]: \n          0         1\n0       NaN  0.000000\n1 -0.494375  0.570994\n2       NaN  0.000000\n3  1.876360 -0.229738\n4       NaN  0.000000\n']
627;;1;13316001;13148429.0;2;58;;;;"[""df.insert(0, 'mean', df.mean(1))\n""]"
628;;4;13332682;13331518.0;2;27;;;;['x = p.Series()\nN = 4\nfor i in xrange(N):\n   x = x.set_value(i, i**2)\n', '0    0\n1    1\n2    4\n3    9\n']
629;;11;13337376;13331698.0;2;143;;;;['apply', 'axis = 1', 'f', 'In [49]: df\nOut[49]: \n          0         1\n0  1.000000  0.000000\n1 -0.494375  0.570994\n2  1.000000  0.000000\n3  1.876360 -0.229738\n4  1.000000  0.000000\n\nIn [50]: def f(x):    \n   ....:  return x[0] + x[1]  \n   ....:  \n\nIn [51]: df.apply(f, axis=1) #passes a Series object, row-wise\nOut[51]: \n0    1.000000\n1    0.076619\n2    1.000000\n3    1.646622\n4    1.000000\n', 'group', 'apply']
630;;2;13384494;13293810.0;2;51;;;;"[""In [2]: read_csv('sample.csv', dtype={'ID': object})\nOut[2]: \n                           ID\n0  00013007854817840016671868\n1  00013007854817840016749251\n2  00013007854817840016754630\n3  00013007854817840016781876\n4  00013007854817840017028824\n5  00013007854817840017963235\n6  00013007854817840018860166\n""]"
631;;0;13385921;13385860.0;2;19;;;;"['df[""Make""] = df[""Make""].map(str.strip)', 'df']"
632;;0;13386025;13385860.0;2;30;;;;"['import pandas as pd\n\ndef strip(text):\n    try:\n        return text.strip()\n    except AttributeError:\n        return text\n\ndef make_int(text):\n    return int(text.strip(\'"" \'))\n\ntable = pd.read_table(""data.csv"", sep=r\',\',\n                      names=[""Year"", ""Make"", ""Model"", ""Description""],\n                      converters = {\'Description\' : strip,\n                                    \'Model\' : strip,\n                                    \'Make\' : strip,\n                                    \'Year\' : make_int})\nprint(table)\n', '   Year     Make   Model              Description\n0  1997     Ford    E350                     None\n1  1997     Ford    E350                     None\n2  1997     Ford    E350   Super, luxurious truck\n3  1997     Ford    E350  Super ""luxurious"" truck\n4  1997     Ford    E350    Super luxurious truck\n5  1997     Ford    E350                     None\n6  1997     Ford    E350                     None\n7  2000  Mercury  Cougar                     None\n']"
633;;0;13413842;13404468.0;2;43;;;;"[""from scipy.stats import ttest_ind\n\ncat1 = my_data[my_data['Category']=='cat1']\ncat2 = my_data[my_data['Category']=='cat2']\n\nttest_ind(cat1['values'], cat2['values'])\n>>> (1.4927289925706944, 0.16970867501294376)\n""]"
634;;4;13413845;13413590.0;2;235;;;;"['drop', 'EPS', ""df = df[np.isfinite(df['EPS'])]\n""]"
635;;0;13415772;13411544.0;2;23;;;;"['[]', 'df.column_name', 'In [1]: df = DataFrame([[1, 2, 3], [4, 5, 6]])\n\nIn [2]: df[1]\nOut[2]: \n0    2\n1    5\nName: 1\n\nIn [3]: df.1\n  File ""<ipython-input-3-e4803c0d1066>"", line 1\n    df.1\n       ^\nSyntaxError: invalid syntax\n']"
636;;2;13434501;13413590.0;2;410;;;;"['dropna()', 'In [24]: df = pd.DataFrame(np.random.randn(10,3))\n\nIn [25]: df.iloc[::2,0] = np.nan; df.iloc[::4,1] = np.nan; df.iloc[::3,2] = np.nan;\n\nIn [26]: df\nOut[26]:\n          0         1         2\n0       NaN       NaN       NaN\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n4       NaN       NaN  0.050742\n5 -1.250970  0.030561 -2.678622\n6       NaN  1.036043       NaN\n7  0.049896 -0.308003  0.823295\n8       NaN       NaN  0.637482\n9 -0.310130  0.078891       NaN\n', 'In [27]: df.dropna()     #drop all rows that have any NaN values\nOut[27]:\n          0         1         2\n1  2.677677 -1.466923 -0.750366\n5 -1.250970  0.030561 -2.678622\n7  0.049896 -0.308003  0.823295\n', ""In [28]: df.dropna(how='all')     #drop only if ALL columns are NaN\nOut[28]:\n          0         1         2\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n4       NaN       NaN  0.050742\n5 -1.250970  0.030561 -2.678622\n6       NaN  1.036043       NaN\n7  0.049896 -0.308003  0.823295\n8       NaN       NaN  0.637482\n9 -0.310130  0.078891       NaN\n"", 'In [29]: df.dropna(thresh=2)   #Drop row if it does not have at least two values that are **not** NaN\nOut[29]:\n          0         1         2\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n5 -1.250970  0.030561 -2.678622\n7  0.049896 -0.308003  0.823295\n9 -0.310130  0.078891       NaN\n', 'In [30]: df.dropna(subset=[1])   #Drop only if NaN in specific column (as asked in the question)\nOut[30]:\n          0         1         2\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n5 -1.250970  0.030561 -2.678622\n6       NaN  1.036043       NaN\n7  0.049896 -0.308003  0.823295\n9 -0.310130  0.078891       NaN\n']"
637;;4;13445630;13445241.0;2;22;;;;['d = d.applymap(lambda x: np.nan if isinstance(x, basestring) and x.isspace() else x)\n', 'applymap']
638;;1;13456432;13331518.0;2;12;;;;['test.append(pd.Series(200, index=[101]))\n']
639;;5;13485766;13411544.0;2;374;;;;['del df.column_name', 'del df[name]', 'df.__delitem__(name)']
640;;3;13581730;13575090.0;2;42;;;;"['pd.DataFrame.from_dict', ""orient='index'"", ""user_dict = {12: {'Category 1': {'att_1': 1, 'att_2': 'whatever'},\n                  'Category 2': {'att_1': 23, 'att_2': 'another'}},\n             15: {'Category 1': {'att_1': 10, 'att_2': 'foo'},\n                  'Category 2': {'att_1': 30, 'att_2': 'bar'}}}\n\npd.DataFrame.from_dict({(i,j): user_dict[i][j] \n                           for i in user_dict.keys() \n                           for j in user_dict[i].keys()},\n                       orient='index')\n\n\n               att_1     att_2\n12 Category 1      1  whatever\n   Category 2     23   another\n15 Category 1     10       foo\n   Category 2     30       bar\n"", ""user_ids = []\nframes = []\n\nfor user_id, d in user_dict.iteritems():\n    user_ids.append(user_id)\n    frames.append(pd.DataFrame.from_dict(d, orient='index'))\n\npd.concat(frames, keys=user_ids)\n\n               att_1     att_2\n12 Category 1      1  whatever\n   Category 2     23   another\n15 Category 1     10       foo\n   Category 2     30       bar\n""]"
641;;2;13583024;13582449.0;2;12;;;;"['kl.aggregate(np.sum)', "">>> df = DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n>>>                         'foo', 'bar', 'foo', 'foo'],\n...                  'B' : ['one', 'one', 'two', 'three',\n...                         'two', 'two', 'one', 'three'],\n...                  'C' : randn(8), 'D' : randn(8)})\n>>> grouped = df.groupby('A')\n>>> grouped\n<pandas.core.groupby.DataFrameGroupBy object at 0x04E2F630>\n>>> test = grouped.aggregate(np.sum)\n>>> test\n            C         D\nA                      \nbar -1.852376  2.204224\nfoo -3.398196 -0.045082\n""]"
642;;6;13592901;12589481.0;2;32;;;;"['In [20]: df.groupby(""dummy"").agg({""returns"": [np.mean, np.sum]})\nOut[20]: \n        returns          \n            sum      mean\n\ndummy                    \n1      0.285833  0.028583\n', ""In [21]: df.groupby('dummy').agg({'returns':\n                                  {'Mean': np.mean, 'Sum': np.sum}})\nOut[21]: \n        returns          \n            Sum      Mean\ndummy                    \n1      0.285833  0.028583\n""]"
643;;9;13616382;13611065.0;2;86;;;;"[""In [11]: df.loc[df['col1'] >= 1, 'col1']\nOut[11]: \n1    1\n2    2\nName: col1\n\nIn [12]: df[df['col1'] >= 1]\nOut[12]: \n   col1  col2\n1     1    11\n2     2    12\n\nIn [13]: df[(df['col1'] >= 1) & (df['col1'] <=1 )]\nOut[13]: \n   col1  col2\n1     1    11\n"", ""In [14]: def b(x, col, op, n): \n             return op(x[col],n)\n\nIn [15]: def f(x, *b):\n             return x[(np.logical_and(*b))]\n\nIn [16]: b1 = b(df, 'col1', ge, 1)\n\nIn [17]: b2 = b(df, 'col1', le, 1)\n\nIn [18]: f(df, b1, b2)\nOut[18]: \n   col1  col2\n1     1    11\n"", 'In [21]: df.query(\'col1 <= 1 & 1 <= col1\')\nOut[21]:\n   col1  col2\n1     1    11\n\nIn [22]: df.query(""col1 <= 1 and 1 <= df[\'col1\']"") \xa0# use df[] syntax if not a valid identifier\nOut[22]:\n   col1  col2\n1     1    11\n']"
644;;1;13653490;13651117.0;2;69;;;;"[""df[df['field'] > constant]"", ""iter_csv = pandas.read_csv('file.csv', iterator=True, chunksize=1000)\ndf = pd.concat([chunk[chunk['field'] > constant] for chunk in iter_csv])\n"", 'chunksize']"
645;;2;13655271;13654699.0;2;41;;;;"['pd.to_datetime', ""In [1]: import pandas as pd\n\nIn [2]: pd.to_datetime('2008-02-27')\nOut[2]: datetime.datetime(2008, 2, 27, 0, 0)\n"", 'df.index = pd.to_datetime(df.index)\n', ""df['date_col'] = df['date_col'].apply(pd.to_datetime)\n""]"
646;;6;13659944;13659881.0;2;31;;;;"[""d.groupby(['ip', 'useragent']).count()\n"", 'ip          useragent               \n192.168.0.1 a           2\n            b           1\n192.168.0.2 b           1\n']"
647;;1;13674286;12945971.0;2;62;;;;"['pandas', 'matplotlib.dates', 'matplotlib.units', 'matplotlib.dates', 'matplotlib.dates', 'import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport matplotlib.dates as dates\n\nidx = pd.date_range(\'2011-05-01\', \'2011-07-01\')\ns = pd.Series(np.random.randn(len(idx)), index=idx)\n\nfig, ax = plt.subplots()\nax.plot_date(idx.to_pydatetime(), s, \'v-\')\nax.xaxis.set_minor_locator(dates.WeekdayLocator(byweekday=(1),\n                                                interval=1))\nax.xaxis.set_minor_formatter(dates.DateFormatter(\'%d\\n%a\'))\nax.xaxis.grid(True, which=""minor"")\nax.yaxis.grid()\nax.xaxis.set_major_locator(dates.MonthLocator())\nax.xaxis.set_major_formatter(dates.DateFormatter(\'\\n\\n\\n%b\\n%Y\'))\nplt.tight_layout()\nplt.show()\n']"
648;;4;13680953;13636848.0;2;31;;;;"['difflib', 'get_closest_matches', 'df2', 'join', 'In [23]: import difflib \n\nIn [24]: difflib.get_close_matches\nOut[24]: <function difflib.get_close_matches>\n\nIn [25]: df2.index = df2.index.map(lambda x: difflib.get_close_matches(x, df1.index)[0])\n\nIn [26]: df2\nOut[26]: \n      letter\none        a\ntwo        b\nthree      c\nfour       d\nfive       e\n\nIn [31]: df1.join(df2)\nOut[31]: \n       number letter\none         1      a\ntwo         2      b\nthree       3      c\nfour        4      d\nfive        5      e\n', 'merge', ""df1 = DataFrame([[1,'one'],[2,'two'],[3,'three'],[4,'four'],[5,'five']], columns=['number', 'name'])\ndf2 = DataFrame([['a','one'],['b','too'],['c','three'],['d','fours'],['e','five']], columns=['letter', 'name'])\n\ndf2['name'] = df2['name'].apply(lambda x: difflib.get_close_matches(x, df1['name'])[0])\ndf1.merge(df2)\n""]"
649;;4;13682381;13682044.0;2;64;;;;"[""data['result'] = data['result'].map(lambda x: x.lstrip('+-').rstrip('aAbBcC'))\n""]"
650;;1;13688105;13682044.0;2;10;;;;"['str.lstrip', 'str.rstrip', ""In [8]: df['result'].str.lstrip('+-').str.rstrip('aAbBcC')\nOut[8]: \n1     52\n2     62\n3     44\n4     30\n5    110\nName: result\n""]"
651;;2;13703721;13703720.0;2;7;;;;"['str', 'to_datetime', ""In [11]: str(dt64)\nOut[11]: '2012-05-01T01:00:00.000000+0100'\n\nIn [12]: pd.to_datetime(str(dt64))\nOut[12]: datetime.datetime(2012, 5, 1, 1, 0, tzinfo=tzoffset(None, 3600))\n"", 'dt', 'In [13]: pd.to_datetime(str(dt64)).replace(tzinfo=None)\nOut[13]: datetime.datetime(2012, 5, 1, 1, 0)\n', ""In [21]: dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100')\n\nIn [22]: pd.to_datetime(str(dt64)).replace(tzinfo=None)\nOut[22]: datetime.datetime(2002, 6, 28, 1, 0)\n""]"
652;;5;13703930;13703720.0;2;21;;;;['>>> dt64.tolist()\ndatetime.datetime(2012, 5, 1, 0, 0)\n', 'DatetimeIndex', 'tolist', 'datetime', 'datetime64', 'datetime']
653;;9;13704307;13703720.0;2;72;;;;"['numpy.datetime64', 'numpy-1.8', "">>> from datetime import datetime\n>>> import numpy as np\n>>> dt = datetime.utcnow()\n>>> dt\ndatetime.datetime(2012, 12, 4, 19, 51, 25, 362455)\n>>> dt64 = np.datetime64(dt)\n>>> ts = (dt64 - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\n>>> ts\n1354650685.3624549\n>>> datetime.utcfromtimestamp(ts)\ndatetime.datetime(2012, 12, 4, 19, 51, 25, 362455)\n>>> np.__version__\n'1.8.0.dev-7b75899'\n"", 'np.datetime64', 'numpy-1.6', '>>> np.datetime64(datetime.utcnow()).astype(datetime)\ndatetime.datetime(2012, 12, 4, 13, 34, 52, 827542)\n', "">>> from datetime import datetime\n>>> import numpy \n>>> numpy.datetime64('2002-06-28T01:00:00.000000000+0100').astype(datetime)\ndatetime.datetime(2002, 6, 28, 0, 0)\n>>> numpy.__version__\n'1.6.2' # current version available via pip install numpy\n"", 'long', 'numpy-1.8.0', 'pip install git+https://github.com/numpy/numpy.git#egg=numpy-dev\n', "">>> from datetime import datetime\n>>> import numpy\n>>> numpy.datetime64('2002-06-28T01:00:00.000000000+0100').astype(datetime)\n1025222400000000000L\n>>> numpy.__version__\n'1.8.0.dev-7b75899'\n"", 'long', 'numpy.datetime64', '.astype(datetime)', '.astype(object)', 'long', 'numpy-1.8', "">>> dt64.dtype\ndtype('<M8[ns]')\n>>> ns = 1e-9 # number of seconds in a nanosecond\n>>> datetime.utcfromtimestamp(dt64.astype(int) * ns)\ndatetime.datetime(2002, 6, 28, 0, 0)\n"", "">>> dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100', 's')\n>>> dt64.dtype\ndtype('<M8[s]')\n>>> datetime.utcfromtimestamp(dt64.astype(int))\ndatetime.datetime(2002, 6, 28, 0, 0)\n""]"
654;;0;13731128;9758450.0;2;27;;;;['[tuple(x) for x in data_set.to_records(index=False)]\n']
655;;2;13753918;13703720.0;2;82;;;;"['pandas.Timestamp', ""In [16]: Timestamp(numpy.datetime64('2012-05-01T01:00:00.000000'))\nOut[16]: <Timestamp: 2012-05-01 01:00:00>\n"", ""numpy.datetime64('2012-05-01T01:00:00.000000+0100')\n"", 'pandas.to_datetime', ""In [24]: pandas.to_datetime('2012-05-01T01:00:00.000000+0100')\nOut[24]: datetime.datetime(2012, 5, 1, 1, 0, tzinfo=tzoffset(None, 3600))\n""]"
656;;7;13786327;13784192.0;2;118;;;;"['date_range', ""import datetime\nimport pandas as pd\nimport numpy as np\n\ntodays_date = datetime.datetime.now().date()\nindex = pd.date_range(todays_date-datetime.timedelta(10), periods=10, freq='D')\n\ncolumns = ['A','B', 'C']\n"", 'NaN', 'df_ = pd.DataFrame(index=index, columns=columns)\ndf_ = df_.fillna(0) # with 0s rather than NaNs\n', 'data = np.array([np.arange(10)]*3).T\n', 'In [10]: df = pd.DataFrame(data, index=index, columns=columns)\n\nIn [11]: df\nOut[11]: \n            A  B  C\n2012-11-29  0  0  0\n2012-11-30  1  1  1\n2012-12-01  2  2  2\n2012-12-02  3  3  3\n2012-12-03  4  4  4\n2012-12-04  5  5  5\n2012-12-05  6  6  6\n2012-12-06  7  7  7\n2012-12-07  8  8  8\n2012-12-08  9  9  9\n']"
657;;1;13833239;11232275.0;2;12;;;;"[""df.drop_duplicates(['foo','bar'])\ndf.pivot('foo','bar','baz')\n""]"
658;;8;13839029;13838405.0;2;33;;;;"['In [21]: df[\'m\'] = pd.Categorical(df[\'m\'], [""March"", ""April"", ""Dec""])\n\nIn [22]: df  # looks the same!\nOut[22]:\n   a  b      m\n0  1  2  March\n1  5  6    Dec\n2  3  4  April\n', 'In [23]: df.sort(""m"")\nOut[23]:\n   a  b      m\n0  1  2  March\n2  3  4  April\n1  5  6    Dec\n', 'set_index', ""df = pd.DataFrame([[1, 2, 'March'],[5, 6, 'Dec'],[3, 4, 'April']], columns=['a','b','m'])\ns = df['m'].apply(lambda x: {'March':0, 'April':1, 'Dec':3}[x])\ns.sort()\n\nIn [4]: df.set_index(s.index).sort()\nOut[4]: \n   a  b      m\n0  1  2  March\n1  3  4  April\n2  5  6    Dec\n"", 'replace', ""s = df['m'].replace({'March':0, 'April':1, 'Dec':3})\n""]"
659;;8;13842286;13842088.0;2;168;;;;"[""df.set_value('C', 'x', 10)"", ""df.xs('C')['x']=10"", ""df.xs('C')"", ""df.xs('C')['x']=10\n"", ""df['x']"", 'df', ""df['x']['C'] = 10\n"", 'df', ""df.xs('C', copy = False)['x']=10\n"", 'df', ""df.set_value('C', 'x', 10)"", ""In [18]: %timeit df.set_value('C', 'x', 10)\n100000 loops, best of 3: 2.9 s per loop\n\nIn [20]: %timeit df['x']['C'] = 10\n100000 loops, best of 3: 6.31 s per loop\n\nIn [19]: %timeit df.ix['C','x'] = 10\n10000 loops, best of 3: 104 s per loop\n\nIn [32]: %timeit df.xs('C', copy=False)['x'] = 10\n10000 loops, best of 3: 89.2 s per loop\n""]"
660;;3;13843741;12555323.0;2;99;;;;"[""df['e'] = e""]"
661;;1;13851602;13851535.0;2;42;;;;"[""len(df['column name'])"", 'len', ""df['column name'].map(len)"", ""df[df['column name'].map(len) < 2]\n""]"
662;;3;13866073;13636592.0;2;30;;;;"['sort_index', 'sort_values', ""In [23]: songs.sort_index(by=['Peak', 'Weeks'], ascending=[True, False])\nOut[23]: \n                                      Song  Peak  Weeks\n10                           She Loves You     1     36\n118                               Hey Jude     1     27\n20                I Want To Hold Your Hand     1     24\n22                       Can't Buy Me Love     1     17\n56                                   Help!     1     17\n76                        Paperback Writer     1     16\n109                   All You Need Is Love     1     16\n45                             I Feel Fine     1     15\n29                      A Hard Day's Night     1     14\n48                          Ticket To Ride     1     14\n85                           Eleanor Rigby     1     14\n87                        Yellow Submarine     1     14\n173            The Ballad Of John And Yoko     1     13\n60                             Day Tripper     1     12\n61                      We Can Work It Out     1     12\n117                           Lady Madonna     1      9\n8                           From Me To You     1      7\n115                          Hello Goodbye     1      7\n155                               Get Back     1      6\n2                         Please Please Me     2     20\n107                   Magical Mystery Tour     2     16\n176                              Let It Be     2     14\n93                              Penny Lane     2     13\n92               Strawberry Fields Forever     2     12\n0                               Love Me Do     4     26\n166                          Come Together     4     10\n157                              Something     4      9\n58                               Yesterday     8     21\n135                   Back In The U.S.S.R.    19      3\n164                     Here Comes The Sun    58     19\n96   Sgt. Pepper's Lonely Hearts Club Band    63     12\n105     With A Little Help From My Friends    63      7\n""]"
663;;3;13873014;13872533.0;2;144;;;;['ax = df1.plot()\ndf2.plot(ax=ax)\n']
664;;0;13876784;13872533.0;2;15;;;;"['groupby', 'unstack', ""In [1]: df\nOut[1]:\n            value  \ndatetime                         \n2010-01-01      1  \n2010-02-01      1  \n2009-01-01      1  \n\n# create additional month and year columns for convenience\ndf['Month'] = map(lambda x: x.month, df.index)\ndf['Year'] = map(lambda x: x.year, df.index)    \n\nIn [5]: df.groupby(['Month','Year']).mean().unstack()\nOut[5]:\n       value      \nYear    2009  2010\nMonth             \n1          1     1\n2        NaN     1\n"", ""df.groupby(['Month','Year']).mean().unstack().plot()\n""]"
665;;4;13888546;13888468.0;2;24;;;;['index.levels', 'In [11]: df\nOut[11]: \n       C\nA B     \n0 one  3\n1 one  2\n2 two  1\n\nIn [12]: df.index.levels[1]\nOut[12]: Index([one, two], dtype=object)\n']
666;;0;13921674;13921647.0;2;74;;;;['df.shape', 'df']
667;;0;14000420;13999850.0;2;27;;;;"['strftime', ""df['date'] = df['datetime'].apply(lambda x: x.strftime('%d%m%Y'))\ndf['time'] = df['datetime'].apply(lambda x: x.strftime('%H%M%S'))\n"", ""df[['date', 'time', ... ]].to_csv('df.csv')\n""]"
668;;0;14016590;14016247.0;2;24;;;;"['df', ""import numpy as np\nindex = df['b'].index[df['b'].apply(np.isnan)]\n"", 'MultiIndex', 'df', ""df['a'].ix[index[0]]\n>>> 1.452354\n"", 'df_index = df.index.values.tolist()\n[df_index.index(i) for i in index]\n>>> [3, 6]\n']"
669;;2;14033137;14016247.0;2;95;;;;['inds = pd.isnull(df).any(1).nonzero()[0]', 'In [9]: df\nOut[9]: \n          0         1\n0  0.450319  0.062595\n1 -0.673058  0.156073\n2 -0.871179 -0.118575\n3  0.594188       NaN\n4 -1.017903 -0.484744\n5  0.860375  0.239265\n6 -0.640070       NaN\n7 -0.535802  1.632932\n8  0.876523 -0.153634\n9 -0.686914  0.131185\n\nIn [10]: pd.isnull(df).any(1).nonzero()[0]\nOut[10]: array([3, 6])\n']
670;;3;14058892;14057007.0;2;39;;;;['numpy.logical_not', 'isin', 'In [63]: s = pd.Series(np.arange(10.0))\n\nIn [64]: x = range(4, 8)\n\nIn [65]: mask = np.logical_not(s.isin(x))\n\nIn [66]: s[mask]\nOut[66]: \n0    0\n1    1\n2    2\n3    3\n8    8\n9    9\n', 's[-s.isin(x)]\n']
671;;1;14059783;14059094.0;2;15;;;;"['apply', ""order_df['Value'] = order_df.apply(lambda row: (row['Prices']*row['Amount']\n                                               if row['Action']=='Sell'\n                                               else -row['Prices']*row['Amount']),\n                                   axis=1)\n""]"
672;;0;14060625;14059094.0;2;7;;;;"[""In [22]: orders_df['C'] = orders_df.Action.apply(\n               lambda x: (1 if x == 'Sell' else -1))\n\nIn [23]: orders_df   # New column C represents the sign of the transaction\nOut[23]:\n   Prices  Amount Action  C\n0       3      57   Sell  1\n1      89      42   Sell  1\n2      45      70    Buy -1\n3       6      43   Sell  1\n4      60      47   Sell  1\n5      19      16    Buy -1\n6      56      89   Sell  1\n7       3      28    Buy -1\n8      56      69   Sell  1\n9      90      49    Buy -1\n"", 'if', 'DataFrame.apply()', 'for', ""In [24]: orders_df['Value'] = orders_df.Prices * orders_df.Amount * orders_df.C\n\nIn [25]: orders_df   # The resulting dataframe\nOut[25]:\n   Prices  Amount Action  C  Value\n0       3      57   Sell  1    171\n1      89      42   Sell  1   3738\n2      45      70    Buy -1  -3150\n3       6      43   Sell  1    258\n4      60      47   Sell  1   2820\n5      19      16    Buy -1   -304\n6      56      89   Sell  1   4984\n7       3      28    Buy -1    -84\n8      56      69   Sell  1   3864\n9      90      49    Buy -1  -4410\n""]"
673;;4;14071265;14059094.0;2;50;;;;"['where', 'API docs', ""In [37]: values = df.Prices * df.Amount\n\nIn [38]: df['Values'] = values.where(df.Action == 'Sell', other=-values)\n\nIn [39]: df\nOut[39]: \n   Prices  Amount Action  Values\n0       3      57   Sell     171\n1      89      42   Sell    3738\n2      45      70    Buy   -3150\n3       6      43   Sell     258\n4      60      47   Sell    2820\n5      19      16    Buy    -304\n6      56      89   Sell    4984\n7       3      28    Buy     -84\n8      56      69   Sell    3864\n9      90      49    Buy   -4410\n""]"
674;;1;14163174;14162723.0;2;8;;;;"['nan', 'None', "">>> x = np.array([1, np.nan, 3])\n>>> y = np.where(np.isnan(x), None, x)\n>>> print y\n[1.0 None 3.0]\n>>> print type(y[1])\n<type 'NoneType'>\n""]"
675;;5;14163209;14162723.0;2;56;;;;"['where', 'df1 = df.where((pd.notnull(df)), None)\n', 'object', 'In [1]: df = pd.DataFrame([1, np.nan])\n\nIn [2]: df\nOut[2]: \n    0\n0   1\n1 NaN\n\nIn [3]: df1 = df.where((pd.notnull(df)), None)\n\nIn [4]: df1\nOut[4]: \n      0\n0     1\n1  None\n', 'dtype', 'astype', 'fillna', ""df1 = df.astype(object).replace(np.nan, 'None')\n"", 'replace', 'None']"
676;;5;14225838;14225676.0;2;67;;;;"['ExcelWriter', 'from pandas import ExcelWriter\n# from pandas.io.parsers import ExcelWriter\n', 'save_xls', ""def save_xls(list_dfs, xls_path):\n    writer = ExcelWriter(xls_path)\n    for n, df in enumerate(list_dfs):\n        df.to_excel(writer,'sheet%s' % n)\n    writer.save()\n""]"
677;;0;14247708;14247586.0;2;119;;;;['pandas', 'isnull', 'DataFrame', 'isnull', 'any', '>>> df = pd.DataFrame([range(3), [0, np.NaN, 0], [0, 0, np.NaN], range(3), range(3)])\n>>> df.isnull()\n       0      1      2\n0  False  False  False\n1  False   True  False\n2  False  False   True\n3  False  False  False\n4  False  False  False\n>>> df.isnull().any(axis=1)\n0    False\n1     True\n2     True\n3    False\n4    False\ndtype: bool\n>>> df[df.isnull().any(axis=1)]\n   0   1   2\n1  0 NaN   0\n2  0   0 NaN\n', 'pandas', 'isnull', 'In [56]: df = pd.DataFrame([range(3), [0, np.NaN, 0], [0, 0, np.NaN], range(3), range(3)])\n\nIn [57]: df\nOut[57]: \n   0   1   2\n0  0   1   2\n1  0 NaN   0\n2  0   0 NaN\n3  0   1   2\n4  0   1   2\n\nIn [58]: pd.isnull(df)\nOut[58]: \n       0      1      2\n0  False  False  False\n1  False   True  False\n2  False  False   True\n3  False  False  False\n4  False  False  False\n\nIn [59]: pd.isnull(df).any(axis=1)\nOut[59]: \n0    False\n1     True\n2     True\n3    False\n4    False\n', 'In [60]: df[pd.isnull(df).any(axis=1)]\nOut[60]: \n   0   1   2\n1  0 NaN   0\n2  0   0 NaN\n']
678;;19;14268804;14262433.0;2;364;;;;"['0.10.1', ""import numpy as np\nimport pandas as pd\n\n# create a store\nstore = pd.HDFStore('mystore.h5')\n\n# this is the key to your storage:\n#    this maps your fields to a specific group, and defines \n#    what you want to have as data_columns.\n#    you might want to create a nice class wrapping this\n#    (as you will want to have this map and its inversion)  \ngroup_map = dict(\n    A = dict(fields = ['field_1','field_2',.....], dc = ['field_1',....,'field_5']),\n    B = dict(fields = ['field_10',......        ], dc = ['field_10']),\n    .....\n    REPORTING_ONLY = dict(fields = ['field_1000','field_1001',...], dc = []),\n\n)\n\ngroup_map_inverted = dict()\nfor g, v in group_map.items():\n    group_map_inverted.update(dict([ (f,g) for f in v['fields'] ]))\n"", 'append_to_multiple', ""for f in files:\n   # read in the file, additional options hmay be necessary here\n   # the chunksize is not strictly necessary, you may be able to slurp each \n   # file into memory in which case just eliminate this part of the loop \n   # (you can also change chunksize if necessary)\n   for chunk in pd.read_table(f, chunksize=50000):\n       # we are going to append to each table by group\n       # we are not going to create indexes at this time\n       # but we *ARE* going to create (some) data_columns\n\n       # figure out the field groupings\n       for g, v in group_map.items():\n             # create the frame for this group\n             frame = chunk.reindex(columns = v['fields'], copy = False)    \n\n             # append it\n             store.append(g, frame, index=False, data_columns = v['dc'])\n"", ""frame = store.select(group_that_I_want)\n# you can optionally specify:\n# columns = a list of the columns IN THAT GROUP (if you wanted to\n#     select only say 3 out of the 20 columns in this sub-table)\n# and a where clause if you want a subset of the rows\n\n# do calculations on this frame\nnew_frame = cool_function_on_frame(frame)\n\n# to 'add columns', create a new group (you probably want to\n# limit the columns in this new_group to be only NEW ones\n# (e.g. so you don't overlap from the other tables)\n# add this info to the group_map\nstore.append(new_group, new_frame.reindex(columns = new_columns_created, copy = False), data_columns = new_columns_created)\n"", ""# This may be a bit tricky; and depends what you are actually doing.\n# I may need to modify this function to be a bit more general:\nreport_data = store.select_as_multiple([groups_1,groups_2,.....], where =['field_1>0', 'field_1000=foo'], selector = group_1)\n"", ""store.select(group, where = ['field_1000=foo', 'field_1001>0'])\n""]"
679;;2;14287518;14262433.0;2;41;;;;"['dict', 'chunksize', 'read_csv', 'aCollection.insert((a[1].to_dict() for a in df.iterrows()))\n', ""pd.DataFrame(list(mongoCollection.find({'anAttribute':{'$gt':2887000, '$lt':2889000}})))\n"", '.find()', 'ichunked', ""aJoinDF = pandas.DataFrame(list(mongoCollection.find({'anAttribute':{'$in':Att_Keys}})))\n"", 'aJoinDF', ""df = pandas.merge(df, aJoinDF, on=aKey, how='left')\n"", 'collection.update({primarykey:foo},{key:change})\n', 'dict']"
680;;6;14306902;14300137.0;2;76;;;;"['DataFrame', ""df = pd.DataFrame(np.random.randn(10,2), columns=['col1','col2'])\ndf['col3'] = np.arange(len(df))**2 * 100 + 100\n\nIn [5]: df\nOut[5]: \n       col1      col2  col3\n0 -1.000075 -0.759910   100\n1  0.510382  0.972615   200\n2  1.872067 -0.731010   500\n3  0.131612  1.075142  1000\n4  1.497820  0.237024  1700\n"", ""plt.scatter(df.col1, df.col2, s=df.col3)\n# OR (with pandas 0.13 and up)\ndf.plot(kind='scatter', x='col1', y='col2', s=df.col3)\n"", ""colors = np.where(df.col3 > 300, 'r', 'k')\nplt.scatter(df.col1, df.col2, s=120, c=colors)\n# OR (with pandas 0.13 and up)\ndf.plot(kind='scatter', x='col1', y='col2', s=120, c=colors)\n"", 'plt.scatter', ""cond = df.col3 > 300\nsubset_a = df[cond].dropna()\nsubset_b = df[~cond].dropna()\nplt.scatter(subset_a.col1, subset_a.col2, s=120, c='b', label='col3 > 300')\nplt.scatter(subset_b.col1, subset_b.col2, s=60, c='r', label='col3 <= 300') \nplt.legend()\n"", 'isnull', 'df[df.col3.isnull()]', 'select', ""df['subset'] = np.select([df.col3 < 150, df.col3 < 400, df.col3 < 600],\n                         [0, 1, 2], -1)\nfor color, label in zip('bgrm', [0, 1, 2, -1]):\n    subset = df[df.subset == label]\n    plt.scatter(subset.col1, subset.col2, s=120, c=color, label=str(label))\nplt.legend()\n""]"
681;;2;14349645;14349055.0;2;10;;;;[]
682;;9;14349766;14349055.0;2;41;;;;"['seaborn', 'import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn\nseaborn.set(style=\'ticks\')\n# Data to be represented\nX = np.random.randn(256)\n\n# Actual plotting\nfig = plt.figure(figsize=(8,6), dpi=72, facecolor=""white"")\naxes = plt.subplot(111)\nheights, positions, patches = axes.hist(X, color=\'white\')\nseaborn.despine(ax=axes, offset=10, trim=True)\nfig.tight_layout()\nplt.show()\n', 'matplotlibrc', 'xtick.direction : out', 'ytick.direction : out', 'import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n# Data to be represented\nX = np.random.randn(256)\n\n# Actual plotting\nfig = plt.figure(figsize=(8,6), dpi=72, facecolor=""white"")\naxes = plt.subplot(111)\nheights, positions, patches = axes.hist(X, color=\'white\')\n\naxes.spines[\'right\'].set_color(\'none\')\naxes.spines[\'top\'].set_color(\'none\')\naxes.xaxis.set_ticks_position(\'bottom\')\n\n# was: axes.spines[\'bottom\'].set_position((\'data\',1.1*X.min()))\naxes.spines[\'bottom\'].set_position((\'axes\', -0.05))\naxes.yaxis.set_ticks_position(\'left\')\naxes.spines[\'left\'].set_position((\'axes\', -0.05))\n\naxes.set_xlim([np.floor(positions.min()), np.ceil(positions.max())])\naxes.set_ylim([0,70])\naxes.xaxis.grid(False)\naxes.yaxis.grid(False)\nfig.tight_layout()\nplt.show()\n', ""axes.spines['bottom'].set_position?""]"
683;;3;14351567;14349055.0;2;27;;;;"[""def customaxis(ax, c_left='k', c_bottom='k', c_right='none', c_top='none',\n               lw=3, size=20, pad=8):\n\n    for c_spine, spine in zip([c_left, c_bottom, c_right, c_top],\n                              ['left', 'bottom', 'right', 'top']):\n        if c_spine != 'none':\n            ax.spines[spine].set_color(c_spine)\n            ax.spines[spine].set_linewidth(lw)\n        else:\n            ax.spines[spine].set_color('none')\n    if (c_bottom == 'none') & (c_top == 'none'): # no bottom and no top\n        ax.xaxis.set_ticks_position('none')\n    elif (c_bottom != 'none') & (c_top != 'none'): # bottom and top\n        ax.tick_params(axis='x', direction='out', width=lw, length=7,\n                      color=c_bottom, labelsize=size, pad=pad)\n    elif (c_bottom != 'none') & (c_top == 'none'): # bottom but not top\n        ax.xaxis.set_ticks_position('bottom')\n        ax.tick_params(axis='x', direction='out', width=lw, length=7,\n                       color=c_bottom, labelsize=size, pad=pad)\n    elif (c_bottom == 'none') & (c_top != 'none'): # no bottom but top\n        ax.xaxis.set_ticks_position('top')\n        ax.tick_params(axis='x', direction='out', width=lw, length=7,\n                       color=c_top, labelsize=size, pad=pad)\n    if (c_left == 'none') & (c_right == 'none'): # no left and no right\n        ax.yaxis.set_ticks_position('none')\n    elif (c_left != 'none') & (c_right != 'none'): # left and right\n        ax.tick_params(axis='y', direction='out', width=lw, length=7,\n                       color=c_left, labelsize=size, pad=pad)\n    elif (c_left != 'none') & (c_right == 'none'): # left but not right\n        ax.yaxis.set_ticks_position('left')\n        ax.tick_params(axis='y', direction='out', width=lw, length=7,\n                       color=c_left, labelsize=size, pad=pad)\n    elif (c_left == 'none') & (c_right != 'none'): # no left but right\n        ax.yaxis.set_ticks_position('right')\n        ax.tick_params(axis='y', direction='out', width=lw, length=7,\n                       color=c_right, labelsize=size, pad=pad)\n"", ""def adjust_spines(ax,spines):\n    for loc, spine in ax.spines.items():\n        if loc in spines:\n            spine.set_position(('outward',10)) # outward by 10 points\n            spine.set_smart_bounds(True)\n        else:\n            spine.set_color('none') # don't draw spine\n"", ""import numpy as np\nimport matplotlib.pyplot as plt\n\nfig,(ax1,ax2) = plt.subplots(figsize=(8,5), ncols=2)\nax1.plot(np.random.rand(20), np.random.rand(20), 'ok')\nax2.plot(np.random.rand(20), np.random.rand(20), 'ok')\n\ncustomaxis(ax2) # remove top and right spines, ticks out\nadjust_spines(ax2, ['left', 'bottom']) # non touching spines\n\nplt.show()\n""]"
684;;5;14365647;14365542.0;2;70;;;;"[""import pandas as pd\nprint pd.read_csv('value.txt')\n\n        Date    price  factor_1  factor_2\n0  2012-06-11  1600.20     1.255     1.548\n1  2012-06-12  1610.02     1.258     1.554\n2  2012-06-13  1618.07     1.249     1.552\n3  2012-06-14  1624.40     1.253     1.556\n4  2012-06-15  1626.15     1.258     1.552\n5  2012-06-16  1626.15     1.263     1.558\n6  2012-06-17  1626.15     1.264     1.572\n"", ""R's""]"
685;;0;14366084;14365542.0;2;6;;;;"[""import csv\nfrom pprint import pprint\nwith open('foo.csv', 'rb') as f:\n    reader = csv.reader(f)\n    headers = reader.next()\n    column = {h:[] for h in headers}\n    for row in reader:\n        for h, v in zip(headers, row):\n            column[h].append(v)\n    pprint(column)    # Pretty printer\n"", ""{'Date': ['2012-06-11',\n          '2012-06-12',\n          '2012-06-13',\n          '2012-06-14',\n          '2012-06-15',\n          '2012-06-16',\n          '2012-06-17'],\n 'factor_1': ['1.255', '1.258', '1.249', '1.253', '1.258', '1.263', '1.264'],\n 'factor_2': ['1.548', '1.554', '1.552', '1.556', '1.552', '1.558', '1.572'],\n 'price': ['1600.20',\n           '1610.02',\n           '1618.07',\n           '1624.40',\n           '1626.15',\n           '1626.15',\n           '1626.15']}\n""]"
686;;5;14383654;14380371.0;2;50;;;;['to_latex', 'In [42]: df = pd.DataFrame(np.random.random((5, 5)))\n\nIn [43]: df\nOut[43]: \n          0         1         2         3         4\n0  0.886864  0.518538  0.359964  0.167291  0.940414\n1  0.834130  0.022920  0.265131  0.059002  0.530584\n2  0.648019  0.953043  0.263551  0.595798  0.153969\n3  0.207003  0.015721  0.931170  0.045044  0.432870\n4  0.039886  0.898780  0.728195  0.112069  0.468485\n\nIn [44]: print df.to_latex()\n\\begin{tabular}{|l|c|c|c|c|c|c|}\n\\hline\n{} &         0 &         1 &         2 &         3 &         4 \\\\\n\\hline\n0 &  0.886864 &  0.518538 &  0.359964 &  0.167291 &  0.940414 \\\\\n1 &  0.834130 &  0.022920 &  0.265131 &  0.059002 &  0.530584 \\\\\n2 &  0.648019 &  0.953043 &  0.263551 &  0.595798 &  0.153969 \\\\\n3 &  0.207003 &  0.015721 &  0.931170 &  0.045044 &  0.432870 \\\\\n4 &  0.039886 &  0.898780 &  0.728195 &  0.112069 &  0.468485 \\\\\n\\hline\n\\end{tabular}\n', 'to_latex']
687;;4;14487936;12047193.0;2;60;;;;"['import pandas as pd\n\ndf = pd.read_sql(sql, cnxn)\n', 'import pyodbc\nimport pandas.io.sql as psql\n\ncnxn = pyodbc.connect(connection_info) \ncursor = cnxn.cursor()\nsql = ""SELECT * FROM TABLE""\n\ndf = psql.frame_query(sql, cnxn)\ncnxn.close()\n']"
688;;3;14508355;14507794.0;2;151;;;;"['df.columns = df.columns.get_level_values(0)\n', 'join', ""df.columns = [' '.join(col).strip() for col in df.columns.values]\n"", 'strip', ""In [11]: [' '.join(col).strip() for col in df.columns.values]\nOut[11]: \n['USAF',\n 'WBAN',\n 'day',\n 'month',\n 's_CD sum',\n 's_CL sum',\n 's_CNT sum',\n 's_PC sum',\n 'tempf amax',\n 'tempf amin',\n 'year']\n""]"
689;;1;14508639;14507794.0;2;13;;;;"[""In [34]: df\nOut[34]: \n     USAF   WBAN  day  month  s_CD  s_CL  s_CNT  s_PC  tempf         year\n                               sum   sum    sum   sum   amax   amin      \n0  702730  26451    1      1    12     0     13     1  30.92  24.98  1993\n1  702730  26451    2      1    13     0     13     0  32.00  24.98  1993\n2  702730  26451    3      1     2    10     13     1  23.00   6.98  1993\n3  702730  26451    4      1    12     0     13     1  10.04   3.92  1993\n4  702730  26451    5      1    10     0     13     3  19.94  10.94  1993\n\n\nIn [35]: mi = df.columns\n\nIn [36]: mi\nOut[36]: \nMultiIndex\n[(USAF, ), (WBAN, ), (day, ), (month, ), (s_CD, sum), (s_CL, sum), (s_CNT, sum), (s_PC, sum), (tempf, amax), (tempf, amin), (year, )]\n\n\nIn [37]: mi.tolist()\nOut[37]: \n[('USAF', ''),\n ('WBAN', ''),\n ('day', ''),\n ('month', ''),\n ('s_CD', 'sum'),\n ('s_CL', 'sum'),\n ('s_CNT', 'sum'),\n ('s_PC', 'sum'),\n ('tempf', 'amax'),\n ('tempf', 'amin'),\n ('year', '')]\n\nIn [38]: ind = pd.Index([e[0] + e[1] for e in mi.tolist()])\n\nIn [39]: ind\nOut[39]: Index([USAF, WBAN, day, month, s_CDsum, s_CLsum, s_CNTsum, s_PCsum, tempfamax, tempfamin, year], dtype=object)\n\nIn [40]: df.columns = ind\n\n\n\n\nIn [46]: df\nOut[46]:\xa0\n\xa0 \xa0 \xa0USAF \xa0 WBAN \xa0day \xa0month \xa0s_CDsum \xa0s_CLsum \xa0s_CNTsum \xa0s_PCsum \xa0tempfamax \xa0tempfamin \xa0\\\n0 \xa0702730 \xa026451 \xa0 \xa01 \xa0 \xa0 \xa01 \xa0 \xa0 \xa0 12 \xa0 \xa0 \xa0 \xa00 \xa0 \xa0 \xa0 \xa013 \xa0 \xa0 \xa0 \xa01 \xa0 \xa0 \xa030.92 \xa0 \xa0 \xa024.98 \xa0\xa0\n1 \xa0702730 \xa026451 \xa0 \xa02 \xa0 \xa0 \xa01 \xa0 \xa0 \xa0 13 \xa0 \xa0 \xa0 \xa00 \xa0 \xa0 \xa0 \xa013 \xa0 \xa0 \xa0 \xa00 \xa0 \xa0 \xa032.00 \xa0 \xa0 \xa024.98 \xa0\xa0\n2 \xa0702730 \xa026451 \xa0 \xa03 \xa0 \xa0 \xa01 \xa0 \xa0 \xa0 \xa02 \xa0 \xa0 \xa0 10 \xa0 \xa0 \xa0 \xa013 \xa0 \xa0 \xa0 \xa01 \xa0 \xa0 \xa023.00 \xa0 \xa0 \xa0 6.98 \xa0\xa0\n3 \xa0702730 \xa026451 \xa0 \xa04 \xa0 \xa0 \xa01 \xa0 \xa0 \xa0 12 \xa0 \xa0 \xa0 \xa00 \xa0 \xa0 \xa0 \xa013 \xa0 \xa0 \xa0 \xa01 \xa0 \xa0 \xa010.04 \xa0 \xa0 \xa0 3.92 \xa0\xa0\n4 \xa0702730 \xa026451 \xa0 \xa05 \xa0 \xa0 \xa01 \xa0 \xa0 \xa0 10 \xa0 \xa0 \xa0 \xa00 \xa0 \xa0 \xa0 \xa013 \xa0 \xa0 \xa0 \xa03 \xa0 \xa0 \xa019.94 \xa0 \xa0 \xa010.94 \xa0\xa0\n\n\n\n\n\xa0 \xa0year \xa0\n0 \xa01993 \xa0\n1 \xa01993 \xa0\n2 \xa01993 \xa0\n3 \xa01993 \xa0\n4 \xa01993\n""]"
690;;6;14530027;14529838.0;2;78;;;;"[""In [28]: df\nOut[28]:\n          A         B         C         D         E  GRP\n0  0.395670  0.219560  0.600644  0.613445  0.242893    0\n1  0.323911  0.464584  0.107215  0.204072  0.927325    0\n2  0.321358  0.076037  0.166946  0.439661  0.914612    1\n3  0.133466  0.447946  0.014815  0.130781  0.268290    1\n\nIn [26]: f = {'A':['sum','mean'], 'B':['prod']}\n\nIn [27]: df.groupby('GRP').agg(f)\nOut[27]:\n            A                   B\n          sum      mean      prod\nGRP\n0    0.719580  0.359790  0.102004\n1    0.454824  0.227412  0.034060\n"", ""In [67]: f = {'A':['sum','mean'], 'B':['prod'], 'D': lambda g: df.ix[g.index].E.sum()}\n\nIn [69]: df.groupby('GRP').agg(f)\nOut[69]:\n            A                   B         D\n          sum      mean      prod  <lambda>\nGRP\n0    0.719580  0.359790  0.102004  1.170219\n1    0.454824  0.227412  0.034060  1.182901\n"", 'g.index', 'df.ix[]', 'g[]', ""In [95]: cust = lambda g: g[df.ix[g.index]['C'] < 0.5].sum()\n\nIn [96]: f = {'A':['sum','mean'], 'B':['prod'], 'D': {'my name': cust}}\n\nIn [97]: df.groupby('GRP').agg(f)\nOut[97]:\n            A                   B         D\n          sum      mean      prod   my name\nGRP\n0    0.719580  0.359790  0.102004  0.204072\n1    0.454824  0.227412  0.034060  0.570441\n""]"
691;;4;14630250;14627380.0;2;18;;;;"['to_html', 'formatters', '<span class=""significant"">', '</span>', '*', '<', '&lt;', 'escape=False']"
692;;5;14661768;14661701.0;2;201;;;;['In [65]: df\nOut[65]: \n       one  two\none      1    4\ntwo      2    3\nthree    3    2\nfour     4    1\n\n\nIn [66]: df.drop(df.index[[1,3]])\nOut[66]: \n       one  two\none      1    4\nthree    3    2\n']
693;;1;14669654;14663004.0;2;22;;;;"['ix', 'df.iloc[-3:]\n', 'iloc', 'IndexError', '.head()', '.tail()', '>>> pd.__version__\n\'0.12.0\'\n>>> df = pd.DataFrame([{""a"": 1}, {""a"": 2}])\n>>> df.iloc[-5:]\n...\nIndexError: out-of-bounds on slice (end)\n>>> df.tail(5)\n   a\n0  1\n1  2\n', 'irows', 'In [11]: df1.irow(slice(-3, None))\nOut[11]: \n    STK_ID  RPT_Date  TClose   sales  discount\n8      568  20080331   38.75  12.668       NaN\n9      568  20080630   30.09  21.102       NaN\n10     568  20080930   26.00  30.769       NaN\n', 'iget']"
694;;5;14688398;14688306.0;2;40;;;;"['pandas.DataFrame', ""import pandas as pd\ndf = pd.DataFrame([])\ndf.instrument_name = 'Binky'\n"", 'groupby', 'pivot', 'join', 'loc']"
695;;0;14688529;14688306.0;2;9;;;;[]
696;;6;14734148;14733871.0;2;53;;;;"[""In [11]: df.sort([('Group1', 'C')], ascending=False)\nOut[11]: \n  Group1       Group2      \n       A  B  C      A  B  C\n2      5  6  9      1  0  0\n1      1  0  3      2  5  7\n3      7  0  2      0  3  5\n""]"
697;;2;14734627;14734533.0;2;94;;;;"['get_group', ""In [21]: gb.get_group('foo')\nOut[21]: \n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n"", 'dict(iter(gb))', 'In [22]: gb[[""A"", ""B""]].get_group(""foo"")\nOut[22]:\n     A         B\n0  foo  1.624345\n2  foo -0.528172\n4  foo  0.865408\n\nIn [23]: gb[""C""].get_group(""foo"")\nOut[23]:\n0     5\n2    11\n4    14\nName: C, dtype: int64\n']"
698;;3;14745484;14745022.0;2;34;;;;"[""In [34]: import pandas as pd\n\nIn [35]: df\nOut[35]: \n                        row\n0       00000 UNITED STATES\n1             01000 ALABAMA\n2  01001 Autauga County, AL\n3  01003 Baldwin County, AL\n4  01005 Barbour County, AL\n\nIn [36]: df = pd.DataFrame(df.row.str.split(' ',1).tolist(),\n                                   columns = ['flips','row'])\n\nIn [37]: df\nOut[37]: \n   flips                 row\n0  00000       UNITED STATES\n1  01000             ALABAMA\n2  01001  Autauga County, AL\n3  01003  Baldwin County, AL\n4  01005  Barbour County, AL\n""]"
699;;1;14746845;14744068.0;2;51;;;;"[""df['Firstlevel'] = 'Foo'\ndf.set_index('Firstlevel', append=True, inplace=True)\n"", ""df.reorder_levels(['Firstlevel', 'A', 'B'])\n"", '                      Vals\nFirstlevel A  B           \nFoo        a1 b1  0.871563\n              b2  0.494001\n           a2 b3 -0.167811\n           a3 b4 -1.353409\n']"
700;;0;14760930;14663004.0;2;155;;;;['DataFrame.tail', 'df1.tail(10)']
701;;1;14809026;14808945.0;2;59;;;;"['import pandas as pd\n\ndef f(var):\n    if isinstance(var, pd.DataFrame):\n        print ""do stuff""\n']"
702;;0;14809149;14808945.0;2;27;;;;"['isinstance', 'Yes: if isinstance(obj, int):\nNo:  if type(obj) is type(1):\n', 'if obj.__class__.__name__ = ""MyInheritedClass"":\n    expect_problems_some_day()\n', 'isinstance', 'str', 'unicode', 'basestring', 'if isinstance(obj, basestring):\n    i_am_string(obj)\n']"
703;;4;14900065;13035764.0;2;82;;;;"['drop_duplicates', ""df4 = df3.drop_duplicates(subset='rownum', keep='last')\n"", ""df3 = df3.reset_index().drop_duplicates(subset='index', keep='last').set_index('index')\n"", 'sort', 'df3 = df3.sort()\n']"
704;;5;14942625;14941366.0;2;8;;;;"[""In [10]: sum_B_over_A = df.groupby('A').sum().B\n\nIn [11]: sum_B_over_A\nOut[11]: \nA\nbar    0.253652\nbaz   -2.829711\nfoo    0.551376\nName: B\n\nin [12]: df['sum_B_over_A'] = df.A.apply(sum_B_over_A.get_value)\n\nIn [13]: df\nOut[13]: \n     A         B      C  sum_B_over_A\n0  foo  1.624345  False      0.551376\n1  bar -0.611756   True      0.253652\n2  baz -0.528172  False     -2.829711\n3  foo -1.072969   True      0.551376\n4  bar  0.865408  False      0.253652\n5  baz -2.301539   True     -2.829711\n\nIn [14]: df.sort(['sum_B_over_A', 'A', 'B'])\nOut[14]: \n     A         B      C   sum_B_over_A\n5  baz -2.301539   True      -2.829711\n2  baz -0.528172  False      -2.829711\n1  bar -0.611756   True       0.253652\n4  bar  0.865408  False       0.253652\n3  foo -1.072969   True       0.551376\n0  foo  1.624345  False       0.551376\n"", ""In [15]: df.sort(['sum_B_over_A', 'A', 'B']).drop('sum_B_over_A', axis=1)\nOut[15]: \n     A         B      C\n5  baz -2.301539   True\n2  baz -0.528172  False\n1  bar -0.611756   True\n4  bar  0.865408  False\n3  foo -1.072969   True\n0  foo  1.624345  False\n""]"
705;;4;14946246;14941366.0;2;43;;;;"[""In [0]: grp = df.groupby('A')\n"", ""In [1]: grp[['B']].transform(sum).sort('B')\nOut[1]:\n          B\n2 -2.829710\n5 -2.829710\n1  0.253651\n4  0.253651\n0  0.551377\n3  0.551377\n"", ""In [2]: sort1 = df.ix[grp[['B']].transform(sum).sort('B').index]\n\nIn [3]: sort1\nOut[3]:\n     A         B      C\n2  baz -0.528172  False\n5  baz -2.301539   True\n1  bar -0.611756   True\n4  bar  0.865408  False\n0  foo  1.624345  False\n3  foo -1.072969   True\n"", 'sort=False', ""In [4]: f = lambda x: x.sort('C', ascending=False)\n\nIn [5]: sort2 = sort1.groupby('A', sort=False).apply(f)\n\nIn [6]: sort2\nOut[6]:\n         A         B      C\nA\nbaz 5  baz -2.301539   True\n    2  baz -0.528172  False\nbar 1  bar -0.611756   True\n    4  bar  0.865408  False\nfoo 3  foo -1.072969   True\n    0  foo  1.624345  False\n"", 'reset_index', 'drop=True', 'In [7]: sort2.reset_index(0, drop=True)\nOut[7]:\n     A         B      C\n5  baz -2.301539   True\n2  baz -0.528172  False\n1  bar -0.611756   True\n4  bar  0.865408  False\n3  foo -1.072969   True\n0  foo  1.624345  False\n']"
706;;1;14964637;14964493.0;2;26;;;;"[""In [63]: df.xs(('B',), level='Alpha')\nOut[63]:\n                  I        II       III        IV         V        VI       VII\nInt Bool                                                                       \n0   True  -0.430563  0.139969 -0.356883 -0.574463 -0.107693 -1.030063  0.271250\n    False  0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951\n1   True  -0.450375  1.237018  0.398290  0.246182 -0.237919  1.372239 -0.805403\n    False -0.064493  0.967132 -0.674451  0.666691 -0.350378  1.721682 -0.791897\n2   True   0.143154 -0.061543 -1.157361  0.864847 -0.379616 -0.762626  0.645582\n    False -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494\n\nIn [64]: df.xs(('B', False), level=('Alpha', 'Bool'))\nOut[64]:\n            I        II       III        IV         V        VI       VII\nInt                                                                      \n0    0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951\n1   -0.064493  0.967132 -0.674451  0.666691 -0.350378  1.721682 -0.791897\n2   -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494 \n"", 'get_level_values', 'isin', 'In [87]: ix_vals = set(i for _, i, _ in df.index if i % 2 == 0)\n         ix_vals\n\nOut[87]: set([0L, 2L])\n', 'isin', ""In [89]: ix = df.index.get_level_values('Int').isin(ix_vals)\nIn [90]: df[ix]\nOut[90]:                I        II       III        IV         V        VI       VII\nAlpha Int Bool                                                                       \nA     0   True  -1.315409  1.203800  0.330372 -0.295718 -0.679039  1.402114  0.778572\n          False  0.008189 -0.104372  0.419110  0.302978 -0.880262 -1.037645 -0.264265\n      2   True  -2.414290  0.896990  0.986167 -0.527074  0.550753 -0.302920  0.228165\n          False  1.275831  0.448089 -0.635874 -0.733855 -0.747774 -1.108976  0.151474\nB     0   True  -0.430563  0.139969 -0.356883 -0.574463 -0.107693 -1.030063  0.271250\n          False  0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951\n      2   True   0.143154 -0.061543 -1.157361  0.864847 -0.379616 -0.762626  0.645582\n          False -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494 \n""]"
707;;2;14985695;14984119.0;2;20;;;;"[""df = df['Time', 'Time Relative', 'N2']"", ""In [101]: vals = np.random.randint(0,20, (4,3))\n          vals\nOut[101]:\narray([[ 3, 13,  0],\n       [ 1, 15, 14],\n       [14, 19, 14],\n       [19,  5,  1]])\n\nIn [106]: df = pd.DataFrame(np.hstack([vals, vals]), columns=['Time', 'H1', 'N2', 'Time Relative', 'N2', 'Time'] )\n          df\nOut[106]:\n   Time  H1  N2  Time Relative  N2  Time\n0     3  13   0              3  13     0\n1     1  15  14              1  15    14\n2    14  19  14             14  19    14\n3    19   5   1             19   5     1\n\nIn [107]: df.T.drop_duplicates().T\nOut[107]:\n   Time  H1  N2\n0     3  13   0\n1     1  15  14\n2    14  19  14\n3    19   5   1\n"", 'Time    H1  N2  Time    N2  Time Relative\n3   13  13  3   13  0\n1   15  15  1   15  14\n14  19  19  14  19  14\n19  5   5   19  5   1\n', 'read_table', ""In [151]: df2 = pd.read_table('dummy.csv')\n          df2\nOut[151]:\n         Time  H1  N2  Time.1  N2.1  Time Relative\n      0     3  13  13       3    13              0\n      1     1  15  15       1    15             14\n      2    14  19  19      14    19             14\n      3    19   5   5      19     5              1\nIn [152]: df2.T.drop_duplicates().T\nOut[152]:\n             Time  H1  Time Relative\n          0     3  13              0\n          1     1  15             14\n          2    14  19             14\n          3    19   5              1  \n"", ""In [169]: df2 = pd.read_table('dummy.csv', header=None)\n          df2\nOut[169]:\n              0   1   2     3   4              5\n        0  Time  H1  N2  Time  N2  Time Relative\n        1     3  13  13     3  13              0\n        2     1  15  15     1  15             14\n        3    14  19  19    14  19             14\n        4    19   5   5    19   5              1\nIn [171]: from collections import defaultdict\n          col_counts = defaultdict(int)\n          col_ix = df2.first_valid_index()\nIn [172]: cols = []\n          for col in df2.ix[col_ix]:\n              cnt = col_counts[col]\n              col_counts[col] += 1\n              suf = '_' + str(cnt) if cnt else ''\n              cols.append(col + suf)\n          cols\nOut[172]:\n          ['Time', 'H1', 'N2', 'Time_1', 'N2_1', 'Time Relative']\nIn [174]: df2.columns = cols\n          df2 = df2.drop([col_ix])\nIn [177]: df2\nOut[177]:\n          Time  H1  N2 Time_1 N2_1 Time Relative\n        1    3  13  13      3   13             0\n        2    1  15  15      1   15            14\n        3   14  19  19     14   19            14\n        4   19   5   5     19    5             1\nIn [178]: df2.T.drop_duplicates().T\nOut[178]:\n          Time  H1 Time Relative\n        1    3  13             0\n        2    1  15            14\n        3   14  19            14\n        4   19   5             1 \n""]"
708;;0;14988913;14988480.0;2;18;;;;['Frame = Frame.append(pandas.DataFrame(data = SomeNewLineOfData))\n', 'Frame = Frame.append(pandas.DataFrame(data = SomeNewLineOfData), ignore_index=True)\n']
709;;2;14989047;14988480.0;2;10;;;;"[""import numpy as np\nimport pandas as pd\n\ndates = np.asarray(pd.date_range('1/1/2000', periods=8))\ndf1 = pd.DataFrame(np.random.randn(8, 4), index=dates, columns=['A', 'B', 'C', 'D'])\ndf2 = df1.copy()\ndf = df1.append(df2)\n"", '                   A         B         C         D\n2000-01-01 -0.327208  0.552500  0.862529  0.493109\n2000-01-02  1.039844 -2.141089 -0.781609  1.307600\n2000-01-03 -0.462831  0.066505 -1.698346  1.123174\n2000-01-04 -0.321971 -0.544599 -0.486099 -0.283791\n2000-01-05  0.693749  0.544329 -1.606851  0.527733\n2000-01-06 -2.461177 -0.339378 -0.236275  0.155569\n2000-01-07 -0.597156  0.904511  0.369865  0.862504\n2000-01-08 -0.958300 -0.583621 -2.068273  0.539434\n2000-01-01 -0.327208  0.552500  0.862529  0.493109\n2000-01-02  1.039844 -2.141089 -0.781609  1.307600\n2000-01-03 -0.462831  0.066505 -1.698346  1.123174\n2000-01-04 -0.321971 -0.544599 -0.486099 -0.283791\n2000-01-05  0.693749  0.544329 -1.606851  0.527733\n2000-01-06 -2.461177 -0.339378 -0.236275  0.155569\n2000-01-07 -0.597156  0.904511  0.369865  0.862504\n2000-01-08 -0.958300 -0.583621 -2.068273  0.539434\n', 'pandas']"
710;;2;14992237;14991195.0;2;53;;;;"['dropna', 'subset', 'how', ""df2.dropna(subset=['three', 'four', 'five'], how='all')\n"", ""how='all'"", 'subset', 'NaN', ""'any'"", 'subset', 'NaN', 'k', 'subset=df2.columns[-k:]\n', 'subset=filter(lambda x: len(x) > 3, df2.columns)\n']"
711;;1;15006495;15006298.0;2;27;;;;"['DataFrame', '>>> df = pd.DataFrame({""A"": range(1000), ""B"": range(1000)})\n>>> df\n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 1000 entries, 0 to 999\nData columns:\nA    1000  non-null values\nB    1000  non-null values\ndtypes: int64(2)\n>>> df[:5]\n   A  B\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n4  4  4\n', '.ix', '>>> df = pd.DataFrame({i: range(1000) for i in range(100)})\n>>> df.ix[:5, :10]\n   0   1   2   3   4   5   6   7   8   9   10\n0   0   0   0   0   0   0   0   0   0   0   0\n1   1   1   1   1   1   1   1   1   1   1   1\n2   2   2   2   2   2   2   2   2   2   2   2\n3   3   3   3   3   3   3   3   3   3   3   3\n4   4   4   4   4   4   4   4   4   4   4   4\n5   5   5   5   5   5   5   5   5   5   5   5\n']"
712;;1;15009160;15008970.0;2;48;;;;"['nrows', 'nrows : int, default None\n\n    Number of rows of file to read. Useful for reading pieces of large files\n', 'In [1]: import pandas as pd\n\nIn [2]: time z = pd.read_csv(""P00000001-ALL.csv"", nrows=20)\nCPU times: user 0.00 s, sys: 0.00 s, total: 0.00 s\nWall time: 0.00 s\n\nIn [3]: len(z)\nOut[3]: 20\n\nIn [4]: time z = pd.read_csv(""P00000001-ALL.csv"")\nCPU times: user 27.63 s, sys: 1.92 s, total: 29.55 s\nWall time: 30.23 s\n']"
713;;1;15026839;15026698.0;2;52;;;;"['delim_whitespace', '>>> import pandas as pd\n>>> for line in open(""whitespace.csv""):\n...     print repr(line)\n...     \n\'a\\t  b\\tc 1 2\\n\'\n\'d\\t  e\\tf 3 4\\n\'\n>>> pd.read_csv(""whitespace.csv"", header=None, delimiter=r""\\s+"")\n   0  1  2  3  4\n0  a  b  c  1  2\n1  d  e  f  3  4\n>>> pd.read_csv(""whitespace.csv"", header=None, delim_whitespace=True)\n   0  1  2  3  4\n0  a  b  c  1  2\n1  d  e  f  3  4\n']"
714;;1;15030455;15017072.0;2;7;;;;"['index_col', 'dummy', 'parse_dates', 'usecols', 'names', 'import pandas as pd\nfrom StringIO import StringIO\n\ncsv = """"""dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nbar,20090103,b,5\n""""""\n\ndf = pd.read_csv(StringIO(csv),\n        index_col=[0,1],\n        usecols=[1,2,3], \n        parse_dates=[0],\n        header=0,\n        names=[""date"", ""loc"", """", ""x""])\n\nprint df\n', '                x\ndate       loc   \n2009-01-01 a    1\n2009-01-02 a    3\n2009-01-03 a    5\n2009-01-01 b    1\n2009-01-02 b    3\n2009-01-03 b    5\n']"
715;;2;15097125;10065051.0;2;22;;;;"['import pandas as pd\nimport sqlite3\n\nwith sqlite3.connect(""whatever.sqlite"") as con:\n    sql = ""SELECT * FROM table_name""\n    df = pd.read_sql_query(sql, con)\n    print df.shape\n']"
716;;0;15100193;15017072.0;2;8;;;;"['import pandas as pd\nfrom StringIO import StringIO\n\ncsv = r""""""dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nbar,20090103,b,5""""""\n\ndf = pd.read_csv(StringIO(csv),\n        index_col=[""date"", ""loc""], \n        usecols=[""dummy"", ""date"", ""loc"", ""x""],\n        parse_dates=[""date""],\n        header=0,\n        names=[""dummy"", ""date"", ""loc"", ""x""])\ndel df[\'dummy\']\n', '                x\ndate       loc\n2009-01-01 a    1\n2009-01-02 a    3\n2009-01-03 a    5\n2009-01-01 b    1\n2009-01-02 b    3\n2009-01-03 b    5\n']"
717;;2;15125793;15118111.0;2;18;;;;"[""def calculate(s):\n    a = s['path'] + 2*s['row'] # Simple calc for example\n    b = s['path'] * 0.153\n    return pd.Series(dict(col1=a, col2=b))\n"", ""st.ix[i]['a'] = a\n"", ""st.ix[i, 'a'] = a\n""]"
718;;2;15203886;15203623.0;2;23;;;;"['[t.value // 10 ** 9 for t in tsframe.index]\n', ""In [1]: t = pd.Timestamp('2000-02-11 00:00:00')\n\nIn [2]: t\nOut[2]: <Timestamp: 2000-02-11 00:00:00>\n\nIn [3]: t.value\nOut[3]: 950227200000000000L\n\nIn [4]: time.mktime(t.timetuple())\nOut[4]: 950227200.0\n"", 'tsframe.index.astype(np.int64) // 10 ** 9\n']"
719;;2;15204235;15203623.0;2;50;;;;['DatetimeIndex', 'ndarray', 'In [1]: import numpy as np\n\nIn [2]: import pandas as pd\n\nIn [3]: from datetime import datetime\n\nIn [4]: dates = [datetime(2012, 5, 1), datetime(2012, 5, 2), datetime(2012, 5, 3)]\n   ...: index = pd.DatetimeIndex(dates)\n   ...: \nIn [5]: index.astype(np.int64)\nOut[5]: array([1335830400000000000, 1335916800000000000, 1336003200000000000], \n        dtype=int64)\n\nIn [6]: index.astype(np.int64) // 10**9\nOut[6]: array([1335830400, 1335916800, 1336003200], dtype=int64)\n\n%timeit [t.value // 10 ** 9 for t in index]\n10000 loops, best of 3: 119 us per loop\n\n%timeit index.astype(np.int64) // 10**9\n100000 loops, best of 3: 18.4 us per loop\n']
720;;5;15213171;15210962.0;2;18;;;;"[""# dont' use dtype converters explicity for the columns you care about\n# they will be converted to float64 if possible, or object if they cannot\ndf = pd.read_csv('test.csv'.....)\n\n#### this is optional and related to the issue you posted ####\n# force anything that is not a numeric to nan\n# columns are the list of columns that you are interesetd in\ndf[columns] = df[columns].convert_objects(convert_numeric=True)\n\n\n    # astype\n    df[columns] = df[columns].astype('float32')\n\nsee http://pandas.pydata.org/pandas-docs/dev/basics.html#object-conversion\n\nIts not as efficient as doing it directly in read_csv (but that requires\n"", ""In [5]: x = pd.read_csv(StringIO.StringIO(data), dtype={'a': np.float32}, delim_whitespace=True)\n\nIn [6]: x\nOut[6]: \n         a        b\n0  0.76398  0.81394\n1  0.32136  0.91063\n\nIn [7]: x.dtypes\nOut[7]: \na    float32\nb    float64\ndtype: object\n\nIn [8]: pd.__version__\nOut[8]: '0.11.0.dev-385ff82'\n\nIn [9]: quit()\nvagrant@precise32:~/pandas$ uname -a\nLinux precise32 3.2.0-23-generic-pae #36-Ubuntu SMP Tue Apr 10 22:19:09 UTC 2012 i686 i686 i386 GNU/Linux\n\n some low-level changes)\n""]"
721;;5;15220374;15210962.0;2;6;;;;"[""In [22]: df.a.dtype = pd.np.float32\n\nIn [23]: df.a.dtype\nOut[23]: dtype('float32')\n""]"
722;;3;15222976;15222754.0;2;7;;;;"['agg', 'Series', ""'Short name'"", 'stats.mode', ""source.groupby(['Country','City']).agg(lambda x: stats.mode(x)[0][0])\n"", '                         Short name\nCountry City                       \nRussia  Sankt-Petersburg        Spb\nUSA     New-York                 NY\n']"
723;;3;15223034;15222754.0;2;52;;;;"['value_counts()', ""import pandas as pd\n\nsource = pd.DataFrame({'Country' : ['USA', 'USA', 'Russia','USA'], \n                  'City' : ['New-York', 'New-York', 'Sankt-Petersburg', 'New-York'],\n                  'Short name' : ['NY','New','Spb','NY']})\n\nsource.groupby(['Country','City']).agg(lambda x:x.value_counts().index[0])\n""]"
724;;0;15244074;15242746.0;2;7;;;;"[""import pandas as pd\n\ndf = pd.DataFrame()\n\nwith open(filepath, 'r') as f:\n    for line in f:\n        df = pd.concat( [df, pd.DataFrame([tuple(line.strip().split(','))])], ignore_index=True )\n""]"
725;;4;15252012;15242746.0;2;42;;;;"['>>> !cat ragged.csv\n1,2,3\n1,2,3,4\n1,2,3,4,5\n1,2\n1,2,3,4\n>>> my_cols = [""A"", ""B"", ""C"", ""D"", ""E""]\n>>> pd.read_csv(""ragged.csv"", names=my_cols, engine=\'python\')\n   A  B   C   D   E\n0  1  2   3 NaN NaN\n1  1  2   3   4 NaN\n2  1  2   3   4   5\n3  1  2 NaN NaN NaN\n4  1  2   3   4 NaN\n']"
726;;5;15315507;15315452.0;2;156;;;;"["">>> import pandas as pd\n>>> from random import randint\n>>> df = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],\n                   'B': [randint(1, 9)*10 for x in xrange(10)],\n                   'C': [randint(1, 9)*100 for x in xrange(10)]})\n>>> df\n   A   B    C\n0  9  40  300\n1  9  70  700\n2  5  70  900\n3  8  80  900\n4  7  50  200\n5  9  30  900\n6  2  80  700\n7  2  80  400\n8  5  80  300\n9  7  70  800\n"", '>>> df[""B""] > 50\n0    False\n1     True\n2     True\n3     True\n4    False\n5    False\n6     True\n7     True\n8     True\n9     True\nName: B\n>>> (df[""B""] > 50) & (df[""C""] == 900)\n0    False\n1    False\n2     True\n3     True\n4    False\n5    False\n6    False\n7    False\n8    False\n9    False\n', '.loc', '>>> df[""A""][(df[""B""] > 50) & (df[""C""] == 900)]\n2    5\n3    8\nName: A, dtype: int64\n', '.loc', '>>> df.loc[(df[""B""] > 50) & (df[""C""] == 900), ""A""]\n2    5\n3    8\nName: A, dtype: int64\n>>> df.loc[(df[""B""] > 50) & (df[""C""] == 900), ""A""].values\narray([5, 8], dtype=int64)\n>>> df.loc[(df[""B""] > 50) & (df[""C""] == 900), ""A""] *= 1000\n>>> df\n      A   B    C\n0     9  40  300\n1     9  70  700\n2  5000  70  900\n3  8000  80  900\n4     7  50  200\n5     9  30  900\n6     2  80  700\n7     2  80  400\n8     5  80  300\n9     7  70  800\n', '== 900', '!= 900', '~(df[""C""] == 900)']"
727;;2;15322715;15322632.0;2;63;;;;"['agg', 'aggregate', 'Series', 'DataFrame', 'idxmax', ""idx = df.groupby('word')['count'].idxmax()\nprint(idx)\n"", 'word\na       2\nan      3\nthe     1\nName: count\n', 'loc', 'word', 'tag', ""print(df.loc[idx, ['word', 'tag']])\n"", '  word tag\n2    a   T\n3   an   T\n1  the   S\n', 'idxmax', 'df.loc', 'df.loc', 'idx', 'df.index.is_unique', 'True', 'idxmax', 'df.loc', 'apply', 'apply', ""import pandas as pd\ndf = pd.DataFrame({'word':'a the a an the'.split(),\n                   'tag': list('SSTTT'),\n                   'count': [30, 20, 60, 5, 10]})\n\nprint(df.groupby('word').apply(lambda subf: subf['tag'][subf['count'].idxmax()]))\n"", 'word\na       T\nan      T\nthe     S\n', 'idxmax', 'loc', 'apply', ""N = 10000\ndf = pd.DataFrame({'word':'a the a an the'.split()*N,\n                   'tag': list('SSTTT')*N,\n                   'count': [30, 20, 60, 5, 10]*N})\ndef using_apply(df):\n    return (df.groupby('word').apply(lambda subf: subf['tag'][subf['count'].idxmax()]))\n\ndef using_idxmax_loc(df):\n    idx = df.groupby('word')['count'].idxmax()\n    return df.loc[idx, ['word', 'tag']]\n\nIn [22]: %timeit using_apply(df)\n100 loops, best of 3: 7.68 ms per loop\n\nIn [23]: %timeit using_idxmax_loc(df)\n100 loops, best of 3: 5.43 ms per loop\n"", 'set_index', 'to_dict', ""In [36]: df2 = df.loc[idx, ['word', 'tag']].set_index('word')\n\nIn [37]: df2\nOut[37]: \n     tag\nword    \na      T\nan     T\nthe    S\n\nIn [38]: df2.to_dict()['tag']\nOut[38]: {'a': 'T', 'an': 'T', 'the': 'S'}\n""]"
728;;1;15322920;15322632.0;2;17;;;;"[""In [33]: def f(x):\n....:     print type(x)\n....:     print x\n....:     \n\nIn [34]: df.groupby('word').apply(f)\n<class 'pandas.core.frame.DataFrame'>\n  word tag  count\n0    a   S     30\n2    a   T     60\n<class 'pandas.core.frame.DataFrame'>\n  word tag  count\n0    a   S     30\n2    a   T     60\n<class 'pandas.core.frame.DataFrame'>\n  word tag  count\n3   an   T      5\n<class 'pandas.core.frame.DataFrame'>\n  word tag  count\n1  the   S     20\n4  the   T     10\n"", ""In [41]: df.groupby('word').sum()\nOut[41]: \n      count\nword       \na        90\nan        5\nthe      30\n"", ""In [42]: df.groupby('word').apply(lambda x: x.sum())\nOut[42]: \n        word tag count\nword                  \na         aa  ST    90\nan        an   T     5\nthe   thethe  ST    30\n"", ""In [43]: df.groupby('word').apply(lambda x: x['count'].sum())\nOut[43]: \nword\na       90\nan       5\nthe     30\n""]"
729;;1;15333283;15325182.0;2;72;;;;"[""In [10]: df.b.str.contains('^f')\nOut[10]: \n0    False\n1     True\n2     True\n3    False\nName: b, dtype: bool\n""]"
730;;4;15361537;15360925.0;2;50;;;;['x[x.columns[0]]\n']
731;;5;15362700;15360925.0;2;66;;;;"["">>> import pandas as pd\n>>> df = pd.DataFrame({'x' : [1, 2, 3, 4], 'y' : [4, 5, 6, 7]})\n>>> df\n   x  y\n0  1  4\n1  2  5\n2  3  6\n3  4  7\n>>> s = df.ix[:,0]\n>>> type(s)\n<class 'pandas.core.series.Series'>\n>>>\n""]"
732;;0;15364468;15360925.0;2;24;;;;['in 0.11\n\nIn [7]: df.iloc[:,0]\nOut[7]: \n0    1\n1    2\n2    3\n3    4\nName: x, dtype: int64\n']
733;;5;15411596;15411158.0;2;147;;;;"[""table.groupby('YEARMONTH').CLIENTCODE.nunique()\n"", ""In [2]: table\nOut[2]: \n   CLIENTCODE  YEARMONTH\n0           1     201301\n1           1     201301\n2           2     201301\n3           1     201302\n4           2     201302\n5           2     201302\n6           3     201302\n\nIn [3]: table.groupby('YEARMONTH').CLIENTCODE.nunique()\nOut[3]: \nYEARMONTH\n201301       2\n201302       3\n""]"
734;;3;15466103;15465645.0;2;38;;;;"[""import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsin = np.sin\ncos = np.cos\npi = np.pi\nN = 100\n\nx = np.linspace(0, pi, N)\na = sin(x)\nb = cos(x)\n\ndf = pd.DataFrame({\n    'A': [True]*N + [False]*N,\n    'B': np.hstack((a,b))\n    })\n\nfor key, grp in df.groupby(['A']):\n    plt.plot(grp['B'], label=key)\n    grp['D'] = pd.rolling_mean(grp['B'], window=5)    \n    plt.plot(grp['D'], label='rolling ({k})'.format(k=key))\nplt.legend(loc='best')    \nplt.show()\n""]"
735;;4;15558350;14262433.0;2;34;;;;"['def transpose_table(h_in, table_path, h_out, group_name=""data"", group_path=""/""):\n    # Get a reference to the input data.\n    tb = h_in.getNode(table_path)\n    # Create the output group to hold the columns.\n    grp = h_out.createGroup(group_path, group_name, filters=tables.Filters(complevel=1))\n    for col_name in tb.colnames:\n        logger.debug(""Processing %s"", col_name)\n        # Get the data.\n        col_data = tb.col(col_name)\n        # Create the output array.\n        arr = h_out.createCArray(grp,\n                                 col_name,\n                                 tables.Atom.from_dtype(col_data.dtype),\n                                 col_data.shape)\n        # Store the data.\n        arr[:] = col_data\n    h_out.flush()\n', 'def read_hdf5(hdf5_path, group_path=""/data"", columns=None):\n    """"""Read a transposed data set from a HDF5 file.""""""\n    if isinstance(hdf5_path, tables.file.File):\n        hf = hdf5_path\n    else:\n        hf = tables.openFile(hdf5_path)\n\n    grp = hf.getNode(group_path)\n    if columns is None:\n        data = [(child.name, child[:]) for child in grp]\n    else:\n        data = [(child.name, child[:]) for child in grp if child.name in columns]\n\n    # Convert any float32 columns to float64 for processing.\n    for i in range(len(data)):\n        name, vec = data[i]\n        if vec.dtype == np.float32:\n            data[i] = (name, vec.astype(np.float64))\n\n    if not isinstance(hdf5_path, tables.file.File):\n        hf.close()\n    return pd.DataFrame.from_items(data)\n']"
736;;0;15570546;15570099.0;2;14;;;;"['import pandas as pd\nimport StringIO\n\nincsv = StringIO.StringIO(""""""Date,State,City,SalesToday,SalesMTD,SalesYTD\n20130320,stA,ctA,20,400,1000\n20130320,stA,ctB,30,500,1100\n20130320,stB,ctC,10,500,900\n20130320,stB,ctD,40,200,1300\n20130320,stC,ctF,30,300,800"""""")\n\ndf = pd.read_csv(incsv, index_col=[\'Date\'], parse_dates=True)\n', ""dfsum = df.groupby('State', as_index=False).sum()\ndfsum['City'] = 'All'\n\nprint dfsum\n\n  State  SalesToday  SalesMTD  SalesYTD City\n0   stA          50       900      2100  All\n1   stB          50       700      2200  All\n2   stC          30       300       800  All\n"", ""dfsum.append(df).set_index(['State','City']).sort_index()\n\nprint dfsum\n\n            SalesMTD  SalesToday  SalesYTD\nState City                                \nstA   All        900          50      2100\n      ctA        400          20      1000\n      ctB        500          30      1100\nstB   All        700          50      2200\n      ctC        500          10       900\n      ctD        200          40      1300\nstC   All        300          30       800\n      ctF        300          30       800\n""]"
737;;1;15574875;15570099.0;2;33;;;;"[""In [10]: table = pivot_table(df, values=['SalesToday', 'SalesMTD','SalesYTD'],\\\n                     rows=['State'], cols=['City'], aggfunc=np.sum, margins=True)\n\n\nIn [11]: table.stack('City')\nOut[11]: \n            SalesMTD  SalesToday  SalesYTD\nState City                                \nstA   All        900          50      2100\n      ctA        400          20      1000\n      ctB        500          30      1100\nstB   All        700          50      2200\n      ctC        500          10       900\n      ctD        200          40      1300\nstC   All        300          30       800\n      ctF        300          30       800\nAll   All       1900         130      5100\n      ctA        400          20      1000\n      ctB        500          30      1100\n      ctC        500          10       900\n      ctD        200          40      1300\n      ctF        300          30       800\n""]"
738;;2;15705958;15705630.0;2;78;;;;"[""In [1]: df\nOut[1]:\n    Sp  Mt Value  count\n0  MM1  S1     a      3\n1  MM1  S1     n      2\n2  MM1  S3    cb      5\n3  MM2  S3    mk      8\n4  MM2  S4    bg     10\n5  MM2  S4   dgd      1\n6  MM4  S2    rd      2\n7  MM4  S2    cb      2\n8  MM4  S2   uyi      7\n\nIn [2]: df.groupby(['Mt'], sort=False)['count'].max()\nOut[2]:\nMt\nS1     3\nS3     8\nS4    10\nS2     7\nName: count\n"", ""In [3]: idx = df.groupby(['Mt'])['count'].transform(max) == df['count']\n\nIn [4]: df[idx]\nOut[4]:\n    Sp  Mt Value  count\n0  MM1  S1     a      3\n3  MM2  S3    mk      8\n4  MM2  S4    bg     10\n8  MM4  S2   uyi      7\n"", ""In [5]: df['count_max'] = df.groupby(['Mt'])['count'].transform(max)\n\nIn [6]: df\nOut[6]:\n    Sp  Mt Value  count  count_max\n0  MM1  S1     a      3          3\n1  MM1  S1     n      2          3\n2  MM1  S3    cb      5          8\n3  MM2  S3    mk      8          8\n4  MM2  S4    bg     10         10\n5  MM2  S4   dgd      1         10\n6  MM4  S2    rd      2          7\n7  MM4  S2    cb      2          7\n8  MM4  S2   uyi      7          7\n""]"
739;;4;15723905;15723628.0;2;43;;;;"['astype', ""df['col_name'] = df['col_name'].astype(object)\n"", 'df = df.astype(object)\n', ""df['col_name'] = df['col_name'].astype('category')\n"", 'pd.Factor', 'pd.Categorical']"
740;;0;15723994;15723628.0;2;12;;;;"['Factor', 'Categorical', 'pandas.Categorical.from_array', ""In [27]: df = pd.DataFrame({'a' : [1, 2, 3, 4, 5], 'b' : ['yes', 'no', 'yes', 'no', 'absent']})\n\nIn [28]: df\nOut[28]: \n   a       b\n0  1     yes\n1  2      no\n2  3     yes\n3  4      no\n4  5  absent\n\nIn [29]: df['c'] = pd.Categorical.from_array(df.b).labels\n\nIn [30]: df\nOut[30]: \n   a       b  c\n0  1     yes  2\n1  2      no  1\n2  3     yes  2\n3  4      no  1\n4  5  absent  0\n""]"
741;;1;15742147;15741759.0;2;36;;;;"['df', ""In [34]: df.loc[df['Value'].idxmax()]\nOut[34]: \nCountry        US\nPlace      Kansas\nValue         894\nName: 7\n"", 'idxmax', 'df.loc', 'df', 'stack', 'set_index', 'df = df.reset_index()\n']"
742;;5;15772263;15771472.0;2;32;;;;"['fill_method', 'pd.rolling_mean', 'pd.rolling_mean(df.resample(""1D"", fill_method=""ffill""), window=3, min_periods=1)\n\n            favorable  unfavorable     other\nenddate\n2012-10-25   0.495000     0.485000  0.025000\n2012-10-26   0.527500     0.442500  0.032500\n2012-10-27   0.521667     0.451667  0.028333\n2012-10-28   0.515833     0.450000  0.035833\n2012-10-29   0.488333     0.476667  0.038333\n2012-10-30   0.495000     0.470000  0.038333\n2012-10-31   0.512500     0.460000  0.029167\n2012-11-01   0.516667     0.456667  0.026667\n2012-11-02   0.503333     0.463333  0.033333\n2012-11-03   0.490000     0.463333  0.046667\n2012-11-04   0.494000     0.456000  0.043333\n2012-11-05   0.500667     0.452667  0.036667\n2012-11-06   0.507333     0.456000  0.023333\n2012-11-07   0.510000     0.443333  0.013333\n', 'df.resample(""1d"").sum().fillna(0).rolling(window=3, min_periods=1).mean()\n']"
743;;4;15772330;15772009.0;2;162;;;;"[""In [1]: df = pd.DataFrame({'A':range(10), 'B':range(10)})\n\nIn [2]: df\nOut[2]:\n   A  B\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n4  4  4\n5  5  5\n6  6  6\n7  7  7\n8  8  8\n9  9  9\n\n\nIn [3]: df.reindex(np.random.permutation(df.index))\nOut[3]:\n   A  B\n0  0  0\n5  5  5\n6  6  6\n3  3  3\n8  8  8\n7  7  7\n9  9  9\n1  1  1\n2  2  2\n4  4  4\n""]"
744;;6;15772356;15772009.0;2;21;;;;"[""In [16]: def shuffle(df, n=1, axis=0):     \n    ...:     df = df.copy()\n    ...:     for _ in range(n):\n    ...:         df.apply(np.random.shuffle, axis=axis)\n    ...:     return df\n    ...:     \n\nIn [17]: df = pd.DataFrame({'A':range(10), 'B':range(10)})\n\nIn [18]: shuffle(df)\n\nIn [19]: df\nOut[19]: \n   A  B\n0  8  5\n1  1  7\n2  7  3\n3  6  2\n4  3  4\n5  0  1\n6  9  0\n7  4  6\n8  2  8\n9  5  9\n""]"
745;;2;15778297;15777951.0;2;47;;;;"[""import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n""]"
746;;1;15786557;15006298.0;2;13;;;;"['def _sw(df, up_rows=10, down_rows=5, left_cols=4, right_cols=3, return_df=False):\n    \'\'\' display df data at four corners\n        A,B (up_pt)\n        C,D (down_pt)\n        parameters : up_rows=10, down_rows=5, left_cols=4, right_cols=3\n        usage:\n            df = pd.DataFrame(np.random.randn(20,10), columns=list(\'ABCDEFGHIJKLMN\')[0:10])\n            df.sw(5,2,3,2)\n            df1 = df.set_index([\'A\',\'B\'], drop=True, inplace=False)\n            df1.sw(5,2,3,2)\n    \'\'\'\n    #pd.set_printoptions(max_columns = 80, max_rows = 40)\n    ncol, nrow = len(df.columns), len(df)\n\n    # handle columns\n    if ncol <= (left_cols + right_cols) :\n        up_pt = df.ix[0:up_rows, :]         # screen width can contain all columns\n        down_pt = df.ix[-down_rows:, :]\n    else:                                   # screen width can not contain all columns\n        pt_a = df.ix[0:up_rows,  0:left_cols]\n        pt_b = df.ix[0:up_rows,  -right_cols:]\n        pt_c = df[-down_rows:].ix[:,0:left_cols]\n        pt_d = df[-down_rows:].ix[:,-right_cols:]\n\n        up_pt   = pt_a.join(pt_b, how=\'inner\')\n        down_pt = pt_c.join(pt_d, how=\'inner\')\n        up_pt.insert(left_cols, \'..\', \'..\')\n        down_pt.insert(left_cols, \'..\', \'..\')\n\n    overlap_qty = len(up_pt) + len(down_pt) - len(df)\n    down_pt = down_pt.drop(down_pt.index[range(overlap_qty)]) # remove overlap rows\n\n    dt_str_list = down_pt.to_string().split(\'\\n\') # transfer down_pt to string list\n\n    # Display up part data\n    print up_pt\n\n    start_row = (1 if df.index.names[0] is None else 2) # start from 1 if without index\n\n    # Display omit line if screen height is not enought to display all rows\n    if overlap_qty < 0:\n        print ""."" * len(dt_str_list[start_row])\n\n    # Display down part data row by row\n    for line in dt_str_list[start_row:]:\n        print line\n\n    # Display foot note\n    print ""\\n""\n    print ""Index :"",df.index.names\n    print ""Column:"","","".join(list(df.columns.values))\n    print ""row: %d    col: %d""%(len(df), len(df.columns))\n    print ""\\n""\n\n    return (df if return_df else None)\nDataFrame.sw = _sw  #add a method to DataFrame class\n', "">>> df = pd.DataFrame(np.random.randn(20,10), columns=list('ABCDEFGHIJKLMN')[0:10])\n\n>>> df.sw()\n         A       B       C       D  ..       H       I       J\n0  -0.8166  0.0102  0.0215 -0.0307  .. -0.0820  1.2727  0.6395\n1   1.0659 -1.0102 -1.3960  0.4700  ..  1.0999  1.1222 -1.2476\n2   0.4347  1.5423  0.5710 -0.5439  ..  0.2491 -0.0725  2.0645\n3  -1.5952 -1.4959  2.2697 -1.1004  .. -1.9614  0.6488 -0.6190\n4  -1.4426 -0.8622  0.0942 -0.1977  .. -0.7802 -1.1774  1.9682\n5   1.2526 -0.2694  0.4841 -0.7568  ..  0.2481  0.3608 -0.7342\n6   0.2108  2.5181  1.3631  0.4375  .. -0.1266  1.0572  0.3654\n7  -1.0617 -0.4743 -1.7399 -1.4123  .. -1.0398 -1.4703 -0.9466\n8  -0.5682 -1.3323 -0.6992  1.7737  ..  0.6152  0.9269  2.1854\n9   0.2361  0.4873 -1.1278 -0.2251  ..  1.4232  2.1212  2.9180\n10  2.0034  0.5454 -2.6337  0.1556  ..  0.0016 -1.6128 -0.8093\n..............................................................\n15  1.4091  0.3540 -1.3498 -1.0490  ..  0.9328  0.3668  1.3948\n16  0.4528 -0.3183  0.4308 -0.1818  ..  0.1295  1.2268  0.1365\n17 -0.7093  1.3991  0.9501  2.1227  .. -1.5296  1.1908  0.0318\n18  1.7101  0.5962  0.8948  1.5606  .. -0.6862  0.9558 -0.5514\n19  1.0329 -1.2308 -0.6896 -0.5112  ..  0.2719  1.1478 -0.1459\n\n\nIndex : [None]\nColumn: A,B,C,D,E,F,G,H,I,J\nrow: 20    col: 10\n\n\n>>> df.sw(4,2,3,4)\n        A       B       C  ..       G       H       I       J\n0 -0.8166  0.0102  0.0215  ..  0.3671 -0.0820  1.2727  0.6395\n1  1.0659 -1.0102 -1.3960  ..  1.0984  1.0999  1.1222 -1.2476\n2  0.4347  1.5423  0.5710  ..  1.6675  0.2491 -0.0725  2.0645\n3 -1.5952 -1.4959  2.2697  ..  0.4856 -1.9614  0.6488 -0.6190\n4 -1.4426 -0.8622  0.0942  .. -0.0947 -0.7802 -1.1774  1.9682\n..............................................................\n18  1.7101  0.5962  0.8948  .. -0.8592 -0.6862  0.9558 -0.5514\n19  1.0329 -1.2308 -0.6896  .. -0.3954  0.2719  1.1478 -0.1459\n\n\nIndex : [None]\nColumn: A,B,C,D,E,F,G,H,I,J\nrow: 20    col: 10\n""]"
747;;3;15822811;15819050.0;2;23;;;;"[""In [17]: df1 = pd.DataFrame(dict(A = range(10000)),index=pd.date_range('20130101',periods=10000,freq='s'))\n\nIn [18]: df1\nOut[18]: \n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 10000 entries, 2013-01-01 00:00:00 to 2013-01-01 02:46:39\nFreq: S\nData columns (total 1 columns):\nA    10000  non-null values\ndtypes: int64(1)\n\nIn [19]: df4 = pd.DataFrame()\n\nThe concat\n\nIn [20]: %timeit pd.concat([df1,df2,df3])\n1000 loops, best of 3: 270 us per loop\n\nThis is equavalent of your append\n\nIn [21]: %timeit pd.concat([df4,df1,df2,df3])\n10 loops, best of \n\n 3: 56.8 ms per loop\n""]"
748;;1;15855998;15854878.0;2;60;;;;"['z[k] = sum_n a[n] * conj(v[n+k])\n', ""df['a']"", ""df['b']"", 'df.corr', 'Beta', 'cov = np.cov(a, b)\nbeta = cov[1, 0] / cov[0, 0]\n', 'import numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(100)\n\n\ndef geometric_brownian_motion(T=1, N=100, mu=0.1, sigma=0.01, S0=20):\n    """"""\n    http://stackoverflow.com/a/13203189/190597 (unutbu)\n    """"""\n    dt = float(T) / N\n    t = np.linspace(0, T, N)\n    W = np.random.standard_normal(size=N)\n    W = np.cumsum(W) * np.sqrt(dt)  # standard brownian motion ###\n    X = (mu - 0.5 * sigma ** 2) * t + sigma * W\n    S = S0 * np.exp(X)  # geometric brownian motion ###\n    return S\n\nN = 10 ** 6\na = geometric_brownian_motion(T=1, mu=0.1, sigma=0.01, N=N)\nb = geometric_brownian_motion(T=1, mu=0.2, sigma=0.01, N=N)\n\ncov = np.cov(a, b)\nprint(cov)\n# [[ 0.38234755  0.80525967]\n#  [ 0.80525967  1.73517501]]\nbeta = cov[1, 0] / cov[0, 0]\nprint(beta)\n# 2.10609347015\n\nplt.plot(a)\nplt.plot(b)\nplt.show()\n', 'mu', 'beta', 'df.corr', ""import pandas as pd\ndf = pd.DataFrame({'a': a, 'b': b})\nbeta2 = (df.corr() * df['b'].std() * df['a'].std() / df['a'].var()).ix[0, 1]\nprint(beta2)\n# 2.10609347015\nassert np.allclose(beta, beta2)\n""]"
749;;2;15863028;15862034.0;2;26;;;;"['.iloc', '>>> df = pd.DataFrame({""date"": range(10, 64, 8)})\n>>> df.index += 17\n>>> df\n    date\n17    10\n18    18\n19    26\n20    34\n21    42\n22    50\n23    58\n>>> df[""date""].iloc[0]\n10\n>>> df[""date""].iloc[-1]\n58\n', '.iget()', '>>> df = pd.DataFrame({""date"": range(10, 64, 8)})\n>>> df.index += 17\n>>> df\n    date\n17    10\n18    18\n19    26\n20    34\n21    42\n22    50\n23    58\n>>> df[\'date\'].iget(0)\n10\n>>> df[\'date\'].iget(-1)\n58\n', "">>> df['date'][df.index[0]]\n10\n>>> df['date'][df.index[-1]]\n58\n"", '.first_valid_index()', '.last_valid_index()', 'NaN', 'df.ix[0]', 'df.ix[0]', '>>> df.ix[0]\nTraceback (most recent call last):\n  File ""<ipython-input-489-494245247e87>"", line 1, in <module>\n    df.ix[0]\n[...]\nKeyError: 0\n']"
750;;4;15923878;15923826.0;2;28;;;;['import random\n\ndef some(x, n):\n    return x.ix[random.sample(x.index, n)]\n']
751;;7;15943975;15943769.0;2;331;;;;['.shape', 'len(DataFrame.index)', '.shape', 'In [1]: import numpy as np\n\nIn [2]: import pandas as pd\n\nIn [3]: df = pd.DataFrame(np.arange(9).reshape(3,3))\n\nIn [4]: df\nOut[4]: \n   0  1  2\n0  0  1  2\n1  3  4  5\n2  6  7  8\n\nIn [5]: df.shape\nOut[5]: (3, 3)\n\nIn [6]: timeit df.shape\n1000000 loops, best of 3: 1.17 us per loop\n\nIn [7]: timeit df[0].count()\n10000 loops, best of 3: 56 us per loop\n\nIn [8]: len(df.index)\nOut[8]: 3\n\nIn [9]: timeit len(df.index)\n1000000 loops, best of 3: 381 ns per loop\n', 'len(df.index)', 'df[0].count()', 'count', 'NaN']
752;;1;15998251;15998188.0;2;8;;;;['In [9]: s = Series([True, True, True, False])\n\nIn [10]: s\nOut[10]: \n0     True\n1     True\n2     True\n3    False\n\nIn [11]: -s\nOut[11]: \n0    False\n1    False\n2    False\n3     True\n']
753;;8;15998993;15998188.0;2;95;;;;['~s', 'In [7]: s = pd.Series([True, True, False, True])\n\nIn [8]: ~s\nOut[8]: \n0    False\n1    False\n2     True\n3    False\ndtype: bool\n', 'In [119]: s = pd.Series([True, True, False, True]*10000)\n\nIn [10]:  %timeit np.invert(s)\n10000 loops, best of 3: 91.8 s per loop\n\nIn [11]: %timeit ~s\n10000 loops, best of 3: 73.5 s per loop\n\nIn [12]: %timeit (-s)\n10000 loops, best of 3: 73.5 s per loop\n', 'numpy.ndarray', 'pd.NDFrame', 'np.invert(s)', '~s', '-s', 'timeit']
754;;9;16033048;16031056.0;2;30;;;;"[""In [10]: df\nOut[10]:\n          A         B       lat      long\n0  1.428987  0.614405  0.484370 -0.628298\n1 -0.485747  0.275096  0.497116  1.047605\n2  0.822527  0.340689  2.120676 -2.436831\n3  0.384719 -0.042070  1.426703 -0.634355\n4 -0.937442  2.520756 -1.662615 -1.377490\n5 -0.154816  0.617671 -0.090484 -0.191906\n6 -0.705177 -1.086138 -0.629708  1.332853\n7  0.637496 -0.643773 -0.492668 -0.777344\n8  1.109497 -0.610165  0.260325  2.533383\n9 -1.224584  0.117668  1.304369 -0.152561\n\nIn [11]: df['lat_long'] = df[['lat', 'long']].apply(tuple, axis=1)\n\nIn [12]: df\nOut[12]:\n          A         B       lat      long                             lat_long\n0  1.428987  0.614405  0.484370 -0.628298      (0.484370195967, -0.6282975278)\n1 -0.485747  0.275096  0.497116  1.047605      (0.497115615839, 1.04760475074)\n2  0.822527  0.340689  2.120676 -2.436831      (2.12067574274, -2.43683074367)\n3  0.384719 -0.042070  1.426703 -0.634355      (1.42670326172, -0.63435462504)\n4 -0.937442  2.520756 -1.662615 -1.377490     (-1.66261469102, -1.37749004179)\n5 -0.154816  0.617671 -0.090484 -0.191906  (-0.0904840623396, -0.191905582481)\n6 -0.705177 -1.086138 -0.629708  1.332853     (-0.629707821728, 1.33285348929)\n7  0.637496 -0.643773 -0.492668 -0.777344   (-0.492667604075, -0.777344111021)\n8  1.109497 -0.610165  0.260325  2.533383        (0.26032456699, 2.5333825651)\n9 -1.224584  0.117668  1.304369 -0.152561     (1.30436900612, -0.152560909725)\n""]"
755;;4;16068497;16031056.0;2;57;;;;"['zip', ""df['new_col'] = list(zip(df.lat, df.long))\n"", 'apply', 'map', 'np.dstack', 'zip']"
756;;7;16074407;16074392.0;2;47;;;;['ax.yaxis.grid(True)', 'ax.yaxis.grid()', 'ax.grid', 'ax = plt.gca()\nax.grid(True)\n']
757;;3;16089219;16088741.0;2;37;;;;"[""df['bar', 'three'] = [0, 1, 2]\ndf = df.sort_index(axis=1)\nprint(df)\n\n        bar                        baz          \n        one       two  three       one       two\nA -0.212901  0.503615      0 -1.660945  0.446778\nB -0.803926 -0.417570      1 -0.336827  0.989343\nC  3.400885 -0.214245      2  0.895745  1.011671\n""]"
758;;0;16099579;16096627.0;2;12;;;;['df[key]', 'key']
759;;3;16104482;16096627.0;2;247;;;;"['.iloc', '.loc', ""In [1]: df = DataFrame(randn(5,2),index=range(0,10,2),columns=list('AB'))\n\nIn [2]: df\nOut[2]: \n          A         B\n0  1.068932 -0.794307\n2 -0.470056  1.192211\n4 -0.284561  0.756029\n6  1.037563 -0.267820\n8 -0.538478 -0.800654\n\nIn [5]: df.iloc[[2]]\nOut[5]: \n          A         B\n4 -0.284561  0.756029\n\nIn [6]: df.loc[[2]]\nOut[6]: \n          A         B\n2 -0.470056  1.192211\n"", '[]']"
760;;9;16134561;15891038.0;2;300;;;;"[""a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\ndf = pd.DataFrame(a, columns=['one', 'two', 'three'])\ndf\nOut[16]: \n  one  two three\n0   a  1.2   4.2\n1   b   70  0.03\n2   x    5     0\n\ndf.dtypes\nOut[17]: \none      object\ntwo      object\nthree    object\n\ndf[['two', 'three']] = df[['two', 'three']].astype(float)\n\ndf.dtypes\nOut[19]: \none       object\ntwo      float64\nthree    float64\n""]"
761;;4;16176457;16175874.0;2;37;;;;['searchsorted', 'In [15]: df = pd.DataFrame([1, 2, 3], index=[dt.datetime(2013, 1, 1), dt.datetime(2013, 1, 3), dt.datetime(2013, 1, 5)])\n\nIn [16]: df\nOut[16]: \n            0\n2013-01-01  1\n2013-01-03  2\n2013-01-05  3\n\nIn [22]: start = df.index.searchsorted(dt.datetime(2013, 1, 2))\n\nIn [23]: end = df.index.searchsorted(dt.datetime(2013, 1, 4))\n\nIn [24]: df.ix[start:end]\nOut[24]: \n            0\n2013-01-03  2\n']
762;;5;16179190;16175874.0;2;20;;;;"['data.sort()', ""In [12]: df\nOut[12]: \n                   0\n2013-04-20  1.120024\n2013-04-21 -0.721101\n2013-04-22  0.379392\n2013-04-23  0.924535\n2013-04-24  0.531902\n2013-04-25 -0.957936\n\nIn [13]: df['20130419':'20130422']\nOut[13]: \n                   0\n2013-04-20  1.120024\n2013-04-21 -0.721101\n2013-04-22  0.379392\n"", 'In [17]: df\nOut[17]: \n                   0\n2013-04-22  1.120024\n2013-04-20 -0.721101\n2013-04-24  0.379392\n2013-04-23  0.924535\n2013-04-21  0.531902\n2013-04-25 -0.957936\n', ""In [18]: df['20130419':'20130422']\nOut[18]: \n                   0\n2013-04-22  1.120024\n2013-04-20 -0.721101\n2013-04-24  0.379392\n"", 'df.sort_index()\n']"
763;;0;16242202;16236684.0;2;48;;;;"[""df = pd.DataFrame({'textcol' : np.random.rand(5)})\n\ndf\n    textcol\n0  0.626524\n1  0.119967\n2  0.803650\n3  0.100880\n4  0.017859\n\ndf.textcol.apply(lambda s: pd.Series({'feature1':s+1, 'feature2':s-1}))\n   feature1  feature2\n0  1.626524 -0.373476\n1  1.119967 -0.880033\n2  1.803650 -0.196350\n3  1.100880 -0.899120\n4  1.017859 -0.982141\n"", ""pd.concat([df, df.textcol.apply(lambda s: pd.Series({'feature1':s+1, 'feature2':s-1}))], axis=1)\n    textcol feature1  feature2\n0  0.626524 1.626524 -0.373476\n1  0.119967 1.119967 -0.880033\n2  0.803650 1.803650 -0.196350\n3  0.100880 1.100880 -0.899120\n4  0.017859 1.017859 -0.982141\n""]"
764;;1;16245109;16236684.0;2;44;;;;"['df.merge', ""df.merge(df.textcol.apply(lambda s: pd.Series({'feature1':s+1, 'feature2':s-1})), \n    left_index=True, right_index=True)\n\n    textcol  feature1  feature2\n0  0.772692  1.772692 -0.227308\n1  0.857210  1.857210 -0.142790\n2  0.065639  1.065639 -0.934361\n3  0.819160  1.819160 -0.180840\n4  0.088212  1.088212 -0.911788\n""]"
765;;4;16255680;16249736.0;2;52;;;;"['pymongo', 'import pandas as pd\nfrom pymongo import MongoClient\n\n\ndef _connect_mongo(host, port, username, password, db):\n    """""" A util for making a connection to mongo """"""\n\n    if username and password:\n        mongo_uri = \'mongodb://%s:%s@%s:%s/%s\' % (username, password, host, port, db)\n        conn = MongoClient(mongo_uri)\n    else:\n        conn = MongoClient(host, port)\n\n\n    return conn[db]\n\n\ndef read_mongo(db, collection, query={}, host=\'localhost\', port=27017, username=None, password=None, no_id=True):\n    """""" Read from Mongo and Store into DataFrame """"""\n\n    # Connect to MongoDB\n    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\n\n    # Make a query to the specific DB and Collection\n    cursor = db[collection].find(query)\n\n    # Expand the cursor and construct the DataFrame\n    df =  pd.DataFrame(list(cursor))\n\n    # Delete the _id\n    if no_id:\n        del df[\'_id\']\n\n    return df\n']"
766;;2;16266318;16266019.0;2;25;;;;['df', 'times = pd.to_datetime(df.timestamp_col)\ndf.groupby([times.hour, times.minute]).value_col.sum()\n']
767;;2;16271849;11697887.0;2;32;;;;"[""import pandas as pd\nimport datetime\nfrom myapp.models import BlogPost\n\ndf = pd.DataFrame(list(BlogPost.objects.all().values()))\ndf = pd.DataFrame(list(BlogPost.objects.filter(date__gte=datetime.datetime(2012, 5, 1)).values()))\n\n# limit which fields\ndf = pd.DataFrame(list(BlogPost.objects.all().values('author', 'date', 'slug')))\n""]"
768;;2;16327135;16327055.0;2;117;;;;"['>>> df = pd.DataFrame({""A"": [1,2,3], ""B"": [2,3,4]})\n>>> df\n   A  B\n0  1  2\n1  2  3\n2  3  4\n>>> df[""C""] = """"\n>>> df[""D""] = np.nan\n>>> df\n   A  B C   D\n0  1  2   NaN\n1  2  3   NaN\n2  3  4   NaN\n']"
769;;1;16354103;16353729.0;2;12;;;;"['apply', ""In [7]: df['a'] % df['c']                                                                                                                                                        \nOut[7]: \n0   -1.132022                                                                                                                                                                    \n1   -0.939493                                                                                                                                                                    \n2    0.201931                                                                                                                                                                    \n3    0.511374                                                                                                                                                                    \n4   -0.694647                                                                                                                                                                    \n5   -0.023486                                                                                                                                                                    \nName: a\n""]"
770;;4;16354730;16353729.0;2;133;;;;"[""''"", ""In [43]: df['Value'] = df.apply(lambda row: my_test(row['a'], row['c']), axis=1)\n\nIn [44]: df\nOut[44]:\n                    a    b         c     Value\n          0 -1.674308  foo  0.343801  0.044698\n          1 -2.163236  bar -2.046438 -0.116798\n          2 -0.199115  foo -0.458050 -0.199115\n          3  0.918646  bar -0.007185 -0.001006\n          4  1.336830  foo  0.534292  0.268245\n          5  0.976844  bar -0.773630 -0.570417\n"", ""In [53]: def my_test2(row):\n....:     return row['a'] % row['c']\n....:     \n\nIn [54]: df['Value'] = df.apply(my_test2, axis=1)\n""]"
771;;5;16393023;16392921.0;2;48;;;;"[""fig, axs = plt.subplots(1,2)\n\ndf['korisnika'].plot(ax=axs[0])\ndf['osiguranika'].plot(ax=axs[1])\n""]"
772;;4;16398361;16396903.0;2;63;;;;['iloc', 'ix', 'df2 = df1.iloc[3:] #edited since .ix is now deprecated.\n']
773;;2;16433953;16424493.0;2;79;;;;"['display.height', 'display.max_rows', ""pd.set_option('display.height', 500)\npd.set_option('display.max_rows', 500)\n"", ""pd.describe_option('display')""]"
774;;1;16476974;16476924.0;2;556;;;;"[""In [18]: for index, row in df.iterrows():\n   ....:     print row['c1'], row['c2']\n   ....:     \n10 100\n11 110\n12 120\n""]"
775;;4;16522626;16522380.0;2;134;;;;['import matplotlib.pyplot as plt\n', 'plt.show()\n']
776;;0;16545324;14941366.0;2;19;;;;"[""df['a_bsum'] = df.groupby('A')['B'].transform(sum)\ndf.sort(['a_bsum','C'], ascending=[True, False]).drop('a_bsum', axis=1)\n"", '    A       B           C\n5   baz     -2.301539   True\n2   baz     -0.528172   False\n1   bar     -0.611756   True\n4   bar      0.865408   False\n3   foo     -1.072969   True\n0   foo      1.624345   False\n', 'sort', 'sort_values']"
777;;4;16597375;16597265.0;2;156;;;;"['>>> df = pd.DataFrame()\n>>> data = pd.DataFrame({""A"": range(3)})\n>>> df.append(data)\n   A\n0  0\n1  1\n2  2\n', 'append', '>>> df\nEmpty DataFrame\nColumns: []\nIndex: []\n>>> df = df.append(data)\n>>> df\n   A\n0  0\n1  1\n2  2\n']"
778;;3;16629243;16628329.0;2;58;;;;"['HDFStore', 'HDFStore', 'In [14]: %timeit test_sql_write(df)\n1 loops, best of 3: 6.24 s per loop\n\nIn [15]: %timeit test_hdf_fixed_write(df)\n1 loops, best of 3: 237 ms per loop\n\nIn [16]: %timeit test_hdf_table_write(df)\n1 loops, best of 3: 901 ms per loop\n\nIn [17]: %timeit test_csv_write(df)\n1 loops, best of 3: 3.44 s per loop\n', 'In [18]: %timeit test_sql_read()\n1 loops, best of 3: 766 ms per loop\n\nIn [19]: %timeit test_hdf_fixed_read()\n10 loops, best of 3: 19.1 ms per loop\n\nIn [20]: %timeit test_hdf_table_read()\n10 loops, best of 3: 39 ms per loop\n\nIn [22]: %timeit test_csv_read()\n1 loops, best of 3: 620 ms per loop\n', 'import sqlite3\nimport os\nfrom pandas.io import sql\n\nIn [3]: df = DataFrame(randn(1000000,2),columns=list(\'AB\'))\n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 1000000 entries, 0 to 999999\nData columns (total 2 columns):\nA    1000000  non-null values\nB    1000000  non-null values\ndtypes: float64(2)\n\ndef test_sql_write(df):\n    if os.path.exists(\'test.sql\'):\n        os.remove(\'test.sql\')\n    sql_db = sqlite3.connect(\'test.sql\')\n    sql.write_frame(df, name=\'test_table\', con=sql_db)\n    sql_db.close()\n\ndef test_sql_read():\n    sql_db = sqlite3.connect(\'test.sql\')\n    sql.read_frame(""select * from test_table"", sql_db)\n    sql_db.close()\n\ndef test_hdf_fixed_write(df):\n    df.to_hdf(\'test_fixed.hdf\',\'test\',mode=\'w\')\n\ndef test_csv_read():\n    pd.read_csv(\'test.csv\',index_col=0)\n\ndef test_csv_write(df):\n    df.to_csv(\'test.csv\',mode=\'w\')    \n\ndef test_hdf_fixed_read():\n    pd.read_hdf(\'test_fixed.hdf\',\'test\')\n\ndef test_hdf_table_write(df):\n    df.to_hdf(\'test_table.hdf\',\'test\',format=\'table\',mode=\'w\')\n\ndef test_hdf_table_read():\n    pd.read_hdf(\'test_table.hdf\',\'test\')\n']"
779;;3;16667215;11346283.0;2;216;;;;"['rename', ""In [11]: df.columns\nOut[11]: Index([u'$a', u'$b', u'$c', u'$d', u'$e'], dtype=object)\n\nIn [12]: df.rename(columns=lambda x: x[1:], inplace=True)\n\nIn [13]: df.columns\nOut[13]: Index([u'a', u'b', u'c', u'd', u'e'], dtype=object)\n""]"
780;;3;16689573;16689514.0;2;48;;;;"['df.mean()', '>>> df\n                 A      B\nDATE                     \n2013-05-01  473077  71333\n2013-05-02   35131  62441\n2013-05-03     727  27381\n2013-05-04     481   1206\n2013-05-05     226   1733\n2013-05-06     NaN   4064\n2013-05-07     NaN  41151\n2013-05-08     NaN   8144\n2013-05-09     NaN     23\n2013-05-10     NaN     10\n>>> df.mean(axis=1)\nDATE\n2013-05-01    272205.0\n2013-05-02     48786.0\n2013-05-03     14054.0\n2013-05-04       843.5\n2013-05-05       979.5\n2013-05-06      4064.0\n2013-05-07     41151.0\n2013-05-08      8144.0\n2013-05-09        23.0\n2013-05-10        10.0\ndtype: float64\n', 'df[[""A"", ""B""]].mean(axis=1)']"
781;;0;16729635;16729483.0;2;21;;;;"['df.column_name = df.column_name.astype(float)', 'NaN', '.fillna', ""In [12]: df\nOut[12]: \n     a    b\n0  0.1  0.2\n1  NaN  0.3\n2  0.4  0.5\n\nIn [13]: df.a.values\nOut[13]: array(['0.1', nan, '0.4'], dtype=object)\n\nIn [14]: df.a = df.a.astype(float).fillna(0.0)\n\nIn [15]: df\nOut[15]: \n     a    b\n0  0.1  0.2\n1  0.0  0.3\n2  0.4  0.5\n\nIn [16]: df.a.values\nOut[16]: array([ 0.1,  0. ,  0.4])\n""]"
782;;5;16729808;16729574.0;2;100;;;;"['iloc', ""In [3]: sub_df\nOut[3]:\n          A         B\n2 -0.133653 -0.030854\n\nIn [4]: sub_df.iloc[0]\nOut[4]:\nA   -0.133653\nB   -0.030854\nName: 2, dtype: float64\n\nIn [5]: sub_df.iloc[0]['A']\nOut[5]: -0.13365288513107493\n""]"
783;;6;16735476;16729483.0;2;43;;;;"['astype', ""In [10]: df = DataFrame(dict(A = Series(['1.0','1']), B = Series(['1.0','foo'])))\n\nIn [11]: df\nOut[11]: \n     A    B\n0  1.0  1.0\n1    1  foo\n\nIn [12]: df.dtypes\nOut[12]: \nA    object\nB    object\ndtype: object\n\nIn [13]: df.convert_objects(convert_numeric=True)\nOut[13]: \n   A   B\n0  1   1\n1  1 NaN\n\nIn [14]: df.convert_objects(convert_numeric=True).dtypes\nOut[14]: \nA    float64\nB    float64\ndtype: object\n""]"
784;;0;16735536;16729574.0;2;51;;;;"[""In [15]: df = DataFrame(randn(5,3),columns=list('ABC'))\n\nIn [16]: df\nOut[16]: \n          A         B         C\n0 -0.074172 -0.090626  0.038272\n1 -0.128545  0.762088 -0.714816\n2  0.201498 -0.734963  0.558397\n3  1.563307 -1.186415  0.848246\n4  0.205171  0.962514  0.037709\n\nIn [17]: df.iat[0,0]\nOut[17]: -0.074171888537611502\n\nIn [18]: df.at[0,'A']\nOut[18]: -0.074171888537611502\n""]"
785;;0;16780413;16777570.0;2;39;;;;"[""In [196]: df\nOut[196]: \n                     value\n2012-03-16 23:50:00      1\n2012-03-16 23:56:00      2\n2012-03-17 00:08:00      3\n2012-03-17 00:10:00      4\n2012-03-17 00:12:00      5\n2012-03-17 00:20:00      6\n2012-03-20 00:43:00      7\n\nIn [199]: df.index\nOut[199]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-03-16 23:50:00, ..., 2012-03-20 00:43:00]\nLength: 7, Freq: None, Timezone: None\n"", ""In [200]: df['tvalue'] = df.index\n\nIn [201]: df['delta'] = (df['tvalue']-df['tvalue'].shift()).fillna(0)\n\nIn [202]: df\nOut[202]: \n                     value              tvalue            delta\n2012-03-16 23:50:00      1 2012-03-16 23:50:00         00:00:00\n2012-03-16 23:56:00      2 2012-03-16 23:56:00         00:06:00\n2012-03-17 00:08:00      3 2012-03-17 00:08:00         00:12:00\n2012-03-17 00:10:00      4 2012-03-17 00:10:00         00:02:00\n2012-03-17 00:12:00      5 2012-03-17 00:12:00         00:02:00\n2012-03-17 00:20:00      6 2012-03-17 00:20:00         00:08:00\n2012-03-20 00:43:00      7 2012-03-20 00:43:00 3 days, 00:23:00\n"", ""In [204]: df['ans'] = df['delta'].apply(lambda x: x  / np.timedelta64(1,'m')).astype('int64') % (24*60)\n\nIn [205]: df\nOut[205]: \n                     value              tvalue            delta  ans\n2012-03-16 23:50:00      1 2012-03-16 23:50:00         00:00:00    0\n2012-03-16 23:56:00      2 2012-03-16 23:56:00         00:06:00    6\n2012-03-17 00:08:00      3 2012-03-17 00:08:00         00:12:00   12\n2012-03-17 00:10:00      4 2012-03-17 00:10:00         00:02:00    2\n2012-03-17 00:12:00      5 2012-03-17 00:12:00         00:02:00    2\n2012-03-17 00:20:00      6 2012-03-17 00:20:00         00:08:00    8\n2012-03-20 00:43:00      7 2012-03-20 00:43:00 3 days, 00:23:00   23\n""]"
786;;3;16789254;16782323.0;2;40;;;;"[""In [10]: df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])\n\nIn [11]: df\nOut[11]:\n   A  B\n0  1  2\n1  3  4\n\nIn [12]: df[['A']]\n\nIn [13]: df[[0]]\n\nIn [14]: df.loc[:, ['A']]\n\nIn [15]: df.iloc[:, [0]]\n\nOut[12-15]:  # they all return the same thing:\n   A\n0  1\n1  3\n"", ""In [16]: df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 0])\n\nIn [17]: df\nOut[17]:\n   A  0\n0  1  2\n1  3  4\n\nIn [18]: df[[0]]  # ambiguous\nOut[18]:\n   A\n0  1\n1  3\n""]"
787;;0;16789834;11361985.0;2;20;;;;['import sys\n\nparamdata.to_csv(sys.stdout)\n']
788;;0;16826250;13331698.0;2;7;;;;[]
789;;6;16853161;16852911.0;2;42;;;;"[""In [31]: df\nOut[31]: \n   a        time\n0  1  2013-01-01\n1  2  2013-01-02\n2  3  2013-01-03\n\nIn [32]: df['time'] = df['time'].astype('datetime64[ns]')\n\nIn [33]: df\nOut[33]: \n   a                time\n0  1 2013-01-01 00:00:00\n1  2 2013-01-02 00:00:00\n2  3 2013-01-03 00:00:00\n""]"
790;;9;16854430;16852911.0;2;58;;;;"['to_datetime', 'dayfirst', ""In [11]: df\nOut[11]:\n   a        time\n0  1  2013-01-01\n1  2  2013-01-02\n2  3  2013-01-03\n\nIn [12]: pd.to_datetime(df['time'])\nOut[12]:\n0   2013-01-01 00:00:00\n1   2013-01-02 00:00:00\n2   2013-01-03 00:00:00\nName: time, dtype: datetime64[ns]\n\nIn [13]: df['time'] = pd.to_datetime(df['time'])\n\nIn [14]: df\nOut[14]:\n   a                time\n0  1 2013-01-01 00:00:00\n1  2 2013-01-02 00:00:00\n2  3 2013-01-03 00:00:00\n""]"
791;;0;16884805;10636024.0;2;7;;;;"['# Format floating point numbers with 2 decimal places.\ndata_table = df.to_html(float_format=lambda x: \'%6.2f\' % x,\n    classes=""table display"")\n# The to_html() method forces a html table border of 1 pixel.\n# I use 0  in my table so I  change the html, since there is no \n# border argument in the to_html() method.\ndata_table = data_table.replace(\'border=""1""\',\'border=""0""\')\n# I alson like to display blanks instead on nan.\ndata_table = data_table.replace(\'nan\', \'\')\n', '$(\'.table\').dataTable({\n    ""bPaginate"": true,\n    ""bLengthChange"": true,\n    ""bSort"": false,\n    ""bStateSave"": true,\n    ""sScrollY"": 900,\n    ""sScrollX"": 1000,\n    ""aLengthMenu"": [[50, 100, 250, 500, 1000, -1], [50, 100, 250, 500, 1000, ""All""]],\n    ""iDisplayLength"": 100,\n});\n']"
792;;6;16896091;16888888.0;2;40;;;;['DataFrame', 'xl_file = pd.ExcelFile(file_name)\n\ndfs = {sheet_name: xl_file.parse(sheet_name) \n          for sheet_name in xl_file.sheet_names}\n', 'sheetname=None', 'read_excel', 'dfs = pd.read_excel(file_name, sheetname=None)\n']
793;;6;16923367;16923281.0;2;353;;;;"['sep', 'to_csv', ""df.to_csv(file_name, sep='\\t')\n"", 'encoding', ""df.to_csv(file_name, sep='\\t', encoding='utf-8')\n""]"
794;;4;16949498;16947336.0;2;41;;;;"['pandas.crosstab', 'import numpy as np\nimport pandas\n\ndf = pandas.DataFrame({""a"": np.random.random(100),\n                       ""b"": np.random.random(100),\n                       ""id"": np.arange(100)})\n\n# Bin the data frame by ""a"" with 10 bins...\nbins = np.linspace(df.a.min(), df.a.max(), 10)\ngroups = df.groupby(np.digitize(df.a, bins))\n\n# Get the mean of each bin:\nprint groups.mean() # Also could do ""groups.aggregate(np.mean)""\n\n# Similarly, the median:\nprint groups.median()\n\n# Apply some arbitrary function to aggregate binned data\nprint groups.aggregate(lambda x: np.mean(x[x > 0.5]))\n', 'b', 'a', 'groups.mean().b\n', 'pandas.cut', 'numpy.digitize', 'pandas.cut', 'import numpy as np\nimport pandas\n\ndf = pandas.DataFrame({""a"": np.random.random(100), \n                       ""b"": np.random.random(100) + 10})\n\n# Bin the data frame by ""a"" with 10 bins...\nbins = np.linspace(df.a.min(), df.a.max(), 10)\ngroups = df.groupby(pandas.cut(df.a, bins))\n\n# Get the mean of b, binned by the values in a\nprint groups.mean().b\n', 'a\n(0.00186, 0.111]    10.421839\n(0.111, 0.22]       10.427540\n(0.22, 0.33]        10.538932\n(0.33, 0.439]       10.445085\n(0.439, 0.548]      10.313612\n(0.548, 0.658]      10.319387\n(0.658, 0.767]      10.367444\n(0.767, 0.876]      10.469655\n(0.876, 0.986]      10.571008\nName: b\n']"
795;;2;16949500;16947336.0;2;19;;;;"['In [144]: df = DataFrame({""a"": np.random.random(100), ""b"": np.random.random(100), ""id"":   np.arange(100)})\n\nIn [145]: bins = [0, .25, .5, .75, 1]\n\nIn [146]: a_bins = df.a.groupby(cut(df.a,bins))\n\nIn [147]: b_bins = df.b.groupby(cut(df.b,bins))\n\nIn [148]: a_bins.agg([mean,median])\nOut[148]:\n                 mean    median\na\n(0, 0.25]    0.124173  0.114613\n(0.25, 0.5]  0.367703  0.358866\n(0.5, 0.75]  0.624251  0.626730\n(0.75, 1]    0.875395  0.869843\n\nIn [149]: b_bins.agg([mean,median])\nOut[149]:\n                 mean    median\nb\n(0, 0.25]    0.147936  0.166900\n(0.25, 0.5]  0.394918  0.386729\n(0.5, 0.75]  0.636111  0.655247\n(0.75, 1]    0.851227  0.838805\n']"
796;;3;17001474;17001389.0;2;175;;;;['B       business day frequency\nC       custom business day frequency (experimental)\nD       calendar day frequency\nW       weekly frequency\nM       month end frequency\nSM      semi-month end frequency (15th and end of month)\nBM      business month end frequency\nCBM     custom business month end frequency\nMS      month start frequency\nSMS     semi-month start frequency (1st and 15th)\nBMS     business month start frequency\nCBMS    custom business month start frequency\nQ       quarter end frequency\nBQ      business quarter endfrequency\nQS      quarter start frequency\nBQS     business quarter start frequency\nA       year end frequency\nBA      business year end frequency\nAS      year start frequency\nBAS     business year start frequency\nBH      business hour frequency\nH       hourly frequency\nT       minutely frequency\nS       secondly frequency\nL       milliseonds\nU       microseconds\nN       nanoseconds\n', 'how', 'how']
797;;1;17027507;16628819.0;2;10;;;;['pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])\n', 'replace', 'Timestamp', 'def replace(self, **kwds):\n    return Timestamp(datetime.replace(self, **kwds),\n                     offset=self.offset)\n', 'datetime.datetime', 'datetime.datetime.replace']
798;;3;17056022;16740887.0;2;9;;;;[]
799;;4;17063653;17063458.0;2;104;;;;"['ExcelFile', '.parse', '>>> xl = pd.ExcelFile(""dummydata.xlsx"")\n>>> xl.sheet_names\n[u\'Sheet1\', u\'Sheet2\', u\'Sheet3\']\n>>> df = xl.parse(""Sheet1"")\n>>> df.head()\n                  Tid  dummy1    dummy2    dummy3    dummy4    dummy5  \\\n0 2006-09-01 00:00:00       0  5.894611  0.605211  3.842871  8.265307   \n1 2006-09-01 01:00:00       0  5.712107  0.605211  3.416617  8.301360   \n2 2006-09-01 02:00:00       0  5.105300  0.605211  3.090865  8.335395   \n3 2006-09-01 03:00:00       0  4.098209  0.605211  3.198452  8.170187   \n4 2006-09-01 04:00:00       0  3.338196  0.605211  2.970015  7.765058   \n\n     dummy6  dummy7    dummy8    dummy9  \n0  0.623354       0  2.579108  2.681728  \n1  0.554211       0  7.210000  3.028614  \n2  0.567841       0  6.940000  3.644147  \n3  0.581470       0  6.630000  4.016155  \n4  0.595100       0  6.350000  3.974442  \n', '>>> parsed = pd.io.parsers.ExcelFile.parse(xl, ""Sheet1"")\n>>> parsed.columns\nIndex([u\'Tid\', u\'dummy1\', u\'dummy2\', u\'dummy3\', u\'dummy4\', u\'dummy5\', u\'dummy6\', u\'dummy7\', u\'dummy8\', u\'dummy9\'], dtype=object)\n']"
800;;14;17071908;17071871.0;2;755;;;;"['some_value', '==', ""df.loc[df['column_name'] == some_value]\n"", 'some_values', 'isin', ""df.loc[df['column_name'].isin(some_values)]\n"", '&', ""df.loc[(df['column_name'] == some_value) & df['other_column'].isin(some_values)]\n"", 'some_value', '!=', ""df.loc[df['column_name'] != some_value]\n"", 'isin', 'some_values', '~', ""df.loc[~df['column_name'].isin(some_values)]\n"", ""import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n                   'B': 'one one two three two two one three'.split(),\n                   'C': np.arange(8), 'D': np.arange(8) * 2})\nprint(df)\n#      A      B  C   D\n# 0  foo    one  0   0\n# 1  bar    one  1   2\n# 2  foo    two  2   4\n# 3  bar  three  3   6\n# 4  foo    two  4   8\n# 5  bar    two  5  10\n# 6  foo    one  6  12\n# 7  foo  three  7  14\n\nprint(df.loc[df['A'] == 'foo'])\n"", '     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n', 'isin', ""print(df.loc[df['B'].isin(['one','three'])])\n"", '     A      B  C   D\n0  foo    one  0   0\n1  bar    one  1   2\n3  bar  three  3   6\n6  foo    one  6  12\n7  foo  three  7  14\n', 'df.loc', ""df = df.set_index(['B'])\nprint(df.loc['one'])\n"", '       A  C   D\nB              \none  foo  0   0\none  bar  1   2\none  foo  6  12\n', 'df.index.isin', ""df.loc[df.index.isin(['one','two'])]\n"", '       A  C   D\nB              \none  foo  0   0\none  bar  1   2\ntwo  foo  2   4\ntwo  foo  4   8\ntwo  bar  5  10\none  foo  6  12\n']"
801;;0;17086321;17071871.0;2;9;;;;"['from pandas import DataFrame\n\n# Create data set\nd = {\'Revenue\':[100,111,222], \n     \'Cost\':[333,444,555]}\ndf = DataFrame(d)\n\n\n# mask = Return True when the value in column ""Revenue"" is equal to 111\nmask = df[\'Revenue\'] == 111\n\nprint mask\n\n# Result:\n# 0    False\n# 1     True\n# 2    False\n# Name: Revenue, dtype: bool\n\n\n# Select * FROM df WHERE Revenue = 111\ndf[mask]\n\n# Result:\n#    Cost    Revenue\n# 1  444     111\n']"
802;;11;17092113;17091769.0;2;47;;;;"[""df['y']"", '.loc', '.ix', 'y', ""In [7]: df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'])\n\nIn [8]: df.loc['y'] = pandas.Series({'a':1, 'b':5, 'c':2, 'd':3})\n\nIn [9]: df\nOut[9]: \n     a    b    c    d\nx  NaN  NaN  NaN  NaN\ny    1    5    2    3\nz  NaN  NaN  NaN  NaN\n""]"
803;;7;17095620;17095101.0;2;56;;;;"['In [21]: ne = (df1 != df2).any(1)\n\nIn [22]: ne\nOut[22]:\n0    False\n1     True\n2     True\ndtype: bool\n', ""In [23]: ne_stacked = (df1 != df2).stack()\n\nIn [24]: changed = ne_stacked[ne_stacked]\n\nIn [25]: changed.index.names = ['id', 'col']\n\nIn [26]: changed\nOut[26]:\nid  col\n1   score         True\n2   isEnrolled    True\n    Comment       True\ndtype: bool\n"", ""In [27]: difference_locations = np.where(df1 != df2)\n\nIn [28]: changed_from = df1.values[difference_locations]\n\nIn [29]: changed_to = df2.values[difference_locations]\n\nIn [30]: pd.DataFrame({'from': changed_from, 'to': changed_to}, index=changed.index)\nOut[30]:\n               from           to\nid col\n1  score       1.11         1.21\n2  isEnrolled  True        False\n   Comment     None  On vacation\n"", 'df1', 'df2', 'df1.index & df2.index']"
804;;3;17096675;17095101.0;2;9;;;;"[""import pandas as pd\nimport io\n\ntexts = ['''\\\nid   Name   score                    isEnrolled                        Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.11                     False                           Graduated\n113  Zoe    4.12                     True       ''',\n\n         '''\\\nid   Name   score                    isEnrolled                        Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.21                     False                           Graduated\n113  Zoe    4.12                     False                         On vacation''']\n\n\ndf1 = pd.read_fwf(io.BytesIO(texts[0]), widths=[5,7,25,21,20])\ndf2 = pd.read_fwf(io.BytesIO(texts[1]), widths=[5,7,25,21,20])\ndf = pd.concat([df1,df2]) \n\nprint(df)\n#     id  Name  score isEnrolled               Comment\n# 0  111  Jack   2.17       True  He was late to class\n# 1  112  Nick   1.11      False             Graduated\n# 2  113   Zoe   4.12       True                   NaN\n# 0  111  Jack   2.17       True  He was late to class\n# 1  112  Nick   1.21      False             Graduated\n# 2  113   Zoe   4.12      False           On vacation\n\ndf.set_index(['id', 'Name'], inplace=True)\nprint(df)\n#           score isEnrolled               Comment\n# id  Name                                        \n# 111 Jack   2.17       True  He was late to class\n# 112 Nick   1.11      False             Graduated\n# 113 Zoe    4.12       True                   NaN\n# 111 Jack   2.17       True  He was late to class\n# 112 Nick   1.21      False             Graduated\n# 113 Zoe    4.12      False           On vacation\n\ndef report_diff(x):\n    return x[0] if x[0] == x[1] else '{} | {}'.format(*x)\n\nchanges = df.groupby(level=['id', 'Name']).agg(report_diff)\nprint(changes)\n"", '                score    isEnrolled               Comment\nid  Name                                                 \n111 Jack         2.17          True  He was late to class\n112 Nick  1.11 | 1.21         False             Graduated\n113 Zoe          4.12  True | False     nan | On vacation\n']"
805;;8;17097397;17097236.0;2;36;;;;"['df.replace(\'-\', None)\nTypeError: If ""to_replace"" and ""value"" are both None then regex must be a mapping\n', ""In [11]: df.replace('-', df.replace(['-'], [None]) # or .replace('-', {0: None})\nOut[11]:\n      0\n0  None\n1     3\n2     2\n3     5\n4     1\n5    -5\n6    -1\n7  None\n8     9\n"", ""In [12]: df.replace('-', np.nan)\nOut[12]:\n     0\n0  NaN\n1    3\n2    2\n3    5\n4    1\n5   -5\n6   -1\n7  NaN\n8    9\n""]"
806;;2;17097777;17097643.0;2;31;;;;"['~df[""col""].str.contains(word)\n']"
807;;2;17098736;17098654.0;2;170;;;;"['to_pickle', 'df.to_pickle(file_name)  # where to save it, usually as a .pkl\n', 'df = pd.read_pickle(file_name)\n', 'save', 'load', 'to_pickle', 'read_pickle', ""store = HDFStore('store.h5')\n\nstore['df'] = df  # save it\nstore['df']  # load it\n""]"
808;;0;17098885;17098654.0;2;22;;;;['pandas.read_csv()', 'pandas.read_csv(..., nrows=1000)', 'DataFrame.to_feather()', 'pd.read_feather()', 'pandas.to_pickle()']
809;;6;17116976;17116814.0;2;145;;;;"[""In [43]: df\nOut[43]: \n   CustNum     CustomerName  ItemQty Item                 Seatblocks  ItemExt\n0    32363  McCartney, Paul        3  F04               2:218:10:4,6       60\n1    31316     Lennon, John       25  F01  1:13:36:1,12 1:13:37:1,13      300\n\nIn [44]: s = df['Seatblocks'].str.split(' ').apply(Series, 1).stack()\n\nIn [45]: s.index = s.index.droplevel(-1) # to line up with df's index\n\nIn [46]: s.name = 'Seatblocks' # needs a name to join\n\nIn [47]: s\nOut[47]: \n0    2:218:10:4,6\n1    1:13:36:1,12\n1    1:13:37:1,13\nName: Seatblocks, dtype: object\n\nIn [48]: del df['Seatblocks']\n\nIn [49]: df.join(s)\nOut[49]: \n   CustNum     CustomerName  ItemQty Item  ItemExt    Seatblocks\n0    32363  McCartney, Paul        3  F04       60  2:218:10:4,6\n1    31316     Lennon, John       25  F01      300  1:13:36:1,12\n1    31316     Lennon, John       25  F01      300  1:13:37:1,13\n"", ""In [50]: df.join(s.apply(lambda x: Series(x.split(':'))))\nOut[50]: \n   CustNum     CustomerName  ItemQty Item  ItemExt  0    1   2     3\n0    32363  McCartney, Paul        3  F04       60  2  218  10   4,6\n1    31316     Lennon, John       25  F01      300  1   13  36  1,12\n1    31316     Lennon, John       25  F01      300  1   13  37  1,13\n""]"
810;;12;17134750;17134716.0;2;159;;;;"['to_datetime', ""df['col'] = pd.to_datetime(df['col'])\n"", 'dayfirst', ""In [11]: pd.to_datetime(pd.Series(['05/23/2005']))\nOut[11]:\n0   2005-05-23 00:00:00\ndtype: datetime64[ns]\n"", 'In [12]: pd.to_datetime(pd.Series([\'05/23/2005\']), format=""%m/%d/%Y"")\nOut[12]:\n0   2005-05-23\ndtype: datetime64[ns]\n']"
811;;6;17141755;17141558.0;2;126;;;;"['sort', 'sort_values', 'sort', ""df.sort_values(['a', 'b'], ascending=[True, False])\n"", 'sort', ""df.sort(['a', 'b'], ascending=[True, False])\n"", ""In [11]: df1 = pd.DataFrame(np.random.randint(1, 5, (10,2)), columns=['a','b'])\n\nIn [12]: df1.sort(['a', 'b'], ascending=[True, False])\nOut[12]:\n   a  b\n2  1  4\n7  1  3\n1  1  2\n3  1  2\n4  3  2\n6  4  4\n0  4  3\n9  4  3\n5  4  1\n8  4  1\n"", ""df1 = df1.sort(['a', 'b'], ascending=[True, False])\n"", ""df1.sort(['a', 'b'], ascending=[True, False], inplace=True)\n""]"
812;;2;17142595;17142304.0;2;49;;;;"[""In [126]: df.replace(['very bad', 'bad', 'poor', 'good', 'very good'], \n                     [1, 2, 3, 4, 5]) \nOut[126]: \n      resp  A  B  C\n   0     1  3  3  4\n   1     2  4  3  4\n   2     3  5  5  5\n   3     4  2  3  2\n   4     5  1  1  1\n   5     6  3  4  1\n   6     7  4  4  4\n   7     8  5  5  5\n   8     9  2  2  1\n   9    10  1  1  1\n""]"
813;;5;17171819;12190874.0;2;75;;;;['np.random.choice()', 'rows = np.random.choice(df.index.values, 10)\nsampled_df = df.ix[rows]\n']
814;;4;17241104;17241004.0;2;33;;;;['df.index', 'df.index.tolist()']
815;;2;17242374;17241004.0;2;132;;;;"['values', ""In [1]: df = pd.DataFrame(index=['a', 'b'])\n\nIn [2]: df.index.values\nOut[2]: array(['a', 'b'], dtype=object)\n""]"
816;;4;17302673;14734533.0;2;43;;;;"['groups = dict(list(gb))\n', ""groups['foo']\n"", '     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n']"
817;;3;17383140;17383094.0;2;13;;;;['True', '1', 'False', '0', '>>> True == 1\nTrue\n>>> False == 0\nTrue\n', '>>> issubclass(bool, int)\nTrue\n>>> True * 5\n5\n', 'is', 'True', '1']
818;;0;17383325;17383094.0;2;12;;;;['In [104]: df = DataFrame(dict(A = True, B = False),index=range(3))\n\nIn [105]: df\nOut[105]: \n      A      B\n0  True  False\n1  True  False\n2  True  False\n\nIn [106]: df.dtypes\nOut[106]: \nA    bool\nB    bool\ndtype: object\n\nIn [107]: df.astype(int)\nOut[107]: \n   A  B\n0  1  0\n1  1  0\n2  1  0\n\nIn [108]: df.astype(int).dtypes\nOut[108]: \nA    int64\nB    int64\ndtype: object\n']
819;;0;17439693;17438906.0;2;39;;;;"['>>> df\n              val1  val2  val3\ncity_id                       \nhouston,tx       1     2     0\nhouston,tx       0     0     1\nhouston,tx       2     1     1\nsomewhere,ew     4     3     7\n', '>>> df.groupby(df.index).sum()\n              val1  val2  val3\ncity_id                       \nhouston,tx       3     3     2\nsomewhere,ew     4     3     7\n', '>>> df.reset_index().groupby(""city_id"").sum()\n              val1  val2  val3\ncity_id                       \nhouston,tx       3     3     2\nsomewhere,ew     4     3     7\n', 'city_id', 'groupby', 'city_id', 'DataFrameGroupBy', '>>> df.groupby(df.index)\n<pandas.core.groupby.DataFrameGroupBy object at 0x1045a1790>\n>>> df.groupby(df.index).max()\n              val1  val2  val3\ncity_id                       \nhouston,tx       2     2     1\nsomewhere,ew     4     3     7\n>>> df.groupby(df.index).mean()\n              val1  val2      val3\ncity_id                           \nhouston,tx       1     1  0.666667\nsomewhere,ew     4     3  7.000000\n']"
820;;2;17468012;17465045.0;2;110;;;;"['parse_dates=True', ""parse_dates=['column name']"", ""dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n\ndf = pd.read_csv(infile, parse_dates=['datetime'], date_parser=dateparse)\n"", ""dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n\ndf = pd.read_csv(infile, parse_dates={'datetime': ['date', 'time']}, date_parser=dateparse)\n""]"
821;;3;17468154;17465045.0;2;10;;;;['parse_dates : boolean, list of ints or names, list of lists, or dict\nIf True -> try parsing the index. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a\nseparate date column. If [[1, 3]] -> combine columns 1 and 3 and parse as a single date\ncolumn. {\x91foo\x92 : [1, 3]} -> parse columns 1, 3 as date and call result \x91foo\x92\n', 'date_parser : function\nFunction to use for converting a sequence of string columns to an array of datetime\ninstances. The default uses dateutil.parser.parser to do the conversion.\n']
822;;0;17478495;17477979.0;2;116;;;;"['replace', 'df.replace([np.inf, -np.inf], np.nan)\n', 'dropna', 'df.replace([np.inf, -np.inf], np.nan).dropna(subset=[""col1"", ""col2""], how=""all"")\n', 'In [11]: df = pd.DataFrame([1, 2, np.inf, -np.inf])\n\nIn [12]: df.replace([np.inf, -np.inf], np.nan)\nOut[12]:\n    0\n0   1\n1   2\n2 NaN\n3 NaN\n']"
823;;6;17496530;10715965.0;2;139;;;;['rows_list = []\nfor row in input_rows:\n\n        dict1 = {}\n        # get input row in dictionary format\n        # key = col_name\n        dict1.update(blah..) \n\n        rows_list.append(dict1)\n\ndf = pd.DataFrame(rows_list)               \n']
824;;5;17531025;17530542.0;2;89;;;;"[""with open('my_csv.csv', 'a') as f:\n    df.to_csv(f, header=False)\n"", 'foo.csv', ',A,B,C\n0,1,2,3\n1,4,5,6\n', 'df + 6', ""In [1]: df = pd.read_csv('foo.csv', index_col=0)\n\nIn [2]: df\nOut[2]:\n   A  B  C\n0  1  2  3\n1  4  5  6\n\nIn [3]: df + 6\nOut[3]:\n    A   B   C\n0   7   8   9\n1  10  11  12\n\nIn [4]: with open('foo.csv', 'a') as f:\n             (df + 6).to_csv(f, header=False)\n"", 'foo.csv', ',A,B,C\n0,1,2,3\n1,4,5,6\n0,7,8,9\n1,10,11,12\n']"
825;;7;17534256;17534106.0;2;7;;;;['NaN', 'None', 'NaN', 'None', 'NoneType', 'None', 'None', 'None', 'nansum', 'nan_to_num']
826;;2;17534682;17534106.0;2;41;;;;"['isnull', 'notnull', ""#  without forcing dtype it changes None to NaN!\ns_bad = pd.Series([1, None], dtype=object)\ns_good = pd.Series([1, np.nan])\n\nIn [13]: s_bad.dtype\nOut[13]: dtype('O')\n\nIn [14]: s_good.dtype\nOut[14]: dtype('float64')\n"", 'np.nan', 'None', 'In [15]: s_bad.sum()\nOut[15]: 1\n\nIn [16]: s_good.sum()\nOut[16]: 1.0\n', 'pd.isnull', 'pd.notnull']"
827;;0;17619032;17618981.0;2;27;;;;"["">>> import pandas\n>>> df = pandas.DataFrame([{'c1':3,'c2':10},{'c1':2, 'c2':30},{'c1':1,'c2':20},{'c1':2,'c2':15},{'c1':2,'c2':100}])\n>>> df.sort(['c1','c2'], ascending=[False,True])\n   c1   c2\n0   3   10\n3   2   15\n1   2   30\n4   2  100\n2   1   20\n"", "">>> df.sort(['c1','c2'], ascending=[True,True])\n   c1   c2\n2   1   20\n3   2   15\n1   2   30\n4   2  100\n0   3   10\n"", 'DataFrame.sort', 'DataFrame.sort_values', "">>> df.sort(['c1','c2'], ascending=[False,True])\n__main__:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n   c1   c2\n0   3   10\n3   2   15\n1   2   30\n4   2  100\n2   1   20\n>>> df.sort_values(['c1','c2'], ascending=[False,True])\n   c1   c2\n0   3   10\n3   2   15\n1   2   30\n4   2  100\n2   1   20\n""]"
828;;1;17679517;17679089.0;2;43;;;;"['size', ""In [11]: df.groupby(['col5', 'col2']).size()\nOut[11]:\ncol5  col2\n1     A       1\n      D       3\n2     B       2\n3     A       3\n      C       1\n4     B       1\n5     B       2\n6     B       1\ndtype: int64\n"", ""In [12]: df.groupby(['col5', 'col2']).size().groupby(level=1).max()\nOut[12]:\ncol2\nA       3\nB       2\nC       1\nD       3\ndtype: int64\n""]"
829;;1;17679980;17679089.0;2;29;;;;"[""In [56]: df.groupby(['col5','col2']).size().reset_index().groupby('col2')[[0]].max()\nOut[56]: \n      0\ncol2   \nA     3\nB     2\nC     1\nD     3\n""]"
830;;4;17682662;17682613.0;2;70;;;;"[""In [79]: df[df.c > 0.5][['b', 'e']].values\nOut[79]: \narray([[ 0.98836259,  0.82403141],\n       [ 0.337358  ,  0.02054435],\n       [ 0.29271728,  0.37813099],\n       [ 0.70033513,  0.69919695]])\n""]"
831;;0;17682665;17682613.0;2;16;;;;"["">>> df = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\n>>> df[df['c']>.5][['b','e']]\n          b         e\n1  0.071146  0.132145\n2  0.495152  0.420219\n"", "">>> df[df['c']>.5][['b','e']].values\narray([[ 0.07114556,  0.13214495],\n       [ 0.49515157,  0.42021946]])\n""]"
832;;2;17682726;17682613.0;2;7;;;;"['.loc', '.ix/.iloc', ""In [1]: df = DataFrame(np.random.rand(4,5), columns = list('abcde'))\n\nIn [2]: df\nOut[2]: \n          a         b         c         d         e\n0  0.669701  0.780497  0.955690  0.451573  0.232194\n1  0.952762  0.585579  0.890801  0.643251  0.556220\n2  0.900713  0.790938  0.952628  0.505775  0.582365\n3  0.994205  0.330560  0.286694  0.125061  0.575153\n\nIn [5]: df.loc[df['c']>0.5,['a','d']]\nOut[5]: \n          a         d\n0  0.669701  0.451573\n1  0.952762  0.643251\n2  0.900713  0.505775\n"", ""In [6]: df.loc[df['c']>0.5,['a','d']].values\nOut[6]: \narray([[ 0.66970138,  0.45157274],\n       [ 0.95276167,  0.64325143],\n       [ 0.90071271,  0.50577509]])\n""]"
833;;13;17690795;17690738.0;2;41;;;;"["">>> import pandas as pd\n>>> date_stngs = ('2008-12-20','2008-12-21','2008-12-22','2008-12-23')\n>>> a = pd.Series([pd.to_datetime(date) for date in date_stngs])\n>>> a\n0    2008-12-20 00:00:00\n1    2008-12-21 00:00:00\n2    2008-12-22 00:00:00\n3    2008-12-23 00:00:00\n"", '>>> pd.to_datetime(pd.Series(date_stngs))\n0   2008-12-20 00:00:00\n1   2008-12-21 00:00:00\n2   2008-12-22 00:00:00\n3   2008-12-23 00:00:00\n']"
834;;3;17690868;17690738.0;2;33;;;;['In [46]: pd.to_datetime(pd.Series(date_stngs))\nOut[46]: \n0   2008-12-20 00:00:00\n1   2008-12-21 00:00:00\n2   2008-12-22 00:00:00\n3   2008-12-23 00:00:00\ndtype: datetime64[ns]\n', 'In [43]: dates = [(dt.datetime(1960, 1, 1)+dt.timedelta(days=i)).date().isoformat() for i in range(20000)]\n\nIn [44]: timeit pd.Series([pd.to_datetime(date) for date in dates])\n1 loops, best of 3: 1.71 s per loop\n\nIn [45]: timeit pd.to_datetime(pd.Series(dates))\n100 loops, best of 3: 5.71 ms per loop\n']
835;;2;17692156;17691447.0;2;31;;;;['In [14]: df.stack().value_counts()\nOut[14]: \n192.248.8.183    3   \n192.168.2.85     3   \n66.249.74.52     2   \n192.168.2.161    2   \n124.43.113.22    1   \ndtype: int64\n']
836;;0;17811984;17326973.0;2;13;;;;"[""writer.sheets['Summary'].column_dimensions['A'].width = 15\n""]"
837;;0;17813222;17812978.0;2;35;;;;"['style', 'df.plot', ""df.plot(x='col_name_1', y='col_name_2', style='o')\n"", 'style', 'dict', 'list', ""import numpy as np\nimport pandas as pd\n\nd = {'one' : np.random.rand(10),\n     'two' : np.random.rand(10)}\n\ndf = pd.DataFrame(d)\n\ndf.plot(style=['o','rx'])\n"", 'matplotlib.pyplot.plot']"
838;;1;17813277;17812978.0;2;29;;;;"[""import matplotlib.pyplot as plt\nplt.scatter(df['col_name_1'], df['col_name_2'])\nplt.show() # Depending on whether you use IPython or interactive mode, etc.\n"", 'df.col_name_1.values', 'datetime64']"
839;;5;17819427;17818783.0;2;22;;;;"[""In [37]: col = np.array([0,0,1,2,2,2])\n\nIn [38]: data = np.array([1,2,3,4,5,6],dtype='float64')\n\nIn [39]: m = csc_matrix( (data,(row,col)), shape=(3,3) )\n\nIn [40]: m\nOut[40]: \n<3x3 sparse matrix of type '<type 'numpy.float64'>'\n        with 6 stored elements in Compressed Sparse Column format>\n\nIn [46]: pd.SparseDataFrame([ pd.SparseSeries(m[i].toarray().ravel()) \n                              for i in np.arange(m.shape[0]) ])\nOut[46]: \n   0  1  2\n0  1  0  4\n1  0  0  5\n2  2  3  6\n\nIn [47]: df = pd.SparseDataFrame([ pd.SparseSeries(m[i].toarray().ravel()) \n                                   for i in np.arange(m.shape[0]) ])\n\nIn [48]: type(df)\nOut[48]: pandas.sparse.frame.SparseDataFrame\n""]"
840;;3;17840195;17839973.0;2;111;;;;"["">>> df = pd.DataFrame({'A': [a], 'B': [b]})\n>>> df\n   A  B\n0  2  3\n"", "">>> df = pd.DataFrame({'A': a, 'B': b}, index=[0])\n>>> df\n   A  B\n0  2  3\n""]"
841;;0;17840197;17839973.0;2;6;;;;"[""df2 = pd.DataFrame({'A':[a],'B':[b]})\n""]"
842;;10;17841294;17841149.0;2;70;;;;"[""In [4]: df = read_csv(StringIO(data),sep='\\s+')\n\nIn [5]: df\nOut[5]: \n   A         B       C\n0  1  0.749065    This\n1  2  0.301084      is\n2  3  0.463468       a\n3  4  0.643961  random\n4  1  0.866521  string\n5  2  0.120737       !\n\nIn [6]: df.dtypes\nOut[6]: \nA      int64\nB    float64\nC     object\ndtype: object\n"", '.sum()', ""In [8]: df.groupby('A').apply(lambda x: x.sum())\nOut[8]: \n   A         B           C\nA                         \n1  2  1.615586  Thisstring\n2  4  0.421821         is!\n3  3  0.463468           a\n4  4  0.643961      random\n"", ""In [9]: df.groupby('A')['C'].apply(lambda x: x.sum())\nOut[9]: \nA\n1    Thisstring\n2           is!\n3             a\n4        random\ndtype: object\n"", 'In [11]: df.groupby(\'A\')[\'C\'].apply(lambda x: ""{%s}"" % \', \'.join(x))\nOut[11]: \nA\n1    {This, string}\n2           {is, !}\n3               {a}\n4          {random}\ndtype: object\n', 'def f(x):\n     return Series(dict(A = x[\'A\'].sum(), \n                        B = x[\'B\'].sum(), \n                        C = ""{%s}"" % \', \'.join(x[\'C\'])))\n\nIn [14]: df.groupby(\'A\').apply(f)\nOut[14]: \n   A         B               C\nA                             \n1  2  1.615586  {This, string}\n2  4  0.421821         {is, !}\n3  3  0.463468             {a}\n4  4  0.643961        {random}\n']"
843;;1;17841308;17841149.0;2;21;;;;"['apply', 'set', 'list', "">>> d\n   A       B\n0  1    This\n1  2      is\n2  3       a\n3  4  random\n4  1  string\n5  2       !\n>>> d.groupby('A')['B'].apply(list)\nA\n1    [This, string]\n2           [is, !]\n3               [a]\n4          [random]\ndtype: object\n"", 'apply']"
844;;3;17877159;17874063.0;2;25;;;;"['normed=True', 'np.sum(pdf * np.diff(bins))', ""x = np.random.randn(30)\n\nfig, ax = plt.subplots(1,2, figsize=(10,4))\n\nax[0].hist(x, normed=True, color='grey')\n\nhist, bins = np.histogram(x)\nax[1].bar(bins[:-1], hist.astype(np.float32) / hist.sum(), width=(bins[1]-bins[0]), color='grey')\n\nax[0].set_title('normed=True')\nax[1].set_title('hist = hist / hist.sum()')\n""]"
845;;6;17950081;16396903.0;2;69;;;;['df.drop(label)\n', 'df.drop(label, inplace=True)\n', 'df.drop(df.index[:3], inplace=True)\n', 'df.drop(df.head(3).index, inplace=True)\n']
846;;4;17950531;17950374.0;2;46;;;;"[""In [16]: df = DataFrame(np.arange(10).reshape(5,2),columns=list('AB'))\n\nIn [17]: df\nOut[17]: \n   A  B\n0  0  1\n1  2  3\n2  4  5\n3  6  7\n4  8  9\n\nIn [18]: df.dtypes\nOut[18]: \nA    int64\nB    int64\ndtype: object\n"", ""In [19]: df['A'].apply(str)\nOut[19]: \n0    0\n1    2\n2    4\n3    6\n4    8\nName: A, dtype: object\n\nIn [20]: df['A'].apply(str)[0]\nOut[20]: '0'\n"", ""In [21]: df.applymap(str)\nOut[21]: \n   A  B\n0  0  1\n1  2  3\n2  4  5\n3  6  7\n4  8  9\n\nIn [22]: df.applymap(str).iloc[0,0]\nOut[22]: '0'\n""]"
847;;2;17973255;17972938.0;2;39;;;;"['str.contains', ""In [11]: pattern = '|'.join(mylist)\n\nIn [12]: pattern\nOut[12]: 'dog|cat|fish'\n\nIn [13]: frame.a.str.contains(pattern)\nOut[13]:\n0     True\n1    False\n2     True\nName: a, dtype: bool\n""]"
848;;0;17975690;17530542.0;2;130;;;;[]
849;;7;17977609;17977540.0;2;60;;;;"['sheet_names', ""xl = pd.ExcelFile('foo.xls')\n\nxl.sheet_names  # see all sheet names\n\nxl.parse(sheet_name)  # read a specific sheet to DataFrame\n""]"
850;;3;17978188;17978092.0;2;49;;;;"['read_csv', ""parse_dates=[['Date', 'Time']]"", 'to_datetime', ""In [11]: df['Date'] + ' ' + df['Time']\nOut[11]:\n0    01-06-2013 23:00:00\n1    02-06-2013 01:00:00\n2    02-06-2013 21:00:00\n3    02-06-2013 22:00:00\n4    02-06-2013 23:00:00\n5    03-06-2013 01:00:00\n6    03-06-2013 21:00:00\n7    03-06-2013 22:00:00\n8    03-06-2013 23:00:00\n9    04-06-2013 01:00:00\ndtype: object\n\nIn [12]: pd.to_datetime(df['Date'] + ' ' + df['Time'])\nOut[12]:\n0   2013-01-06 23:00:00\n1   2013-02-06 01:00:00\n2   2013-02-06 21:00:00\n3   2013-02-06 22:00:00\n4   2013-02-06 23:00:00\n5   2013-03-06 01:00:00\n6   2013-03-06 21:00:00\n7   2013-03-06 22:00:00\n8   2013-03-06 23:00:00\n9   2013-04-06 01:00:00\ndtype: datetime64[ns]\n"", 'raise']"
851;;1;17978414;17978133.0;2;26;;;;"[""df2[list('xab')]  # df2 but only with columns x, a, and b\n\ndf1.merge(df2[list('xab')])\n""]"
852;;4;18023468;18022845.0;2;125;;;;"['name', ""In [7]: df.index.name\nOut[7]: 'Index Title'\n\nIn [8]: df.index.name = 'foo'\n\nIn [9]: df.index.name\nOut[9]: 'foo'\n\nIn [10]: df\nOut[10]: \n         Column 1\nfoo              \nApples          1\nOranges         2\nPuppies         3\nDucks           4\n""]"
853;;1;18023485;18022845.0;2;22;;;;['df.index.name', 'dir', 'dir(df.index)']
854;;1;18039175;18039057.0;2;17;;;;['data = pd.read_csv(path, skiprows=2)']
855;;1;18046682;16392921.0;2;42;;;;['   plot(a)\n   show()\n   plot(b)\n   show()\n']
856;;0;18062430;18062135.0;2;23;;;;['reset_index', 'In [2]: s1 = Series(randn(5),index=[1,2,4,5,6])\n\nIn [4]: s2 = Series(randn(5),index=[1,2,4,5,6])\n\nIn [8]: DataFrame(dict(s1 = s1, s2 = s2)).reset_index()\nOut[8]: \n   index        s1        s2\n0      1 -0.176143  0.128635\n1      2 -1.286470  0.908497\n2      4 -0.995881  0.528050\n3      5  0.402241  0.458870\n4      6  0.380457  0.072251\n']
857;;7;18062521;18062135.0;2;172;;;;"['concat', ""In [1]: s1 = pd.Series([1, 2], index=['A', 'B'], name='s1')\n\nIn [2]: s2 = pd.Series([3, 4], index=['A', 'B'], name='s2')\n\nIn [3]: pd.concat([s1, s2], axis=1)\nOut[3]:\n   s1  s2\nA   1   3\nB   2   4\n\nIn [4]: pd.concat([s1, s2], axis=1).reset_index()\nOut[4]:\n  index  s1  s2\n0     A   1   3\n1     B   2   4\n""]"
858;;10;18079695;18079563.0;2;24;;;;['pd.Series(list(set(s1).intersection(set(s2))))\n', 'Series(list(set(s1) & set(s2)))\n']
859;;5;18080142;18079563.0;2;8;;;;['intersect1d', 'pd.Series(np.intersect1d(pd.Series([1,2,3,5,42]), pd.Series([4,5,6,20,42])))\n']
860;;4;18090009;18089667.0;2;8;;;;['dtype', 'numpy', 'nbytes', 'DataFrame', 'nbytes = sum(block.values.nbytes for block in df.blocks.values())\n', 'object', 'PyObject', 'read_csv', 'object', 'numpy', 'object', 'dtype', 'list']
861;;7;18090393;18089667.0;2;23;;;;"[""In [4]: DataFrame(randn(1000000,20)).to_csv('test.csv')\n\nIn [5]: !ls -ltr test.csv\n-rw-rw-r-- 1 users 399508276 Aug  6 16:55 test.csv\n"", 'In [16]: df.values.nbytes + df.index.nbytes + df.columns.nbytes\nOut[16]: 168000160\n', ""DataFrame(randn(1000000,20)).to_hdf('test.h5','df')\n\n!ls -ltr test.h5\n-rw-rw-r-- 1 users 168073944 Aug  6 16:57 test.h5\n"", ""In [12]: DataFrame(randn(1000000,20)).to_hdf('test.h5','df',complevel=9,complib='blosc')\n\nIn [13]: !ls -ltr test.h5\n-rw-rw-r-- 1 users 154727012 Aug  6 16:58 test.h5\n""]"
862;;5;18129082;18039057.0;2;127;;;;"[""data = pd.read_csv('file1.csv', error_bad_lines=False)\n""]"
863;;14;18145399;13411544.0;2;1202;;;;"[""df = df.drop('column_name', 1)\n"", '1', '0', '1', 'df', ""df.drop('column_name', axis=1, inplace=True)\n"", 'df.drop(df.columns[[0, 1, 3]], axis=1)  # df.columns is zero-based pd.Index \n']"
864;;3;18172249;18171739.0;2;155;;;;"['read_csv', 'encoding', 'read_csv(\'file\', encoding = ""ISO-8859-1"")', 'encoding = utf8', 'utf-8', 'to_csv', ""'latin1'"", ""'ISO-8859-1'""]"
865;;9;18173074;18172851.0;2;352;;;;['df = df[df.line_race != 0]\n']
866;;5;18173088;18172851.0;2;28;;;;"['In [56]: df\nOut[56]:\n     line_date  daysago  line_race  rating    raw  wrating\n0   2007-03-31       62         11      56  1.000   56.000\n1   2007-03-10       83         11      67  1.000   67.000\n2   2007-02-10      111          9      66  1.000   66.000\n3   2007-01-13      139         10      83  0.881   73.096\n4   2006-12-23      160         10      88  0.793   69.787\n5   2006-11-09      204          9      52  0.637   33.106\n6   2006-10-22      222          8      66  0.582   38.408\n7   2006-09-29      245          9      70  0.519   36.318\n8   2006-09-16      258         11      68  0.486   33.063\n9   2006-08-30      275          8      72  0.447   32.160\n10  2006-02-11      475          5      65  0.165   10.698\n11  2006-01-13      504          0      70  0.142    9.969\n12  2006-01-02      515          0      64  0.135    8.627\n13  2005-12-06      542          0      70  0.118    8.246\n14  2005-11-29      549          0      70  0.114    7.963\n15  2005-11-22      556          0      -1  0.110   -0.110\n16  2005-11-01      577          0      -1  0.099   -0.099\n17  2005-10-20      589          0      -1  0.093   -0.093\n18  2005-09-27      612          0      -1  0.083   -0.083\n19  2005-09-07      632          0      -1  0.075   -0.075\n20  2005-06-12      719          0      69  0.049    3.360\n21  2005-05-29      733          0      -1  0.045   -0.045\n22  2005-05-02      760          0      -1  0.040   -0.040\n23  2005-04-02      790          0      -1  0.034   -0.034\n24  2005-03-13      810          0      -1  0.031   -0.031\n25  2004-11-09      934          0      -1  0.017   -0.017\n\nIn [57]: df[df.line_race != 0]\nOut[57]:\n     line_date  daysago  line_race  rating    raw  wrating\n0   2007-03-31       62         11      56  1.000   56.000\n1   2007-03-10       83         11      67  1.000   67.000\n2   2007-02-10      111          9      66  1.000   66.000\n3   2007-01-13      139         10      83  0.881   73.096\n4   2006-12-23      160         10      88  0.793   69.787\n5   2006-11-09      204          9      52  0.637   33.106\n6   2006-10-22      222          8      66  0.582   38.408\n7   2006-09-29      245          9      70  0.519   36.318\n8   2006-09-16      258         11      68  0.486   33.063\n9   2006-08-30      275          8      72  0.447   32.160\n10  2006-02-11      475          5      65  0.165   10.698\n', ""df.query('line_race != 0')""]"
867;;0;18196299;18196203.0;2;47;;;;"[""df.loc[df['line_race'] == 0, 'rating'] = 0\n""]"
868;;4;18215499;18215317.0;2;67;;;;"["">>> x = np.timedelta64(2069211000000000, 'ns')\n>>> days = x.astype('timedelta64[D]')\n>>> days / np.timedelta64(1, 'D')\n23\n"", 'days.astype(int)', 'timedelta', ""'D'"", ""'ns'""]"
869;;3;18261958;13167391.0;2;28;;;;['>>> grouped.filter(lambda x: len(x) > 1)\n\n     A  B\n0  foo  0\n2  foo  2\n3  foo  3\n']
870;;4;18316830;18316211.0;2;27;;;;"['apply', ""In [27]: s.apply(lambda x: type(x))\nOut[27]: \na  b\n1  2    <type 'numpy.float64'>\n3  6    <type 'numpy.float64'>\n4  4    <type 'numpy.float64'>\n"", 'Series(s.reset_index().apply(f, axis=1).values, index=s.index)\n', 's.get_level_values', 's.iterrows()', 'f']"
871;;0;18317067;15943769.0;2;56;;;;['len(df)', '__len__()', 'Returns length of index', 'In [7]: timeit len(df.index)\n1000000 loops, best of 3: 248 ns per loop\n\nIn [8]: timeit len(df)\n1000000 loops, best of 3: 573 ns per loop\n', 'len(df.index)']
872;;1;18327852;18327624.0;2;57;;;;['>>> myseries[myseries == 7]\n3    7\ndtype: int64\n>>> myseries[myseries == 7].index[0]\n3\n']
873;;3;18334025;18327624.0;2;16;;;;['get_loc', 'In [1]: myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])\n\nIn [3]: Index(myseries).get_loc(7)\nOut[3]: 3\n\nIn [4]: Index(myseries).get_loc(10)\nKeyError: 10\n', 'In [5]: Index([1,1,2,2,3,4]).get_loc(2)\nOut[5]: slice(2, 4, None)\n', 'In [6]: Index([1,1,2,1,3,2,4]).get_loc(2)\nOut[6]: array([False, False,  True, False, False,  True, False], dtype=bool)\n', 'In [7]: s = Series(randint(0,10,10000))\n\nIn [9]: %timeit s[s == 5]\n1000 loops, best of 3: 203 s per loop\n\nIn [12]: i = Index(s)\n\nIn [13]: %timeit i.get_loc(5)\n1000 loops, best of 3: 226 s per loop\n', 'is_unique', 'In [2]: s = Series(randint(0,10,10000))\n\nIn [3]: %timeit Index(s)\n100000 loops, best of 3: 9.6 s per loop\n\nIn [4]: %timeit Index(s).is_unique\n10000 loops, best of 3: 140 s per loop\n']
874;;1;18360223;18358938.0;2;53;;;;"['index', 'list', 'list', 'tuple', 'Index', 'MultiIndex', ""df.index.values.tolist()  # an ndarray method, you probably shouldn't depend on this\n"", 'list(df.index.values)  # this will always work in pandas\n']"
875;;0;18369312;17709641.0;2;57;;;;[]
876;;10;18431417;18429491.0;2;43;;;;"[""In [11]: df.fillna(-1)\nOut[11]: \n   a   b\n0  1   4\n1  2  -1\n2  3   6\n\nIn [12]: df.fillna(-1).groupby('b').sum()\nOut[12]: \n    a\nb    \n-1  2\n4   1\n6   3\n""]"
877;;5;18467097;15771472.0;2;30;;;;"[""from pandas import Series, DataFrame\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport numpy as np\n\ndef rolling_mean(data, window, min_periods=1, center=False):\n    ''' Function that computes a rolling mean\n\n    Parameters\n    ----------\n    data : DataFrame or Series\n           If a DataFrame is passed, the rolling_mean is computed for all columns.\n    window : int or string\n             If int is passed, window is the number of observations used for calculating \n             the statistic, as defined by the function pd.rolling_mean()\n             If a string is passed, it must be a frequency string, e.g. '90S'. This is\n             internally converted into a DateOffset object, representing the window size.\n    min_periods : int\n                  Minimum number of observations in window required to have a value.\n\n    Returns\n    -------\n    Series or DataFrame, if more than one column    \n    '''\n    def f(x):\n        '''Function to apply that actually computes the rolling mean'''\n        if center == False:\n            dslice = col[x-pd.datetools.to_offset(window).delta+timedelta(0,0,1):x]\n                # adding a microsecond because when slicing with labels start and endpoint\n                # are inclusive\n        else:\n            dslice = col[x-pd.datetools.to_offset(window).delta/2+timedelta(0,0,1):\n                         x+pd.datetools.to_offset(window).delta/2]\n        if dslice.size < min_periods:\n            return np.nan\n        else:\n            return dslice.mean()\n\n    data = DataFrame(data.copy())\n    dfout = DataFrame()\n    if isinstance(window, int):\n        dfout = pd.rolling_mean(data, window, min_periods=min_periods, center=center)\n    elif isinstance(window, basestring):\n        idx = Series(data.index.to_pydatetime(), index=data.index)\n        for colname, col in data.iterkv():\n            result = idx.apply(f)\n            result.name = colname\n            dfout = dfout.join(result, how='outer')\n    if dfout.columns.size == 1:\n        dfout = dfout.ix[:,0]\n    return dfout\n\n\n# Example\nidx = [datetime(2011, 2, 7, 0, 0),\n       datetime(2011, 2, 7, 0, 1),\n       datetime(2011, 2, 7, 0, 1, 30),\n       datetime(2011, 2, 7, 0, 2),\n       datetime(2011, 2, 7, 0, 4),\n       datetime(2011, 2, 7, 0, 5),\n       datetime(2011, 2, 7, 0, 5, 10),\n       datetime(2011, 2, 7, 0, 6),\n       datetime(2011, 2, 7, 0, 8),\n       datetime(2011, 2, 7, 0, 9)]\nidx = pd.Index(idx)\nvals = np.arange(len(idx)).astype(float)\ns = Series(vals, index=idx)\nrm = rolling_mean(s, window='2min')\n""]"
878;;2;18500854;10591000.0;2;22;;;;"[""d = pandas.read_csv('foo.csv', dtype={'BAR': 'S10'})\n""]"
879;;3;18505101;18504967.0;2;32;;;;"[""df['A_perc'] = df['A']/df['sum']\n"", 'div', ""ds.div(ds['sum'], axis=0)\n"", "">>> ds.join(ds.div(ds['sum'], axis=0), rsuffix='_perc')\n          A         B         C         D       sum    A_perc    B_perc  \\\n1  0.151722  0.935917  1.033526  0.941962  3.063127  0.049532  0.305543   \n2  0.033761  1.087302  1.110695  1.401260  3.633017  0.009293  0.299283   \n3  0.761368  0.484268  0.026837  1.276130  2.548603  0.298739  0.190013   \n\n     C_perc    D_perc  sum_perc  \n1  0.337409  0.307517         1  \n2  0.305722  0.385701         1  \n3  0.010530  0.500718         1  \n""]"
880;;3;18527067;11615504.0;2;9;;;;"['format = ""%Y%m%d %H""\ntimes = pd.to_datetime(df.YYYYMMDD + \' \' + df.HH, format=format)\ndf.set_index(times, inplace=True)\n# and maybe for cleanup\ndf = df.drop([\'YYYYMMDD\',\'HH\'], axis=1)\n']"
881;;1;18554949;18554920.0;2;69;;;;"['>>> df\n         date  duration user_id\n0  2013-04-01        30    0001\n1  2013-04-01        15    0001\n2  2013-04-01        20    0002\n3  2013-04-02        15    0002\n4  2013-04-02        30    0002\n>>> df.groupby(""date"").agg({""duration"": np.sum, ""user_id"": pd.Series.nunique})\n            duration  user_id\ndate                         \n2013-04-01        65        2\n2013-04-02        45        1\n>>> df.groupby(""date"").agg({""duration"": np.sum, ""user_id"": lambda x: x.nunique()})\n            duration  user_id\ndate                         \n2013-04-01        65        2\n2013-04-02        45        1\n']"
882;;0;18594595;18594469.0;2;60;;;;['div', 'df.div(df.sum(axis=1), axis=0)\n']
883;;1;18674915;18674064.0;2;117;;;;['df.insert(idx, col_name, value)\n']
884;;4;18689589;18689512.0;2;10;;;;[' if val.dtype == float and np.isnan(val):\n']
885;;0;18689712;18689512.0;2;100;;;;"['pandas.isnull()', ""import pandas as pd\nimport numpy as np\ns = pd.Series(['apple', np.nan, 'banana'])\npd.isnull(s)\nOut[9]: \n0    False\n1     True\n2    False\ndtype: bool\n"", 'numpy.nan', 'pandas', 'pandas', 'pd.NaT', ""In [24]: s = Series([Timestamp('20130101'),np.nan,Timestamp('20130102 9:30')],dtype='M8[ns]')\n\nIn [25]: s\nOut[25]: \n0   2013-01-01 00:00:00\n1                   NaT\n2   2013-01-02 09:30:00\ndtype: datetime64[ns]``\n\nIn [26]: pd.isnull(s)\nOut[26]: \n0    False\n1     True\n2    False\ndtype: bool\n""]"
886;;0;18691949;18689823.0;2;58;;;;['DataFrame.fillna', 'nan', 'In [27]: df \nOut[27]: \n          A         B         C\n0 -0.166919  0.979728 -0.632955\n1 -0.297953 -0.912674 -1.365463\n2 -0.120211 -0.540679 -0.680481\n3       NaN -2.027325  1.533582\n4       NaN       NaN  0.461821\n5 -0.788073       NaN       NaN\n6 -0.916080 -0.612343       NaN\n7 -0.887858  1.033826       NaN\n8  1.948430  1.025011 -2.982224\n9  0.019698 -0.795876 -0.046431\n\nIn [28]: df.mean()\nOut[28]: \nA   -0.151121\nB   -0.231291\nC   -0.530307\ndtype: float64\n\nIn [29]: df.fillna(df.mean())\nOut[29]: \n          A         B         C\n0 -0.166919  0.979728 -0.632955\n1 -0.297953 -0.912674 -1.365463\n2 -0.120211 -0.540679 -0.680481\n3 -0.151121 -2.027325  1.533582\n4 -0.151121 -0.231291  0.461821\n5 -0.788073 -0.231291 -0.530307\n6 -0.916080 -0.612343 -0.530307\n7 -0.887858  1.033826 -0.530307\n8  1.948430  1.025011 -2.982224\n9  0.019698 -0.795876 -0.046431\n', 'fillna', 'value', 'Series', 'df.mean().to_dict()']
887;;2;18695700;18695605.0;2;74;;;;"['to_dict', ""df.set_index('id').to_dict()\n"", 'Series.to_dict()', ""df.set_index('id')['value'].to_dict()\n""]"
888;;3;18793067;18792918.0;2;15;;;;"['lsuffix', 'rsuffix', 'restaurant_review_frame.join(restaurant_ids_dataframe, on=\'business_id\', how=\'left\', lsuffix=""_review"")\n', 'restaurant_ids_dataframe', 'restaurant_review_frame', ""del restaurant_ids_dataframe['stars']""]"
889;;2;18799713;18792918.0;2;50;;;;"[""import pandas as pd\npd.merge(restaurant_ids_dataframe, restaurant_review_frame, on='business_id', how='outer')\n"", ""suffixes=('_x', '_y')"", 'star_restaurant_id', 'star_restaurant_review', "" pd.merge(restaurant_ids_dataframe, restaurant_review_frame, on='business_id', how='outer', suffixes=('_restaurant_id', '_restaurant_review'))\n""]"
890;;0;18835121;18835077.0;2;43;;;;"['get_level_values', ""In [11]: df\nOut[11]:\n     0\nA B\n1 4  1\n2 5  2\n3 6  3\n\nIn [12]: df.iloc[df.index.get_level_values('A') == 1]\nOut[12]:\n     0\nA B\n1 4  1\n"", 'xs', 'drop_level', ""df.xs(1, level='A', drop_level=False) # axis=1 if columns\n"", ""In [21]: df1 = df.T\n\nIn [22]: df1.iloc[:, df1.columns.get_level_values('A') == 1]\nOut[22]:\nA  1\nB  4\n0  1\n""]"
891;;3;18835174;18835077.0;2;15;;;;"['DataFrame.xs()', ""In [36]: df = DataFrame(np.random.randn(10, 4))\n\nIn [37]: df.columns = [np.random.choice(['a', 'b'], size=4).tolist(), np.random.choice(['c', 'd'], size=4)]\n\nIn [38]: df.columns.names = ['A', 'B']\n\nIn [39]: df\nOut[39]:\nA      b             a\nB      d      d      d      d\n0 -1.406  0.548 -0.635  0.576\n1 -0.212 -0.583  1.012 -1.377\n2  0.951 -0.349 -0.477 -1.230\n3  0.451 -0.168  0.949  0.545\n4 -0.362 -0.855  1.676 -2.881\n5  1.283  1.027  0.085 -1.282\n6  0.583 -1.406  0.327 -0.146\n7 -0.518 -0.480  0.139  0.851\n8 -0.030 -0.630 -1.534  0.534\n9  0.246 -1.558 -1.885 -1.543\n\nIn [40]: df.xs('a', level='A', axis=1)\nOut[40]:\nB      d      d\n0 -0.635  0.576\n1  1.012 -1.377\n2 -0.477 -1.230\n3  0.949  0.545\n4  1.676 -2.881\n5  0.085 -1.282\n6  0.327 -0.146\n7  0.139  0.851\n8 -1.534  0.534\n9 -1.885 -1.543\n"", 'A', 'drop_level', ""In [42]: df.xs('a', level='A', axis=1, drop_level=False)\nOut[42]:\nA      a\nB      d      d\n0 -0.635  0.576\n1  1.012 -1.377\n2 -0.477 -1.230\n3  0.949  0.545\n4  1.676 -2.881\n5  0.085 -1.282\n6  0.327 -0.146\n7  0.139  0.851\n8 -1.534  0.534\n9 -1.885 -1.543\n""]"
892;;2;18837378;18837262.0;2;16;;;;"['Date', 'Timestamp', ""df = pd.DataFrame(data.items(), columns=['Date', 'DateValue'])\ndf['Date'] = pd.to_datetime(df['Date'])\n""]"
893;;5;18837389;18837262.0;2;145;;;;"['pd.DataFrame(d)\nValueError: If using all scalar values, you must must pass an index\n', ""In [11]: pd.DataFrame(d.items())  # or list(d.items()) in python 3\nOut[11]:\n             0    1\n0   2012-07-02  392\n1   2012-07-06  392\n2   2012-06-29  391\n3   2012-06-28  391\n...\n\nIn [12]: pd.DataFrame(d.items(), columns=['Date', 'DateValue'])\nOut[12]:\n          Date  DateValue\n0   2012-07-02        392\n1   2012-07-06        392\n2   2012-06-29        391\n"", ""In [21]: s = pd.Series(d, name='DateValue')\nOut[21]:\n2012-06-08    388\n2012-06-09    388\n2012-06-10    388\n\nIn [22]: s.index.name = 'Date'\n\nIn [23]: s.reset_index()\nOut[23]:\n          Date  DateValue\n0   2012-06-08        388\n1   2012-06-09        388\n2   2012-06-10        388\n""]"
894;;3;18878267;18876022.0;2;23;;;;"['.dataframe', 'to_html', 'df.to_html', ""style = '<style>.dataframe td { text-align: right; }</style>'\nHTML( style + df.to_html( formatters=frmt ) )\n"", ""style = '<style>.right_aligned_df td { text-align: right; }</style>'\nHTML(style + df.to_html(formatters=frmt, classes='right_aligned_df'))\n"", 'classes', 'to_html', ""# Some cell at the begining of the notebook\nIn [2]: HTML('''<style>\n                    .right_aligned_df td { text-align: right; }\n                    .left_aligned_df td { text-align: right; }\n                    .pink_df { background-color: pink; }\n                </style>''')\n\n...\n\n# Much later in your notebook\nIn [66]: HTML(df.to_html(classes='pink_df'))\n""]"
895;;1;18885319;18885175.0;2;22;;;;"['open', 'read', ""In [11]: crime2013 = pd.read_csv(z.open('crime_incidents_2013_CSV.csv'))\n\nIn [12]: crime2013\nOut[12]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 24567 entries, 0 to 24566\nData columns (total 15 columns):\nCCN                            24567  non-null values\nREPORTDATETIME                 24567  non-null values\nSHIFT                          24567  non-null values\nOFFENSE                        24567  non-null values\nMETHOD                         24567  non-null values\nLASTMODIFIEDDATE               24567  non-null values\nBLOCKSITEADDRESS               24567  non-null values\nBLOCKXCOORD                    24567  non-null values\nBLOCKYCOORD                    24567  non-null values\nWARD                           24563  non-null values\nANC                            24567  non-null values\nDISTRICT                       24567  non-null values\nPSA                            24567  non-null values\nNEIGHBORHOODCLUSTER            24263  non-null values\nBUSINESSIMPROVEMENTDISTRICT    3613  non-null values\ndtypes: float64(4), int64(1), object(10)\n""]"
896;;4;18942558;18942506.0;2;55;;;;"['if Col2 <= 1', 'False', 'True', 'int64', 'True', 'False', '0', ""df['Col3'] = (df['Col2'] <= 1).astype(int)\n"", 'Col3', 'Col2', ""df['Col3'] = df['Col2'].map(lambda x: 42 if x > 1 else 55)\n"", ""df['Col3'] = 0\ncondition = df['Col2'] > 1\ndf.loc[condition, 'Col3'] = 42\ndf.loc[~condition, 'Col3'] = 55\n""]"
897;;6;18973430;18973404.0;2;49;;;;"['.set_color', "">>> barlist=plt.bar([1,2,3,4], [1,2,3,4])\n>>> barlist[0].set_color('r')\n>>> plt.show()\n"", "">>> f=plt.figure()\n>>> ax=f.add_subplot(1,1,1)\n>>> ax.bar([1,2,3,4], [1,2,3,4])\n<Container object of 4 artists>\n>>> ax.get_children()\n[<matplotlib.axis.XAxis object at 0x6529850>, \n <matplotlib.axis.YAxis object at 0x78460d0>,  \n <matplotlib.patches.Rectangle object at 0x733cc50>, \n <matplotlib.patches.Rectangle object at 0x733cdd0>, \n <matplotlib.patches.Rectangle object at 0x777f290>, \n <matplotlib.patches.Rectangle object at 0x777f710>, \n <matplotlib.text.Text object at 0x7836450>, \n <matplotlib.patches.Rectangle object at 0x7836390>, \n <matplotlib.spines.Spine object at 0x6529950>, \n <matplotlib.spines.Spine object at 0x69aef50>,\n <matplotlib.spines.Spine object at 0x69ae310>, \n <matplotlib.spines.Spine object at 0x69aea50>]\n>>> ax.get_children()[2].set_color('r') \n #You can also try to locate the first patches.Rectangle object \n #instead of direct calling the index.\n"", '>>> import matplotlib\n>>> childrenLS=ax.get_children()\n>>> barlist=filter(lambda x: isinstance(x, matplotlib.patches.Rectangle), childrenLS)\n[<matplotlib.patches.Rectangle object at 0x3103650>, \n <matplotlib.patches.Rectangle object at 0x3103810>, \n <matplotlib.patches.Rectangle object at 0x3129850>, \n <matplotlib.patches.Rectangle object at 0x3129cd0>, \n <matplotlib.patches.Rectangle object at 0x3112ad0>]\n']"
898;;0;18975065;18973404.0;2;15;;;;"['kwds : keywords\nOptions to pass to matplotlib plotting method\n', 'import pandas as pd\nimport matplotlib.pyplot as plt\n\ns = pd.Series(\n    [5, 4, 4, 1, 12],\n    index = [""AK"", ""AX"", ""GA"", ""SQ"", ""WN""]\n)\n\n#Set descriptions:\nplt.title(""Total Delay Incident Caused by Carrier"")\nplt.ylabel(\'Delay Incident\')\nplt.xlabel(\'Carrier\')\n\n#Set tick colors:\nax = plt.gca()\nax.tick_params(axis=\'x\', colors=\'blue\')\nax.tick_params(axis=\'y\', colors=\'red\')\n\n#Plot the data:\nmy_colors = \'rgbkymc\'  #red, green, blue, black, etc.\n\npd.Series.plot(\n    s, \n    kind=\'bar\', \n    color=my_colors,\n)\n\nplt.show()\n']"
899;;1;18992172;18992086.0;2;69;;;;"['Figure.savefig()', ""ax = s.hist()  # s is an instance of Series\nfig = ax.get_figure()\nfig.savefig('/path/to/figure.pdf')\n"", 'pdf', 'pyplot', 'savefig', ""s.hist()\nsavefig('path/to/figure.pdf')  # saves the current figure\n""]"
900;;1;19031661;12047193.0;2;25;;;;['sqlalchemy.orm.query.Query', 'data_records = [rec.__dict__ for rec in query.all()]\ndf = pandas.DataFrame.from_records(data_records)\n']
901;;3;19078773;19078325.0;2;39;;;;"['df = data.groupby(...).agg(...)\ndf.columns = df.columns.droplevel(0)\n', 'df.columns = [""_"".join(x) for x in df.columns.ravel()]\n', ""import pandas as pd\nimport pandas.rpy.common as com\nimport numpy as np\n\ndata = com.load_data('Loblolly')\nprint(data.head())\n#     height  age Seed\n# 1     4.51    3  301\n# 15   10.89    5  301\n# 29   28.72   10  301\n# 43   41.74   15  301\n# 57   52.70   20  301\n\ndf = data.groupby('Seed').agg(\n    {'age':['sum'],\n     'height':['mean', 'std']})\nprint(df.head())\n#       age     height           \n#       sum        std       mean\n# Seed                           \n# 301    78  22.638417  33.246667\n# 303    78  23.499706  34.106667\n# 305    78  23.927090  35.115000\n# 307    78  22.222266  31.328333\n# 309    78  23.132574  33.781667\n\ndf.columns = df.columns.droplevel(0)\nprint(df.head())\n"", '      sum        std       mean\nSeed                           \n301    78  22.638417  33.246667\n303    78  23.499706  34.106667\n305    78  23.927090  35.115000\n307    78  22.222266  31.328333\n309    78  23.132574  33.781667\n', 'df = data.groupby(\'Seed\').agg(\n    {\'age\':[\'sum\'],\n     \'height\':[\'mean\', \'std\']})\ndf.columns = [""_"".join(x) for x in df.columns.ravel()]\n', '      age_sum   height_std  height_mean\nSeed                           \n301        78    22.638417    33.246667\n303        78    23.499706    34.106667\n305        78    23.927090    35.115000\n307        78    22.222266    31.328333\n309        78    23.132574    33.781667\n']"
902;;0;19112890;19112398.0;2;113;;;;['df = DataFrame(table, columns=headers)\ndf\n', 'Out[7]:\n   Heading1  Heading2\n0         1         2\n1         3         4\n']
903;;7;19125531;19125091.0;2;33;;;;"['cols_to_use = df2.columns - df.columns\n', 'tolist()', ""dfNew = merge(df, df2[cols_to_use], left_index=True, right_index=True, how='outer')\n"", 'cols_to_use = df2.columns.difference(df.columns)\n']"
904;;2;19126566;19124601.0;2;127;;;;"[""def print_full(x):\n    pd.set_option('display.max_rows', len(x))\n    print(x)\n    pd.reset_option('display.max_rows')\n"", '.table-striped']"
905;;0;19155860;19155718.0;2;52;;;;['    List = [1, 3]\n    df.ix[List]\n', 'ix', '.iloc', '.loc']
906;;1;19213836;19213789.0;2;75;;;;"['plt.axvline(x_position)\n', 'linestlye', 'color', 'axes', ""ax.axvline(x, color='k', linestyle='--')\n""]"
907;;9;19231939;19231871.0;2;68;;;;"['In [20]: df = DataFrame(data[\'values\'])\n\nIn [21]: df.columns = [""date"",""price""]\n\nIn [22]: df\nOut[22]: \n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 358 entries, 0 to 357\nData columns (total 2 columns):\ndate     358  non-null values\nprice    358  non-null values\ndtypes: float64(1), int64(1)\n\nIn [23]: df.head()\nOut[23]: \n         date  price\n0  1349720105  12.08\n1  1349806505  12.35\n2  1349892905  12.15\n3  1349979305  12.19\n4  1350065705  12.15\nIn [25]: df[\'date\'] = pd.to_datetime(df[\'date\'],unit=\'s\')\n\nIn [26]: df.head()\nOut[26]: \n                 date  price\n0 2012-10-08 18:15:05  12.08\n1 2012-10-09 18:15:05  12.35\n2 2012-10-10 18:15:05  12.15\n3 2012-10-11 18:15:05  12.19\n4 2012-10-12 18:15:05  12.15\n\nIn [27]: df.dtypes\nOut[27]: \ndate     datetime64[ns]\nprice           float64\ndtype: object\n']"
908;;5;19237920;19237878.0;2;54;;;;"['Time', 'Product', 'DataFrame', 'df', 'DataFrame', 'DataFrame', ""k1 = df.loc[(df.Product == p_id) & (df.Time >= start_time) & (df.Time < end_time), ['Time', 'Product']]\n"", '&', '&', 'pandas', 'DataFrame.query()', 'select', 'query()', ""df[['Time', 'Product']].query('Product == p_id and Month < mn and Year == yr')\n"", 'In [9]: df = DataFrame({\'gender\': np.random.choice([\'m\', \'f\'], size=10), \'price\': poisson(100, size=10)})\n\nIn [10]: df\nOut[10]:\n  gender  price\n0      m     89\n1      f    123\n2      f    100\n3      m    104\n4      m     98\n5      m    103\n6      f    100\n7      f    109\n8      f     95\n9      m     87\n\nIn [11]: df.query(\'gender == ""m"" and price < 100\')\nOut[11]:\n  gender  price\n0      m     89\n4      m     98\n9      m     87\n', ""k1 = df[['Time', 'Product']].query('Product == p_id and start_time <= Time < end_time')\n""]"
909;;0;19238029;11067027.0;2;12;;;;"['data.reindex_axis(sorted(data.columns, key=lambda x: float(x[1:])), axis=1)\n', ""vals = randint(low=16, high=80, size=25).reshape(5,5)\ncols = ['Q1.3', 'Q6.1', 'Q1.2', 'Q9.1', 'Q10.2']\ndata = DataFrame(vals, columns = cols)\n"", 'data\n\n    Q1.3    Q6.1    Q1.2    Q9.1    Q10.2\n0   73      29      63      51      72\n1   61      29      32      68      57\n2   36      49      76      18      37\n3   63      61      51      30      31\n4   36      66      71      24      77\n', 'data.reindex_axis(sorted(data.columns, key=lambda x: float(x[1:])), axis=1)\n', 'data\n\n\n     Q1.2    Q1.3    Q6.1    Q9.1    Q10.2\n0    2       0       1       3       4\n1    7       5       6       8       9\n2    2       0       1       3       4\n3    2       0       1       3       4\n4    2       0       1       3       4\n']"
910;;0;19295539;19155718.0;2;14;;;;['df.iloc[[1,3],:]\n']
911;;0;19295726;12065885.0;2;34;;;;"[""b = df[(df['a'] > 1) & (df['a'] < 5)]\n""]"
912;;4;19324591;19324453.0;2;83;;;;"['Series.reindex', ""import pandas as pd\n\nidx = pd.date_range('09-01-2013', '09-30-2013')\n\ns = pd.Series({'09-02-2013': 2,\n               '09-03-2013': 10,\n               '09-06-2013': 5,\n               '09-07-2013': 1})\ns.index = pd.DatetimeIndex(s.index)\n\ns = s.reindex(idx, fill_value=0)\nprint(s)\n"", '2013-09-01     0\n2013-09-02     2\n2013-09-03    10\n2013-09-04     0\n2013-09-05     0\n2013-09-06     5\n2013-09-07     1\n2013-09-08     0\n...\n']"
913;;10;19368360;19365513.0;2;30;;;;['loc']
914;;6;19378497;19377969.0;2;110;;;;"['dataframe[""period""] = dataframe[""Year""].map(str) + dataframe[""quarter""]\n']"
915;;3;19385591;19384532.0;2;116;;;;"['groupby', 'agg', ""df[['col1', 'col2', 'col3', 'col4']].groupby(['col1', 'col2']).agg(['mean', 'count'])\n""]"
916;;7;19415186;7837722.0;2;13;;;;['df[b] = df[a].apply(lambda col: do stuff with col here)\n']
917;;0;19473752;13636592.0;2;18;;;;"[""df.sort(['Peak', 'Weeks'], ascending=[True, False], inplace=True)\n"", 'inplace=True']"
918;;0;19482988;19482970.0;2;11;;;;['my_dataframe.columns']
919;;5;19483025;19482970.0;2;548;;;;['list(my_dataframe.columns.values)\n', 'list(my_dataframe)\n']
920;;1;19483602;19482970.0;2;202;;;;['my_dataframe.columns.values.tolist()\n', '.columns', 'Index', '.columns.values', 'array', 'list', 'list(df)\n']
921;;2;19555675;19555525.0;2;7;;;;"[""dtf = pd.DataFrame.from_records(d,columns=h)\ndtf2.plot()\nfig = plt.gcf()\nfig.savefig('output.png')\n""]"
922;;3;19585378;19585280.0;2;31;;;;['iterrows', 'temp=[]\n\nfor row in df.iterrows():\n    index, data = row\n    temp.append(data.tolist())\n', 'apply', 'df.apply(lambda x: x.tolist(), axis=1)\n', 'tolist', '.values', 'In [62]:\ndf.values.tolist()\n\nOut[62]:\n[[0.0, 3.61, 380.0, 3.0],\n [1.0, 3.67, 660.0, 3.0],\n [1.0, 3.19, 640.0, 4.0],\n [0.0, 2.93, 520.0, 4.0]]\n']
923;;0;19585413;19585280.0;2;19;;;;['map(list, df.values)\n']
924;;3;19592693;19584029.0;2;7;;;;"['.hist()', ""df.reset_index().pivot('index','Letter','N').hist()\n"", 'reset_index()', 'index', 'pivot', 'N', 'Letter', 'NaN', 'A, B, C', 'hist()']"
925;;4;19600533;11232275.0;2;55;;;;"[""pandas.pivot_table(df,values='count',index='site_id',columns='week')\n""]"
926;;4;19603918;19584029.0;2;86;;;;"[""df['N'].hist(by=df['Letter'])\n""]"
927;;7;19611857;19611729.0;2;28;;;;"['read_csv()', 'StringIO', ""from StringIO import StringIO  # got moved to io in python3.\n\nimport requests\nr = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&output=csv')\ndata = r.content\n\nIn [10]: df = pd.read_csv(StringIO(data), index_col=0,parse_dates=['Quradate'])\n\nIn [11]: df.head()\nOut[11]: \n          City                                            region     Res_Comm  \\\n0       Dothan  South_Central-Montgomery-Auburn-Wiregrass-Dothan  Residential   \n10       Foley                              South_Mobile-Baldwin  Residential   \n12  Birmingham      North_Central-Birmingham-Tuscaloosa-Anniston   Commercial   \n38       Brent      North_Central-Birmingham-Tuscaloosa-Anniston  Residential   \n44      Athens                 North_Huntsville-Decatur-Florence  Residential   \n\n          mkt_type            Quradate  National_exp  Alabama_exp  Sales_exp  \\\n0            Rural 2010-01-15 00:00:00             2            2          3   \n10  Suburban_Urban 2010-01-15 00:00:00             4            4          4   \n12  Suburban_Urban 2010-01-15 00:00:00             2            2          3   \n38           Rural 2010-01-15 00:00:00             3            3          3   \n44  Suburban_Urban 2010-01-15 00:00:00             4            5          4   \n\n    Inventory_exp  Price_exp  Credit_exp  \n0               2          3           3  \n10              4          4           3  \n12              2          2           3  \n38              3          3           2  \n44              4          4           4  \n""]"
928;;4;19619020;19618912.0;2;6;;;;"['Series.isin()', 'DataFrame.append()', 'In [80]: df1\nOut[80]:\n   rating  user_id\n0       2  0x21abL\n1       1  0x21abL\n2       1   0xdafL\n3       0  0x21abL\n4       4  0x1d14L\n5       2  0x21abL\n6       1  0x21abL\n7       0   0xdafL\n8       4  0x1d14L\n9       1  0x21abL\n\nIn [81]: df2\nOut[81]:\n   rating      user_id\n0       2      0x1d14L\n1       1    0xdbdcad7\n2       1      0x21abL\n3       3      0x21abL\n4       3      0x21abL\n5       1  0x5734a81e2\n6       2      0x1d14L\n7       0       0xdafL\n8       0      0x1d14L\n9       4  0x5734a81e2\n\nIn [82]: ind = df2.user_id.isin(df1.user_id) & df1.user_id.isin(df2.user_id)\n\nIn [83]: ind\nOut[83]:\n0     True\n1    False\n2     True\n3     True\n4     True\n5    False\n6     True\n7     True\n8     True\n9    False\nName: user_id, dtype: bool\n\nIn [84]: df1[ind].append(df2[ind])\nOut[84]:\n   rating  user_id\n0       2  0x21abL\n2       1   0xdafL\n3       0  0x21abL\n4       4  0x1d14L\n6       1  0x21abL\n7       0   0xdafL\n8       4  0x1d14L\n0       2  0x1d14L\n2       1  0x21abL\n3       3  0x21abL\n4       3  0x21abL\n6       2  0x1d14L\n7       0   0xdafL\n8       0  0x1d14L\n', 'pandas', 'df1', 'df2', ""In [93]: df1.index & df2.index\nOut[93]: Int64Index([], dtype='int64')\n""]"
929;;0;19632099;19632075.0;2;11;;;;"['pd.read_csv(""whitespace.csv"", header=None, delimiter=r""\\s+"")\n']"
930;;0;19633103;19632075.0;2;43;;;;['delim_whitespace=True']
931;;9;19736406;19736080.0;2;36;;;;['In [6]: d = dict( A = np.array([1,2]), B = np.array([1,2,3,4]) )\n\nIn [7]: DataFrame(dict([ (k,Series(v)) for k,v in d.items() ]))\nOut[7]: \n    A  B\n0   1  1\n1   2  2\n2 NaN  3\n3 NaN  4\n', 'd.items()', 'd.iteritems()']
932;;2;19739768;14262433.0;2;38;;;;[]
933;;8;19758398;19758364.0;2;145;;;;"[""data.rename(columns={'gdp':'log(gdp)'}, inplace=True)\n"", 'rename', 'columns']"
934;;5;19791302;19790790.0;2;28;;;;"[""#create some data with Names column\ndata = pd.DataFrame({'Names': ['Joe', 'John', 'Jasper', 'Jez'] *4, 'Ob1' : np.random.rand(16), 'Ob2' : np.random.rand(16)})\n\n#create unique list of names\nUniqueNames = data.Names.unique()\n\n#create a data frame dictionary to store your data frames\nDataFrameDict = {elem : pd.DataFrame for elem in UniqueNames}\n\nfor key in DataFrameDict.keys():\n    DataFrameDict[key] = data[:][data.Names == key]\n"", ""DataFrameDict['Joe']\n""]"
935;;4;19798528;19798153.0;2;221;;;;"[""In [116]: frame = DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n\nIn [117]: frame\nOut[117]: \n               b         d         e\nUtah   -0.029638  1.081563  1.280300\nOhio    0.647747  0.831136 -1.549481\nTexas   0.513416 -0.884417  0.195343\nOregon -0.485454 -0.477388 -0.309548\n\nIn [118]: f = lambda x: x.max() - x.min()\n\nIn [119]: frame.apply(f)\nOut[119]: \nb    1.133201\nd    1.965980\ne    2.829781\ndtype: float64\n"", ""In [120]: format = lambda x: '%.2f' % x\n\nIn [121]: frame.applymap(format)\nOut[121]: \n            b      d      e\nUtah    -0.03   1.08   1.28\nOhio     0.65   0.83  -1.55\nTexas    0.51  -0.88   0.20\nOregon  -0.49  -0.48  -0.31\n"", ""In [122]: frame['e'].map(format)\nOut[122]: \nUtah       1.28\nOhio      -1.55\nTexas      0.20\nOregon    -0.31\nName: e, dtype: object\n"", 'apply', 'applymap', 'map']"
936;;8;19809616;19790790.0;2;26;;;;"[""'name'"", ""# sort the dataframe\ndf.sort(columns=['name'], inplace=True)\n# set the index to be this and don't drop\ndf.set_index(keys=['name'], drop=False,inplace=True)\n# get a list of names\nnames=df['name'].unique().tolist()\n# now we can perform a lookup on a 'view' of the dataframe\njoe = df.loc[df.name=='joe']\n# now you can query all 'joes'\n""]"
937;;1;19821311;17001389.0;2;62;;;;['B   business day frequency\nC   custom business day frequency (experimental)\nD   calendar day frequency\nW   weekly frequency\nM   month end frequency\nBM  business month end frequency\nMS  month start frequency\nBMS business month start frequency\nQ   quarter end frequency\nBQ  business quarter endfrequency\nQS  quarter start frequency\nBQS business quarter start frequency\nA   year end frequency\nBA  business year end frequency\nAS  year start frequency\nBAS business year start frequency\nH   hourly frequency\nT   minutely frequency\nS   secondly frequency\nL   milliseconds\nU   microseconds\n']
938;;2;19828967;19828822.0;2;171;;;;"['df.empty', ""if df.empty:\n    print('DataFrame is empty!')\n""]"
939;;0;19851521;19851005.0;2;95;;;;"['rename', ""df.index.names = ['Date']\n"", 'Index', 'MultiIndex', ""In [1]: df = pd.DataFrame([[1, 2, 3], [4, 5 ,6]], columns=list('ABC'))\n\nIn [2]: df\nOut[2]: \n   A  B  C\n0  1  2  3\n1  4  5  6\n\nIn [3]: df1 = df.set_index('A')\n\nIn [4]: df1\nOut[4]: \n   B  C\nA      \n1  2  3\n4  5  6\n"", ""In [5]: df1.rename(index={1: 'a'})\nOut[5]: \n   B  C\nA      \na  2  3\n4  5  6\n\nIn [6]: df1.rename(columns={'B': 'BB'})\nOut[6]: \n   BB  C\nA       \n1   2  3\n4   5  6\n"", ""In [7]: df1.index.names = ['index']\n        df1.columns.names = ['column']\n"", 'In [8]: df1\nOut[8]: \ncolumn  B  C\nindex       \n1       2  3\n4       5  6\n']"
940;;5;19913845;19913659.0;2;154;;;;"[""df['color'] = np.where(df['Set']=='Z', 'green', 'red')\n"", ""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})\ndf['color'] = np.where(df['Set']=='Z', 'green', 'red')\nprint(df)\n"", '  Set Type  color\n0   Z    A  green\n1   Z    B  green\n2   X    B    red\n3   Y    C    red\n', 'np.select', 'yellow', ""(df['Set'] == 'Z') & (df['Type'] == 'A')"", 'blue', ""(df['Set'] == 'Z') & (df['Type'] == 'B')"", 'purple', ""(df['Type'] == 'B')"", 'black', ""df = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})\nconditions = [\n    (df['Set'] == 'Z') & (df['Type'] == 'A'),\n    (df['Set'] == 'Z') & (df['Type'] == 'B'),\n    (df['Type'] == 'B')]\nchoices = ['yellow', 'blue', 'purple']\ndf['color'] = np.select(conditions, choices, default='black')\nprint(df)\n"", '  Set Type   color\n0   Z    A  yellow\n1   Z    B    blue\n2   X    B  purple\n3   Y    C   black\n']"
941;;1;19915115;19914937.0;2;13;;;;"[""df['newcolumn'] = df.A * df.B\n"", ""def fab(row):\n  return row['A'] * row['B']\n\ndf['newcolumn'] = df.apply(fab, axis=1)\n""]"
942;;0;19922732;19914937.0;2;77;;;;"["">>> def fxy(x, y):\n...     return x * y\n\n>>> df['newcolumn'] = df.apply(lambda x: fxy(x['A'], x['B']), axis=1)\n>>> df\n    A   B  newcolumn\n0  10  20        200\n1  20  30        600\n2  30  10        300\n""]"
943;;0;19928288;19928284.0;2;30;;;;['from pandas.util.testing import assert_frame_equal\n']
944;;0;19937902;18792918.0;2;9;;;;"[""import pandas as pd\nT1 = pd.merge(T1, T2, on=T1.index, how='outer')\n""]"
945;;7;19960116;19960077.0;2;200;;;;"['something.isin(somewhere)', '~something.isin(somewhere)', "">>> df\n  countries\n0        US\n1        UK\n2   Germany\n3     China\n>>> countries\n['UK', 'China']\n>>> df.countries.isin(countries)\n0    False\n1     True\n2    False\n3     True\nName: countries, dtype: bool\n>>> df[df.countries.isin(countries)]\n  countries\n1        UK\n3     China\n>>> df[~df.countries.isin(countries)]\n  countries\n0        US\n2   Germany\n""]"
946;;2;19960136;19960077.0;2;6;;;;"[""criterion = lambda row: row['countries'] not in countries\nnot_in = df[df.apply(criterion, axis=1)]\n""]"
947;;1;19961557;19961490.0;2;42;;;;['>>> df = pd.DataFrame(data)\n>>> df.pivot(index=0, columns=1, values=2)\n# avg DataFrame\n1      c1     c2\n0               \nr1  avg11  avg12\nr2  avg21  avg22\n>>> df.pivot(index=0, columns=1, values=3)\n# stdev DataFrame\n1        c1       c2\n0                   \nr1  stdev11  stdev12\nr2  stdev21  stdev22\n']
948;;4;19961872;19961490.0;2;24;;;;"[""df = pandas.DataFrame(data, columns=['R_Number', 'C_Number', 'Avg', 'Std'])\n\n# Possibly also this if these can always be the indexes:\n# df = df.set_index(['R_Number', 'C_Number'])\n"", ""df.set_index(['R_Number', 'C_Number']).Avg.unstack(level=1)\n"", 'pivot']"
949;;0;19973722;11361985.0;2;10;;;;['ipython', 'print paramdata.head(100).to_string()\n']
950;;1;19976286;19914937.0;2;45;;;;"['>>> import numpy as np\n>>> df = pd.DataFrame({""A"": [10,20,30], ""B"": [20, 30, 10]})\n>>> df[\'new_column\'] = np.multiply(df[\'A\'], df[\'B\'])\n>>> df\n    A   B  new_column\n0  10  20         200\n1  20  30         600\n2  30  10         300\n', "">>> def fx(x, y):\n...     return x*y\n...\n>>> df['new_column'] = np.vectorize(fx)(df['A'], df['B'])\n>>> df\n    A   B  new_column\n0  10  20         200\n1  20  30         600\n2  30  10         300\n""]"
951;;4;19991632;19991445.0;2;78;;;;"['pandas', 'pandas.stats', '>>> import pandas as pd\n>>> import statsmodels.formula.api as sm\n>>> df = pd.DataFrame({""A"": [10,20,30,40,50], ""B"": [20, 30, 10, 40, 50], ""C"": [32, 234, 23, 23, 42523]})\n>>> result = sm.ols(formula=""A ~ B + C"", data=df).fit()\n>>> print result.params\nIntercept    14.952480\nB             0.401182\nC             0.000352\ndtype: float64\n>>> print result.summary()\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      A   R-squared:                       0.579\nModel:                            OLS   Adj. R-squared:                  0.158\nMethod:                 Least Squares   F-statistic:                     1.375\nDate:                Thu, 14 Nov 2013   Prob (F-statistic):              0.421\nTime:                        20:04:30   Log-Likelihood:                -18.178\nNo. Observations:                   5   AIC:                             42.36\nDf Residuals:                       2   BIC:                             41.19\nDf Model:                           2                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n------------------------------------------------------------------------------\nIntercept     14.9525     17.764      0.842      0.489       -61.481    91.386\nB              0.4012      0.650      0.617      0.600        -2.394     3.197\nC              0.0004      0.001      0.650      0.583        -0.002     0.003\n==============================================================================\nOmnibus:                          nan   Durbin-Watson:                   1.061\nProb(Omnibus):                    nan   Jarque-Bera (JB):                0.498\nSkew:                          -0.123   Prob(JB):                        0.780\nKurtosis:                       1.474   Cond. No.                     5.21e+04\n==============================================================================\n\nWarnings:\n[1] The condition number is large, 5.21e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n']"
952;;4;19996208;19991445.0;2;55;;;;"['pandas.stats', 'pandas.stats.ols', '>>> from pandas.stats.api import ols\n>>> df = pd.DataFrame({""A"": [10,20,30,40,50], ""B"": [20, 30, 10, 40, 50], ""C"": [32, 234, 23, 23, 42523]})\n>>> res = ols(y=df[\'A\'], x=df[[\'B\',\'C\']])\n>>> res\n-------------------------Summary of Regression Analysis-------------------------\n\nFormula: Y ~ <B> + <C> + <intercept>\n\nNumber of Observations:         5\nNumber of Degrees of Freedom:   3\n\nR-squared:         0.5789\nAdj R-squared:     0.1577\n\nRmse:             14.5108\n\nF-stat (2, 2):     1.3746, p-value:     0.4211\n\nDegrees of Freedom: model 2, resid 2\n\n-----------------------Summary of Estimated Coefficients------------------------\n      Variable       Coef    Std Err     t-stat    p-value    CI 2.5%   CI 97.5%\n--------------------------------------------------------------------------------\n             B     0.4012     0.6497       0.62     0.5999    -0.8723     1.6746\n             C     0.0004     0.0005       0.65     0.5826    -0.0007     0.0014\n     intercept    14.9525    17.7643       0.84     0.4886   -19.8655    49.7705\n---------------------------------End of Summary---------------------------------\n', 'statsmodels', 'pandas.stats.ols']"
953;;1;20006954;20003290.0;2;23;;;;"[""df_data['vals'] = df_data['vals'].map(lambda x: '%2.1f' % x)\n\ndf_data.to_csv(outfile, index=False, header=False, float_format='%11.6f')\n""]"
954;;6;20019449;19991445.0;2;10;;;;['>>> data = np.asarray(df)\n', '>>> from sklearn.linear_model import LinearRegression\n>>> lr = LinearRegression()\n>>> X, y = data[:, 1:], data[:, 0]\n>>> lr.fit(X, y)\nLinearRegression(copy_X=True, fit_intercept=True, normalize=False)\n>>> lr.coef_\narray([  4.01182386e-01,   3.51587361e-04])\n>>> lr.intercept_\n14.952479503953672\n']
955;;1;20033218;20033111.0;2;11;;;;"["">>> frame['HighScore'] = frame[['test1','test2','test3']].apply(max, axis=1)\n>>> frame\n    name  test1  test2  test3  HighScore\n0   bill     85     35     51        85\n1    joe     75     45     61        75\n2  steve     85     83     45        85\n""]"
956;;2;20033232;20033111.0;2;59;;;;"["">>> frame['HighScore'] = frame[['test1','test2','test3']].max(axis=1)\n>>> frame\n    name  test1  test2  test3  HighScore\n0   bill     85     35     51         85\n1    joe     75     45     61         75\n2  steve     85     83     45         85\n""]"
957;;0;20039057;14940743.0;2;44;;;;"[""cols = [col for col in df.columns if col not in ['B', 'D']]\ndf2 = df[cols]\n""]"
958;;0;20051631;15723628.0;2;7;;;;"[""# use the df data from @herrfz\n\nIn [150]: pd.factorize(df.b)\nOut[150]: (array([0, 1, 0, 1, 2]), array(['yes', 'no', 'absent'], dtype=object))\nIn [152]: df['c'] = pd.factorize(df.b)[0]\n\nIn [153]: df\nOut[153]: \n   a       b  c\n0  1     yes  0\n1  2      no  1\n2  3     yes  0\n3  4      no  1\n4  5  absent  2\n""]"
959;;4;20067665;20067636.0;2;78;;;;"["">>> df.groupby('id').first()\n     value\nid        \n1    first\n2    first\n3    first\n4   second\n5    first\n6    first\n7   fourth\n"", 'id', "">>> df.groupby('id').first().reset_index()\n   id   value\n0   1   first\n1   2   first\n2   3   first\n3   4  second\n4   5   first\n5   6   first\n6   7  fourth\n"", "">>> df.groupby('id').head(2).reset_index(drop=True)\n    id   value\n0    1   first\n1    1  second\n2    2   first\n3    2  second\n4    3   first\n5    3   third\n6    4  second\n7    4   fifth\n8    5   first\n9    6   first\n10   6  second\n11   7  fourth\n12   7   fifth\n""]"
960;;11;20069379;20069009.0;2;61;;;;"[""df.groupby('id').head(2)"", "">>> df.groupby('id').head(2)\n       id  value\nid             \n1  0   1      1\n   1   1      2 \n2  3   2      1\n   4   2      2\n3  7   3      1\n4  8   4      1\n"", ""df.groupby('id').head(2).reset_index(drop=True)"", "">>> df.groupby('id').head(2).reset_index(drop=True)\n    id  value\n0   1      1\n1   1      2\n2   2      1\n3   2      2\n4   3      1\n5   4      1\n""]"
961;;1;20076611;20076195.0;2;62;;;;"[""df['word'].value_counts()"", 'count', 'max', 'size']"
962;;21;20084843;20083098.0;2;7;;;;"[""def f(df):\n    store = pd.HDFStore('test.h5','w')\n    store['df'] = df\n    store.close()\n"", ""def f2(df):\n    store = pd.HDFStore('test.h5','w')\n    store.append('df',df,index=False)\n    store.close()\n"", ""def f3(df):\n    store = pd.HDFStore('test.h5','w')\n    store.append('df',df)\n    store.close()\n"", ""In [25]: df = concat([DataFrame(np.random.randn(10000000,10)),DataFrame(np.random.randint(0,10,size=50000000).reshape(10000000,5))],axis=1)\n\nIn [26]: df\nOut[26]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10000000 entries, 0 to 9999999\nColumns: 15 entries, 0 to 4\ndtypes: float64(10), int64(5)\n\n\nv0.12.0\n\nIn [27]: %timeit f(df)\n1 loops, best of 3: 14.7 s per loop\n\nIn [28]: %timeit f2(df)\n1 loops, best of 3: 32 s per loop\n\nIn [29]: %timeit f3(df)\n1 loops, best of 3: 40.1 s per loop\n\nmaster/v0.13.0\n\nIn [5]: %timeit f(df)\n1 loops, best of 3: 12.9 s per loop\n\nIn [6]: %timeit f2(df)\n1 loops, best of 3: 17.5 s per loop\n\nIn [7]: %timeit f3(df)\n1 loops, best of 3: 24.3 s per loop\n"", ""In [4]: df = pd.read_hdf('test.h5','df')\n\nIn [5]: df\nOut[5]: \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 28880943 entries, 0 to 28880942\nColumns: 14 entries, node_id to kernel_type\ndtypes: float64(4), int64(10)\n"", ""In [6]: %timeit df.to_hdf('test.hdf','df',mode='w')\n1 loops, best of 3: 36.2 s per loop\n"", ""In [7]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',index=False)\n1 loops, best of 3: 45 s per loop\n\nIn [8]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',index=False,chunksize=2000000)\n1 loops, best of 3: 44.5 s per loop\n"", ""In [9]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',chunksize=2000000)\n1 loops, best of 3: 1min 36s per loop\n"", ""In [10]: %timeit df.to_hdf('test.hdf','df',mode='w',format='table',chunksize=2000000,complib='blosc')\n1 loops, best of 3: 46.5 s per loop\n\nIn [11]: %timeit pd.read_hdf('test.hdf','df')\n1 loops, best of 3: 10.8 s per loop\n"", 'In [13]: !ls -ltr test.h*\n-rw-r--r-- 1 jreback users 3471518282 Nov 20 18:20 test.h5\n-rw-rw-r-- 1 jreback users  649327780 Nov 20 21:17 test.hdf\n', 'chunksize']"
963;;5;20084895;20084382.0;2;70;;;;['In [1]: df = DataFrame(np.random.randint(0,10,size=100).reshape(10,10))\n\nIn [2]: df\nOut[2]: \n   0  1  2  3  4  5  6  7  8  9\n0  2  2  3  2  6  1  9  9  3  3\n1  1  2  5  8  5  2  5  0  6  3\n2  0  7  0  7  5  5  9  1  0  3\n3  5  3  2  3  7  6  8  3  8  4\n4  8  0  2  2  3  9  7  1  2  7\n5  3  2  8  5  6  4  3  7  0  8\n6  4  2  6  5  3  3  4  5  3  2\n7  7  6  0  6  6  7  1  7  5  1\n8  7  4  3  1  0  6  9  7  7  3\n9  5  3  4  5  2  0  8  6  4  7\n\nIn [13]: Series(df.values.ravel()).unique()\nOut[13]: array([9, 1, 4, 6, 0, 7, 5, 8, 3, 2])\n', 'In [14]: df = DataFrame(np.random.randint(0,10,size=10000).reshape(100,100))\n\nIn [15]: %timeit Series(df.values.ravel()).unique()\n10000 loops, best of 3: 137 ?s per loop\n\nIn [16]: %timeit np.unique(df.values.ravel())\n1000 loops, best of 3: 270 ?s per loop\n']
964;;1;20096827;20095673.0;2;49;;;;['In [44]: df.gdp = df.gdp.shift(-1)\n\nIn [45]: df\nOut[45]: \n   y  gdp  cap\n0  1    3    5\n1  2    7    9\n2  8    4    2\n3  3    7    7\n4  6  NaN    7\n\nIn [46]: df[:-1]                                                                                                                                                                                                                                                                                                               \nOut[46]: \n   y  gdp  cap\n0  1    3    5\n1  2    7    9\n2  8    4    2\n3  3    7    7\n']
965;;5;20107825;20107570.0;2;22;;;;['df.reset_index()']
966;;2;20159305;20109391.0;2;100;;;;"[""In [1]: df = pd.DataFrame([[1, 2], [1, 3], [4, 6]], columns=['A', 'B'])\n"", ""pd.read_clipboard(sep='\\s\\s+')"", 'In [2]: df\nOut[2]: \n   A  B\n0  1  2\n1  1  3\n2  4  6\n', ""pd.read_clipboard(sep='\\s\\s+')"", 'df = df.head()', 'df = pd.DataFrame(np.random.randn(100000000, 10))', 'In [3]: iwantthis\nOut[3]: \n   A  B\n0  1  5\n1  4  6\n', ""In [4]: df.groupby('A').sum()\nOut[4]: \n   B\nA   \n1  5\n4  6\n"", ""df.groupby('A', as_index=False).sum()"", 'pd.to_datetime', ""df['date'] = pd.to_datetime(df['date']) # this column ought to be date..\n"", 'In [11]: df\nOut[11]:\n     C\nA B   \n1 2  3\n  2  6\n', 'set_index', ""In [12]: df = pd.DataFrame([[1, 2, 3], [1, 2, 6]], columns=['A', 'B', 'C']).set_index(['A', 'B'])\n\nIn [13]: df\nOut[13]: \n     C\nA B   \n1 2  3\n  2  6\n"", '   B\nA   \n1  1\n5  0\n', ""df = pd.read_csv('my_secret_file.csv')  # ideally with lots of parsing options\n""]"
967;;8;20221655;20219254.0;2;61;;;;"['ExcelWriter', 'import pandas\nfrom openpyxl import load_workbook\n\nbook = load_workbook(\'Masterfile.xlsx\')\nwriter = pandas.ExcelWriter(\'Masterfile.xlsx\', engine=\'openpyxl\') \nwriter.book = book\nwriter.sheets = dict((ws.title, ws) for ws in book.worksheets)\n\ndata_filtered.to_excel(writer, ""Main"", cols=[\'Diff1\', \'Diff2\'])\n\nwriter.save()\n']"
968;;3;20230859;20230326.0;2;46;;;;['drop', 'df.drop(column_name, axis=1)\n']
969;;4;20231632;20230326.0;2;11;;;;"[""collist = ['col1', 'col2', 'col3']\ndf1 = df[collist]\n"", ""collist = df.columns.tolist()\n# you can now select from this list any arbritrary range\ndf1 = df[collist[0:1]]\n# or remove a column\ncollist.remove('col2')\n# now select\ndf1 = df[collist]\n# df1 will now only have 'col1' and 'col3'\n""]"
970;;3;20233649;20233071.0;2;30;;;;"[""df.loc[df.index < '2013-10-16 08:00:00']""]"
971;;1;20235451;20235401.0;2;62;;;;['>>> s = pd.Series([1,2,3,4,np.NaN,5,np.NaN])\n>>> s[~s.isnull()]\n0    1\n1    2\n2    3\n3    4\n5    5\n', 'pandas.Series.dropna()', '>>> s.dropna()\n0    1\n1    2\n2    3\n3    4\n5    5\n']
972;;5;20250947;20250771.0;2;26;;;;"['di', 'di', ""df['col1']"", 'di', 'di', 'update', ""df['col1'].update(pd.Series(di))\n"", 'import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\'col1\':[\'w\', 10, 20],\n                   \'col2\': [\'a\', 30, np.nan]},\n                  index=[1,2,0])\n#   col1 col2\n# 1    w    a\n# 2   10   30\n# 0   20  NaN\n\ndi = {0: ""A"", 2: ""B""}\n\n# The value at the 0-index is mapped to \'A\', the value at the 2-index is mapped to \'B\'\ndf[\'col1\'].update(pd.Series(di))\nprint(df)\n', '  col1 col2\n1    w    a\n2    B   30\n0    A  NaN\n', 'update', 'di', 'di', ""df['col1']"", 'replace', 'import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\'col1\':[\'w\', 10, 20],\n                   \'col2\': [\'a\', 30, np.nan]},\n                  index=[1,2,0])\nprint(df)\n#   col1 col2\n# 1    w    a\n# 2   10   30\n# 0   20  NaN\n\ndi = {10: ""A"", 20: ""B""}\n\n# The values 10 and 20 are replaced by \'A\' and \'B\'\ndf[\'col1\'].replace(di, inplace=True)\nprint(df)\n', '  col1 col2\n1    w    a\n2    A   30\n0    B  NaN\n', 'di', ""df['col1']"", 'di', ""df['col1'].put(di.keys(), di.values())\n"", 'df = pd.DataFrame({\'col1\':[\'w\', 10, 20],\n                   \'col2\': [\'a\', 30, np.nan]},\n                  index=[1,2,0])\ndi = {0: ""A"", 2: ""B""}\n\n# The values at the 0 and 2 index locations are replaced by \'A\' and \'B\'\ndf[\'col1\'].put(di.keys(), di.values())\nprint(df)\n', '  col1 col2\n1    A    a\n2   10   30\n0    B  NaN\n', 'di', '0', '2']"
973;;1;20250996;20250771.0;2;85;;;;"['.replace', '>>> df = pd.DataFrame({\'col2\': {0: \'a\', 1: 2, 2: np.nan}, \'col1\': {0: \'w\', 1: 1, 2: 2}})\n>>> di = {1: ""A"", 2: ""B""}\n>>> df\n  col1 col2\n0    w    a\n1    1    2\n2    2  NaN\n>>> df.replace({""col1"": di})\n  col1 col2\n0    w    a\n1    A    2\n2    B  NaN\n', 'Series', 'df[""col1""].replace(di, inplace=True)']"
974;;3;20297639;20297332.0;2;86;;;;"['import pandas as pd\ndf = pd.DataFrame({""pear"": [1,2,3], ""apple"": [2,3,4], ""orange"": [3,4,5]})\n\nlen(df.columns)\n3\n']"
975;;0;20301769;20297317.0;2;45;;;;['i', 'df.drop(df.columns[i], axis=1)\n', 'df = df.iloc[:, [j for j, c in enumerate(df.columns) if j != i]]\n']
976;;1;20304311;20297332.0;2;32;;;;['df.shape[1]\n', 'df.shape[0]']
977;;0;20334902;16522380.0;2;26;;;;['%matplotlib inline']
978;;2;20375692;20375561.0;2;47;;;;"[""pd.merge(frame_1, frame_2, left_on = 'county_ID', right_on = 'countyid')\n"", ""pd.merge(frame_1, frame_2, how = 'left', left_on = 'county_ID', right_on = 'countyid')\n""]"
979;;5;20384317;20383647.0;2;27;;;;['loc', 'In [2]: type(df.loc[[3]])\nOut[2]: pandas.core.frame.DataFrame\n\nIn [3]: type(df.loc[[1]])\nOut[3]: pandas.core.frame.DataFrame\n']
980;;3;20444256;20444087.0;2;70;;;;"['data.reindex(index=data.index[::-1])\n', 'data.iloc[::-1]\n', 'for', ""for idx in reversed(data.index):\n    print(idx, data.loc[idx, 'Even'], data.loc[idx, 'Odd'])\n"", 'for idx in reversed(data.index):\n    print(idx, data.Even[idx], data.Odd[idx])\n', 'reversed', 'data.__len__()', 'data[j - 1]', 'j', 'range(6, 0, -1)', 'data[5]', 'data[5]']"
981;;2;20461206;20461165.0;2;231;;;;"[""df['index1'] = df.index\n"", '.reset_index', 'df.reset_index(level=0, inplace=True)\n', '>>> df\n                       val\ntick       tag obs        \n2016-02-26 C   2    0.0139\n2016-02-27 A   2    0.5577\n2016-02-28 C   6    0.0303\n', 'tick', 'obs', "">>> df.reset_index(level=['tick', 'obs'])\n          tick  obs     val\ntag                        \nC   2016-02-26    2  0.0139\nA   2016-02-27    2  0.5577\nC   2016-02-28    6  0.0303\n""]"
982;;2;20491748;20490274.0;2;231;;;;['reset_index()', 'df = df.reset_index(drop=True)\n']
983;;5;20574460;20574257.0;2;25;;;;['>>> df_asint = df.astype(int)\n>>> coocc = df_asint.T.dot(df_asint)\n>>> coocc\n       Dop  Snack  Trans\nDop      4      2      3\nSnack    2      3      2\nTrans    3      2      4\n', 'fill_diagonal', '>>> import numpy as np\n>>> np.fill_diagonal(coocc.values, 0)\n>>> coocc\n       Dop  Snack  Trans\nDop      0      2      3\nSnack    2      0      2\nTrans    3      2      0\n']
984;;0;20612691;20612645.0;2;137;;;;"['pandas.__version__', ""In [76]: import pandas as pd\n\nIn [77]: pd.__version__\nOut[77]: '0.12.0-933-g281dc4e'\n"", 'pd.show_versions()', 'In [53]: pd.show_versions(as_json=False)\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-45-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.15.2-113-g5531341\nnose: 1.3.1\nCython: 0.21.1\nnumpy: 1.8.2\nscipy: 0.14.0.dev-371b4ff\nstatsmodels: 0.6.0.dev-a738b4f\nIPython: 2.0.0-dev\nsphinx: 1.2.2\npatsy: 0.3.0\ndateutil: 1.5\npytz: 2012c\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.2.2\nmatplotlib: 1.4.2\nopenpyxl: None\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: None\nlxml: 3.3.3\nbs4: 4.3.2\nhtml5lib: 0.999\nhttplib2: 0.8\napiclient: None\nrpy2: 2.5.5\nsqlalchemy: 0.9.8\npymysql: None\npsycopg2: 2.4.5 (dt dec mx pq3 ext)\n']"
985;;8;20627316;20625582.0;2;232;;;;"['SettingWithCopyWarning', ""df[df['A'] > 2]['B'] = new_val  # new_val not set in df\n"", ""df.loc[df['A'] > 2, 'B'] = new_val\n"", ""df = df[df['A'] > 2]\ndf['B'] = new_val\n"", ""pd.options.mode.chained_assignment = None  # default='warn'\n""]"
986;;2;20637559;20637439.0;2;37;;;;"['>>> import pandas as pd\n>>> from StringIO import StringIO\n>>> s = """"""1, 2\n... 3, 4\n... 5, 6""""""\n>>> pd.read_csv(StringIO(s), skiprows=[1], header=None)\n   0  1\n0  1  2\n1  5  6\n>>> pd.read_csv(StringIO(s), skiprows=1, header=None)\n   0  1\n0  3  4\n1  5  6\n']"
987;;9;20638258;20638006.0;2;319;;;;['d', 'pd.DataFrame(d)\n']
988;;12;20644369;20625582.0;2;68;;;;"['SettingWithCopyWarning', ""In [1]: df = DataFrame(np.random.randn(5,2),columns=list('AB'))\n\nIn [2]: dfa = df.ix[:,[1,0]]\n\nIn [3]: dfa.is_copy\nOut[3]: True\n\nIn [4]: dfa['A'] /= 2\n/usr/local/bin/ipython:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  #!/usr/local/bin/python\n"", 'is_copy', 'False', ""In [5]: dfa.is_copy = False\n\nIn [6]: dfa['A'] /= 2\n"", ""In [7]: dfa = df.ix[:,[1,0]].copy()\n\nIn [8]: dfa['A'] /= 2\n"", 'reindex', ""quote_df = quote_df(columns=['STK',.......])\n""]"
989;;1;20657592;20656663.0;2;66;;;;"['hist', ""s = pd.Series([1,2,3,2,2,3,5,2,3,2,np.nan])\nfig, ax = plt.subplots()\nax.hist(s, alpha=0.9, color='blue')\n"", 'AttributeError: max must be larger than min in range parameter.', ""ax.hist(s.dropna(), alpha=0.9, color='blue')\n"", 'hist', 'axes[0]', 'ax', ""dfj2_MARKET1['VSPD1_perc'].hist(ax=axes[0], alpha=0.9, color='blue')\n""]"
990;;1;20687887;19798153.0;2;9;;;;['    frame.apply(np.sqrt)\n    Out[102]: \n                   b         d         e\n    Utah         NaN  1.435159       NaN\n    Ohio    1.098164  0.510594  0.729748\n    Texas        NaN  0.456436  0.697337\n    Oregon  0.359079       NaN       NaN\n\n    frame.applymap(np.sqrt)\n    Out[103]: \n                   b         d         e\n    Utah         NaN  1.435159       NaN\n    Ohio    1.098164  0.510594  0.729748\n    Texas        NaN  0.456436  0.697337\n    Oregon  0.359079       NaN       NaN\n']
991;;0;20687984;17627219.0;2;26;;;;"['scipy.spatial.distance.pdist', ""# base similarity matrix (all dot products)\n# replace this with A.dot(A.T).toarray() for sparse representation\nsimilarity = numpy.dot(A, A.T)\n\n\n# squared magnitude of preference vectors (number of occurrences)\nsquare_mag = numpy.diag(similarity)\n\n# inverse squared magnitude\ninv_square_mag = 1 / square_mag\n\n# if it doesn't occur, set it's inverse magnitude to zero (instead of inf)\ninv_square_mag[numpy.isinf(inv_square_mag)] = 0\n\n# inverse of the magnitude\ninv_mag = numpy.sqrt(inv_square_mag)\n\n# cosine similarity (elementwise multiply by inverse magnitudes)\ncosine = similarity * inv_mag\ncosine = cosine.T * inv_mag\n"", 'A', 'scipy.sparse', 'numpy', 'scipy.sparse']"
992;;1;20690383;14262433.0;2;78;;;;[]
993;;1;20693013;16249736.0;2;15;;;;['Monary']
994;;0;20763459;20763012.0;2;88;;;;['data', 'index', 'columns', 'DataFrame', '>>> pd.DataFrame(data=data[1:,1:],    # values\n...              index=data[1:,0],    # 1st column as index\n...              columns=data[0,1:])  # 1st row as the column names\n', 'np.int_(data[1:,1:])']
995;;0;20868446;20868394.0;2;185;;;;"[""In [27]: df=df.rename(columns = {'two':'new_name'})\n\nIn [28]: df\nOut[28]: \n  one three  new_name\n0    1     a         9\n1    2     b         8\n2    3     c         7\n3    4     d         6\n4    5     e         5\n"", 'rename']"
996;;1;20937592;20937538.0;2;101;;;;"[""import pandas as pd\npd.options.display.float_format = '${:,.2f}'.format\ndf = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\nprint(df)\n"", '        cost\nfoo  $123.46\nbar  $234.57\nbaz  $345.68\nquux $456.79\n', ""import pandas as pd\ndf = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\ndf['foo'] = df['cost']\ndf['cost'] = df['cost'].map('${:,.2f}'.format)\nprint(df)\n"", '         cost       foo\nfoo   $123.46  123.4567\nbar   $234.57  234.5678\nbaz   $345.68  345.6789\nquux  $456.79  456.7890\n']"
997;;4;20965090;20965046.0;2;44;;;;"[""df['cum_sum'] = df.val1.cumsum()\ndf['cum_perc'] = 100*df.cum_sum/df.val1.sum()\n"", 'df', 'df']"
998;;8;20970328;20970279.0;2;34;;;;"["">>> df['StateInitial'] = df['state'].str[:2]\n>>> df\n   pop       state  year StateInitial\n0  1.5    Auckland  2000           Au\n1  1.7       Otago  2001           Ot\n2  3.6  Wellington  2002           We\n3  2.4     Dunedin  2001           Du\n4  2.9    Hamilton  2002           Ha\n"", ""df['state'].str[-2:]"", 'apply', "">>> df['state'].apply(lambda x: x[len(x)/2-1:len(x)/2+1])\n0    kl\n1    ta\n2    in\n3    ne\n4    il\n""]"
999;;2;20995313;20995196.0;2;14;;;;"['>>> df = pd.DataFrame({""class"":[1,1,1,2,2], ""value"":[1,2,3,4,5]})\n>>> df[df[""class""]==1].sum()\nclass    3\nvalue    6\ndtype: int64\n>>> df[df[""class""]==1].sum()[""value""]\n6\n>>> df[df[""class""]==1].count()[""value""]\n3\n', 'df[""class""]==1']"
1000;;4;20995428;20995196.0;2;24;;;;"['sum', "">> df = pd.DataFrame({'a': [1, 2, 3]})\n>> df[df.a > 1].sum()   \na    5\ndtype: int64\n"", '>> df[(df.a > 1) & (df.a < 3)].sum()\na    2\ndtype: int64\n']"
1001;;1;21020411;21018654.0;2;89;;;;[]
1002;;5;21032532;17116814.0;2;42;;;;"['time python -c ""import pandas as pd;\ndf = pd.DataFrame([\'a b c\']*100000, columns=[\'col\']);\nprint df[\'col\'].apply(lambda x : pd.Series(x.split(\' \'))).head()""\n', 'time python -c ""import pandas as pd;\nfrom scipy import array, concatenate;\ndf = pd.DataFrame([\'a b c\']*100000, columns=[\'col\']);\nprint pd.DataFrame(concatenate(df[\'col\'].apply( lambda x : [x.split(\' \')]))).head()""\n', 'time python -c ""import pandas as pd;\ndf = pd.DataFrame([\'a b c\']*100000, columns=[\'col\']);\nprint pd.DataFrame(dict(zip(range(3), [df[\'col\'].apply(lambda x : x.split(\' \')[i]) for i in range(3)]))).head()""\n', 'time python -c ""import pandas as pd;\ndf = pd.DataFrame([\'a b c\']*100000, columns=[\'col\']);\nprint pd.DataFrame(df.col.str.split().tolist()).head()""\n', 'time python -c ""import pandas as pd;\ndf = pd.DataFrame([\'a b c\']*100000, columns=[\'col\']);\nprint pd.DataFrame(list(df.col.str.split())).head()""\n']"
1003;;3;21140339;21137150.0;2;73;;;;"[""In [25]: pd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nIn [28]: Series(np.random.randn(3))*1000000000\nOut[28]: \n0    -757322420.605\n1   -1436160588.997\n2   -1235116117.064\ndtype: float64\n"", ""In [6]: Series(np.random.randn(3)).apply(lambda x: '%.3f' % x)\nOut[6]: \n0     0.026\n1    -0.482\n2    -0.694\ndtype: object\n""]"
1004;;2;21175114;18079563.0;2;12;;;;['s1 = pd.Series([4,5,6,20,42])\ns2 = pd.Series([1,2,3,5,42])\n', '%%timeit\npd.Series(list(set(s1).intersection(set(s2))))\n10000 loops, best of 3: 57.7 s per loop\n\n%%timeit\npd.Series(np.intersect1d(s1,s2))\n1000 loops, best of 3: 659 s per loop\n\n%%timeit\npd.Series(np.intersect1d(s1.values,s2.values))\n10000 loops, best of 3: 64.7 s per loop\n', 'values']
1005;;6;21197863;21197774.0;2;28;;;;['convert_objects', 'In [11]: df\nOut[11]: \n   x  y\n0  a  1\n1  b  2\n\nIn [12]: df.dtypes\nOut[12]: \nx    object\ny    object\ndtype: object\n\nIn [13]: df.convert_objects(convert_numeric=True)\nOut[13]: \n   x  y\n0  a  1\n1  b  2\n\nIn [14]: df.convert_objects(convert_numeric=True).dtypes\nOut[14]: \nx    object\ny     int64\ndtype: object\n']
1006;;2;21221138;14349055.0;2;29;;;;"[""In [1]: import matplotlib as mpl\n\nIn [2]: import matplotlib.pyplot as plt\n\nIn [3]: import numpy as np\n\nIn [4]: mpl.style.available\nOut[4]: [u'dark_background', u'grayscale', u'ggplot']\n\nIn [5]: mpl.style.use('ggplot')\n\nIn [6]: plt.hist(np.random.randn(100000))\nOut[6]: \n...\n""]"
1007;;9;21232849;20906474.0;2;107;;;;"['csv', 'header=0', 'csv', 'path =r\'C:\\DRO\\DCL_rawdata_files\' # use your path\nallFiles = glob.glob(path + ""/*.csv"")\nframe = pd.DataFrame()\nlist_ = []\nfor file_ in allFiles:\n    df = pd.read_csv(file_,index_col=None, header=0)\n    list_.append(df)\nframe = pd.concat(list_)\n']"
1008;;1;21260328;10065051.0;2;9;;;;"['import MySQLdb as db\nfrom pandas import DataFrame\nfrom pandas.io.sql import frame_query\n\ndatabase = db.connect(\'localhost\',\'username\',\'password\',\'database\')\ndata     = frame_query(""SELECT * FROM data"", database)\n']"
1009;;1;21266043;21104592.0;2;61;;;;"[""from urllib2 import Request, urlopen\nimport json\nfrom pandas.io.json import json_normalize\n\npath1 = '42.974049,-81.205203|42.974298,-81.195755'\nrequest=Request('http://maps.googleapis.com/maps/api/elevation/json?locations='+path1+'&sensor=false')\nresponse = urlopen(request)\nelevations = response.read()\ndata = json.loads(elevations)\njson_normalize(data['results'])\n""]"
1010;;0;21271103;21269399.0;2;7;;;;"[""import pandas as pd\nfrom datetime import datetime\nheaders = ['col1', 'col2', 'col3', 'col4'] \ndtypes = [datetime, datetime, str, float] \npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes)\n"", ""pd.read_csv(file, sep='\\t', header=None, names=headers, parse_dates=True)\n""]"
1011;;11;21285575;21285380.0;2;60;;;;"['DataFrame.columns', ""import pandas as pd\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\n\nspike_cols = [col for col in df.columns if 'spike' in col]\nprint(list(df.columns))\nprint(spike_cols)\n"", ""['hey spke', 'no', 'spike-2', 'spiked-in']\n['spike-2', 'spiked-in']\n"", 'df.columns', ""[col for col in df.columns if 'spike' in col]"", 'df.columns', 'col', 'col', ""'spike'"", ""df2 = df.filter(regex='spike')\nprint(df2)\n"", '   spike-2  spiked-in\n0        1          7\n1        2          8\n2        3          9\n']"
1012;;0;21287539;13842088.0;2;25;;;;"[""df.ix['x','C']=10\n"", ""df['x']['C']""]"
1013;;5;21290084;21287624.0;2;48;;;;[]
1014;;9;21291383;21291259.0;2;60;;;;"['>>> df = pd.DataFrame(np.random.rand(3,4), columns=list(""ABCD""))\n>>> df\n          A         B         C         D\n0  0.542447  0.949988  0.669239  0.879887\n1  0.068542  0.757775  0.891903  0.384542\n2  0.021274  0.587504  0.180426  0.574300\n>>> df[list(""ABCD"")] = df[list(""ABCD"")].astype(int)\n>>> df\n   A  B  C  D\n0  0  0  0  0\n1  0  0  0  0\n2  0  0  0  0\n', '>>> df\n          A         B     C         D\n0  0.475103  0.355453  0.66  0.869336\n1  0.260395  0.200287   NaN  0.617024\n2  0.517692  0.735613  0.18  0.657106\n>>> df[list(""ABCD"")] = df[list(""ABCD"")].fillna(0.0).astype(int)\n>>> df\n   A  B  C  D\n0  0  0  0  0\n1  0  0  0  0\n2  0  0  0  0\n>>>\n']"
1015;;4;21291622;21291259.0;2;55;;;;"[""df= pd.DataFrame(range(5), columns=['a'])\ndf.a = df.a.astype(float)\ndf\n\nOut[33]:\n\n          a\n0 0.0000000\n1 1.0000000\n2 2.0000000\n3 3.0000000\n4 4.0000000\n\npd.options.display.float_format = '{:,.0f}'.format\ndf\n\nOut[35]:\n\n   a\n0  0\n1  1\n2  2\n3  3\n4  4\n""]"
1016;;3;21296915;14745022.0;2;24;;;;"[""In [11]: df.row.str.extract('(?P<fips>\\d{5})((?P<state>[A-Z ]*$)|(?P<county>.*?), (?P<state_code>[A-Z]{2}$))')\nOut[11]: \n    fips                    1           state           county state_code\n0  00000        UNITED STATES   UNITED STATES              NaN        NaN\n1  01000              ALABAMA         ALABAMA              NaN        NaN\n2  01001   Autauga County, AL             NaN   Autauga County         AL\n3  01003   Baldwin County, AL             NaN   Baldwin County         AL\n4  01005   Barbour County, AL             NaN   Barbour County         AL\n\n[5 rows x 5 columns]\n"", '(?P<fips>\\d{5})\n', '\\d', '""fips""', '((?P<state>[A-Z ]*$)|(?P<county>.*?), (?P<state_code>[A-Z]{2}$))\n', '|', '(?P<state>[A-Z ]*$)\n', '*', '[A-Z ]', '""state""', '$', '(?P<county>.*?), (?P<state_code>[A-Z]{2}$))\n', '.*', 'state_code', '$']"
1017;;0;21315199;19482970.0;2;6;;;;['[column for column in my_dataframe]\n']
1018;;4;21320011;21319929.0;2;35;;;;"['in', ""In [11]: s = pd.Series(list('abc'))\n\nIn [12]: s\nOut[12]: \n0    a\n1    b\n2    c\ndtype: object\n\nIn [13]: 1 in s\nOut[13]: True\n\nIn [14]: 'a' in s\nOut[14]: False\n"", ""In [21]: s.unique()\nOut[21]: array(['a', 'b', 'c'], dtype=object)\n\nIn [22]: 'a' in s.unique()\nOut[22]: True\n"", ""In [23]: set(s)\nOut[23]: {'a', 'b', 'c'}\n\nIn [24]: 'a' in set(s)\nOut[24]: True\n"", ""In [31]: s.values\nOut[31]: array(['a', 'b', 'c'], dtype=object)\n\nIn [32]: 'a' in s.values\nOut[32]: True\n""]"
1019;;4;21361994;21360361.0;2;61;;;;['IPython.display', '%matplotlib inline\nimport time\nimport pylab as pl\nfrom IPython import display\nfor i in range(10):\n    pl.plot(pl.randn(100))\n    display.clear_output(wait=True)\n    display.display(pl.gcf())\n    time.sleep(1.0)\n']
1020;;6;21415990;21415661.0;2;61;;;;"[""(a['x']==1) and (a['y']==10)\n"", ""(a['x']==1)"", ""(a['y']==10)"", 'ValueError: The truth value of an array is ambiguous. Use a.empty, a.any() or a.all().\n', 'empty()', 'all()', 'any()', '&', ""(a['x']==1) & (a['y']==10)\n"", '&', '==', ""a['x']==1 & a['y']==10"", ""a['x'] == (1 & a['y']) == 10"", ""(a['x'] == (1 & a['y'])) and ((1 & a['y']) == 10)"", 'Series and Series', 'and', 'ValueError']"
1021;;1;21441621;21441259.0;2;47;;;;"['pd.cut', '>>> df.groupby(pd.cut(df[""B""], np.arange(0, 1.0+0.155, 0.155))).sum()\n                      A         B\nB                                \n(0, 0.155]     2.775458  0.246394\n(0.155, 0.31]  1.123989  0.471618\n(0.31, 0.465]  2.051814  1.882763\n(0.465, 0.62]  2.277960  1.528492\n(0.62, 0.775]  1.577419  2.810723\n(0.775, 0.93]  0.535100  1.694955\n(0.93, 1.085]       NaN       NaN\n\n[7 rows x 2 columns]\n']"
1022;;2;21487560;21487329.0;2;116;;;;"['df.plot()', 'matplotlib.axes.AxesSubplot', 'In [4]: ax = df2.plot(lw=2,colormap=\'jet\',marker=\'.\',markersize=10,title=\'Video streaming dropout by category\')\n\nIn [6]: ax.set_xlabel(""x label"")\nOut[6]: <matplotlib.text.Text at 0x10e0af2d0>\n\nIn [7]: ax.set_ylabel(""y label"")\nOut[7]: <matplotlib.text.Text at 0x10e0ba1d0>\n', 'ax.set(xlabel=""x label"", ylabel=""y label"")', ""df2.index.name = 'x label'""]"
1023;;0;21487868;21487329.0;2;14;;;;"[""import matplotlib.pyplot as plt \nimport pandas as pd\n\nplt.figure()\nvalues = [[1,2], [2,5]]\ndf2 = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])\ndf2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')\nplt.xlabel('xlabel')\nplt.ylabel('ylabel')\nplt.show()\n""]"
1024;;1;21607530;21606987.0;2;48;;;;['rename', 'str.strip()', 'In [5]: df\nOut[5]: \n   Year  Month   Value\n0     1       2      3\n\n[1 rows x 3 columns]\n\nIn [6]: df.rename(columns=lambda x: x.strip())\nOut[6]: \n   Year  Month  Value\n0     1      2      3\n\n[1 rows x 3 columns]\n']
1025;;2;21608417;21608228.0;2;52;;;;"[""df.ix[df.my_channel > 20000, 'my_channel'] = 0\n""]"
1026;;1;21655221;21654635.0;2;14;;;;"['plt.scatter', 'df = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range(\'2010-01-01\', freq = \'M\', periods = 10), columns = (\'one\', \'two\', \'three\'))\ndf[\'key1\'] = (4,4,4,6,6,6,8,8,8,8)\nfig1 = plt.figure(1)\nax1 = fig1.add_subplot(111)\nx=ax1.scatter(df[\'one\'], df[\'two\'], marker = \'o\', c = df[\'key1\'], alpha = 0.8)\n\nccm=x.get_cmap()\ncircles=[Line2D(range(1), range(1), color=\'w\', marker=\'o\', markersize=10, markerfacecolor=item) for item in ccm((array([4,6,8])-4.0)/4)]\nleg = plt.legend(circles, [\'4\',\'6\',\'8\'], loc = ""center left"", bbox_to_anchor = (1, 0.5), numpoints = 1)\n']"
1027;;4;21655256;21654635.0;2;55;;;;"['scatter', 'key1', 'plot', ""import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nnp.random.seed(1974)\n\n# Generate Data\nnum = 20\nx, y = np.random.random((2, num))\nlabels = np.random.choice(['a', 'b', 'c'], num)\ndf = pd.DataFrame(dict(x=x, y=y, label=labels))\n\ngroups = df.groupby('label')\n\n# Plot\nfig, ax = plt.subplots()\nax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\nfor name, group in groups:\n    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=name)\nax.legend()\n\nplt.show()\n"", 'pandas', 'rcParams', ""import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nnp.random.seed(1974)\n\n# Generate Data\nnum = 20\nx, y = np.random.random((2, num))\nlabels = np.random.choice(['a', 'b', 'c'], num)\ndf = pd.DataFrame(dict(x=x, y=y, label=labels))\n\ngroups = df.groupby('label')\n\n# Plot\nplt.rcParams.update(pd.tools.plotting.mpl_stylesheet)\ncolors = pd.tools.plotting._get_standard_colors(len(groups), color_type='random')\n\nfig, ax = plt.subplots()\nax.set_color_cycle(colors)\nax.margins(0.05)\nfor name, group in groups:\n    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=name)\nax.legend(numpoints=1, loc='upper left')\n\nplt.show()\n""]"
1028;;5;21709413;15705630.0;2;18;;;;"[""df = pd.DataFrame({\n    'sp' : ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n    'mt' : ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n    'val' : ['a', 'n', 'cb', 'mk', 'bg', 'dgb', 'rd', 'cb', 'uyi'],\n    'count' : [3,2,5,8,10,1,2,2,7]\n    })\n\ndf_grouped = df.groupby(['sp', 'mt']).agg({'count':'max'})\n\ndf_grouped = df_grouped.reset_index()\n\ndf_grouped = df_grouped.rename(columns={'count':'count_max'})\n\ndf = pd.merge(df, df_grouped, how='left', on=['sp', 'mt'])\n\ndf = df[df['count'] == df['count_max']]\n""]"
1029;;3;21734254;21733893.0;2;40;;;;"[""# Set a default value\ndf['Age_Group'] = '<40'\n# Set Age_Group value for all row indexes which Age are greater than 40\ndf['Age_Group'][df['Age'] > 40] = '>40'\n# Set Age_Group value for all row indexes which Age are greater than 18 and < 40\ndf['Age_Group'][(df['Age'] > 18) & (df['Age'] < 40)] = '>18'\n# Set Age_Group value for all row indexes which Age are less than 18\ndf['Age_Group'][df['Age'] < 18] = '<18'\n"", "">>> d = {'Age' : pd.Series([36., 42., 6., 66., 38.]) }\n>>> df = pd.DataFrame(d)\n>>> df\n   Age\n0   36\n1   42\n2    6\n3   66\n4   38\n>>> df['Age_Group'] = '<40'\n>>> df['Age_Group'][df['Age'] > 40] = '>40'\n>>> df['Age_Group'][(df['Age'] > 18) & (df['Age'] < 40)] = '>18'\n>>> df['Age_Group'][df['Age'] < 18] = '<18'\n>>> df\n   Age Age_Group\n0   36       >18\n1   42       >40\n2    6       <18\n3   66       >40\n4   38       >18\n"", "">>> df['Age_Group'] = '<40'\n>>> df.loc[df['Age'] < 40,'Age_Group'] = '<40'\n>>> df.loc[(df['Age'] > 18) & (df['Age'] < 40), 'Age_Group'] = '>18'\n>>> df.loc[df['Age'] < 18,'Age_Group'] = '<18'\n>>> df\n   Age Age_Group\n0   36       >18\n1   42       <40\n2    6       <18\n3   66       <40\n4   38       >18\n""]"
1030;;2;21787325;21786490.0;2;53;;;;"['df1', 'df2', 'df3', ""In [33]: s1 = pd.merge(df1, df2, how='left', on=['Year', 'Week', 'Colour'])\n"", ""In [39]: df = pd.merge(s1, df3[['Week', 'Colour', 'Val3']],\n                       how='left', on=['Week', 'Colour'])\n\nIn [40]: df\nOut[40]: \n   Year Week Colour  Val1  Val2 Val3\n0  2014    A    Red    50   NaN  NaN\n1  2014    B    Red    60   NaN   60\n2  2014    B  Black    70   100   10\n3  2014    C    Red    10    20  NaN\n4  2014    D  Green    20   NaN   20\n\n[5 rows x 6 columns]\n""]"
1031;;4;21800319;21800169.0;2;120;;;;"['df.iloc[i]', 'ith', 'df', 'i', 'i', 'index', ""df[df['BoolCol'] == True].index.tolist()\n"", ""df[df['BoolCol']].index.tolist()\n"", ""df = pd.DataFrame({'BoolCol': [True, False, False, True, True]},\n       index=[10,20,30,40,50])\n\nIn [53]: df\nOut[53]: \n   BoolCol\n10    True\n20   False\n30   False\n40    True\n50    True\n\n[5 rows x 1 columns]\n\nIn [54]: df[df['BoolCol']].index.tolist()\nOut[54]: [10, 40, 50]\n"", ""In [56]: idx = df[df['BoolCol']].index.tolist()\n\nIn [57]: idx\nOut[57]: [10, 40, 50]\n"", 'loc', 'iloc', 'In [58]: df.loc[idx]\nOut[58]: \n   BoolCol\n10    True\n40    True\n50    True\n\n[3 rows x 1 columns]\n', 'loc', ""In [55]: df.loc[df['BoolCol']]\nOut[55]: \n   BoolCol\n10    True\n40    True\n50    True\n\n[3 rows x 1 columns]\n"", 'mask', 'np.flatnonzero', ""In [110]: np.flatnonzero(df['BoolCol'])\nOut[112]: array([0, 3, 4])\n"", 'df.iloc', ""In [113]: df.iloc[np.flatnonzero(df['BoolCol'])]\nOut[113]: \n   BoolCol\n10    True\n40    True\n50    True\n""]"
1032;;7;21916253;13703720.0;2;109;;;;[]
1033;;7;21942746;13445241.0;2;60;;;;"['df.replace()', ""df = pd.DataFrame([\n    [-0.532681, 'foo', 0],\n    [1.490752, 'bar', 1],\n    [-1.387326, 'foo', 2],\n    [0.814772, 'baz', ' '],     \n    [-0.222552, '   ', 4],\n    [-1.176781,  'qux', '  '],         \n], columns='A B C'.split(), index=pd.date_range('2000-01-01','2000-01-06'))\n\nprint df.replace(r'\\s+', np.nan, regex=True)\n"", '                   A    B   C\n2000-01-01 -0.532681  foo   0\n2000-01-02  1.490752  bar   1\n2000-01-03 -1.387326  foo   2\n2000-01-04  0.814772  baz NaN\n2000-01-05 -0.222552  NaN   4\n2000-01-06 -1.176781  qux NaN\n']"
1034;;1;22006514;22005911.0;2;80;;;;"[""total_rows['ColumnID'] = total_rows['ColumnID'].astype(str)\n"", 'to_json', 'In [11]: df = pd.DataFrame([[\'A\', 2], [\'A\', 4], [\'B\', 6]])\n\nIn [12]: df.to_json()\nOut[12]: \'{""0"":{""0"":""A"",""1"":""A"",""2"":""B""},""1"":{""0"":2,""1"":4,""2"":6}}\'\n\nIn [13]: df[0].to_json()\nOut[13]: \'{""0"":""A"",""1"":""A"",""2"":""B""}\'\n']"
1035;;0;22018873;10665889.0;2;28;;;;['>>> data.iloc[:,[0,3]]\n', '          a         d\n0  0.883283  0.100975\n1  0.614313  0.221731\n2  0.438963  0.224361\n3  0.466078  0.703347\n4  0.955285  0.114033\n5  0.268443  0.416996\n6  0.613241  0.327548\n7  0.370784  0.359159\n8  0.692708  0.659410\n9  0.806624  0.875476\n']
1036;;2;22084742;22084338.0;2;41;;;;"[""import timeit\n\nsetup = '''\nimport numpy, pandas\ndf = pandas.DataFrame(numpy.zeros(shape=[10, 1000]))\ndictionary = df.to_dict()\n'''\n\n# f = ['value = dictionary[5][5]', 'value = df.loc[5, 5]', 'value = df.iloc[5, 5]']\nf = ['value = [val[5] for col,val in dictionary.items()]', 'value = df.loc[5]', 'value = df.iloc[5]']\n\nfor func in f:\n    print(func)\n    print(min(timeit.Timer(func, setup).repeat(3, 100000)))\n"", 'value = [val[5] for col,val in dictionary.iteritems()]\n25.5416321754\nvalue = df.loc[5]\n5.68071913719\nvalue = df.iloc[5]\n4.56006002426\n', 'df.iloc', ""df.loc['2000-1-1':'2000-3-31']\n""]"
1037;;10;22086347;22086116.0;2;40;;;;"['&', '()', ""males = df[(df[Gender]=='Male') & (df[Year]==2014)]\n"", 'dict', ""from collections import defaultdict\ndic={}\nfor g in ['male', 'female']:\n  dic[g]=defaultdict(dict)\n  for y in [2013, 2014]:\n    dic[g][y]=df[(df[Gender]==g) & (df[Year]==y)] #store the DataFrames to a dict of dict\n"", 'getDF', ""def getDF(dic, gender, year):\n  return dic[gender][year]\n\nprint genDF(dic, 'male', 2014)\n""]"
1038;;0;22127685;22127569.0;2;30;;;;"['.pivot', "">>> origin.pivot(index='label', columns='type')['value']\ntype   a  b  c\nlabel         \nx      1  2  3\ny      4  5  6\nz      7  8  9\n\n[3 rows x 3 columns]\n"", 'pivot_table', "">>> origin.pivot_table(values='value', index='label', columns='type')\n       value      \ntype       a  b  c\nlabel             \nx          1  2  3\ny          4  5  6\nz          7  8  9\n\n[3 rows x 3 columns]\n"", '.groupby', '.unstack', "">>> origin.groupby(['label', 'type'])['value'].aggregate('mean').unstack()\ntype   a  b  c\nlabel         \nx      1  2  3\ny      4  5  6\nz      7  8  9\n\n[3 rows x 3 columns]\n""]"
1039;;7;22137890;22137723.0;2;38;;;;"[""In [ 9]: import locale\n\nIn [10]: from locale import atof\n\nIn [11]: locale.setlocale(locale.LC_NUMERIC, '')\nOut[11]: 'en_GB.UTF-8'\n\nIn [12]: df.applymap(atof)\nOut[12]:\n      0        1\n0  1200  4200.00\n1  7000    -0.03\n2     5     0.00\n"", ""df.read_csv('foo.tsv', sep='\\t', thousands=',')\n""]"
1040;;4;22149930;22149584.0;2;130;;;;['axis=0', 'numpy.mean', 'axis', 'numpy.mean', 'axis=0', 'axis=1', '+------------+---------+--------+\n|            |  A      |  B     |\n+------------+---------+---------\n|      0     | 0.626386| 1.52325|----axis=1----->\n+------------+---------+--------+\n                |         |\n                | axis=0  |\n                ?         ?\n']
1041;;2;22166224;10636024.0;2;6;;;;['DataFrameModel', 'DataFrameWidget', 'from pandas.sandbox.qtpandas import DataFrameModel, DataFrameWidget\n']
1042;;10;22181298;22180993.0;2;28;;;;"['@app.route(\'/analysis/<filename>\')\ndef analysis(filename):\n    x = pd.DataFrame(np.random.randn(20, 5))\n    return render_template(""analysis.html"", name=filename, data=x.to_html())\n                                                                # ^^^^^^^^^\n', '{% extends ""base.html"" %}\n{% block content %}\n<h1>{{name}}</h1>\n{{data | safe}}\n{% endblock %}\n']"
1043;;1;22211821;22211737.0;2;40;;;;"['sort_index', 'inplace=True', ""import pandas as pd\ndf = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150], columns=['A'])\ndf.sort_index(inplace=True)\nprint(df.to_string())\n"", '     A\n1    4\n29   2\n100  1\n150  5\n234  3\n']"
1044;;0;22221272;22219004.0;2;6;;;;"['groupby', 'pd.DataFrame', "" L = ['A','A','B','B','B','C']\n N = [1,2,5,5,4,6]\n\n import pandas as pd\n df = pd.DataFrame(zip(L,N),columns = list('LN'))\n\n\n groups = df.groupby(df.L)\n\n groups.groups\n      {'A': [0, 1], 'B': [2, 3, 4], 'C': [5]}\n"", "" groups.get_group('A')\n\n     L  N\n  0  A  1\n  1  A  2\n\n  groups.get_group('B')\n\n     L  N\n  2  B  5\n  3  B  5\n  4  B  4\n""]"
1045;;9;22221675;22219004.0;2;62;;;;"['groupby', 'apply', 'list', ""In [1]:\n# create the dataframe    \ndf = pd.DataFrame( {'a':['A','A','B','B','B','C'], 'b':[1,2,5,5,4,6]})\ndf\nOut[1]:\n   a  b\n0  A  1\n1  A  2\n2  B  5\n3  B  5\n4  B  4\n5  C  6\n\n[6 rows x 2 columns]\n\nIn [76]:\ndf.groupby('a')['b'].apply(list)\n\nOut[76]:\na\nA       [1, 2]\nB    [5, 5, 4]\nC          [6]\nName: b, dtype: object\n""]"
1046;;1;22233719;22233488.0;2;97;;;;"['MultiIndex.droplevel', '>>> cols = pd.MultiIndex.from_tuples([(""a"", ""b""), (""a"", ""c"")])\n>>> df = pd.DataFrame([[1,2], [3,4]], columns=cols)\n>>> df\n   a   \n   b  c\n0  1  2\n1  3  4\n\n[2 rows x 2 columns]\n>>> df.columns = df.columns.droplevel()\n>>> df\n   b  c\n0  1  2\n1  3  4\n\n[2 rows x 2 columns]\n']"
1047;;1;22233851;22180993.0;2;15;;;;"['@app.route(\'/analysis/<filename>\')\ndef analysis(filename):\n    x = pd.DataFrame(np.random.randn(20, 5))\n    return render_template(""analysis.html"", name=filename, data=x)\n', '{% extends ""base.html"" %}\n{% block content %}\n<h1>{{name}}</h1>\n{{ data.to_html(classes=""table table-striped"") | safe}}\n{% endblock %}\n']"
1048;;0;22235393;22235245.0;2;42;;;;['describe', 'In [43]:\n\ndf.describe()\n\nOut[43]:\n\n       shopper_num is_martian  number_of_items  count_pineapples\ncount      14.0000         14        14.000000                14\nmean        7.5000          0         3.357143                 0\nstd         4.1833          0         6.452276                 0\nmin         1.0000      False         0.000000                 0\n25%         4.2500          0         0.000000                 0\n50%         7.5000          0         0.000000                 0\n75%        10.7500          0         3.500000                 0\nmax        14.0000      False        22.000000                 0\n\n[8 rows x 4 columns]\n', 'In [47]:\n\ndf.describe().transpose()\n\nOut[47]:\n\n                 count      mean       std    min   25%  50%    75%    max\nshopper_num         14       7.5    4.1833      1  4.25  7.5  10.75     14\nis_martian          14         0         0  False     0    0      0  False\nnumber_of_items     14  3.357143  6.452276      0     0    0    3.5     22\ncount_pineapples    14         0         0      0     0    0      0      0\n\n[4 rows x 8 columns]\n']
1049;;3;22238380;13682044.0;2;14;;;;"[""data['result'] = data['result'].map(lambda x: str(x)[:-1])\n"", ""data['result'] = data['result'].map(lambda x: str(x)[2:])\n""]"
1050;;8;22341390;22341271.0;2;109;;;;"['.tolist()', 'from pandas import *\n\nd = {\'one\' : Series([1., 2., 3.], index=[\'a\', \'b\', \'c\']),\n    \'two\' : Series([1., 2., 3., 4.], index=[\'a\', \'b\', \'c\', \'d\'])}\n\ndf = DataFrame(d)\n\n#print df\n\nprint ""DF"", type(df[\'one\']), ""\\n"", df[\'one\']\n\ndfList = df[\'one\'].tolist()\n\nprint ""DF list"", dfList, type(dfList)\n', 'my_list = df[""cluster""].tolist()']"
1051;;6;22391554;22391433.0;2;73;;;;"['groupby', 'count', ""In [37]:\ndf = pd.DataFrame({'a':list('abssbab')})\ndf.groupby('a').count()\n\nOut[37]:\n\n   a\na   \na  2\nb  3\ns  2\n\n[3 rows x 1 columns]\n"", 'value_counts()', ""In [38]:\ndf['a'].value_counts()\n\nOut[38]:\n\nb    3\na    2\ns    2\ndtype: int64\n"", 'transform', ""In [41]:\ndf['freq'] = df.groupby('a')['a'].transform('count')\ndf\n\nOut[41]:\n\n   a freq\n0  a    2\n1  b    3\n2  s    2\n3  s    2\n4  b    3\n5  a    2\n6  b    3\n\n[7 rows x 2 columns]\n""]"
1052;;0;22455322;22403469.0;2;21;;;;['NaN values']
1053;;1;22471217;22470690.0;2;13;;;;"[""In [11]: df = pd.DataFrame([[1, 2.3456, 'c']])\n\nIn [12]: df.dtypes\nOut[12]: \n0      int64\n1    float64\n2     object\ndtype: object\n\nIn [13]: msk = df.dtypes == np.float64  # or object, etc.\n\nIn [14]: msk\nOut[14]: \n0    False\n1     True\n2    False\ndtype: bool\n"", 'In [15]: df.loc[:, msk]\nOut[15]: \n        1\n0  2.3456\n', 'In [16]: np.round(df.loc[:, msk], 2)\nOut[16]: \n      1\n0  2.35\n\nIn [17]: df.loc[:, msk] = np.round(df.loc[:, msk], 2)\n\nIn [18]: df\nOut[18]: \n   0     1  2\n0  1  2.35  c\n']"
1054;;2;22475141;22470690.0;2;126;;;;"['groupby', '>>> df = pd.DataFrame([[1, 2.3456, \'c\', \'d\', 78]], columns=list(""ABCDE""))\n>>> df\n   A       B  C  D   E\n0  1  2.3456  c  d  78\n\n[1 rows x 5 columns]\n>>> df.dtypes\nA      int64\nB    float64\nC     object\nD     object\nE      int64\ndtype: object\n>>> g = df.columns.to_series().groupby(df.dtypes).groups\n>>> g\n{dtype(\'int64\'): [\'A\', \'E\'], dtype(\'float64\'): [\'B\'], dtype(\'O\'): [\'C\', \'D\']}\n>>> {k.name: v for k, v in g.items()}\n{\'object\': [\'C\', \'D\'], \'int64\': [\'A\', \'E\'], \'float64\': [\'B\']}\n']"
1055;;4;22484249;22483588.0;2;86;;;;['ax', 'import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\n\ndf1.plot(ax=axes[0,0])\ndf2.plot(ax=axes[0,1])\n...\n', 'axes', 'axes', 'sharex=True', 'plt.subplots']
1056;;1;22496075;16852911.0;2;12;;;;"[""dfcsv = pd.read_csv('xyz.csv', parse_dates=[0])"", ', index_col=0']"
1057;;0;22543333;22543208.0;2;21;;;;"['style', 'ggplot', ""from matplotlib import pyplot as plt\nplt.style.use('ggplot')\n"", ""pd.options.display.mpl_style = 'default'\n"", 'import seaborn']"
1058;;6;22546459;22546425.0;2;40;;;;"['&', '|', ""foo = df.ix[(df['column1']==value) | (df['columns2'] == 'b') | (df['column3'] == 'c')]\n"", 'and', 'or', 'np.all', 'np.any']"
1059;;0;22547347;22546425.0;2;11;;;;"['DataFrame.isin()', 'DataFrame.any()', ""In [27]: n = 10\n\nIn [28]: df = DataFrame(randint(4, size=(n, 2)), columns=list('ab'))\n\nIn [29]: df\nOut[29]:\n   a  b\n0  0  0\n1  1  1\n2  1  1\n3  2  3\n4  2  3\n5  0  2\n6  1  2\n7  3  0\n8  1  1\n9  2  2\n\n[10 rows x 2 columns]\n\nIn [30]: df.isin([1, 2])\nOut[30]:\n       a      b\n0  False  False\n1   True   True\n2   True   True\n3   True  False\n4   True  False\n5  False   True\n6   True   True\n7  False  False\n8   True   True\n9   True   True\n\n[10 rows x 2 columns]\n\nIn [31]: df.isin([1, 2]).any(1)\nOut[31]:\n0    False\n1     True\n2     True\n3     True\n4     True\n5     True\n6     True\n7    False\n8     True\n9     True\ndtype: bool\n\nIn [32]: df.loc[df.isin([1, 2]).any(1)]\nOut[32]:\n   a  b\n1  1  1\n2  1  1\n3  2  3\n4  2  3\n5  0  2\n6  1  2\n8  1  1\n9  2  2\n\n[8 rows x 2 columns]\n""]"
1060;;4;22553757;22551403.0;2;73;;;;['nms.dropna(thresh=2)\n', 'NaN', 'NaN', 'In [87]:\n\nnms\nOut[87]:\n  movie    name  rating\n0   thg    John       3\n1   thg     NaN       4\n3   mol  Graham     NaN\n4   lob     NaN     NaN\n5   lob     NaN     NaN\n\n[5 rows x 3 columns]\nIn [89]:\n\nnms = nms.dropna(thresh=2)\nIn [90]:\n\nnms[nms.name.notnull()]\nOut[90]:\n  movie    name  rating\n0   thg    John       3\n3   mol  Graham     NaN\n\n[2 rows x 3 columns]\n', 'dropna', 'nms[nms.name.notnull()]\n']
1061;;7;22588340;22588316.0;2;10;;;;"['re.sub()', 'value = re.sub(r""[^0-9]+"", """", value)\n']"
1062;;1;22591024;22588316.0;2;53;;;;"['Series.str.replace', ""import pandas as pd\n\ndf = pd.DataFrame(['$40,000*','$40000 conditions attached'], columns=['P'])\nprint(df)\n#                             P\n# 0                    $40,000*\n# 1  $40000 conditions attached\n\ndf['P'] = df['P'].str.replace(r'\\D+', '').astype('int')\nprint(df)\n"", '       P\n0  40000\n1  40000\n', '\\D']"
1063;;3;22591267;22591174.0;2;49;;;;"['df1', 'df1 = df[(df.a != -1) & (df.b != -1)]\n', 'df.a', 'df.b', 'df2', 'df2 = df[(df.a != -1) | (df.b != -1)]\n', 'df.a', 'df.b', ""df['a'][1] = -1"", '.loc', '.iloc']"
1064;;3;22596982;13411544.0;2;135;;;;['df.drop([Column Name or list],inplace=True,axis=1)\n']
1065;;1;22605281;22604564.0;2;138;;;;"['StringIO', 'pandas.read_csv', 'import sys\nif sys.version_info[0] < 3: \n    from StringIO import StringIO\nelse:\n    from io import StringIO\n\nimport pandas as pd\n\nTESTDATA=StringIO(""""""col1;col2;col3\n    1;4.4;99\n    2;4.5;200\n    3;4.7;65\n    4;3.2;140\n    """""")\n\ndf = pd.read_csv(TESTDATA, sep="";"")\n']"
1066;;6;22650075;22649693.0;2;34;;;;"[""> df = pd.DataFrame({'a':[0,0,1,1], 'b':[0,1,0,1]})\n> df = df[(df.T != 0).any()]\n> df\n   a  b\n1  0  1\n2  1  0\n3  1  1\n""]"
1067;;3;22650162;22649693.0;2;32;;;;['df.loc[~(df==0).all(axis=1)]\n', 'df.loc[(df!=0).any(axis=1)]\n']
1068;;2;22653050;13187778.0;2;22;;;;"['In [8]: df\nOut[8]: \n          A         B         C\n0 -0.982726  0.150726  0.691625\n1  0.617297 -0.471879  0.505547\n2  0.417123 -1.356803 -1.013499\n3 -0.166363 -0.957758  1.178659\n4 -0.164103  0.074516 -0.674325\n5 -0.340169 -0.293698  1.231791\n6 -1.062825  0.556273  1.508058\n7  0.959610  0.247539  0.091333\n\n[8 rows x 3 columns]\n\nIn [9]: df.reset_index().values\nOut[9]:\narray([[ 0.        , -0.98272574,  0.150726  ,  0.69162512],\n       [ 1.        ,  0.61729734, -0.47187926,  0.50554728],\n       [ 2.        ,  0.4171228 , -1.35680324, -1.01349922],\n       [ 3.        , -0.16636303, -0.95775849,  1.17865945],\n       [ 4.        , -0.16410334,  0.0745164 , -0.67432474],\n       [ 5.        , -0.34016865, -0.29369841,  1.23179064],\n       [ 6.        , -1.06282542,  0.55627285,  1.50805754],\n       [ 7.        ,  0.95961001,  0.24753911,  0.09133339]])\n', ""In [10]: df.reset_index().values.ravel().view(dtype=[('index', int), ('A', float), ('B', float), ('C', float)])\nOut[10]:\narray([( 0, -0.98272574,  0.150726  ,  0.69162512),\n       ( 1,  0.61729734, -0.47187926,  0.50554728),\n       ( 2,  0.4171228 , -1.35680324, -1.01349922),\n       ( 3, -0.16636303, -0.95775849,  1.17865945),\n       ( 4, -0.16410334,  0.0745164 , -0.67432474),\n       ( 5, -0.34016865, -0.29369841,  1.23179064),\n       ( 6, -1.06282542,  0.55627285,  1.50805754),\n       ( 7,  0.95961001,  0.24753911,  0.09133339),\n       dtype=[('index', '<i8'), ('A', '<f8'), ('B', '<f8'), ('C', '<f8')])\n""]"
1069;;0;22657894;11346283.0;2;83;;;;"[""df = df.rename(columns=lambda x: x.replace('$', ''))\n"", ""df.rename(columns=lambda x: x.replace('$', ''), inplace=True)\n""]"
1070;;0;22674279;19237878.0;2;14;;;;"[""df[(df.Product == p_id) & (df.Time> start_time) & (df.Time < end_time)][['Time','Product']]\n"", 'data.loc', 'query']"
1071;;2;22676213;22676081.0;2;27;;;;"['join', ""import pandas as pd\nleft = pd.DataFrame({'key': ['foo', 'bar'], 'val': [1, 2]}).set_index('key')\nright = pd.DataFrame({'key': ['foo', 'bar'], 'val': [4, 5]}).set_index('key')\nleft.join(right, lsuffix='_l', rsuffix='_r')\n\n     val_l  val_r\nkey            \nfoo      1      4\nbar      2      5\n"", 'merge', ""left = pd.DataFrame({'key': ['foo', 'bar'], 'val': [1, 2]})\nright = pd.DataFrame({'key': ['foo', 'bar'], 'val': [4, 5]})\nleft.merge(right, on=('key'), suffixes=('_l', '_r'))\n\n   key  val_l  val_r\n0  foo      1      4\n1  bar      2      5\n""]"
1072;;3;22697903;22697773.0;2;41;;;;['dtype', 'for y in agg.columns:\n    if(agg[y].dtype == np.float64 or agg[y].dtype == np.int64):\n          treat_numeric(agg[y])\n    else:\n          treat_str(agg[y])\n']
1073;;0;22719983;11858472.0;2;9;;;;"[""df['bar'] = df['bar'].str.cat(df['foo'].values.astype(str), sep=' is ')\n""]"
1074;;1;22798849;13999850.0;2;45;;;;"[""df.to_csv(filename, date_format='%Y%m%d')\n""]"
1075;;8;22845857;22787209.0;2;31;;;;"['import pandas as pd\nimport matplotlib.cm as cm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_clustered_stacked(dfall, labels=None, title=""multiple stacked bar plot"",  H=""/"", **kwargs):\n    """"""Given a list of dataframes, with identical columns and index, create a clustered stacked bar plot. \nlabels is a list of the names of the dataframe, used for the legend\ntitle is a string for the title of the plot\nH is the hatch used for identification of the different dataframe""""""\n\n    n_df = len(dfall)\n    n_col = len(dfall[0].columns) \n    n_ind = len(dfall[0].index)\n    axe = plt.subplot(111)\n\n    for df in dfall : # for each data frame\n        axe = df.plot(kind=""bar"",\n                      linewidth=0,\n                      stacked=True,\n                      ax=axe,\n                      legend=False,\n                      grid=False,\n                      **kwargs)  # make bar plots\n\n    h,l = axe.get_legend_handles_labels() # get the handles we want to modify\n    for i in range(0, n_df * n_col, n_col): # len(h) = n_col * n_df\n        for j, pa in enumerate(h[i:i+n_col]):\n            for rect in pa.patches: # for each index\n                rect.set_x(rect.get_x() + 1 / float(n_df + 1) * i / float(n_col))\n                rect.set_hatch(H * int(i / n_col)) #edited part     \n                rect.set_width(1 / float(n_df + 1))\n\n    axe.set_xticks((np.arange(0, 2 * n_ind, 2) + 1 / float(n_df + 1)) / 2.)\n    axe.set_xticklabels(df.index, rotation = 0)\n    axe.set_title(title)\n\n    # Add invisible data to add another legend\n    n=[]        \n    for i in range(n_df):\n        n.append(axe.bar(0, 0, color=""gray"", hatch=H * i))\n\n    l1 = axe.legend(h[:n_col], l[:n_col], loc=[1.01, 0.5])\n    if labels is not None:\n        l2 = plt.legend(n, labels, loc=[1.01, 0.1]) \n    axe.add_artist(l1)\n    return axe\n\n# create fake dataframes\ndf1 = pd.DataFrame(np.random.rand(4, 5),\n                   index=[""A"", ""B"", ""C"", ""D""],\n                   columns=[""I"", ""J"", ""K"", ""L"", ""M""])\ndf2 = pd.DataFrame(np.random.rand(4, 5),\n                   index=[""A"", ""B"", ""C"", ""D""],\n                   columns=[""I"", ""J"", ""K"", ""L"", ""M""])\ndf3 = pd.DataFrame(np.random.rand(4, 5),\n                   index=[""A"", ""B"", ""C"", ""D""], \n                   columns=[""I"", ""J"", ""K"", ""L"", ""M""])\n\n# Then, just call :\nplot_clustered_stacked([df1, df2, df3],[""df1"", ""df2"", ""df3""])\n', 'cmap', 'plot_clustered_stacked([df1, df2, df3],\n                       [""df1"", ""df2"", ""df3""],\n                       cmap=plt.cm.viridis)\n', 'df1[""Name""] = ""df1""\ndf2[""Name""] = ""df2""\ndf3[""Name""] = ""df3""\ndfall = pd.concat([pd.melt(i.reset_index(),\n                           id_vars=[""Name"", ""index""]) # transform in tidy format each df\n                   for i in [df1, df2, df3]],\n                   ignore_index=True)\n', 'dfall.set_index([""Name"", ""index"", ""variable""], inplace=1)\ndfall[""vcs""] = dfall.groupby(level=[""Name"", ""index""]).cumsum()\ndfall.reset_index(inplace=True) \n\n>>> dfall.head(6)\n  Name index variable     value       vcs\n0  df1     A        I  0.717286  0.717286\n1  df1     B        I  0.236867  0.236867\n2  df1     C        I  0.952557  0.952557\n3  df1     D        I  0.487995  0.487995\n4  df1     A        J  0.174489  0.891775\n5  df1     B        J  0.332001  0.568868\n', 'variable', 'c = [""blue"", ""purple"", ""red"", ""green"", ""pink""]\nfor i, g in enumerate(dfall.groupby(""variable"")):\n    ax = sns.barplot(data=g[1],\n                     x=""index"",\n                     y=""vcs"",\n                     hue=""Name"",\n                     color=c[i],\n                     zorder=-i, # so first bars stay on top\n                     edgecolor=""k"")\nax.legend_.remove() # remove the redundant legends \n']"
1076;;5;22898920;22898824.0;2;49;;;;"['.ix', '.loc', ""df.ix['2014-01-01':'2014-02-01']\n"", ""df[(df['date'] > '2013-01-01') & (df['date'] < '2013-02-01')]""]"
1077;;2;22920808;20853474.0;2;120;;;;['pip', 'sudo apt-get install python-pip\n', 'python-dateutil', 'sudo pip install python-dateutil\n']
1078;;2;22924683;22923775.0;2;29;;;;"[""import pandas\ndf = pandas.DataFrame(columns=['to','fr','ans'])\ndf.to = [pandas.Timestamp('2014-01-24 13:03:12.050000'), pandas.Timestamp('2014-01-27 11:57:18.240000'), pandas.Timestamp('2014-01-23 10:07:47.660000')]\ndf.fr = [pandas.Timestamp('2014-01-26 23:41:21.870000'), pandas.Timestamp('2014-01-27 15:38:22.540000'), pandas.Timestamp('2014-01-23 18:50:41.420000')]\n(df.fr-df.to).astype('timedelta64[h]')\n"", '0    58\n1     3\n2     8\ndtype: float64\n']"
1079;;1;22964673;22963263.0;2;38;;;;['d = pd.DataFrame(0, index=np.arange(len(data)), columns=feature_list)\n']
1080;;2;22974440;17097236.0;2;9;;;;"['where', ""data=data.where(data=='-', None) \n"", 'where']"
1081;;1;22992568;11350770.0;2;17;;;;"['df[\'stridx\']=df.index\ndf[df[\'stridx\'].str.contains(""Hello|Britain"")]\n']"
1082;;3;23088780;17095101.0;2;15;;;;"[""import pandas as pd\nimport io\n\ntexts = ['''\\\nid   Name   score                    isEnrolled                       Date\n111  Jack                            True              2013-05-01 12:00:00\n112  Nick   1.11                     False             2013-05-12 15:05:23\n     Zoe    4.12                     True                                  ''',\n\n         '''\\\nid   Name   score                    isEnrolled                       Date\n111  Jack   2.17                     True              2013-05-01 12:00:00\n112  Nick   1.21                     False                                \n     Zoe    4.12                     False             2013-05-01 12:00:00''']\n\n\ndf1 = pd.read_fwf(io.BytesIO(texts[0]), widths=[5,7,25,17,20], parse_dates=[4])\ndf2 = pd.read_fwf(io.BytesIO(texts[1]), widths=[5,7,25,17,20], parse_dates=[4])\n"", ""def report_diff(x):\n    return x[0] if x[0] == x[1] else '{} | {}'.format(*x)\n"", 'my_panel = pd.Panel(dict(df1=df1,df2=df2))\nprint my_panel.apply(report_diff, axis=0)\n\n#          id  Name        score    isEnrolled                       Date\n#0        111  Jack   nan | 2.17          True        2013-05-01 12:00:00\n#1        112  Nick  1.11 | 1.21         False  2013-05-12 15:05:23 | NaT\n#2  nan | nan   Zoe         4.12  True | False  NaT | 2013-05-01 12:00:00\n', 'from IPython.display import HTML\npd.options.display.max_colwidth = 500  # You need this, otherwise pandas\n#                          will limit your HTML strings to 50 characters\n\ndef report_diff(x):\n    if x[0]==x[1]:\n        return unicode(x[0].__str__())\n    elif pd.isnull(x[0]) and pd.isnull(x[1]):\n        return u\'<table style=""background-color:#00ff00;font-weight:bold;"">\'+\\\n            \'<tr><td>%s</td></tr><tr><td>%s</td></tr></table>\' % (\'nan\', \'nan\')\n    elif pd.isnull(x[0]) and ~pd.isnull(x[1]):\n        return u\'<table style=""background-color:#ffff00;font-weight:bold;"">\'+\\\n            \'<tr><td>%s</td></tr><tr><td>%s</td></tr></table>\' % (\'nan\', x[1])\n    elif ~pd.isnull(x[0]) and pd.isnull(x[1]):\n        return u\'<table style=""background-color:#0000ff;font-weight:bold;"">\'+\\\n            \'<tr><td>%s</td></tr><tr><td>%s</td></tr></table>\' % (x[0],\'nan\')\n    else:\n        return u\'<table style=""background-color:#ff0000;font-weight:bold;"">\'+\\\n            \'<tr><td>%s</td></tr><tr><td>%s</td></tr></table>\' % (x[0], x[1])\n\nHTML(my_panel.apply(report_diff, axis=0).to_html(escape=False))\n']"
1083;;6;23143081;23142967.0;2;48;;;;"[""df['dA'] = df['A'] - df['A'].shift(-1)\n""]"
1084;;1;23143110;23142967.0;2;17;;;;"['diff', '-1', 'periods', '>>> df = pd.DataFrame({""A"": [9, 4, 2, 1], ""B"": [12, 7, 5, 4]})\n>>> df[""dA""] = df[""A""].diff(-1)\n>>> df\n   A   B  dA\n0  9  12   5\n1  4   7   2\n2  2   5   1\n3  1   4 NaN\n\n[4 rows x 3 columns]\n']"
1085;;5;23200666;23199796.0;2;60;;;;"['boolean', 'numpy.array', ""df=pd.DataFrame({'Data':np.random.normal(size=200)})  #example dataset of normally distributed data. \ndf[np.abs(df.Data-df.Data.mean())<=(3*df.Data.std())] #keep only the ones that are within +3 to -3 standard deviations in the column 'Data'.\ndf[~(np.abs(df.Data-df.Data.mean())>(3*df.Data.std()))] #or if you prefer the other way around\n"", 'S=pd.Series(np.random.normal(size=200))\nS[~((S-S.mean()).abs()>3*S.std())]\n']"
1086;;2;23202269;23199796.0;2;51;;;;['df = pd.DataFrame(np.random.randn(100, 3))\n\nfrom scipy import stats\ndf[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n']
1087;;3;23235618;13413590.0;2;60;;;;"[""import pandas as pd\ndf = df[pd.notnull(df['EPS'])]\n""]"
1088;;1;23282290;23282130.0;2;47;;;;['pandas', 'import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\ndf = pd.DataFrame(data=np.random.normal(0, 1, (20, 10)))\n\npca = PCA(n_components=5)\npca.fit(df)\n', 'pca.components_ \n']
1089;;23;23296545;23296282.0;2;36;;;;"['inplace=True', '.loc/.ix/.iloc/.iat/.at', '.query', 'numexpr', 'chained indexing', ""df[df.C <= df.B].ix[:,'B':'E']\n"", ""df.ix[df.C <= df.B, 'B':'E']\n"", 'SettingWithCopyWarning']"
1090;;2;23307361;23307301.0;2;72;;;;"[""w['female'] = w['female'].map({'female': 1, 'male': 0})\n"", '""1""', '""0""', ""['female']"", ""'female'"", ""w['female']['female']""]"
1091;;0;23317595;23317342.0;2;33;;;;"[""foo = lambda x: pd.Series([i for i in reversed(x.split(','))])\nrev = df['City, State, Country'].apply(foo)\nprint rev\n\n      0    1        2\n0   HUN  NaN      NaN\n1   ESP  NaN      NaN\n2   GBR  NaN      NaN\n3   ESP  NaN      NaN\n4   FRA  NaN      NaN\n5   USA   ID      NaN\n6   USA   GA      NaN\n7   USA   NJ  Hoboken\n8   USA   NJ      NaN\n9   AUS  NaN      NaN\n"", ""rev.rename(columns={0:'Country',1:'State',2:'City'},inplace=True)\nrev = rev[['City','State','Country']]\nprint rev\n\n     City State Country\n0      NaN   NaN     HUN\n1      NaN   NaN     ESP\n2      NaN   NaN     GBR\n3      NaN   NaN     ESP\n4      NaN   NaN     FRA\n5      NaN    ID     USA\n6      NaN    GA     USA\n7  Hoboken    NJ     USA\n8      NaN    NJ     USA\n9      NaN   NaN     AUS\n""]"
1092;;0;23331659;23330654.0;2;6;;;;"[""df.ix[i, 'exp']=X"", ""df.loc[i, 'exp']=X"", ""df.ix[i]['ifor'] = x"", '-c:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead', 'DataFrame']"
1093;;0;23331896;10065051.0;2;15;;;;"['from sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy import Table\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom pandas import DataFrame\nimport datetime\n\n# We are connecting to an existing service\nengine = create_engine(\'dialect://user:pwd@host:port/db\', echo=False)\nSession = sessionmaker(bind=engine)\nsession = Session()\nBase = declarative_base()\n\n# And we want to query an existing table\ntablename = Table(\'tablename\', \n    Base.metadata, \n    autoload=True, \n    autoload_with=engine, \n    schema=\'ownername\')\n\n# These are the ""Where"" parameters, but I could as easily \n# create joins and limit results\nus = tablename.c.country_code.in_([\'US\',\'MX\'])\ndc = tablename.c.locn_name.like(\'%DC%\')\ndt = tablename.c.arr_date >= datetime.date.today() # Give me convenience or...\n\nq = session.query(tablename).\\\n            filter(us & dc & dt) # That\'s where the magic happens!!!\n\ndef querydb(query):\n    """"""\n    Function to execute query and return DataFrame.\n    """"""\n    df = DataFrame(query.all());\n    df.columns = [x[\'name\'] for x in query.column_descriptions]\n    return df\n\nquerydb(q)\n']"
1094;;2;23377155;23377108.0;2;24;;;;"['div', ""import numpy as np\nimport pandas as pd\nnp.random.seed(0)\ndf = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,\n               'office_id': list(range(1, 7)) * 2,\n               'sales': [np.random.randint(100000, 999999) for _ in range(12)]})\n\nstate_office = df.groupby(['state', 'office_id']).agg({'sales': 'sum'})\nstate = df.groupby(['state']).agg({'sales': 'sum'})\nstate_office.div(state, level='state') * 100\n\n\n                     sales\nstate office_id           \nAZ    2          16.981365\n      4          19.250033\n      6          63.768601\nCA    1          19.331879\n      3          33.858747\n      5          46.809373\nCO    1          36.851857\n      3          19.874290\n      5          43.273852\nWA    2          34.707233\n      4          35.511259\n      6          29.781508\n"", ""level='state'"", 'div', 'state']"
1095;;9;23377232;23377108.0;2;52;;;;"['groupby', 'groupby', 'state_office', 'sales', ""# From Paul H\nimport numpy as np\nimport pandas as pd\nnp.random.seed(0)\ndf = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,\n                   'office_id': list(range(1, 7)) * 2,\n                   'sales': [np.random.randint(100000, 999999)\n                             for _ in range(12)]})\nstate_office = df.groupby(['state', 'office_id']).agg({'sales': 'sum'})\n# Change: groupby state_office and divide by sum\nstate_pcts = state_office.groupby(level=0).apply(lambda x:\n                                                 100 * x / float(x.sum()))\n"", '                     sales\nstate office_id           \nAZ    2          16.981365\n      4          19.250033\n      6          63.768601\nCA    1          19.331879\n      3          33.858747\n      5          46.809373\nCO    1          36.851857\n      3          19.874290\n      5          43.273852\nWA    2          34.707233\n      4          35.511259\n      6          29.781508\n']"
1096;;2;23394497;10715965.0;2;37;;;;"['loc/ix', 'In [1]: se = pd.Series([1,2,3])\n\nIn [2]: se\nOut[2]: \n0    1\n1    2\n2    3\ndtype: int64\n\nIn [3]: se[5] = 5.\n\nIn [4]: se\nOut[4]: \n0    1.0\n1    2.0\n2    3.0\n5    5.0\ndtype: float64\n', ""In [1]: dfi = pd.DataFrame(np.arange(6).reshape(3,2),\n   .....:                 columns=['A','B'])\n   .....: \n\nIn [2]: dfi\nOut[2]: \n   A  B\n0  0  1\n1  2  3\n2  4  5\n\nIn [3]: dfi.loc[:,'C'] = dfi.loc[:,'A']\n\nIn [4]: dfi\nOut[4]: \n   A  B  C\n0  0  1  0\n1  2  3  2\n2  4  5  4\nIn [5]: dfi.loc[3] = 5\n\nIn [6]: dfi\nOut[6]: \n   A  B  C\n0  0  1  0\n1  2  3  2\n2  4  5  4\n3  5  5  5\n""]"
1097;;2;23464103;15411158.0;2;21;;;;['len(unique())', 'nunique()']
1098;;3;23478395;19851005.0;2;11;;;;"['FrozenList', 'Index.rename()', 'DataFrame.reindex()', ""df.index.names = ['Date']\n"", ""df = df.reindex(df.index.rename(['Date']))\n""]"
1099;;2;23549599;23549231.0;2;76;;;;"[""'g' in df.index\n""]"
1100;;1;23671390;23668427.0;2;42;;;;"[""# Merge multiple dataframes\ndf1 = pd.DataFrame(np.array([\n    ['a', 5, 9],\n    ['b', 4, 61],\n    ['c', 24, 9]]),\n    columns=['name', 'attr11', 'attr12'])\ndf2 = pd.DataFrame(np.array([\n    ['a', 5, 19],\n    ['b', 14, 16],\n    ['c', 4, 9]]),\n    columns=['name', 'attr21', 'attr22'])\ndf3 = pd.DataFrame(np.array([\n    ['a', 15, 49],\n    ['b', 4, 36],\n    ['c', 14, 9]]),\n    columns=['name', 'attr31', 'attr32'])\n\npd.merge(pd.merge(df1,df2,on='name'),df3,on='name')\n"", ""df1.merge(df2,on='name').merge(df3,on='name')\n""]"
1101;;0;23691692;16947336.0;2;13;;;;"['groups.size()', 'df = pandas.DataFrame({""a"": np.random.random(100), \n                    ""b"": np.random.random(100) + 10})\n\n# Bin the data frame by ""a"" with 10 bins...\ngroups = df.groupby(pandas.cut(df.a, 10))\n\n# Get the mean of b, binned by the values in a\nprint(groups.mean().b)\n']"
1102;;0;23739252;20637439.0;2;11;;;;[]
1103;;2;23741480;13148429.0;2;101;;;;"[""df = df[['mean', '0', '1', '2', '3']]\n"", 'cols = list(df.columns.values)\n', ""['0', '1', '2', '3', 'mean']\n""]"
1104;;6;23749057;23748995.0;2;88;;;;"['.values', 'numpy.array', '.tolist()', ""import pandas as pd\ndf = pd.DataFrame({'a':[1,3,5,7,4,5,6,4,7,8,9],\n                   'b':[3,5,6,2,4,6,7,8,7,8,9]})\n"", "">>> df['a'].values.tolist()\n[1, 3, 5, 7, 4, 5, 6, 4, 7, 8, 9]\n"", "">>> df['a'].tolist()\n[1, 3, 5, 7, 4, 5, 6, 4, 7, 8, 9]\n"", "">>> df['a'].drop_duplicates().values.tolist()\n[1, 3, 5, 7, 4, 6, 8, 9]\n>>> list(set(df['a'])) # as pointed out by EdChum\n[1, 3, 4, 5, 6, 7, 8, 9]\n""]"
1105;;4;23853569;23853553.0;2;34;;;;['read_csv(..., nrows=999999)\n', 'read_csv(..., skiprows=1000000, nrows=999999)\n']
1106;;5;23901625;10511024.0;2;121;;;;['%matplotlib inline\nimport matplotlib.pyplot as plt\n']
1107;;1;23922119;20937538.0;2;39;;;;"[""import pandas as pd\npd.options.display.float_format = '${:,.2f}'.format\ndf = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\n\n\nprint df.to_string(formatters={'cost':'${:,.2f}'.format})\n"", '        cost\nfoo  $123.46\nbar  $234.57\nbaz  $345.68\nquux $456.79\n']"
1108;;3;24040239;24039023.0;2;10;;;;"['NaN', 'df.index', 'Index', 'pandas', 'NaN', 'reindex', 'align', 'DataFrame.align()', ""In [7]: from pandas import DataFrame\n\nIn [8]: from numpy.random import randint\n\nIn [9]: df = DataFrame({'a': randint(3, size=10)})\n\nIn [10]:\n\nIn [10]: df\nOut[10]:\n   a\n0  0\n1  2\n2  0\n3  1\n4  0\n5  0\n6  0\n7  0\n8  0\n9  0\n\nIn [11]: s = df.a[:5]\n\nIn [12]: dfa, sa = df.align(s, axis=0)\n\nIn [13]: dfa\nOut[13]:\n   a\n0  0\n1  2\n2  0\n3  1\n4  0\n5  0\n6  0\n7  0\n8  0\n9  0\n\nIn [14]: sa\nOut[14]:\n0     0\n1     2\n2     0\n3     1\n4     0\n5   NaN\n6   NaN\n7   NaN\n8   NaN\n9   NaN\nName: a, dtype: float64\n""]"
1109;;0;24041761;24041436.0;2;31;;;;"[""is_none = df.set_index(['Company', 'date'], inplace=True)\ndf  # the dataframe you want\nis_none # has the value None\n"", ""df = df.set_index(['Company', 'date'], inplace=True)\n"", 'df', 'df', ""df.set_index(['Company', 'date'], inplace=True)\n""]"
1110;;0;24074316;19913659.0;2;11;;;;"[""df['color'] = df.Set.map( lambda x: 'red' if x == 'Z' else 'green')\n""]"
1111;;2;24083253;24082784.0;2;53;;;;"['pd.groupby(b,by=[b.index.month,b.index.year])\n', ""df.groupby(pd.TimeGrouper(freq='M'))\n""]"
1112;;1;24112443;19530568.0;2;19;;;;"[""import pandas as pd\n\ndf = pd.DataFrame( {'A' : [1, 1, 1, 1, 2, 2, 3], 'B' : [10, 12, 11, 10, 11, 12, 14], 'C' : [22, 20,     8, 10, 13, 10, 0]})\nprint df\n\ndf2=df.groupby(['A']).apply(lambda tdf: pd.Series(  dict([[vv,tdf[vv].unique().tolist()] for vv in tdf if vv not in ['A']])  )) \nprint df2\n"", 'In [3]: run tmp\n   A   B   C\n0  1  10  22\n1  1  12  20\n2  1  11   8\n3  1  10  10\n4  2  11  13\n5  2  12  10\n6  3  14   0\n\n[7 rows x 3 columns]\n              B                C\nA                               \n1  [10, 12, 11]  [22, 20, 8, 10]\n2      [11, 12]         [13, 10]\n3          [14]              [0]\n\n[3 rows x 2 columns]\n']"
1113;;11;24147363;24147278.0;2;111;;;;['randn', 'In [11]: df = pd.DataFrame(np.random.randn(100, 2))\n\nIn [12]: msk = np.random.rand(len(df)) < 0.8\n\nIn [13]: train = df[msk]\n\nIn [14]: test = df[~msk]\n', 'In [15]: len(test)\nOut[15]: 21\n\nIn [16]: len(train)\nOut[16]: 79\n']
1114;;8;24151789;24147278.0;2;211;;;;['train_test_split', 'import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(df, test_size = 0.2)\n']
1115;;2;24216489;24216425.0;2;48;;;;"['df[""B""] = df[""A""].map(equiv)', 'In [55]:\n\nimport pandas as pd\nequiv = {7001:1, 8001:2, 9001:3}\ndf = pd.DataFrame( {""A"": [7001, 8001, 9001]} )\ndf[""B""] = df[""A""].map(equiv)\nprint(df)\n      A  B\n0  7001  1\n1  8001  2\n2  9001  3\n\n[3 rows x 2 columns]\n', 'In [56]:\n\nimport pandas as pd\nequiv = {7001:1, 8001:2, 9001:3}\ndf = pd.DataFrame( {""A"": [7001, 8001, 9001, 10000]} )\ndf[""B""] = df[""A""].map(equiv)\nprint(df)\n       A   B\n0   7001   1\n1   8001   2\n2   9001   3\n3  10000 NaN\n\n[4 rows x 2 columns]\n']"
1116;;1;24222837;15118111.0;2;8;;;;"['map()', ""st['a'] = map(lambda path, row: path + 2 * row, st['path'], st['row'])\n"", ""title_dict = {'male': 'mr.', 'female': 'ms.'}\ntable['title'] = map(lambda title,\n    gender: title if title != None else title_dict[gender],\n    table['title'], table['gender'])\n""]"
1117;;2;24251426;24251219.0;2;16;;;;"[""dashboard_df = pd.read_csv(p_file, sep=',', error_bad_lines=False, index_col=False, dtype='unicode')\n""]"
1118;;3;24283087;24193174.0;2;39;;;;"[""def set_color_cycle(self, clist=None):\n    if clist is None:\n        clist = rcParams['axes.color_cycle']\n    self.color_cycle = itertools.cycle(clist\n"", 'def set_color_cycle(self, clist):\n    """"""\n    Set the color cycle for any future plot commands on this Axes.\n\n    *clist* is a list of mpl color specifiers.\n    """"""\n    self._get_lines.set_color_cycle(clist)\n    self._get_patches_for_fill.set_color_cycle(clist)\n', 'import matplotlib.pyplot as plt\nimport numpy as np\n\nfor i in range(3):\n    plt.plot(np.arange(10) + i)\n\nplt.gca().set_color_cycle(None)\n\nfor i in range(3):\n    plt.plot(np.arange(10, 1, -1) + i)\n\nplt.show()\n']"
1119;;1;24284515;24284342.0;2;12;;;;"["">>> pd.DataFrame(np.array([[2, 3, 4]]), columns=['A', 'B', 'C']).append(df, ignore_index=True)\nOut[330]: \n   A  B  C\n0  2  3  4\n1  5  6  7\n2  7  8  9\n"", 'pd.prepend()', 'ignore_index', '1', '0', "">>> index = np.array([0, 1, 2])\n>>> df2 = pd.DataFrame(columns=['A', 'B', 'C'], index=index)\n>>> df2.loc[0:1] = [list(s1), list(s2)]\n>>> df2\nOut[336]: \n     A    B    C\n0    5    6    7\n1    7    8    9\n2  NaN  NaN  NaN\n>>> df2 = pd.DataFrame(columns=['A', 'B', 'C'], index=index)\n>>> df2.loc[1:] = [list(s1), list(s2)]\n"", 'df', '>>> df2\nOut[339]: \n     A    B    C\n0  NaN  NaN  NaN\n1    5    6    7\n2    7    8    9\n', '>>> df2.loc[0] = np.array([2, 3, 4])\n>>> df2\nOut[341]: \n   A  B  C\n0  2  3  4\n1  5  6  7\n2  7  8  9\n']"
1120;;2;24284680;24284342.0;2;36;;;;['loc', ' df.loc[-1] = [2, 3, 4]  # adding a row\n df.index = df.index + 1  # shifting index\n df = df.sort()  # sorting by index\n', '    A  B  C\n 0  2  3  4\n 1  5  6  7\n 2  7  8  9\n']
1121;;0;24287210;24284342.0;2;6;;;;"["">>>df2 = pd.DataFrame([[2,3,4]],columns=['A','B','C'])\n>>>pd.concat([df2,df])\n""]"
1122;;2;24368660;18695605.0;2;14;;;;"["">>> ptest = p.DataFrame([['a',1],['a',2],['b',3]], columns=['id', 'value']) \n>>> ptest\n  id  value\n0  a      1\n1  a      2\n2  b      3\n\n# note that in both cases the association a->1 is lost:\n>>> ptest.set_index('id')['value'].to_dict()\n{'a': 2, 'b': 3}\n>>> dict(zip(ptest.id, ptest.value))\n{'a': 2, 'b': 3}\n"", "">>> mydict = {}\n>>> for x in range(len(ptest)):\n...     currentid = ptest.iloc[x,0]\n...     currentvalue = ptest.iloc[x,1]\n...     mydict.setdefault(currentid, [])\n...     mydict[currentid].append(currentvalue)\n>>> mydict\n{'a': [1, 2], 'b': [3]}\n""]"
1123;;2;24370510;18695605.0;2;27;;;;"['groupby', '>>> ptest = pd.DataFrame([[\'a\',1],[\'a\',2],[\'b\',3]], columns=[\'id\', \'value\']) \n>>> ptest\n  id  value\n0  a      1\n1  a      2\n2  b      3\n>>> {k: g[""value""].tolist() for k,g in ptest.groupby(""id"")}\n{\'a\': [1, 2], \'b\': [3]}\n']"
1124;;1;24396554;11067027.0;2;17;;;;[]
1125;;1;24489283;13888468.0;2;20;;;;"['index.levels[blah]', ""import pandas as pd\nimport numpy as np\n\nnp.random.seed(0)\n\nidx = pd.MultiIndex.from_product([['John', 'Josh', 'Alex'], list('abcde')], \n                                 names=['Person', 'Letter'])\nlarge = pd.DataFrame(data=np.random.randn(15, 2), \n                     index=idx, \n                     columns=['one', 'two'])\nsmall = large.loc[['Jo'==d[0:2] for d in large.index.get_level_values('Person')]]\n\nprint small.index.levels[0]\nprint large.index.levels[0]\n"", ""Index([u'Alex', u'John', u'Josh'], dtype='object')\nIndex([u'Alex', u'John', u'Josh'], dtype='object')\n"", ""Index([u'John', u'Josh'], dtype='object')\nIndex([u'Alex', u'John', u'Josh'], dtype='object')\n"", ""small.index.get_level_values('Person').unique()\nlarge.index.get_level_values('Person').unique()\n""]"
1126;;4;24489602;18172851.0;2;91;;;;['df = df[df.line_race != 0]', 'None', 'df = df[df.line_race != 0]\n', 'df = df[df.line_race != None]\n', 'df = df[df.line_race.notnull()]\n']
1127;;4;24496435;24495695.0;2;37;;;;"[""In [11]: df.index.get_level_values('co').unique()\nOut[11]: array(['DE', 'FR'], dtype=object)\n""]"
1128;;6;24517695;13842088.0;2;146;;;;"['.ix', ""df.set_value('C', 'x', 10)""]"
1129;;4;24674675;19555525.0;2;42;;;;"['plot = dtf.plot()\nfig = plot.get_figure()\nfig.savefig(""output.png"")\n']"
1130;;1;24775756;24775648.0;2;42;;;;['|', ' df[(df < 3) | (df == 5)]\n']
1131;;1;24793359;13187778.0;2;58;;;;['numpyMatrix = df.as_matrix()\n']
1132;;2;24804512;21360361.0;2;23;;;;['wait=True', 'clear_output', 'display.clear_output(wait=True)\ndisplay.display(pl.gcf())\n']
1133;;0;24870404;24870306.0;2;134;;;;"[""if 'A' in df:\n"", ""if 'A' in df.columns:\n""]"
1134;;3;24871316;24870953.0;2;48;;;;"['iterrows', '1) vectorization\n2) using a custom cython routine\n3) apply\n    a) reductions that can be performed in cython\n    b) iteration in python space\n4) itertuples\n5) iterrows\n6) updating an empty frame (e.g. using loc one-row-at-a-time)\n', 'df.apply(lambda x: np.sum(x))', 'df.sum(1)', ""df.apply(lambda x: x['b'] + 1)"", 'itertuples', 'iterrows', 'concat']"
1135;;5;24888331;10715965.0;2;180;;;;"["">>> df = DataFrame(columns=('lib', 'qty1', 'qty2'))\n>>> for i in range(5):\n>>>     df.loc[i] = [randint(-1,1) for n in range(3)]\n>>>\n>>> print(df)\n    lib  qty1  qty2\n0    0     0    -1\n1   -1    -1     1\n2    1    -1     1\n3    0     0     0\n4    1    -1    -1\n\n[5 rows x 3 columns]\n""]"
1136;;2;24913075;10715965.0;2;42;;;;"[""import pandas as pd\nimport numpy as np\n# we know we're gonna have 5 rows of data\nnumberOfRows = 5\n# create dataframe\ndf = pd.DataFrame(index=np.arange(0, numberOfRows), columns=('lib', 'qty1', 'qty2') )\n\n# now fill it up row by row\nfor x in np.arange(0, numberOfRows):\n    #loc or iloc both work here since the index is natural numbers\n    df.loc[x] = [np.random.randint(-1,1) for n in range(3)]\nIn[23]: df\nOut[23]: \n   lib  qty1  qty2\n0   -1    -1    -1\n1    0     0     0\n2   -1     0    -1\n3    0    -1     0\n4   -1     0     0\n"", 'In[30]: %timeit tryThis() # function wrapper for this answer\nIn[31]: %timeit tryOther() # function wrapper without index (see, for example, @fred)\n1000 loops, best of 3: 1.23 ms per loop\n100 loops, best of 3: 2.31 ms per loop\n']"
1137;;3;24933234;19828822.0;2;25;;;;"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randn(10000, 4), columns=list('ABCD'))\n\ndef empty(df):\n    return df.empty\n\ndef lenz(df):\n    return len(df) == 0\n\ndef lenzi(df):\n    return len(df.index) == 0\n\n'''\n%timeit empty(df)\n%timeit lenz(df)\n%timeit lenzi(df)\n\n10000 loops, best of 3: 13.9 s per loop\n100000 loops, best of 3: 2.34 s per loop\n1000000 loops, best of 3: 695 ns per loop\n\nlen on index seems to be faster\n'''\n""]"
1138;;0;25023460;13148429.0;2;8;;;;"['def order(frame,var):\n    varlist =[w for w in frame.columns if w not in var]\n    frame = frame[var+varlist]\n    return frame \n', ""frame = order(frame,['Total'])\n"", ""frame = order(frame,['Total','Date'])\n"", 'frame = order(frame,[v for v in frame.columns if ""VAR"" in v])\n']"
1139;;5;25030617;10065051.0;2;41;;;;"['sqlalchemy', ""from sqlalchemy import create_engine\nimport pandas as pd\nengine = create_engine('dialect://user:pass@host:port/schema', echo=False)\nf = pd.read_sql_query('SELECT * FROM mytable', engine, index_col = 'ID')\n""]"
1140;;1;25057724;25055712.0;2;41;;;;['iloc', 'df.iloc[::5, :]\n']
1141;;3;25146337;25146121.0;2;42;;;;"['year', 'month', 'datetime.datetime', ""In [15]: t = pandas.tslib.Timestamp.now()\n\nIn [16]: t\nOut[16]: Timestamp('2014-08-05 14:49:39.643701', tz=None)\n\nIn [17]: t.to_datetime()\nOut[17]: datetime.datetime(2014, 8, 5, 14, 49, 39, 643701)\n\nIn [18]: t.day\nOut[18]: 5\n\nIn [19]: t.month\nOut[19]: 8\n\nIn [20]: t.year\nOut[20]: 2014\n"", '201408', ""df['YearMonth'] = df['ArrivalDate'].map(lambda x: 1000*x.year + x.month)\n"", 'calendar', ""import calendar\nimport datetime\ndf['AdjustedDateToEndOfMonth'] = df['ArrivalDate'].map(\n    lambda x: datetime.datetime(\n        x.year,\n        x.month,\n        max(calendar.monthcalendar(x.year, x.month)[-1][:5])\n    )\n)\n"", 'strftime', 'datetime.datetime', ""In [5]: df\nOut[5]: \n            date_time\n0 2014-10-17 22:00:03\n\nIn [6]: df.date_time\nOut[6]: \n0   2014-10-17 22:00:03\nName: date_time, dtype: datetime64[ns]\n\nIn [7]: df.date_time.map(lambda x: x.strftime('%Y-%m-%d'))\nOut[7]: \n0    2014-10-17\nName: date_time, dtype: object\n""]"
1142;;3;25149272;25146121.0;2;96;;;;"[""df['year'] = pd.DatetimeIndex(df['ArrivalDate']).year\ndf['month'] = pd.DatetimeIndex(df['ArrivalDate']).month\n"", ""df['year'] = df['ArrivalDate'].dt.year\ndf['month'] = df['ArrivalDate'].dt.month\n""]"
1143;;5;25208947;18889588.0;2;37;;;;"[""In [4]: df\nOut[4]:\n      label\n0  (a, c, e)\n1     (a, d)\n2       (b,)\n3     (d, e)\n\nIn [5]: df['label'].str.join(sep='*').str.get_dummies(sep='*')\nOut[5]:\n   a  b  c  d  e\n0  1  0  1  0  1\n1  1  0  0  1  0\n2  0  1  0  0  0\n3  0  0  0  1  1\n""]"
1144;;1;25213438;25212986.0;2;35;;;;"[""lm = sns.lmplot('X','Y',df,col='Z',sharex=False,sharey=False)\n"", 'axes = lm.axes\n', 'axes[0,0].set_ylim(0,)\naxes[0,1].set_ylim(0,)\n']"
1145;;2;25213614;25212986.0;2;48;;;;"['lmplot', 'FacetGrid', 'set', 'key=value', 'None', ""g = sns.lmplot('X', 'Y', df, col='Z', sharex=False, sharey=False)\ng.set(ylim=(0, None))\n""]"
1146;;1;25217425;19736080.0;2;28;;;;"[""In[20]: my_dict = dict( A = np.array([1,2]), B = np.array([1,2,3,4]) )\nIn[21]: df = pd.DataFrame.from_dict(my_dict, orient='index')\nIn[22]: df\nOut[22]: \n   0  1   2   3\nA  1  2 NaN NaN\nB  1  2   3   4\nIn[23]: df.transpose()\nOut[23]: \n    A  B\n0   1  1\n1   2  2\n2 NaN  3\n3 NaN  4\n""]"
1147;;0;25230582;20845213.0;2;164;;;;"['index=False', ""pd.to_csv('your.csv', index=False)\n""]"
1148;;0;25254087;25254016.0;2;115;;;;"['ith', 'iloc', 'In [31]: df_test.iloc[0]\nOut[31]: \nATime     1.2\nX         2.0\nY        15.0\nZ         2.0\nBtime     1.2\nC        12.0\nD        25.0\nE        12.0\nName: 0, dtype: float64\n', 'Btime', ""In [30]: df_test['Btime'].iloc[0]\nOut[30]: 1.2\n"", ""df_test.ix[i, 'Btime']"", 'ith', 'ix', 'ix[i]', 'i', 'ith', ""In [1]: df = pd.DataFrame({'foo':list('ABC')}, index=[0,2,1])\n\nIn [2]: df\nOut[2]: \n  foo\n0   A\n2   B\n1   C\n\nIn [4]: df.ix[1, 'foo']\nOut[4]: 'C'\n""]"
1149;;3;25352191;25351968.0;2;43;;;;"['display.max_colwidth', '-1', ""pd.set_option('display.max_colwidth', -1)\n"", 'set_option']"
1150;;4;25376997;19365513.0;2;43;;;;"[""df.loc[len(df)]=['8/19/2014','Jun','Fly','98765'] \n""]"
1151;;2;25401328;25386870.0;2;6;;;;"[""# using pandas version 0.14.1\nfrom pandas import DataFrame\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = {'ColB': {('A', 4): 3.0,\n('C', 2): 0.0,\n('B', 4): 51.0,\n('B', 1): 0.0,\n('C', 3): 0.0,\n('B', 2): 7.0,\n('Code', 'Month'): '',\n('A', 3): 5.0,\n('C', 1): 0.0,\n('C', 4): 0.0,\n('B', 3): 12.0},\n'ColA': {('A', 4): 66.0,\n('C', 2): 5.0,\n('B', 4): 125.0,\n('B', 1): 5.0,\n('C', 3): 41.0,\n('B', 2): 52.0,\n('Code', 'Month'): '',\n('A', 3): 22.0,\n('C', 1): 14.0,\n('C', 4): 51.0,\n('B', 3): 122.0}}\n\ndf = DataFrame(data)\n"", ""f, a = plt.subplots(3,1)\ndf.xs('A').plot(kind='bar',ax=a[0])\ndf.xs('B').plot(kind='bar',ax=a[1])\ndf.xs('C').plot(kind='bar',ax=a[2])\n""]"
1152;;1;25412939;25386870.0;2;43;;;;"[""summed_group.unstack(level=0).plot(kind='bar', subplots=True)\n""]"
1153;;1;25415404;11707586.0;2;59;;;;"[""pd.set_option('display.expand_frame_repr', False)\n""]"
1154;;6;25493765;25493625.0;2;39;;;;"['left', 'sku', ""In [26]:\n\ndf.merge(df1, on='sku', how='left')\nOut[26]:\n   sku  loc   flag dept\n0  122   61   True    b\n1  122   62   True    b\n2  122   63  False    b\n3  123   61   True    b\n4  123   62  False    b\n5  113   62   True    a\n6  301   63   True    c\n"", 'sku', ""In [28]:\n\ndf.merge(df1, left_index=True, right_index=True, how='left')\nOut[28]:\n     loc   flag dept\nsku                 \n113   62   True    a\n122   61   True    b\n122   62   True    b\n122   63  False    b\n123   61   True    b\n123   62  False    b\n301   63   True    c\n"", 'map', 'sku', ""In [19]:\n\ndf['dept']=df.sku.map(df1.dept)\ndf\nOut[19]:\n   sku  loc   flag dept\n0  122   61   True    b\n1  123   61   True    b\n2  113   62   True    a\n3  122   62   True    b\n4  123   62  False    b\n5  122   63  False    b\n6  301   63   True    c\n""]"
1155;;0;25535803;13148429.0;2;6;;;;"[""df = df.reindex_axis(['mean'] + list(df.columns[:-1]), axis=1)""]"
1156;;4;25562948;25239958.0;2;40;;;;"['import pandas as pd\nimport numpy as np\n\nfrom sklearn.base import TransformerMixin\n\nclass DataFrameImputer(TransformerMixin):\n\n    def __init__(self):\n        """"""Impute missing values.\n\n        Columns of dtype object are imputed with the most frequent value \n        in column.\n\n        Columns of other types are imputed with mean of column.\n\n        """"""\n    def fit(self, X, y=None):\n\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype(\'O\') else X[c].mean() for c in X],\n            index=X.columns)\n\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n\ndata = [\n    [\'a\', 1, 2],\n    [\'b\', 1, 1],\n    [\'b\', 2, 2],\n    [np.nan, np.nan, np.nan]\n]\n\nX = pd.DataFrame(data)\nxt = DataFrameImputer().fit_transform(X)\n\nprint(\'before...\')\nprint(X)\nprint(\'after...\')\nprint(xt)\n', 'before...\n     0   1   2\n0    a   1   2\n1    b   1   1\n2    b   2   2\n3  NaN NaN NaN\nafter...\n   0         1         2\n0  a  1.000000  2.000000\n1  b  1.000000  1.000000\n2  b  2.000000  2.000000\n3  b  1.333333  1.666667\n']"
1157;;0;25574089;20868394.0;2;61;;;;"['inplace', ""df.rename(columns={'two':'new_name'}, inplace=True)\n""]"
1158;;2;25588487;19726663.0;2;25;;;;"['DataFrame.plot()', ""ax = df.plot()\nfig = ax.get_figure()\nfig.savefig('asdf.png')\n""]"
1159;;0;25643178;11285613.0;2;9;;;;"['drop()', ""colsToDrop = ['a']\ndf.drop(colsToDrop, axis=1)\n"", 'b', 'c', 'drop']"
1160;;1;25646414;25646200.0;2;35;;;;"['td', ""import numpy as np\n\n(td / np.timedelta64(1, 'D')).astype(int)\n""]"
1161;;5;25715719;14688306.0;2;8;;;;[]
1162;;0;25733562;20110170.0;2;82;;;;['.reset_index(inplace=True)', 'df.reset_index(inplace=True)  \n']
1163;;4;25748741;25748683.0;2;10;;;;"[""df['e'] = df.a + df.b + df.d\n"", 'e', '   a  b   c  d   e\n0  1  2  dd  5   8\n1  2  3  ee  9  14\n2  3  4  ff  1   8\n']"
1164;;4;25748826;25748683.0;2;71;;;;"['sum', 'axis=1', ""In [91]:\n\ndf = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})\ndf['e'] = df.sum(axis=1)\ndf\nOut[91]:\n   a  b   c  d   e\n0  1  2  dd  5   8\n1  2  3  ee  9  14\n2  3  4  ff  1   8\n"", ""In [98]:\n\ncol_list= list(df)\ncol_list.remove('d')\ncol_list\nOut[98]:\n['a', 'b', 'c']\nIn [99]:\n\ndf['e'] = df[col_list].sum(axis=1)\ndf\nOut[99]:\n   a  b   c  d  e\n0  1  2  dd  5  3\n1  2  3  ee  9  5\n2  3  4  ff  1  7\n""]"
1165;;4;25774395;25773245.0;2;107;;;;['axis=0', 'axis=1', 'axis', 'df.mean(axis=1)', 'df.mean(axis=0)', 'df.drop(name, axis=1)', 'axis=0']
1166;;2;25774932;10202570.0;2;11;;;;"[""In [1]: from pandas import Series, DataFrame\n\nIn [2]: s=Series([2,4,4,3],index=['a','b','c','d'])\n\nIn [3]: s.idxmax()\nOut[3]: 'b'\n\nIn [4]: s[s==s.max()]\nOut[4]: \nb    4\nc    4\ndtype: int64\n""]"
1167;;2;25799781;16074392.0;2;20;;;;['plt.gca().xaxis.grid(True)']
1168;;0;25959539;15006298.0;2;26;;;;['# Say you have a df object containing your dataframe\ndf.head(5) # will print out the first 5 rows\ndf.tail(5) # will print out the 5 last rows\n# Note: it is similar to R\n']
1169;;12;25962187;25962114.0;2;60;;;;['chunksize', 'chunksize = 10 ** 6\nfor chunk in pd.read_csv(filename, chunksize=chunksize):\n    process(chunk)\n']
1170;;2;26017289;25962114.0;2;18;;;;"[""chunks=pd.read_table('aphro.csv',chunksize=1000000,sep=';',\\\n       names=['lat','long','rf','date','slno'],index_col='slno',\\\n       header=None,parse_dates=['date'])\n\ndf=pd.DataFrame()\n%time df=pd.concat(chunk.groupby(['lat','long',chunk['date'].map(lambda x: x.year)])['rf'].agg(['sum']) for chunk in chunks)\n""]"
1171;;5;26139658;26139423.0;2;58;;;;"['plt.scatter', 'c', 'colors', ""import matplotlib.pyplot as plt\nimport pandas as pd\n\ncarat = [5, 10, 20, 30, 5, 10, 20, 30, 5, 10, 20, 30]\nprice = [100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 600, 600]\ncolor =['D', 'D', 'D', 'E', 'E', 'E', 'F', 'F', 'F', 'G', 'G', 'G',]\n\ndf = pd.DataFrame(dict(carat=carat, price=price, color=color))\n\nfig, ax = plt.subplots()\n\ncolors = {'D':'red', 'E':'blue', 'F':'green', 'G':'black'}\n\nax.scatter(df['carat'], df['price'], c=df['color'].apply(lambda x: colors[x]))\n\nplt.show()\n"", ""df['color'].apply(lambda x: colors[x])"", 'seaborn', 'seaborn', 'matplotlib', 'seaborn.lmplot', 'fit_reg=False', ""hue='color'"", ""import matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pandas as pd\n\ncarat = [5, 10, 20, 30, 5, 10, 20, 30, 5, 10, 20, 30]\nprice = [100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 600, 600]\ncolor =['D', 'D', 'D', 'E', 'E', 'E', 'F', 'F', 'F', 'G', 'G', 'G',]\n\ndf = pd.DataFrame(dict(carat=carat, price=price, color=color))\n\nsns.lmplot('carat', 'price', data=df, hue='color', fit_reg=False)\n\nplt.show()\n"", 'seaborn', 'pandas.groupby', 'pandas.groupby', ""fig, ax = plt.subplots()\n\ncolors = {'D':'red', 'E':'blue', 'F':'green', 'G':'black'}\n\ngrouped = df.groupby('color')\nfor key, group in grouped:\n    group.plot(ax=ax, kind='scatter', x='carat', y='price', label=key, color=colors[key])\n\nplt.show()\n"", 'color', 'colors', 'D', 'red']"
1172;;3;26240208;26047209.0;2;56;;;;['DataFrame', 'DataFrame', 'Series', 'DataFrame', 'Series', 'DataFrame', 'Series', 'Series', 'DataFrame']
1173;;2;26266439;26266362.0;2;30;;;;['count_nan = len(df) - df.count()\n', 'isnull']
1174;;4;26266451;26266362.0;2;191;;;;"['isnull()', 'In [1]: s = pd.Series([1,2,3, np.nan, np.nan])\n\nIn [4]: s.isnull().sum()\nOut[4]: 2\n', ""In [5]: df = pd.DataFrame({'a':[1,2,np.nan], 'b':[np.nan,1,np.nan]})\n\nIn [6]: df.isnull().sum()\nOut[6]:\na    1\nb    2\ndtype: int64\n""]"
1175;;0;26272425;26266362.0;2;15;;;;"[""import pandas as pd\ndf = pd.DataFrame({'a':[1,2,np.nan], 'b':[np.nan,1,np.nan]})\nfor col in df:\n    print df[col].value_counts(dropna=False)\n\n2     1\n 1     1\nNaN    1\ndtype: int64\nNaN    2\n 1     1\ndtype: int64\n""]"
1176;;0;26286140;14262433.0;2;7;;;;[]
1177;;1;26301947;26277757.0;2;49;;;;"['max_colwidth', ""pd.set_option('display.max_colwidth', -1)\n""]"
1178;;1;26310294;26309962.0;2;14;;;;"["">>> import pandas as pd\n>>> df = pd.DataFrame(columns=['col1', 'col2'])\n>>> df = df.append(pd.Series(['a', 'b'], index=['col1','col2']), ignore_index=True)\n>>> df = df.append(pd.Series(['d', 'e'], index=['col1','col2']), ignore_index=True) \n>>> df\n  col1 col2\n0    a    b\n1    d    e\n""]"
1179;;0;26311118;26309962.0;2;22;;;;"["">>> import pandas as pd\n>>> df = pd.DataFrame()\n>>> df = df.append({'foo':1, 'bar':2}, ignore_index=True)\n""]"
1180;;0;26320276;10665889.0;2;14;;;;['op = df[list(df.columns[0:899]) + list(df.columns[3593:])]\nprint op\n']
1181;;3;26347456;26347412.0;2;60;;;;['[..]', 'df.drop(df.columns[[1, 69]], axis=1, inplace=True)\n']
1182;;0;26356675;14940743.0;2;9;;;;"['DataFrame.filter', 'df.filter(regex=""[^BD]"")\n', 'df.filter(regex=""^(?!(B|D)$).*$"")\n', 'exclude_cols = [\'B\',\'C\']\ndf.filter(regex=""^(?!({0})$).*$"".format(\'|\'.join(exclude_cols)))\n']"
1183;;6;26474062;26473681.0;2;42;;;;['apt-get install python-numpy\n', '--system-site-packages\n']
1184;;0;26510251;26483254.0;2;23;;;;"[""df3.set_value(1, 'B', abc)"", ""df['B'] = df['B'].astype(object)""]"
1185;;1;26599892;18039057.0;2;24;;;;"['sep', 'header', 'read_csv', ""df = pandas.read_csv(fileName, sep='delimiter', header=None)\n"", 'sep', 'header=None', 'sep']"
1186;;0;26640189;26640145.0;2;37;;;;"['df.index\n', 'list(df.index)\n', ""df.index['Row 2':'Row 5'] \n""]"
1187;;4;26654201;26645515.0;2;42;;;;"[""In [173]:\n\ndf_a.join(df_b, on='mukey', how='left', lsuffix='_left', rsuffix='_right')\nOut[173]:\n       mukey_left  DI  PI  mukey_right  niccdcd\nindex                                          \n0          100000  35  14          NaN      NaN\n1         1000005  44  14          NaN      NaN\n2         1000006  44  14          NaN      NaN\n3         1000007  43  13          NaN      NaN\n4         1000008  43  13          NaN      NaN\n"", 'merge', ""In [176]:\n\ndf_a.merge(df_b, on='mukey', how='left')\nOut[176]:\n     mukey  DI  PI  niccdcd\n0   100000  35  14      NaN\n1  1000005  44  14      NaN\n2  1000006  44  14      NaN\n3  1000007  43  13      NaN\n4  1000008  43  13      NaN\n""]"
1188;;2;26658301;26658240.0;2;28;;;;"['name', ""In [182]:\n\ndf = pd.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])\ndef rowFunc(row):\n    return row['a'] + row['b'] * row['c']\n\ndef rowIndex(row):\n    return row.name\ndf['d'] = df.apply(rowFunc, axis=1)\ndf['rowIndex'] = df.apply(rowIndex, axis=1)\ndf\nOut[182]:\n   a  b  c   d  rowIndex\n0  1  2  3   7         0\n1  4  5  6  34         1\n"", ""In [198]:\n\ndf['d'] = df['a'] + df['b'] * df['c']\ndf\nOut[198]:\n   a  b  c   d\n0  1  2  3   7\n1  4  5  6  34\n\nIn [199]:\n\n%timeit df['a'] + df['b'] * df['c']\n%timeit df.apply(rowIndex, axis=1)\n10000 loops, best of 3: 163 s per loop\n1000 loops, best of 3: 286 s per loop\n""]"
1189;;0;26716759;26716616.0;2;8;;;;"['Zip', 'df = pd.read_csv(""file"")\nd= dict([(i,[a,b,c ]) for i, a,b,c in zip(df.ID, df.A,df.B,df.C)])\nprint d\n', ""{'p': [1, 3, 2], 'q': [4, 3, 2], 'r': [4, 0, 9]}\n""]"
1190;;3;26716774;26716616.0;2;56;;;;"['to_dict()', 'to_dict()', 'outtype', '{index:value}', "">>> df.set_index('ID').T.to_dict('list')\n{'p': [1, 3, 2], 'q': [4, 3, 2], 'r': [4, 0, 9]}\n""]"
1191;;4;26721808;23549231.0;2;15;;;;['g in df.<your selected field>.values\ng in df.index.values\n']
1192;;0;26724725;12065885.0;2;71;;;;"['isin()', 'str.contains', ""'600'"", "">>> rpt[rpt['STK_ID'].str.contains(r'^600[0-9]{3}$')] # ^ means start of string\n...   STK_ID   ...                                    # [0-9]{3} means any three digits\n...  '600809'  ...                                    # $ means end of string\n...  '600141'  ...\n...  '600329'  ...\n...      ...   ...\n"", ""'STK_ID'"", ""endstrings = ['01$', '02$', '05$']\n"", '|', 'str.contains', "">>> rpt[rpt['STK_ID'].str.contains('|'.join(endstrings)]\n...   STK_ID   ...\n...  '155905'  ...\n...  '633101'  ...\n...  '210302'  ...\n...      ...   ...\n"", 'contains', 'case=False', ""str.contains('pandas', case=False)\n"", 'PANDAS', 'PanDAs', 'paNdAs123']"
1193;;1;26763793;26763344.0;2;93;;;;"['to_datetime', ""raw_data['Mycol'] =  pd.to_datetime(raw_data['Mycol'], format='%d%b%Y:%H:%M:%S.%f')\n""]"
1194;;2;26763810;26763344.0;2;19;;;;"['.apply()', "">>> df = pd.DataFrame(['05SEP2014:00:00:00.000'],columns=['Mycol'])\n>>> df\n                    Mycol\n0  05SEP2014:00:00:00.000\n>>> import datetime as dt\n>>> df['Mycol'] = df['Mycol'].apply(lambda x: \n                                    dt.datetime.strptime(x,'%d%b%Y:%H:%M:%S.%f'))\n>>> df\n       Mycol\n0 2014-09-05\n""]"
1195;;3;26787032;26786960.0;2;42;;;;"['index=False', ""df_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w', index=False)\n""]"
1196;;2;26803731;13575090.0;2;7;;;;"[""import pandas as pd\nd\n{'RAY Index': {datetime.date(2014, 11, 3): {'PX_LAST': 1199.46,\n'PX_OPEN': 1200.14},\ndatetime.date(2014, 11, 4): {'PX_LAST': 1195.323, 'PX_OPEN': 1197.69},\ndatetime.date(2014, 11, 5): {'PX_LAST': 1200.936, 'PX_OPEN': 1195.32},\ndatetime.date(2014, 11, 6): {'PX_LAST': 1206.061, 'PX_OPEN': 1200.62}},\n'SPX Index': {datetime.date(2014, 11, 3): {'PX_LAST': 2017.81,\n'PX_OPEN': 2018.21},\ndatetime.date(2014, 11, 4): {'PX_LAST': 2012.1, 'PX_OPEN': 2015.81},\ndatetime.date(2014, 11, 5): {'PX_LAST': 2023.57, 'PX_OPEN': 2015.29},\ndatetime.date(2014, 11, 6): {'PX_LAST': 2031.21, 'PX_OPEN': 2023.33}}}\n"", ""pd.Panel(d)\n<class 'pandas.core.panel.Panel'>\nDimensions: 2 (items) x 2 (major_axis) x 4 (minor_axis)\nItems axis: RAY Index to SPX Index\nMajor_axis axis: PX_LAST to PX_OPEN\nMinor_axis axis: 2014-11-03 to 2014-11-06\n"", ""pd.Panel(d)['SPX Index']\n2014-11-03  2014-11-04  2014-11-05 2014-11-06\nPX_LAST 2017.81 2012.10 2023.57 2031.21\nPX_OPEN 2018.21 2015.81 2015.29 2023.33\n"", 'pd.Panel(d).to_frame().reset_index()\nmajor   minor      RAY Index    SPX Index\nPX_LAST 2014-11-03  1199.460    2017.81\nPX_LAST 2014-11-04  1195.323    2012.10\nPX_LAST 2014-11-05  1200.936    2023.57\nPX_LAST 2014-11-06  1206.061    2031.21\nPX_OPEN 2014-11-03  1200.140    2018.21\nPX_OPEN 2014-11-04  1197.690    2015.81\nPX_OPEN 2014-11-05  1195.320    2015.29\nPX_OPEN 2014-11-06  1200.620    2023.33\n', 'pd.Panel(d).transpose(2,0,1).to_frame().reset_index()\nmajor        minor  2014-11-03  2014-11-04  2014-11-05  2014-11-06\nRAY Index   PX_LAST 1199.46    1195.323     1200.936    1206.061\nRAY Index   PX_OPEN 1200.14    1197.690     1195.320    1200.620\nSPX Index   PX_LAST 2017.81    2012.100     2023.570    2031.210\nSPX Index   PX_OPEN 2018.21    2015.810     2015.290    2023.330\n']"
1197;;4;26816746;17241004.0;2;12;;;;['get_values', 'df.index.get_values()\n']
1198;;2;26838140;26837998.0;2;32;;;;"[""import numpy as np\ndf1 = df.replace(np.nan, '', regex=True)\n""]"
1199;;1;26849064;11350770.0;2;64;;;;"['df[df[\'A\'].str.contains(""Hello|Britain"")]\n', '""cannot index with vector containing NA / NaN values""\n', 'df[df[\'A\'].str.contains(""Hello|Britain"")==True]\n']"
1200;;1;26851412;11350770.0;2;15;;;;"['DataFrame', "">>> df = pd.DataFrame([['hello', 'hello world'], ['abcd', 'defg']], columns=['a','b'])\n>>> df\n       a            b\n0  hello  hello world\n1   abcd         defg\n"", 'in', "">>> df.apply(lambda x: x['a'] in x['b'], axis=1)\n0     True\n1    False\ndtype: bool\n"", 'axis=1', 'apply']"
1201;;0;26865524;20853474.0;2;10;;;;['python setup.py install']
1202;;6;26887820;26886653.0;2;102;;;;"[""def label_race (row):\n   if row['eri_hispanic'] == 1 :\n      return 'Hispanic'\n   if row['eri_afr_amer'] + row['eri_asian'] + row['eri_hawaiian'] + row['eri_nat_amer'] + row['eri_white'] > 1 :\n      return 'Two Or More'\n   if row['eri_nat_amer'] == 1 :\n      return 'A/I AK Native'\n   if row['eri_asian'] == 1:\n      return 'Asian'\n   if row['eri_afr_amer']  == 1:\n      return 'Black/AA'\n   if row['eri_hawaiian'] == 1:\n      return 'Haw/Pac Isl.'\n   if row['eri_white'] == 1:\n      return 'White'\n   return 'Other'\n"", 'df.apply (lambda row: label_race (row),axis=1)\n', '0           White\n1        Hispanic\n2           White\n3           White\n4           Other\n5           White\n6     Two Or More\n7           White\n8    Haw/Pac Isl.\n9           White\n', ""df['race_label'] = df.apply (lambda row: label_race (row),axis=1)\n"", '      lname   fname rno_cd  eri_afr_amer  eri_asian  eri_hawaiian   eri_hispanic  eri_nat_amer  eri_white rno_defined    race_label\n0      MOST    JEFF      E             0          0             0              0             0          1       White         White\n1    CRUISE     TOM      E             0          0             0              1             0          0       White      Hispanic\n2      DEPP  JOHNNY    NaN             0          0             0              0             0          1     Unknown         White\n3     DICAP     LEO    NaN             0          0             0              0             0          1     Unknown         White\n4    BRANDO  MARLON      E             0          0             0              0             0          0       White         Other\n5     HANKS     TOM    NaN             0          0             0              0             0          1     Unknown         White\n6    DENIRO  ROBERT      E             0          1             0              0             0          1       White   Two Or More\n7    PACINO      AL      E             0          0             0              0             0          1       White         White\n8  WILLIAMS   ROBIN      E             0          0             1              0             0          0       White  Haw/Pac Isl.\n9  EASTWOOD   CLINT      E             0          0             0              0             0          1       White         White\n']"
1203;;6;26918510;12182744.0;2;22;;;;['Series.apply(func, convert_dtype=True, args=(), **kwds)\n\nargs : tuple\n\nx = my_series.apply(my_function, args = (arg1,))\n']
1204;;3;26977495;26977076.0;2;56;;;;"['np.unique', "">>> np.unique(df[['Col1', 'Col2']])\narray(['Bill', 'Bob', 'Joe', 'Mary', 'Steve'], dtype=object)\n"", '.values', ""np.unique(df[['Col1', 'Col2']].values)\n"", 'pd.unique', 'ravel()', "">>> pd.unique(df[['Col1', 'Col2']].values.ravel())\narray(['Bob', 'Joe', 'Steve', 'Bill', 'Mary'], dtype=object)\n"", "">>> df1 = pd.concat([df]*100000) # DataFrame with 500000 rows\n>>> %timeit np.unique(df1[['Col1', 'Col2']].values)\n1 loops, best of 3: 619 ms per loop\n\n>>> %timeit pd.unique(df1[['Col1', 'Col2']].values.ravel())\n10 loops, best of 3: 49.9 ms per loop\n""]"
1205;;2;27009771;13838405.0;2;8;;;;"['df.iloc[index]', 'df.loc', 'def sort_pd(key=None,reverse=False,cmp=None):\n    def sorter(series):\n        series_list = list(series)\n        return [series_list.index(i) \n           for i in sorted(series_list,key=key,reverse=reverse,cmp=cmp)]\n    return sorter\n', ""df = pd.DataFrame([\n    [1, 2, 'March'],\n    [5, 6, 'Dec'],\n    [3, 4, 'April']], \n  columns=['a','b','m'])\n\ncustom_dict = {'March':0, 'April':1, 'Dec':3}\nsort_by_custom_dict = sort_pd(key=custom_dict.get)\n\nIn [6]: df.iloc[sort_by_custom_dict(df['m'])]\nOut[6]:\n   a  b  m\n0  1  2  March\n2  3  4  April\n1  5  6  Dec\n"", ""months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n\ndf = pd.DataFrame([\n    ['New York','Mar',12714],\n    ['New York','Apr',89238],\n    ['Atlanta','Jan',8161],\n    ['Atlanta','Sep',5885],\n  ],columns=['location','month','sales']).set_index(['location','month'])\n\nsort_by_month = sort_pd(key=months.index)\n\nIn [10]: df.iloc[sort_by_month(df.index.get_level_values('month'))]\nOut[10]:\n                 sales\nlocation  month  \nAtlanta   Jan    8161\nNew York  Mar    12714\n          Apr    89238\nAtlanta   Sep    5885\n\nsort_by_last_digit = sort_pd(key=lambda x: x%10)\n\nIn [12]: pd.Series(list(df['sales'])).iloc[sort_by_last_digit(df['sales'])]\nOut[12]:\n2    8161\n0   12714\n3    5885\n1   89238\n""]"
1206;;4;27026479;26187759.0;2;9;;;;"['concat', '%%time', ""## make some example data\nimport pandas as pd\n\nnp.random.seed(1)\nn=10000\ndf = pd.DataFrame({'mygroup' : np.random.randint(1000, size=n), \n                   'data' : np.random.rand(n)})\ngrouped = df.groupby('mygroup')\n"", 'dflist = []\nfor name, group in grouped:\n    dflist.append(group)\n', 'from IPython.parallel import Client\nrc = Client()\nlview = rc.load_balanced_view()\nlview.block = True\n', ""def myFunc(inDf):\n    inDf['newCol'] = inDf.data ** 10\n    return inDf\n"", '%%time\nserial_list = map(myFunc, dflist)\nCPU times: user 14 s, sys: 19.9 ms, total: 14 s\nWall time: 14 s\n', '%%time\nparallel_list = lview.map(myFunc, dflist)\n\nCPU times: user 1.46 s, sys: 86.9 ms, total: 1.54 s\nWall time: 1.56 s\n', '%%time\ncombinedDf = pd.concat(parallel_list)\n CPU times: user 296 ms, sys: 5.27 ms, total: 301 ms\nWall time: 300 ms\n']"
1207;;4;27027632;26187759.0;2;43;;;;"[""import pandas as pd\nfrom joblib import Parallel, delayed\nimport multiprocessing\n\ndef tmpFunc(df):\n    df['c'] = df.a + df.b\n    return df\n\ndef applyParallel(dfGrouped, func):\n    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)\n    return pd.concat(retLst)\n\nif __name__ == '__main__':\n    df = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])\n    print 'parallel version: '\n    print applyParallel(df.groupby(df.index), tmpFunc)\n\n    print 'regular version: '\n    print df.groupby(df.index).apply(tmpFunc)\n\n    print 'ideal version (does not work): '\n    print df.groupby(df.index).applyParallel(tmpFunc)\n""]"
1208;;9;27232309;24251219.0;2;119;;;;"['low_memory', 'low_memory', ""dtype={'user_id': int}\n"", 'pd.read_csv()', '""foobar""', 'user_id', 'import pandas as pd\nfrom StringIO import StringIO\n\n\ncsvdata = """"""user_id,username\n1,Alice\n3,Bob\nfoobar,Caesar""""""\nsio = StringIO(csvdata)\npd.read_csv(sio, dtype={""user_id"": int, ""username"": object})\n\nValueError: invalid literal for long() with base 10: \'foobar\'\n', 'dtype=object', 'dtype=unicode', 'unicode', 'object', ""'foobar'"", 'int']"
1209;;0;27236748;19482970.0;2;44;;;;['dataframe.columns.values.tolist()', 'In [1]: %timeit [column for column in df]\n1000 loops, best of 3: 81.6 s per loop\n\nIn [2]: %timeit df.columns.values.tolist()\n10000 loops, best of 3: 16.1 s per loop\n\nIn [3]: %timeit list(df)\n10000 loops, best of 3: 44.9 s per loop\n\nIn [4]: % timeit list(df.columns.values)\n10000 loops, best of 3: 38.4 s per loop\n', 'list(dataframe)']
1210;;1;27242735;27236275.0;2;44;;;;['affinity_matrix.columns']
1211;;2;27266225;27263805.0;2;30;;;;"["">>> df\n                samples  subject  trial_num\n0  [-0.07, -2.9, -2.44]        1          1\n1   [-1.52, -0.35, 0.1]        1          2\n2  [-0.17, 0.57, -0.65]        1          3\n3  [-0.82, -1.06, 0.47]        2          1\n4   [0.79, 1.35, -0.09]        2          2\n5   [1.17, 1.14, -1.79]        2          3\n>>>\n>>> s = df.apply(lambda x: pd.Series(x['samples']),axis=1).stack().reset_index(level=1, drop=True)\n>>> s.name = 'sample'\n>>>\n>>> df.drop('samples', axis=1).join(s)\n   subject  trial_num  sample\n0        1          1   -0.07\n0        1          1   -2.90\n0        1          1   -2.44\n1        1          2   -1.52\n1        1          2   -0.35\n1        1          2    0.10\n2        1          3   -0.17\n2        1          3    0.57\n2        1          3   -0.65\n3        2          1   -0.82\n3        2          1   -1.06\n3        2          1    0.47\n4        2          2    0.79\n4        2          2    1.35\n4        2          2   -0.09\n5        2          3    1.17\n5        2          3    1.14\n5        2          3   -1.79\n"", 'reset_index(drop=True)', "">>> res = df.set_index(['subject', 'trial_num'])['samples'].apply(pd.Series).stack()\n>>> res = res.reset_index()\n>>> res.columns = ['subject','trial_num','sample_num','sample']\n>>> res\n    subject  trial_num  sample_num  sample\n0         1          1           0    1.89\n1         1          1           1   -2.92\n2         1          1           2    0.34\n3         1          2           0    0.85\n4         1          2           1    0.24\n5         1          2           2    0.72\n6         1          3           0   -0.96\n7         1          3           1   -2.72\n8         1          3           2   -0.11\n9         2          1           0   -1.33\n10        2          1           1    3.13\n11        2          1           2   -0.65\n12        2          2           0    0.10\n13        2          2           1    0.65\n14        2          2           2    0.15\n15        2          3           0    0.64\n16        2          3           1   -0.10\n17        2          3           2   -0.76\n""]"
1212;;1;27282644;14262433.0;2;40;;;;[]
1213;;5;27360130;13851535.0;2;151;;;;['df = df.drop(some labels)', 'df = df.drop(df[<some boolean condition>].index)', 'df = df.drop(df[df.score < 50].index)', 'df.drop(df[df.score < 50].index, inplace=True)', '|', 'or', '&', 'and', '~', 'not', 'df = df.drop(df[(df.score < 50) & (df.score > 20)].index)']
1214;;0;27361326;11067027.0;2;13;;;;['df.sort_index(axis=1, inplace=True)\n']
1215;;2;27362540;17383094.0;2;68;;;;['df.somecolumn = df.somecolumn.astype(int)']
1216;;1;27368948;19798153.0;2;13;;;;['Series', 'In [41]: p=pd.Series([1,2,3])\n\nIn [42]: p.apply(lambda x: pd.Series([x, x]))\nOut[42]: \n   0  1\n0  1  1\n1  2  2\n2  3  3\n\nIn [43]: p.map(lambda x: pd.Series([x, x]))\nOut[43]: \n0    0    1\n1    1\ndtype: int64\n1    0    2\n1    2\ndtype: int64\n2    0    3\n1    3\ndtype: int64\ndtype: object\n', 'apply', 'series.apply(download_file_for_every_element) \n', 'Map', '1 2 3 4 5\n2 1 4 5 3\n', '1 2 3 4 5\n1 2 5 3 4\n', 'map', '0.15.1', 'In [39]: p=pd.Series([1,0,3,4,2])\n\nIn [40]: p.map(p)\nOut[40]: \n0    0\n1    1\n2    4\n3    2\n4    3\ndtype: int64\n']
1217;;4;27385043;16236684.0;2;65;;;;"['zip', "">>> df = pd.DataFrame([[i] for i in range(10)], columns=['num'])\n>>> df\n    num\n0    0\n1    1\n2    2\n3    3\n4    4\n5    5\n6    6\n7    7\n8    8\n9    9\n\n>>> def powers(x):\n>>>     return x, x**2, x**3, x**4, x**5, x**6\n\n>>> df['p1'], df['p2'], df['p3'], df['p4'], df['p5'], df['p6'] = \\\n>>>     zip(*df['num'].map(powers))\n\n>>> df\n        num     p1      p2      p3      p4      p5      p6\n0       0       0       0       0       0       0       0\n1       1       1       1       1       1       1       1\n2       2       2       4       8       16      32      64\n3       3       3       9       27      81      243     729\n4       4       4       16      64      256     1024    4096\n5       5       5       25      125     625     3125    15625\n6       6       6       36      216     1296    7776    46656\n7       7       7       49      343     2401    16807   117649\n8       8       8       64      512     4096    32768   262144\n9       9       9       81      729     6561    59049   531441\n""]"
1218;;0;27422749;27405483.0;2;37;;;;"[""df.groupby('l_customer_id_i').agg(lambda x: ','.join(x))"", 'df.groupby(...)', 'GroupBy', ""grouped = df.groupby('A')\n\nfor name, group in grouped:\n    ...\n"", 'df.groupby(...).agg(...)', 'transform', 'apply', 'mean']"
1219;;3;27520877;21487329.0;2;17;;;;"[""import pandas as pd\nvalues = [[1,2], [2,5]]\ndf = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])\ndf.columns.name = 'Type'\ndf.index.name = 'Index'\ndf.plot(lw=2, colormap='jet', marker='.', markersize=10, title='Video streaming dropout by category')\n"", 'plt.ylabel']"
1220;;0;27617290;16249736.0;2;10;;;;['import pymongo\nimport pandas as pd\nfrom pymongo import MongoClient\nclient = MongoClient()\ndb = client.database_name\ncollection = db.collection_name\ndata = pd.DataFrame(list(collection.find()))\n']
1221;;5;27667801;27667759.0;2;50;;;;['ix', 'ix', 'iloc', 'loc']
1222;;0;27680109;27673231.0;2;41;;;;"[""df = DataFrame({'x': [1,2]})\ndf_sub = df[0:1]\ndf_sub.x = -1\nprint(df)\n"", 'x\n0 -1\n1  2\n', 'df_sub_copy = df[0:1].copy()\ndf_sub_copy.x = -1\n']"
1223;;2;27747726;21269399.0;2;6;;;;"[""import pandas as pd\nfrom datetime import datetime\nheaders = ['col1', 'col2', 'col3', 'col4'] \ndtypes = [datetime, datetime, str, float] \npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes)\n"", 'TypeError: data type not understood\n', ""import pandas as pd\nfrom datetime import datetime\nheaders = ['col1', 'col2', 'col3', 'col4'] \ndtypes = [datetime.datetime, datetime.datetime, str, float] \npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes)\n""]"
1224;;2;27791362;15017072.0;2;32;;;;"['import pandas as pd\nfrom StringIO import StringIO\n\ncsv = r""""""dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nbar,20090103,b,5""""""\n\ndf = pd.read_csv(StringIO(csv),\n        header=0,\n        index_col=[""date"", ""loc""], \n        usecols=[""date"", ""loc"", ""x""],\n        parse_dates=[""date""])\n', '                x\ndate       loc\n2009-01-01 a    1\n2009-01-02 a    3\n2009-01-03 a    5\n2009-01-01 b    1\n2009-01-02 b    3\n2009-01-03 b    5\n']"
1225;;2;27844045;27842613.0;2;39;;;;"[""In [60]: df_agg = df.groupby(['job','source']).agg({'count':sum})\n"", ""In [63]: g = df_agg['count'].groupby(level=0, group_keys=False)\n"", 'In [64]: res = g.apply(lambda x: x.order(ascending=False).head(3))\n', 'nlargest', 'In [65]: g.nlargest(3)\nOut[65]:\njob     source\nmarket  A         5\n        D         4\n        B         3\nsales   E         7\n        C         6\n        B         4\ndtype: int64\n']"
1226;;0;27905350;27905295.0;2;35;;;;"['fillna', 'ffill', "">>> df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])\n>>> df.fillna(method='ffill')\n   0  1  2\n0  1  2  3\n1  4  2  3\n2  4  2  9\n"", 'bfill', 'inplace=True', ""df.fillna(method='ffill', inplace=True)\n""]"
1227;;0;27905354;27905295.0;2;7;;;;"['pandas.DataFrame.fillna', ""method='ffill'"", ""'ffill'"", ""'bfill'"", ""import pandas as pd\n\ndf = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])\ndf = df.fillna(method='ffill')\n\nprint(df)\n#   0  1  2\n#0  1  2  3\n#1  4  2  3\n#2  4  2  9\n"", 'pandas.DataFrame.ffill']"
1228;;0;27951930;27517425.0;2;59;;;;"['.transform', '.apply', '.transform', 'Series', ""df.groupby('A').transform(lambda x: (x['C'] - x['D']))\ndf.groupby('A').transform(lambda x: (x['C'] - x['D']).mean())\n"", '.transform', 'transform', 'len(input_column)', '.transform', 'Series', 'Series', ""zscore = lambda x: (x - x.mean()) / x.std() # Note that it does not reference anything outside of 'x' and for transform 'x' is one column.\ndf.groupby('A').transform(zscore)\n"", '       C      D\n0  0.989  0.128\n1 -0.478  0.489\n2  0.889 -0.589\n3 -0.671 -1.150\n4  0.034 -0.285\n5  1.149  0.662\n6 -1.404 -0.907\n7 -0.509  1.653\n', ""df.groupby('A')['C'].transform(zscore)\n"", '0    0.989\n1   -0.478\n2    0.889\n3   -0.671\n4    0.034\n5    1.149\n6   -1.404\n7   -0.509\n', '.apply', ""df.groupby('A')['C'].apply(zscore)"", ""df.groupby('A').apply(zscore)\n"", 'ValueError: operands could not be broadcast together with shapes (6,) (2,)\n', '.transform', ""df['sum_C'] = df.groupby('A')['C'].transform(sum)\ndf.sort('A') # to clearly see the scalar ('sum') applies to the whole column of the group\n"", '     A      B      C      D  sum_C\n1  bar    one  1.998  0.593  3.973\n3  bar  three  1.287 -0.639  3.973\n5  bar    two  0.687 -1.027  3.973\n4  foo    two  0.205  1.274  4.373\n2  foo    two  0.128  0.924  4.373\n6  foo    one  2.113 -0.516  4.373\n7  foo  three  0.657 -1.179  4.373\n0  foo    one  1.270  0.201  4.373\n', '.apply', 'NaNs', 'sum_C', '.apply', 'Series', ""df.groupby('A')['C'].apply(sum)\n"", 'A\nbar    3.973\nfoo    4.373\n', '.transform', ""df[df.groupby(['B'])['D'].transform(sum) < -1]\n\n     A      B      C      D\n3  bar  three  1.287 -0.639\n7  foo  three  0.657 -1.179\n""]"
1229;;0;27954411;22470690.0;2;52;;;;"['select_dtypes()', ""In [2]: df = pd.DataFrame({'NAME': list('abcdef'),\n    'On_Time': [True, False] * 3,\n    'On_Budget': [False, True] * 3})\n\nIn [3]: df.select_dtypes(include=['bool'])\nOut[3]:\n  On_Budget On_Time\n0     False    True\n1      True   False\n2     False    True\n3      True   False\n4     False    True\n5      True   False\n\nIn [4]: mylist = list(df.select_dtypes(include=['bool']).columns)\n\nIn [5]: mylist\nOut[5]: ['On_Budget', 'On_Time']\n""]"
1230;;0;27975191;27975069.0;2;6;;;;"["">>> mask = df['ids'].str.contains('ball')    \n>>> mask\n0     True\n1     True\n2    False\n3     True\nName: ids, dtype: bool\n\n>>> df[mask]\n     ids  vals\n0  aball     1\n1  bball     2\n3  fball     4\n""]"
1231;;7;27975230;27975069.0;2;47;;;;"['In [3]: df[df[\'ids\'].str.contains(""ball"")]\nOut[3]:\n     ids  vals\n0  aball     1\n1  bball     2\n3  fball     4\n']"
1232;;3;27975789;27975069.0;2;25;;;;"[""df[df['ids'].str.contains('ball', na = False)] # valid for (at least) pandas version 0.17.1\n"", ""df['ids']"", 'ids', 'df[ids]', 'pandas.Series', ""df['ids'].str"", 'lower', 'contains', ""df['ids'].str.contains('ball')"", 'True', 'False', ""df[df['ids'].str.contains('ball')]"", 'na = False']"
1233;;3;27999688;17709641.0;2;35;;;;['easy_install --upgrade numpy\n', 'pip install pandas\n']
1234;;3;28006809;28006793.0;2;50;;;;['tolist', '>>> df = pd.DataFrame([[1,2,3],[3,4,5]])\n>>> lol = df.values.tolist()\n>>> lol\n[[1L, 2L, 3L], [3L, 4L, 5L]]\n']
1235;;0;28142820;17071871.0;2;7;;;;"[""df.loc[df['column_name'] == some_value]\n"", ""df.loc[df['column_name'].isin(some_values)]\n"", ""import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n               'B': 'one one two three two two one three'.split(),\n               'C': np.arange(8), 'D': np.arange(8) * 2})\nprint(df)\n#      A      B  C   D\n# 0  foo    one  0   0\n# 1  bar    one  1   2\n# 2  foo    two  2   4\n# 3  bar  three  3   6\n# 4  foo    two  4   8\n# 5  bar    two  5  10\n# 6  foo    one  6  12\n# 7  foo  three  7  14\n\nprint(df.loc[df['A'] == 'foo'])\n"", '     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n', ""print(df.loc[df['B'].isin(['one','three'])])\n"", '      A      B  C   D\n0  foo    one  0   0\n1  bar    one  1   2\n3  bar  three  3   6\n6  foo    one  6  12\n7  foo  three  7  14\n', ""df = df.set_index(['A'])\nprint(df.loc['foo'])\n"", '  A      B  C   D\nfoo    one  0   0\nfoo    two  2   4\nfoo    two  4   8\nfoo    one  6  12\nfoo  three  7  14\n']"
1236;;2;28150450;20297332.0;2;9;;;;['len(df.columns)\n', 'len(df.index)\n', 'df.shape\n']
1237;;2;28155580;25039626.0;2;38;;;;"['select_dtypes', ""numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnewdf = df.select_dtypes(include=numerics)\n""]"
1238;;2;28159296;11869910.0;2;47;;;;"[""df = pd.DataFrame( np.random.randn(30,3), columns = ['a','b','c'])\ndf_filtered = df.query('a>0').query('0<b<2')\n"", ""df_filtered = df.query('a>0 and 0<b<2')\n""]"
1239;;1;28182629;12680754.0;2;36;;;;"[""b = DataFrame(a.var1.str.split(',').tolist(), index=a.var2).stack()\nb = b.reset_index()[[0, 'var2']] # var1 variable is currently labeled 0\nb.columns = ['var1', 'var2'] # renaming var1\n""]"
1240;;0;28192263;12207326.0;2;6;;;;"[""[my_series[c].value_counts() for c in list(my_series.select_dtypes(include=['O']).columns)]\n"", ""my_series.select_dtypes(include=['O']) \n"", ""list(my_series.select_dtypes(include=['O']).columns) \n"", ""[my_series[c].value_counts() for c in list(my_series.select_dtypes(include=['O']).columns)] \n""]"
1241;;0;28218909;28218698.0;2;11;;;;['ix', 'df1.ix[:,1]\n', 'df1.ix[0,]\n', 'df1.ix[:,1]\n', 'df1.ix[0,1]\n', 'enumerate()', 'returns.keys():']
1242;;6;28236391;28236305.0;2;23;;;;"['a', '1', 'b', 'loc', "">>> df.loc[df['a'] == 1, 'b'].sum()\n15\n"", 'groupby', 'a', "">>> df.groupby('a')['b'].sum()[1]\n15\n"", 'groupby', 'a']"
1243;;2;28252957;13682044.0;2;19;;;;"[""data['result'].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n""]"
1244;;2;28312011;28311655.0;2;40;;;;"['In [11]: df = pd.DataFrame([[""foo1""], [""foo2""], [""bar""], [np.nan]], columns=[\'a\'])\n\nIn [12]: df.a.str.contains(""foo"")\nOut[12]:\n0     True\n1     True\n2    False\n3      NaN\nName: a, dtype: object\n\nIn [13]: df.a.str.contains(""foo"", na=False)\nOut[13]:\n0     True\n1     True\n2    False\n3    False\nName: a, dtype: bool\n', 'str.replace', 'In [21]: df.loc[df.a.str.contains(""foo"", na=False)]\nOut[21]:\n      a\n0  foo1\n1  foo2\n']"
1245;;3;28371706;11622652.0;2;31;;;;"['chunksize', ""import pandas as pd\nimport sqlite3\nfrom pandas.io import sql\nimport subprocess\n\n# In and output file paths\nin_csv = '../data/my_large.csv'\nout_sqlite = '../data/my.sqlite'\n\ntable_name = 'my_table' # name for the SQLite database table\nchunksize = 100000 # number of lines to process at each iteration\n\n# columns that should be read from the CSV file\ncolumns = ['molecule_id','charge','db','drugsnow','hba','hbd','loc','nrb','smiles']\n\n# Get number of lines in the CSV file\nnlines = subprocess.check_output('wc -l %s' % in_csv, shell=True)\nnlines = int(nlines.split()[0]) \n\n# connect to database\ncnx = sqlite3.connect(out_sqlite)\n\n# Iteratively read CSV and dump lines into the SQLite table\nfor i in range(0, nlines, chunksize):\n\n    df = pd.read_csv(in_csv,  \n            header=None,  # no header, define column header manually later\n            nrows=chunksize, # number of rows to read at each iteration\n            skiprows=i)   # skip rows that were already read\n\n    # columns to read        \n    df.columns = columns\n\n    sql.to_sql(df, \n                name=table_name, \n                con=cnx, \n                index=False, # don't use CSV file index\n                index_label='molecule_id', # use a unique column from DataFrame as index\n                if_exists='append') \ncnx.close()    \n""]"
1246;;3;28390992;26837998.0;2;95;;;;"[""df = df.fillna('')\n"", ""df.column1 = df.column1.fillna('')\n""]"
1247;;3;28479181;24645153.0;2;15;;;;"[""dfTest = pd.DataFrame({\n           'A':[14.00,90.20,90.95,96.27,91.21],\n           'B':[103.02,107.26,110.35,114.23,114.68], \n           'C':['big','small','big','small','small']\n         })\ndfTest[['A','B']] = dfTest[['A','B']].apply(\n                           lambda x: MinMaxScaler().fit_transform(x))\ndfTest\n\n    A           B           C\n0   0.000000    0.000000    big\n1   0.926219    0.363636    small\n2   0.935335    0.628645    big\n3   1.000000    0.961407    small\n4   0.938495    1.000000    small\n""]"
1248;;2;28507257;26873127.0;2;20;;;;['from IPython.display import display\ndisplay(df)  # OR\nprint df.to_html()\n']
1249;;2;28541443;23307301.0;2;45;;;;"['df.loc[<row selection>, <column selection>]\n', ""w.loc[w.female != 'female', 'female'] = 0\nw.loc[w.female == 'female', 'female'] = 1\n""]"
1250;;7;28648923;15891038.0;2;270;;;;"['pd.to_numeric', 'apply', 'errors', 'NaN', 's', "">>> s = pd.Series(['1', '2', '4.7', 'pandas', '10'])\n>>> s\n0         1\n1         2\n2       4.7\n3    pandas\n4        10\ndtype: object\n"", "">>> pd.to_numeric(s) # or pd.to_numeric(s, errors='raise')\nValueError: Unable to parse string\n"", 'NaN', "">>> pd.to_numeric(s, errors='coerce')\n0     1.0\n1     2.0\n2     4.7\n3     NaN\n4    10.0\ndtype: float64\n"", "">>> pd.to_numeric(s, errors='ignore')\n# the original Series is returned untouched\n"", 'DataFrame.apply', "">>> a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\n>>> df = pd.DataFrame(a, columns=['col1','col2','col3'])\n>>> df\n  col1 col2  col3\n0    a  1.2   4.2\n1    b   70  0.03\n2    x    5     0\n"", ""df[['col2','col3']] = df[['col2','col3']].apply(pd.to_numeric)\n"", 'float64', ""df.apply(pd.to_numeric, errors='ignore')\n"", 'pd.to_datetime', 'pd.to_timedelta', 'infer_objects()', "">>> df = pd.DataFrame({'a': [7, 1, 5], 'b': ['3','2','1']}, dtype='object')\n>>> df.dtypes\na    object\nb    object\ndtype: object\n"", 'infer_objects()', '>>> df = df.infer_objects()\n>>> df.dtypes\na     int64\nb    object\ndtype: object\n', 'df.astype(int)']"
1251;;1;28680078;28679930.0;2;39;;;;"['In [91]: df = pd.DataFrame(dict(A=[5,3,5,6], C=[""foo"",""bar"",""fooXYZbar"", ""bat""]))\n\nIn [92]: df\nOut[92]:\n   A          C\n0  5        foo\n1  3        bar\n2  5  fooXYZbar\n3  6        bat\n\nIn [93]: df[df.C.str.contains(""XYZ"") == False]\nOut[93]:\n   A    C\n0  5  foo\n1  3  bar\n3  6  bat\n']"
1252;;0;28756099;25537399.0;2;12;;;;"[""import pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n\n# R code on R sample dataset\n\n#> anova(with(ChickWeight, lm(weight ~ Time + Diet)))\n#Analysis of Variance Table\n#\n#Response: weight\n#           Df  Sum Sq Mean Sq  F value    Pr(>F)\n#Time        1 2042344 2042344 1576.460 < 2.2e-16 ***\n#Diet        3  129876   43292   33.417 < 2.2e-16 ***\n#Residuals 573  742336    1296\n#write.csv(file='ChickWeight.csv', x=ChickWeight, row.names=F)\n\ncw = pd.read_csv('ChickWeight.csv')\n\ncw_lm=ols('weight ~ Time + C(Diet)', data=cw).fit() #Specify C for Categorical\nprint(sm.stats.anova_lm(cw_lm, typ=2))\n#                  sum_sq   df            F         PR(>F)\n#C(Diet)    129876.056995    3    33.416570   6.473189e-20\n#Time      2016357.148493    1  1556.400956  1.803038e-165\n#Residual   742336.119560  573          NaN            NaN\n""]"
1253;;1;28847219;14734533.0;2;12;;;;"[""gb.get_group('foo')\n"", 'gb.groups', ""df.loc[gb.groups['foo']]\n"", ""df.loc[gb.groups['foo'],('A','B')]\n""]"
1254;;1;28881373;13331698.0;2;9;;;;"[""df['col_3'] = df.col_1.combine(df.col_2, func=get_sublist)\n"", 'ValueError: setting an array element with a sequence.\n', ""df['col_3'] = df.col_1.astype(object).combine(df.col_2, func=get_sublist)\n\ndf\n\n   ID   col_1   col_2   col_3\n0   1   0   1   [a, b]\n1   2   2   4   [c, d, e]\n2   3   3   5   [d, e, f]\n""]"
1255;;6;28902170;28901683.0;2;46;;;;"[""In [119]:\n\ncommon = df1.merge(df2,on=['col1','col2'])\nprint(common)\ndf1[(~df1.col1.isin(common.col1))&(~df1.col2.isin(common.col2))]\n   col1  col2\n0     1    10\n1     2    11\n2     3    12\nOut[119]:\n   col1  col2\n3     4    13\n4     5    14\n"", 'isin', 'NaN', 'In [138]:\n\ndf1[~df1.isin(df2)].dropna()\nOut[138]:\n   col1  col2\n3     4    13\n4     5    14\n', ""df2 = pd.DataFrame(data = {'col1' : [2, 3,4], 'col2' : [11, 12,13]})\n"", 'In [140]:\n\ndf1[~df1.isin(df2)].dropna()\nOut[140]:\n   col1  col2\n0     1    10\n1     2    11\n2     3    12\n3     4    13\n4     5    14\n']"
1256;;6;28931750;28931224.0;2;49;;;;"['freq_series.plot', 'ax', 'fig', 'ax.patches', 'ax.text', 'import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrequencies = [6, 16, 75, 160, 244, 260, 145, 73, 16, 4, 1]   # bring some raw data\n\nfreq_series = pd.Series.from_array(frequencies)   # in my original code I create a series and run on that, so for consistency I create a series from the list.\n\nx_labels = [108300.0, 110540.0, 112780.0, 115020.0, 117260.0, 119500.0, 121740.0, 123980.0, 126220.0, 128460.0, 130700.0]\n\n# now to plot the figure...\nplt.figure(figsize=(12, 8))\nax = freq_series.plot(kind=\'bar\')\nax.set_title(""Amount Frequency"")\nax.set_xlabel(""Amount ($)"")\nax.set_ylabel(""Frequency"")\nax.set_xticklabels(x_labels)\n\nrects = ax.patches\n\n# Now make some labels\nlabels = [""label%d"" % i for i in xrange(len(rects))]\n\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha=\'center\', va=\'bottom\')\n\nplt.savefig(""image.png"")\n']"
1257;;0;28991603;17465045.0;2;9;;;;"['date_parser', ""dateparse = lambda dates: [pd.datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in dates]\n\ndf = pd.read_csv(infile, parse_dates=['datetime'], date_parser=dateparse)\n""]"
1258;;8;29036738;27365467.0;2;61;;;;"['        date\n0 2001-08-10\n1 2002-08-31\n2 2003-08-29\n3 2006-06-21\n4 2002-03-27\n5 2003-07-14\n6 2004-06-15\n7 2003-08-14\n8 2003-07-29\n', 'df[""date""] = df[""date""].astype(""datetime64"")\n', 'df.groupby(df[""date""].dt.month).count().plot(kind=""bar"")\n', '.dt', 'df.groupby([df[""date""].dt.year, df[""date""].dt.month]).count().plot(kind=""bar"")\n']"
1259;;7;29074073;11707586.0;2;15;;;;"[""pd.set_option('display.width', pd.util.terminal.get_terminal_size()[0])\n""]"
1260;;1;29108799;12065885.0;2;29;;;;"[""rpt.query('STK_ID in (600809,600141,600329)')\n"", ""rpt.query('60000 < STK_ID < 70000')\n""]"
1261;;0;29177664;29177498.0;2;42;;;;"['df', ""df.Temp_Rating.fillna(df.Farheit, inplace=True)\ndel df['Farheit']\ndf.columns = 'File heat Observations'.split()\n"", 'NaN', 'df.Farheit', ""'Farheit'"", 'DataFrame']"
1262;;7;29233999;29226210.0;2;26;;;;['pandas', 'sc.textFile', 'pandas.read_csv', 'toPandas()', 'Series', 'interpolate', 'Column', 'pandas.read_csv']
1263;;1;29247205;11869910.0;2;6;;;;['In [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [99]: df[(df.A == 1) & (df.D == 6)]\nOut[99]:\n   A  B  C  D\nd  1  3  9  6\n', '(... == True)', 'df[((df.A==1) == True) | ((df.D==6) == True)]\n']
1264;;3;29262040;23330654.0;2;42;;;;"[""for i, row in df.iterrows():\n  ifor_val = something\n  if <condition>:\n    ifor_val = something_else\n  df.set_value(i,'ifor',ifor_val)\n""]"
1265;;0;29281494;26187759.0;2;20;;;;['from multiprocessing import Pool, cpu_count\n\ndef applyParallel(dfGrouped, func):\n    with Pool(cpu_count()) as p:\n        ret_list = p.map(func, [group for name, group in dfGrouped])\n    return pandas.concat(ret_list)\n', 'axis=1', 'pandas.concat()']
1266;;0;29287549;29287224.0;2;41;;;;['header=None', 'usecols=[3,6]', 'df = pd.read_csv(file_path, header=None, usecols=[3,6])\n']
1267;;0;29319200;14940743.0;2;113;;;;"[""    ##Using DataFrame.drop\n    df.drop(df.columns[[1, 2]], axis=1, inplace=True)\n\n    # drop by Name\n    df1 = df1.drop(['B', 'C'], axis=1)\n\n\n    ## Select the ones you want\n    df1 = df[['a','d']]\n""]"
1268;;0;29370182;29370057.0;2;104;;;;"['df.loc[mask]', 'df[start_date : end_date]', ""df['date']"", 'datetime64[ns]', ""df['date'] = pd.to_datetime(df['date'])  \n"", 'start_date', 'end_date', 'datetime.datetime', 'np.datetime64', 'pd.Timestamp', ""mask = (df['date'] > start_date) & (df['date'] <= end_date)\n"", 'df.loc[mask]\n', 'df', 'df = df.loc[mask]\n', ""import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.random((200,3)))\ndf['date'] = pd.date_range('2000-1-1', periods=200, freq='D')\nmask = (df['date'] > '2000-6-1') & (df['date'] <= '2000-6-10')\nprint(df.loc[mask])\n"", '            0         1         2       date\n153  0.208875  0.727656  0.037787 2000-06-02\n154  0.750800  0.776498  0.237716 2000-06-03\n155  0.812008  0.127338  0.397240 2000-06-04\n156  0.639937  0.207359  0.533527 2000-06-05\n157  0.416998  0.845658  0.872826 2000-06-06\n158  0.440069  0.338690  0.847545 2000-06-07\n159  0.202354  0.624833  0.740254 2000-06-08\n160  0.465746  0.080888  0.155452 2000-06-09\n161  0.858232  0.190321  0.432574 2000-06-10\n', 'date', 'df.loc[start_date:end_date]', ""import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.random((200,3)))\ndf['date'] = pd.date_range('2000-1-1', periods=200, freq='D')\ndf = df.set_index(['date'])\nprint(df.loc['2000-6-1':'2000-6-10'])\n"", '                   0         1         2\ndate                                    \n2000-06-01  0.040457  0.326594  0.492136    # <- includes start_date\n2000-06-02  0.279323  0.877446  0.464523\n2000-06-03  0.328068  0.837669  0.608559\n2000-06-04  0.107959  0.678297  0.517435\n2000-06-05  0.131555  0.418380  0.025725\n2000-06-06  0.999961  0.619517  0.206108\n2000-06-07  0.129270  0.024533  0.154769\n2000-06-08  0.441010  0.741781  0.470402\n2000-06-09  0.682101  0.375660  0.009916\n2000-06-10  0.754488  0.352293  0.339337\n', 'df.loc[start_date : end_date]', 'start_date', 'end_date', 'pd.read_csv', 'parse_dates', 'date', 'datetime64', 'parse_dates', ""df['date'] = pd.to_datetime(df['date'])""]"
1269;;2;29383624;18039057.0;2;7;;;;"[""sep='/t'"", 'read_csv', '(\\t)', '/t', 'data=pd.read_csv(""File_path"", sep=\'\\t\')\n']"
1270;;0;29432741;29432629.0;2;64;;;;['matshow()', 'plt.matshow(dataframe.corr())']
1271;;0;29494537;19482970.0;2;19;;;;['df.columns.tolist()\n']
1272;;4;29500330;19530568.0;2;40;;;;"[""grouped = df.groupby('A')\n\ndf = grouped.aggregate(lambda x: tuple(x))\n\ndf['grouped'] = df['B'] + df['C']\n""]"
1273;;0;29517089;29517072.0;2;40;;;;"[""df['Name']='abc'"", ""In [79]:\n\ndf\nOut[79]:\n         Date, Open, High,  Low,  Close\n0  01-01-2015,  565,  600,  400,    450\nIn [80]:\n\ndf['Name'] = 'abc'\ndf\nOut[80]:\n         Date, Open, High,  Low,  Close Name\n0  01-01-2015,  565,  600,  400,    450  abc\n""]"
1274;;0;29517102;29517072.0;2;11;;;;"[""df['Name'] = 'abc'\n"", 'Name', 'abc']"
1275;;7;29528483;12286607.0;2;62;;;;"['heatmap()', ""import numpy as np \nfrom pandas import DataFrame\nimport seaborn as sns\n%matplotlib\n\nIndex= ['aaa', 'bbb', 'ccc', 'ddd', 'eee']\nCols = ['A', 'B', 'C', 'D']\ndf = DataFrame(abs(np.random.randn(5, 4)), index=Index, columns=Cols)\n\nsns.heatmap(df)\n"", '%matplotlib']"
1276;;5;29528804;29525808.0;2;67;;;;['df = pd.read_sql(query.statement, query.session.bind)\n', 'pandas.read_sql']
1277;;0;29530303;29530232.0;2;11;;;;['df.isnull().any().any()']
1278;;0;29530559;29530232.0;2;59;;;;['import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randn(10,6))\n# Make a few areas have NaN values\ndf.iloc[1:3,1] = np.nan\ndf.iloc[5,3] = np.nan\ndf.iloc[7:9,5] = np.nan\n', '          0         1         2         3         4         5\n0  0.520113  0.884000  1.260966 -0.236597  0.312972 -0.196281\n1 -0.837552       NaN  0.143017  0.862355  0.346550  0.842952\n2 -0.452595       NaN -0.420790  0.456215  1.203459  0.527425\n3  0.317503 -0.917042  1.780938 -1.584102  0.432745  0.389797\n4 -0.722852  1.704820 -0.113821 -1.466458  0.083002  0.011722\n5 -0.622851 -0.251935 -1.498837       NaN  1.098323  0.273814\n6  0.329585  0.075312 -0.690209 -3.807924  0.489317 -0.841368\n7 -1.123433 -1.187496  1.868894 -2.046456 -0.949718       NaN\n8  1.133880 -0.110447  0.050385 -1.158387  0.188222       NaN\n9 -0.513741  1.196259  0.704537  0.982395 -0.585040 -1.693810\n', 'df.isnull().any().any()', 'isnull()', '       0      1      2      3      4      5\n0  False  False  False  False  False  False\n1  False   True  False  False  False  False\n2  False   True  False  False  False  False\n3  False  False  False  False  False  False\n4  False  False  False  False  False  False\n5  False  False  False   True  False  False\n6  False  False  False  False  False  False\n7  False  False  False  False  False   True\n8  False  False  False  False  False   True\n9  False  False  False  False  False  False\n', 'df.isnull().any()', 'NaN', '0    False\n1     True\n2    False\n3     True\n4    False\n5     True\ndtype: bool\n', '.any()', 'True', '> df.isnull().any().any()\nTrue\n', 'df.isnull().sum().sum()', 'NaN', '.any().any()', 'NaN', 'df.isnull().sum()\n0    0\n1    2\n2    0\n3    1\n4    0\n5    2\ndtype: int64\n', 'df.isnull().sum().sum()\n5\n']
1279;;8;29530601;29530232.0;2;134;;;;['df.isnull().values.any()\n', 'In [2]: df = pd.DataFrame(np.random.randn(1000,1000))\n\nIn [3]: df[df > 0.9] = pd.np.nan\n\nIn [4]: %timeit df.isnull().any().any()\n100 loops, best of 3: 14.7 ms per loop\n\nIn [5]: %timeit df.isnull().values.sum()\n100 loops, best of 3: 2.15 ms per loop\n\nIn [6]: %timeit df.isnull().sum().sum()\n100 loops, best of 3: 18 ms per loop\n\nIn [7]: %timeit df.isnull().values.any()\n1000 loops, best of 3: 948 s per loop\n', 'df.isnull().sum().sum()', 'NaNs']
1280;;0;29576803;29576430.0;2;33;;;;"['np.random.permutation', 'np.random.choice', 'In [12]: df = pd.read_csv(StringIO(s), sep=""\\s+"")\n\nIn [13]: df\nOut[13]: \n    Col1  Col2  Col3  Type\n0      1     2     3     1\n1      4     5     6     1\n20     7     8     9     2\n21    10    11    12     2\n45    13    14    15     3\n46    16    17    18     3\n\nIn [14]: df.iloc[np.random.permutation(len(df))]\nOut[14]: \n    Col1  Col2  Col3  Type\n46    16    17    18     3\n45    13    14    15     3\n20     7     8     9     2\n0      1     2     3     1\n1      4     5     6     1\n21    10    11    12     2\n', 'df_shuffled.reset_index(drop=True)']"
1281;;1;29665452;26873127.0;2;119;;;;"['HTML()', 'display()', 'from IPython.display import display, HTML\n\n# Assuming that dataframes df1 and df2 are already defined:\nprint ""Dataframe 1:""\ndisplay(df1)\nprint ""Dataframe 2:""\nHTML(df2.to_html())\n', 'print df1.to_html()', 'IPython.core.display']"
1282;;3;29675706;21360361.0;2;9;;;;"['display', 'clear_output', 'KeyboardInterrupt', ""import matplotlib.pylab as plt\nimport pandas as pd\nimport numpy as np\nimport time\nfrom IPython import display\n%matplotlib inline\n\ni = pd.date_range('2013-1-1',periods=100,freq='s')\n\nwhile True:\n    try:\n        plt.plot(pd.Series(data=np.random.randn(100), index=i))\n        display.display(plt.gcf())\n        display.clear_output(wait=True)\n        time.sleep(1)\n    except KeyboardInterrupt:\n        break\n""]"
1283;;2;29763653;29763620.0;2;52;;;;"['df.columns', ""df.ix[:, df.columns != 'b']\n\n          a         c         d\n0  0.561196  0.013768  0.772827\n1  0.882641  0.615396  0.075381\n2  0.368824  0.651378  0.397203\n3  0.788730  0.568099  0.869127\n""]"
1284;;0;29774704;17241004.0;2;18;;;;"[""df.index.get_level_values('name_sub_index')\n"", 'name_sub_index', 'FrozenList', 'df.index.names']"
1285;;2;29815523;29815129.0;2;45;;;;"['df.T.to_dict().values()', ""In [1]: df\nOut[1]:\n   customer  item1   item2   item3\n0         1  apple    milk  tomato\n1         2  water  orange  potato\n2         3  juice   mango   chips\n\nIn [2]: df.T.to_dict().values()\nOut[2]:\n[{'customer': 1.0, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},\n {'customer': 2.0, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},\n {'customer': 3.0, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]\n"", ""df.to_dict('records')"", ""In [20]: timeit df.T.to_dict().values()\n1000 loops, best of 3: 395 s per loop\n\nIn [21]: timeit df.to_dict('records')\n10000 loops, best of 3: 53 s per loop\n""]"
1286;;2;29816143;29815129.0;2;67;;;;"[""df.to_dict('records')"", ""In [2]: df.to_dict('records')\nOut[2]:\n[{'customer': 1L, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},\n {'customer': 2L, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},\n {'customer': 3L, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]\n""]"
1287;;0;29837754;13331698.0;2;24;;;;"[""import pandas as pd\n\ndef sublst(row):\n    return lst[row['J1']:row['J2']]\n\ndf = pd.DataFrame({'ID':['1','2','3'], 'J1': [0,2,3], 'J2':[1,4,5]})\nprint df\nlst = ['a','b','c','d','e','f']\n\ndf['J3'] = df.apply(sublst,axis=1)\nprint df\n"", '  ID  J1  J2\n0  1   0   1\n1  2   2   4\n2  3   3   5\n  ID  J1  J2      J3\n0  1   0   1     [a]\n1  2   2   4  [c, d]\n2  3   3   5  [d, e]\n', ""import pandas as pd\n\ndf = pd.DataFrame({'ID':['1','2','3'], 'J1': [0,2,3], 'J2':[1,4,5]})\nprint df\nlst = ['a','b','c','d','e','f']\n\ndf['J3'] = df.apply(lambda row:lst[row['J1']:row['J2']],axis=1)\nprint df\n""]"
1288;;0;29910919;14262433.0;2;11;;;;[]
1289;;7;29916004;13148429.0;2;8;;;;"[""df = df[['mean'] + df.columns[:-1].tolist()]\n""]"
1290;;2;29919489;29919306.0;2;45;;;;"['idxmax()', '>>> df.idxmax(axis=1)\n0    Communications\n1          Business\n2    Communications\n3    Communications\n4          Business\ndtype: object\n', ""df['Max'] = df.idxmax(axis=1)""]"
1291;;3;29922207;13148429.0;2;72;;;;"['<dataframe>.columns', ""In [39]: df\nOut[39]: \n          0         1         2         3         4  mean\n0  0.172742  0.915661  0.043387  0.712833  0.190717     1\n1  0.128186  0.424771  0.590779  0.771080  0.617472     1\n2  0.125709  0.085894  0.989798  0.829491  0.155563     1\n3  0.742578  0.104061  0.299708  0.616751  0.951802     1\n4  0.721118  0.528156  0.421360  0.105886  0.322311     1\n5  0.900878  0.082047  0.224656  0.195162  0.736652     1\n6  0.897832  0.558108  0.318016  0.586563  0.507564     1\n7  0.027178  0.375183  0.930248  0.921786  0.337060     1\n8  0.763028  0.182905  0.931756  0.110675  0.423398     1\n9  0.848996  0.310562  0.140873  0.304561  0.417808     1\n\nIn [40]: df = df[['mean', 4,3,2,1]]\n"", 'In [41]: df\nOut[41]: \n   mean         4         3         2         1\n0     1  0.190717  0.712833  0.043387  0.915661\n1     1  0.617472  0.771080  0.590779  0.424771\n2     1  0.155563  0.829491  0.989798  0.085894\n3     1  0.951802  0.616751  0.299708  0.104061\n4     1  0.322311  0.105886  0.421360  0.528156\n5     1  0.736652  0.195162  0.224656  0.082047\n6     1  0.507564  0.586563  0.318016  0.558108\n7     1  0.337060  0.921786  0.930248  0.375183\n8     1  0.423398  0.110675  0.931756  0.182905\n9     1  0.417808  0.304561  0.140873  0.310562\n']"
1292;;1;29930255;20158597.0;2;12;;;;['pandas.tools.tile.qcut', 'bins = algos.quantile(x, quantiles)', 'import numpy as np\nimport pandas as pd\nimport pandas.core.algorithms as algos\nfrom pandas import Series\n', 'zs = np.zeros(300)\nrs = np.random.randint(1, 100, size=300)\narr=np.concatenate((zs, rs))\nser = Series(arr)\n', 'bins = algos.quantile(np.unique(ser), np.linspace(0, 1, 11))\nresult = pd.tools.tile._bins_to_cuts(ser, bins, include_lowest=True)\n', 'In[61]: result.value_counts()\nOut[61]: \n[0, 9.3]        323\n(27.9, 38.2]     37\n(9.3, 18.6]      37\n(88.7, 99]       35\n(57.8, 68.1]     32\n(68.1, 78.4]     31\n(78.4, 88.7]     30\n(38.2, 48.5]     27\n(48.5, 57.8]     26\n(18.6, 27.9]     22\ndtype: int64\n', 'mx = np.ma.masked_equal(arr, 0, copy=True)\nbins = algos.quantile(arr[~mx.mask], np.linspace(0, 1, 11))\nbins = np.insert(bins, 0, 0)\nbins[1] = bins[1]-(bins[1]/2)\nresult = pd.tools.tile._bins_to_cuts(arr, bins, include_lowest=True)\n', 'In[133]: result.value_counts()\nOut[133]: \n[0, 0.5]        300\n(0.5, 11]        32\n(11, 18.8]       28\n(18.8, 29.7]     30\n(29.7, 39]       35\n(39, 50]         26\n(50, 59]         31\n(59, 71]         31\n(71, 79.2]       27\n(79.2, 90.2]     30\n(90.2, 99]       30\ndtype: int64\n']
1293;;1;29990874;11697887.0;2;6;;;;['class MyModel(models.Model):\n    full_name = models.CharField(max_length=25)\n    age = models.IntegerField()\n    department = models.CharField(max_length=3)\n    wage = models.FloatField()\n\nfrom django_pandas.io import read_frame\nqs = MyModel.objects.all()\ndf = read_frame(qs)\n']
1294;;2;30022658;28757389.0;2;70;;;;[]
1295;;1;30201213;12021730.0;2;24;;;;"[""data = read_table('sample.txt', skiprows=3, header=None, delim_whitespace=True)\n""]"
1296;;9;30267328;24458645.0;2;43;;;;"['fit()', 'transform()', 'fit_transform()', ""import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\n\n# Create some toy data in a Pandas dataframe\nfruit_data = pd.DataFrame({\n    'fruit':  ['apple','orange','pear','orange'],\n    'color':  ['red','orange','green','green'],\n    'weight': [5,6,3,4]\n})\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n"", 'fruit', 'color', 'weight', ""MultiColumnLabelEncoder(columns = ['fruit','color']).fit_transform(fruit_data)\n"", 'fruit_data', 'columns', ""MultiColumnLabelEncoder().fit_transform(fruit_data.drop('weight',axis=1))\n"", ""encoding_pipeline = Pipeline([\n    ('encoding',MultiColumnLabelEncoder(columns=['fruit','color']))\n    # add more pipeline steps as needed\n])\nencoding_pipeline.fit_transform(fruit_data)\n""]"
1297;;0;30292938;17530542.0;2;10;;;;"['def appendDFToCSV_void(df, csvFilePath, sep="",""):\n    import os\n    if not os.path.isfile(csvFilePath):\n        df.to_csv(csvFilePath, mode=\'a\', index=False, sep=sep)\n    elif len(df.columns) != len(pd.read_csv(csvFilePath, nrows=1, sep=sep).columns):\n        raise Exception(""Columns do not match!! Dataframe has "" + str(len(df.columns)) + "" columns. CSV file has "" + str(len(pd.read_csv(csvFilePath, nrows=1, sep=sep).columns)) + "" columns."")\n    elif not (df.columns == pd.read_csv(csvFilePath, nrows=1, sep=sep).columns).all():\n        raise Exception(""Columns and column order of dataframe and csv file do not match!!"")\n    else:\n        df.to_csv(csvFilePath, mode=\'a\', index=False, sep=sep, header=False)\n']"
1298;;0;30370897;11728836.0;2;10;;;;"['pandas', 'rosetta', 'pandas', 'pandas', 'cython', ""import pandas as pd\nimport para_group_demo\n\ndf = pd.DataFrame({'a': [1, 2, 1, 2, 1, 1, 0], 'b': range(7)})\nprint para_group_demo.sum(df.a, df.b)\n"", '     sum\nkey     \n0      6\n1      11\n2      4\n', 'pandas', 'pandas', 'from libc.stdint cimport int64_t, uint64_t\nfrom libcpp.vector cimport vector\nfrom libcpp.unordered_map cimport unordered_map\n\ncimport cython\nfrom cython.operator cimport dereference as deref, preincrement as inc\nfrom cython.parallel import prange\n\nimport pandas as pd\n\nctypedef unordered_map[int64_t, uint64_t] counts_t\nctypedef unordered_map[int64_t, uint64_t].iterator counts_it_t\nctypedef vector[counts_t] counts_vec_t\n', 'unordered_map', 'vector', 'sum', 'def sum(crit, vals):\n    cdef int64_t[:] crit_view = crit.values\n    cdef int64_t[:] vals_view = vals.values\n', '    cdef uint64_t num_threads = 4\n    cdef uint64_t l = len(crit)\n    cdef uint64_t s = l / num_threads + 1\n    cdef uint64_t i, j, e\n    cdef counts_vec_t counts\n    counts = counts_vec_t(num_threads)\n    counts.resize(num_threads)\n    with cython.boundscheck(False):\n        for i in prange(num_threads, nogil=True): \n            j = i * s\n            e = j + s\n            if e > l:\n                e = l\n            while j < e:\n                counts[i][crit_view[j]] += vals_view[j]\n                inc(j)\n', 'unordered_map', '    cdef counts_t total\n    cdef counts_it_t it, e_it\n    for i in range(num_threads):\n        it = counts[i].begin()\n        e_it = counts[i].end()\n        while it != e_it:\n            total[deref(it).first] += deref(it).second\n            inc(it)        \n', 'DataFrame', ""    key, sum_ = [], []\n    it = total.begin()\n    e_it = total.end()\n    while it != e_it:\n        key.append(deref(it).first)\n        sum_.append(deref(it).second)\n        inc(it)\n\n    df = pd.DataFrame({'key': key, 'sum': sum_})\n    df.set_index('key', inplace=True)\n    return df\n""]"
1299;;4;30380922;11346283.0;2;38;;;;"[""old_names = ['$a', '$b', '$c', '$d', '$e'] \nnew_names = ['a', 'b', 'c', 'd', 'e']\ndf.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n"", 'new_names']"
1300;;2;30424537;20109391.0;2;28;;;;"['import numpy as np\nimport pandas as pd\n\nnp.random.seed(123)\n', ""df = pd.DataFrame({ \n\n    # some ways to create random data\n    'a':np.random.randn(6),\n    'b':np.random.choice( [5,7,np.nan], 6),\n    'c':np.random.choice( ['panda','python','shark'], 6),\n\n    # some ways to create systematic groups for indexing or groupby\n    # this is similar to r's expand.grid(), see note 2 below\n    'd':np.repeat( range(3), 2 ),\n    'e':np.tile(   range(2), 3 ),\n\n    # a date range and set of random dates\n    'f':pd.date_range('1/1/2011', periods=6, freq='D'),\n    'g':np.random.choice( pd.date_range('1/1/2011', periods=365, \n                          freq='D'), 6, replace=False) \n    })\n"", '          a   b       c  d  e          f          g\n0 -1.085631 NaN   panda  0  0 2011-01-01 2011-08-12\n1  0.997345   7   shark  0  1 2011-01-02 2011-11-10\n2  0.282978   5   panda  1  0 2011-01-03 2011-10-30\n3 -1.506295   7  python  1  1 2011-01-04 2011-09-07\n4 -0.578600 NaN   shark  2  0 2011-01-05 2011-02-27\n5  1.651437   7  python  2  1 2011-01-06 2011-02-03\n', 'np.repeat', 'np.tile', 'd', 'e', 'expand.grid()', 'expand.grid()', 'itertools', 'np.meshgrid', 'np.random.choice', 'g', 'replace=False', 'np.tile', 'date_range', ""stocks = pd.DataFrame({ \n    'ticker':np.repeat( ['aapl','goog','yhoo','msft'], 25 ),\n    'date':np.tile( pd.date_range('1/1/2011', periods=25, freq='D'), 4 ),\n    'price':(np.random.randn(100).cumsum() + 10) })\n"", "">>> stocks.head(5)\n\n        date      price ticker\n0 2011-01-01   9.497412   aapl\n1 2011-01-02  10.261908   aapl\n2 2011-01-03   9.438538   aapl\n3 2011-01-04   9.515958   aapl\n4 2011-01-05   7.554070   aapl\n\n>>> stocks.groupby('ticker').head(2)\n\n         date      price ticker\n0  2011-01-01   9.497412   aapl\n1  2011-01-02  10.261908   aapl\n25 2011-01-01   8.277772   goog\n26 2011-01-02   7.714916   goog\n50 2011-01-01   5.613023   yhoo\n51 2011-01-02   6.397686   yhoo\n75 2011-01-01  11.736584   msft\n76 2011-01-02  11.944519   msft\n""]"
1301;;2;30454743;24147278.0;2;20;;;;"[""from sklearn.cross_validation import train_test_split\n\n\ny = df.pop('output')\nX = df\n\nX_train,X_test,y_train,y_test = train_test_split(X.index,y,test_size=0.2)\nX.iloc[X_train] # return dataframe train\n""]"
1302;;3;30511605;19482970.0;2;21;;;;"["">>> list(my_dataframe)\n['y', 'gdp', 'cap']\n"", "">>> [c for c in my_dataframe]\n['y', 'gdp', 'cap']\n""]"
1303;;4;30512931;23668427.0;2;133;;;;"['import pandas as pd\n', 'reduce', 'dfs = [df0, df1, df2, dfN]\n', 'name', ""df_final = reduce(lambda left,right: pd.merge(left,right,on='name'), dfs)\n"", 'reduce', 'functools']"
1304;;2;30522778;30522724.0;2;59;;;;"[""lst1 = range(100)\nlst2 = range(100)\nlst3 = range(100)\npercentile_list = pd.DataFrame(\n    {'lst1Tite': lst1,\n     'lst2Tite': lst2,\n     'lst3Tite': lst3\n    })\n\npercentile_list\n    lst1Tite  lst2Tite  lst3Tite\n0          0         0         0\n1          1         1         1\n2          2         2         2\n3          3         3         3\n4          4         4         4\n5          5         5         5\n6          6         6         6\n...\n"", 'np.column_stack', 'zip', ""percentile_list = pd.DataFrame(np.column_stack([lst1, lst2, lst3]), \n                               columns=['lst1tite', 'lst2itie', 'lst3tite'])\n""]"
1305;;0;30523225;30522982.0;2;6;;;;[]
1306;;0;30525128;30522982.0;2;23;;;;"[""users2['name']""]"
1307;;1;30531939;30530663.0;2;52;;;;"['drop_duplicates', ""In [29]: df = pd.DataFrame({'a':[1,2,1,2], 'b':[3,4,3,5]})\n\nIn [30]: df\nOut[30]:\n   a  b\n0  1  3\n1  2  4\n2  1  3\n3  2  5\n\nIn [32]: df.drop_duplicates()\nOut[32]:\n   a  b\n0  1  3\n1  2  4\n3  2  5\n"", 'subset']"
1308;;0;30535957;19618912.0;2;32;;;;"[""s1 = pd.merge(df1, df2, how='inner', on=['user_id'])\n""]"
1309;;0;30546734;11346283.0;2;76;;;;"[""df.columns = df.columns.str.replace('$','')\n""]"
1310;;3;30566899;16476924.0;2;37;;;;"['df.apply()', ""def valuation_formula(x, y):\n    return x * y * 0.5\n\ndf['price'] = df.apply(lambda row: valuation_formula(row['x'], row['y']), axis=1)\n""]"
1311;;0;30587837;13295735.0;2;15;;;;['idx = pd.IndexSlice\ndf.loc[idx[:,mask_1],idx[mask_2,:]].fillna(value=0,inplace=True)\n', 'df.update(df.loc[idx[:,mask_1],idx[[mask_2],:]].fillna(value=0))\n']
1312;;3;30590280;20025325.0;2;30;;;;"[""df = pd.DataFrame({'d': [1, 2, 3]}, index=['FOO', 'BAR', 'BAZ'])\ndf\n        d\nFOO     1\nBAR     2\nBAZ     3\n\ndf.index = df.index.map(str.lower)\ndf\n        d\nfoo     1\nbar     2\nbaz     3\n"", 'df.index.map(str.lower)', 'pd.Series(df.index.map(str.lower))\n', 'Index', 'StringAccessorMixin', 'df.index.str.lower()\n']"
1313;;1;30633167;18022845.0;2;7;;;;"[""df.columns.name = 'foo'\n"", ""df.index.name = 'foo'\n""]"
1314;;1;30647987;22483588.0;2;13;;;;['subplots=True', 'layout=(,)', 'plot', 'df.plot(subplots=True, layout=(1,2))\n', 'fig.add_subplot()']
1315;;2;30653988;30631325.0;2;37;;;;"[""import pandas as pd\nimport mysql.connector\nfrom sqlalchemy import create_engine\n\nengine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)\ndata.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)\n""]"
1316;;6;30691921;19124601.0;2;221;;;;"[""with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n    print(df)\n""]"
1317;;1;30733959;10464738.0;2;6;;;;"[""import pandas\nfrom   numpy import nan\nimport numpy\n\ndataGrid = pandas.DataFrame({1: {1: 1, 3: 2},\n                             2: {1: 3, 3: 4}})\n\n\ndef getExtrapolatedInterpolatedValue(x, y):\n    global dataGrid\n    if x not in dataGrid.index:\n        dataGrid.ix[x] = nan\n        dataGrid = dataGrid.sort()\n        dataGrid = dataGrid.interpolate(method='index', axis=0).ffill(axis=0).bfill(axis=0)\n\n    if y not in dataGrid.columns.values:\n        dataGrid = dataGrid.reindex(columns=numpy.append(dataGrid.columns.values, y))\n        dataGrid = dataGrid.sort_index(axis=1)\n        dataGrid = dataGrid.interpolate(method='index', axis=1).ffill(axis=1).bfill(axis=1)\n\n    return dataGrid[y][x]\n\n\nprint getExtrapolatedInterpolatedValue(2, 1.4)\n>>2.3\n""]"
1318;;1;30777185;12555323.0;2;12;;;;"['SettingWithCopyWarning', ""df.insert(len(df.columns), 'e', pd.Series(np.random.randn(sLength),  index=df.index))\n""]"
1319;;3;30778300;13611065.0;2;10;;;;['import numpy as np\nimport functools\ndef conjunction(*conditions):\n    return functools.reduce(np.logical_and, conditions)\n\nc_1 = data.col1 == True\nc_2 = data.col2 < 64\nc_3 = data.col3 != 4\n\ndata_filtered = data[conjunction(c1,c2,c3)]\n']
1320;;2;30858753;12190874.0;2;6;;;;[]
1321;;6;30926717;30926670.0;2;24;;;;"['concat', ""In [23]:\ndf = pd.DataFrame(columns=['A'])\ndf\n\nOut[23]:\nEmpty DataFrame\nColumns: [A]\nIndex: []\n\nIn [24]:    \npd.concat([df,pd.DataFrame(columns=list('BCD'))])\n\nOut[24]:\nEmpty DataFrame\nColumns: [A, B, C, D]\nIndex: []\n""]"
1322;;0;30943503;30926670.0;2;26;;;;"['df.reindex', ""In [18]: df = pd.DataFrame(np.random.randint(10, size=(5,1)), columns=['A'])\n\nIn [19]: df\nOut[19]: \n   A\n0  4\n1  7\n2  0\n3  7\n4  6\n\nIn [20]: df.reindex(columns=list('ABCD'))\nOut[20]: \n   A   B   C   D\n0  4 NaN NaN NaN\n1  7 NaN NaN NaN\n2  0 NaN NaN NaN\n3  7 NaN NaN NaN\n4  6 NaN NaN NaN\n"", 'reindex', ""In [31]: df.reindex(columns=list('DCBA'))\nOut[31]: \n    D   C   B  A\n0 NaN NaN NaN  4\n1 NaN NaN NaN  7\n2 NaN NaN NaN  0\n3 NaN NaN NaN  7\n4 NaN NaN NaN  6\n"", 'reindex', 'fill_value', ""In [22]: df.reindex(columns=list('ABCD'), fill_value=0)\nOut[22]: \n   A  B  C  D\n0  4  0  0  0\n1  7  0  0  0\n2  0  0  0  0\n3  7  0  0  0\n4  6  0  0  0\n""]"
1323;;2;30971633;12190874.0;2;13;;;;[]
1324;;0;30991980;22233488.0;2;10;;;;"[""df.columns = ['a', 'b']""]"
1325;;0;31017785;20937538.0;2;7;;;;"['applymap', 'import pandas as pd\ndf = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=[\'foo\',\'bar\',\'baz\',\'quux\'],\n                  columns=[\'cost\'])\n\ndf = df.applymap(""${0:.2f}"".format)\n']"
1326;;1;31029857;31029560.0;2;43;;;;"['value_counts', ""df['colour'].value_counts().plot(kind='bar')\n""]"
1327;;0;31029861;31029560.0;2;9;;;;"[""df.groupby('colour').size().plot(kind='bar')\n""]"
1328;;2;31033603;31029560.0;2;6;;;;"['mosaic', ""from statsmodels.graphics.mosaicplot import mosaic\nplt.rcParams['font.size'] = 16.0\nmosaic(df, ['direction', 'colour']);\n""]"
1329;;1;31036962;12680754.0;2;7;;;;"['>> a=pd.DataFrame({""var1"":""a,b,c d,e,f"".split(),""var2"":[1,2]})\n>> s = a.var1.str.split("","").apply(pd.Series, 1).stack()\n>> s.index = s.index.droplevel(-1)\n>> del a[\'var1\']\n>> a.join(s)\n   var2 var1\n0     1    a\n0     1    b\n0     1    c\n1     2    d\n1     2    e\n1     2    f\n']"
1330;;1;31037040;10715965.0;2;34;;;;"[""mycolumns = ['A', 'B']\ndf = pd.DataFrame(columns=mycolumns)\nrows = [[1,2],[3,4],[5,6]]\nfor row in rows:\n    df.loc[len(df)] = row\n""]"
1331;;4;31061820;20219254.0;2;21;;;;"[""writer = pd.ExcelWriter(excel_file, engine='openpyxl')\n"", ""AttributeError: 'Workbook' object has no attribute 'add_worksheet'\n""]"
1332;;0;31075478;20461165.0;2;18;;;;"[""df['si_name'] = R.index.get_level_values('si_name') \n"", 'si_name']"
1333;;2;31076657;15325182.0;2;6;;;;"[""frame[frame.filename.str.match('*.'+MetaData+'.*') & frame.file_path.str.match('C:\\test\\test.txt')]\n""]"
1334;;1;31173785;19913659.0;2;42;;;;"[""df['color'] = ['red' if x == 'Z' else 'green' for x in df['Set']]\n"", ""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})\n%timeit df['color'] = ['red' if x == 'Z' else 'green' for x in df['Set']]\n%timeit df['color'] = np.where(df['Set']=='Z', 'green', 'red')\n%timeit df['color'] = df.Set.map( lambda x: 'red' if x == 'Z' else 'green')\n\n1000 loops, best of 3: 239 s per loop\n1000 loops, best of 3: 523 s per loop\n1000 loops, best of 3: 263 s per loop\n""]"
1335;;2;31296878;17071871.0;2;58;;;;"['select * from table where column_name = some_value\n', 'table[table.column_name == some_value]\n', 'table((table.column_name == some_value) | (table.column_name2 == some_value2))\n', ""table.query('column_name == some_value | column_name2 == some_value2')\n"", ""import pandas as pd\n\n# Create data set\nd = {'foo':[100, 111, 222], \n     'bar':[333, 444, 555]}\ndf = pd.DataFrame(d)\n\n# Full dataframe:\ndf\n\n# Shows:\n#    bar   foo \n# 0  333   100\n# 1  444   111\n# 2  555   222\n\n# Output only the row(s) in df where foo is 222:\ndf[df.foo == 222]\n\n# Shows:\n#    bar  foo\n# 2  555  222\n"", 'df[df.foo == 222]', '222', 'df[(df.foo == 222) | (df.bar == 444)]\n#    bar  foo\n# 1  444  111\n# 2  555  222\n', ""df.query('foo == 222 | bar == 444')\n""]"
1336;;2;31357733;31357611.0;2;40;;;;"['ax', ""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randn(100,5))\n\n# you get ax from here\nax = df.plot()\ntype(ax)  # matplotlib.axes._subplots.AxesSubplot\n\n# manipulate\nvals = ax.get_yticks()\nax.set_yticklabels(['{:3.2f}%'.format(x*100) for x in vals])\n""]"
1337;;0;31364094;22483588.0;2;7;;;;['figure', 'subplot', 'plt.gca()', 'plt.figure(1)\nplt.subplot(2,2,1)\ndf.A.plot() #no need to specify for first axis\nplt.subplot(2,2,2)\ndf.B.plot(ax=plt.gca())\nplt.subplot(2,2,3)\ndf.C.plot(ax=plt.gca())\n']
1338;;5;31364127;31361721.0;2;30;;;;['map_partitions', 'map_partitions', 'df.map_partitions(func, columns=...)\n', 'pandas apply', 'map', 'apply', 'map', 'df.mycolumn.map(func)\n', 'apply', 'df.apply(func, axis=1)\n', 'dask.dataframes', 'df = dd.read_csv(...)\n\nfrom dask.multiprocessing import get\ndf.map_partitions(func, columns=...).compute(get=get)\n', 'apply', 'apply', 'numba', 'numba', 'In [1]: import numpy as np\nIn [2]: import pandas as pd\nIn [3]: s = pd.Series([10000]*120)\n\nIn [4]: %paste\ndef slow_func(k):\n    A = np.random.normal(size = k) # k = 10000\n    s = 0\n    for a in A:\n        if a > 0:\n            s += 1\n        else:\n            s -= 1\n    return s\n## -- End pasted text --\n\nIn [5]: %time _ = s.apply(slow_func)\nCPU times: user 345 ms, sys: 3.28 ms, total: 348 ms\nWall time: 347 ms\n\nIn [6]: import numba\nIn [7]: fast_func = numba.jit(slow_func)\n\nIn [8]: %time _ = s.apply(fast_func)  # First time incurs compilation overhead\nCPU times: user 179 ms, sys: 0 ns, total: 179 ms\nWall time: 175 ms\n\nIn [9]: %time _ = s.apply(fast_func)  # Subsequent times are all gain\nCPU times: user 68.8 ms, sys: 27 s, total: 68.8 ms\nWall time: 68.7 ms\n', 'numba', 'dask', 'pandas']
1339;;0;31384328;29432629.0;2;32;;;;"[""def plot_corr(df,size=10):\n    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n\n    Input:\n        df: pandas DataFrame\n        size: vertical and horizontal size of the plot'''\n\n    corr = df.corr()\n    fig, ax = plt.subplots(figsize=(size, size))\n    ax.matshow(corr)\n    plt.xticks(range(len(corr.columns)), corr.columns);\n    plt.yticks(range(len(corr.columns)), corr.columns);\n""]"
1340;;3;31431997;13411544.0;2;56;;;;"['df.drop(df.columns[[0,1,3]], axis=1, inplace=True)\n', 'df.drop(df.columns[[0]], axis=1, inplace=True)\n', 'inplace', 'column-name', ""df.pop('column-name')\n"", ""df = DataFrame.from_items([('A', [1, 2, 3]), ('B', [4, 5, 6]), ('C', [7,8, 9])], orient='index', columns=['one', 'two', 'three'])\n"", 'print df', '   one  two  three\nA    1    2      3\nB    4    5      6\nC    7    8      9\n', 'df.drop(df.columns[[0]], axis=1, inplace=True)', 'print df', '   two  three\nA    2      3\nB    5      6\nC    8      9\n', ""three = df.pop('three')"", 'print df', '   two\nA    2\nB    5\nC    8\n']"
1341;;0;31502974;23199796.0;2;13;;;;"['lambda', 'scipy stats', ""df = pd.DataFrame(np.random.randn(100, 3), columns=list('ABC'))\n\ndf[df.apply(lambda x: np.abs(x - x.mean()) / x.std() < 3).all(axis=1)]\n"", 'df[((df.B - df.B.mean()) / df.B.std()).abs() < 3]\n']"
1342;;3;31541600;17557074.0;2;12;;;;"['dtype', 'name, age, birthday\nAlice, 30, 1985-01-01\nBob, 35, 1980-01-01\nCharlie, 25, 1990-01-01\n', ""dtype={'age':int}"", '.read_csv()', 'name, age, birthday\nAlice, 30, 1985-01-01\nBob, 35, 1980-01-01\nCharlie, 25, 1990-01-01\nDennis, 40+, None-Ur-Bz\n', ""dtype={'age':int}"", '.read_csv()', '""40+""', ""df = pd.DataFrame(pd.np.random.choice(['1.0', '0.6666667', '150000.1'],(100000, 10)))\nresource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# 224544 (~224 MB)\n\ndf = pd.DataFrame(pd.np.random.choice([1.0, 0.6666667, 150000.1],(100000, 10)))\nresource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# 79560 (~79 MB)\n""]"
1343;;0;31543407;18089667.0;2;10;;;;['resource', 'StringIO']
1344;;0;31570270;28218698.0;2;6;;;;['DataFrame', 'for column_name, column in df.transpose().iterrows():\n    print column_name\n']
1345;;5;31573180;26473681.0;2;29;;;;['LC_ALL=C', 'export LC_ALL=C', '$ LC_ALL=C pip install ...']
1346;;11;31593712;31593201.0;2;370;;;;"['ix', 'loc', 'iloc', 'ix', 'ix', 'loc', 'iloc', 'ix', 'loc', 'iloc', 'ix', 'ix', 'ix', 'ix', '>>> s = pd.Series(np.nan, index=[49,48,47,46,45, 1, 2, 3, 4, 5])\n>>> s\n49   NaN\n48   NaN\n47   NaN\n46   NaN\n45   NaN\n1    NaN\n2    NaN\n3    NaN\n4    NaN\n5    NaN\n', 's.iloc[:3]', 's.loc[:3]', '>>> s.iloc[:3] # slice the first three rows\n49   NaN\n48   NaN\n47   NaN\n\n>>> s.loc[:3] # slice up to and including label 3\n49   NaN\n48   NaN\n47   NaN\n46   NaN\n45   NaN\n1    NaN\n2    NaN\n3    NaN\n\n>>> s.ix[:3] # the integer is in the index so s.ix[:3] works like loc\n49   NaN\n48   NaN\n47   NaN\n46   NaN\n45   NaN\n1    NaN\n2    NaN\n3    NaN\n', 's.ix[:3]', 's.loc[:3]', '6', 's.iloc[:6]', 's.loc[:6]', '6', '>>> s.iloc[:6]\n49   NaN\n48   NaN\n47   NaN\n46   NaN\n45   NaN\n1    NaN\n\n>>> s.loc[:6]\nKeyError: 6\n\n>>> s.ix[:6]\nKeyError: 6\n', 's.ix[:6]', 'loc', '6', 'iloc', 'ix', 'iloc', "">>> s2 = pd.Series(np.nan, index=['a','b','c','d','e', 1, 2, 3, 4, 5])\n>>> s2.index.is_mixed() # index is mix of types\nTrue\n>>> s2.ix[:6] # behaves like iloc given integer\na   NaN\nb   NaN\nc   NaN\nd   NaN\ne   NaN\n1   NaN\n"", 'ix', 'loc', "">>> s2.ix[:'c'] # behaves like loc given non-integer\na   NaN\nb   NaN\nc   NaN\n"", 'loc', 'iloc', 'ix', "">>> df = pd.DataFrame(np.nan, \n                      index=list('abcde'),\n                      columns=['x','y','z', 8, 9])\n>>> df\n    x   y   z   8   9\na NaN NaN NaN NaN NaN\nb NaN NaN NaN NaN NaN\nc NaN NaN NaN NaN NaN\nd NaN NaN NaN NaN NaN\ne NaN NaN NaN NaN NaN\n"", 'ix', 'ix', '4', "">>> df.ix[:'c', :4]\n    x   y   z   8\na NaN NaN NaN NaN\nb NaN NaN NaN NaN\nc NaN NaN NaN NaN\n"", 'iloc', "">>> df.iloc[:df.index.get_loc('c') + 1, :4]\n    x   y   z   8\na NaN NaN NaN NaN\nb NaN NaN NaN NaN\nc NaN NaN NaN NaN\n"", 'get_loc()', 'iloc']"
1347;;2;31594055;31593201.0;2;59;;;;"['iloc', 'df.iloc[0]\n', 'df.iloc[-5:]\n', 'df.iloc[:, 2]    # the : in the first position indicates all rows\n', 'df.iloc[:3, :3] # The upper-left 3 X 3 entries (assuming df has 3+ rows and columns)\n', '.loc', ""df = pd.DataFrame(index=['a', 'b', 'c'], columns=['time', 'date', 'name'])\n"", ""df.loc['a']     # equivalent to df.iloc[0]\n"", ""'date'"", ""df.loc['b':, 'date']   # equivalent to df.iloc[1:, 1]\n"", 'DataFrame', 'iloc', 'loc', 'df.loc[:5]', '__getitem__', ""df['time']    # equivalent to df.loc[:, 'time']\n"", '.ix', ""df.ix[:2, 'time']    # the first two rows of the 'time' column\n"", 'loc', ' b = [True, False, True]\n df.loc[b] \n', 'df', 'df[b]', ""df.loc[b, 'name'] = 'Mary', 'John'\n""]"
1348;;3;31611678;31609600.0;2;71;;;;"['%matplotlib inline\n', '""matplotlib.axes._subplots.AxesSubplot at 0xebf8b70"".\n']"
1349;;1;31839240;16597265.0;2;39;;;;"[""df = pd.DataFrame()\ndf = df.append({'name': 'Zed', 'age': 9, 'height': 2}, ignore_index=True)\n"", '   age  height name\n0    9       2  Zed\n']"
1350;;0;31859215;28757389.0;2;28;;;;"[""df = pd.DataFrame({'A':['a', 'b', 'c'], 'B':[54, 67, 89]}, index=[100, 200, 300])\n\ndf\n\n                        A   B\n                100     a   54\n                200     b   67\n                300     c   89\nIn [19]:    \ndf.loc[100]\n\nOut[19]:\nA     a\nB    54\nName: 100, dtype: object\n\nIn [20]:    \ndf.iloc[0]\n\nOut[20]:\nA     a\nB    54\nName: 100, dtype: object\n\nIn [24]:    \ndf2 = df.set_index([df.index,'A'])\ndf2\n\nOut[24]:\n        B\n    A   \n100 a   54\n200 b   67\n300 c   89\n\nIn [25]:    \ndf2.ix[100, 'a']\n\nOut[25]:    \nB    54\nName: (100, a), dtype: int64\n""]"
1351;;10;31939145;24458645.0;2;121;;;;['df.apply(LabelEncoder().fit_transform)\n', 'from collections import defaultdict\nd = defaultdict(LabelEncoder)\n', 'LabelEncoder', '# Encoding the variable\nfit = df.apply(lambda x: d[x.name].fit_transform(x))\n\n# Inverse the encoded\nfit.apply(lambda x: d[x.name].inverse_transform(x))\n\n# Using the dictionary to label future data\ndf.apply(lambda x: d[x.name].transform(x))\n']
1352;;3;32011969;32011359.0;2;55;;;;"[""dataframe['c'].cat.codes"", 'select_dtypes', ""In [75]: df = pd.DataFrame({'col1':[1,2,3,4,5], 'col2':list('abcab'),  'col3':list('ababb')})\n\nIn [76]: df['col2'] = df['col2'].astype('category')\n\nIn [77]: df['col3'] = df['col3'].astype('category')\n\nIn [78]: df.dtypes\nOut[78]:\ncol1       int64\ncol2    category\ncol3    category\ndtype: object\n"", 'select_dtypes', '.cat.codes', ""In [80]: cat_columns = df.select_dtypes(['category']).columns\n\nIn [81]: cat_columns\nOut[81]: Index([u'col2', u'col3'], dtype='object')\n\nIn [83]: df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n\nIn [84]: df\nOut[84]:\n   col1  col2  col3\n0     1     0     0\n1     2     1     1\n2     3     2     0\n3     4     0     1\n4     5     1     1\n""]"
1353;;0;32066997;16266019.0;2;6;;;;['pandas', 'pandas 0.16.2', 'grp = data.groupby(by=[data.datetime_col.map(lambda x : (x.hour, x.minute))])\ngrp.count()\n', 'grp = data.groupby(by=[data.datetime_col.map(lambda x : x.hour),\n                       data.datetime_col.map(lambda x : x.minute)])\n']
1354;;1;32103253;25254016.0;2;9;;;;"[""In [4]: df = pd.DataFrame({'foo':list('ABC')}, index=[0,2,1])\nIn [5]: df['bar'] = 100\nIn [6]: df['bar'].iloc[0] = 99\n/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas-0.16.0_19_g8d2818e-py2.7-macosx-10.9-x86_64.egg/pandas/core/indexing.py:118: SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._setitem_with_indexer(indexer, value)\n"", ""In [7]: df.loc[df.index[0], 'foo']\nOut[7]: 'A'\nIn [8]: df.loc[df.index[0], 'bar'] = 99\nIn [9]: df\nOut[9]:\n  foo  bar\n0   A   99\n2   B  100\n1   C  100\n""]"
1355;;0;32103678;15943769.0;2;11;;;;['df.axes', 'len()', 'total_rows=len(df.axes[0])\ntotal_cols=len(df.axes[1])\n']
1356;;0;32131398;13148429.0;2;16;;;;"['df = df[cols]', ""cols = ['mean']  + [col for col in df if col != 'mean']\ndf = df[cols]\n"", 'cols = [df.columns[-1]] + [col for col in df if col != df.columns[-1]]\ndf = df[cols]\n', ""inserted_cols = ['a', 'b', 'c']\ncols = ([col for col in inserted_cols if col in df] \n        + [col for col in df if col not in inserted cols])\ndf = df[cols]\n""]"
1357;;1;32152755;13003051.0;2;8;;;;"['difference()', ""exclude = ['bad col1', 'bad col2']\ndf.ix[:, df.columns.difference(exclude)].hist() \n""]"
1358;;0;32244161;32244019.0;2;52;;;;"['rot=0', 'import matplotlib\nmatplotlib.style.use(\'ggplot\')\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.DataFrame({ \'celltype\':[""foo"",""bar"",""qux"",""woz""], \'s1\':[5,9,1,7], \'s2\':[12,90,13,87]})\ndf = df[[""celltype"",""s1"",""s2""]]\ndf.set_index([""celltype""],inplace=True)\ndf.plot(kind=\'bar\',alpha=0.75, rot=0)\nplt.xlabel("""")\nplt.show()\n']"
1359;;0;32245025;32244753.0;2;9;;;;"['savefig', 'sns_plot', 'sns_plot.savefig(""output.png"")\n', 'sns_plot', 'fig = sns_plot.fig\n', 'get_figure']"
1360;;2;32245026;32244753.0;2;34;;;;"['get_figure', ""sns_plot.savefig('output.png')"", 'df = sns.load_dataset(\'iris\')\nsns_plot = sns.pairplot(df, hue=\'species\', size=2.5)\nsns_plot.savefig(""output.png"")\n']"
1361;;3;32307259;10373660.0;2;54;;;;"['as_index=False', 'as_index=True', 'as_index=False', 'mean', 'sum', 'size', 'count', 'std', 'var', 'sem', 'describe', 'first', 'last', 'nth', 'min', 'max', 'DataFrame.sum()', 'Series', 'import pandas as pd\n\ndf1 = pd.DataFrame({""Name"":[""Alice"", ""Bob"", ""Mallory"", ""Mallory"", ""Bob"" , ""Mallory""],\n                    ""City"":[""Seattle"",""Seattle"",""Portland"",""Seattle"",""Seattle"",""Portland""]})\nprint df1\n#\n#       City     Name\n#0   Seattle    Alice\n#1   Seattle      Bob\n#2  Portland  Mallory\n#3   Seattle  Mallory\n#4   Seattle      Bob\n#5  Portland  Mallory\n#\ng1 = df1.groupby([""Name"", ""City""], as_index=False).count()\nprint g1\n#\n#                  City  Name\n#Name    City\n#Alice   Seattle      1     1\n#Bob     Seattle      2     2\n#Mallory Portland     2     2\n#        Seattle      1     1\n#\n', '0.17.1', 'subset', 'count', 'reset_index', 'name', 'size', 'print df1.groupby([""Name"", ""City""], as_index=False ).count()\n#IndexError: list index out of range\n\nprint df1.groupby([""Name"", ""City""]).count()\n#Empty DataFrame\n#Columns: []\n#Index: [(Alice, Seattle), (Bob, Seattle), (Mallory, Portland), (Mallory, Seattle)]\n\nprint df1.groupby([""Name"", ""City""])[[\'Name\',\'City\']].count()\n#                  Name  City\n#Name    City                \n#Alice   Seattle      1     1\n#Bob     Seattle      2     2\n#Mallory Portland     2     2\n#        Seattle      1     1\n\nprint df1.groupby([""Name"", ""City""]).size().reset_index(name=\'count\')\n#      Name      City  count\n#0    Alice   Seattle      1\n#1      Bob   Seattle      2\n#2  Mallory  Portland      2\n#3  Mallory   Seattle      1\n', 'count', 'size']"
1362;;2;32322596;11346283.0;2;14;;;;"['columns = df.columns\ncolumns = [row.replace(""$"","""") for row in columns]\ndf.rename(columns=dict(zip(columns, things)), inplace=True)\ndf.head() #to validate the output\n', 'import pandas as pd\nimport cProfile, pstats, re\n\nold_names = [\'$a\', \'$b\', \'$c\', \'$d\', \'$e\']\nnew_names = [\'a\', \'b\', \'c\', \'d\', \'e\']\ncol_dict = {\'$a\': \'a\', \'$b\': \'b\',\'$c\':\'c\',\'$d\':\'d\',\'$e\':\'e\'}\n\ndf = pd.DataFrame({\'$a\':[1,2], \'$b\': [10,20],\'$c\':[\'bleep\',\'blorp\'],\'$d\':[1,2],\'$e\':[\'texa$\',\'\']})\n\ndf.head()\n\ndef eumiro(df,nn):\n    df.columns = nn\n    #This direct renaming approach is duplicated in methodology in several other answers: \n    return df\n\ndef lexual1(df):\n    return df.rename(columns=col_dict)\n\ndef lexual2(df,col_dict):\n    return df.rename(columns=col_dict, inplace=True)\n\ndef Panda_Master_Hayden(df):\n    return df.rename(columns=lambda x: x[1:], inplace=True)\n\ndef paulo1(df):\n    return df.rename(columns=lambda x: x.replace(\'$\', \'\'))\n\ndef paulo2(df):\n    return df.rename(columns=lambda x: x.replace(\'$\', \'\'), inplace=True)\n\ndef migloo(df,on,nn):\n    return df.rename(columns=dict(zip(on, nn)), inplace=True)\n\ndef kadee(df):\n    return df.columns.str.replace(\'$\',\'\')\n\ndef awo(df):\n    columns = df.columns\n    columns = [row.replace(""$"","""") for row in columns]\n    return df.rename(columns=dict(zip(columns, \'\')), inplace=True)\n\ndef kaitlyn(df):\n    df.columns = [col.strip(\'$\') for col in df.columns]\n    return df\n\nprint \'eumiro\'\ncProfile.run(\'eumiro(df,new_names)\')\nprint \'lexual1\'\ncProfile.run(\'lexual1(df)\')\nprint \'lexual2\'\ncProfile.run(\'lexual2(df,col_dict)\')\nprint \'andy hayden\'\ncProfile.run(\'Panda_Master_Hayden(df)\')\nprint \'paulo1\'\ncProfile.run(\'paulo1(df)\')\nprint \'paulo2\'\ncProfile.run(\'paulo2(df)\')\nprint \'migloo\'\ncProfile.run(\'migloo(df,old_names,new_names)\')\nprint \'kadee\'\ncProfile.run(\'kadee(df)\')\nprint \'awo\'\ncProfile.run(\'awo(df)\')\nprint \'kaitlyn\'\ncProfile.run(\'kaitlyn(df)\')\n']"
1363;;2;32344037;18837262.0;2;42;;;;"['pandas.DataFrame()', 'pandas.DataFrame.from_dict', ""orient='index'"", ""In[7]: pandas.DataFrame.from_dict({u'2012-06-08': 388,\n u'2012-06-09': 388,\n u'2012-06-10': 388,\n u'2012-06-11': 389,\n u'2012-06-12': 389,\n u'2012-06-13': 389,\n u'2012-06-14': 389,\n u'2012-06-15': 389,\n u'2012-06-16': 389,\n u'2012-06-17': 389,\n u'2012-06-18': 390,\n u'2012-06-19': 390,\n u'2012-06-20': 390,\n u'2012-06-21': 390,\n u'2012-06-22': 390,\n u'2012-06-23': 390,\n u'2012-06-24': 390,\n u'2012-06-25': 391,\n u'2012-06-26': 391,\n u'2012-06-27': 391,\n u'2012-06-28': 391,\n u'2012-06-29': 391,\n u'2012-06-30': 391,\n u'2012-07-01': 391,\n u'2012-07-02': 392,\n u'2012-07-03': 392,\n u'2012-07-04': 392,\n u'2012-07-05': 392,\n u'2012-07-06': 392}, orient='index')\nOut[7]: \n              0\n2012-06-13  389\n2012-06-16  389\n2012-06-12  389\n2012-07-03  392\n2012-07-02  392\n2012-06-29  391\n2012-06-30  391\n2012-07-01  391\n2012-06-15  389\n2012-06-08  388\n2012-06-09  388\n2012-07-05  392\n2012-07-04  392\n2012-06-14  389\n2012-07-06  392\n2012-06-17  389\n2012-06-20  390\n2012-06-21  390\n2012-06-22  390\n2012-06-23  390\n2012-06-11  389\n2012-06-10  388\n2012-06-26  391\n2012-06-27  391\n2012-06-28  391\n2012-06-24  390\n2012-06-19  390\n2012-06-18  390\n2012-06-25  391\n""]"
1364;;0;32366268;16266019.0;2;16;;;;['times = pd.DatetimeIndex(data.datetime_col)\ngrouped = df.groupby([times.hour, times.minute])\n']
1365;;0;32397818;20069009.0;2;50;;;;"['nlargest', 'nsmallest', 'groupby', ""In [23]: df.groupby('id')['value'].nlargest(2)\nOut[23]: \nid   \n1   2    3\n    1    2\n2   6    4\n    5    3\n3   7    1\n4   8    1\ndtype: int64\n"", '.reset_index(level=1, drop=True)', 'Series', 'SeriesGroupBy']"
1366;;3;32399908;12725417.0;2;14;;;;"[""source.select_dtypes(['number']) or source.select_dtypes([np.number]""]"
1367;;4;32400969;32400867.0;2;32;;;;"['pandas.read_csv', 'io.StringIO', 'StringIO.StringIO', 'html', 'Raw', 'import pandas as pd\nimport io\nimport requests\nurl=""https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv""\ns=requests.get(url).content\nc=pd.read_csv(io.StringIO(s.decode(\'utf-8\')))\n', '0.19.2']"
1368;;3;32401251;32400867.0;2;6;;;;"['c=pd.read_csv(io.StringIO(s.decode(""utf-8"")))', 's = requests.get(url).text', 'pd.read_csv(StringIO(s))', 'read_csv', 'c = pd.read_csv(""https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv"")\n\nprint(c)\n', '                              Country         Region\n0                             Algeria         AFRICA\n1                              Angola         AFRICA\n2                               Benin         AFRICA\n3                            Botswana         AFRICA\n4                             Burkina         AFRICA\n5                             Burundi         AFRICA\n6                            Cameroon         AFRICA\n..................................\n']"
1369;;0;32469151;32468402.0;2;7;;;;"['EXPLODE', ""import numpy as np\n\ndf = pd.DataFrame({'listcol':[[1,2,3],[4,5,6]]})\nX = pd.concat([pd.DataFrame(v, index=np.repeat(k,len(v))) \n            for k,v in df.listcol.to_dict().items()])    \n"", 'pd.merge']"
1370;;0;32470490;32468402.0;2;22;;;;"['lambda', 'nearest_neighbors', 'name', 'opponent', 'name', 'opponent', ""df = (pd.DataFrame({'name': ['A.J. Price'] * 3, \n                    'opponent': ['76ers', 'blazers', 'bobcats'], \n                    'nearest_neighbors': [['Zach LaVine', 'Jeremy Lin', 'Nate Robinson', 'Isaia']] * 3})\n      .set_index(['name', 'opponent']))\n\n>>> df\n                                                    nearest_neighbors\nname       opponent                                                  \nA.J. Price 76ers     [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]\n           blazers   [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]\n           bobcats   [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]\n\ndf.reset_index(inplace=True)\nrows = []\n_ = df.apply(lambda row: [rows.append([row['name'], row['opponent'], nn]) \n                         for nn in row.nearest_neighbors], axis=1)\ndf_new = pd.DataFrame(rows, columns=df.columns).set_index(['name', 'opponent'])\n\n>>> df_new\n                    nearest_neighbors\nname       opponent                  \nA.J. Price 76ers          Zach LaVine\n           76ers           Jeremy Lin\n           76ers        Nate Robinson\n           76ers                Isaia\n           blazers        Zach LaVine\n           blazers         Jeremy Lin\n           blazers      Nate Robinson\n           blazers              Isaia\n           bobcats        Zach LaVine\n           bobcats         Jeremy Lin\n           bobcats      Nate Robinson\n           bobcats              Isaia\n"", "">>> (pd.melt(df.nearest_neighbors.apply(pd.Series).reset_index(), \n             id_vars=['name', 'opponent'],\n             value_name='nearest_neighbors')\n     .set_index(['name', 'opponent'])\n     .drop('variable', axis=1)\n     .dropna()\n     .sort_index()\n     )\n""]"
1371;;3;32489918;8991709.0;2;24;;;;['R DT key', 'R DT', 'Py pandas']
1372;;4;32529152;19377969.0;2;57;;;;"[""df = pd.DataFrame({'Year': ['2014', '2015'], 'quarter': ['q1', 'q2']})\ndf['period'] = df[['Year', 'quarter']].apply(lambda x: ''.join(x), axis=1)\n"", '   Year quarter  period\n0  2014      q1  2014q1\n1  2015      q2  2015q2\n', ""df[['Year', 'quarter']]"", ""df.iloc[:,0:2].apply(lambda x: ''.join(x), axis=1)""]"
1373;;0;32536193;20109391.0;2;7;;;;"[""df = pd.DataFrame({'A': [...], 'B': [...], ...})"", ""stocks = pd.DataFrame({ \n    'ticker':np.repeat( ['aapl','goog','yhoo','msft'], 25 ),\n    'date':np.tile( pd.date_range('1/1/2011', periods=25, freq='D'), 4 ),\n    'price':(np.random.randn(100).cumsum() + 10) })\n"", "">>> stocks.head(5).to_dict()\n{'date': {0: Timestamp('2011-01-01 00:00:00'),\n  1: Timestamp('2011-01-01 00:00:00'),\n  2: Timestamp('2011-01-01 00:00:00'),\n  3: Timestamp('2011-01-01 00:00:00'),\n  4: Timestamp('2011-01-02 00:00:00')},\n 'price': {0: 10.284260107718254,\n  1: 11.930300761831457,\n  2: 10.93741046217319,\n  3: 10.884574289565609,\n  4: 11.78005850418319},\n 'ticker': {0: 'aapl', 1: 'aapl', 2: 'aapl', 3: 'aapl', 4: 'aapl'}}\n\n>>> pd.concat([stocks.head(), stocks.tail()], ignore_index=True).to_dict()\n{'date': {0: Timestamp('2011-01-01 00:00:00'),\n  1: Timestamp('2011-01-01 00:00:00'),\n  2: Timestamp('2011-01-01 00:00:00'),\n  3: Timestamp('2011-01-01 00:00:00'),\n  4: Timestamp('2011-01-02 00:00:00'),\n  5: Timestamp('2011-01-24 00:00:00'),\n  6: Timestamp('2011-01-25 00:00:00'),\n  7: Timestamp('2011-01-25 00:00:00'),\n  8: Timestamp('2011-01-25 00:00:00'),\n  9: Timestamp('2011-01-25 00:00:00')},\n 'price': {0: 10.284260107718254,\n  1: 11.930300761831457,\n  2: 10.93741046217319,\n  3: 10.884574289565609,\n  4: 11.78005850418319,\n  5: 10.017209045035006,\n  6: 10.57090128181566,\n  7: 11.442792747870204,\n  8: 11.592953372130493,\n  9: 12.864146419530938},\n 'ticker': {0: 'aapl',\n  1: 'aapl',\n  2: 'aapl',\n  3: 'aapl',\n  4: 'aapl',\n  5: 'msft',\n  6: 'msft',\n  7: 'msft',\n  8: 'msft',\n  9: 'msft'}}\n"", ""stocks.info()\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 100 entries, 0 to 99\nData columns (total 3 columns):\ndate      100 non-null datetime64[ns]\nprice     100 non-null float64\nticker    100 non-null object\ndtypes: datetime64[ns](1), float64(1), object(1)\n"", 'to_dict', 'set_index', ""# MultiIndex example.  First create a MultiIndex DataFrame.\ndf = stocks.set_index(['date', 'ticker'])\n>>> df\n    price\ndate       ticker           \n2011-01-01 aapl    10.284260\n           aapl    11.930301\n           aapl    10.937410\n           aapl    10.884574\n2011-01-02 aapl    11.780059\n...\n\n# After resetting the index and passing the DataFrame to `to_dict`, make sure to use \n# `set_index` to restore the original MultiIndex.  This DataFrame can then be restored.\n\nd = df.reset_index().to_dict()\ndf_new = pd.DataFrame(d).set_index(['date', 'ticker'])\n>>> df_new.head()\n                       price\ndate       ticker           \n2011-01-01 aapl    10.284260\n           aapl    11.930301\n           aapl    10.937410\n           aapl    10.884574\n2011-01-02 aapl    11.780059\n""]"
1374;;3;32558621;28218698.0;2;87;;;;['for column in df:\n    print(df[column])\n']
1375;;2;32606673;15923826.0;2;66;;;;['0.16.x', 'DataFrame.sample', 'import pandas\n\ndf = pandas.DataFrame(data)\n\n# Randomly sample 70% of your dataframe\ndf_0.7 = df.sample(frac=0.7)\n\n# Randomly sample 7 elements from your dataframe\ndf_7 = df.sample(n=7)\n', 'df_rest = df.loc[~df.index.isin(df_0.7.index)]\n']
1376;;0;32658847;22084338.0;2;9;;;;['at', 'iat', 'In [1]: import numpy, pandas\n   ...: df = pandas.DataFrame(numpy.zeros(shape=[10, 10]))\n   ...: dictionary = df.to_dict()\n\nIn [2]: %timeit value = dictionary[5][5]\nThe slowest run took 34.06 times longer than the fastest. This could mean that an intermediate result is being cached \n1000000 loops, best of 3: 310 ns per loop\n\nIn [4]: %timeit value = df.loc[5, 5]\n10000 loops, best of 3: 104 s per loop\n\nIn [5]: %timeit value = df.iloc[5, 5]\n10000 loops, best of 3: 98.8 s per loop\n\nIn [6]: %timeit value = df.iat[5, 5]\nThe slowest run took 6.67 times longer than the fastest. This could mean that an intermediate result is being cached \n100000 loops, best of 3: 9.58 s per loop\n\nIn [7]: %timeit value = df.at[5, 5]\nThe slowest run took 6.59 times longer than the fastest. This could mean that an intermediate result is being cached \n100000 loops, best of 3: 9.26 s per loop\n', 'at', 'iat', 'loc', 'iloc']
1377;;0;32662331;24644656.0;2;48;;;;['print df.to_string(index=False)\n']
1378;;4;32680162;16476924.0;2;65;;;;"['iterrows()', 'itertuples()', ""df = pd.DataFrame({'a': randn(1000), 'b': randn(1000),'N': randint(100, 1000, (1000)), 'x': 'x'})\n\n%timeit [row.a * 2 for idx, row in df.iterrows()]\n# => 10 loops, best of 3: 50.3 ms per loop\n\n%timeit [row[1] * 2 for row in df.itertuples()]\n# => 1000 loops, best of 3: 541 s per loop\n""]"
1379;;3;32700453;13269890.0;2;13;;;;"[""days = pd.DataFrame({'date':list_of_days})\nstores = pd.DataFrame({'store_id':list_of_stores})\nstores['key'] = 0\ndays['key'] = 0\ndays_and_stores = days.merge(stores, how='left', on = 'key')\ndays_and_stores.drop('key',1, inplace=True)\n""]"
1380;;0;32748510;17874063.0;2;8;;;;"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randn(100,2), columns=list('AB'))\n\ndf.hist(normed=1)\n"", ""df.plot(kind='hist', normed=1, bins=20, stacked=False, alpha=.5)\n""]"
1381;;2;32783825;11391969.0;2;27;;;;['data.groupby(data.date.dt.year)\n', 'dt', 'weekofyear', 'dayofweek']
1382;;2;32801170;19384532.0;2;215;;;;"['df.groupby(key_columns).size()\n', 'key_columns', ""key_columns = ['col1','col2']"", ""In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame([['a', 1],\n                           ['b', 2],\n                           ['c', 3],\n                           ['a', 4],\n                           ['b', 5]], \n                          columns=['col1', 'col2'])\n\nIn [3]: counts = df.groupby('col1').size(); counts\nOut[3]: \ncol1\na    2\nb    2\nc    1\ndtype: int64\n"", 'counts', 'In [4]: type(counts)\nOut[4]: pandas.core.series.Series\n', ""In [5]: counts_df = pd.DataFrame(df.groupby('col1').size().rename('counts'))\n\nIn [6]: counts_df\nOut[6]: \n      counts\ncol1        \na          2\nb          2\nc          1\n\nIn [7]: type(counts_df)\nOut[7]: pandas.core.frame.DataFrame\n"", ""In[1]:\nimport numpy as np\nimport pandas as pd \n\nkeys = np.array([\n        ['A', 'B'],\n        ['A', 'B'],\n        ['A', 'B'],\n        ['A', 'B'],\n        ['C', 'D'],\n        ['C', 'D'],\n        ['C', 'D'],\n        ['E', 'F'],\n        ['E', 'F'],\n        ['G', 'H'] \n        ])\n\ndf = pd.DataFrame(np.hstack([keys,np.random.randn(10,4).round(2)]), \n                  columns = ['col1', 'col2', 'col3', 'col4', 'col5', 'col6'])\n\ndf[['col3', 'col4', 'col5', 'col6']] = \\\n    df[['col3', 'col4', 'col5', 'col6']].astype(float)\n"", 'In [2]: df.dtypes\nOut[2]:\ncol1     object\ncol2     object\ncol3    float64\ncol4    float64\ncol5    float64\ncol6    float64\ndtype: object\n\nIn [3]: df\nOut[3]:\n  col1 col2  col3  col4  col5  col6\n0    A    B  1.50 -1.70 -0.46 -0.30\n1    A    B  0.04 -0.22 -0.91  2.43\n2    A    B  0.25 -1.00 -0.78  0.46\n3    A    B  2.66 -1.56 -0.30 -0.44\n4    C    D -1.05  1.04 -0.31 -0.88\n5    C    D -0.19 -1.08  0.31 -0.91\n6    C    D -1.34 -1.83 -2.06 -2.09\n7    E    F  1.83  1.56  0.86 -0.70\n8    E    F  0.87 -1.03 -2.59 -1.35\n9    G    H -0.13  0.53 -0.40 -1.64\n', 'mean', 'count', 'agg', 'count', ""In [8]: df[['col1', 'col2', 'col3', 'col4']]\\\n            .groupby(['col1', 'col2']).agg(['mean', 'count'])\nOut[8]:\n             col3            col4      \n             mean count      mean count\ncol1 col2                              \nA    B     1.1125     4 -1.120000     4\nC    D    -0.8600     3 -0.623333     3\nE    F     1.3500     2  0.265000     2\nG    H    -0.1300     1  0.530000     1\n"", 'count', 'NaN', 'count', 'count', 'count', 'groupby', 'join', 'means', 'counts', ""In [9]: groupby_object = df[['col1', 'col2', 'col3', 'col4']]\\\n            .groupby(['col1', 'col2'])\n\nIn [10]: groupby_object.agg('mean')\\\n             .rename(columns = lambda x: x + ' mean')\\\n             .join(pd.DataFrame(groupby_object.size(), \n                                columns=['counts']))\nOut[10]:\n           col3 mean  col4 mean  counts\ncol1 col2                              \nA    B        1.1125  -1.120000       4\nC    D       -0.8600  -0.623333       3\nE    F        1.3500   0.265000       2\nG    H       -0.1300   0.530000       1\n""]"
1383;;2;32850652;14745022.0;2;12;;;;"['df[""flips""], df[""row_name""] = zip(*df[""row""].str.split().tolist())\ndel df[""row""]  \n']"
1384;;2;32909107;20297317.0;2;17;;;;['cols = [1,2,4,5,12]\ndf.drop(df.columns[cols],axis=1,inplace=True)\n']
1385;;0;32944421;18022845.0;2;6;;;;['df.columns.values']
1386;;5;32961145;14984119.0;2;8;;;;"['def duplicate_columns(frame):\n    groups = frame.columns.to_series().groupby(frame.dtypes).groups\n    dups = []\n    for t, v in groups.items():\n        dcols = frame[v].to_dict(orient=""list"")\n\n        vs = dcols.values()\n        ks = dcols.keys()\n        lvs = len(vs)\n\n        for i in range(lvs):\n            for j in range(i+1,lvs):\n                if vs[i] == vs[j]: \n                    dups.append(ks[i])\n                    break\n\n    return dups       \n', 'dups = duplicate_columns(frame)\nframe = frame.drop(dups, axis=1)\n', 'from pandas.core.common import array_equivalent\n\ndef duplicate_columns(frame):\n    groups = frame.columns.to_series().groupby(frame.dtypes).groups\n    dups = []\n\n    for t, v in groups.items():\n\n        cs = frame[v].columns\n        vs = frame[v]\n        lcs = len(cs)\n\n        for i in range(lcs):\n            ia = vs.iloc[:,i].values\n            for j in range(i+1, lcs):\n                ja = vs.iloc[:,j].values\n                if array_equivalent(ia, ja):\n                    dups.append(cs[i])\n                    break\n\n    return dups\n']"
1387;;3;32970117;18089667.0;2;24;;;;['df.memory_usage()', '>>> df.memory_usage()\n\nRow_ID            20906600\nHousehold_ID      20906600\nVehicle           20906600\nCalendar_Year     20906600\nModel_Year        20906600\n...\n', 'index=True', '>>> df.memory_usage(index=True).sum()\n731731000\n']
1388;;3;32993553;18885175.0;2;41;;;;"['df = pd.read_csv(filename.tar.gz, compression=\'gzip\', header=0, sep=\',\', quotechar=\'""\')\n']"
1389;;5;33020669;20638006.0;2;27;;;;['pd.DataFrame.from_records(d)']
1390;;2;33040290;22391433.0;2;9;;;;"[""df = pd.DataFrame({'a':list('tuhimerisabhain')})\ndf.a.value_counts()\n\n>>> df.a.value_counts()\ni    3\nh    2\na    2\nn    1\nb    1\nm    1\nr    1\nt    1\ne    1\nu    1\ns    1\n""]"
1391;;1;33050438;17098654.0;2;8;;;;"[""import pandas as pd\ndf.to_pickle('123.pkl')    #to save the dataframe, df to 123.pkl\ndf1 = pd.read_pickle('123.pkl') #to load 123.pkl back to the dataframe df\n""]"
1392;;1;33149986;13842088.0;2;7;;;;['df.loc[row_index,col_indexer] = value']
1393;;2;33161955;24251219.0;2;7;;;;"[""df = pd.read_csv('somefile.csv', low_memory=False)\n""]"
1394;;0;33178896;18689823.0;2;9;;;;"[""sub2['income'].fillna((sub2['income'].mean()), inplace=True)\n""]"
1395;;0;33250288;12525722.0;2;19;;;;"['apply', 'import numpy as np\nimport pandas as pd\n\nnp.random.seed(1)\n\ndf = pd.DataFrame(np.random.randn(4,4)* 4 + 3)\n\n          0         1         2         3\n0  9.497381  0.552974  0.887313 -1.291874\n1  6.461631 -6.206155  9.979247 -0.044828\n2  4.276156  2.002518  8.848432 -5.240563\n3  1.710331  1.463783  7.535078 -1.399565\n\ndf.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n\n          0         1         2         3\n0  0.515087  0.133967 -0.651699  0.135175\n1  0.125241 -0.689446  0.348301  0.375188\n2 -0.155414  0.310554  0.223925 -0.624812\n3 -0.484913  0.244924  0.079473  0.114448\n', 'groupby', ""df['grp'] = ['A', 'A', 'B', 'B']\n\n          0         1         2         3 grp\n0  9.497381  0.552974  0.887313 -1.291874   A\n1  6.461631 -6.206155  9.979247 -0.044828   A\n2  4.276156  2.002518  8.848432 -5.240563   B\n3  1.710331  1.463783  7.535078 -1.399565   B\n\n\ndf.groupby(['grp'])[[0,1,2,3]].apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n\n     0    1    2    3\n0  0.5  0.5 -0.5 -0.5\n1 -0.5 -0.5  0.5  0.5\n2  0.5  0.5  0.5 -0.5\n3 -0.5 -0.5 -0.5  0.5\n""]"
1396;;1;33374834;17063458.0;2;10;;;;['import pandas as pd\n\n# open the file\nxlsx = pd.ExcelFile(PATH\\FileName.xlsx)\n\n# get the first sheet as an object\nsheet1 = xlsx.parse(0)\n\n# get the first column as a list you can loop through\n# where the is 0 in the code below change to the row or column number you want    \ncolumn = sheet1.icol(0).real\n\n# get the first row as a list you can loop through\nrow = sheet1.irow(0).real\n']
1397;;1;33375383;11707586.0;2;26;;;;"[""with pd.option_context('display.max_rows', 999, 'display.max_columns', 5):\n    print df\n"", 'with']"
1398;;0;33555435;11067027.0;2;8;;;;"[""sequence = ['Q1.1','Q1.2','Q1.3',.....'Q6.1',......]\nyour_dataframe = your_dataframe.reindex(columns=sequence)\n""]"
1399;;4;33570065;17098654.0;2;22;;;;['pickle', 'pickle-p2', 'numpy.fromfile']
1400;;0;33577649;16852911.0;2;11;;;;"[""df['column'].dt.date""]"
1401;;1;33687073;16729483.0;2;9;;;;"[""df['MyColumnName'] = df['MyColumnName'].astype('float64') \n""]"
1402;;2;33768634;12190874.0;2;12;;;;['sample_dataframe = your_dataframe.sample(n=how_many_rows_you_want)\n']
1403;;0;33786696;28901683.0;2;6;;;;"[""In [77]: df1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]})\nIn [78]: df2 = pandas.DataFrame(data = {'col1' : [1, 3, 4], 'col2' : [10, 12, 13]})\nIn [79]: df1.loc[~df1.set_index(list(df1.columns)).index.isin(df2.set_index(list(df2.columns)).index)]\nOut[79]:\n   col1  col2\n1     2    11\n4     5    14\n"", 'pandas.MultiIndex.from_tuples(list(df<N>.to_records(index = False)))\n']"
1404;;0;33798922;11707586.0;2;10;;;;"[""pd.set_option('max_colwidth', 800)\n""]"
1405;;1;33837592;17141558.0;2;16;;;;"['DataFrame.sort()', 'DataFrame.sort_values', ""df.sort_values(['b', 'c'], ascending=[True, False], inplace=True)\n""]"
1406;;1;33872824;11346283.0;2;9;;;;"[""df.columns = [col.strip('$') for col in df.columns]\n"", 'strip']"
1407;;0;33913961;19377969.0;2;9;;;;"['df.map(str)', 'df.astype(str)', 'import pandas as pd\ndf = pd.DataFrame({\'Year\': [\'2014\', \'2015\'], \'quarter\': [\'q1\', \'q2\']})\n\nIn [131]: %timeit df[""Year""].map(str)\n10000 loops, best of 3: 132 us per loop\n\nIn [132]: %timeit df[""Year""].astype(str)\n10000 loops, best of 3: 82.2 us per loop\n']"
1408;;0;33986975;11346283.0;2;9;;;;"[""df.columns = ['Name1', 'Name2', 'Name3'...]\n""]"
1409;;0;33997632;10665889.0;2;39;;;;"[""# Load dataset (pip install seaborn)\n>> import seaborn.apionly as sns\n>> titanic = sns.load_dataset('titanic')\n"", "">> titanic.loc[:,['sex','age','fare']]\n"", '>> titanic.iloc[:,[2,3,6]]\n', '>> titanic.ix[:,[\x91sex\x92,\x92age\x92,\x92fare\x92]]\n', '>> titanic.ix[:,[2,3,6]]\n', "">> titanic.reindex(columns=['sex','age','fare'])\n""]"
1410;;0;34013098;34001922.0;2;33;;;;"['FailedPreconditionError', '""Variable_1""', 'tf.initialize_all_variables().run()\n', 'tf.InteractiveSession', 'tf.Session', 'init_op = tf.initialize_all_variables()\n\nsess = tf.Session()\nsess.run(init_op)\n']"
1411;;1;34082664;13413590.0;2;15;;;;['In [332]: df[df.EPS.notnull()]\nOut[332]:\n   STK_ID  RPT_Date  STK_ID.1  EPS  cash\n2  600016  20111231    600016  4.3   NaN\n4  601939  20111231    601939  2.5   NaN\n\n\nIn [334]: df[~df.EPS.isnull()]\nOut[334]:\n   STK_ID  RPT_Date  STK_ID.1  EPS  cash\n2  600016  20111231    600016  4.3   NaN\n4  601939  20111231    601939  2.5   NaN\n\n\nIn [347]: df[~np.isnan(df.EPS)]\nOut[347]:\n   STK_ID  RPT_Date  STK_ID.1  EPS  cash\n2  600016  20111231    600016  4.3   NaN\n4  601939  20111231    601939  2.5   NaN\n']
1412;;2;34092032;34091877.0;2;46;;;;"['names', 'read_csv', 'Cov = pd.read_csv(""path/to/file.txt"", sep=\'\\t\', \n                  names = [""Sequence"", ""Start"", ""End"", ""Coverage""])\n', 'Cov', 'Frame=pd.DataFrame([Cov], columns = [""Sequence"", ""Start"", ""End"", ""Coverage""])\n']"
1413;;0;34094058;34091877.0;2;25;;;;"['header=None', 'df.columns', 'Cov = pd.read_csv(""path/to/file.txt"", sep=\'\\t\', header=None)\nCov.columns = [""Sequence"", ""Start"", ""End"", ""Coverage""]\n']"
1414;;0;34097939;19482970.0;2;7;;;;['df.columns.values.tolist()', 'df.columns.tolist()', 'In [97]: %timeit df.columns.values.tolist()\n100000 loops, best of 3: 2.97 s per loop\n\nIn [98]: %timeit df.columns.tolist()\n10000 loops, best of 3: 9.67 s per loop\n']
1415;;1;34192820;20868394.0;2;18;;;;"['df.columns.values[2] = ""new_name""\n']"
1416;;1;34262133;14507794.0;2;35;;;;['pd.DataFrame(df.to_records()) # multiindex become columns and new index is integers only\n']
1417;;6;34277514;16176996.0;2;50;;;;"['0.15.0', '.dt', ""df['just_date'] = df['dates'].dt.date\n""]"
1418;;5;34282362;16740887.0;2;7;;;;"['d', 'CACHE = {}\nSTORE = \'store.h5\'   # Note: another option is to keep the actual file open\n\ndef process_row(d, key, max_len=5000, _cache=CACHE):\n    """"""\n    Append row d to the store \'key\'.\n\n    When the number of items in the key\'s cache reaches max_len,\n    append the list of rows to the HDF5 store and clear the list.\n\n    """"""\n    # keep the rows for each key separate.\n    lst = _cache.setdefault(key, [])\n    if len(lst) >= max_len:\n        store_and_clear(lst, key)\n    lst.append(d)\n\ndef store_and_clear(lst, key):\n    """"""\n    Convert key\'s cache list to a DataFrame and append that to HDF5.\n    """"""\n    df = pd.DataFrame(lst)\n    with pd.HDFStore(STORE) as store:\n        store.append(key, df)\n    lst.clear()\n', 'process_row({\'time\' :\'2013-01-01 00:00:00\', \'stock\' : \'BLAH\', \'high\' : 4.0, \'low\' : 3.0, \'open\' : 2.0, \'close\' : 1.0},\n            key=""df"")\n', 'store_and_clear', 'for k, lst in CACHE.items():  # you can instead use .iteritems() in python 2\n    store_and_clear(lst, k)\n', 'with pd.HDFStore(STORE) as store:\n    df = store[""df""]                    # other keys will be store[key]\n', 'len(df)', 'def get_latest(key, _cache=CACHE):\n    store_and_clear(_cache[key], key)\n    with pd.HDFStore(STORE) as store:\n        return store[key]\n', 'df = get_latest(""df"")\n']"
1419;;4;34297689;13035764.0;2;117;;;;"[""df3 = df3[~df3.index.duplicated(keep='first')]\n"", "">>> %timeit df3.reset_index().drop_duplicates(subset='index', keep='first').set_index('index')\n1000 loops, best of 3: 1.54 ms per loop\n\n>>> %timeit df3.groupby(df3.index).first()\n1000 loops, best of 3: 580 s per loop\n\n>>> %timeit df3[~df3.index.duplicated(keep='first')]\n1000 loops, best of 3: 307 s per loop\n"", 'MultiIndex', "">>> %timeit df1.groupby(level=df1.index.names).last()\n1000 loops, best of 3: 771 s per loop\n\n>>> %timeit df1[~df1.index.duplicated(keep='last')]\n1000 loops, best of 3: 365 s per loop\n""]"
1420;;3;34311080;7837722.0;2;43;;;;"[""t = pd.DataFrame({'a': range(0, 10000), 'b': range(10000, 20000)})\nB = []\nC = []\nA = time.time()\nfor i,r in t.iterrows():\n    C.append((r['a'], r['b']))\nB.append(time.time()-A)\n\nC = []\nA = time.time()\nfor ir in t.itertuples():\n    C.append((ir[1], ir[2]))    \nB.append(time.time()-A)\n\nC = []\nA = time.time()\nfor r in zip(t['a'], t['b']):\n    C.append((r[0], r[1]))\nB.append(time.time()-A)\n\nprint B\n"", '[0.5639059543609619, 0.017839908599853516, 0.005645036697387695]\n']"
1421;;1;34333886;28901683.0;2;6;;;;"[""df_1['key1'] = 1\ndf_2['key2'] = 1\ndf_1 = pd.merge(df_1, df_2, on=['field_x', 'field_y'], how = 'left')\ndf_1 = df_1[~(df_1.key2 == df_1.key1)]\ndf_1 = df_1.drop(['key1','key2'], axis=1)\n""]"
1422;;0;34530065;25039626.0;2;25;;;;['df._get_numeric_data()\n', 'In [32]: data\nOut[32]:\n   A  B\n0  1  s\n1  2  s\n2  3  s\n3  4  s\n\nIn [33]: data._get_numeric_data()\nOut[33]:\n   A\n0  1\n1  2\n2  3\n3  4\n']
1423;;0;34548894;9652832.0;2;22;;;;"['from_csv', ""pd.read_csv(fpath, sep='\\t')"", 'pd.read_table(fpath)']"
1424;;0;34551914;9758450.0;2;16;;;;['list(data_set.itertuples(index=False))\n']
1425;;0;34576537;13411544.0;2;31;;;;"[""df.drop(['col_name_1','col_name_2',...,'col_name_N'],inplace=True,axis=1,errors='ignore')\n""]"
1426;;0;34614046;14661701.0;2;54;;;;['df.drop(df.index[[1,3]], inplace=True)\n']
1427;;1;34687479;16628819.0;2;18;;;;"['tz_localize(None)', 'In [4]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=2, freq=\'H\',\n                          tz= ""Europe/Brussels"")\n\nIn [5]: t\nOut[5]: DatetimeIndex([\'2013-05-18 12:00:00+02:00\', \'2013-05-18 13:00:00+02:00\'],\n                       dtype=\'datetime64[ns, Europe/Brussels]\', freq=\'H\')\n', 'tz_localize(None)', ""In [6]: t.tz_localize(None)\nOut[6]: DatetimeIndex(['2013-05-18 12:00:00', '2013-05-18 13:00:00'], \n                      dtype='datetime64[ns]', freq='H')\n"", 'tz_convert(None)', ""In [7]: t.tz_convert(None)\nOut[7]: DatetimeIndex(['2013-05-18 10:00:00', '2013-05-18 11:00:00'], \n                      dtype='datetime64[ns]', freq='H')\n"", 'datetime.replace', 'In [31]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=10000, freq=\'H\',\n                           tz=""Europe/Brussels"")\n\nIn [32]: %timeit t.tz_localize(None)\n1000 loops, best of 3: 233 s per loop\n\nIn [33]: %timeit pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])\n10 loops, best of 3: 99.7 ms per loop\n']"
1428;;2;34756965;17874063.0;2;6;;;;['df.hist(bins=20, weights=np.ones_like(df[df.columns[0]]) * 100. / len(df))\n']
1429;;2;34790248;17618981.0;2;17;;;;"['sort', 'sort_values', ""df = df.sort_values(by=['c1','c2'], ascending=[False,True])\n""]"
1430;;0;34879805;29576430.0;2;140;;;;['.sample', 'df.sample(frac=1)\n', 'frac', 'frac=1', 'df = df.sample(frac=1).reset_index(drop=True)\n', 'drop=True', '.reset_index']
1431;;0;34962199;34962104.0;2;67;;;;"['df', 'a,b\n1,2\n2,3\n3,4\n4,5\n', ""df['a'] = df['a'].apply(lambda x: x + 1)\n"", '   a  b\n0  2  2\n1  3  3\n2  4  4\n3  5  5\n']"
1432;;0;34962592;34962104.0;2;9;;;;"["">>> df = pd.DataFrame({'a': [100, 1000], 'b': [200, 2000], 'c': [300, 3000]})\n>>> df\n\n      a     b     c\n0   100   200   300\n1  1000  2000  3000\n"", 'a', '>>> df.a = df.a / 2\n>>> df\n\n     a     b     c\n0   50   200   300\n1  500  2000  3000\n']"
1433;;0;34996876;19112398.0;2;34;;;;"[""table = [[1 , 2], [3, 4]]\ndf = DataFrame(table)\ndf = df.transpose()\ndf.columns = ['Heading1', 'Heading2']\n"", '      Heading1  Heading2\n0         1        3\n1         2        4\n']"
1434;;0;35068123;11346283.0;2;8;;;;['str.slice', 'df.columns = df.columns.str.slice(1)\n']
1435;;0;35203149;11285613.0;2;16;;;;"[""columns = ['b', 'c']\ndf1 = pd.DataFrame(df, columns=columns)\n""]"
1436;;2;35203658;22898824.0;2;21;;;;['import datetime \ndf.ix[datetime.date(year=2014,month=1,day=1):datetime.date(year=2014,month=2,day=1)]\n']
1437;;0;35212740;18039057.0;2;10;;;;"[""data = pd.read_csv('file1.csv', error_bad_lines=False)\n"", ""line     = []\nexpected = []\nsaw      = []     \ncont     = True \n\nwhile cont == True:     \n    try:\n        data = pd.read_csv('file1.csv',skiprows=line)\n        cont = False\n    except Exception as e:    \n        errortype = e.message.split('.')[0].strip()                                \n        if errortype == 'Error tokenizing data':                        \n           cerror      = e.message.split(':')[1].strip().replace(',','')\n           nums        = [n for n in cerror.split(' ') if str.isdigit(n)]\n           expected.append(int(nums[0]))\n           saw.append(int(nums[2]))\n           line.append(int(nums[1])-1)\n         else:\n           cerror      = 'Unknown'\n           print 'Unknown Error - 222'\n\nif line != []:\n    # Handle the errors however you want\n""]"
1438;;0;35219658;20230326.0;2;19;;;;['df.ix[:, df.columns != col]\n', 'col']
1439;;2;35240942;17950374.0;2;32;;;;['df.column_name = df.column_name.astype(np.int64)', 'df.column_name = df.column_name.astype(str)']
1440;;0;35245297;20763012.0;2;8;;;;"[""import pandas\nimport numpy\n\ndtype = [('Col1','int32'), ('Col2','float32'), ('Col3','float32')]\nvalues = numpy.zeros(20, dtype=dtype)\nindex = ['Row'+str(i) for i in range(1, len(values)+1)]\n\ndf = pandas.DataFrame(values, index=index)\n""]"
1441;;4;35246041;19611729.0;2;16;;;;"['StringIO', ""test = pd.read_csv('https://docs.google.com/spreadsheets/d/' + \n                   '0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc' +\n                   '/export?gid=0&format=csv',\n                   # Set first column as rownames in data frame\n                   index_col=0,\n                   # Parse column values to datetime\n                   parse_dates=['Quradate']\n                  )\ntest.head(5)  # Same result as @TomAugspurger\n"", '?gid=']"
1442;;2;35282530;17071871.0;2;24;;;;"['query()', ""df.query('col == val')"", ""In [167]: n = 10\n\nIn [168]: df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))\n\nIn [169]: df\nOut[169]: \n          a         b         c\n0  0.687704  0.582314  0.281645\n1  0.250846  0.610021  0.420121\n2  0.624328  0.401816  0.932146\n3  0.011763  0.022921  0.244186\n4  0.590198  0.325680  0.890392\n5  0.598892  0.296424  0.007312\n6  0.634625  0.803069  0.123872\n7  0.924168  0.325076  0.303746\n8  0.116822  0.364564  0.454607\n9  0.986142  0.751953  0.561512\n\n# pure python\nIn [170]: df[(df.a < df.b) & (df.b < df.c)]\nOut[170]: \n          a         b         c\n3  0.011763  0.022921  0.244186\n8  0.116822  0.364564  0.454607\n\n# query\nIn [171]: df.query('(a < b) & (b < c)')\nOut[171]: \n          a         b         c\n3  0.011763  0.022921  0.244186\n8  0.116822  0.364564  0.454607\n"", '@', ""exclude = ('red', 'orange')\ndf.query('color not in @exclude')\n""]"
1443;;0;35385805;13411544.0;2;19;;;;"[""df.drop([col for col in ['col_name_1','col_name_2',...,'col_name_N'] if col in df], \n        axis=1, inplace=True)\n""]"
1444;;0;35387028;11346283.0;2;10;;;;"[""df = pd.DataFrame({'$a': [1], '$b': [1], '$c': [1], '$d': [1], '$e': [1]}\n"", ""new_cols = ['a', 'b', 'c', 'd', 'e']\ndf.columns = new_cols\n>>> df\n   a  b  c  d  e\n0  1  1  1  1  1\n"", ""d = {'$a': 'a', '$b': 'b', '$c': 'c', '$d': 'd', '$e': 'e'}\ndf.columns.map(lambda col: d[col])\n>>> df\n   a  b  c  d  e\n0  1  1  1  1  1\n"", '$', ""df.columns = [col[1:] if col[0] == '$' else col for col in df]\n""]"
1445;;3;35387129;12555323.0;2;50;;;;"['e', 'df1', 'e', 'e', ""df['e'] = e.values\n"", 'assign', 'df1 = df1.assign(e=e.values)\n', 'assign', ""df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\n>>> df.assign(mean_a=df.a.mean(), mean_b=df.b.mean())\n   a  b  mean_a  mean_b\n0  1  3     1.5     3.5\n1  2  4     1.5     3.5\n"", ""np.random.seed(0)\ndf1 = pd.DataFrame(np.random.randn(10, 4), columns=['a', 'b', 'c', 'd'])\nmask = df1.applymap(lambda x: x <-0.7)\ndf1 = df1[-mask.any(axis=1)]\nsLength = len(df1['a'])\ne = pd.Series(np.random.randn(sLength))\n\n>>> df1\n          a         b         c         d\n0  1.764052  0.400157  0.978738  2.240893\n2 -0.103219  0.410599  0.144044  1.454274\n3  0.761038  0.121675  0.443863  0.333674\n7  1.532779  1.469359  0.154947  0.378163\n9  1.230291  1.202380 -0.387327 -0.302303\n\n>>> e\n0   -1.048553\n1   -1.420018\n2   -1.706270\n3    1.950775\n4   -0.509652\ndtype: float64\n\ndf1 = df1.assign(e=e.values)\n\n>>> df1\n          a         b         c         d         e\n0  1.764052  0.400157  0.978738  2.240893 -1.048553\n2 -0.103219  0.410599  0.144044  1.454274 -1.420018\n3  0.761038  0.121675  0.443863  0.333674 -1.706270\n7  1.532779  1.469359  0.154947  0.378163  1.950775\n9  1.230291  1.202380 -0.387327 -0.302303 -0.509652\n""]"
1446;;3;35446404;31357611.0;2;29;;;;"['FuncFormatter', ""import pandas as pd\nimport numpy as np\nfrom matplotlib.ticker import FuncFormatter\n\ndf = pd.DataFrame(np.random.randn(100,5))\n\nax = df.plot()\nax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y))) \n"", 'FuncFormatter']"
1447;;2;35476539;17709641.0;2;24;;;;['pip install --upgrade numpy\npip install --upgrade scipy\npip install --upgrade pandas\n', 'pip install --upgrade numpy scipy pandas\n']
1448;;0;35509134;26309962.0;2;17;;;;"["">>> import pandas as pd\n>>> list=[['a','b']]\n>>> list.append(['e','f'])\n>>> df=pd.DataFrame(list,columns=['col1','col2'])\n   col1 col2\n0    a    b\n1    e    f\n""]"
1449;;0;35523946;15943769.0;2;50;;;;['Count_Row=df.shape[0] #gives number of row count\nCount_Col=df.shape[1] #gives number of col count\n']
1450;;3;35531218;24147278.0;2;87;;;;['train=df.sample(frac=0.8,random_state=200)\ntest=df.drop(train.index)\n']
1451;;2;35583219;10715965.0;2;17;;;;"['ignore_index', "">>> f = pandas.DataFrame(data = {'Animal':['cow','horse'], 'Color':['blue', 'red']})\n>>> f\n  Animal Color\n0    cow  blue\n1  horse   red\n>>> f.append({'Animal':'mouse', 'Color':'black'}, ignore_index=True)\n  Animal  Color\n0    cow   blue\n1  horse    red\n2  mouse  black\n""]"
1452;;0;35616082;18062135.0;2;9;;;;['a.to_frame().join(b.to_frame())\n']
1453;;0;35768306;13636592.0;2;18;;;;"['sort', 'sort_values', ""df.sort_values(['Peak', 'Weeks'], ascending=[True, False], inplace=True)\n""]"
1454;;0;35783766;17477979.0;2;8;;;;"['.loc', 's.loc[(~np.isfinite(s)) & s.notnull()] = np.nan\n', ""df = pd.DataFrame(np.ones((3, 3)), columns=list('ABC'))\n\nfor i in range(3): \n    df.iat[i, i] = np.inf\n\ndf\n          A         B         C\n0       inf  1.000000  1.000000\n1  1.000000       inf  1.000000\n2  1.000000  1.000000       inf\n\ndf.sum()\nA    inf\nB    inf\nC    inf\ndtype: float64\n\ndf.apply(lambda s: s[np.isfinite(s)].dropna()).sum()\nA    2\nB    2\nC    2\ndtype: float64\n""]"
1455;;0;35784666;15772009.0;2;57;;;;['df.sample(frac=1)\n']
1456;;2;35850749;19377969.0;2;47;;;;"['cat()', '.str', '>>> import pandas as pd\n>>> df = pd.DataFrame([[""2014"", ""q1""], \n...                    [""2015"", ""q3""]],\n...                   columns=(\'Year\', \'Quarter\'))\n>>> print(df)\n   Year Quarter\n0  2014      q1\n1  2015      q3\n>>> df[\'Period\'] = df.Year.str.cat(df.Quarter)\n>>> print(df)\n   Year Quarter  Period\n0  2014      q1  2014q1\n1  2015      q3  2015q3\n', 'cat()', "">>> import pandas as pd\n>>> df = pd.DataFrame([[2014, 1],\n...                    [2015, 3]],\n...                   columns=('Year', 'Quarter'))\n>>> print(df)\n   Year Quarter\n0  2014       1\n1  2015       3\n>>> df['Period'] = df.Year.astype(str).str.cat(df.Quarter.astype(str), sep='q')\n>>> print(df)\n   Year Quarter  Period\n0  2014       1  2014q1\n1  2015       3  2015q3\n""]"
1457;;0;35902487;19213789.0;2;11;;;;"[""ax.axvline(pd.to_datetime('2015-11-01'), color='r', linestyle='--', lw=2)\n"", ""xposition = [pd.to_datetime('2010-01-01'), pd.to_datetime('2015-12-31')]\nfor xc in xposition:\n    ax.axvline(x=xc, color='k', linestyle='-')\n""]"
1458;;3;35970794;17839973.0;2;9;;;;"['pd.DataFrame.from_records', ""df = pd.DataFrame.from_records([{ 'A':a,'B':b }])\n"", ""df = pd.DataFrame.from_records([{ 'A':a,'B':b }], index='A')\n""]"
1459;;0;36012306;20612645.0;2;7;;;;"['import pandas as pd\npd.__version__\n', ""'0.14.1'\n""]"
1460;;0;36013757;18022845.0;2;21;;;;"['0.18.0', 'rename_axis', 'print df\n             Column 1\nIndex Title          \nApples            1.0\nOranges           2.0\nPuppies           3.0\nDucks             4.0\n', ""print df.rename_axis('foo')\n         Column 1\nfoo              \nApples        1.0\nOranges       2.0\nPuppies       3.0\nDucks         4.0\n"", 'axis', 'print df\nCol Name     Column 1\nIndex Title          \nApples            1.0\nOranges           2.0\nPuppies           3.0\nDucks             4.0\n', 'print df.rename_axis(\'foo\').rename_axis(""bar"", axis=""columns"")\nbar      Column 1\nfoo              \nApples        1.0\nOranges       2.0\nPuppies       3.0\nDucks         4.0\n\nprint df.rename_axis(\'foo\').rename_axis(""bar"", axis=1)\nbar      Column 1\nfoo              \nApples        1.0\nOranges       2.0\nPuppies       3.0\nDucks         4.0\n']"
1461;;0;36041831;19377969.0;2;12;;;;"[""import pandas as pd\ndf = pd.DataFrame({'Year': ['2014', '2015'], 'Quarter': ['q1', 'q2']})\nprint df\ndf['YearQuarter'] = df[['Year','Quarter']].apply(lambda x : '{}{}'.format(x[0],x[1]), axis=1)\nprint df\n\n  Quarter  Year\n0      q1  2014\n1      q2  2015\n  Quarter  Year YearQuarter\n0      q1  2014      2014q1\n1      q2  2015      2015q2\n"", ""import pandas as pd\ndf = pd.DataFrame({'Year': ['2014', '2015'], 'Quarter': [1, 2]})\nprint df.dtypes\nprint df\n\ndf['YearQuarter'] = df[['Year','Quarter']].apply(lambda x : '{}q{}'.format(x[0],x[1]), axis=1)\nprint df\n\nQuarter     int64\nYear       object\ndtype: object\n   Quarter  Year\n0        1  2014\n1        2  2015\n   Quarter  Year YearQuarter\n0        1  2014      2014q1\n1        2  2015      2015q2\n""]"
1462;;1;36059898;21733893.0;2;8;;;;"[""df['Age_group'] = np.where(df.Age<18, 'under 18',\n                           np.where(df.Age<40,'under 40', '>40'))\n""]"
1463;;2;36073837;20067636.0;2;13;;;;"[""df.groupby('id').nth(1) \n""]"
1464;;2;36074520;27842613.0;2;24;;;;"[""In[34]: df.sort_values(['job','count'],ascending=False).groupby('job').head(3)\n\nOut[35]: \n   count     job source\n4      7   sales      E\n2      6   sales      C\n1      4   sales      B\n5      5  market      A\n8      4  market      D\n6      3  market      B\n""]"
1465;;0;36082588;21606987.0;2;13;;;;"['.str.strip', ""In [5]:\ndf = pd.DataFrame(columns=['Year', 'Month ', 'Value'])\nprint(df.columns.tolist())\ndf.columns = df.columns.str.strip()\ndf.columns.tolist()\n\n['Year', 'Month ', 'Value']\nOut[5]:\n['Year', 'Month', 'Value']\n""]"
1466;;0;36149967;11346283.0;2;41;;;;"[""df.columns = ['a', 'b', 'c', 'd', 'e']\n"", ""df.columns.values[2] = 'c'    #renames the 2nd column to 'c'\n""]"
1467;;0;36184396;21197774.0;2;35;;;;"['convert_objects', 'FutureWarning: convert_objects is deprecated.  Use the data-type specific converters \npd.to_datetime, pd.to_timedelta and pd.to_numeric.\n', 'df =', 'df.astype(np.float)', 'df[""A""] =', 'pd.to_numeric(df[""A""])']"
1468;;2;36188131;14262433.0;2;32;;;;[]
1469;;0;36236885;23307301.0;2;12;;;;['w.female.replace(to_replace=dict(female=1, male=0), inplace=True)\n']
1470;;0;36257640;29525808.0;2;8;;;;['pd.read_sql(session.query(Complaint).filter(Complaint.id == 2).statement,session.bind) \n']
1471;;4;36319915;31357611.0;2;12;;;;"['PercentFormatter', 'matplotlib.ticker', ""import ...\nimport matplotlib.ticker as mtick\n\nax = df['myvar'].plot(kind='bar')\nax.yaxis.set_major_formatter(mtick.PercentFormatter())\n"", 'PercentFormatter()', 'max', 'decimals', 'symbol', 'max', 'PercentFormatter(1.0)', 'None', ""'%'"", 'decimals=None']"
1472;;0;36372667;28218698.0;2;18;;;;"['iteritems()', ""for name, values in df.iteritems():\n    print '{name}: {value}'.format(name=name, value=values[0])\n""]"
1473;;0;36373866;11707586.0;2;6;;;;"[""pd.set_option('display.large_repr', 'truncate')\npd.set_option('display.max_columns', 0)\n""]"
1474;;8;36416258;20906474.0;2;97;;;;"['path = r\'C:\\DRO\\DCL_rawdata_files\'                     # use your path\nall_files = glob.glob(os.path.join(path, ""*.csv""))     # advisable to use os.path.join as this makes concatenation OS independent\n\ndf_from_each_file = (pd.read_csv(f) for f in all_files)\nconcatenated_df   = pd.concat(df_from_each_file, ignore_index=True)\n# doesn\'t create a list, nor does it append to one\n']"
1475;;3;36434248;22391433.0;2;27;;;;['df.apply(pd.value_counts)\n']
1476;;0;36475297;24645153.0;2;24;;;;"['pandas', 'apply', "">>> import pandas as pd\n>>> from sklearn.preprocessing import MinMaxScaler\n\n\n>>> scaler = MinMaxScaler()\n\n>>> dfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],\n                           'B':[103.02,107.26,110.35,114.23,114.68],\n                           'C':['big','small','big','small','small']})\n\n>>> dfTest[['A', 'B']] = scaler.fit_transform(dfTest[['A', 'B']])\n\n>>> dfTest\n          A         B      C\n0  0.000000  0.000000    big\n1  0.926219  0.363636  small\n2  0.935335  0.628645    big\n3  1.000000  0.961407  small\n4  0.938495  1.000000  small\n""]"
1477;;0;36513262;14984119.0;2;9;;;;"['Cols = list(df.columns)\nfor i,item in enumerate(df.columns):\n    if item in df.columns[:i]: Cols[i] = ""toDROP""\ndf.columns = Cols\ndf = df.drop(""toDROP"",1)\n']"
1478;;1;36519122;36519086.0;2;34;;;;"['index=False', ""In [37]:\ndf = pd.DataFrame(np.random.randn(5,3), columns=list('abc'))\npd.read_csv(io.StringIO(df.to_csv()))\n\nOut[37]:\n   Unnamed: 0         a         b         c\n0           0  0.109066 -1.112704 -0.545209\n1           1  0.447114  1.525341  0.317252\n2           2  0.507495  0.137863  0.886283\n3           3  1.452867  1.888363  1.168101\n4           4  0.901371 -0.704805  0.088335\n"", 'In [38]:\npd.read_csv(io.StringIO(df.to_csv(index=False)))\n\nOut[38]:\n          a         b         c\n0  0.109066 -1.112704 -0.545209\n1  0.447114  1.525341  0.317252\n2  0.507495  0.137863  0.886283\n3  1.452867  1.888363  1.168101\n4  0.901371 -0.704805  0.088335\n', 'read_csv', 'index_col=0', 'In [40]:\npd.read_csv(io.StringIO(df.to_csv()), index_col=0)\n\nOut[40]:\n          a         b         c\n0  0.109066 -1.112704 -0.545209\n1  0.447114  1.525341  0.317252\n2  0.507495  0.137863  0.886283\n3  1.452867  1.888363  1.168101\n4  0.901371 -0.704805  0.088335\n']"
1479;;1;36572039;20107570.0;2;53;;;;[' df.to_csv(filename ,  index = False)\n', 'df.read_csv(filename ,  index = False)  \n']
1480;;1;36590692;17063458.0;2;34;;;;"[""import pandas\ndf = pandas.read_excel(open('your_xls_xlsx_filename','rb'), sheetname='Sheet 1')\n# or using sheet index starting 0\ndf = pandas.read_excel(open('your_xls_xlsx_filename','rb'), sheetname=2)\n""]"
1481;;0;36682678;22233488.0;2;15;;;;"['df', 'df', "">>> df\n\n    a\n    b   c\n0   1   2\n1   3   4\n\n>>> df = df.xs('a', axis=1, drop_level=True)\n\n    # 'a' : key on which to get cross section\n    # axis=1 : get cross section of column\n    # drop_level=True : returns cross section without the multilevel index\n\n>>> df\n\n    b   c\n0   1   2\n1   3   4\n""]"
1482;;0;36710126;26266362.0;2;10;;;;[' %%timeit\n df.isnull().any().any()\n', ' %timeit \n df.isnull().values.sum()\n', ' df.isnull().any()\n']
1483;;7;36911306;19377969.0;2;49;;;;"[""df['period'] = df['Year'].astype(str) + df['quarter']\n"", ""df['period'] = df[['Year','quarter']].astype(str).sum(axis=1)\n"", 'In [250]: df\nOut[250]:\n   Year quarter\n0  2014      q1\n1  2015      q2\n\nIn [251]: df = pd.concat([df] * 10**5)\n\nIn [252]: df.shape\nOut[252]: (200000, 2)\n', ""In [107]: %timeit df['Year'].astype(str) + df['quarter']\n10 loops, best of 3: 131 ms per loop\n\nIn [106]: %timeit df['Year'].map(str) + df['quarter']\n10 loops, best of 3: 161 ms per loop\n\nIn [108]: %timeit df.Year.str.cat(df.quarter)\n10 loops, best of 3: 189 ms per loop\n\nIn [109]: %timeit df.loc[:, ['Year','quarter']].astype(str).sum(axis=1)\n1 loop, best of 3: 567 ms per loop\n\nIn [110]: %timeit df[['Year','quarter']].astype(str).sum(axis=1)\n1 loop, best of 3: 584 ms per loop\n\nIn [111]: %timeit df[['Year','quarter']].apply(lambda x : '{}{}'.format(x[0],x[1]), axis=1)\n1 loop, best of 3: 24.7 s per loop\n"", ""In [113]: %timeit df['Year'].astype(str) + df['quarter']\n10 loops, best of 3: 53.3 ms per loop\n\nIn [114]: %timeit df['Year'].map(str) + df['quarter']\n10 loops, best of 3: 65.5 ms per loop\n\nIn [115]: %timeit df.Year.str.cat(df.quarter)\n10 loops, best of 3: 79.9 ms per loop\n\nIn [116]: %timeit df.loc[:, ['Year','quarter']].astype(str).sum(axis=1)\n1 loop, best of 3: 230 ms per loop\n\nIn [117]: %timeit df[['Year','quarter']].astype(str).sum(axis=1)\n1 loop, best of 3: 230 ms per loop\n\nIn [118]: %timeit df[['Year','quarter']].apply(lambda x : '{}{}'.format(x[0],x[1]), axis=1)\n1 loop, best of 3: 9.38 s per loop\n""]"
1484;;0;36922103;36921951.0;2;48;;;;"['or', 'and', 'truth', 'pandas', '|', '&', ""result = result[(result['var']>0.25) | (result['var']<-0.25)]\n"", 'or', 'and', 'bool', 'pandas.Series', '>>> import pandas as pd\n>>> x = pd.Series([1])\n>>> bool(x)\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n', 'bool', 'or', 'and', 'if', 'while', "">>> x or x\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n>>> x and x\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n>>> if x:\n...     print('fun')\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n>>> while x:\n...     print('fun')\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n"", 'bool', 'any', 'all', 'filter', 'pandas.Series', 'and', 'or', 'numpy.logical_or', '>>> import numpy as np\n>>> np.logical_or(x, y)\n', '|', '>>> x | y\n', 'numpy.logical_and', '>>> np.logical_and(x, y)\n', '&', '>>> x & y\n', 'pandas.Series', 'if', 'while', '>>> x = pd.Series([])\n>>> x.empty\nTrue\n>>> x = pd.Series([1])\n>>> x.empty\nFalse\n', 'len', 'list', 'tuple', 'if x.size', 'if not x.empty', 'if x', 'Series', '>>> x = pd.Series([100])\n>>> (x > 50).bool()\nTrue\n>>> (x < 50).bool()\nFalse\n', '.bool()', '>>> x = pd.Series([100])\n>>> x.item()\n100\n', '>>> x = pd.Series([0, 1, 2])\n>>> x.all()   # because one element is zero\nFalse\n>>> x.any()   # because one (or more) elements are non-zero\nTrue\n']"
1485;;0;36922486;36921951.0;2;7;;;;"['&', '|', ""np.random.seed(0)\ndf = pd.DataFrame(np.random.randn(5,3), columns=list('ABC'))\n\n>>> df\n          A         B         C\n0  1.764052  0.400157  0.978738\n1  2.240893  1.867558 -0.977278\n2  0.950088 -0.151357 -0.103219\n3  0.410599  0.144044  1.454274\n4  0.761038  0.121675  0.443863\n\n>>> df.loc[(df.C > 0.25) | (df.C < -0.25)]\n          A         B         C\n0  1.764052  0.400157  0.978738\n1  2.240893  1.867558 -0.977278\n3  0.410599  0.144044  1.454274\n4  0.761038  0.121675  0.443863\n"", 'df.C > 0.25\n0     True\n1    False\n2    False\n3     True\n4     True\nName: C, dtype: bool\n', 'and', 'or', '# Any value in either column is True?\n(df.C > 0.25).any() or (df.C < -0.25).any()\nTrue\n\n# All values in either column is True?\n(df.C > 0.25).all() or (df.C < -0.25).all()\nFalse\n', '>>> df[[any([a, b]) for a, b in zip(df.C > 0.25, df.C < -0.25)]]\n          A         B         C\n0  1.764052  0.400157  0.978738\n1  2.240893  1.867558 -0.977278\n3  0.410599  0.144044  1.454274\n4  0.761038  0.121675  0.443863\n']"
1486;;2;36951842;22691010.0;2;20;;;;"['grouped_df = df.groupby(\'A\')\n\nfor key, item in grouped_df:\n    print grouped_df.get_group(key), ""\\n\\n""\n', 'grouped_df = df.groupby(\'A\')    \ngb = grouped_df.groups\n\nfor key, values in gb.iteritems():\n    print df.ix[values], ""\\n\\n""\n', 'gb = grouped_df.groups\ngb.keys()\n\nkey_list_from_gb = [key1, key2, key3]\n\nfor key, values in gb.iteritems():\n    if key in key_list_from_gb:\n        print df.ix[values], ""\\n""\n']"
1487;;0;36955053;11285613.0;2;24;;;;"['.loc', ""df.loc[:, 'C':'E']\n"", 'C', 'E', ""import pandas as pd\nimport numpy as np\nnp.random.seed(5)\ndf = pd.DataFrame(np.random.randint(100, size=(100, 6)), \n                  columns=list('ABCDEF'), \n                  index=['R{}'.format(i) for i in range(100)])\ndf.head()\n\nOut: \n     A   B   C   D   E   F\nR0  99  78  61  16  73   8\nR1  62  27  30  80   7  76\nR2  15  53  80  27  44  77\nR3  75  65  47  30  84  86\nR4  18   9  41  62   1  82\n"", ""df.loc[:, 'C':'E']\n\nOut: \n      C   D   E\nR0   61  16  73\nR1   30  80   7\nR2   80  27  44\nR3   47  30  84\nR4   41  62   1\nR5    5  58   0\n...\n"", ""df.loc['R6':'R10', 'C':'E']\n\nOut: \n      C   D   E\nR6   51  27  31\nR7   83  19  18\nR8   11  67  65\nR9   78  27  29\nR10   7  16  94\n"", '.loc', 'True', ""df.columns.isin(list('BCD'))"", 'array([False,  True,  True,  True, False, False], dtype=bool)', ""['B', 'C', 'D']"", ""df.loc[:, df.columns.isin(list('BCD'))]\n\nOut: \n      B   C   D\nR0   78  61  16\nR1   27  30  80\nR2   53  80  27\nR3   65  47  30\nR4    9  41  62\nR5   78   5  58\n...\n""]"
1488;;0;36957431;23307301.0;2;6;;;;"[""w.female.replace(['male', 'female'], [1, 0], inplace=True)\n""]"
1489;;1;36958937;13411544.0;2;33;;;;"[""df.drop(['column_name'], axis = 1, inplace = True, errors = 'ignore')\n""]"
1490;;1;37000877;13411544.0;2;44;;;;"['del df.column_name', ""del df['column']"", ""df.__delitem__('column')"", ""del df['column_name']"", 'del df.column_name', 'del df.column_name', '__delattr__', ""del df['column_name']"", 'del df.dtypes', '__delattr__', '.ix', '.loc', '.iloc', 'del df.column_name', 'del df.dtypes']"
1491;;2;37043071;13187778.0;2;65;;;;['df=df.values']
1492;;2;37097791;14661701.0;2;21;;;;"[""In[17]: df\nOut[17]: \n            a         b         c         d         e\none  0.456558 -2.536432  0.216279 -1.305855 -0.121635\ntwo -1.015127 -0.445133  1.867681  2.179392  0.518801\n\nIn[18]: df.drop('one')\nOut[18]: \n            a         b         c         d         e\ntwo -1.015127 -0.445133  1.867681  2.179392  0.518801\n"", 'In[19]: df.drop(df.index[[0]])\nOut[19]: \n            a         b         c         d         e\ntwo -1.015127 -0.445133  1.867681  2.179392  0.518801\n']"
1493;;3;37199623;12525722.0;2;34;;;;"['sklearn', ""import pandas as pd\nfrom sklearn import preprocessing\n\ndata = {'score': [234,24,14,27,-74,46,73,-18,59,160]}\ndf = pd.DataFrame(data)\ndf\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(df)\ndf_normalized = pd.DataFrame(np_scaled)\ndf_normalized\n""]"
1494;;2;37347783;19124601.0;2;14;;;;"[""pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n"", ""pd.describe_option('display')\n""]"
1495;;0;37384347;16096627.0;2;6;;;;['np_df = df.as_matrix()\n', 'np_df[i] \n']
1496;;4;37441204;14940743.0;2;45;;;;"['difference', ""df2 = df[df.columns.difference(['B', 'D'])]\n"", 'B', 'D', 'df']"
1497;;0;37442692;29530232.0;2;11;;;;['df.isnull().T.any().T.sum()\n', 'nan_rows = df[df.isnull().T.any().T]\n']
1498;;4;37447530;10636024.0;2;17;;;;['_ > 0', '(_ >= date(2016, 1, 1)) & (_ <= date(2016, 1, 31))']
1499;;2;37453925;21269399.0;2;60;;;;"['pandas.read_csv()', 'parse_dates', 'date_parser', 'dateutil.parser.parser', ""headers = ['col1', 'col2', 'col3', 'col4']\ndtypes = {'col1': 'str', 'col2': 'str', 'col3': 'str', 'col4': 'float'}\nparse_dates = ['col1', 'col2']\npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes, parse_dates=parse_dates)\n"", 'col1', 'col2', 'pandas.read_csv()', 'date_parser', 'date_parser = pd.datetools.to_datetime\n', 'date_parser = pd.datetools.to_datetime()\n']"
1500;;2;37592047;17116814.0;2;8;;;;"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'ItemQty': {0: 3, 1: 25}, \n                   'Seatblocks': {0: '2:218:10:4,6', 1: '1:13:36:1,12 1:13:37:1,13'}, \n                   'ItemExt': {0: 60, 1: 300}, \n                   'CustomerName': {0: 'McCartney, Paul', 1: 'Lennon, John'}, \n                   'CustNum': {0: 32363, 1: 31316}, \n                   'Item': {0: 'F04', 1: 'F01'}}, \n                    columns=['CustNum','CustomerName','ItemQty','Item','Seatblocks','ItemExt'])\n\nprint (df)\n   CustNum     CustomerName  ItemQty Item                 Seatblocks  ItemExt\n0    32363  McCartney, Paul        3  F04               2:218:10:4,6       60\n1    31316     Lennon, John       25  F01  1:13:36:1,12 1:13:37:1,13      300\n"", 'reset_index', 'rename', ""print (df.drop('Seatblocks', axis=1)\n             .join\n             (\n             df.Seatblocks\n             .str\n             .split(expand=True)\n             .stack()\n             .reset_index(drop=True, level=1)\n             .rename('Seatblocks')           \n             ))\n\n   CustNum     CustomerName  ItemQty Item  ItemExt    Seatblocks\n0    32363  McCartney, Paul        3  F04       60  2:218:10:4,6\n1    31316     Lennon, John       25  F01      300  1:13:36:1,12\n1    31316     Lennon, John       25  F01      300  1:13:37:1,13\n"", 'NaN', 'list', 'DataFrame', ""df = pd.DataFrame(['a b c']*100000, columns=['col'])\n\nIn [141]: %timeit (pd.DataFrame(dict(zip(range(3), [df['col'].apply(lambda x : x.split(' ')[i]) for i in range(3)]))))\n1 loop, best of 3: 211 ms per loop\n\nIn [142]: %timeit (pd.DataFrame(df.col.str.split().tolist()))\n10 loops, best of 3: 87.8 ms per loop\n\nIn [143]: %timeit (pd.DataFrame(list(df.col.str.split())))\n10 loops, best of 3: 86.1 ms per loop\n\nIn [144]: %timeit (df.col.str.split(expand=True))\n10 loops, best of 3: 156 ms per loop\n\nIn [145]: %timeit (pd.DataFrame([ x.split() for x in df['col'].tolist()]))\n10 loops, best of 3: 54.1 ms per loop\n"", 'NaN', 'str.split', 'expand=True', 'DataFrame', ""df = pd.DataFrame(['a b c']*10, columns=['col'])\ndf.loc[0] = np.nan\nprint (df.head())\n     col\n0    NaN\n1  a b c\n2  a b c\n3  a b c\n4  a b c\n\nprint (df.col.str.split(expand=True))\n     0     1     2\n0  NaN  None  None\n1    a     b     c\n2    a     b     c\n3    a     b     c\n4    a     b     c\n5    a     b     c\n6    a     b     c\n7    a     b     c\n8    a     b     c\n9    a     b     c\n""]"
1501;;0;37647160;17383094.0;2;11;;;;['[1]: data = pd.DataFrame([[True, False, True], [False, False, True]])\n[2]: print data\n          0      1     2\n     0   True  False  True\n     1   False False  True\n\n[3]: print data*1\n         0  1  2\n     0   1  0  1\n     1   0  0  1\n']
1502;;0;37655063;27236275.0;2;36;;;;['df[df.index.duplicated()]']
1503;;1;37717675;29763620.0;2;39;;;;"['ix', 'df.drop()', "">>> df\n\n          a         b         c         d\n0  0.175127  0.191051  0.382122  0.869242\n1  0.414376  0.300502  0.554819  0.497524\n2  0.142878  0.406830  0.314240  0.093132\n3  0.337368  0.851783  0.933441  0.949598\n\n>>> df.drop('b', axis=1)\n\n          a         c         d\n0  0.175127  0.382122  0.869242\n1  0.414376  0.554819  0.497524\n2  0.142878  0.314240  0.093132\n3  0.337368  0.933441  0.949598\n"", '.drop()', 'df', 'b', 'df', ""df.drop('b', inplace=True)"", 'df.drop()', ""df.drop(['a', 'b'], axis=1)"", 'a', 'b']"
1504;;1;37793940;20158597.0;2;8;;;;"['a = pd.Series(range(100) + ([0]*20))\n\ndef jitter(a_series, noise_reduction=1000000):\n    return (np.random.random(len(a_series))*a_series.std()/noise_reduction)-(a_series.std()/(2*noise_reduction))\n\n# and now this works by adding a little noise\na_deciles = pd.qcut(a + jitter(a), 10, labels=False)\n', 'a_deciles = pd.qcut(a, 10, labels=False)\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/usr/local/lib/python2.7/site-packages/pandas/tools/tile.py"", line 173, in qcut\n    precision=precision, include_lowest=True)\n  File ""/usr/local/lib/python2.7/site-packages/pandas/tools/tile.py"", line 192, in _bins_to_cuts\n    raise ValueError(\'Bin edges must be unique: %s\' % repr(bins))\nValueError: Bin edges must be unique: array([  0.        ,   0.        ,   0.        ,   3.8       ,\n        11.73333333,  19.66666667,  27.6       ,  35.53333333,\n        43.46666667,  51.4       ,  59.33333333,  67.26666667,\n        75.2       ,  83.13333333,  91.06666667,  99.        ])\n']"
1505;;0;37806819;12433076.0;2;10;;;;"['In [12]: from pandas_datareader import data\n', 'AAPL', '1980-01-01', ""#In [13]: aapl = data.DataReader('AAPL', 'yahoo', '1980-01-01')\n\n# yahoo api is inconsistent for getting historical data, please use google instead.\nIn [13]: aapl = data.DataReader('AAPL', 'google', '1980-01-01')\n"", 'In [14]: aapl.head()\nOut[14]:\n                 Open       High     Low   Close     Volume  Adj Close\nDate\n1980-12-12  28.750000  28.875000  28.750  28.750  117258400   0.431358\n1980-12-15  27.375001  27.375001  27.250  27.250   43971200   0.408852\n1980-12-16  25.375000  25.375000  25.250  25.250   26432000   0.378845\n1980-12-17  25.875000  25.999999  25.875  25.875   21610400   0.388222\n1980-12-18  26.625000  26.750000  26.625  26.625   18362400   0.399475\n', 'In [15]: aapl.tail()\nOut[15]:\n                 Open       High        Low      Close    Volume  Adj Close\nDate\n2016-06-07  99.250000  99.870003  98.959999  99.029999  22366400  99.029999\n2016-06-08  99.019997  99.559998  98.680000  98.940002  20812700  98.940002\n2016-06-09  98.500000  99.989998  98.459999  99.650002  26419600  99.650002\n2016-06-10  98.529999  99.349998  98.480003  98.830002  31462100  98.830002\n2016-06-13  98.690002  99.120003  97.099998  97.339996  37612900  97.339996\n', ""In [16]: aapl.to_csv('d:/temp/aapl_data.csv')\n"", 'Date,Open,High,Low,Close,Volume,Adj Close\n1980-12-12,28.75,28.875,28.75,28.75,117258400,0.431358\n1980-12-15,27.375001,27.375001,27.25,27.25,43971200,0.408852\n1980-12-16,25.375,25.375,25.25,25.25,26432000,0.378845\n1980-12-17,25.875,25.999999,25.875,25.875,21610400,0.38822199999999996\n1980-12-18,26.625,26.75,26.625,26.625,18362400,0.399475\n...\n']"
1506;;0;37891437;22676081.0;2;56;;;;"['pandas.merge()', 'pandas.DataFrame.merge()', 'pandas.DataFrame.join()', 'pandas.merge()', 'df1.merge(right=df2, ...)', 'pandas.merge(left=df1, right=df2, ...)', 'df.join()', 'df.merge()', 'df1.join(df2)', 'df2', 'df1.merge(df2)', 'df2', 'df2', 'right_index=True', 'df1.join(df2)', 'df1', 'df1.merge(df2)', 'df1', 'df1.join(df2, on=key_or_keys)', 'df1.merge(df2, right_index=True)', 'df1.join(df2)', 'df1', 'df.merge', 'df1', 'df2', 'pandas.merge(df1, df2)', 'df1.merge(df2)', 'df1', 'df2', 'df1.join(df2)', 'merge', 'DataFrame.join', 'merge', 'merge', 'DataFrame.join', ""left.join(right, on=key_or_keys)\npd.merge(left, right, left_on=key_or_keys, right_index=True, how='left', sort=False)\n""]"
1507;;0;37992805;13331518.0;2;15;;;;"['In [1]: import pandas as pd\nIn [2]: import numpy as np\n\nIn [3]: s = pd.Series(np.arange(4)**2, index=np.arange(4))\n\nIn [4]: s\nOut[4]:\n0    0\n1    1\n2    4\n3    9\ndtype: int64\n\nIn [6]: id(s.index), id(s.values)\nOut[6]: (4470549648, 4470593296)\n', 'In [7]: s[2] = 14  \n\nIn [8]: id(s.index), id(s.values)\nOut[8]: (4470549648, 4470593296)\n', 'In [9]: s[4] = 16\n\nIn [10]: s\nOut[10]:\n0     0\n1     1\n2    14\n3     9\n4    16\ndtype: int64\n\nIn [11]: id(s.index), id(s.values)\nOut[11]: (4470548560, 4470595056)\n', 'In [13]: new_items = {item: item**2 for item in range(5, 7)}\n\nIn [14]: s2 = pd.Series(new_items)\n\nIn [15]: s2  # keys are guaranteed to be sorted!\nOut[15]:\n5    25\n6    36\ndtype: int64\n\nIn [16]: s = s.append(s2); s\nOut[16]:\n0     0\n1     1\n2    14\n3     9\n4    16\n5    25\n6    36\ndtype: int64\n']"
1508;;0;38025280;15943769.0;2;7;;;;['df.index\n', 'df.columns\n', 'len(func)', 'len(df.index)', 'shape[0] and shape[1]']
1509;;0;38034085;19125091.0;2;15;;;;"['suffixes', '.merge()', ""dfNew = df.merge(df2, left_index=True, right_index=True,\n                 how='outer', suffixes=('', '_y'))\n""]"
1510;;0;38156594;21197774.0;2;6;;;;"[""import pandas as pd\nimport numpy as np    \n\nx = np.empty((10,), dtype=[('x', np.uint8), ('y', np.float64)])\ndf = pd.DataFrame(x)\n\ndf.dtypes ->\n\nx      uint8\ny    float64\n""]"
1511;;1;38251063;38250710.0;2;18;;;;"['import numpy as np\nimport pandas as pd\n\ndef train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n    np.random.seed(seed)\n    perm = np.random.permutation(df.index)\n    m = len(df)\n    train_end = int(train_percent * m)\n    validate_end = int(validate_percent * m) + train_end\n    train = df.ix[perm[:train_end]]\n    validate = df.ix[perm[train_end:validate_end]]\n    test = df.ix[perm[validate_end:]]\n    return train, validate, test\n', ""np.random.seed([3,1415])\ndf = pd.DataFrame(np.random.rand(10, 5), columns=list('ABCDE'))\ndf\n"", 'train, validate, test = train_validate_test_split(df)\n\ntrain\n', 'validate\n', 'test\n']"
1512;;9;38251213;38250710.0;2;35;;;;['In [305]: train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n\nIn [306]: train\nOut[306]:\n          A         B         C         D         E\n0  0.046919  0.792216  0.206294  0.440346  0.038960\n2  0.301010  0.625697  0.604724  0.936968  0.870064\n1  0.642237  0.690403  0.813658  0.525379  0.396053\n9  0.488484  0.389640  0.599637  0.122919  0.106505\n8  0.842717  0.793315  0.554084  0.100361  0.367465\n7  0.185214  0.603661  0.217677  0.281780  0.938540\n\nIn [307]: validate\nOut[307]:\n          A         B         C         D         E\n5  0.806176  0.008896  0.362878  0.058903  0.026328\n6  0.145777  0.485765  0.589272  0.806329  0.703479\n\nIn [308]: test\nOut[308]:\n          A         B         C         D         E\n4  0.521640  0.332210  0.370177  0.859169  0.401087\n3  0.333348  0.964011  0.083498  0.670386  0.169619\n', 'indices_or_sections', 'np.split()', 'In [45]: a = np.arange(1, 21)\n\nIn [46]: a\nOut[46]: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n\nIn [47]: np.split(a, [int(.8 * len(a)), int(.9 * len(a))])\nOut[47]:\n[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16]),\n array([17, 18]),\n array([19, 20])]\n']
1513;;1;38278787;16729483.0;2;19;;;;"[""import pandas as pd\ns = pd.Series(['1.0', '2', -3])\npd.to_numeric(s)\ns = pd.Series(['apple', '1.0', '2', -3])\npd.to_numeric(s, errors='ignore')\npd.to_numeric(s, errors='coerce')\n""]"
1514;;0;38341066;29370057.0;2;9;;;;"[""df = df[(df['date'] > '2000-6-1') & (df['date'] <= '2000-6-10')]\n""]"
1515;;0;38348167;10715965.0;2;8;;;;"[""import pandas as pd \n\nBaseData = pd.DataFrame({ 'Customer' : ['Acme','Mega','Acme','Acme','Mega','Acme'],\n                          'Territory'  : ['West','East','South','West','East','South'],\n                          'Product'  : ['Econ','Luxe','Econ','Std','Std','Econ']})\nBaseData\n\ncolumns = ['Customer','Num Unique Products', 'List Unique Products']\n\nrows_list=[]\nfor name, group in BaseData.groupby('Customer'):\n    RecordtoAdd={} #initialise an empty dict \n    RecordtoAdd.update({'Customer' : name}) #\n    RecordtoAdd.update({'Num Unique Products' : len(pd.unique(group['Product']))})      \n    RecordtoAdd.update({'List Unique Products' : pd.unique(group['Product'])})                   \n\n    rows_list.append(RecordtoAdd)\n\nAnalysedData = pd.DataFrame(rows_list)\n\nprint('Base Data : \\n',BaseData,'\\n\\n Analysed Data : \\n',AnalysedData)\n""]"
1516;;0;38421614;17095101.0;2;6;;;;"['nan', 'import pandas as pd\nimport numpy as np\n\n\ndef diff_pd(df1, df2):\n    """"""Identify differences between two pandas DataFrames""""""\n    assert (df1.columns == df2.columns).all(), \\\n        ""DataFrame column names are different""\n    if df1.equals(df2):\n        return None\n    else:\n        # need to account for np.nan != np.nan returning True\n        diff_mask = (df1 != df2) & ~(df1.isnull() & df2.isnull())\n        ne_stacked = diff_mask.stack()\n        changed = ne_stacked[ne_stacked]\n        changed.index.names = [\'id\', \'col\']\n        difference_locations = np.where(diff_mask)\n        changed_from = df1.values[difference_locations]\n        changed_to = df2.values[difference_locations]\n        return pd.DataFrame({\'from\': changed_from, \'to\': changed_to},\n                            index=changed.index)\n', 'import sys\nif sys.version_info[0] < 3:\n    from StringIO import StringIO\nelse:\n    from io import StringIO\n\nDF1 = StringIO(""""""id   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 ""He was late to class""\n112  Nick   1.11                     False                ""Graduated""\n113  Zoe    NaN                     True                  "" ""\n"""""")\nDF2 = StringIO(""""""id   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 ""He was late to class""\n112  Nick   1.21                     False                ""Graduated""\n113  Zoe    NaN                     False                ""On vacation"" """""")\ndf1 = pd.read_table(DF1, sep=\'\\s+\', index_col=\'id\')\ndf2 = pd.read_table(DF2, sep=\'\\s+\', index_col=\'id\')\ndiff_pd(df1, df2)\n', '                from           to\nid  col                          \n112 score       1.11         1.21\n113 isEnrolled  True        False\n    Comment           On vacation\n']"
1517;;0;38466059;20109391.0;2;12;;;;[]
1518;;1;38467449;13842088.0;2;17;;;;['.loc', 'df.loc[df[<some_column_name>] == <condition>, <another_column_name>] = <value_to_add>\n', '<some_column_name', '<condition>', '<another_column_name>', '<value_to_add>']
1519;;0;38503561;17679089.0;2;6;;;;"[""import pandas as pd\ndf = pd.DataFrame([['A','C','A','B','C','A','B','B','A','A'], ['ONE','TWO','ONE','ONE','ONE','TWO','ONE','TWO','ONE','THREE']]).T\ndf.columns = [['Alphabet','Words']]\nprint(df)   #printing dataframe.\n"", ""df['COUNTER'] =1       #initially, set that counter to 1.\ngroup_data = df.groupby(['Alphabet','Words'])['COUNTER'].sum() #sum function\nprint(group_data)\n""]"
1520;;0;38510820;12555323.0;2;21;;;;['df1 = df1.assign(e=np.random.randn(sLength))']
1521;;1;38544742;26473681.0;2;8;;;;['pip install -U pip\n']
1522;;0;38709267;11927715.0;2;17;;;;"['colormap', '.plot()', ""df.plot(kind='bar', stacked=True, colormap='Paired')\n""]"
1523;;0;38776854;11346283.0;2;7;;;;"['delimiters=', ""import pandas as pd\nimport re\n\n\ndf = pd.DataFrame({'$a':[1,2], '$b': [3,4],'$c':[5,6], '$d': [7,8], '$e': [9,10]})\n\ndelimiters = '$'\nmatchPattern = '|'.join(map(re.escape, delimiters))\ndf.columns = [re.split(matchPattern, i)[1] for i in df.columns ]\n"", '>>> df\n   $a  $b  $c  $d  $e\n0   1   3   5   7   9\n1   2   4   6   8  10\n\n>>> df\n   a  b  c  d   e\n0  1  3  5  7   9\n1  2  4  6  8  10\n']"
1524;;0;38900352;19798153.0;2;7;;;;['DataFrame.apply', 'DataFrame.applymap', 'Series.apply', 'Series.map', 'Series.apply', 'Series.map']
1525;;0;38902835;15772009.0;2;8;;;;"['sklearn.utils.shuffle()', ""# Generate data\nimport pandas as pd\ndf = pd.DataFrame({'A':range(5), 'B':range(5)})\nprint('df: {0}'.format(df))\n\n# Shuffle Pandas data frame\nimport sklearn.utils\ndf = sklearn.utils.shuffle(df)\nprint('\\n\\ndf: {0}'.format(df))\n"", 'df:    A  B\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n4  4  4\n\n\ndf:    A  B\n1  1  1\n0  0  0\n3  3  3\n4  4  4\n2  2  2\n', 'df.reset_index()', ""df = df.reset_index(drop=True)\nprint('\\n\\ndf: {0}'.format(df)\n"", 'df:    A  B\n0  1  1\n1  0  0\n2  4  4\n3  2  2\n4  3  3\n']"
1526;;0;39104306;17627219.0;2;19;;;;"[""from sklearn.metrics.pairwise import cosine_similarity\nfrom scipy import sparse\n\nA =  np.array([[0, 1, 0, 0, 1], [0, 0, 1, 1, 1],[1, 1, 0, 1, 0]])\nA_sparse = sparse.csr_matrix(A)\n\nsimilarities = cosine_similarity(A_sparse)\nprint('pairwise dense output:\\n {}\\n'.format(similarities))\n\n#also can output sparse matrices\nsimilarities_sparse = cosine_similarity(A_sparse,dense_output=False)\nprint('pairwise sparse output:\\n {}\\n'.format(similarities_sparse))\n"", 'pairwise dense output:\n[[ 1.          0.40824829  0.40824829]\n[ 0.40824829  1.          0.33333333]\n[ 0.40824829  0.33333333  1.        ]]\n\npairwise sparse output:\n(0, 1)  0.408248290464\n(0, 2)  0.408248290464\n(0, 0)  1.0\n(1, 0)  0.408248290464\n(1, 2)  0.333333333333\n(1, 1)  1.0\n(2, 1)  0.333333333333\n(2, 0)  0.408248290464\n(2, 2)  1.0\n', 'A_sparse.transpose()\n']"
1527;;4;39116381;24193174.0;2;6;;;;['set_color_cycle', 'set_prop_cycle', 'import matplotlib.pyplot as plt\nimport numpy as np\n\nfor i in range(3):\n    plt.plot(np.arange(10) + i)\n\nplt.gca().set_prop_cycle(None)\n\nfor i in range(3):\n    plt.plot(np.arange(10, 0, -1) + i)\n\nplt.show()\n', 'np.arange(10,1,-1)', 'np.arange(10,0,-1)', 'rcParams']
1528;;1;39192113;29763620.0;2;9;;;;"[""df[df.columns.difference(['b'])]\n\nOut: \n          a         c         d\n0  0.427809  0.459807  0.333869\n1  0.678031  0.668346  0.645951\n2  0.996573  0.673730  0.314911\n3  0.786942  0.719665  0.330833\n""]"
1529;;0;39206377;25146121.0;2;13;;;;"[""    df['mnth_yr'] = df['date_column'].apply(lambda x: x.strftime('%B-%Y'))     \n"", ""    df['date_column'] = pd.to_datetime(df['date_column'])\n""]"
1530;;0;39237712;13148429.0;2;19;;;;"[""df = df.reindex_axis(['mean',0,1,2,3,4], axis=1)\n"", ""df = df.reindex_axis(sorted(df.columns), axis=1)\ndf = df.reindex_axis(['opened'] + list([a for a in df.columns if a != 'opened']), axis=1)\n""]"
1531;;0;39246607;17618981.0;2;7;;;;"['f.sort_values(by=[""c1"",""c2""], ascending=[False, True])\n', '    c1  c2\n    3   10\n    2   15\n    2   30\n    2   100\n    1   20\n']"
1532;;0;39251401;21654635.0;2;13;;;;"['pip install seaborn', 'sns.pairplot(x_vars=[""one""], y_vars=[""two""], data=df, hue=""key1"", size=5)', 'import seaborn as sns\nimport pandas as pd\nimport numpy as np\nnp.random.seed(1974)\n\ndf = pd.DataFrame(\n    np.random.normal(10, 1, 30).reshape(10, 3),\n    index=pd.date_range(\'2010-01-01\', freq=\'M\', periods=10),\n    columns=(\'one\', \'two\', \'three\'))\ndf[\'key1\'] = (4, 4, 4, 6, 6, 6, 8, 8, 8, 8)\n\nsns.pairplot(x_vars=[""one""], y_vars=[""two""], data=df, hue=""key1"", size=5)\n', 'sns.pairplot(vars=[""one"",""two"",""three""], data=df, hue=""key1"", size=5)\n']"
1533;;1;39259437;13331698.0;2;12;;;;"[""df['col_3'] = df[['col_1','col_2']].apply(lambda x: f(*x), axis=1)\n""]"
1534;;2;39358752;19726663.0;2;8;;;;"[""import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport six\n\ndf = pd.DataFrame()\ndf['date'] = ['2016-04-01', '2016-04-02', '2016-04-03']\ndf['calories'] = [2200, 2100, 1500]\ndf['sleep hours'] = [2200, 2100, 1500]\ndf['gym'] = [True, False, False]\n\ndef render_mpl_table(data, col_width=3.0, row_height=0.625, font_size=14,\n                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n                     bbox=[0, 0, 1, 1], header_columns=0,\n                     ax=None, **kwargs):\n    if ax is None:\n        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n        fig, ax = plt.subplots(figsize=size)\n        ax.axis('off')\n\n    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n\n    mpl_table.auto_set_font_size(False)\n    mpl_table.set_fontsize(font_size)\n\n    for k, cell in six.iteritems(mpl_table._cells):\n        cell.set_edgecolor(edge_color)\n        if k[0] == 0 or k[1] < header_columns:\n            cell.set_text_props(weight='bold', color='w')\n            cell.set_facecolor(header_color)\n        else:\n            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n    return ax\n\nrender_mpl_table(df, header_columns=0, col_width=2.0)\n""]"
1535;;4;39358924;14745022.0;2;81;;;;"[""df['A'], df['B'] = df['AB'].str.split(' ', 1).str\n"", ""df['AB'].str.split(' ', 1, expand=True)\n"", '.tolist()', 'zip()', 'str.extract()', '.str.split()', "">>> import pandas as pd\n>>> df = pd.DataFrame({'AB': ['A1-B1', 'A2-B2']})\n>>> df\n\n      AB\n0  A1-B1\n1  A2-B2\n>>> df['AB_split'] = df['AB'].str.split('-')\n>>> df\n\n      AB  AB_split\n0  A1-B1  [A1, B1]\n1  A2-B2  [A2, B2]\n"", '.str.split()', '.str', '>>> upper_lower_df = pd.DataFrame({""U"": [""A"", ""B"", ""C""]})\n>>> upper_lower_df\n\n   U\n0  A\n1  B\n2  C\n>>> upper_lower_df[""L""] = upper_lower_df[""U""].str.lower()\n>>> upper_lower_df\n\n   U  L\n0  A  a\n1  B  b\n2  C  c\n', "">>> df['AB'].str[0]\n\n0    A\n1    A\nName: AB, dtype: object\n\n>>> df['AB'].str[1]\n\n0    1\n1    2\nName: AB, dtype: object\n"", '.str', "">>> df['AB'].str.split('-', 1).str[0]\n\n0    A1\n1    A2\nName: AB, dtype: object\n\n>>> df['AB'].str.split('-', 1).str[1]\n\n0    B1\n1    B2\nName: AB, dtype: object\n"", "">>> df['A'], df['B'] = df['AB'].str.split('-', 1).str\n>>> df\n\n      AB  AB_split   A   B\n0  A1-B1  [A1, B1]  A1  B1\n1  A2-B2  [A2, B2]  A2  B2\n"", '.str.split()', 'expand=True', "">>> df['AB'].str.split('-', 1, expand=True)\n\n    0   1\n0  A1  B1\n1  A2  B2\n"", "">>> df = df[['AB']]\n>>> df\n\n      AB\n0  A1-B1\n1  A2-B2\n\n>>> df.join(df['AB'].str.split('-', 1, expand=True).rename(columns={0:'A', 1:'B'}))\n\n      AB   A   B\n0  A1-B1  A1  B1\n1  A2-B2  A2  B2\n""]"
1536;;1;39370553;16476924.0;2;23;;;;"[""for i in range(0, len(df)):\n    print df.iloc[i]['c1'], df.iloc[i]['c2']\n""]"
1537;;2;39371897;24870306.0;2;18;;;;"['set.issubset', ""if set(['A','C']).issubset(df.columns):\n   df['sum'] = df['A'] + df['C']                \n"", 'set([])', ""if {'A', 'C'}.issubset(df.columns):\n""]"
1538;;0;39405540;16327055.0;2;10;;;;"[""df['C'] = np.nan"", '.reindex(columns=[...])', ""mydf = mydf.reindex( mydf.columns.tolist() + ['newcol1','newcol2'])  # version >= 0.20.0\n"", ""mydf = mydf.reindex( mydf.columns.values + ['newcol1','newcol2'])  # version < 0.20.0\n""]"
1539;;0;39474812;17978092.0;2;9;;;;"['string', ""df.apply(lambda r : pd.datetime.combine(r['date_column_name'],r['time_column_name']),1)\n""]"
1540;;0;39478896;13295735.0;2;6;;;;"[""import pandas\n\ndf = pandas.read_csv('somefile.txt')\n\ndf = df.fillna(0)\n""]"
1541;;2;39482402;32244753.0;2;27;;;;"[""AttributeError: 'AxesSubplot' object has no attribute 'fig'\nWhen trying to access the figure\n\nAttributeError: 'AxesSubplot' object has no attribute 'savefig'\nwhen trying to use the savefig directly as a function\n"", 'swarm_plot = sns.swarmplot(...)\nfig = swarm_plot.get_figure()\nfig.savefig(...) \n', 'fig = myGridPlotObject.fig\n']"
1542;;0;39628860;27673231.0;2;11;;;;['df2 = df\nfunc1(df2)\nfunc2(df)\n', 'df2 = df.copy()\nfunc1(df2)\nfunc2(df)\n']
1543;;0;39657077;19758364.0;2;10;;;;"['list-comprehension', ""df.columns = ['log(gdp)' if x=='gdp' else x for x in df.columns]\n"", ""df.columns = ['log(gdp)' if x=='gdp' else 'cap_mod' if x=='cap' else x for x in df.columns]\n"", 'dictionary', 'list-comprehension', 'get', ""col_dict = {'gdp': 'log(gdp)', 'cap': 'cap_mod'}   ## key?old name, value?new name\n\ndf.columns = [col_dict.get(x, x) for x in df.columns]\n"", ""%%timeit\ndf.rename(columns={'gdp':'log(gdp)'}, inplace=True)\n10000 loops, best of 3: 168 s per loop\n\n%%timeit\ndf.columns = ['log(gdp)' if x=='gdp' else x for x in df.columns]\n10000 loops, best of 3: 58.5 s per loop\n""]"
1544;;0;39673666;29576430.0;2;27;;;;['from sklearn.utils import shuffle\ndf = shuffle(df)\n']
1545;;1;39762850;26277757.0;2;7;;;;"[""pd.set_option('display.max_colwidth', -1)"", ""    old_width = pd.get_option('display.max_colwidth')\n    pd.set_option('display.max_colwidth', -1)\n    open('some_file.html', 'w').write(some_data.to_html())\n    pd.set_option('display.max_colwidth', old_width)\n""]"
1546;;0;39770407;11346283.0;2;19;;;;"['numpy.array', 'numpy.array', '.name', 'df.columns', 'list', 'Series', '.name', 'Series', ""df.columns = ['column_one', 'column_two']\ndf.columns.names = ['name of the list of columns']\ndf.index.names = ['name of the index']\n\nname of the list of columns     column_one  column_two\nname of the index       \n0                                    4           1\n1                                    5           2\n2                                    6           3\n"", '.name', ""df.columns = ['one', 'two']"", 'df.one.name', ""'one'"", ""df.one.name = 'three'"", 'df.columns', ""['one', 'two']"", 'df.one.name', ""'three'"", 'pd.DataFrame(df.one)', '    three\n0       1\n1       2\n2       3\n', '.name', 'Series', '    |one            |\n    |one      |two  |\n0   |  4      |  1  |\n1   |  5      |  2  |\n2   |  6      |  3  |\n', ""df.columns = [['one', 'one'], ['one', 'two']]\n""]"
1547;;0;39820329;22086116.0;2;8;;;;"[""df = df[df[['col_1','col_2']].apply(lambda x: f(*x), axis=1)]\n""]"
1548;;0;39837358;18695605.0;2;19;;;;['mydict = dict(zip(df.id, df.value))\n']
1549;;1;39891994;13295735.0;2;19;;;;"[""df['column']=df['column'].fillna(value)\n""]"
1550;;6;39923958;19124601.0;2;88;;;;['print(df.to_string())\n']
1551;;1;39946744;12680754.0;2;8;;;;"['Series', 'stack', 'def tidy_split(df, column, sep=\'|\', keep=False):\n    """"""\n    Split the values of a column and expand so the new DataFrame has one split\n    value per row. Filters rows where the column is missing.\n\n    Params\n    ------\n    df : pandas.DataFrame\n        dataframe with the column to split and expand\n    column : str\n        the column to split and expand\n    sep : str\n        the string used to split the column\'s values\n    keep : bool\n        whether to retain the presplit value as it\'s own row\n\n    Returns\n    -------\n    pandas.DataFrame\n        Returns a dataframe with the same columns as `df`.\n    """"""\n    indexes = list()\n    new_values = list()\n    df = df.dropna(subset=[column])\n    for i, presplit in enumerate(df[column].astype(str)):\n        values = presplit.split(sep)\n        if keep and len(values) > 1:\n            indexes.append(i)\n            new_values.append(presplit)\n        for value in values:\n            indexes.append(i)\n            new_values.append(value)\n    new_df = df.iloc[indexes, :].copy()\n    new_df[column] = new_values\n    return new_df\n', ""tidy_split(a, 'var1', sep=',')\n""]"
1552;;1;40008322;11350770.0;2;7;;;;"[""df.loc[:, df.columns.to_series().str.contains('a').tolist()]\n""]"
1553;;0;40110335;13851535.0;2;20;;;;"['df= df[df[""score""] > 50]\n']"
1554;;5;40214434;20625582.0;2;19;;;;"['quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\n', 'pandas.ix', '.ix', '.ix', 'df = pd.DataFrame({""a"": [1,2,3,4], ""b"": [1,1,2,2]})\n', 'dfcopy = df.ix[:,[""a""]]\ndfcopy.a.ix[0] = 2\n', 'dfcopy', 'df', 'df.ix[0, ""a""] = 3\n', '.loc', '.ix', '.iloc', '.loc', '.loc', '.loc', 'pd.read_csv', ""quote_df = pd.read_csv(StringIO(str_of_all), sep=',', names=list('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg')) #dtype={'A': object, 'B': object, 'C': np.float64}\nquote_df.rename(columns={'A':'STK', 'B':'TOpen', 'C':'TPCLOSE', 'D':'TPrice', 'E':'THigh', 'F':'TLow', 'I':'TVol', 'J':'TAmt', 'e':'TDate', 'f':'TTime'}, inplace=True)\nquote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\n"", ""columns = ['STK', 'TPrice', 'TPCLOSE', 'TOpen', 'THigh', 'TLow', 'TVol', 'TAmt', 'TDate', 'TTime']\ndf = pd.read_csv(StringIO(str_of_all), sep=',', usecols=[0,3,2,1,4,5,8,9,30,31])\ndf.columns = columns\n"", '.ix']"
1555;;1;40228738;29432629.0;2;22;;;;['import seaborn as sns\ncorr = dataframe.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)\n']
1556;;0;40429755;21319929.0;2;6;;;;"[""'a' in s.values"", ""In [2]: s = pd.Series(list('abc'))\n\nIn [3]: s\nOut[3]: \n0    a\n1    b\n2    c\ndtype: object\n\nIn [3]: s.isin(['a'])\nOut[3]: \n0    True\n1    False\n2    False\ndtype: bool\n\nIn [4]: s[s.isin(['a'])].empty\nOut[4]: False\n\nIn [5]: s[s.isin(['z'])].empty\nOut[5]: True\n"", "">>> df = DataFrame({'A': [1, 2, 3], 'B': [1, 4, 7]})\n>>> df.isin({'A': [1, 3], 'B': [4, 7, 12]})\n       A      B\n0   True  False  # Note that B didn't match 1 here.\n1  False   True\n2   True   True\n""]"
1557;;3;40435354;14984119.0;2;48;;;;"['df = df.loc[:,~df.columns.duplicated()]\n', ""['alpha','beta','alpha']"", 'df.columns.duplicated()', 'True', 'False', 'False', 'True', '[False,False,True]', 'Pandas', 'True', '[True, True, False] = ~[False,False,True]', 'df.loc[:,[True,True,False]]']"
1558;;2;40449726;12680754.0;2;22;;;;"['normal', 'list', ""def explode(df, lst_cols, fill_value=''):\n    # make sure `lst_cols` is a list\n    if lst_cols and not isinstance(lst_cols, list):\n        lst_cols = [lst_cols]\n    # all columns except `lst_cols`\n    idx_cols = df.columns.difference(lst_cols)\n\n    # calculate lengths of lists\n    lens = df[lst_cols[0]].str.len()\n\n    if (lens > 0).all():\n        # ALL lists in cells aren't empty\n        return pd.DataFrame({\n            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())\n            for col in idx_cols\n        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n          .loc[:, df.columns]\n    else:\n        # at least one list in cells is empty\n        return pd.DataFrame({\n            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())\n            for col in idx_cols\n        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n          .append(df.loc[lens==0, idx_cols]).fillna(fill_value) \\\n          .loc[:, df.columns]\n"", 'list', 'list', ""In [36]: df\nOut[36]:\n   aaa  myid        num          text\n0   10     1  [1, 2, 3]  [aa, bb, cc]\n1   11     2     [1, 2]      [cc, dd]\n2   12     3         []            []\n3   13     4         []            []\n\nIn [37]: explode(df, ['num','text'], fill_value='')\nOut[37]:\n   aaa  myid num text\n0   10     1   1   aa\n1   10     1   2   bb\n2   10     1   3   cc\n3   11     2   1   cc\n4   11     2   2   dd\n2   12     3\n3   13     4\n"", ""df = pd.DataFrame({\n 'aaa': {0: 10, 1: 11, 2: 12, 3: 13},\n 'myid': {0: 1, 1: 2, 2: 3, 3: 4},\n 'num': {0: [1, 2, 3], 1: [1, 2], 2: [], 3: []},\n 'text': {0: ['aa', 'bb', 'cc'], 1: ['cc', 'dd'], 2: [], 3: []}\n})\n"", ""In [46]: df\nOut[46]:\n        var1  var2 var3\n0      a,b,c     1   XX\n1  d,e,f,x,y     2   ZZ\n\nIn [47]: explode(df.assign(var1=df.var1.str.split(',')), 'var1')\nOut[47]:\n  var1  var2 var3\n0    a     1   XX\n1    b     1   XX\n2    c     1   XX\n3    d     2   ZZ\n4    e     2   ZZ\n5    f     2   ZZ\n6    x     2   ZZ\n7    y     2   ZZ\n"", 'list', ""In [48]: df.assign(var1=df.var1.str.split(','))\nOut[48]:\n              var1  var2 var3\n0        [a, b, c]     1   XX\n1  [d, e, f, x, y]     2   ZZ\n"", 'In [177]: df\nOut[177]:\n        var1  var2 var3\n0      a,b,c     1   XX\n1  d,e,f,x,y     2   ZZ\n', ""In [178]: lst_col = 'var1' \n\nIn [179]: x = df.assign(**{lst_col:df[lst_col].str.split(',')})\n\nIn [180]: x\nOut[180]:\n              var1  var2 var3\n0        [a, b, c]     1   XX\n1  [d, e, f, x, y]     2   ZZ\n"", 'In [181]: pd.DataFrame({\n     ...:     col:np.repeat(x[col].values, x[lst_col].str.len())\n     ...:     for col in x.columns.difference([lst_col])\n     ...: }).assign(**{lst_col:np.concatenate(x[lst_col].values)})[x.columns.tolist()]\n     ...:\nOut[181]:\n  var1  var2 var3\n0    a     1   XX\n1    b     1   XX\n2    c     1   XX\n3    d     2   ZZ\n4    e     2   ZZ\n5    f     2   ZZ\n6    x     2   ZZ\n7    y     2   ZZ\n', ""In [2]: df = pd.DataFrame(\n   ...:    [{'var1': 'a,b,c', 'var2': 1, 'var3': 'XX'},\n   ...:     {'var1': 'd,e,f,x,y', 'var2': 2, 'var3': 'ZZ'}]\n   ...: )\n\nIn [3]: df\nOut[3]:\n        var1  var2 var3\n0      a,b,c     1   XX\n1  d,e,f,x,y     2   ZZ\n\nIn [4]: (df.set_index(df.columns.drop('var1',1).tolist())\n   ...:    .var1.str.split(',', expand=True)\n   ...:    .stack()\n   ...:    .reset_index()\n   ...:    .rename(columns={0:'var1'})\n   ...:    .loc[:, df.columns]\n   ...: )\nOut[4]:\n  var1  var2 var3\n0    a     1   XX\n1    b     1   XX\n2    c     1   XX\n3    d     2   ZZ\n4    e     2   ZZ\n5    f     2   ZZ\n6    x     2   ZZ\n7    y     2   ZZ\n""]"
1559;;1;40501868;17091769.0;2;6;;;;"[""df = DataFrame(columns=('col1', 'col2', 'col3'))\nfor i in range(5):\n   df.loc[i] = ['<some value for first>','<some value for second>','<some value for third>']""]"
1560;;1;40535454;17326973.0;2;6;;;;"[""# Given a dict of dataframes, for example:\n# dfs = {'gadgets': df_gadgets, 'widgets': df_widgets}\n\nwriter = pd.ExcelWriter(filename, engine='xlsxwriter')\nfor sheetname, df in dfs.items():  # loop through `dict` of dataframes\n    df.to_excel(writer, sheet_name=sheetname)  # send df to writer\n    worksheet = writer.sheets[sheetname]  # pull worksheet object\n    for idx, col in enumerate(df):  # loop through all columns\n        series = df[col]\n        max_len = max((\n            series.astype(str).map(len).max(),  # len of largest item\n            len(str(series.name))  # len of column name/header\n            )) + 1  # adding a little extra space\n        worksheet.set_column(idx, idx, max_len)  # set column width\nwriter.save()\n""]"
1561;;2;40629420;15705630.0;2;14;;;;"[""df.sort_values('count', ascending=False).drop_duplicates(['Sp','Mt'])\n""]"
1562;;0;40834052;22591174.0;2;15;;;;"[""df_filtered = df.query('a == 4 & b != 2')\n""]"
1563;;0;41022840;16476924.0;2;38;;;;"['for index, row in df.iterrows():\n    print row[""c1""], row[""c2""]\n', 'for row in df.itertuples(index=True, name=\'Pandas\'):\n    print getattr(row, ""c1""), getattr(row, ""c2"")\n', 'itertuples()', 'iterrows()', 'dtype', 'new_df = df.apply(lambda x: x * 2)\n']"
1564;;2;41173392;17097643.0;2;9;;;;"['    df[df[""col""].str.contains(\'this\'|\'that\')==False]\n']"
1565;;0;41210491;13129618.0;2;7;;;;['pd.cut', 'value_counts', 'step = 50\nbin_range = np.arange(-200, 1000+step, step)\nout, bins  = pd.cut(s, bins=bin_range, include_lowest=True, right=False, retbins=True)\nout.value_counts(sort=False).plot.bar()\n', 'out.value_counts().head()\n[-100, -50)    18\n[0, 50)        16\n[800, 850)      2\n[-50, 0)        2\n[950, 1000)     1\ndtype: int64\n', 'out.cat.categories = bins[:-1]\nout.value_counts(sort=False).plot.bar()\n']
1566;;3;41218519;21291259.0;2;8;;;;"['    df = pd.DataFrame(10*np.random.rand(3, 4), columns=list(""ABCD""))\n\n              A         B         C         D\n    0  8.362940  0.354027  1.916283  6.226750\n    1  1.988232  9.003545  9.277504  8.522808\n    2  1.141432  4.935593  2.700118  7.739108\n\n    cols = [\'A\', \'B\']\n    df[cols] = df[cols].applymap(np.int64)\n\n       A  B         C         D\n    0  8  0  1.916283  6.226750\n    1  1  9  9.277504  8.522808\n    2  1  4  2.700118  7.739108\n\n    df[\'C\'] = df[\'C\'].apply(np.int64)\n       A  B  C         D\n    0  8  0  1  6.226750\n    1  1  9  9  8.522808\n    2  1  4  2  7.739108\n']"
1567;;2;41228272;16777570.0;2;8;;;;"['to_series', 'timedelta64[ns]', '.dt', ""In [13]: df['deltaT'] = df.index.to_series().diff().dt.seconds.div(60, fill_value=0)\n    ...: df                                 # use .astype(int) to obtain integer values\nOut[13]: \n                     value  deltaT\ntime                              \n2012-03-16 23:50:00      1     0.0\n2012-03-16 23:56:00      2     6.0\n2012-03-17 00:08:00      3    12.0\n2012-03-17 00:10:00      4     2.0\n2012-03-17 00:12:00      5     2.0\n2012-03-17 00:20:00      6     8.0\n2012-03-20 00:43:00      7    23.0\n"", 'diff', 'In [8]: ser_diff = df.index.to_series().diff()\n\nIn [9]: ser_diff\nOut[9]: \ntime\n2012-03-16 23:50:00               NaT\n2012-03-16 23:56:00   0 days 00:06:00\n2012-03-17 00:08:00   0 days 00:12:00\n2012-03-17 00:10:00   0 days 00:02:00\n2012-03-17 00:12:00   0 days 00:02:00\n2012-03-17 00:20:00   0 days 00:08:00\n2012-03-20 00:43:00   3 days 00:23:00\nName: time, dtype: timedelta64[ns]\n', 'In [10]: ser_diff.dt.seconds.div(60, fill_value=0)\nOut[10]: \ntime\n2012-03-16 23:50:00     0.0\n2012-03-16 23:56:00     6.0\n2012-03-17 00:08:00    12.0\n2012-03-17 00:10:00     2.0\n2012-03-17 00:12:00     2.0\n2012-03-17 00:20:00     8.0\n2012-03-20 00:43:00    23.0\nName: time, dtype: float64\n', 'date', 'dt.total_seconds', 'In [12]: ser_diff.dt.total_seconds().div(60, fill_value=0)\nOut[12]: \ntime\n2012-03-16 23:50:00       0.0\n2012-03-16 23:56:00       6.0\n2012-03-17 00:08:00      12.0\n2012-03-17 00:10:00       2.0\n2012-03-17 00:12:00       2.0\n2012-03-17 00:20:00       8.0\n2012-03-20 00:43:00    4343.0    # <-- number of minutes in 3 days 23 minutes\nName: time, dtype: float64\n']"
1568;;0;41228807;18215317.0;2;7;;;;"['dt.days', ""In [14]: s = pd.Series(pd.timedelta_range(start='1 days', end='12 days', freq='3000T'))\n\nIn [15]: s\nOut[15]: \n0    1 days 00:00:00\n1    3 days 02:00:00\n2    5 days 04:00:00\n3    7 days 06:00:00\n4    9 days 08:00:00\n5   11 days 10:00:00\ndtype: timedelta64[ns]\n\nIn [16]: s.dt.days\nOut[16]: \n0     1\n1     3\n2     5\n3     7\n4     9\n5    11\ndtype: int64\n"", '.components', 'timedelta', 'In [17]: s.dt.components\nOut[17]: \n   days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds\n0     1      0        0        0             0             0            0\n1     3      2        0        0             0             0            0\n2     5      4        0        0             0             0            0\n3     7      6        0        0             0             0            0\n4     9      8        0        0             0             0            0\n5    11     10        0        0             0             0            0\n', 'hours', 'In [23]: s.dt.components.hours\nOut[23]: \n0     0\n1     2\n2     4\n3     6\n4     8\n5    10\nName: hours, dtype: int64\n']"
1569;;0;41452422;32244753.0;2;7;;;;"['.fig', '.savefig()', 'sns_plot.figure.savefig(""output.png"")\n']"
1570;;0;41517319;19991445.0;2;8;;;;"['sklearn', 'pandas', 'sklearn', ""from sklearn import linear_model\n\nreg = linear_model.LinearRegression()\nreg.fit(df[['B', 'C']], df['A'])\n\n>>> reg.coef_\narray([  4.01182386e-01,   3.51587361e-04])\n""]"
1571;;1;41529411;13784192.0;2;28;;;;"[""newDF = pd.DataFrame() #creates a new dataframe that's empty\nnewDF = newDF.append(oldDF, ignore_index = True) # ignoring index is optional\n# try printing some data from newDF\nprint newDF.head() #again optional \n""]"
1572;;0;41554866;26266362.0;2;6;;;;"[""import pandas as pd\n## df1 as an example data frame \n## col1 name of column for which you want to calculate the nan values\nsum(pd.isnull(df1['col1']))\n""]"
1573;;3;41607207;34001922.0;2;13;;;;['tf.global_variables_initializer()\n', 'with tf.Session() as sess:\n     sess.run(tf.global_variables_initializer())\n']
1574;;2;41678874;20250771.0;2;16;;;;"[""df['col1'].map(di)\n"", 'map', 'update', ""df['col1'].update( df['col1'].map(di) )   # note: series update is an inplace operation\n"", 'di = {1: ""A"", 2: ""B"", 3: ""C"", 4: ""D"", 5: ""E"", 6: ""F"", 7: ""G"", 8: ""H"" }\ndf = pd.DataFrame({ \'col1\': np.random.choice( range(1,9), 100000 ) })\n\n%timeit df.replace({""col1"": di})\n10 loops, best of 3: 55.6 ms per loop\n\n%timeit df[\'col1\'].map(di)\n100 loops, best of 3: 4.16 ms per loop\n']"
1575;;0;41802199;29370057.0;2;7;;;;"['isin', 'date', 'df[df[""date""].isin(pd.date_range(start_date, end_date))]', 'import numpy as np   \nimport pandas as pd\n\n# Make a DataFrame with dates and random numbers\ndf = pd.DataFrame(np.random.random((30, 3)))\ndf[\'date\'] = pd.date_range(\'2017-1-1\', periods=30, freq=\'D\')\n\n# Select the rows between two dates\nin_range_df = df[df[""date""].isin(pd.date_range(""2017-01-15"", ""2017-01-20""))]\n\nprint(in_range_df)  # print result\n', '           0         1         2       date\n14  0.960974  0.144271  0.839593 2017-01-15\n15  0.814376  0.723757  0.047840 2017-01-16\n16  0.911854  0.123130  0.120995 2017-01-17\n17  0.505804  0.416935  0.928514 2017-01-18\n18  0.204869  0.708258  0.170792 2017-01-19\n19  0.014389  0.214510  0.045201 2017-01-20\n']"
1576;;1;41816244;22697773.0;2;15;;;;"[""df.select_dtypes(include=['float64']).apply(your_function)\ndf.select_dtypes(exclude=['string','object']).apply(your_other_function)\n""]"
1577;;0;41845355;22898824.0;2;7;;;;"[""df[(df['date']>datetime.date(2016,1,1)) & (df['date']<datetime.date(2016,3,1))]  \n"", 'import datetime\ndatetime.datetime.strptime\n']"
1578;;0;41876593;21285380.0;2;9;;;;"[""import pandas as pd\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6]}\ndf = pd.DataFrame(data)\n\nprint(df.filter(like='spike').columns)\n"", ""print(df.filter(regex='spike|spke').columns)\n""]"
1579;;2;41880513;32400867.0;2;35;;;;"['0.19.2', 'import pandas as pd\n\nurl=""https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv""\nc=pd.read_csv(url)\n']"
1580;;1;42094658;14744068.0;2;10;;;;"['pandas.concat()', ""import pandas as pd\n\npd.concat([df], keys=['Foo'], names=['Firstlevel'])\n""]"
1581;;0;42133330;14057007.0;2;11;;;;"['df[-df[""column""].isin([""value""])]', 'df[~df[""column""].isin([""value""])]', 'df[df[""column""].isin([""value""]) == False]', 'df[np.logical_not(df[""column""].isin([""value""]))]', 'import numpy as np']"
1582;;1;42247228;25646200.0;2;8;;;;['dt.days', 'td', 'td.dt.days\n']
1583;;0;42293737;25351968.0;2;7;;;;"[""pd.set_option('display.max_columns', None)  \n"", 'id']"
1584;;1;42335527;42157944.0;2;11;;;;['try']
1585;;1;42392805;26873127.0;2;10;;;;"['from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = ""all""\n', 'df1\ndf2\n']"
1586;;1;42401977;42157944.0;2;7;;;;"[""import pandas as pd\nfrom multiprocessing import Pool\n\ndef reader(filename):\n    return pd.read_excel(filename)\n\ndef main():\n    pool = Pool(4) # number of cores you want to use\n    file_list = [file1.xlsx, file2.xlsx, file3.xlsx, ...]\n    df_list = pool.map(reader, file_list) #creates a list of the loaded df's\n    df = pd.concat(df_list) # concatenates all the df's into a single df\n\nif __name__ == '__main__':\n    main()\n"", 'echo %NUMBER_OF_PROCESSORS%\n']"
1587;;4;42426751;42347868.0;2;18;;;;"['print(\n    df\n    .head(10)\n    .to_string(\n        formatters={""total_bill"": ""${:,.2f}"".format, \n                    ""tip"": ""${:,.2f}"".format,\n                    ""date"": lambda x: ""{:%m/%d/%Y}"".format(pd.to_datetime(x, unit=""D""))\n        }\n    )\n)\n\n  total_bill   tip     sex smoker  day    time  size       date\n0     $16.99 $1.01  Female     No  Sun  Dinner     2 02/08/2017\n1     $10.34 $1.66    Male     No  Sun  Dinner     3 02/09/2017\n2     $21.01 $3.50    Male     No  Sun  Dinner     3 02/10/2017\n3     $23.68 $3.31    Male     No  Sun  Dinner     2 02/11/2017\n4     $24.59 $3.61  Female     No  Sun  Dinner     4 02/12/2017\n5     $25.29 $4.71    Male     No  Sun  Dinner     4 02/13/2017\n6      $8.77 $2.00    Male     No  Sun  Dinner     2 02/14/2017\n7     $26.88 $3.12    Male     No  Sun  Dinner     4 02/15/2017\n8     $15.04 $1.96    Male     No  Sun  Dinner     2 02/16/2017\n9     $14.78 $3.23    Male     No  Sun  Dinner     2 02/17/2017\n']"
1588;;0;42545576;22149584.0;2;6;;;;['axis', 'pd.DataFrame', 'axis=0', 'axis=1', 'ndarray', '(3,5,7)', 'a = np.ones((3,5,7))\n', 'a', 'ndarray', 'a', 'a[0,:,:]', 'a[1,:,:]', 'a.sum(axis=0)', 'sum()', 'a', '(5,7)', 'a.sum(axis=0)', 'b = np.zeros((5,7))\nfor i in range(5):\n    for j in range(7):\n        b[i,j] += a[:,i,j].sum()\n', 'b', 'a.sum(axis=0)', 'array([[ 3.,  3.,  3.,  3.,  3.,  3.,  3.],\n       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.],\n       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.],\n       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.],\n       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.]])\n', 'pd.DataFrame', 'numpy.array', 'axis=0', 'sum()', 'axis=0', 'axis=1']
1589;;0;42550516;22219004.0;2;6;;;;"[""import numpy as np\n\ndf = pd.DataFrame( {'a':np.random.randint(0,60,600), 'b':[1,2,5,5,4,6]*100})\n\ndef f(df):\n         keys,values=df.sort_values('a').values.T\n         ukeys,index=np.unique(keys,True)\n         arrays=np.split(values,index[1:])\n         df2=pd.DataFrame({'a':ukeys,'b':[list(a) for a in arrays]})\n         return df2\n"", ""In [301]: %timeit f(df)\n1000 loops, best of 3: 1.64 ms per loop\n\nIn [302]: %timeit df.groupby('a')['b'].apply(list)\n100 loops, best of 3: 5.26 ms per loop\n""]"
1590;;1;42837693;17091769.0;2;10;;;;"['df = pd.Dataframe(columns=[""firstname"", ""lastname""])\ndf = df.append({\n     ""firstname"": ""John"",\n     ""lastname"":  ""Johny""\n      }, ignore_index=True)\n']"
1591;;4;42932524;38250710.0;2;8;;;;['train', 'test', 'cv', '0.6', '0.2', '0.2', 'train_test_split', 'x, x_test, y, y_test = train_test_split(xtrain,labels,test_size=0.2,train_size=0.8)\nx_train, x_cv, y_train, y_cv = train_test_split(x,y,test_size = 0.25,train_size =0.75)\n']
1592;;2;42965916;26139423.0;2;7;;;;"['sns.palplot(sns.color_palette(""Set2"", 8))\n', 'matplotlib', '# Unique category labels: \'D\', \'F\', \'G\', ...\ncolor_labels = df[\'color\'].unique()\n\n# List of RGB triplets\nrgb_values = sns.color_palette(""Set2"", 8)\n\n# Map label to RGB\ncolor_map = dict(zip(color_labels, rgb_values))\n\n# Finally use the mapped values\nplt.scatter(df[\'carat\'], df[\'price\'], c=df[\'color\'].map(color_map))\n']"
1593;;1;42977946;29432629.0;2;10;;;;"[""pd.scatter_matrix(dataframe, alpha = 0.3, figsize = (14,8), diagonal = 'kde');\n"", 'sns.pairplot(dataframe)\n', 'import seaborn as sns\n\nf, ax = pl.subplots(figsize=(10, 8))\ncorr = dataframe.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\n']"
1594;;1;42978156;20845213.0;2;7;;;;"[""df.to_csv('file_name.csv',index=False)"", ""df.to_csv(' file_name.csv ')"", ""df_new = pd.read_csv('file_name.csv').drop(['unnamed 0'],axis=1)""]"
1595;;0;43093390;23199796.0;2;14;;;;"['q = df[""col""].quantile(0.99)\n', 'df[df[""col""] < q]\n']"
1596;;1;43180437;12555323.0;2;6;;;;"['__getitem__', '[]', '__setitem__', '[] =', '[]', ""    size      name color\n0    big      rose   red\n1  small    violet  blue\n2  small     tulip   red\n3  small  harebell  blue\n\ndf['protected'] = ['no', 'no', 'no', 'yes']\n\n    size      name color protected\n0    big      rose   red        no\n1  small    violet  blue        no\n2  small     tulip   red        no\n3  small  harebell  blue       yes\n"", ""df.index = [3,2,1,0]\ndf['protected'] = ['no', 'no', 'no', 'yes']\n    size      name color protected\n3    big      rose   red        no\n2  small    violet  blue        no\n1  small     tulip   red        no\n0  small  harebell  blue       yes\n"", 'pd.Series', ""df['protected'] = pd.Series(['no', 'no', 'no', 'yes'])\n    size      name color protected\n3    big      rose   red       yes\n2  small    violet  blue        no\n1  small     tulip   red        no\n0  small  harebell  blue        no\n"", 'pd.Series', '[] =', '[] =', ""df['column'] = series"", '[]=', '[]=', 'pd.Series', 'pd.Series', 'np.ndarray', 'list', ""df['protected'] = pd.Series(['no', 'no', 'no', 'yes']).values\n"", ""df['protected'] = list(pd.Series(['no', 'no', 'no', 'yes']))\n"", 'pd.Series', 'df', ""df['protected'] = pd.Series(['no', 'no', 'no', 'yes'], index=df.index)\n"", 'pd.Series', ""protected_series = pd.Series(['no', 'no', 'no', 'yes'])\nprotected_series.index = df.index\n\n3     no\n2     no\n1     no\n0    yes\n"", ""df['protected'] = protected_series\n\n    size      name color protected\n3    big      rose   red        no\n2  small    violet  blue        no\n1  small     tulip   red        no\n0  small  harebell  blue       yes\n"", 'df.reset_index()', ""df.reset_index(drop=True)\nprotected_series.reset_index(drop=True)\ndf['protected'] = protected_series\n\n    size      name color protected\n0    big      rose   red        no\n1  small    violet  blue        no\n2  small     tulip   red        no\n3  small  harebell  blue       yes\n"", 'df.assign', 'df.assign', '[]=', ""df.assign(protected=pd.Series(['no', 'no', 'no', 'yes']))\n    size      name color protected\n3    big      rose   red       yes\n2  small    violet  blue        no\n1  small     tulip   red        no\n0  small  harebell  blue        no\n"", 'df.assign', 'self', 'df.assign', ""df.assign(self=pd.Series(['no', 'no', 'no', 'yes'])\nTypeError: assign() got multiple values for keyword argument 'self'\n"", 'self']"
1597;;1;43190411;19377969.0;2;6;;;;"[""In [1]: import pandas as pd \n\nIn [2]: df = pd.DataFrame([[0, 'the', 'quick', 'brown'],\n   ...:                    [1, 'fox', 'jumps', 'over'], \n   ...:                    [2, 'the', 'lazy', 'dog']],\n   ...:                   columns=['c0', 'c1', 'c2', 'c3'])\n\nIn [3]: def str_join(df, sep, *cols):\n   ...:     from functools import reduce\n   ...:     return reduce(lambda x, y: x.astype(str).str.cat(y.astype(str), sep=sep), \n   ...:                   [df[col] for col in cols])\n   ...: \n\nIn [4]: df['cat'] = str_join(df, '-', 'c0', 'c1', 'c2', 'c3')\n\nIn [5]: df\nOut[5]: \n   c0   c1     c2     c3                cat\n0   0  the  quick  brown  0-the-quick-brown\n1   1  fox  jumps   over   1-fox-jumps-over\n2   2  the   lazy    dog     2-the-lazy-dog\n""]"
1598;;0;43289220;21197774.0;2;8;;;;"['DataFrame.astype(dtype, copy=True, raise_on_error=True, **kwargs)', 'dtype', 'import pandas as pd\nwheel_number = 5\ncar_name = \'jeep\'\nminutes_spent = 4.5\n\n# set the columns\ndata_columns = [\'wheel_number\', \'car_name\', \'minutes_spent\']\n\n# create an empty dataframe\ndata_df = pd.DataFrame(columns = data_columns)\ndf_temp = pd.DataFrame([[wheel_number, car_name, minutes_spent]],columns = data_columns)\ndata_df = data_df.append(df_temp, ignore_index=True) \n\nIn [11]: data_df.dtypes\nOut[11]:\nwheel_number     float64\ncar_name          object\nminutes_spent    float64\ndtype: object\n\ndata_df = data_df.astype(dtype= {""wheel_number"":""int64"",\n        ""car_name"":""object"",""minutes_spent"":""float64""})\n', 'In [18]: data_df.dtypes\nOut[18]:\nwheel_number       int64\ncar_name          object\nminutes_spent    float64\n']"
1599;;0;43421391;14661701.0;2;12;;;;['df.drop(df.index[])', '100M rows x 3 cols', '10k', 'take', 'indexes_to_drop', '[1, 2, 4]', 'indexes_to_keep = set(range(df.shape[0])) - set(indexes_to_drop)\ndf_sliced = df.take(list(indexes_to_keep))\n', '20.5s', 'df.drop', '5min 27s']
1600;;1;43559496;43423347.0;2;10;;;;"[""x = pd.DataFrame(list(zip(range(4), range(4))), columns=['a', 'b'])\nprint(x)\n   a  b\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n"", "" q = x.loc[:, 'a']\n"", 'q += 2\nprint(x)  # checking x again, wow! it changed!\n   a  b\n0  2  0\n1  3  1\n2  4  2\n3  5  3\n', '.copy()', ""x = pd.DataFrame(list(zip(range(4), range(4))), columns=['a', 'b'])\nprint(x)\n   a  b\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n\nq = x.loc[:, 'a'].copy()\nq += 2\nprint(x)  # oh, x did not change because q is a copy now\n   a  b\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n"", 'q', '.copy()', ""df.loc[:, 'a']"", ""df.loc[df.index, 'a']"", ':', 'df.index', '.copy()', '.copy()', '.copy()', 'df.is_copy = None', 'df[columns]', 'df.loc[indexer, columns]']"
1601;;2;43665978;43423347.0;2;6;;;;"['copy()', '.copy()', 'copy()', 'SettingWithCopyWarning', '.copy()', ""@profile\ndef foo():\n    df = pd.DataFrame(np.random.randn(2 * 10 ** 7))\n\n    d1 = df[:]\n    d1 = d1.copy()\n\nif __name__ == '__main__':\n    foo()\n"", '.copy()', '> python -m memory_profiler demo.py \nFilename: demo.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     4   61.195 MiB    0.000 MiB   @profile\n     5                             def foo():\n     6  213.828 MiB  152.633 MiB    df = pd.DataFrame(np.random.randn(2 * 10 ** 7))\n     7                             \n     8  213.863 MiB    0.035 MiB    d1 = df[:]\n     9  366.457 MiB  152.594 MiB    d1 = d1.copy()\n', 'df', 'df', 'MemoryError', '.copy()', 'MemoryError', 'df = df.copy()', 'df', 'copy()', '> mprof run -T 0.001 demo.py\nLine #    Mem usage    Increment   Line Contents\n================================================\n     7     62.9 MiB      0.0 MiB   @profile\n     8                             def foo():\n     9    215.5 MiB    152.6 MiB    df = pd.DataFrame(np.random.randn(2 * 10 ** 7))\n    10    215.5 MiB      0.0 MiB    df = df.copy()\n']"
1602;;2;43896119;25146121.0;2;11;;;;"['date_column', ""import datetime as dt\ndf['month_year'] = df.date_column.dt.to_period('M')\n""]"
1603;;4;43897124;19078325.0;2;17;;;;"[""# Create a sample data frame\ndf = pd.DataFrame({'A': [1, 1, 1, 2, 2],\n                   'B': range(5),\n                   'C': range(5)})\n\n# ==== SINGLE COLUMN (SERIES) ====\n# Syntax soon to be deprecated\ndf.groupby('A').B.agg({'foo': 'count'})\n# Recommended replacement syntax\ndf.groupby('A').B.agg(['count']).rename(columns={'count': 'foo'})\n\n# ==== MULTI COLUMN ====\n# Syntax soon to be deprecated\ndf.groupby('A').agg({'B': {'foo': 'sum'}, 'C': {'bar': 'min'}})\n# Recommended replacement syntax\ndf.groupby('A').agg({'B': 'sum', 'C': 'min'}).rename(columns={'B': 'foo', 'C': 'bar'})\n# As the recommended syntax is more verbose, parentheses can\n# be used to introduce line breaks and increase readability\n(df.groupby('A')\n    .agg({'B': 'sum', 'C': 'min'})\n    .rename(columns={'B': 'foo', 'C': 'bar'})\n)\n"", 'lambda', '.agg', "">>> df.groupby('A').agg({'B': {'min': lambda x: x.min(), 'max': lambda x: x.max()}})\n\n    B    \n  max min\nA        \n1   2   0\n2   4   3\n"", "">>> df.groupby('A').agg({'B': [np.min, np.max]})\n\n     B     \n  amin amax\nA          \n1    0    2\n2    3    4\n"", '<lambda>', "">>> df.groupby('A').agg({'B': [lambda x: x.min(), lambda x: x.max]})\nSpecificationError: Function names must be unique, found multiple named <lambda>\n"", 'SpecificationError', 'lambda', '.rename', "">>> def my_min(x):\n>>>     return x.min()\n\n>>> def my_max(x):\n>>>     return x.max()\n\n>>> df.groupby('A').agg({'B': [my_min, my_max]})\n\n       B       \n  my_min my_max\nA              \n1      0      2\n2      3      4\n""]"
1604;;2;43968774;28757389.0;2;14;;;;"['pandas', '0.20', 'ix', 'loc', 'iloc', 'at', 'iat', 'set_value', 'loc', 'loc', ""# label based, but we can use position values\n# to get the labels from the index object\ndf.loc[df.index[2], 'ColName'] = 3\n"", ""df.loc[df.index[1:3], 'ColName'] = 3\n"", 'iloc', 'loc', ""# position based, but we can get the position\n# from the columns object via the `get_loc` method\ndf.iloc[2, df.columns.get_loc('ColName')] = 3\n"", 'df.iloc[2, 4] = 3\n', 'df.iloc[:3, 2:4] = 3\n', 'at', 'loc', 'loc', ""# label based, but we can use position values\n# to get the labels from the index object\ndf.at[df.index[2], 'ColName'] = 3\n"", ""df.at['C', 'ColName'] = 3\n"", 'iat', 'iloc', 'iloc', ""# position based, but we can get the position\n# from the columns object via the `get_loc` method\nIBM.iat[2, IBM.columns.get_loc('PNL')] = 3\n"", 'set_value', 'loc', 'pandas', ""# label based, but we can use position values\n# to get the labels from the index object\ndf.set_value(df.index[2], 'ColName', 3)\n"", 'set_value', 'takable=True', 'iloc', 'pandas', ""# position based, but we can get the position\n# from the columns object via the `get_loc` method\ndf.set_value(2, df.columns.get_loc('ColName'), 3, takable=True)\n""]"
1605;;3;44311454;21608228.0;2;19;;;;"['.ix', '.ix', '.loc', 'iloc', ""mask = df.my_channel > 20000\ncolumn_name = 'my_channel'\ndf.loc[mask, column_name] = 0\n"", 'mask', 'df.my_channel > 20000', 'True', 'df.loc[mask, column_name] = 0', 'mask', 'column_name', 'loc', 'iloc', 'NotImplementedError']"
1606;;0;44736467;10665889.0;2;18;;;;"['.loc', '.loc', 'foo', 'bar', 'quz', 'ant', 'cat', 'sat', 'dat', ""# selects all rows and all columns beginning at 'foo' up to and including 'ant'\ndf.loc[:, 'foo':'sat']\n# foo bar quz ant cat sat\n"", '.loc', 'start:stop:step', ""# slice from 'foo' to 'cat' by every 2nd column\ndf.loc[:, 'foo':'cat':2]\n# foo quz cat\n\n# slice from the beginning to 'bar'\ndf.loc[:, :'bar']\n# foo bar\n\n# slice from 'quz' to the end by 3\ndf.loc[:, 'quz'::3]\n# quz sat\n\n# attempt from 'sat' to 'bar'\ndf.loc[:, 'sat':'bar']\n# no columns returned\n\n# slice from 'sat' to 'bar'\ndf.loc[:, 'sat':'bar':-1]\nsat cat ant quz bar\n\n# slice notation is syntatic sugar for the slice function\n# slice from 'quz' to the end by 2 with slice function\ndf.loc[:, slice('quz',None, 2)]\n# quz cat dat\n\n# select specific columns with a list\n# select columns foo, bar and dat\ndf.loc[:, ['foo','bar','dat']]\n# foo bar dat\n"", 'v', 'w', 'x', 'y', 'z', ""# slice from 'w' to 'y' and 'foo' to 'ant' by 3\ndf.loc['w':'y', 'foo':'ant':3]\n#    foo ant\n# w\n# x\n# y\n""]"
1607;;0;44913631;12307099.0;2;11;;;;"['loc', '>>> import pandas as pd \n>>> import numpy as np \n>>> df = pd.DataFrame({""A"":[0,1,0], ""B"":[2,0,5]}, columns=list(\'AB\'))\n>>> df.loc[df.A == 0, \'B\'] = np.nan\n>>> df\n   A   B\n0  0 NaN\n1  1   0\n2  0 NaN\n>>> \n']"
1608;;0;45357725;13411544.0;2;10;;;;"[""#for dropping single column \ndf = df.drop('your_column', axis=1)\n\n#for dropping multiple columns\ndf = df.drop(['col_1','col_2','col_3'], axis=1)\n""]"
1609;;0;45568211;22697773.0;2;6;;;;"['pandas 0.20.2', ""from pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\n\nis_string_dtype(df['A'])\n>>>> True\n\nis_numeric_dtype(df['B'])\n>>>> True\n"", 'for y in agg.columns:\n    if (is_string_dtype(agg[y])):\n        treat_str(agg[y])\n    elif (is_numeric_dtype(agg[y])):\n        treat_numeric(agg[y])\n']"
