;Unnamed: 0;Unnamed: 0.1;Unnamed: 0.1.1;AnswerCount;CommentCount;Id;ParentId;PostTypeId;Score;Tags;Title;ViewCount;Code;CodeList;AllCode;ProcessedCode;PandasImportCode;PandasImportNormalized;DataFrameCode;DataFrameNormalized;OriginalCompiled;OriginalTotal;OriginalErrors;OriginalErrors Type;PandasImportCompiled;PandasImportTotal;PandasImportErrors;PandasImportErrors Type;DataFrameCompiled;DataFrameTotal;DataFrameErrors;DataFrameErrors Type
0;0;0;0;2.0;7;7776679;;1;25;<python><pandas>;append two data frame with pandas;25479.0;['bigdata = data1.append(data2)\nException: Index cannot contain duplicate values!\n    meta  particle  ratio   area    type    \n0   2     part10    1.348   0.8365  touching\n1   2     part18    1.558   0.8244  single  \n2   2     part2     1.893   0.894   single  \n3   2     part37    0.6695  1.005   single  \n....clip...\n36  2     part23    1.051   0.8781  single  \n37  2     part3     80.54   0.9714  nuclei  \n38  2     part34    1.071   0.9337  single  \n    meta  particle  ratio    area    type    \n0   3     part10    0.4756   1.025   single  \n1   3     part18    0.04387  1.232   dusts   \n2   3     part2     1.132    0.8927  single  \n...clip...\n46  3     part46    13.71    1.001   nuclei  \n47  3     part3     0.7439   0.9038  single  \n48  3     part34    0.4349   0.9956  single \n'];['bigdata = data1.append(data2)\n', 'Exception: Index cannot contain duplicate values!\n', '    meta  particle  ratio   area    type    \n0   2     part10    1.348   0.8365  touching\n1   2     part18    1.558   0.8244  single  \n2   2     part2     1.893   0.894   single  \n3   2     part37    0.6695  1.005   single  \n....clip...\n36  2     part23    1.051   0.8781  single  \n37  2     part3     80.54   0.9714  nuclei  \n38  2     part34    1.071   0.9337  single  \n', '    meta  particle  ratio    area    type    \n0   3     part10    0.4756   1.025   single  \n1   3     part18    0.04387  1.232   dusts   \n2   3     part2     1.132    0.8927  single  \n...clip...\n46  3     part46    13.71    1.001   nuclei  \n47  3     part3     0.7439   0.9038  single  \n48  3     part34    0.4349   0.9956  single \n'];['bigdata = data1.append(data2)\n', 'Exception: Index cannot contain duplicate values!\n', 'data1', '    meta  particle  ratio   area    type    \n0   2     part10    1.348   0.8365  touching\n1   2     part18    1.558   0.8244  single  \n2   2     part2     1.893   0.894   single  \n3   2     part37    0.6695  1.005   single  \n....clip...\n36  2     part23    1.051   0.8781  single  \n37  2     part3     80.54   0.9714  nuclei  \n38  2     part34    1.071   0.9337  single  \n', 'data2', '    meta  particle  ratio    area    type    \n0   3     part10    0.4756   1.025   single  \n1   3     part18    0.04387  1.232   dusts   \n2   3     part2     1.132    0.8927  single  \n...clip...\n46  3     part46    13.71    1.001   nuclei  \n47  3     part3     0.7439   0.9038  single  \n48  3     part34    0.4349   0.9956  single \n'];['bigdata = data1.append(data2)\n'];['bigdata = data1.append(data2)\n'];False;['import pandas as pd\ndata2 = pd.DataFrame()\ndata1 = pd.DataFrame()\nbigdata = data1.append(data2)\n'];True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1;1;1;1;7.0;3;7837722;;1;187;<python><performance><for-loop><pandas>;What is the most efficient way to loop through dataframes with pandas?;195364.0;"[""Date,Open,High,Low,Close,Volume,Adj Close\n2011-10-19,27.37,27.47,27.01,27.13,42880000,27.13\n2011-10-18,26.94,27.40,26.80,27.31,52487900,27.31\n2011-10-17,27.11,27.42,26.85,26.98,39433400,26.98\n2011-10-14,27.31,27.50,27.02,27.27,50947700,27.27\n\n....\n#!/usr/bin/env python\nfrom pandas import *\n\ndf = read_csv('table.csv')\n\nfor i, row in enumerate(df.values):\n    date = df.index[i]\n    open, high, low, close, adjclose = row\n    #now perform analysis on open/close based on date, etc..\n""]";"['Date,Open,High,Low,Close,Volume,Adj Close\n2011-10-19,27.37,27.47,27.01,27.13,42880000,27.13\n2011-10-18,26.94,27.40,26.80,27.31,52487900,27.31\n2011-10-17,27.11,27.42,26.85,26.98,39433400,26.98\n2011-10-14,27.31,27.50,27.02,27.27,50947700,27.27\n\n....\n', ""#!/usr/bin/env python\nfrom pandas import *\n\ndf = read_csv('table.csv')\n\nfor i, row in enumerate(df.values):\n    date = df.index[i]\n    open, high, low, close, adjclose = row\n    #now perform analysis on open/close based on date, etc..\n""]";"['Date,Open,High,Low,Close,Volume,Adj Close\n2011-10-19,27.37,27.47,27.01,27.13,42880000,27.13\n2011-10-18,26.94,27.40,26.80,27.31,52487900,27.31\n2011-10-17,27.11,27.42,26.85,26.98,39433400,26.98\n2011-10-14,27.31,27.50,27.02,27.27,50947700,27.27\n\n....\n', ""#!/usr/bin/env python\nfrom pandas import *\n\ndf = read_csv('table.csv')\n\nfor i, row in enumerate(df.values):\n    date = df.index[i]\n    open, high, low, close, adjclose = row\n    #now perform analysis on open/close based on date, etc..\n"", 'df.iteritems']";"[""2011-10-19,27.37,27.47,27.01,27.13,42880000,27.13\n2011-10-18,26.94,27.40,26.80,27.31,52487900,27.31\n2011-10-17,27.11,27.42,26.85,26.98,39433400,26.98\n2011-10-14,27.31,27.50,27.02,27.27,50947700,27.27\n\n#!/usr/bin/env python\nfrom pandas import *\n\ndf = read_csv('table.csv')\n\n    #now perform analysis on open/close based on date, etc..\n""]";"[""2011-10-19,27.37,27.47,27.01,27.13,42880000,27.13\n2011-10-18,26.94,27.40,26.80,27.31,52487900,27.31\n2011-10-17,27.11,27.42,26.85,26.98,39433400,26.98\n2011-10-14,27.31,27.50,27.02,27.27,50947700,27.27\n\n#!/usr/bin/env python\nfrom pandas import *\n\ndf = read_csv('table.csv')\n\n    #now perform analysis on open/close based on date, etc..\n""]";False;"[""import pandas as pd\n2011-10-19,27.37,27.47,27.01,27.13,42880000,27.13\n2011-10-18,26.94,27.40,26.80,27.31,52487900,27.31\n2011-10-17,27.11,27.42,26.85,26.98,39433400,26.98\n2011-10-14,27.31,27.50,27.02,27.27,50947700,27.27\n\n#!/usr/bin/env python\nfrom pandas import *\n\ndf = read_csv('table.csv')\n\n    #now perform analysis on open/close based on date, etc..\n""]";False;4;6;"[""name 'close' is not defined"", 'Sucess', 'Sucess', 'Sucess', 'Sucess', ""name 'pd' is not defined""]";['NameError', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'NameError'];4;6;"[""name 'close' is not defined"", 'Sucess', 'Sucess', 'Sucess', 'Sucess', ""name 'time' is not defined""]";['NameError', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'NameError'];4;6;"[""name 'close' is not defined"", 'Sucess', 'Sucess', 'Sucess', 'Sucess', ""name 'time' is not defined""]";['NameError', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'NameError']
2;2;2;2;2.0;0;8916302;;1;26;<python><csv><numpy><tab-delimited><pandas>;selecting across multiple columns with python pandas?;19805.0;"['df_greater_than10 = df[df[""colA""] > 10]\n']";"['df_greater_than10 = df[df[""colA""] > 10]\n']";"['df', 'pandas.read_table', 'colA', 'df_greater_than10 = df[df[""colA""] > 10]\n', 'df', 'colA', 'colB']";"['df_greater_than10 = df[df[""colA""] > 10]\n']";"['df_greater_than10 = df[df[""colA""] > 10]\n']";False;"['import pandas as pd\ndf = pd.DataFrame()\ndf_greater_than10 = df[df[""colA""] > 10]\n']";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'A'""]";['KeyError']
3;3;3;3;3.0;16;8991709;;1;134;<python><r><join><data.table><pandas>;Why are pandas merges in python faster than data.table merges in R?;16173.0;[''];[];['pandas', 'data.table', 'data.table', 'merge(X, Y, all=FALSE)', 'merge(X, Y, all=TRUE)'];[''];[''];False;['import pandas as pd\n'];False;3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess']
4;4;4;4;8.0;5;9555635;;1;19;<python><numpy><scipy><enthought><pandas>;Open source Enthought Python alternative;6585.0;[''];[];['numpy', 'scipy', 'sci-kits'];[''];[''];False;['import pandas as pd\n'];False;6;6;['Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess'];6;6;['Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess'];6;6;['Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess']
5;5;5;5;2.0;2;9588331;;1;21;<python><pandas>;Simple cross-tabulation in pandas;9493.0;['AB,100.00\nAB,200.00\nAC,150.00\nAD,500.00\nAB,500.00\nAC,250.00\nAX,900.00\nAB,300.00,2\nAC,150.00,1\nAD,500.00,1\nAB,500.00,1\nAC,250.00,1\nAX,900.00,1\n'];['AB,100.00\nAB,200.00\nAC,150.00\nAD,500.00\n', 'AB,500.00\nAC,250.00\nAX,900.00\n', 'AB,300.00,2\nAC,150.00,1\nAD,500.00,1\n', 'AB,500.00,1\nAC,250.00,1\nAX,900.00,1\n'];['pivot/crosstab/indexing', 'Panel', 'DataFrames', 'AB,100.00\nAB,200.00\nAC,150.00\nAD,500.00\n', 'AB,500.00\nAC,250.00\nAX,900.00\n', 'AB,300.00,2\nAC,150.00,1\nAD,500.00,1\n', 'AB,500.00,1\nAC,250.00,1\nAX,900.00,1\n', 'pivot/crosstab/groupby/an index'];['AB,100.00\nAB,200.00\nAC,150.00\nAD,500.00\nAB,500.00\nAC,250.00\nAX,900.00\nAB,300.00,2\nAC,150.00,1\nAD,500.00,1\nAB,500.00,1\nAC,250.00,1\nAX,900.00,1\n'];['AB,100.00\nAB,200.00\nAC,150.00\nAD,500.00\nAB,500.00\nAC,250.00\nAX,900.00\nAB,300.00,2\nAC,150.00,1\nAD,500.00,1\nAB,500.00,1\nAC,250.00,1\nAX,900.00,1\n'];False;['import pandas as pd\nAB,100.00\nAB,200.00\nAC,150.00\nAD,500.00\nAB,500.00\nAC,250.00\nAX,900.00\nAB,300.00,2\nAC,150.00,1\nAD,500.00,1\nAB,500.00,1\nAC,250.00,1\nAX,900.00,1\n'];False;0;2;"[""name 'category' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'category' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'category' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError']
6;6;6;6;3.0;0;9652832;;1;38;<python><pandas><tsv>;How to I load a tsv file into a Pandas DataFrame?;32040.0;"['>>> df1 = DataFrame(csv.reader(open(\'c:/~/trainSetRel3.txt\'), delimiter=\'\\t\'))\n\nTraceback (most recent call last):\n  File ""<pyshell#28>"", line 1, in <module>\n    df1 = DataFrame(csv.reader(open(\'c:/~/trainSetRel3.txt\'), delimiter=\'\\t\'))\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 318, in __init__\n    raise PandasError(\'DataFrame constructor not properly called!\')\nPandasError: DataFrame constructor not properly called!\n']";"['>>> df1 = DataFrame(csv.reader(open(\'c:/~/trainSetRel3.txt\'), delimiter=\'\\t\'))\n\nTraceback (most recent call last):\n  File ""<pyshell#28>"", line 1, in <module>\n    df1 = DataFrame(csv.reader(open(\'c:/~/trainSetRel3.txt\'), delimiter=\'\\t\'))\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 318, in __init__\n    raise PandasError(\'DataFrame constructor not properly called!\')\nPandasError: DataFrame constructor not properly called!\n']";"['tsv', 'DataFrame', '>>> df1 = DataFrame(csv.reader(open(\'c:/~/trainSetRel3.txt\'), delimiter=\'\\t\'))\n\nTraceback (most recent call last):\n  File ""<pyshell#28>"", line 1, in <module>\n    df1 = DataFrame(csv.reader(open(\'c:/~/trainSetRel3.txt\'), delimiter=\'\\t\'))\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 318, in __init__\n    raise PandasError(\'DataFrame constructor not properly called!\')\nPandasError: DataFrame constructor not properly called!\n']";['\n'];['\n'];False;['import pandas as pd\n\n'];False;2;3;"[""name 'DataFrame' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess'];2;3;"[""name 'DataFrame' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess'];2;3;"[""name 'DataFrame' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess']
7;7;7;7;6.0;0;9758450;;1;34;<python><pandas>;Pandas convert dataframe to array of tuples;36433.0;['In [182]: data_set\nOut[182]: \n  index data_date   data_1  data_2\n0  14303 2012-02-17  24.75   25.03 \n1  12009 2012-02-16  25.00   25.07 \n2  11830 2012-02-15  24.99   25.15 \n3  6274  2012-02-14  24.68   25.05 \n4  2302  2012-02-13  24.62   24.77 \n5  14085 2012-02-10  24.38   24.61 \n[(datetime.date(2012,2,17),24.75,25.03),\n(datetime.date(2012,2,16),25.00,25.07),\n...etc. ]\n'];['In [182]: data_set\nOut[182]: \n  index data_date   data_1  data_2\n0  14303 2012-02-17  24.75   25.03 \n1  12009 2012-02-16  25.00   25.07 \n2  11830 2012-02-15  24.99   25.15 \n3  6274  2012-02-14  24.68   25.05 \n4  2302  2012-02-13  24.62   24.77 \n5  14085 2012-02-10  24.38   24.61 \n', '[(datetime.date(2012,2,17),24.75,25.03),\n(datetime.date(2012,2,16),25.00,25.07),\n...etc. ]\n'];['In [182]: data_set\nOut[182]: \n  index data_date   data_1  data_2\n0  14303 2012-02-17  24.75   25.03 \n1  12009 2012-02-16  25.00   25.07 \n2  11830 2012-02-15  24.99   25.15 \n3  6274  2012-02-14  24.68   25.05 \n4  2302  2012-02-13  24.62   24.77 \n5  14085 2012-02-10  24.38   24.61 \n', '[(datetime.date(2012,2,17),24.75,25.03),\n(datetime.date(2012,2,16),25.00,25.07),\n...etc. ]\n'];['data_set\n'];['data_set\n'];False;['import pandas as pd\ndata = pd.DataFrame()\ndata_set\n'];True;0;3;"[""name 'data_set' is not defined"", ""name 'data_set' is not defined"", ""name 'data_set' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'data_set' is not defined"", ""name 'data_set' is not defined"", ""name 'data_set' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'data_set' is not defined"", ""name 'data_set' is not defined"", ""name 'data_set' is not defined""]";['NameError', 'NameError', 'NameError']
8;8;8;8;2.0;0;9762935;;1;11;<python><pandas>;Add indexed column to DataFrame with pandas;8189.0;['df\n\n                            A         B\n2010-01-01 00:00:00  0.340717  0.702432\n2010-01-01 01:00:00  0.649970  0.411799\n2010-01-01 02:00:00  0.932367  0.108047\n2010-01-01 03:00:00  0.051942  0.526318\n2010-01-01 04:00:00  0.518301  0.057809\n2010-01-01 05:00:00  0.779988  0.756221\n2010-01-01 06:00:00  0.597444  0.312495\ndf2\n\n                     C\n2010-01-01 03:00:00  5\n2010-01-01 04:00:00  5\n2010-01-01 05:00:00  5\n                            A         B    C\n2010-01-01 00:00:00  0.340717  0.702432    nan\n2010-01-01 01:00:00  0.649970  0.411799    nan\n2010-01-01 02:00:00  0.932367  0.108047    nan\n2010-01-01 03:00:00  0.051942  0.526318    5\n2010-01-01 04:00:00  0.518301  0.057809    5\n2010-01-01 05:00:00  0.779988  0.756221    5\n2010-01-01 06:00:00  0.597444  0.312495    nan\n'];['df\n\n                            A         B\n2010-01-01 00:00:00  0.340717  0.702432\n2010-01-01 01:00:00  0.649970  0.411799\n2010-01-01 02:00:00  0.932367  0.108047\n2010-01-01 03:00:00  0.051942  0.526318\n2010-01-01 04:00:00  0.518301  0.057809\n2010-01-01 05:00:00  0.779988  0.756221\n2010-01-01 06:00:00  0.597444  0.312495\n', 'df2\n\n                     C\n2010-01-01 03:00:00  5\n2010-01-01 04:00:00  5\n2010-01-01 05:00:00  5\n', '                            A         B    C\n2010-01-01 00:00:00  0.340717  0.702432    nan\n2010-01-01 01:00:00  0.649970  0.411799    nan\n2010-01-01 02:00:00  0.932367  0.108047    nan\n2010-01-01 03:00:00  0.051942  0.526318    5\n2010-01-01 04:00:00  0.518301  0.057809    5\n2010-01-01 05:00:00  0.779988  0.756221    5\n2010-01-01 06:00:00  0.597444  0.312495    nan\n'];['df\n\n                            A         B\n2010-01-01 00:00:00  0.340717  0.702432\n2010-01-01 01:00:00  0.649970  0.411799\n2010-01-01 02:00:00  0.932367  0.108047\n2010-01-01 03:00:00  0.051942  0.526318\n2010-01-01 04:00:00  0.518301  0.057809\n2010-01-01 05:00:00  0.779988  0.756221\n2010-01-01 06:00:00  0.597444  0.312495\n', 'df2\n\n                     C\n2010-01-01 03:00:00  5\n2010-01-01 04:00:00  5\n2010-01-01 05:00:00  5\n', '                            A         B    C\n2010-01-01 00:00:00  0.340717  0.702432    nan\n2010-01-01 01:00:00  0.649970  0.411799    nan\n2010-01-01 02:00:00  0.932367  0.108047    nan\n2010-01-01 03:00:00  0.051942  0.526318    5\n2010-01-01 04:00:00  0.518301  0.057809    5\n2010-01-01 05:00:00  0.779988  0.756221    5\n2010-01-01 06:00:00  0.597444  0.312495    nan\n'];['df\n\ndf2\n\n'];['df\n\ndf2\n\n'];False;['import pandas as pd\ndf2 = pd.DataFrame()\ndf = pd.DataFrame()\ndf\n\ndf2\n\n'];True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
9;9;9;9;2.0;0;9787853;;1;17;<python><pandas>;join or merge with overwrite in pandas;6054.0;[''];[];['df1', 'df2', 'df2', 'df2', 'df1', 'df1', 'df2'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df2' is not defined""]";['NameError'];0;1;"[""name 'df2' is not defined""]";['NameError'];0;1;"[""'DataFrame' object has no attribute 'comb'""]";['AttributeError']
10;10;10;10;6.0;0;9850954;;1;11;<python><pandas>;pandas - get most recent value of a particular column indexed by another column (get maximum value of a particular column indexed by another column);3543.0;"[""   obj_id   data_date   value\n0  4        2011-11-01  59500    \n1  2        2011-10-01  35200 \n2  4        2010-07-31  24860   \n3  1        2009-07-28  15860\n4  2        2008-10-15  200200\nrow_arr = []\nfor grp, grp_df in df.groupby('obj_id'):\n    row_arr.append(dfg.sort('data_date', ascending = False)[:1].values[0])\n\ndf_new = DataFrame(row_arr, columns = ('obj_id', 'data_date', 'value'))\n""]";"['   obj_id   data_date   value\n0  4        2011-11-01  59500    \n1  2        2011-10-01  35200 \n2  4        2010-07-31  24860   \n3  1        2009-07-28  15860\n4  2        2008-10-15  200200\n', ""row_arr = []\nfor grp, grp_df in df.groupby('obj_id'):\n    row_arr.append(dfg.sort('data_date', ascending = False)[:1].values[0])\n\ndf_new = DataFrame(row_arr, columns = ('obj_id', 'data_date', 'value'))\n""]";"['   obj_id   data_date   value\n0  4        2011-11-01  59500    \n1  2        2011-10-01  35200 \n2  4        2010-07-31  24860   \n3  1        2009-07-28  15860\n4  2        2008-10-15  200200\n', ""'data_date'"", ""'value'"", ""'obj_id'"", ""row_arr = []\nfor grp, grp_df in df.groupby('obj_id'):\n    row_arr.append(dfg.sort('data_date', ascending = False)[:1].values[0])\n\ndf_new = DataFrame(row_arr, columns = ('obj_id', 'data_date', 'value'))\n""]";"[""row_arr = []\n\ndf_new = DataFrame(row_arr, columns = ('obj_id', 'data_date', 'value'))\n""]";"[""from pandas import DataFrame\nrow_arr = []\n\ndf_new = DataFrame(row_arr, columns = ('obj_id', 'data_date', 'value'))\n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\nrow_arr = []\n\ndf_new = DataFrame(row_arr, columns = ('obj_id', 'data_date', 'value'))\n""]";True;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"['by argument to sort_index is deprecated, please use .sort_values(by=...)', ""'obj_id'""]";['FutureWarning', 'KeyError']
11;11;11;11;4.0;2;10020591;;1;19;<python><numpy><time-series><pandas>;How to resample a dataframe with different functions applied to each column?;18532.0;"[""import datetime\nimport pandas as pd\nimport numpy as np\n\ndate_times = pd.date_range(datetime.datetime(2012, 4, 5, 8, 0),\n                           datetime.datetime(2012, 4, 5, 12, 0),\n                           freq='1min')\ntamb = np.random.sample(date_times.size) * 10.0\nradiation = np.random.sample(date_times.size) * 10.0\nframe = pd.DataFrame(data={'tamb': tamb, 'radiation': radiation},\n                     index=date_times)\nframe\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 241 entries, 2012-04-05 08:00:00 to 2012-04-05 12:00:00\nFreq: T\nData columns:\nradiation    241  non-null values\ntamb         241  non-null values\ndtypes: float64(2)\n""]";"[""import datetime\nimport pandas as pd\nimport numpy as np\n\ndate_times = pd.date_range(datetime.datetime(2012, 4, 5, 8, 0),\n                           datetime.datetime(2012, 4, 5, 12, 0),\n                           freq='1min')\ntamb = np.random.sample(date_times.size) * 10.0\nradiation = np.random.sample(date_times.size) * 10.0\nframe = pd.DataFrame(data={'tamb': tamb, 'radiation': radiation},\n                     index=date_times)\nframe\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 241 entries, 2012-04-05 08:00:00 to 2012-04-05 12:00:00\nFreq: T\nData columns:\nradiation    241  non-null values\ntamb         241  non-null values\ndtypes: float64(2)\n""]";"['dataframe', ""import datetime\nimport pandas as pd\nimport numpy as np\n\ndate_times = pd.date_range(datetime.datetime(2012, 4, 5, 8, 0),\n                           datetime.datetime(2012, 4, 5, 12, 0),\n                           freq='1min')\ntamb = np.random.sample(date_times.size) * 10.0\nradiation = np.random.sample(date_times.size) * 10.0\nframe = pd.DataFrame(data={'tamb': tamb, 'radiation': radiation},\n                     index=date_times)\nframe\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 241 entries, 2012-04-05 08:00:00 to 2012-04-05 12:00:00\nFreq: T\nData columns:\nradiation    241  non-null values\ntamb         241  non-null values\ndtypes: float64(2)\n"", 'dataframe']";['import datetime\nimport pandas as pd\nimport numpy as np\n\ntamb = np.random.sample(date_times.size) * 10.0\nradiation = np.random.sample(date_times.size) * 10.0\nframe\n'];['import datetime\nimport pandas as pd\nimport numpy as np\n\ntamb = np.random.sample(date_times.size) * 10.0\nradiation = np.random.sample(date_times.size) * 10.0\nframe\n'];False;['import pandas as pd\nimport datetime\nimport pandas as pd\nimport numpy as np\n\ntamb = np.random.sample(date_times.size) * 10.0\nradiation = np.random.sample(date_times.size) * 10.0\nframe\n'];False;0;1;"[""name 'frame' is not defined""]";['NameError'];0;1;"[""name 'frame' is not defined""]";['NameError'];0;1;"[""name 'frame' is not defined""]";['NameError']
12;12;12;12;11.0;4;10065051;;1;77;<python><pandas>;python-pandas and databases like mysql;72432.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;6;"[""No module named 'cx_Oracle'"", ""No module named 'pyodbc'"", 'Sucess', ""No module named 'MySQLdb'"", ""No module named 'sqlalchemy'"", ""No module named 'sqlalchemy'""]";['ImportError', 'ImportError', 'Sucess', 'ImportError', 'ImportError', 'ImportError'];1;6;"[""No module named 'cx_Oracle'"", ""No module named 'pyodbc'"", 'Sucess', ""No module named 'MySQLdb'"", ""No module named 'sqlalchemy'"", ""No module named 'sqlalchemy'""]";['ImportError', 'ImportError', 'Sucess', 'ImportError', 'ImportError', 'ImportError'];1;6;"[""No module named 'cx_Oracle'"", ""No module named 'pyodbc'"", 'Sucess', ""No module named 'MySQLdb'"", ""No module named 'sqlalchemy'"", ""No module named 'sqlalchemy'""]";['ImportError', 'ImportError', 'Sucess', 'ImportError', 'ImportError', 'ImportError']
13;13;13;13;1.0;0;10114399;;1;12;<pandas>;Pandas: simple 'join' not working?;4120.0;"[""In [2]: left = pandas.DataFrame({'ST_NAME': ['Oregon', 'Nebraska'], 'value': [4.685, 2.491]})\n\nIn [3]: right = pandas.DataFrame({'ST_NAME': ['Oregon', 'Nebraska'], 'value2': [6.218, 0.001]})\n\nIn [4]: left.join(right, on='ST_NAME', lsuffix='_left', rsuffix='_right')\nOut[4]: \n  ST_NAME_left  value ST_NAME_right  value2\n0       Oregon  4.685           NaN     NaN\n1     Nebraska  2.491           NaN     NaN\n""]";"[""In [2]: left = pandas.DataFrame({'ST_NAME': ['Oregon', 'Nebraska'], 'value': [4.685, 2.491]})\n\nIn [3]: right = pandas.DataFrame({'ST_NAME': ['Oregon', 'Nebraska'], 'value2': [6.218, 0.001]})\n\nIn [4]: left.join(right, on='ST_NAME', lsuffix='_left', rsuffix='_right')\nOut[4]: \n  ST_NAME_left  value ST_NAME_right  value2\n0       Oregon  4.685           NaN     NaN\n1     Nebraska  2.491           NaN     NaN\n""]";"['pandas', 'DataFrames', ""In [2]: left = pandas.DataFrame({'ST_NAME': ['Oregon', 'Nebraska'], 'value': [4.685, 2.491]})\n\nIn [3]: right = pandas.DataFrame({'ST_NAME': ['Oregon', 'Nebraska'], 'value2': [6.218, 0.001]})\n\nIn [4]: left.join(right, on='ST_NAME', lsuffix='_left', rsuffix='_right')\nOut[4]: \n  ST_NAME_left  value ST_NAME_right  value2\n0       Oregon  4.685           NaN     NaN\n1     Nebraska  2.491           NaN     NaN\n""]";"[""left = pandas.DataFrame({'ST_NAME': ['Oregon', 'Nebraska'], 'value': [4.685, 2.491]})\n\n\n""]";"[""left = pandas.DataFrame({'ST_NAME': ['Oregon', 'Nebraska'], 'value': [4.685, 2.491]})\n\n\n""]";False;"[""import pandas as pd\nleft = pandas.DataFrame({'ST_NAME': ['Oregon', 'Nebraska'], 'value': [4.685, 2.491]})\n\n\n""]";False;0;1;"[""name 'right' is not defined""]";['NameError'];0;1;"[""name 'right' is not defined""]";['NameError'];0;1;"[""name 'right' is not defined""]";['NameError']
14;14;14;14;2.0;3;10175068;;1;15;<python><pandas>;Select data at a particular level from a MultiIndex;6774.0;['             H1       H2  \n   Z    A \n0  100  200  0.3112   -0.4197   \n1  100  201  0.2967   0.4893    \n2  100  202  0.3084   -0.4873   \n3  100  203  0.3069   NaN        \n4  101  203  -0.4956  NaN       \ndf.select(lambda x: x[1]==200, axis=0)  \n'];['             H1       H2  \n   Z    A \n0  100  200  0.3112   -0.4197   \n1  100  201  0.2967   0.4893    \n2  100  202  0.3084   -0.4873   \n3  100  203  0.3069   NaN        \n4  101  203  -0.4956  NaN       \n', 'df.select(lambda x: x[1]==200, axis=0)  \n'];"['             H1       H2  \n   Z    A \n0  100  200  0.3112   -0.4197   \n1  100  201  0.2967   0.4893    \n2  100  202  0.3084   -0.4873   \n3  100  203  0.3069   NaN        \n4  101  203  -0.4956  NaN       \n', ""df[:,'A']"", ""df.xs(203,level='A')"", ""TypeError: xs() got an unexpected keyword argument 'level'"", 'df.xs?', 'df.select(lambda x: x[1]==200, axis=0)  \n']";['df.select(lambda x: x[1]==200, axis=0)  \n'];['df.select(lambda x: x[1]==200, axis=0)  \n'];False;['import pandas as pd\ndf = pd.DataFrame()\ndf.select(lambda x: x[1]==200, axis=0)  \n'];True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
15;15;15;15;4.0;2;10202570;;1;84;<python><pandas>;Pandas DataFrame - Find row where values for column is maximal;75652.0;[''];[];['df.max()'];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'dfrm' is not defined"", ""name 'pandas' is not defined"", ""name 's' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'dfrm' is not defined"", ""name 'pandas' is not defined"", ""name 's' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'dfrm' is not defined"", ""name 'pandas' is not defined"", ""name 's' is not defined""]";['NameError', 'NameError', 'NameError']
16;16;16;16;6.0;0;10373660;;1;173;<python><pandas><dataframe><group-by><multi-index>;Converting a Pandas GroupBy object to DataFrame;167415.0;"['df1 = pandas.DataFrame( { \n    ""Name"" : [""Alice"", ""Bob"", ""Mallory"", ""Mallory"", ""Bob"" , ""Mallory""] , \n    ""City"" : [""Seattle"", ""Seattle"", ""Portland"", ""Seattle"", ""Seattle"", ""Portland""] } )\n   City     Name\n0   Seattle    Alice\n1   Seattle      Bob\n2  Portland  Mallory\n3   Seattle  Mallory\n4   Seattle      Bob\n5  Portland  Mallory\ng1 = df1.groupby( [ ""Name"", ""City""] ).count()\n                  City  Name\nName    City\nAlice   Seattle      1     1\nBob     Seattle      2     2\nMallory Portland     2     2\n        Seattle      1     1\n                  City  Name\nName    City\nAlice   Seattle      1     1\nBob     Seattle      2     2\nMallory Portland     2     2\nMallory Seattle      1     1\n']";"['df1 = pandas.DataFrame( { \n    ""Name"" : [""Alice"", ""Bob"", ""Mallory"", ""Mallory"", ""Bob"" , ""Mallory""] , \n    ""City"" : [""Seattle"", ""Seattle"", ""Portland"", ""Seattle"", ""Seattle"", ""Portland""] } )\n', '   City     Name\n0   Seattle    Alice\n1   Seattle      Bob\n2  Portland  Mallory\n3   Seattle  Mallory\n4   Seattle      Bob\n5  Portland  Mallory\n', 'g1 = df1.groupby( [ ""Name"", ""City""] ).count()\n', '                  City  Name\nName    City\nAlice   Seattle      1     1\nBob     Seattle      2     2\nMallory Portland     2     2\n        Seattle      1     1\n', '                  City  Name\nName    City\nAlice   Seattle      1     1\nBob     Seattle      2     2\nMallory Portland     2     2\nMallory Seattle      1     1\n']";"['df1 = pandas.DataFrame( { \n    ""Name"" : [""Alice"", ""Bob"", ""Mallory"", ""Mallory"", ""Bob"" , ""Mallory""] , \n    ""City"" : [""Seattle"", ""Seattle"", ""Portland"", ""Seattle"", ""Seattle"", ""Portland""] } )\n', '   City     Name\n0   Seattle    Alice\n1   Seattle      Bob\n2  Portland  Mallory\n3   Seattle  Mallory\n4   Seattle      Bob\n5  Portland  Mallory\n', 'g1 = df1.groupby( [ ""Name"", ""City""] ).count()\n', 'GroupBy', '                  City  Name\nName    City\nAlice   Seattle      1     1\nBob     Seattle      2     2\nMallory Portland     2     2\n        Seattle      1     1\n', '                  City  Name\nName    City\nAlice   Seattle      1     1\nBob     Seattle      2     2\nMallory Portland     2     2\nMallory Seattle      1     1\n']";"['g1 = df1.groupby( [ ""Name"", ""City""] ).count()\n']";"['g1 = df1.groupby( [ ""Name"", ""City""] ).count()\n']";False;"['import pandas as pd\ndf1 = pd.DataFrame()\ndf = pd.DataFrame()\ng1 = df1.groupby( [ ""Name"", ""City""] ).count()\n']";True;0;2;"[""name 'g1' is not defined"", ""name 'df1' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'g1' is not defined"", ""name 'df1' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'g1' is not defined"", ""'Name'""]";['NameError', 'KeyError']
17;17;17;17;2.0;14;10376647;;1;12;<python><numpy><pandas>;Installed Python Modules - Python can't find them;25583.0;"['sudo pip install numpy\n\nsudo pip install pandas\nSuccessfully installed pandas\nCleaning up...\nimport pandas\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nImportError: No module named pandas\n']";"['sudo pip install numpy\n\nsudo pip install pandas\n', 'Successfully installed pandas\nCleaning up...\n', 'import pandas\n', 'Traceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nImportError: No module named pandas\n']";"['python', 'pip', 'numpy', 'pandas', 'sudo pip install numpy\n\nsudo pip install pandas\n', 'pandas', 'Successfully installed pandas\nCleaning up...\n', 'pip', 'numpy', 'python', 'import pandas\n', 'Traceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nImportError: No module named pandas\n', 'numpy']";['\nimport pandas\n'];['\nimport pandas\n'];False;['import pandas as pd\n\nimport pandas\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
18;18;18;18;2.0;1;10457584;;1;68;<python><pandas>;Redefining the Index in a Pandas DataFrame object;107691.0;"[""From:\n            a   b   c\n        0   1   2   3\n        1  10  11  12\n        2  20  21  22\n\nTo :\n           b   c\n       1   2   3\n      10  11  12\n      20  21  22\n>>> col = ['a','b','c']\n>>> data = DataFrame([[1,2,3],[10,11,12],[20,21,22]],columns=col)\n>>> data\n    a   b   c\n0   1   2   3\n1  10  11  12\n2  20  21  22\n>>> idx2 = data.a.values\n>>> idx2\narray([ 1, 10, 20], dtype=int64)\n>>> data2 = DataFrame(data,index=idx2,columns=col[1:])\n>>> data2\n     b   c\n1   11  12\n10 NaN NaN\n20 NaN NaN\n""]";"['From:\n            a   b   c\n        0   1   2   3\n        1  10  11  12\n        2  20  21  22\n\nTo :\n           b   c\n       1   2   3\n      10  11  12\n      20  21  22\n', "">>> col = ['a','b','c']\n>>> data = DataFrame([[1,2,3],[10,11,12],[20,21,22]],columns=col)\n>>> data\n    a   b   c\n0   1   2   3\n1  10  11  12\n2  20  21  22\n>>> idx2 = data.a.values\n>>> idx2\narray([ 1, 10, 20], dtype=int64)\n>>> data2 = DataFrame(data,index=idx2,columns=col[1:])\n>>> data2\n     b   c\n1   11  12\n10 NaN NaN\n20 NaN NaN\n""]";"['DataFrame', 'From:\n            a   b   c\n        0   1   2   3\n        1  10  11  12\n        2  20  21  22\n\nTo :\n           b   c\n       1   2   3\n      10  11  12\n      20  21  22\n', "">>> col = ['a','b','c']\n>>> data = DataFrame([[1,2,3],[10,11,12],[20,21,22]],columns=col)\n>>> data\n    a   b   c\n0   1   2   3\n1  10  11  12\n2  20  21  22\n>>> idx2 = data.a.values\n>>> idx2\narray([ 1, 10, 20], dtype=int64)\n>>> data2 = DataFrame(data,index=idx2,columns=col[1:])\n>>> data2\n     b   c\n1   11  12\n10 NaN NaN\n20 NaN NaN\n""]";['\narray([ 1, 10, 20], dtype=int64)\n'];['\narray([ 1, 10, 20], dtype=int64)\n'];False;['import pandas as pd\n\narray([ 1, 10, 20], dtype=int64)\n'];False;0;1;"[""name 'a' is not defined""]";['NameError'];0;1;"[""name 'a' is not defined""]";['NameError'];0;1;"[""name 'a' is not defined""]";['NameError']
19;19;19;19;2.0;0;10464738;;1;23;<python><pandas>;Interpolation on DataFrame in pandas;15524.0;[''];[];"['reindex', 'NaN', ""fillna(method='pad')""]";[''];[''];False;['import pandas as pd\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
20;20;20;20;6.0;3;10511024;;1;64;<python><ipython><pandas>;in Ipython notebook, Pandas is not displying the graph I try to plot;44369.0;['In [7]:\n\npledge.Amount.plot()\n\nOut[7]:\n\n<matplotlib.axes.AxesSubplot at 0x9397c6c>\n'];['In [7]:\n\npledge.Amount.plot()\n\nOut[7]:\n\n<matplotlib.axes.AxesSubplot at 0x9397c6c>\n'];['In [7]:\n\npledge.Amount.plot()\n\nOut[7]:\n\n<matplotlib.axes.AxesSubplot at 0x9397c6c>\n'];['pledge.Amount.plot()\n\n'];['pledge.Amount.plot()\n\n'];False;['import pandas as pd\npledge.Amount.plot()\n\n'];False;1;3;"[""name 'plt' is not defined"", 'Sucess', ""No module named 'matplotlib'""]";['NameError', 'Sucess', 'ImportError'];1;3;"[""name 'plt' is not defined"", 'Sucess', ""No module named 'matplotlib'""]";['NameError', 'Sucess', 'ImportError'];1;3;"[""name 'plt' is not defined"", 'Sucess', ""No module named 'matplotlib'""]";['NameError', 'Sucess', 'ImportError']
21;21;21;21;3.0;0;10565282;;1;16;<python><indexing><time-series><pandas>;pandas, python - how to select specific times in timeseries;16624.0;['SamplingRateMinutes=60\nindex = DateRange(initialTime,finalTime, offset=datetools.Minute(SamplingRateMinutes))\nts=DataFrame(data, index=index)\n selectedData=ts[begin:end]\nmyIndex=ts.index[10<=ts.index.hour<=13 or 20<=ts.index.hour<=23]\nselectedData=ts[myIndex]\n'];['SamplingRateMinutes=60\nindex = DateRange(initialTime,finalTime, offset=datetools.Minute(SamplingRateMinutes))\nts=DataFrame(data, index=index)\n', ' selectedData=ts[begin:end]\n', 'myIndex=ts.index[10<=ts.index.hour<=13 or 20<=ts.index.hour<=23]\nselectedData=ts[myIndex]\n'];['DataFrame', 'SamplingRateMinutes=60\nindex = DateRange(initialTime,finalTime, offset=datetools.Minute(SamplingRateMinutes))\nts=DataFrame(data, index=index)\n', ' selectedData=ts[begin:end]\n', 'myIndex=ts.index[10<=ts.index.hour<=13 or 20<=ts.index.hour<=23]\nselectedData=ts[myIndex]\n'];['SamplingRateMinutes=60\nindex = DateRange(initialTime,finalTime, offset=datetools.Minute(SamplingRateMinutes))\nts=DataFrame(data, index=index)\nmyIndex=ts.index[10<=ts.index.hour<=13 or 20<=ts.index.hour<=23]\nselectedData=ts[myIndex]\n'];['SamplingRateMinutes=60\nindex = DateRange(initialTime,finalTime, offset=datetools.Minute(SamplingRateMinutes))\nts=DataFrame(data, index=index)\nmyIndex=ts.index[10<=ts.index.hour<=13 or 20<=ts.index.hour<=23]\nselectedData=ts[myIndex]\n'];False;['import pandas as pd\ndata = pd.DataFrame()\nSamplingRateMinutes=60\nindex = DateRange(initialTime,finalTime, offset=datetools.Minute(SamplingRateMinutes))\nts=DataFrame(data, index=index)\nmyIndex=ts.index[10<=ts.index.hour<=13 or 20<=ts.index.hour<=23]\nselectedData=ts[myIndex]\n'];True;0;3;"[""name 'hr' is not defined"", ""name 'ts' is not defined"", ""name 'rand' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'hr' is not defined"", ""name 'ts' is not defined"", ""name 'rand' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'hr' is not defined"", ""name 'ts' is not defined"", ""name 'rand' is not defined""]";['NameError', 'NameError', 'NameError']
22;22;22;22;2.0;2;10591000;;1;25;<python><pandas>;Specifying data type in Pandas csv reader;17261.0;[''];[];['read_csv()'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'pandas' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'pandas' is not defined""]";['NameError', 'NameError'];0;2;"[""'DataFrame' object has no attribute 'phone'"", ""name 'pandas' is not defined""]";['AttributeError', 'NameError']
23;23;23;23;14.0;5;10636024;;1;33;<python><user-interface><pandas><dataframe>;Python / Pandas - GUI for viewing a DataFrame or Matrix;26216.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;4;"[""name 'read_csv' is not defined"", ""name 'data_table' is not defined"", ""No module named 'pandas.sandbox'"", 'Sucess']";['NameError', 'NameError', 'ImportError', 'Sucess'];1;4;"[""name 'read_csv' is not defined"", ""name 'data_table' is not defined"", ""No module named 'pandas.sandbox'"", 'Sucess']";['NameError', 'NameError', 'ImportError', 'Sucess'];1;4;"[""name 'read_csv' is not defined"", ""name 'data_table' is not defined"", ""No module named 'pandas.sandbox'"", 'Sucess']";['NameError', 'NameError', 'ImportError', 'Sucess']
24;24;24;24;7.0;3;10665889;;1;126;<python><numpy><pandas><slice>;How to take column-slices of dataframe in pandas;154109.0;"[""data = pandas.read_csv('mydata.csv')\ndata = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))\nobservations = data[:'c']\nfeatures = data['c':]\n""]";"[""data = pandas.read_csv('mydata.csv')\n"", ""data = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))\n"", ""observations = data[:'c']\nfeatures = data['c':]\n""]";"[""data = pandas.read_csv('mydata.csv')\n"", ""data = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))\n"", 'a', 'b', 'c', 'd', 'e', ""observations = data[:'c']\nfeatures = data['c':]\n"", 'pd.Panel', ""data['a']"", 'data[0]', ""data['a':]"", 'data[0:]', 'data[0] != data[0:1]']";"[""data = pandas.read_csv('mydata.csv')\ndata = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))\nobservations = data[:'c']\nfeatures = data['c':]\n""]";"[""data = pandas.read_csv('mydata.csv')\ndata = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))\nobservations = data[:'c']\nfeatures = data['c':]\n""]";False;"[""import pandas as pd\ndata = pandas.read_csv('mydata.csv')\ndata = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))\nobservations = data[:'c']\nfeatures = data['c':]\n""]";False;3;6;"[""name 'pandas' is not defined"", 'Sucess', 'Sucess', ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'Sucess', 'NameError', 'Sucess', 'NameError'];3;6;"[""name 'pandas' is not defined"", 'Sucess', 'Sucess', ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'Sucess', 'NameError', 'Sucess', 'NameError'];4;6;"[""name 'pandas' is not defined"", 'Sucess', 'Sucess', 'Sucess', 'Sucess', '""None of [[\'foo\', \'bar\', \'dat\']] are in the [columns]""']";['NameError', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'KeyError']
25;25;25;25;2.0;0;10715519;;1;19;<dataframe><pandas>;Conditionally fill column values based on another columns value in pandas;14199.0;[''];[];['DataFrame'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
26;26;26;26;13.0;3;10715965;;1;312;<python><pandas>;add one row in a pandas.DataFrame;349935.0;"[""res = DataFrame(columns=('lib', 'qty1', 'qty2'))\nres = res.set_value(len(res), 'qty1', 10.0)\n""]";"[""res = DataFrame(columns=('lib', 'qty1', 'qty2'))\n"", ""res = res.set_value(len(res), 'qty1', 10.0)\n""]";"['DataFrame', ""res = DataFrame(columns=('lib', 'qty1', 'qty2'))\n"", ""res = res.set_value(len(res), 'qty1', 10.0)\n""]";"[""res = DataFrame(columns=('lib', 'qty1', 'qty2'))\nres = res.set_value(len(res), 'qty1', 10.0)\n""]";"[""from pandas import DataFrame\nres = DataFrame(columns=('lib', 'qty1', 'qty2'))\nres = res.set_value(len(res), 'qty1', 10.0)\n""]";True;"[""import pandas as pd\nres = DataFrame(columns=('lib', 'qty1', 'qty2'))\nres = res.set_value(len(res), 'qty1', 10.0)\n""]";False;4;8;"['Sucess', ""name 'pd' is not defined"", ""name 'pd' is not defined"", 'Sucess', 'Sucess', ""name 'pd' is not defined"", 'Sucess', ""name 'BaseData' is not defined""]";['Sucess', 'NameError', 'NameError', 'Sucess', 'Sucess', 'NameError', 'Sucess', 'NameError'];6;8;"['Sucess', 'Sucess', ""name 'dfi' is not defined"", 'Sucess', 'Sucess', 'Sucess', 'Sucess', ""name 'BaseData' is not defined""]";['Sucess', 'Sucess', 'NameError', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'NameError'];6;8;"['Sucess', 'Sucess', ""name 'dfi' is not defined"", 'Sucess', 'Sucess', 'Sucess', 'Sucess', ""name 'BaseData' is not defined""]";['Sucess', 'Sucess', 'NameError', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'NameError']
27;27;27;27;1.0;2;10729210;;1;122;<python><pandas>;iterating row by row through a pandas dataframe;174898.0;['for i in df.index:\n    do_something(df.ix[i])\n'];['for i in df.index:\n    do_something(df.ix[i])\n'];['DataFrame', 'for i in df.index:\n    do_something(df.ix[i])\n'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
28;28;28;28;1.0;2;10751127;;1;41;<python><pandas>;Returning multiple values from pandas apply on a DataFrame;19415.0;"['import numpy\nimport pandas\n\ndf = pandas.DataFrame(numpy.log2(numpy.randn(1000, 4), \n                      columns=[""a"", ""b"", ""c"", ""d""])\n\ndf = df.dropna()\nfrom scipy.stats import ttest_ind\n\ndef t_test_and_mean(series, first, second):\n    first_group = series[first]\n    second_group = series[second]\n    _, pvalue = ttest_ind(first_group, second_group)\n\n    mean_ratio = second_group.mean() / first_group.mean()\n\n    return (pvalue, mean_ratio)\ndf.apply(t_test_and_mean, first=[""a"", ""b""], second=[""c"", ""d""], axis=1)\n']";"['import numpy\nimport pandas\n\ndf = pandas.DataFrame(numpy.log2(numpy.randn(1000, 4), \n                      columns=[""a"", ""b"", ""c"", ""d""])\n\ndf = df.dropna()\n', 'from scipy.stats import ttest_ind\n\ndef t_test_and_mean(series, first, second):\n    first_group = series[first]\n    second_group = series[second]\n    _, pvalue = ttest_ind(first_group, second_group)\n\n    mean_ratio = second_group.mean() / first_group.mean()\n\n    return (pvalue, mean_ratio)\n', 'df.apply(t_test_and_mean, first=[""a"", ""b""], second=[""c"", ""d""], axis=1)\n']";"['DataFrame', 'import numpy\nimport pandas\n\ndf = pandas.DataFrame(numpy.log2(numpy.randn(1000, 4), \n                      columns=[""a"", ""b"", ""c"", ""d""])\n\ndf = df.dropna()\n', 'apply', 'from scipy.stats import ttest_ind\n\ndef t_test_and_mean(series, first, second):\n    first_group = series[first]\n    second_group = series[second]\n    _, pvalue = ttest_ind(first_group, second_group)\n\n    mean_ratio = second_group.mean() / first_group.mean()\n\n    return (pvalue, mean_ratio)\n', 'df.apply(t_test_and_mean, first=[""a"", ""b""], second=[""c"", ""d""], axis=1)\n']";"['import numpy\nimport pandas\n\n\ndf = df.dropna()\nfrom scipy.stats import ttest_ind\n\n\n\ndf.apply(t_test_and_mean, first=[""a"", ""b""], second=[""c"", ""d""], axis=1)\n']";"['import numpy\nimport pandas\n\n\ndf = df.dropna()\nfrom scipy.stats import ttest_ind\n\n\n\ndf.apply(t_test_and_mean, first=[""a"", ""b""], second=[""c"", ""d""], axis=1)\n']";False;"['import pandas as pd\nimport numpy\nimport pandas\n\n\ndf = df.dropna()\nfrom scipy.stats import ttest_ind\n\n\n\ndf.apply(t_test_and_mean, first=[""a"", ""b""], second=[""c"", ""d""], axis=1)\n']";False;0;1;"[""'return' outside function (<ast>, line 1)""]";['SyntaxError'];0;1;"[""'return' outside function (<ast>, line 1)""]";['SyntaxError'];0;1;"[""'return' outside function (<ast>, line 2)""]";['SyntaxError']
29;29;29;29;1.0;1;10857924;;1;39;<python><pandas>;Remove NULL columns in a dataframe Pandas?;33392.0;[''];[];['dataFrame'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'Parameters' is not defined""]";['NameError'];0;1;"[""name 'Parameters' is not defined""]";['NameError'];0;1;"[""name 'Parameters' is not defined""]";['NameError']
30;30;30;30;3.0;0;10867028;;1;23;<python><csv><pandas>;Get pandas.read_csv to read empty values as empty string instead of nan;21259.0;"['One,Two,Three\na,1,one\nb,2,two\n,3,three\nd,4,nan\ne,5,five\nnan,6,\ng,7,seven\n\n>>> pandas.read_csv(\'test.csv\', na_values={\'One\': [], ""Three"": []})\n    One  Two  Three\n0    a    1    one\n1    b    2    two\n2  NaN    3  three\n3    d    4    nan\n4    e    5   five\n5  nan    6    NaN\n6    g    7  seven\n']";"['One,Two,Three\na,1,one\nb,2,two\n,3,three\nd,4,nan\ne,5,five\nnan,6,\ng,7,seven\n\n>>> pandas.read_csv(\'test.csv\', na_values={\'One\': [], ""Three"": []})\n    One  Two  Three\n0    a    1    one\n1    b    2    two\n2  NaN    3  three\n3    d    4    nan\n4    e    5   five\n5  nan    6    NaN\n6    g    7  seven\n']";"['""nan""', 'One,Two,Three\na,1,one\nb,2,two\n,3,three\nd,4,nan\ne,5,five\nnan,6,\ng,7,seven\n\n>>> pandas.read_csv(\'test.csv\', na_values={\'One\': [], ""Three"": []})\n    One  Two  Three\n0    a    1    one\n1    b    2    two\n2  NaN    3  three\n3    d    4    nan\n4    e    5   five\n5  nan    6    NaN\n6    g    7  seven\n', 'str', 'converters', ""converters={'One': str})""]";['One,Two,Three\na,1,one\nb,2,two\nd,4,nan\ne,5,five\nnan,6,\ng,7,seven\n\n'];['One,Two,Three\na,1,one\nb,2,two\nd,4,nan\ne,5,five\nnan,6,\ng,7,seven\n\n'];False;['import pandas as pd\nOne,Two,Three\na,1,one\nb,2,two\nd,4,nan\ne,5,five\nnan,6,\ng,7,seven\n\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
31;31;31;31;1.0;0;10943478;;1;12;<python><dataframe><pandas><reindex>;pandas reindex DataFrame with datetime objects;10727.0;"[""Int64Index: 19610 entries, 0 to 19609\nData columns:\ncntr                  19610  non-null values  #int\ndatflt                19610  non-null values  #float\ndtstamp               19610  non-null values  #datetime object\nDOYtimestamp          19610  non-null values  #float\ndtypes: int64(1), float64(2), object(1)\n>>> df['DOYtimestamp'].values\n    array([ 153.76252315,  153.76253472,  153.7625463 , ...,  153.98945602,\n    153.98946759,  153.98947917])\n >>> df['dtstamp'].values\n     array([2012-06-02 18:18:02, 2012-06-02 18:18:03, 2012-06-02 18:18:04, ...,\n     2012-06-02 23:44:49, 2012-06-02 23:44:50, 2012-06-02 23:44:51], \n     dtype=object)\n>>> df.reindex(index=df.dtstamp)\n    TypeError: can't compare datetime.datetime to long\n""]";"['Int64Index: 19610 entries, 0 to 19609\nData columns:\ncntr                  19610  non-null values  #int\ndatflt                19610  non-null values  #float\ndtstamp               19610  non-null values  #datetime object\nDOYtimestamp          19610  non-null values  #float\ndtypes: int64(1), float64(2), object(1)\n', "">>> df['DOYtimestamp'].values\n    array([ 153.76252315,  153.76253472,  153.7625463 , ...,  153.98945602,\n    153.98946759,  153.98947917])\n"", "" >>> df['dtstamp'].values\n     array([2012-06-02 18:18:02, 2012-06-02 18:18:03, 2012-06-02 18:18:04, ...,\n     2012-06-02 23:44:49, 2012-06-02 23:44:50, 2012-06-02 23:44:51], \n     dtype=object)\n"", "">>> df.reindex(index=df.dtstamp)\n    TypeError: can't compare datetime.datetime to long\n""]";"['DataFrame', 'df', 'Int64Index: 19610 entries, 0 to 19609\nData columns:\ncntr                  19610  non-null values  #int\ndatflt                19610  non-null values  #float\ndtstamp               19610  non-null values  #datetime object\nDOYtimestamp          19610  non-null values  #float\ndtypes: int64(1), float64(2), object(1)\n', 'df', 'DOYtimestamp', 'df.reindex(index=df.dtstamp)', 'DOYtimestamp', "">>> df['DOYtimestamp'].values\n    array([ 153.76252315,  153.76253472,  153.7625463 , ...,  153.98945602,\n    153.98946759,  153.98947917])\n"", 'dtstamp', 'dtstamp', "" >>> df['dtstamp'].values\n     array([2012-06-02 18:18:02, 2012-06-02 18:18:03, 2012-06-02 18:18:04, ...,\n     2012-06-02 23:44:49, 2012-06-02 23:44:50, 2012-06-02 23:44:51], \n     dtype=object)\n"", 'df', 'dtstamp', "">>> df.reindex(index=df.dtstamp)\n    TypeError: can't compare datetime.datetime to long\n""]";[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
32;32;32;32;5.0;0;10951341;;1;45;<python><pandas>;Pandas DataFrame aggregate function using multiple columns;17889.0;"['def wAvg(c, w):\n    return ((c * w).sum() / w.sum())\n\ndf = DataFrame(....) # df has columns c and w, i want weighted average\n                     # of c using w as weight.\ndf.aggregate ({""c"": wAvg}) # and somehow tell it to use w column as weights ...\n']";"['def wAvg(c, w):\n    return ((c * w).sum() / w.sum())\n\ndf = DataFrame(....) # df has columns c and w, i want weighted average\n                     # of c using w as weight.\ndf.aggregate ({""c"": wAvg}) # and somehow tell it to use w column as weights ...\n']";"['DataFrame.agg', 'def wAvg(c, w):\n    return ((c * w).sum() / w.sum())\n\ndf = DataFrame(....) # df has columns c and w, i want weighted average\n                     # of c using w as weight.\ndf.aggregate ({""c"": wAvg}) # and somehow tell it to use w column as weights ...\n']";"['\n                     # of c using w as weight.\ndf.aggregate ({""c"": wAvg}) # and somehow tell it to use w column as weights ...\n']";"['\n                     # of c using w as weight.\ndf.aggregate ({""c"": wAvg}) # and somehow tell it to use w column as weights ...\n']";False;"['import pandas as pd\ndf = pd.DataFrame()\n\n                     # of c using w as weight.\ndf.aggregate ({""c"": wAvg}) # and somehow tell it to use w column as weights ...\n']";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'keys' is not defined""]";['NameError']
33;33;33;33;5.0;0;10972410;;1;18;<python><dataframe><pandas>;pandas: combine two columns in a DataFrame;41826.0;['Index: 239897 entries, 2012-05-11 15:20:00 to 2012-06-02 23:44:51\nData columns:\nfoo                   11516  non-null values\nbar                   228381  non-null values\nTime_UTC              239897  non-null values\ndtstamp               239897  non-null values\ndtypes: float64(4), object(1)\nIndex: 239897 entries, 2012-05-11 15:20:00 to 2012-06-02 23:44:51\nData columns:\nbar                   239897  non-null values\nTime_UTC              239897  non-null values\ndtstamp               239897  non-null values\ndtypes: float64(4), object(1)\n'];['Index: 239897 entries, 2012-05-11 15:20:00 to 2012-06-02 23:44:51\nData columns:\nfoo                   11516  non-null values\nbar                   228381  non-null values\nTime_UTC              239897  non-null values\ndtstamp               239897  non-null values\ndtypes: float64(4), object(1)\n', 'Index: 239897 entries, 2012-05-11 15:20:00 to 2012-06-02 23:44:51\nData columns:\nbar                   239897  non-null values\nTime_UTC              239897  non-null values\ndtstamp               239897  non-null values\ndtypes: float64(4), object(1)\n'];['DataFrame', 'Index: 239897 entries, 2012-05-11 15:20:00 to 2012-06-02 23:44:51\nData columns:\nfoo                   11516  non-null values\nbar                   228381  non-null values\nTime_UTC              239897  non-null values\ndtstamp               239897  non-null values\ndtypes: float64(4), object(1)\n', 'foo', 'bar', 'foo', 'bar', 'bar', 'Index: 239897 entries, 2012-05-11 15:20:00 to 2012-06-02 23:44:51\nData columns:\nbar                   239897  non-null values\nTime_UTC              239897  non-null values\ndtstamp               239897  non-null values\ndtypes: float64(4), object(1)\n', 'foo'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'pandas' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'pandas' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'pandas' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError']
34;34;34;34;3.0;1;10982089;;1;38;<python><pandas><dataframe>;How to shift a column in Pandas DataFrame;31697.0;['##    x1   x2\n##0  206  214\n##1  226  234\n##2  245  253\n##3  265  272\n##4  283  291\n##    x1   x2\n##0  206  nan\n##1  226  214\n##2  245  234\n##3  265  253\n##4  283  272\n##5  nan  291\n'];['##    x1   x2\n##0  206  214\n##1  226  234\n##2  245  253\n##3  265  272\n##4  283  291\n', '##    x1   x2\n##0  206  nan\n##1  226  214\n##2  245  234\n##3  265  253\n##4  283  272\n##5  nan  291\n'];['DataFrame', '##    x1   x2\n##0  206  214\n##1  226  234\n##2  245  253\n##3  265  272\n##4  283  291\n', '##    x1   x2\n##0  206  nan\n##1  226  214\n##2  245  234\n##3  265  253\n##4  283  272\n##5  nan  291\n'];['##    x1   x2\n##0  206  214\n##1  226  234\n##2  245  253\n##3  265  272\n##4  283  291\n##    x1   x2\n##0  206  nan\n##1  226  214\n##2  245  234\n##3  265  253\n##4  283  272\n##5  nan  291\n'];['##    x1   x2\n##0  206  214\n##1  226  234\n##2  245  253\n##3  265  272\n##4  283  291\n##    x1   x2\n##0  206  nan\n##1  226  214\n##2  245  234\n##3  265  253\n##4  283  272\n##5  nan  291\n'];False;['import pandas as pd\n##    x1   x2\n##0  206  214\n##1  226  234\n##2  245  253\n##3  265  272\n##4  283  291\n##    x1   x2\n##0  206  nan\n##1  226  214\n##2  245  234\n##3  265  253\n##4  283  272\n##5  nan  291\n'];False;0;1;"[""name 'a' is not defined""]";['NameError'];0;1;"[""name 'a' is not defined""]";['NameError'];0;1;"[""name 'a' is not defined""]";['NameError']
35;35;35;35;2.0;0;11040626;;1;11;<python><dataframe><pandas>;Pandas DataFrame Add column to index without resetting;9132.0;"[""from pandas import DataFrame\ndf = DataFrame( {'a': range(6), 'b': range(6), 'c': range(6)} )\ndf.set_index(['a','b'], inplace=True)\ndf['d'] = range(6)\n\n# how do I set index to 'a b d' without having to reset it first?\ndf.reset_index(['a','b','d'], inplace=True)\ndf.set_index(['a','b','d'], inplace=True)\n\ndf\n""]";"[""from pandas import DataFrame\ndf = DataFrame( {'a': range(6), 'b': range(6), 'c': range(6)} )\ndf.set_index(['a','b'], inplace=True)\ndf['d'] = range(6)\n\n# how do I set index to 'a b d' without having to reset it first?\ndf.reset_index(['a','b','d'], inplace=True)\ndf.set_index(['a','b','d'], inplace=True)\n\ndf\n""]";"[""'d'"", ""from pandas import DataFrame\ndf = DataFrame( {'a': range(6), 'b': range(6), 'c': range(6)} )\ndf.set_index(['a','b'], inplace=True)\ndf['d'] = range(6)\n\n# how do I set index to 'a b d' without having to reset it first?\ndf.reset_index(['a','b','d'], inplace=True)\ndf.set_index(['a','b','d'], inplace=True)\n\ndf\n""]";"[""from pandas import DataFrame\ndf = DataFrame( {'a': range(6), 'b': range(6), 'c': range(6)} )\ndf.set_index(['a','b'], inplace=True)\ndf['d'] = range(6)\n\n# how do I set index to 'a b d' without having to reset it first?\ndf.reset_index(['a','b','d'], inplace=True)\ndf.set_index(['a','b','d'], inplace=True)\n\ndf\n""]";"[""from pandas import DataFrame\ndf = DataFrame( {'a': range(6), 'b': range(6), 'c': range(6)} )\ndf.set_index(['a','b'], inplace=True)\ndf['d'] = range(6)\n\n# how do I set index to 'a b d' without having to reset it first?\ndf.reset_index(['a','b','d'], inplace=True)\ndf.set_index(['a','b','d'], inplace=True)\n\ndf\n""]";False;"[""import pandas as pd\nfrom pandas import DataFrame\ndf = DataFrame( {'a': range(6), 'b': range(6), 'c': range(6)} )\ndf.set_index(['a','b'], inplace=True)\ndf['d'] = range(6)\n\n# how do I set index to 'a b d' without having to reset it first?\ndf.reset_index(['a','b','d'], inplace=True)\ndf.set_index(['a','b','d'], inplace=True)\n\ndf\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'d'""]";['KeyError']
36;36;36;36;11.0;1;11067027;;1;102;<python><pandas><order>;Python Pandas - Re-ordering columns in a dataframe based on column name;74414.0;"[""['Q1.3','Q6.1','Q1.2','Q1.1',......]\n['Q1.1','Q1.2','Q1.3',.....'Q6.1',......]\n""]";"[""['Q1.3','Q6.1','Q1.2','Q1.1',......]\n"", ""['Q1.1','Q1.2','Q1.3',.....'Q6.1',......]\n""]";"['dataframe', ""['Q1.3','Q6.1','Q1.2','Q1.1',......]\n"", ""['Q1.1','Q1.2','Q1.3',.....'Q6.1',......]\n""]";[''];[''];False;['import pandas as pd\n'];False;1;6;"[""name 'df' is not defined"", 'Sucess', ""name 'data' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'your_dataframe' is not defined""]";['NameError', 'Sucess', 'NameError', 'NameError', 'NameError', 'NameError'];1;6;"[""name 'df' is not defined"", 'Sucess', ""name 'data' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'your_dataframe' is not defined""]";['NameError', 'Sucess', 'NameError', 'NameError', 'NameError', 'NameError'];3;6;"[""'.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead."", 'Sucess', ""name 'data' is not defined"", 'Sucess', 'Sucess', ""name 'your_dataframe' is not defined""]";['FutureWarning', 'Sucess', 'NameError', 'Sucess', 'Sucess', 'NameError']
37;37;37;37;1.0;0;11073609;;1;28;<python><pandas>;How to group DataFrame by a period of time?;20945.0;"[' def gen(date, count=10):\n     while count > 0:\n         yield date, ""event{}"".format(randint(1,9)), ""source{}"".format(randint(1,3))\n         count -= 1\n         date += DateOffset(seconds=randint(40))\n\n df = DataFrame.from_records(list(gen(datetime(2012,1,1,12, 30))), index=\'Time\', columns=[\'Time\', \'Event\', \'Source\'])\n Event  Source\n 2012-01-01 12:30:00     event3  source1\n 2012-01-01 12:30:12     event2  source2\n 2012-01-01 12:30:12     event2  source2\n 2012-01-01 12:30:29     event6  source1\n 2012-01-01 12:30:38     event1  source1\n 2012-01-01 12:31:05     event4  source2\n 2012-01-01 12:31:38     event4  source1\n 2012-01-01 12:31:44     event5  source1\n 2012-01-01 12:31:48     event5  source2\n 2012-01-01 12:32:23     event6  source1\ngrouped = df.groupby(TimeGrouper(freq=\'Min\'))\ngrouped.Source.value_counts()\n2012-01-01 12:30:00  source1    1\n2012-01-01 12:31:00  source2    2\n                     source1    2\n2012-01-01 12:32:00  source2    2\n                     source1    2\n2012-01-01 12:33:00  source1    1\n']";"[' def gen(date, count=10):\n     while count > 0:\n         yield date, ""event{}"".format(randint(1,9)), ""source{}"".format(randint(1,3))\n         count -= 1\n         date += DateOffset(seconds=randint(40))\n\n df = DataFrame.from_records(list(gen(datetime(2012,1,1,12, 30))), index=\'Time\', columns=[\'Time\', \'Event\', \'Source\'])\n', ' Event  Source\n 2012-01-01 12:30:00     event3  source1\n 2012-01-01 12:30:12     event2  source2\n 2012-01-01 12:30:12     event2  source2\n 2012-01-01 12:30:29     event6  source1\n 2012-01-01 12:30:38     event1  source1\n 2012-01-01 12:31:05     event4  source2\n 2012-01-01 12:31:38     event4  source1\n 2012-01-01 12:31:44     event5  source1\n 2012-01-01 12:31:48     event5  source2\n 2012-01-01 12:32:23     event6  source1\n', ""grouped = df.groupby(TimeGrouper(freq='Min'))\ngrouped.Source.value_counts()\n2012-01-01 12:30:00  source1    1\n2012-01-01 12:31:00  source2    2\n                     source1    2\n2012-01-01 12:32:00  source2    2\n                     source1    2\n2012-01-01 12:33:00  source1    1\n""]";"[' def gen(date, count=10):\n     while count > 0:\n         yield date, ""event{}"".format(randint(1,9)), ""source{}"".format(randint(1,3))\n         count -= 1\n         date += DateOffset(seconds=randint(40))\n\n df = DataFrame.from_records(list(gen(datetime(2012,1,1,12, 30))), index=\'Time\', columns=[\'Time\', \'Event\', \'Source\'])\n', ' Event  Source\n 2012-01-01 12:30:00     event3  source1\n 2012-01-01 12:30:12     event2  source2\n 2012-01-01 12:30:12     event2  source2\n 2012-01-01 12:30:29     event6  source1\n 2012-01-01 12:30:38     event1  source1\n 2012-01-01 12:31:05     event4  source2\n 2012-01-01 12:31:38     event4  source1\n 2012-01-01 12:31:44     event5  source1\n 2012-01-01 12:31:48     event5  source2\n 2012-01-01 12:32:23     event6  source1\n', ""df.resample('Min')"", ""df.groupby(date_range(datetime(2012,1,1,12, 30), freq='Min',\nperiods=4))"", ""df.groupby(TimeGrouper(freq='Min'))"", 'DataFrameGroupBy', ""grouped = df.groupby(TimeGrouper(freq='Min'))\ngrouped.Source.value_counts()\n2012-01-01 12:30:00  source1    1\n2012-01-01 12:31:00  source2    2\n                     source1    2\n2012-01-01 12:32:00  source2    2\n                     source1    2\n2012-01-01 12:33:00  source1    1\n"", 'TimeGrouper', ""groupby([TimeGrouper(freq='Min'), df.Source])""]";"[""\ngrouped = df.groupby(TimeGrouper(freq='Min'))\ngrouped.Source.value_counts()\n""]";"[""\ngrouped = df.groupby(TimeGrouper(freq='Min'))\ngrouped.Source.value_counts()\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\n\ngrouped = df.groupby(TimeGrouper(freq='Min'))\ngrouped.Source.value_counts()\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'Source'""]";['KeyError']
38;38;38;38;3.0;3;11077023;;1;106;<python><numpy><scipy><pandas>;What are the differences between Pandas and NumPy+SciPy in Python?;54531.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
39;39;39;39;4.0;1;11106823;;1;26;<python><pandas>;Adding two pandas dataframes;19395.0;[''];[];['dataframes', 'timeseries', 'dataframe', 'dataframe', '.add', 'combined_data = dataframe1 + dataframe2', 'NaN'];[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'x' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'x' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'x' is not defined"", 'Sucess']";['NameError', 'Sucess']
40;40;40;40;4.0;0;11232275;;1;28;<python><pandas>;Pandas pivot warning about repeated entries on index;12221.0;"[""Examples\n--------\n>>> df\n    foo   bar  baz\n0   one   A    1.\n1   one   B    2.\n2   one   C    3.\n3   two   A    4.\n4   two   B    5.\n5   two   C    6.\n\n>>> df.pivot('foo', 'bar', 'baz')\n     A   B   C\none  1   2   3\ntwo  4   5   6\n   name   id     x\n----------------------\n0  john   1      0\n1  john   2      0\n2  mike   1      1\n3  mike   2      0\n      1    2   # (this is the id as columns)\n----------------------\nmike  0    0   # (and this is the 'x' as values)\njohn  1    0\n*** ReshapeError: Index contains duplicate entries, cannot reshape\n""]";"[""Examples\n--------\n>>> df\n    foo   bar  baz\n0   one   A    1.\n1   one   B    2.\n2   one   C    3.\n3   two   A    4.\n4   two   B    5.\n5   two   C    6.\n\n>>> df.pivot('foo', 'bar', 'baz')\n     A   B   C\none  1   2   3\ntwo  4   5   6\n"", '   name   id     x\n----------------------\n0  john   1      0\n1  john   2      0\n2  mike   1      1\n3  mike   2      0\n', ""      1    2   # (this is the id as columns)\n----------------------\nmike  0    0   # (and this is the 'x' as values)\njohn  1    0\n"", '*** ReshapeError: Index contains duplicate entries, cannot reshape\n']";"['pivot', ""Examples\n--------\n>>> df\n    foo   bar  baz\n0   one   A    1.\n1   one   B    2.\n2   one   C    3.\n3   two   A    4.\n4   two   B    5.\n5   two   C    6.\n\n>>> df.pivot('foo', 'bar', 'baz')\n     A   B   C\none  1   2   3\ntwo  4   5   6\n"", 'DataFrame', '   name   id     x\n----------------------\n0  john   1      0\n1  john   2      0\n2  mike   1      1\n3  mike   2      0\n', ""      1    2   # (this is the id as columns)\n----------------------\nmike  0    0   # (and this is the 'x' as values)\njohn  1    0\n"", 'pivot', '*** ReshapeError: Index contains duplicate entries, cannot reshape\n', 'foo', 'name', 'pivot']";['Examples\n\n'];['Examples\n\n'];False;['import pandas as pd\nExamples\n\n'];False;0;2;"[""name 'df' is not defined"", ""name 'pandas' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'pandas' is not defined""]";['NameError', 'NameError'];0;2;"['not enough values to unpack (expected 2, got 0)', ""name 'pandas' is not defined""]";['ValueError', 'NameError']
41;41;41;41;7.0;4;11285613;;1;280;<python><pandas>;Selecting columns;413954.0;"[""index  a   b   c\n1      2   3   4\n2      3   4   5\ndf1 = df['a':'b']\ndf1 = df.ix[:, 'a':'b']\n""]";"['index  a   b   c\n1      2   3   4\n2      3   4   5\n', ""df1 = df['a':'b']\ndf1 = df.ix[:, 'a':'b']\n""]";"['index  a   b   c\n1      2   3   4\n2      3   4   5\n', ""'b'"", ""'c'"", ""df1 = df['a':'b']\ndf1 = df.ix[:, 'a':'b']\n""]";"[""df1 = df['a':'b']\ndf1 = df.ix[:, 'a':'b']\n""]";"[""df1 = df['a':'b']\ndf1 = df.ix[:, 'a':'b']\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf1 = df['a':'b']\ndf1 = df.ix[:, 'a':'b']\n""]";True;0;6;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError', 'NameError', 'NameError'];0;6;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError', 'NameError', 'NameError'];2;6;"['""[\'a\' \'b\'] not in index""', '""[\'b\' \'c\'] not in index""', ""name 'df' is not defined"", ""labels ['a'] not contained in axis"", 'Sucess', 'Sucess']";['KeyError', 'KeyError', 'NameError', 'ValueError', 'Sucess', 'Sucess']
42;42;42;42;16.0;0;11346283;;1;750;<python><pandas><replace><dataframe><rename>;Renaming columns in pandas;767316.0;"[""['$a', '$b', '$c', '$d', '$e'] \n['a', 'b', 'c', 'd', 'e'].\n""]";"[""['$a', '$b', '$c', '$d', '$e'] \n"", ""['a', 'b', 'c', 'd', 'e'].\n""]";"['A', ""['$a', '$b', '$c', '$d', '$e'] \n"", ""['a', 'b', 'c', 'd', 'e'].\n""]";"[""['$a', '$b', '$c', '$d', '$e'] \n""]";"[""['$a', '$b', '$c', '$d', '$e'] \n""]";False;"[""import pandas as pd\n['$a', '$b', '$c', '$d', '$e'] \n""]";False;3;14;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError', 'NameError', 'NameError', 'NameError', 'NameError', 'NameError', 'Sucess', 'NameError', 'NameError', 'NameError', 'Sucess', 'NameError'];3;14;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError', 'NameError', 'NameError', 'NameError', 'NameError', 'NameError', 'Sucess', 'NameError', 'NameError', 'NameError', 'Sucess', 'NameError'];6;14;"['Sucess', ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined"", 'Sucess', ""Can only use .str accessor with string values (i.e. inferred_type is 'string', 'unicode' or 'mixed')"", ""name 'df' is not defined"", 'Sucess', 'Sucess', ""Can only use .str accessor with string values (i.e. inferred_type is 'string', 'unicode' or 'mixed')"", 'Length mismatch: Expected axis has 0 elements, new values have 5 elements', 'Length mismatch: Expected axis has 0 elements, new values have 5 elements', 'Sucess', 'Length mismatch: Expected axis has 0 elements, new values have 2 elements']";['Sucess', 'NameError', 'Sucess', 'NameError', 'Sucess', 'AttributeError', 'NameError', 'Sucess', 'Sucess', 'AttributeError', 'ValueError', 'ValueError', 'Sucess', 'ValueError']
43;43;43;43;2.0;3;11348183;;1;15;<python><legend><pandas>;Pandas bar plot with specific colors and legend location?;23856.0;"['import pylab as pl\nfrom pandas import *\n\nx = DataFrame({""Alpha"": Series({1: 1, 2: 3, 3:2.5}), ""Beta"": Series({1: 2, 2: 2, 3:3.5})})\nx.plot(kind=""bar"")\nx.plot(kind=""bar"", legend=False)\nl = pl.legend((\'Alpha\',\'Beta\'), loc=\'best\')\n']";"['import pylab as pl\nfrom pandas import *\n\nx = DataFrame({""Alpha"": Series({1: 1, 2: 3, 3:2.5}), ""Beta"": Series({1: 2, 2: 2, 3:3.5})})\n', 'x.plot(kind=""bar"")\n', 'x.plot(kind=""bar"", legend=False)\nl = pl.legend((\'Alpha\',\'Beta\'), loc=\'best\')\n']";"['DataFrame', 'import pylab as pl\nfrom pandas import *\n\nx = DataFrame({""Alpha"": Series({1: 1, 2: 3, 3:2.5}), ""Beta"": Series({1: 2, 2: 2, 3:3.5})})\n', 'x.plot(kind=""bar"")\n', 'x.plot(kind=""bar"", legend=False)\nl = pl.legend((\'Alpha\',\'Beta\'), loc=\'best\')\n']";"['import pylab as pl\nfrom pandas import *\n\nx = DataFrame({""Alpha"": Series({1: 1, 2: 3, 3:2.5}), ""Beta"": Series({1: 2, 2: 2, 3:3.5})})\nx.plot(kind=""bar"")\nx.plot(kind=""bar"", legend=False)\nl = pl.legend((\'Alpha\',\'Beta\'), loc=\'best\')\n']";"['from pandas import DataFrame\nimport pylab as pl\nfrom pandas import *\n\nx = DataFrame({""Alpha"": Series({1: 1, 2: 3, 3:2.5}), ""Beta"": Series({1: 2, 2: 2, 3:3.5})})\nx.plot(kind=""bar"")\nx.plot(kind=""bar"", legend=False)\nl = pl.legend((\'Alpha\',\'Beta\'), loc=\'best\')\n']";True;"['import pandas as pd\nimport pylab as pl\nfrom pandas import *\n\nx = DataFrame({""Alpha"": Series({1: 1, 2: 3, 3:2.5}), ""Beta"": Series({1: 2, 2: 2, 3:3.5})})\nx.plot(kind=""bar"")\nx.plot(kind=""bar"", legend=False)\nl = pl.legend((\'Alpha\',\'Beta\'), loc=\'best\')\n']";False;0;2;"[""name 'x' is not defined"", ""name 'x' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'x' is not defined"", ""name 'x' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'x' is not defined"", ""name 'x' is not defined""]";['NameError', 'NameError']
44;44;44;44;6.0;0;11350770;;1;132;<python><pandas>;pandas + dataframe - select by partial string;130134.0;['re.search(pattern, cell_in_question) \n'];['re.search(pattern, cell_in_question) \n'];"['DataFrame', 're.search(pattern, cell_in_question) \n', 'df[df[\'A\'] == ""hello world""]', ""'hello'""]";['re.search(pattern, cell_in_question) \n'];['re.search(pattern, cell_in_question) \n'];False;['import pandas as pd\nre.search(pattern, cell_in_question) \n'];False;2;6;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError', 'NameError', 'Sucess', 'NameError'];2;6;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError', 'NameError', 'Sucess', 'NameError'];4;6;"['Sucess', ""'A'"", 'Sucess', ""'A'"", 'Sucess', 'Sucess']";['Sucess', 'KeyError', 'Sucess', 'KeyError', 'Sucess', 'Sucess']
45;45;45;45;6.0;1;11361985;;1;59;<python><numpy><pandas>;Output data from all columns in a dataframe in pandas;106412.0;"['import pandas\nparamdata = pandas.read_csv(\'params.csv\', names=paramnames)\nparamnames = [""id"",\n""fc"",\n""mc"",\n""markup"",\n""asplevel"",\n""aspreview"",\n""reviewpd""]\nIn[35]: paramdata\nOut[35]: \n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 59 entries, 0 to 58\nData columns:\nid                    59  non-null values\nfc                    59  non-null values\nmc                    59  non-null values\nmarkup                59  non-null values\nasplevel              59  non-null values\naspreview             59  non-null values\nreviewpd              59  non-null values\n']";"[""import pandas\nparamdata = pandas.read_csv('params.csv', names=paramnames)\n"", 'paramnames = [""id"",\n""fc"",\n""mc"",\n""markup"",\n""asplevel"",\n""aspreview"",\n""reviewpd""]\n', ""In[35]: paramdata\nOut[35]: \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 59 entries, 0 to 58\nData columns:\nid                    59  non-null values\nfc                    59  non-null values\nmc                    59  non-null values\nmarkup                59  non-null values\nasplevel              59  non-null values\naspreview             59  non-null values\nreviewpd              59  non-null values\n""]";"['params.csv', 'ipython qtconsole', 'dataframe', ""import pandas\nparamdata = pandas.read_csv('params.csv', names=paramnames)\n"", 'paramnames', 'paramnames', 'paramnames = [""id"",\n""fc"",\n""mc"",\n""markup"",\n""asplevel"",\n""aspreview"",\n""reviewpd""]\n', 'paramdata', ""In[35]: paramdata\nOut[35]: \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 59 entries, 0 to 58\nData columns:\nid                    59  non-null values\nfc                    59  non-null values\nmc                    59  non-null values\nmarkup                59  non-null values\nasplevel              59  non-null values\naspreview             59  non-null values\nreviewpd              59  non-null values\n"", ""paramdata['mc']"", 'mc', 'df', ""paramdata[['id','fc','mc']]""]";"['import pandas\nparamdata = pandas.read_csv(\'params.csv\', names=paramnames)\n""fc"",\n""mc"",\n""markup"",\n""asplevel"",\n""aspreview"",\n']";"['import pandas\nparamdata = pandas.read_csv(\'params.csv\', names=paramnames)\n""fc"",\n""mc"",\n""markup"",\n""asplevel"",\n""aspreview"",\n']";False;"['import pandas as pd\nimport pandas\nparamdata = pandas.read_csv(\'params.csv\', names=paramnames)\n""fc"",\n""mc"",\n""markup"",\n""asplevel"",\n""aspreview"",\n']";False;2;5;"[""name 'paramdata' is not defined"", 'Sucess', ""name 'pandas' is not defined"", ""name 'paramdata' is not defined"", 'Sucess']";['NameError', 'Sucess', 'NameError', 'NameError', 'Sucess'];2;5;"[""name 'paramdata' is not defined"", 'Sucess', ""name 'pandas' is not defined"", ""name 'paramdata' is not defined"", 'Sucess']";['NameError', 'Sucess', 'NameError', 'NameError', 'Sucess'];2;5;"[""name 'paramdata' is not defined"", 'Sucess', ""name 'pandas' is not defined"", ""name 'paramdata' is not defined"", 'Sucess']";['NameError', 'Sucess', 'NameError', 'NameError', 'Sucess']
46;46;46;46;3.0;0;11391969;;1;39;<python><pandas>;How to group pandas DataFrame entries by date in a non-unique column;24050.0;"[""data.groupby(data['date'])\n""]";"[""data.groupby(data['date'])\n""]";"['DataFrame', '""date""', 'datetime', ""data.groupby(data['date'])\n"", 'datetime']";"[""data.groupby(data['date'])\n""]";"[""data.groupby(data['date'])\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\ndata.groupby(data['date'])\n""]";True;0;3;"[""name 'data' is not defined"", ""name 'data' is not defined"", ""name 'data' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'data' is not defined"", ""name 'data' is not defined"", ""name 'data' is not defined""]";['NameError', 'NameError', 'NameError'];1;3;"['Sucess', ""'date'"", ""'DataFrame' object has no attribute 'date'""]";['Sucess', 'KeyError', 'AttributeError']
47;47;47;47;2.0;2;11415701;;1;12;<python><tuples><pandas><dta>;Efficiently construct Pandas DataFrame from large list of tuples/rows;17794.0;"['In [2]: st_time = time.time(); initialload = sm.iolib.genfromdta(""/home/myfile.dta""); ed_time = time.time(); print (ed_time - st_time)\n666.523324013\n\nIn [3]: type(initialload)\nOut[3]: numpy.ndarray\n\nIn [4]: initialload.shape\nOut[4]: (4809584,)\n\nIn [5]: initialload[0]\nOut[5]: (19901130.0, 289.0, 1990.0, 12.0, 19901231.0, 18.0, 40301000.0, \'GB\', 18242.0, -2.368063, 1.0, 1.7783716290878204, 4379.355, 66.17669677734375, -999.0, -999.0, -0.60000002, -999.0, -999.0, -999.0, -999.0, -999.0, 0.2, 371.0)\n']";"['In [2]: st_time = time.time(); initialload = sm.iolib.genfromdta(""/home/myfile.dta""); ed_time = time.time(); print (ed_time - st_time)\n666.523324013\n\nIn [3]: type(initialload)\nOut[3]: numpy.ndarray\n\nIn [4]: initialload.shape\nOut[4]: (4809584,)\n\nIn [5]: initialload[0]\nOut[5]: (19901130.0, 289.0, 1990.0, 12.0, 19901231.0, 18.0, 40301000.0, \'GB\', 18242.0, -2.368063, 1.0, 1.7783716290878204, 4379.355, 66.17669677734375, -999.0, -999.0, -0.60000002, -999.0, -999.0, -999.0, -999.0, -999.0, 0.2, 371.0)\n']";"['scikits.statsmodels', 'genfromdta()', 'In [2]: st_time = time.time(); initialload = sm.iolib.genfromdta(""/home/myfile.dta""); ed_time = time.time(); print (ed_time - st_time)\n666.523324013\n\nIn [3]: type(initialload)\nOut[3]: numpy.ndarray\n\nIn [4]: initialload.shape\nOut[4]: (4809584,)\n\nIn [5]: initialload[0]\nOut[5]: (19901130.0, 289.0, 1990.0, 12.0, 19901231.0, 18.0, 40301000.0, \'GB\', 18242.0, -2.368063, 1.0, 1.7783716290878204, 4379.355, 66.17669677734375, -999.0, -999.0, -0.60000002, -999.0, -999.0, -999.0, -999.0, -999.0, 0.2, 371.0)\n']";"['st_time = time.time(); initialload = sm.iolib.genfromdta(""/home/myfile.dta""); ed_time = time.time(); print (ed_time - st_time)\n666.523324013\n\ninitialload.shape\ninitialload[0]\n']";"['st_time = time.time(); initialload = sm.iolib.genfromdta(""/home/myfile.dta""); ed_time = time.time(); print (ed_time - st_time)\n666.523324013\n\ninitialload.shape\ninitialload[0]\n']";False;"['import pandas as pd\nst_time = time.time(); initialload = sm.iolib.genfromdta(""/home/myfile.dta""); ed_time = time.time(); print (ed_time - st_time)\n666.523324013\n\ninitialload.shape\ninitialload[0]\n']";False;0;1;"[""name 'pandas' is not defined""]";['NameError'];0;1;"[""name 'pandas' is not defined""]";['NameError'];0;1;"[""name 'pandas' is not defined""]";['NameError']
48;48;48;48;3.0;0;11418192;;1;36;<pandas>;pandas: complex filter on rows of DataFrame;20153.0;"[""def f(row):\n  return sin(row['velocity'])/np.prod(['masses']) > 5\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, f)]\ndef g(row):\n  if row['col1'].method1() == 1:\n    val = row['col1'].method2() / row['col1'].method3(row['col3'], row['col4'])\n  else:\n    val = row['col2'].method5(row['col6'])\n  return np.sin(val)\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, g)]\n""]";"[""def f(row):\n  return sin(row['velocity'])/np.prod(['masses']) > 5\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, f)]\n"", ""def g(row):\n  if row['col1'].method1() == 1:\n    val = row['col1'].method2() / row['col1'].method3(row['col3'], row['col4'])\n  else:\n    val = row['col2'].method5(row['col6'])\n  return np.sin(val)\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, g)]\n""]";"[""def f(row):\n  return sin(row['velocity'])/np.prod(['masses']) > 5\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, f)]\n"", ""def g(row):\n  if row['col1'].method1() == 1:\n    val = row['col1'].method2() / row['col1'].method3(row['col3'], row['col4'])\n  else:\n    val = row['col2'].method5(row['col6'])\n  return np.sin(val)\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, g)]\n""]";['\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, f)]\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, g)]\n'];['\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, f)]\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, g)]\n'];False;['import pandas as pd\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, f)]\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, g)]\n'];False;0;2;"[""name 'df' is not defined"", ""name 'pandas' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'pandas' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""name 'pandas' is not defined""]";['NameError', 'NameError']
49;49;49;49;2.0;0;11495051;;1;21;<python><r><pandas><rpy2><statsmodels>;Difference in Python statsmodels OLS and R's lm;4994.0;"['import pandas\nfrom rpy2.robjects import r\n\nfrom functools import partial\n\nloadcsv = partial(pandas.DataFrame.from_csv,\n                  index_col=""seqn"", parse_dates=False)\n\ndemoq = loadcsv(""csv/DEMO.csv"")\nrxq = loadcsv(""csv/quest/RXQ_RX.csv"")\n\nnum_rx = {}\nfor seqn, num in rxq.rxd295.iteritems():\n    try:\n        val = int(num)\n    except ValueError:\n        val = 0\n    num_rx[seqn] = val\n\nseries = pandas.Series(num_rx, name=""num_rx"")\ndemoq = demoq.join(series)\n\nimport pandas.rpy.common as com\ndf = com.convert_to_r_dataframe(demoq)\nr.assign(""demoq"", df)\nr(\'lmout <- lm(demoq$num_rx ~ demoq$ridageyr)\')  # run the regression\nr(\'print(summary(lmout))\')  # print from R\nCall:\nlm(formula = demoq$num_rx ~ demoq$ridageyr)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9086 -0.6908 -0.2940  0.1358 15.7003 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -0.1358216  0.0241399  -5.626 1.89e-08 ***\ndemoq$ridageyr  0.0358161  0.0006232  57.469  < 2e-16 ***\n---\nSignif. codes:  0 \x91***\x92 0.001 \x91**\x92 0.01 \x91*\x92 0.05 \x91.\x92 0.1 \x91 \x92 1 \n\nResidual standard error: 1.545 on 9963 degrees of freedom\nMultiple R-squared: 0.249,  Adjusted R-squared: 0.2489 \nF-statistic:  3303 on 1 and 9963 DF,  p-value: < 2.2e-16\nimport statsmodels.api as sm\nresults = sm.OLS(demoq.num_rx, demoq.ridageyr).fit()\nresults.summary()\nOLS Regression Results\nAdj. R-squared:  0.247\nLog-Likelihood:  -18488.\nNo. Observations:    9965    AIC:   3.698e+04\nDf Residuals:    9964    BIC:   3.698e+04\n             coef   std err  t     P>|t|    [95.0% Conf. Int.]\nridageyr     0.0331  0.000   82.787    0.000        0.032 0.034\n']";"['import pandas\nfrom rpy2.robjects import r\n\nfrom functools import partial\n\nloadcsv = partial(pandas.DataFrame.from_csv,\n                  index_col=""seqn"", parse_dates=False)\n\ndemoq = loadcsv(""csv/DEMO.csv"")\nrxq = loadcsv(""csv/quest/RXQ_RX.csv"")\n\nnum_rx = {}\nfor seqn, num in rxq.rxd295.iteritems():\n    try:\n        val = int(num)\n    except ValueError:\n        val = 0\n    num_rx[seqn] = val\n\nseries = pandas.Series(num_rx, name=""num_rx"")\ndemoq = demoq.join(series)\n\nimport pandas.rpy.common as com\ndf = com.convert_to_r_dataframe(demoq)\nr.assign(""demoq"", df)\nr(\'lmout <- lm(demoq$num_rx ~ demoq$ridageyr)\')  # run the regression\nr(\'print(summary(lmout))\')  # print from R\n', 'Call:\nlm(formula = demoq$num_rx ~ demoq$ridageyr)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9086 -0.6908 -0.2940  0.1358 15.7003 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -0.1358216  0.0241399  -5.626 1.89e-08 ***\ndemoq$ridageyr  0.0358161  0.0006232  57.469  < 2e-16 ***\n---\nSignif. codes:  0 \x91***\x92 0.001 \x91**\x92 0.01 \x91*\x92 0.05 \x91.\x92 0.1 \x91 \x92 1 \n\nResidual standard error: 1.545 on 9963 degrees of freedom\nMultiple R-squared: 0.249,  Adjusted R-squared: 0.2489 \nF-statistic:  3303 on 1 and 9963 DF,  p-value: < 2.2e-16\n', 'import statsmodels.api as sm\nresults = sm.OLS(demoq.num_rx, demoq.ridageyr).fit()\nresults.summary()\n', 'OLS Regression Results\nAdj. R-squared:  0.247\nLog-Likelihood:  -18488.\nNo. Observations:    9965    AIC:   3.698e+04\nDf Residuals:    9964    BIC:   3.698e+04\n             coef   std err  t     P>|t|    [95.0% Conf. Int.]\nridageyr     0.0331  0.000   82.787    0.000        0.032 0.034\n']";"['R', 'import pandas\nfrom rpy2.robjects import r\n\nfrom functools import partial\n\nloadcsv = partial(pandas.DataFrame.from_csv,\n                  index_col=""seqn"", parse_dates=False)\n\ndemoq = loadcsv(""csv/DEMO.csv"")\nrxq = loadcsv(""csv/quest/RXQ_RX.csv"")\n\nnum_rx = {}\nfor seqn, num in rxq.rxd295.iteritems():\n    try:\n        val = int(num)\n    except ValueError:\n        val = 0\n    num_rx[seqn] = val\n\nseries = pandas.Series(num_rx, name=""num_rx"")\ndemoq = demoq.join(series)\n\nimport pandas.rpy.common as com\ndf = com.convert_to_r_dataframe(demoq)\nr.assign(""demoq"", df)\nr(\'lmout <- lm(demoq$num_rx ~ demoq$ridageyr)\')  # run the regression\nr(\'print(summary(lmout))\')  # print from R\n', 'R', 'Call:\nlm(formula = demoq$num_rx ~ demoq$ridageyr)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9086 -0.6908 -0.2940  0.1358 15.7003 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -0.1358216  0.0241399  -5.626 1.89e-08 ***\ndemoq$ridageyr  0.0358161  0.0006232  57.469  < 2e-16 ***\n---\nSignif. codes:  0 \x91***\x92 0.001 \x91**\x92 0.01 \x91*\x92 0.05 \x91.\x92 0.1 \x91 \x92 1 \n\nResidual standard error: 1.545 on 9963 degrees of freedom\nMultiple R-squared: 0.249,  Adjusted R-squared: 0.2489 \nF-statistic:  3303 on 1 and 9963 DF,  p-value: < 2.2e-16\n', 'statsmodels.api', 'import statsmodels.api as sm\nresults = sm.OLS(demoq.num_rx, demoq.ridageyr).fit()\nresults.summary()\n', 'OLS Regression Results\nAdj. R-squared:  0.247\nLog-Likelihood:  -18488.\nNo. Observations:    9965    AIC:   3.698e+04\nDf Residuals:    9964    BIC:   3.698e+04\n             coef   std err  t     P>|t|    [95.0% Conf. Int.]\nridageyr     0.0331  0.000   82.787    0.000        0.032 0.034\n']";"['import pandas\nfrom rpy2.robjects import r\n\nfrom functools import partial\n\n\ndemoq = loadcsv(""csv/DEMO.csv"")\nrxq = loadcsv(""csv/quest/RXQ_RX.csv"")\n\nnum_rx = {}\n\nseries = pandas.Series(num_rx, name=""num_rx"")\ndemoq = demoq.join(series)\n\nimport pandas.rpy.common as com\ndf = com.convert_to_r_dataframe(demoq)\nr.assign(""demoq"", df)\nr(\'lmout <- lm(demoq$num_rx ~ demoq$ridageyr)\')  # run the regression\nr(\'print(summary(lmout))\')  # print from R\n\n\n\nimport statsmodels.api as sm\nresults = sm.OLS(demoq.num_rx, demoq.ridageyr).fit()\nresults.summary()\n']";"['import pandas\nfrom rpy2.robjects import r\n\nfrom functools import partial\n\n\ndemoq = loadcsv(""csv/DEMO.csv"")\nrxq = loadcsv(""csv/quest/RXQ_RX.csv"")\n\nnum_rx = {}\n\nseries = pandas.Series(num_rx, name=""num_rx"")\ndemoq = demoq.join(series)\n\nimport pandas.rpy.common as com\ndf = com.convert_to_r_dataframe(demoq)\nr.assign(""demoq"", df)\nr(\'lmout <- lm(demoq$num_rx ~ demoq$ridageyr)\')  # run the regression\nr(\'print(summary(lmout))\')  # print from R\n\n\n\nimport statsmodels.api as sm\nresults = sm.OLS(demoq.num_rx, demoq.ridageyr).fit()\nresults.summary()\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\nimport pandas\nfrom rpy2.robjects import r\n\nfrom functools import partial\n\n\ndemoq = loadcsv(""csv/DEMO.csv"")\nrxq = loadcsv(""csv/quest/RXQ_RX.csv"")\n\nnum_rx = {}\n\nseries = pandas.Series(num_rx, name=""num_rx"")\ndemoq = demoq.join(series)\n\nimport pandas.rpy.common as com\ndf = com.convert_to_r_dataframe(demoq)\nr.assign(""demoq"", df)\nr(\'lmout <- lm(demoq$num_rx ~ demoq$ridageyr)\')  # run the regression\nr(\'print(summary(lmout))\')  # print from R\n\n\n\nimport statsmodels.api as sm\nresults = sm.OLS(demoq.num_rx, demoq.ridageyr).fit()\nresults.summary()\n']";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
50;50;50;50;2.0;3;11548005;;1;54;<python><numpy><int><pandas><data-type-conversion>;NumPy or Pandas: Keeping array type as integer while having a NaN value;16797.0;[''];[];['numpy', 'int', 'int64', 'numpy.NaN', 'int', 'from_records()', 'coerce_float=False'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
51;51;51;51;8.0;0;11587782;;1;17;<python><pandas>;Creating dummy variables in pandas for python;39556.0;"[""import pandas as pd\n\ndummies = pd.get_dummies(df['Category'])\n""]";"[""import pandas as pd\n\ndummies = pd.get_dummies(df['Category'])\n""]";"['get_dummies', 'get_dummies', ""import pandas as pd\n\ndummies = pd.get_dummies(df['Category'])\n""]";"[""import pandas as pd\n\ndummies = pd.get_dummies(df['Category'])\n""]";"[""import pandas as pd\n\ndummies = pd.get_dummies(df['Category'])\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\n\ndummies = pd.get_dummies(df['Category'])\n""]";True;1;3;"['Sucess', ""No module named 'statsmodels'"", ""name 'pd' is not defined""]";['Sucess', 'ImportError', 'NameError'];1;3;"['Sucess', ""No module named 'statsmodels'"", ""name 'df' is not defined""]";['Sucess', 'ImportError', 'NameError'];1;3;"['Sucess', ""No module named 'statsmodels'"", ""name 'df' is not defined""]";['Sucess', 'ImportError', 'NameError']
52;52;52;52;2.0;0;11615504;;1;21;<python><pandas>;Parse dates when YYYYMMDD and HH are in separate columns using pandas in Python;15326.0;"['YYYYMMDD, HH,    X\n20110101,  1,   10\n20110101,  2,   20\n20110101,  3,   30\nimport pandas as pnd\npnd.read_csv(""..\\\\file.csv"",  parse_dates = True, index_col = [0,1])\n                         X\nYYYYMMDD    HH            \n2011-01-01 2012-07-01   10\n           2012-07-02   20\n           2012-07-03   30\n                      X\nDatetime              \n2011-01-01 01:00:00  10\n2011-01-01 02:00:00  20\n2011-01-01 03:00:00  30\n']";"['YYYYMMDD, HH,    X\n20110101,  1,   10\n20110101,  2,   20\n20110101,  3,   30\n', 'import pandas as pnd\npnd.read_csv(""..\\\\file.csv"",  parse_dates = True, index_col = [0,1])\n', '                         X\nYYYYMMDD    HH            \n2011-01-01 2012-07-01   10\n           2012-07-02   20\n           2012-07-03   30\n', '                      X\nDatetime              \n2011-01-01 01:00:00  10\n2011-01-01 02:00:00  20\n2011-01-01 03:00:00  30\n']";"['YYYYMMDD, HH,    X\n20110101,  1,   10\n20110101,  2,   20\n20110101,  3,   30\n', 'import pandas as pnd\npnd.read_csv(""..\\\\file.csv"",  parse_dates = True, index_col = [0,1])\n', '                         X\nYYYYMMDD    HH            \n2011-01-01 2012-07-01   10\n           2012-07-02   20\n           2012-07-03   30\n', '                      X\nDatetime              \n2011-01-01 01:00:00  10\n2011-01-01 02:00:00  20\n2011-01-01 03:00:00  30\n']";"['YYYYMMDD, HH,    X\n20110101,  1,   10\n20110101,  2,   20\n20110101,  3,   30\nimport pandas as pnd\npnd.read_csv(""..\\\\file.csv"",  parse_dates = True, index_col = [0,1])\nDatetime              \n']";"['YYYYMMDD, HH,    X\n20110101,  1,   10\n20110101,  2,   20\n20110101,  3,   30\nimport pandas as pnd\npnd.read_csv(""..\\\\file.csv"",  parse_dates = True, index_col = [0,1])\nDatetime              \n']";False;"['import pandas as pd\nYYYYMMDD, HH,    X\n20110101,  1,   10\n20110101,  2,   20\n20110101,  3,   30\nimport pandas as pnd\npnd.read_csv(""..\\\\file.csv"",  parse_dates = True, index_col = [0,1])\nDatetime              \n']";False;1;2;"['Sucess', ""name 'pd' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError']
53;53;53;53;6.0;1;11622652;;1;74;<python><pandas><sas>;Large, persistent DataFrame in pandas;45446.0;[''];[];['pandas.read_csv()', 'pandas'];[''];[''];False;['import pandas as pd\n'];False;1;3;"['Sucess', ""File b'large_dataset.csv' does not exist"", ""Command 'wc -l ../data/my_large.csv' returned non-zero exit status 1""]";['Sucess', 'FileNotFoundError', 'CalledProcessError'];1;3;"['Sucess', ""File b'large_dataset.csv' does not exist"", ""Command 'wc -l ../data/my_large.csv' returned non-zero exit status 1""]";['Sucess', 'FileNotFoundError', 'CalledProcessError'];1;3;"['Sucess', ""File b'large_dataset.csv' does not exist"", ""Command 'wc -l ../data/my_large.csv' returned non-zero exit status 1""]";['Sucess', 'FileNotFoundError', 'CalledProcessError']
54;54;54;54;2.0;0;11637384;;1;18;<python><pandas>;Pandas join/merge/concat two dataframes;29299.0;"[""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 1941 entries, 2004-10-19 00:00:00 to 2012-07-23 00:00:00\nData columns:\nclose    1941  non-null values\nhigh     1941  non-null values\nlow      1941  non-null values\nopen     1941  non-null values\ndtypes: float64(4)\n <class 'pandas.core.frame.DataFrame'>\n DatetimeIndex: 1941 entries, 2004-10-19 00:00:00 to 2012-07-23 00:00:00\n Data columns:\n close2    1941  non-null values\n high2     1941  non-null values\n low2      1941  non-null values\n open2     1941  non-null values\n dtypes: float64(4)\n\n y.join(x) or pandas.DataFrame.join(y,x):\n <class 'pandas.core.frame.DataFrame'>\n DatetimeIndex: 34879 entries, 2004-12-16 00:00:00 to 2012-07-12 00:00:00\n Data columns:\n close2    34879  non-null values\n high2     34879  non-null values\n low2      34879  non-null values\n open2     34879  non-null values\n close     34879  non-null values\n high      34879  non-null values\n low       34879  non-null values\n open      34879  non-null values\n dtypes: float64(8)\nIn [83]: pandas.concat([x,y]) \nOut[83]: <class 'pandas.core.frame.DataFrame'> \nDatetimeIndex: 3882 entries, 2004-10-19 00:00:00 to 2012-07-23 00:00:00 \nData columns: \nclose2 3882 non-null values \nhigh2 3882 non-null values \nlow2 3882 non-null values \nopen2 3882 non-null values \ndtypes: float64(4) \n""]";"[""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 1941 entries, 2004-10-19 00:00:00 to 2012-07-23 00:00:00\nData columns:\nclose    1941  non-null values\nhigh     1941  non-null values\nlow      1941  non-null values\nopen     1941  non-null values\ndtypes: float64(4)\n"", "" <class 'pandas.core.frame.DataFrame'>\n DatetimeIndex: 1941 entries, 2004-10-19 00:00:00 to 2012-07-23 00:00:00\n Data columns:\n close2    1941  non-null values\n high2     1941  non-null values\n low2      1941  non-null values\n open2     1941  non-null values\n dtypes: float64(4)\n\n y.join(x) or pandas.DataFrame.join(y,x):\n <class 'pandas.core.frame.DataFrame'>\n DatetimeIndex: 34879 entries, 2004-12-16 00:00:00 to 2012-07-12 00:00:00\n Data columns:\n close2    34879  non-null values\n high2     34879  non-null values\n low2      34879  non-null values\n open2     34879  non-null values\n close     34879  non-null values\n high      34879  non-null values\n low       34879  non-null values\n open      34879  non-null values\n dtypes: float64(8)\n"", ""In [83]: pandas.concat([x,y]) \nOut[83]: <class 'pandas.core.frame.DataFrame'> \nDatetimeIndex: 3882 entries, 2004-10-19 00:00:00 to 2012-07-23 00:00:00 \nData columns: \nclose2 3882 non-null values \nhigh2 3882 non-null values \nlow2 3882 non-null values \nopen2 3882 non-null values \ndtypes: float64(4) \n""]";"['dataframe', ""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 1941 entries, 2004-10-19 00:00:00 to 2012-07-23 00:00:00\nData columns:\nclose    1941  non-null values\nhigh     1941  non-null values\nlow      1941  non-null values\nopen     1941  non-null values\ndtypes: float64(4)\n"", "" <class 'pandas.core.frame.DataFrame'>\n DatetimeIndex: 1941 entries, 2004-10-19 00:00:00 to 2012-07-23 00:00:00\n Data columns:\n close2    1941  non-null values\n high2     1941  non-null values\n low2      1941  non-null values\n open2     1941  non-null values\n dtypes: float64(4)\n\n y.join(x) or pandas.DataFrame.join(y,x):\n <class 'pandas.core.frame.DataFrame'>\n DatetimeIndex: 34879 entries, 2004-12-16 00:00:00 to 2012-07-12 00:00:00\n Data columns:\n close2    34879  non-null values\n high2     34879  non-null values\n low2      34879  non-null values\n open2     34879  non-null values\n close     34879  non-null values\n high      34879  non-null values\n low       34879  non-null values\n open      34879  non-null values\n dtypes: float64(8)\n"", ""In [83]: pandas.concat([x,y]) \nOut[83]: <class 'pandas.core.frame.DataFrame'> \nDatetimeIndex: 3882 entries, 2004-10-19 00:00:00 to 2012-07-23 00:00:00 \nData columns: \nclose2 3882 non-null values \nhigh2 3882 non-null values \nlow2 3882 non-null values \nopen2 3882 non-null values \ndtypes: float64(4) \n""]";['pandas.concat([x,y]) \n'];['pandas.concat([x,y]) \n'];False;['import pandas as pd\npandas.concat([x,y]) \n'];False;1;2;"['Sucess', ""name 'left' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'left' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'left' is not defined""]";['Sucess', 'NameError']
55;55;55;55;1.0;0;11640243;;1;33;<python><pandas>;PANDAS plot multiple Y axes;15502.0;[''];[];['pandas'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
56;56;56;56;4.0;6;11697887;;1;43;<python><django><pandas>;Converting Django QuerySet to pandas DataFrame;10165.0;"[""qs = SomeModel.objects.select_related().filter(date__year=2012)\nq = qs.values('date', 'OtherField')\ndf = pd.DataFrame.from_records(q)\n""]";"[""qs = SomeModel.objects.select_related().filter(date__year=2012)\nq = qs.values('date', 'OtherField')\ndf = pd.DataFrame.from_records(q)\n""]";"['DataFrame', ""qs = SomeModel.objects.select_related().filter(date__year=2012)\nq = qs.values('date', 'OtherField')\ndf = pd.DataFrame.from_records(q)\n""]";"[""qs = SomeModel.objects.select_related().filter(date__year=2012)\nq = qs.values('date', 'OtherField')\ndf = pd.DataFrame.from_records(q)\n""]";"[""import pandas as pd\nqs = SomeModel.objects.select_related().filter(date__year=2012)\nq = qs.values('date', 'OtherField')\ndf = pd.DataFrame.from_records(q)\n""]";True;"[""import pandas as pd\nqs = SomeModel.objects.select_related().filter(date__year=2012)\nq = qs.values('date', 'OtherField')\ndf = pd.DataFrame.from_records(q)\n""]";False;0;2;"[""No module named 'myapp'"", ""No module named 'django_pandas'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'myapp'"", ""No module named 'django_pandas'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'myapp'"", ""No module named 'django_pandas'""]";['ImportError', 'ImportError']
57;57;57;57;8.0;0;11707586;;1;157;<python><pandas><options><display><column-width>;Python pandas, how to widen output display to see more columns?;96153.0;['>Index: 8 entries, count to max  \n>Data columns:  \n>x1          8  non-null values  \n>x2          8  non-null values  \n>x3          8  non-null values  \n>x4          8  non-null values  \n>x5          8  non-null values  \n>x6          8  non-null values  \n>x7          8  non-null values  \n'];['>Index: 8 entries, count to max  \n>Data columns:  \n>x1          8  non-null values  \n>x2          8  non-null values  \n>x3          8  non-null values  \n>x4          8  non-null values  \n>x5          8  non-null values  \n>x6          8  non-null values  \n>x7          8  non-null values  \n'];['dataframe', 'dataframe', 'dataframe', '>Index: 8 entries, count to max  \n>Data columns:  \n>x1          8  non-null values  \n>x2          8  non-null values  \n>x3          8  non-null values  \n>x4          8  non-null values  \n>x5          8  non-null values  \n>x6          8  non-null values  \n>x7          8  non-null values  \n'];[''];[''];False;['import pandas as pd\n'];False;2;8;"['Sucess', ""name 'df' is not defined"", ""name 'Parameters' is not defined"", ""name 'pd' is not defined"", ""name 'pd' is not defined"", 'Sucess', ""name 'pd' is not defined"", ""name 'pd' is not defined""]";['Sucess', 'NameError', 'NameError', 'NameError', 'NameError', 'Sucess', 'NameError', 'NameError'];5;8;"['Sucess', ""name 'df' is not defined"", ""name 'Parameters' is not defined"", 'Sucess', ""module 'pandas.util' has no attribute 'terminal'"", 'Sucess', 'Sucess', 'Sucess']";['Sucess', 'NameError', 'NameError', 'Sucess', 'AttributeError', 'Sucess', 'Sucess', 'Sucess'];5;8;"['Sucess', 'Cannot describe a DataFrame without columns', ""name 'Parameters' is not defined"", 'Sucess', ""module 'pandas.util' has no attribute 'terminal'"", 'Sucess', 'Sucess', 'Sucess']";['Sucess', 'ValueError', 'NameError', 'Sucess', 'AttributeError', 'Sucess', 'Sucess', 'Sucess']
58;58;58;58;1.0;3;11707805;;1;12;<memory><pandas><machine-learning><scikit-learn><classification>;Scikit and Pandas: Fitting Large Data;7090.0;[''];[];['KNeighborsClassifier', 'read_csv', 'dataframe'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
59;59;59;59;2.0;4;11714768;;1;11;<python><numpy><scipy><pandas><yahoo-finance>;concat pandas DataFrame along timeseries indexes;6001.0;"[""           NAB.AX                                  CBA.AX\n            Close    Volume                         Close    Volume\nDate                                    Date\n2009-06-05  36.51   4962900             2009-06-08  21.95         0\n2009-06-04  36.79   5528800             2009-06-05  21.95   8917000\n2009-06-03  36.80   5116500             2009-06-04  22.21  18723600\n2009-06-02  36.33   5303700             2009-06-03  23.11  11643800\n2009-06-01  36.16   5625500             2009-06-02  22.80  14249900\n2009-05-29  35.14  13038600   --AND--   2009-06-01  22.52  11687200\n2009-05-28  33.95   7917600             2009-05-29  22.02  22350700\n2009-05-27  35.13   4701100             2009-05-28  21.63   9679800\n2009-05-26  35.45   4572700             2009-05-27  21.74   9338200\n2009-05-25  34.80   3652500             2009-05-26  21.64   8502900\nkeys = ['CBA.AX','NAB.AX']\nmv = pandas.concat([data['CBA.AX'][650:660],data['NAB.AX'][650:660]], axis=1, keys=stocks,) \n                                 CBA.AX          NAB.AX        \n                              Close  Volume   Close  Volume\nDate                                                      \n2200-08-16 04:24:21.460041     NaN     NaN     NaN     NaN\n2203-05-13 04:24:21.460041     NaN     NaN     NaN     NaN\n2206-02-06 04:24:21.460041     NaN     NaN     NaN     NaN\n2208-11-02 04:24:21.460041     NaN     NaN     NaN     NaN\n2211-07-30 04:24:21.460041     NaN     NaN     NaN     NaN\n2219-10-16 04:24:21.460041     NaN     NaN     NaN     NaN\n2222-07-12 04:24:21.460041     NaN     NaN     NaN     NaN\n2225-04-07 04:24:21.460041     NaN     NaN     NaN     NaN\n2228-01-02 04:24:21.460041     NaN     NaN     NaN     NaN\n2230-09-28 04:24:21.460041     NaN     NaN     NaN     NaN\n2238-12-15 04:24:21.460041     NaN     NaN     NaN     NaN\ndata = {\n'CBA.AX': <class 'pandas.core.frame.DataFrame'>\n    DatetimeIndex: 2313 entries, 2011-12-29 00:00:00 to 2003-01-01 00:00:00\n    Data columns:\n        Close     2313  non-null values\n        Volume    2313  non-null values\n    dtypes: float64(1), int64(1),\n\n 'NAB.AX': <class 'pandas.core.frame.DataFrame'>\n    DatetimeIndex: 2329 entries, 2011-12-29 00:00:00 to 2003-01-01 00:00:00\n    Data columns:\n        Close     2329  non-null values\n        Volume    2329  non-null values\n    dtypes: float64(1), int64(1)\n}\n""]";"['           NAB.AX                                  CBA.AX\n            Close    Volume                         Close    Volume\nDate                                    Date\n2009-06-05  36.51   4962900             2009-06-08  21.95         0\n2009-06-04  36.79   5528800             2009-06-05  21.95   8917000\n2009-06-03  36.80   5116500             2009-06-04  22.21  18723600\n2009-06-02  36.33   5303700             2009-06-03  23.11  11643800\n2009-06-01  36.16   5625500             2009-06-02  22.80  14249900\n2009-05-29  35.14  13038600   --AND--   2009-06-01  22.52  11687200\n2009-05-28  33.95   7917600             2009-05-29  22.02  22350700\n2009-05-27  35.13   4701100             2009-05-28  21.63   9679800\n2009-05-26  35.45   4572700             2009-05-27  21.74   9338200\n2009-05-25  34.80   3652500             2009-05-26  21.64   8502900\n', ""keys = ['CBA.AX','NAB.AX']\nmv = pandas.concat([data['CBA.AX'][650:660],data['NAB.AX'][650:660]], axis=1, keys=stocks,) \n"", '                                 CBA.AX          NAB.AX        \n                              Close  Volume   Close  Volume\nDate                                                      \n2200-08-16 04:24:21.460041     NaN     NaN     NaN     NaN\n2203-05-13 04:24:21.460041     NaN     NaN     NaN     NaN\n2206-02-06 04:24:21.460041     NaN     NaN     NaN     NaN\n2208-11-02 04:24:21.460041     NaN     NaN     NaN     NaN\n2211-07-30 04:24:21.460041     NaN     NaN     NaN     NaN\n2219-10-16 04:24:21.460041     NaN     NaN     NaN     NaN\n2222-07-12 04:24:21.460041     NaN     NaN     NaN     NaN\n2225-04-07 04:24:21.460041     NaN     NaN     NaN     NaN\n2228-01-02 04:24:21.460041     NaN     NaN     NaN     NaN\n2230-09-28 04:24:21.460041     NaN     NaN     NaN     NaN\n2238-12-15 04:24:21.460041     NaN     NaN     NaN     NaN\n', ""data = {\n'CBA.AX': <class 'pandas.core.frame.DataFrame'>\n    DatetimeIndex: 2313 entries, 2011-12-29 00:00:00 to 2003-01-01 00:00:00\n    Data columns:\n        Close     2313  non-null values\n        Volume    2313  non-null values\n    dtypes: float64(1), int64(1),\n\n 'NAB.AX': <class 'pandas.core.frame.DataFrame'>\n    DatetimeIndex: 2329 entries, 2011-12-29 00:00:00 to 2003-01-01 00:00:00\n    Data columns:\n        Close     2329  non-null values\n        Volume    2329  non-null values\n    dtypes: float64(1), int64(1)\n}\n""]";"['DateFrame', '           NAB.AX                                  CBA.AX\n            Close    Volume                         Close    Volume\nDate                                    Date\n2009-06-05  36.51   4962900             2009-06-08  21.95         0\n2009-06-04  36.79   5528800             2009-06-05  21.95   8917000\n2009-06-03  36.80   5116500             2009-06-04  22.21  18723600\n2009-06-02  36.33   5303700             2009-06-03  23.11  11643800\n2009-06-01  36.16   5625500             2009-06-02  22.80  14249900\n2009-05-29  35.14  13038600   --AND--   2009-06-01  22.52  11687200\n2009-05-28  33.95   7917600             2009-05-29  22.02  22350700\n2009-05-27  35.13   4701100             2009-05-28  21.63   9679800\n2009-05-26  35.45   4572700             2009-05-27  21.74   9338200\n2009-05-25  34.80   3652500             2009-05-26  21.64   8502900\n', ""keys = ['CBA.AX','NAB.AX']\nmv = pandas.concat([data['CBA.AX'][650:660],data['NAB.AX'][650:660]], axis=1, keys=stocks,) \n"", '                                 CBA.AX          NAB.AX        \n                              Close  Volume   Close  Volume\nDate                                                      \n2200-08-16 04:24:21.460041     NaN     NaN     NaN     NaN\n2203-05-13 04:24:21.460041     NaN     NaN     NaN     NaN\n2206-02-06 04:24:21.460041     NaN     NaN     NaN     NaN\n2208-11-02 04:24:21.460041     NaN     NaN     NaN     NaN\n2211-07-30 04:24:21.460041     NaN     NaN     NaN     NaN\n2219-10-16 04:24:21.460041     NaN     NaN     NaN     NaN\n2222-07-12 04:24:21.460041     NaN     NaN     NaN     NaN\n2225-04-07 04:24:21.460041     NaN     NaN     NaN     NaN\n2228-01-02 04:24:21.460041     NaN     NaN     NaN     NaN\n2230-09-28 04:24:21.460041     NaN     NaN     NaN     NaN\n2238-12-15 04:24:21.460041     NaN     NaN     NaN     NaN\n', ""data = {\n'CBA.AX': <class 'pandas.core.frame.DataFrame'>\n    DatetimeIndex: 2313 entries, 2011-12-29 00:00:00 to 2003-01-01 00:00:00\n    Data columns:\n        Close     2313  non-null values\n        Volume    2313  non-null values\n    dtypes: float64(1), int64(1),\n\n 'NAB.AX': <class 'pandas.core.frame.DataFrame'>\n    DatetimeIndex: 2329 entries, 2011-12-29 00:00:00 to 2003-01-01 00:00:00\n    Data columns:\n        Close     2329  non-null values\n        Volume    2329  non-null values\n    dtypes: float64(1), int64(1)\n}\n""]";"[""keys = ['CBA.AX','NAB.AX']\nmv = pandas.concat([data['CBA.AX'][650:660],data['NAB.AX'][650:660]], axis=1, keys=stocks,) \nDate                                                      \n\n""]";"[""keys = ['CBA.AX','NAB.AX']\nmv = pandas.concat([data['CBA.AX'][650:660],data['NAB.AX'][650:660]], axis=1, keys=stocks,) \nDate                                                      \n\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nkeys = ['CBA.AX','NAB.AX']\nmv = pandas.concat([data['CBA.AX'][650:660],data['NAB.AX'][650:660]], axis=1, keys=stocks,) \nDate                                                      \n\n""]";True;0;1;['The pandas.io.data module is moved to a separate package (pandas-datareader). After installing the pandas-datareader package (https://github.com/pydata/pandas-datareader), you can change the import ``from pandas.io import data, wb`` to ``from pandas_datareader import data, wb``.'];['ImportError'];0;1;['The pandas.io.data module is moved to a separate package (pandas-datareader). After installing the pandas-datareader package (https://github.com/pydata/pandas-datareader), you can change the import ``from pandas.io import data, wb`` to ``from pandas_datareader import data, wb``.'];['ImportError'];0;1;['The pandas.io.data module is moved to a separate package (pandas-datareader). After installing the pandas-datareader package (https://github.com/pydata/pandas-datareader), you can change the import ``from pandas.io import data, wb`` to ``from pandas_datareader import data, wb``.'];['ImportError']
60;60;60;60;1.0;7;11728836;;1;87;<python><pandas><multiprocessing><shared-memory>;Efficiently applying a function to a grouped pandas DataFrame in parallel;5986.0;[''];[];['DataFrame', 'DataFrame', 'numpy'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""No module named 'para_group_demo'""]";['ImportError'];0;1;"[""No module named 'para_group_demo'""]";['ImportError'];0;1;"[""No module named 'para_group_demo'""]";['ImportError']
61;61;61;61;5.0;0;11811392;;1;17;<python><pandas>;How to generate a list from a pandas DataFrame with the column name and column values?;19595.0;"[""   one  two  three  four  five\n0    1    2      3     4     5\n1    1    1      1     1     1\nnested_list = [['one', 1, 1]\n               ['two', 2, 1]\n               ['three', 3, 1]\n               ['four', 4, 1]\n               ['five', 5, 1]]\n""]";"['   one  two  three  four  five\n0    1    2      3     4     5\n1    1    1      1     1     1\n', ""nested_list = [['one', 1, 1]\n               ['two', 2, 1]\n               ['three', 3, 1]\n               ['four', 4, 1]\n               ['five', 5, 1]]\n""]";"['   one  two  three  four  five\n0    1    2      3     4     5\n1    1    1      1     1     1\n', ""nested_list = [['one', 1, 1]\n               ['two', 2, 1]\n               ['three', 3, 1]\n               ['four', 4, 1]\n               ['five', 5, 1]]\n""]";[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'dt' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'dt' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'dt' is not defined""]";['Sucess', 'NameError']
62;62;62;62;4.0;0;11858472;;1;41;<python><numpy><dataframe><pandas>;Pandas: Combine string and int columns;30814.0;"[""from pandas import *\ndf = DataFrame({'foo':['a','b','c'], 'bar':[1, 2, 3]})\n    bar foo\n0    1   a\n1    2   b\n2    3   c\n     bar\n0    1 is a\n1    2 is b\n2    3 is c\ndf['foo'] = '%s is %s' % (df['bar'], df['foo'])\n>>>print df.ix[0]\n\nbar                                                    a\nfoo    0    a\n1    b\n2    c\nName: bar is 0    1\n1    2\n2\nName: 0\n""]";"[""from pandas import *\ndf = DataFrame({'foo':['a','b','c'], 'bar':[1, 2, 3]})\n"", '    bar foo\n0    1   a\n1    2   b\n2    3   c\n', '     bar\n0    1 is a\n1    2 is b\n2    3 is c\n', ""df['foo'] = '%s is %s' % (df['bar'], df['foo'])\n"", '>>>print df.ix[0]\n\nbar                                                    a\nfoo    0    a\n1    b\n2    c\nName: bar is 0    1\n1    2\n2\nName: 0\n']";"['DataFrame', ""from pandas import *\ndf = DataFrame({'foo':['a','b','c'], 'bar':[1, 2, 3]})\n"", '    bar foo\n0    1   a\n1    2   b\n2    3   c\n', '     bar\n0    1 is a\n1    2 is b\n2    3 is c\n', ""df['foo'] = '%s is %s' % (df['bar'], df['foo'])\n"", '>>>print df.ix[0]\n\nbar                                                    a\nfoo    0    a\n1    b\n2    c\nName: bar is 0    1\n1    2\n2\nName: 0\n']";"[""from pandas import *\ndf = DataFrame({'foo':['a','b','c'], 'bar':[1, 2, 3]})\ndf['foo'] = '%s is %s' % (df['bar'], df['foo'])\n\n2\n""]";"[""from pandas import DataFrame\nfrom pandas import *\ndf = DataFrame({'foo':['a','b','c'], 'bar':[1, 2, 3]})\ndf['foo'] = '%s is %s' % (df['bar'], df['foo'])\n\n2\n""]";True;"[""import pandas as pd\nfrom pandas import *\ndf = DataFrame({'foo':['a','b','c'], 'bar':[1, 2, 3]})\ndf['foo'] = '%s is %s' % (df['bar'], df['foo'])\n\n2\n""]";False;1;3;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];2;3;"['Sucess', 'Sucess', ""'bar'""]";['Sucess', 'Sucess', 'KeyError']
63;63;63;63;8.0;0;11869910;;1;141;<pandas>;pandas: filter rows of DataFrame with operator chaining;187196.0;"[""df_filtered = df[df['column'] == value]\ndf_filtered = df.mask(lambda x: x['column'] == value)\n""]";"[""df_filtered = df[df['column'] == value]\n"", ""df_filtered = df.mask(lambda x: x['column'] == value)\n""]";"['pandas', 'groupby', 'aggregate', 'apply', ""df_filtered = df[df['column'] == value]\n"", 'df', ""df_filtered = df.mask(lambda x: x['column'] == value)\n""]";"[""df_filtered = df[df['column'] == value]\ndf_filtered = df.mask(lambda x: x['column'] == value)\n""]";"[""df_filtered = df[df['column'] == value]\ndf_filtered = df.mask(lambda x: x['column'] == value)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf_filtered = df[df['column'] == value]\ndf_filtered = df.mask(lambda x: x['column'] == value)\n""]";True;0;4;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'np' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""'DataFrame' object has no attribute 'A'"", '0', ""name 'np' is not defined"", ""'DataFrame' object has no attribute 'A'""]";['AttributeError', 'KeyError', 'NameError', 'AttributeError']
64;64;64;64;2.0;0;11881165;;1;20;<python><pandas><slice>;Slice Pandas DataFrame by Row;20437.0;[''];[];"[""hdf = pandas.HDFStore('Survey.h5')"", 'DataFrame', 'DataFrame']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'rows' is not defined""]";['NameError']
65;65;65;65;2.0;0;11927715;;1;38;<python><matplotlib><pandas>;How to give a pandas/matplotlib bar graph custom colors;29388.0;"[""  4 from matplotlib import pyplot\n  5 from pandas import *\n  6 import random\n  7 \n  8 x = [{i:random.randint(1,5)} for i in range(10)]\n  9 df = DataFrame(x)\n 10 \n 11 df.plot(kind='bar', stacked=True)\n""]";"[""  4 from matplotlib import pyplot\n  5 from pandas import *\n  6 import random\n  7 \n  8 x = [{i:random.randint(1,5)} for i in range(10)]\n  9 df = DataFrame(x)\n 10 \n 11 df.plot(kind='bar', stacked=True)\n""]";"[""  4 from matplotlib import pyplot\n  5 from pandas import *\n  6 import random\n  7 \n  8 x = [{i:random.randint(1,5)} for i in range(10)]\n  9 df = DataFrame(x)\n 10 \n 11 df.plot(kind='bar', stacked=True)\n""]";[''];[''];False;['import pandas as pd\n'];False;0;2;"[""No module named 'matplotlib'"", ""name 'df' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'matplotlib'"", ""name 'df' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'matplotlib'"", ""name '_converter' is not defined""]";['ImportError', 'NameError']
66;66;66;66;3.0;0;11941492;;1;30;<python><ipython><pandas>;Selecting rows from a Pandas dataframe with a compound (hierarchical) index;25839.0;"[""import pandas\ndf = pandas.DataFrame({'group1': ['a','a','a','b','b','b'],\n                       'group2': ['c','c','d','d','d','e'],\n                       'value1': [1.1,2,3,4,5,6],\n                       'value2': [7.1,8,9,10,11,12]\n})\ndf = df.set_index(['group1', 'group2'])\ndf['group1' == 'a']\ndf['a','c']\n""]";"[""import pandas\ndf = pandas.DataFrame({'group1': ['a','a','a','b','b','b'],\n                       'group2': ['c','c','d','d','d','e'],\n                       'value1': [1.1,2,3,4,5,6],\n                       'value2': [7.1,8,9,10,11,12]\n})\ndf = df.set_index(['group1', 'group2'])\n"", ""df['group1' == 'a']\n"", ""df['a','c']\n""]";"['dataframe', 'dataframe', ""import pandas\ndf = pandas.DataFrame({'group1': ['a','a','a','b','b','b'],\n                       'group2': ['c','c','d','d','d','e'],\n                       'value1': [1.1,2,3,4,5,6],\n                       'value2': [7.1,8,9,10,11,12]\n})\ndf = df.set_index(['group1', 'group2'])\n"", ""df['group1' == 'a']\n"", ""df['a','c']\n""]";"[""import pandas\ndf = df.set_index(['group1', 'group2'])\ndf['group1' == 'a']\ndf['a','c']\n""]";"[""import pandas\ndf = df.set_index(['group1', 'group2'])\ndf['group1' == 'a']\ndf['a','c']\n""]";False;"[""import pandas as pd\nimport pandas\ndf = df.set_index(['group1', 'group2'])\ndf['group1' == 'a']\ndf['a','c']\n""]";False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"['\n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated', ""'Index' object has no attribute 'get_loc_level'""]";['DeprecationWarning', 'AttributeError']
67;67;67;67;1.0;1;11976503;;1;38;<python><pandas>;How to keep index when using pandas merge;12679.0;"['In [441]: a=DataFrame(data={""col1"": [1,2,3], \'to_merge_on\' : [1,3,4]}, index=[""a"",""b"",""c""])\n\nIn [442]: b=DataFrame(data={""col2"": [1,2,3], \'to_merge_on\' : [1,3,5]})\nIn [443]: a\nOut[443]: \n   col1  to_merge_on\na     1            1\nb     2            3\nc     3            4\n\nIn [444]: b\nOut[444]: \n   col2  to_merge_on\n0     1            1\n1     2            3\n2     3            5\n\n\nIn [445]: a.merge(b, how=""left"")\nOut[445]: \n   col1  to_merge_on  col2\n0     1            1     1\n1     2            3     2\n2     3            4   NaN\n\nIn [446]: _.index\nOut[447]: Int64Index([0, 1, 2])\n']";"['In [441]: a=DataFrame(data={""col1"": [1,2,3], \'to_merge_on\' : [1,3,4]}, index=[""a"",""b"",""c""])\n\nIn [442]: b=DataFrame(data={""col2"": [1,2,3], \'to_merge_on\' : [1,3,5]})\nIn [443]: a\nOut[443]: \n   col1  to_merge_on\na     1            1\nb     2            3\nc     3            4\n\nIn [444]: b\nOut[444]: \n   col2  to_merge_on\n0     1            1\n1     2            3\n2     3            5\n\n\nIn [445]: a.merge(b, how=""left"")\nOut[445]: \n   col1  to_merge_on  col2\n0     1            1     1\n1     2            3     2\n2     3            4   NaN\n\nIn [446]: _.index\nOut[447]: Int64Index([0, 1, 2])\n']";"['DataFrames', 'In [441]: a=DataFrame(data={""col1"": [1,2,3], \'to_merge_on\' : [1,3,4]}, index=[""a"",""b"",""c""])\n\nIn [442]: b=DataFrame(data={""col2"": [1,2,3], \'to_merge_on\' : [1,3,5]})\nIn [443]: a\nOut[443]: \n   col1  to_merge_on\na     1            1\nb     2            3\nc     3            4\n\nIn [444]: b\nOut[444]: \n   col2  to_merge_on\n0     1            1\n1     2            3\n2     3            5\n\n\nIn [445]: a.merge(b, how=""left"")\nOut[445]: \n   col1  to_merge_on  col2\n0     1            1     1\n1     2            3     2\n2     3            4   NaN\n\nIn [446]: _.index\nOut[447]: Int64Index([0, 1, 2])\n']";"['a=DataFrame(data={""col1"": [1,2,3], \'to_merge_on\' : [1,3,4]}, index=[""a"",""b"",""c""])\n\nb\na.merge(b, how=""left"")\n_.index\n']";"['a=DataFrame(data={""col1"": [1,2,3], \'to_merge_on\' : [1,3,4]}, index=[""a"",""b"",""c""])\n\nb\na.merge(b, how=""left"")\n_.index\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\na=DataFrame(data={""col1"": [1,2,3], \'to_merge_on\' : [1,3,4]}, index=[""a"",""b"",""c""])\n\nb\na.merge(b, how=""left"")\n_.index\n']";True;0;1;"[""name 'a' is not defined""]";['NameError'];0;1;"[""name 'a' is not defined""]";['NameError'];0;1;"[""name 'a' is not defined""]";['NameError']
68;68;68;68;3.0;7;12021730;;1;21;<python><pandas>;Can pandas handle variable-length whitespace as column delimiters;8587.0;"['## sample data\nhead sample.txt\n\n#                                                                            --- full sequence --- -------------- this domain -------------   hmm coord   ali coord   env coord\n# target name        accession   tlen query name           accession   qlen   E-value  score  bias   #  of  c-Evalue  i-Evalue  score  bias  from    to  from    to  from    to  acc description of target\n#------------------- ---------- ----- -------------------- ---------- ----- --------- ------ ----- --- --- --------- --------- ------ ----- ----- ----- ----- ----- ----- ----- ---- ---------------------\nABC_membrane         PF00664.18   275 AAF67494.2_AF170880  -            615     8e-29  100.7  11.4   1   1     3e-32     1e-28  100.4   7.9     3   273    42   313    40   315 0.95 ABC transporter transmembrane region\nABC_tran             PF00005.22   118 AAF67494.2_AF170880  -            615   2.6e-20   72.8   0.0   1   1   1.9e-23   6.4e-20   71.5   0.0     1   118   402   527   402   527 0.93 ABC transporter\nSMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   1   2    0.0036        12    4.9   0.0    27    40   391   404   383   408 0.86 RecF/RecN/SMC N terminal domain\nSMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   2   2   1.8e-09   6.1e-06   25.4   0.0   116   210   461   568   428   575 0.85 RecF/RecN/SMC N terminal domain\nAAA_16               PF13191.1    166 AAF67494.2_AF170880  -            615   3.1e-06   27.5   0.3   1   1     2e-09     7e-06   26.4   0.2    20   158   386   544   376   556 0.72 AAA ATPase domain\nYceG                 PF02618.11   297 AAF67495.1_AF170880  -            284   3.4e-64  216.6   0.0   1   1   2.9e-68     4e-64  216.3   0.0    68   296    53   274    29   275 0.85 YceG-like family\nPyr_redox_3          PF13738.1    203 AAF67496.2_AF170880  -            352   2.9e-28   99.1   0.0   1   2   2.8e-30   4.8e-27   95.2   0.0     1   201     4   198     4   200 0.85 Pyridine nucleotide-disulphide oxidoreductase\n\n#load data\nfrom pandas import *\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep="" "")\n\nValueError: Expecting 83 columns, got 91 in row 4\n\n#load data part 2\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep=""\'s*\' "")\n#this mushes some of the columns into the first column and drops the rest.\n    X.1\n1    ABC_tran PF00005.22 118 AAF67494.2_\n2    SMC_N PF02463.14 220 AAF67494.2_\n3    SMC_N PF02463.14 220 AAF67494.2_\n4    AAA_16 PF13191.1 166 AAF67494.2_\n5    YceG PF02618.11 297 AAF67495.1_\n6    Pyr_redox_3 PF13738.1 203 AAF67496.2_\n7    Pyr_redox_3 PF13738.1 203 AAF67496.2_\n8    FMO-like PF00743.14 532 AAF67496.2_\n9    FMO-like PF00743.14 532 AAF67496.2_\n']";"['## sample data\nhead sample.txt\n\n#                                                                            --- full sequence --- -------------- this domain -------------   hmm coord   ali coord   env coord\n# target name        accession   tlen query name           accession   qlen   E-value  score  bias   #  of  c-Evalue  i-Evalue  score  bias  from    to  from    to  from    to  acc description of target\n#------------------- ---------- ----- -------------------- ---------- ----- --------- ------ ----- --- --- --------- --------- ------ ----- ----- ----- ----- ----- ----- ----- ---- ---------------------\nABC_membrane         PF00664.18   275 AAF67494.2_AF170880  -            615     8e-29  100.7  11.4   1   1     3e-32     1e-28  100.4   7.9     3   273    42   313    40   315 0.95 ABC transporter transmembrane region\nABC_tran             PF00005.22   118 AAF67494.2_AF170880  -            615   2.6e-20   72.8   0.0   1   1   1.9e-23   6.4e-20   71.5   0.0     1   118   402   527   402   527 0.93 ABC transporter\nSMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   1   2    0.0036        12    4.9   0.0    27    40   391   404   383   408 0.86 RecF/RecN/SMC N terminal domain\nSMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   2   2   1.8e-09   6.1e-06   25.4   0.0   116   210   461   568   428   575 0.85 RecF/RecN/SMC N terminal domain\nAAA_16               PF13191.1    166 AAF67494.2_AF170880  -            615   3.1e-06   27.5   0.3   1   1     2e-09     7e-06   26.4   0.2    20   158   386   544   376   556 0.72 AAA ATPase domain\nYceG                 PF02618.11   297 AAF67495.1_AF170880  -            284   3.4e-64  216.6   0.0   1   1   2.9e-68     4e-64  216.3   0.0    68   296    53   274    29   275 0.85 YceG-like family\nPyr_redox_3          PF13738.1    203 AAF67496.2_AF170880  -            352   2.9e-28   99.1   0.0   1   2   2.8e-30   4.8e-27   95.2   0.0     1   201     4   198     4   200 0.85 Pyridine nucleotide-disulphide oxidoreductase\n\n#load data\nfrom pandas import *\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep="" "")\n\nValueError: Expecting 83 columns, got 91 in row 4\n\n#load data part 2\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep=""\'s*\' "")\n#this mushes some of the columns into the first column and drops the rest.\n    X.1\n1    ABC_tran PF00005.22 118 AAF67494.2_\n2    SMC_N PF02463.14 220 AAF67494.2_\n3    SMC_N PF02463.14 220 AAF67494.2_\n4    AAA_16 PF13191.1 166 AAF67494.2_\n5    YceG PF02618.11 297 AAF67495.1_\n6    Pyr_redox_3 PF13738.1 203 AAF67496.2_\n7    Pyr_redox_3 PF13738.1 203 AAF67496.2_\n8    FMO-like PF00743.14 532 AAF67496.2_\n9    FMO-like PF00743.14 532 AAF67496.2_\n']";"[""'s*'"", '## sample data\nhead sample.txt\n\n#                                                                            --- full sequence --- -------------- this domain -------------   hmm coord   ali coord   env coord\n# target name        accession   tlen query name           accession   qlen   E-value  score  bias   #  of  c-Evalue  i-Evalue  score  bias  from    to  from    to  from    to  acc description of target\n#------------------- ---------- ----- -------------------- ---------- ----- --------- ------ ----- --- --- --------- --------- ------ ----- ----- ----- ----- ----- ----- ----- ---- ---------------------\nABC_membrane         PF00664.18   275 AAF67494.2_AF170880  -            615     8e-29  100.7  11.4   1   1     3e-32     1e-28  100.4   7.9     3   273    42   313    40   315 0.95 ABC transporter transmembrane region\nABC_tran             PF00005.22   118 AAF67494.2_AF170880  -            615   2.6e-20   72.8   0.0   1   1   1.9e-23   6.4e-20   71.5   0.0     1   118   402   527   402   527 0.93 ABC transporter\nSMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   1   2    0.0036        12    4.9   0.0    27    40   391   404   383   408 0.86 RecF/RecN/SMC N terminal domain\nSMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   2   2   1.8e-09   6.1e-06   25.4   0.0   116   210   461   568   428   575 0.85 RecF/RecN/SMC N terminal domain\nAAA_16               PF13191.1    166 AAF67494.2_AF170880  -            615   3.1e-06   27.5   0.3   1   1     2e-09     7e-06   26.4   0.2    20   158   386   544   376   556 0.72 AAA ATPase domain\nYceG                 PF02618.11   297 AAF67495.1_AF170880  -            284   3.4e-64  216.6   0.0   1   1   2.9e-68     4e-64  216.3   0.0    68   296    53   274    29   275 0.85 YceG-like family\nPyr_redox_3          PF13738.1    203 AAF67496.2_AF170880  -            352   2.9e-28   99.1   0.0   1   2   2.8e-30   4.8e-27   95.2   0.0     1   201     4   198     4   200 0.85 Pyridine nucleotide-disulphide oxidoreductase\n\n#load data\nfrom pandas import *\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep="" "")\n\nValueError: Expecting 83 columns, got 91 in row 4\n\n#load data part 2\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep=""\'s*\' "")\n#this mushes some of the columns into the first column and drops the rest.\n    X.1\n1    ABC_tran PF00005.22 118 AAF67494.2_\n2    SMC_N PF02463.14 220 AAF67494.2_\n3    SMC_N PF02463.14 220 AAF67494.2_\n4    AAA_16 PF13191.1 166 AAF67494.2_\n5    YceG PF02618.11 297 AAF67495.1_\n6    Pyr_redox_3 PF13738.1 203 AAF67496.2_\n7    Pyr_redox_3 PF13738.1 203 AAF67496.2_\n8    FMO-like PF00743.14 532 AAF67496.2_\n9    FMO-like PF00743.14 532 AAF67496.2_\n']";"['## sample data\n\n#                                                                            --- full sequence --- -------------- this domain -------------   hmm coord   ali coord   env coord\n# target name        accession   tlen query name           accession   qlen   E-value  score  bias   #  of  c-Evalue  i-Evalue  score  bias  from    to  from    to  from    to  acc description of target\n#------------------- ---------- ----- -------------------- ---------- ----- --------- ------ ----- --- --- --------- --------- ------ ----- ----- ----- ----- ----- ----- ----- ---- ---------------------\n\n#load data\nfrom pandas import *\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep="" "")\n\n\n#load data part 2\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep=""\'s*\' "")\n#this mushes some of the columns into the first column and drops the rest.\n']";"['## sample data\n\n#                                                                            --- full sequence --- -------------- this domain -------------   hmm coord   ali coord   env coord\n# target name        accession   tlen query name           accession   qlen   E-value  score  bias   #  of  c-Evalue  i-Evalue  score  bias  from    to  from    to  from    to  acc description of target\n#------------------- ---------- ----- -------------------- ---------- ----- --------- ------ ----- --- --- --------- --------- ------ ----- ----- ----- ----- ----- ----- ----- ---- ---------------------\n\n#load data\nfrom pandas import *\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep="" "")\n\n\n#load data part 2\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep=""\'s*\' "")\n#this mushes some of the columns into the first column and drops the rest.\n']";False;"['import pandas as pd\n## sample data\n\n#                                                                            --- full sequence --- -------------- this domain -------------   hmm coord   ali coord   env coord\n# target name        accession   tlen query name           accession   qlen   E-value  score  bias   #  of  c-Evalue  i-Evalue  score  bias  from    to  from    to  from    to  acc description of target\n#------------------- ---------- ----- -------------------- ---------- ----- --------- ------ ----- --- --- --------- --------- ------ ----- ----- ----- ----- ----- ----- ----- ---- ---------------------\n\n#load data\nfrom pandas import *\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep="" "")\n\n\n#load data part 2\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep=""\'s*\' "")\n#this mushes some of the columns into the first column and drops the rest.\n']";False;0;2;"[""name 'read_table' is not defined"", ""name 'read_table' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'read_table' is not defined"", ""name 'read_table' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'read_table' is not defined"", ""name 'read_table' is not defined""]";['NameError', 'NameError']
69;69;69;69;3.0;0;12021754;;1;13;<pandas>;How to slice a Pandas Data Frame by position?;21442.0;['>>> df.shape\n(1000,10)\n>>> my_slice = df.ix[10,:]\n>>> my_slice.shape\n(10,)\n'];['>>> df.shape\n(1000,10)\n>>> my_slice = df.ix[10,:]\n>>> my_slice.shape\n(10,)\n'];['>>> df.shape\n(1000,10)\n>>> my_slice = df.ix[10,:]\n>>> my_slice.shape\n(10,)\n', 'my_slice'];['(1000,10)\n(10,)\n'];['(1000,10)\n(10,)\n'];False;['import pandas as pd\n(1000,10)\n(10,)\n'];False;2;3;"[""name 'df' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess'];2;3;"[""name 'df' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess']
70;70;70;70;12.0;3;12047193;;1;43;<python><mysql><data-structures><pandas>;How to convert SQL Query result to PANDAS Data Structure?;69160.0;"['from sqlalchemy import create_engine\n\n\nengine2 = create_engine(\'mysql://THE DATABASE I AM ACCESSING\')\nconnection2 = engine2.connect()\ndataid = 1022\nresoverall = connection2.execute(""SELECT sum(BLABLA) AS BLA, sum(BLABLABLA2) AS BLABLABLA2, sum(SOME_INT) AS SOME_INT, sum(SOME_INT2) AS SOME_INT2, 100*sum(SOME_INT2)/sum(SOME_INT) AS ctr, sum(SOME_INT2)/sum(SOME_INT) AS cpc FROM daily_report_cooked WHERE campaign_id = \'%s\'""%dataid)\n']";"['from sqlalchemy import create_engine\n\n\nengine2 = create_engine(\'mysql://THE DATABASE I AM ACCESSING\')\nconnection2 = engine2.connect()\ndataid = 1022\nresoverall = connection2.execute(""SELECT sum(BLABLA) AS BLA, sum(BLABLABLA2) AS BLABLABLA2, sum(SOME_INT) AS SOME_INT, sum(SOME_INT2) AS SOME_INT2, 100*sum(SOME_INT2)/sum(SOME_INT) AS ctr, sum(SOME_INT2)/sum(SOME_INT) AS cpc FROM daily_report_cooked WHERE campaign_id = \'%s\'""%dataid)\n']";"['from sqlalchemy import create_engine\n\n\nengine2 = create_engine(\'mysql://THE DATABASE I AM ACCESSING\')\nconnection2 = engine2.connect()\ndataid = 1022\nresoverall = connection2.execute(""SELECT sum(BLABLA) AS BLA, sum(BLABLABLA2) AS BLABLABLA2, sum(SOME_INT) AS SOME_INT, sum(SOME_INT2) AS SOME_INT2, 100*sum(SOME_INT2)/sum(SOME_INT) AS ctr, sum(SOME_INT2)/sum(SOME_INT) AS cpc FROM daily_report_cooked WHERE campaign_id = \'%s\'""%dataid)\n']";"['from sqlalchemy import create_engine\n\n\nengine2 = create_engine(\'mysql://THE DATABASE I AM ACCESSING\')\nconnection2 = engine2.connect()\ndataid = 1022\nresoverall = connection2.execute(""SELECT sum(BLABLA) AS BLA, sum(BLABLABLA2) AS BLABLABLA2, sum(SOME_INT) AS SOME_INT, sum(SOME_INT2) AS SOME_INT2, 100*sum(SOME_INT2)/sum(SOME_INT) AS ctr, sum(SOME_INT2)/sum(SOME_INT) AS cpc FROM daily_report_cooked WHERE campaign_id = \'%s\'""%dataid)\n']";"['from sqlalchemy import create_engine\n\n\nengine2 = create_engine(\'mysql://THE DATABASE I AM ACCESSING\')\nconnection2 = engine2.connect()\ndataid = 1022\nresoverall = connection2.execute(""SELECT sum(BLABLA) AS BLA, sum(BLABLABLA2) AS BLABLABLA2, sum(SOME_INT) AS SOME_INT, sum(SOME_INT2) AS SOME_INT2, 100*sum(SOME_INT2)/sum(SOME_INT) AS ctr, sum(SOME_INT2)/sum(SOME_INT) AS cpc FROM daily_report_cooked WHERE campaign_id = \'%s\'""%dataid)\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\nfrom sqlalchemy import create_engine\n\n\nengine2 = create_engine(\'mysql://THE DATABASE I AM ACCESSING\')\nconnection2 = engine2.connect()\ndataid = 1022\nresoverall = connection2.execute(""SELECT sum(BLABLA) AS BLA, sum(BLABLABLA2) AS BLABLABLA2, sum(SOME_INT) AS SOME_INT, sum(SOME_INT2) AS SOME_INT2, 100*sum(SOME_INT2)/sum(SOME_INT) AS ctr, sum(SOME_INT2)/sum(SOME_INT) AS cpc FROM daily_report_cooked WHERE campaign_id = \'%s\'""%dataid)\n']";True;0;4;"[""No module named 'pydobc'"", ""name 'resoverall' is not defined"", ""name 'sql' is not defined"", ""name 'query' is not defined""]";['ImportError', 'NameError', 'NameError', 'NameError'];0;4;"[""No module named 'pydobc'"", ""name 'resoverall' is not defined"", ""name 'sql' is not defined"", ""name 'query' is not defined""]";['ImportError', 'NameError', 'NameError', 'NameError'];0;4;"[""No module named 'pydobc'"", ""name 'resoverall' is not defined"", ""name 'sql' is not defined"", ""name 'query' is not defined""]";['ImportError', 'NameError', 'NameError', 'NameError']
71;71;71;71;3.0;1;12052067;;1;13;<numpy><scipy><pandas>;If I use python pandas, is there any need for structured arrays?;2375.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
72;72;72;72;6.0;0;12065885;;1;223;<python><pandas><dataframe>;Filter dataframe rows if value in column is in a set list of values;129843.0;"[""rpt\n<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 47518 entries, ('000002', '20120331') to ('603366', '20091231')\nData columns:\nSTK_ID                    47518  non-null values\nSTK_Name                  47518  non-null values\nRPT_Date                  47518  non-null values\nsales                     47518  non-null values\n<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 25 entries, ('600809', '20120331') to ('600809', '20060331')\nData columns:\nSTK_ID                    25  non-null values\nSTK_Name                  25  non-null values\nRPT_Date                  25  non-null values\nsales                     25  non-null values\nstk_list = ['600809','600141','600329']\n\nrst = rpt[rpt['STK_ID'] in stk_list] # this does not works in pandas \n""]";"[""rpt\n<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 47518 entries, ('000002', '20120331') to ('603366', '20091231')\nData columns:\nSTK_ID                    47518  non-null values\nSTK_Name                  47518  non-null values\nRPT_Date                  47518  non-null values\nsales                     47518  non-null values\n"", ""<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 25 entries, ('600809', '20120331') to ('600809', '20060331')\nData columns:\nSTK_ID                    25  non-null values\nSTK_Name                  25  non-null values\nRPT_Date                  25  non-null values\nsales                     25  non-null values\n"", ""stk_list = ['600809','600141','600329']\n\nrst = rpt[rpt['STK_ID'] in stk_list] # this does not works in pandas \n""]";"['rpt', ""rpt\n<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 47518 entries, ('000002', '20120331') to ('603366', '20091231')\nData columns:\nSTK_ID                    47518  non-null values\nSTK_Name                  47518  non-null values\nRPT_Date                  47518  non-null values\nsales                     47518  non-null values\n"", ""'600809'"", ""rpt[rpt['STK_ID'] == '600809']"", ""<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 25 entries, ('600809', '20120331') to ('600809', '20060331')\nData columns:\nSTK_ID                    25  non-null values\nSTK_Name                  25  non-null values\nRPT_Date                  25  non-null values\nsales                     25  non-null values\n"", ""['600809','600141','600329']"", ""stk_list = ['600809','600141','600329']\n\nrst = rpt[rpt['STK_ID'] in stk_list] # this does not works in pandas \n""]";"[""rpt\nstk_list = ['600809','600141','600329']\n\nrst = rpt[rpt['STK_ID'] in stk_list] # this does not works in pandas \n""]";"[""rpt\nstk_list = ['600809','600141','600329']\n\nrst = rpt[rpt['STK_ID'] in stk_list] # this does not works in pandas \n""]";False;"[""import pandas as pd\nrpt\nstk_list = ['600809','600141','600329']\n\nrst = rpt[rpt['STK_ID'] in stk_list] # this does not works in pandas \n""]";False;1;4;"['Sucess', ""name 'df' is not defined"", ""type object 'str' has no attribute 'contains'"", ""name 'rpt' is not defined""]";['Sucess', 'NameError', 'AttributeError', 'NameError'];1;4;"['Sucess', ""name 'df' is not defined"", ""type object 'str' has no attribute 'contains'"", ""name 'rpt' is not defined""]";['Sucess', 'NameError', 'AttributeError', 'NameError'];1;4;"['Sucess', ""'a'"", ""type object 'str' has no attribute 'contains'"", ""name 'rpt' is not defined""]";['Sucess', 'KeyError', 'AttributeError', 'NameError']
73;73;73;73;1.0;1;12096252;;1;241;<python><pandas>;use a list of values to select rows from a pandas dataframe;178009.0;"[""df = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})\ndf\n\n     A   B\n0    5   1\n1    6   2\n2    3   3\n3    4   5\nx = df[df['A'] == 3]\nx\n\n     A   B\n2    3   3\nlist_of_values = [3,6]\n\ny = df[df['A'] in list_of_values]\n""]";"[""df = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})\ndf\n\n     A   B\n0    5   1\n1    6   2\n2    3   3\n3    4   5\n"", ""x = df[df['A'] == 3]\nx\n\n     A   B\n2    3   3\n"", ""list_of_values = [3,6]\n\ny = df[df['A'] in list_of_values]\n""]";"[""df = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})\ndf\n\n     A   B\n0    5   1\n1    6   2\n2    3   3\n3    4   5\n"", ""x = df[df['A'] == 3]\nx\n\n     A   B\n2    3   3\n"", ""list_of_values = [3,6]\n\ny = df[df['A'] in list_of_values]\n""]";"[""df = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})\ndf\n\nx = df[df['A'] == 3]\nx\n\nlist_of_values = [3,6]\n\ny = df[df['A'] in list_of_values]\n""]";"[""from pandas import DataFrame\ndf = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})\ndf\n\nx = df[df['A'] == 3]\nx\n\nlist_of_values = [3,6]\n\ny = df[df['A'] in list_of_values]\n""]";True;"[""import pandas as pd\ndf = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})\ndf\n\nx = df[df['A'] == 3]\nx\n\nlist_of_values = [3,6]\n\ny = df[df['A'] in list_of_values]\n""]";False;0;1;"[""name 'DataFrame' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];0;1;"[""name 'DataFrame' is not defined""]";['NameError']
74;74;74;74;2.0;0;12100497;;1;16;<python><pandas>;pandas: set values with (row, col) indices;25380.0;"[""In [49]: index = ['a', 'b', 'c', 'd']\n\nIn [50]: columns = ['one', 'two', 'three', 'four']\n\nIn [51]: M = pandas.DataFrame(np.random.randn(4,4), index=index, columns=columns)\n\nIn [52]: M\nOut[52]: \n        one       two     three      four\na -0.785841 -0.538572  0.376594  1.316647\nb  0.530288 -0.975547  1.063946 -1.049940\nc -0.794447 -0.886721  1.794326 -0.714834\nd -0.158371  0.069357 -1.003039 -0.807431\n\nIn [53]: M.lookup(index, columns) # diagonal entries\nOut[53]: array([-0.78584142, -0.97554698,  1.79432641, -0.8074308 ])\n""]";"[""In [49]: index = ['a', 'b', 'c', 'd']\n\nIn [50]: columns = ['one', 'two', 'three', 'four']\n\nIn [51]: M = pandas.DataFrame(np.random.randn(4,4), index=index, columns=columns)\n\nIn [52]: M\nOut[52]: \n        one       two     three      four\na -0.785841 -0.538572  0.376594  1.316647\nb  0.530288 -0.975547  1.063946 -1.049940\nc -0.794447 -0.886721  1.794326 -0.714834\nd -0.158371  0.069357 -1.003039 -0.807431\n\nIn [53]: M.lookup(index, columns) # diagonal entries\nOut[53]: array([-0.78584142, -0.97554698,  1.79432641, -0.8074308 ])\n""]";"['pandas', ""In [49]: index = ['a', 'b', 'c', 'd']\n\nIn [50]: columns = ['one', 'two', 'three', 'four']\n\nIn [51]: M = pandas.DataFrame(np.random.randn(4,4), index=index, columns=columns)\n\nIn [52]: M\nOut[52]: \n        one       two     three      four\na -0.785841 -0.538572  0.376594  1.316647\nb  0.530288 -0.975547  1.063946 -1.049940\nc -0.794447 -0.886721  1.794326 -0.714834\nd -0.158371  0.069357 -1.003039 -0.807431\n\nIn [53]: M.lookup(index, columns) # diagonal entries\nOut[53]: array([-0.78584142, -0.97554698,  1.79432641, -0.8074308 ])\n"", 'M']";"[""index = ['a', 'b', 'c', 'd']\n\n\n\nM.lookup(index, columns) # diagonal entries\n""]";"[""index = ['a', 'b', 'c', 'd']\n\n\n\nM.lookup(index, columns) # diagonal entries\n""]";False;"[""import pandas as pd\nindex = ['a', 'b', 'c', 'd']\n\n\n\nM.lookup(index, columns) # diagonal entries\n""]";False;0;2;"[""name 'M' is not defined"", ""name 'M' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'M' is not defined"", ""name 'M' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'M' is not defined"", ""name 'M' is not defined""]";['NameError', 'NameError']
75;75;75;75;2.0;5;12101113;;1;12;<python><pandas>;Prevent pandas from automatically inferring type in read_csv;5964.0;"[""In [149]: d = pandas.read_csv('resources/names/fos_names.csv',  sep='#', header=None, names=['int_field', 'floatlike_field', 'str_field'])\n\nIn [150]: d\nOut[150]: \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1673 entries, 0 to 1672\nData columns:\nint_field          1673  non-null values\nfloatlike_field    1673  non-null values\nstr_field          1673  non-null values\ndtypes: float64(1), int64(1), object(1)\n""]";"[""In [149]: d = pandas.read_csv('resources/names/fos_names.csv',  sep='#', header=None, names=['int_field', 'floatlike_field', 'str_field'])\n\nIn [150]: d\nOut[150]: \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1673 entries, 0 to 1672\nData columns:\nint_field          1673  non-null values\nfloatlike_field    1673  non-null values\nstr_field          1673  non-null values\ndtypes: float64(1), int64(1), object(1)\n""]";"['pandas.read_csv', ""In [149]: d = pandas.read_csv('resources/names/fos_names.csv',  sep='#', header=None, names=['int_field', 'floatlike_field', 'str_field'])\n\nIn [150]: d\nOut[150]: \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1673 entries, 0 to 1672\nData columns:\nint_field          1673  non-null values\nfloatlike_field    1673  non-null values\nstr_field          1673  non-null values\ndtypes: float64(1), int64(1), object(1)\n"", 'pandas', 'converters', 'pandas']";"[""d = pandas.read_csv('resources/names/fos_names.csv',  sep='#', header=None, names=['int_field', 'floatlike_field', 'str_field'])\n\n""]";"[""d = pandas.read_csv('resources/names/fos_names.csv',  sep='#', header=None, names=['int_field', 'floatlike_field', 'str_field'])\n\n""]";False;"[""import pandas as pd\nd = pandas.read_csv('resources/names/fos_names.csv',  sep='#', header=None, names=['int_field', 'floatlike_field', 'str_field'])\n\n""]";False;1;2;"[""name 'pandas' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'pandas' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'pandas' is not defined"", 'Sucess']";['NameError', 'Sucess']
76;76;76;76;1.0;0;12133075;;1;19;<python><pandas>;pandas - how to sort the result of DataFrame.groupby.mean()?;5843.0;['means = df.testColumn.groupby(df.testCategory).mean()\nmeans.sort()\n...\n-> Exception: This Series is a view of some other array, to sort in-place you must create a copy\nmeansCopy = Series(means)\nmeansCopy.sort()\n-> Exception: This Series is a view of some other array, to sort in-place you must create a copy\n'];['means = df.testColumn.groupby(df.testCategory).mean()\n', 'means.sort()\n...\n-> Exception: This Series is a view of some other array, to sort in-place you must create a copy\n', 'meansCopy = Series(means)\nmeansCopy.sort()\n-> Exception: This Series is a view of some other array, to sort in-place you must create a copy\n'];['means = df.testColumn.groupby(df.testCategory).mean()\n', 'means.sort()\n...\n-> Exception: This Series is a view of some other array, to sort in-place you must create a copy\n', 'meansCopy = Series(means)\nmeansCopy.sort()\n-> Exception: This Series is a view of some other array, to sort in-place you must create a copy\n'];['means = df.testColumn.groupby(df.testCategory).mean()\nmeans.sort()\n...\nmeansCopy = Series(means)\nmeansCopy.sort()\n'];['means = df.testColumn.groupby(df.testCategory).mean()\nmeans.sort()\n...\nmeansCopy = Series(means)\nmeansCopy.sort()\n'];False;['import pandas as pd\ndf = pd.DataFrame()\nmeans = df.testColumn.groupby(df.testCategory).mean()\nmeans.sort()\n...\nmeansCopy = Series(means)\nmeansCopy.sort()\n'];True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
77;77;77;77;2.0;0;12152716;;1;20;<python><pandas><equivalent>;Python pandas equivalent for replace;20843.0;[''];[];"['replace', ""replace(df$column, df$column==1,'Type 1');"", 'apply', 'np.where', 'data_frame.values']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'DataFrame' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];0;1;"[""name 'DataFrame' is not defined""]";['NameError']
78;78;78;78;3.0;0;12168648;;1;12;<python><indexing><dataframe><pandas>;Pandas (python): How to add column to dataframe for index?;37697.0;['Int64Index([171, 174,173, 172, 199..............\n        ....175, 200])\n[1, 2, 3, 4, 5......................., 30]\n'];['Int64Index([171, 174,173, 172, 199..............\n        ....175, 200])\n', '[1, 2, 3, 4, 5......................., 30]\n'];['Int64Index([171, 174,173, 172, 199..............\n        ....175, 200])\n', '[1, 2, 3, 4, 5......................., 30]\n'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess']
79;79;79;79;1.0;0;12169170;;1;13;<python><dataframe><pandas>;How should I take the max of 2 columns in a dataframe and make it another column?;11405.0;[''];[];['A', 'B', 'C', 'C = max(A, B)'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
80;80;80;80;3.0;1;12182744;;1;49;<python><pandas><apply>;python pandas: apply a function with arguments to a series;45364.0;['x = my_series.apply(my_function, more_arguments_1)\ny = my_series.apply(my_function, more_arguments_2)\n...\n'];['x = my_series.apply(my_function, more_arguments_1)\ny = my_series.apply(my_function, more_arguments_2)\n...\n'];['x = my_series.apply(my_function, more_arguments_1)\ny = my_series.apply(my_function, more_arguments_2)\n...\n'];['x = my_series.apply(my_function, more_arguments_1)\ny = my_series.apply(my_function, more_arguments_2)\n...\n'];['x = my_series.apply(my_function, more_arguments_1)\ny = my_series.apply(my_function, more_arguments_2)\n...\n'];False;['import pandas as pd\nx = my_series.apply(my_function, more_arguments_1)\ny = my_series.apply(my_function, more_arguments_2)\n...\n'];False;0;2;"[""name 'my_series' is not defined"", ""name 'Series' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'my_series' is not defined"", ""name 'Series' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'my_series' is not defined"", ""name 'Series' is not defined""]";['NameError', 'NameError']
81;81;81;81;5.0;1;12190874;;1;59;<python><partitioning><pandas>;Pandas: Sampling a DataFrame;57789.0;['rows = data.index\nrow_count = len(rows)\nrandom.shuffle(list(rows))\n\ndata.reindex(rows)\n\ntraining_data = data[row_count // 10:]\ntesting_data = data[:row_count // 10]\nIndexError: each subindex must be either a slice, an integer, Ellipsis, or newaxis\n'];['rows = data.index\nrow_count = len(rows)\nrandom.shuffle(list(rows))\n\ndata.reindex(rows)\n\ntraining_data = data[row_count // 10:]\ntesting_data = data[:row_count // 10]\n', 'IndexError: each subindex must be either a slice, an integer, Ellipsis, or newaxis\n'];['rows = data.index\nrow_count = len(rows)\nrandom.shuffle(list(rows))\n\ndata.reindex(rows)\n\ntraining_data = data[row_count // 10:]\ntesting_data = data[:row_count // 10]\n', 'sklearn', 'IndexError: each subindex must be either a slice, an integer, Ellipsis, or newaxis\n'];['rows = data.index\nrow_count = len(rows)\nrandom.shuffle(list(rows))\n\ndata.reindex(rows)\n\ntraining_data = data[row_count // 10:]\ntesting_data = data[:row_count // 10]\n'];['rows = data.index\nrow_count = len(rows)\nrandom.shuffle(list(rows))\n\ndata.reindex(rows)\n\ntraining_data = data[row_count // 10:]\ntesting_data = data[:row_count // 10]\n'];False;['import pandas as pd\nrows = data.index\nrow_count = len(rows)\nrandom.shuffle(list(rows))\n\ndata.reindex(rows)\n\ntraining_data = data[row_count // 10:]\ntesting_data = data[:row_count // 10]\n'];False;3;5;"['Sucess', ""name 'np' is not defined"", 'Sucess', 'Sucess', ""name 'your_dataframe' is not defined""]";['Sucess', 'NameError', 'Sucess', 'Sucess', 'NameError'];3;5;"['Sucess', ""name 'np' is not defined"", 'Sucess', 'Sucess', ""name 'your_dataframe' is not defined""]";['Sucess', 'NameError', 'Sucess', 'Sucess', 'NameError'];3;5;"['Sucess', ""name 'np' is not defined"", 'Sucess', 'Sucess', ""name 'your_dataframe' is not defined""]";['Sucess', 'NameError', 'Sucess', 'Sucess', 'NameError']
82;82;82;82;5.0;3;12200693;;1;34;<python><group-by><dataframe><pandas>;Python Pandas How to assign groupby operation results back to columns in parent dataframe?;28220.0;"['In [261]: bdata\nOut[261]:\n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 21210 entries, 0 to 21209\nData columns:\nBloombergTicker      21206  non-null values\nCompany              21210  non-null values\nCountry              21210  non-null values\nMarketCap            21210  non-null values\nPriceReturn          21210  non-null values\nSEDOL                21210  non-null values\nyearmonth            21210  non-null values\ndtypes: float64(2), int64(1), object(4)\nIn [262]: bdata.groupby(""yearmonth"").apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\nOut[262]:\nyearmonth\n201204      -0.109444\n201205      -0.290546\nIn [263]: dateGrps = bdata.groupby(""yearmonth"")\n\nIn [264]: dateGrps[""MarketReturn""] = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/mnt/bos-devrnd04/usr6/home/espears/ws/Research/Projects/python-util/src/util/<ipython-input-264-4a68c8782426> in <module>()\n----> 1 dateGrps[""MarketReturn""] = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n\nTypeError: \'DataFrameGroupBy\' object does not support item assignment\nmarketRetsByDate  = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n\nbdata[""MarketReturn""] = np.repeat(np.NaN, len(bdata))\n\nfor elem in marketRetsByDate.index.values:\n    bdata[""MarketReturn""][bdata[""yearmonth""]==elem] = marketRetsByDate.ix[elem]\n']";"[""In [261]: bdata\nOut[261]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 21210 entries, 0 to 21209\nData columns:\nBloombergTicker      21206  non-null values\nCompany              21210  non-null values\nCountry              21210  non-null values\nMarketCap            21210  non-null values\nPriceReturn          21210  non-null values\nSEDOL                21210  non-null values\nyearmonth            21210  non-null values\ndtypes: float64(2), int64(1), object(4)\n"", 'In [262]: bdata.groupby(""yearmonth"").apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\nOut[262]:\nyearmonth\n201204      -0.109444\n201205      -0.290546\n', 'In [263]: dateGrps = bdata.groupby(""yearmonth"")\n\nIn [264]: dateGrps[""MarketReturn""] = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/mnt/bos-devrnd04/usr6/home/espears/ws/Research/Projects/python-util/src/util/<ipython-input-264-4a68c8782426> in <module>()\n----> 1 dateGrps[""MarketReturn""] = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n\nTypeError: \'DataFrameGroupBy\' object does not support item assignment\n', 'marketRetsByDate  = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n\nbdata[""MarketReturn""] = np.repeat(np.NaN, len(bdata))\n\nfor elem in marketRetsByDate.index.values:\n    bdata[""MarketReturn""][bdata[""yearmonth""]==elem] = marketRetsByDate.ix[elem]\n']";"[""In [261]: bdata\nOut[261]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 21210 entries, 0 to 21209\nData columns:\nBloombergTicker      21206  non-null values\nCompany              21210  non-null values\nCountry              21210  non-null values\nMarketCap            21210  non-null values\nPriceReturn          21210  non-null values\nSEDOL                21210  non-null values\nyearmonth            21210  non-null values\ndtypes: float64(2), int64(1), object(4)\n"", 'In [262]: bdata.groupby(""yearmonth"").apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\nOut[262]:\nyearmonth\n201204      -0.109444\n201205      -0.290546\n', 'In [263]: dateGrps = bdata.groupby(""yearmonth"")\n\nIn [264]: dateGrps[""MarketReturn""] = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/mnt/bos-devrnd04/usr6/home/espears/ws/Research/Projects/python-util/src/util/<ipython-input-264-4a68c8782426> in <module>()\n----> 1 dateGrps[""MarketReturn""] = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n\nTypeError: \'DataFrameGroupBy\' object does not support item assignment\n', 'marketRetsByDate  = dateGrps.apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n\nbdata[""MarketReturn""] = np.repeat(np.NaN, len(bdata))\n\nfor elem in marketRetsByDate.index.values:\n    bdata[""MarketReturn""][bdata[""yearmonth""]==elem] = marketRetsByDate.ix[elem]\n']";"['bdata\nbdata.groupby(""yearmonth"").apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n']";"['bdata\nbdata.groupby(""yearmonth"").apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\nbdata\nbdata.groupby(""yearmonth"").apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""].sum()).sum())\n']";True;1;3;"[""name 'pandas' is not defined"", ""name 'df' is not defined"", 'Sucess']";['NameError', 'NameError', 'Sucess'];1;3;"[""name 'pandas' is not defined"", ""name 'df' is not defined"", 'Sucess']";['NameError', 'NameError', 'Sucess'];2;3;"[""name 'pandas' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess']
83;83;83;83;1.0;0;12203901;;1;12;<python><pandas>;pandas crashes on repeated DataFrame.reset_index();5744.0;"['import pandas\nA = pandas.DataFrame({\n    \'val\' :  [\'aaaaa\', \'acaca\', \'ddddd\', \'zzzzz\'],\n    \'extra\' : range(10,14),\n})\nA = A.reset_index()\nA = A.reset_index()\nA = A.reset_index()\n....\n    A = A.reset_index()\n  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py"", line 2393, in reset_index\n    new_obj.insert(0, name, _maybe_cast(self.index.values))\n  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py"", line 1787, in insert\n    self._data.insert(loc, column, value)\n  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/internals.py"", line 893, in insert\n    raise Exception(\'cannot insert %s, already exists\' % item)\nException: cannot insert level_0, already exists\n']";"[""import pandas\nA = pandas.DataFrame({\n    'val' :  ['aaaaa', 'acaca', 'ddddd', 'zzzzz'],\n    'extra' : range(10,14),\n})\nA = A.reset_index()\nA = A.reset_index()\nA = A.reset_index()\n"", '....\n    A = A.reset_index()\n  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py"", line 2393, in reset_index\n    new_obj.insert(0, name, _maybe_cast(self.index.values))\n  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py"", line 1787, in insert\n    self._data.insert(loc, column, value)\n  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/internals.py"", line 893, in insert\n    raise Exception(\'cannot insert %s, already exists\' % item)\nException: cannot insert level_0, already exists\n']";"[""import pandas\nA = pandas.DataFrame({\n    'val' :  ['aaaaa', 'acaca', 'ddddd', 'zzzzz'],\n    'extra' : range(10,14),\n})\nA = A.reset_index()\nA = A.reset_index()\nA = A.reset_index()\n"", '....\n    A = A.reset_index()\n  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py"", line 2393, in reset_index\n    new_obj.insert(0, name, _maybe_cast(self.index.values))\n  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py"", line 1787, in insert\n    self._data.insert(loc, column, value)\n  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/internals.py"", line 893, in insert\n    raise Exception(\'cannot insert %s, already exists\' % item)\nException: cannot insert level_0, already exists\n']";['import pandas\nA = A.reset_index()\nA = A.reset_index()\nA = A.reset_index()\n'];['import pandas\nA = A.reset_index()\nA = A.reset_index()\nA = A.reset_index()\n'];False;['import pandas as pd\nimport pandas\nA = A.reset_index()\nA = A.reset_index()\nA = A.reset_index()\n'];False;0;1;"[""name 'A' is not defined""]";['NameError'];0;1;"[""name 'A' is not defined""]";['NameError'];0;1;"[""name 'A' is not defined""]";['NameError']
84;84;84;84;2.0;0;12207326;;1;60;<python><statistics><pandas><frequency>;Frequency table for a single variable;54196.0;['my_series = pandas.Series([1,2,2,3,3,3])\npandas.magical_frequency_function( my_series )\n\n>> {\n     1 : 1,\n     2 : 2, \n     3 : 3\n   }\n'];['my_series = pandas.Series([1,2,2,3,3,3])\npandas.magical_frequency_function( my_series )\n\n>> {\n     1 : 1,\n     2 : 2, \n     3 : 3\n   }\n'];['my_series = pandas.Series([1,2,2,3,3,3])\npandas.magical_frequency_function( my_series )\n\n>> {\n     1 : 1,\n     2 : 2, \n     3 : 3\n   }\n'];['my_series = pandas.Series([1,2,2,3,3,3])\npandas.magical_frequency_function( my_series )\n\n'];['my_series = pandas.Series([1,2,2,3,3,3])\npandas.magical_frequency_function( my_series )\n\n'];False;['import pandas as pd\nmy_series = pandas.Series([1,2,2,3,3,3])\npandas.magical_frequency_function( my_series )\n\n'];False;1;2;"['Sucess', ""name 'my_series' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'my_series' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'my_series' is not defined""]";['Sucess', 'NameError']
85;85;85;85;2.0;2;12250024;;1;19;<python><excel><pandas><xlrd>;How to obtain sheet names from XLS files without loading the whole file?;16981.0;['xls = pandas.ExcelFile(path)\nsheets = xls.sheet_names\nxls = xlrd.open_workbook(path)\nsheets = xls.sheet_names\n'];['xls = pandas.ExcelFile(path)\nsheets = xls.sheet_names\n', 'xls = xlrd.open_workbook(path)\nsheets = xls.sheet_names\n'];['xls = pandas.ExcelFile(path)\nsheets = xls.sheet_names\n', 'xls = xlrd.open_workbook(path)\nsheets = xls.sheet_names\n'];['xls = pandas.ExcelFile(path)\nsheets = xls.sheet_names\nxls = xlrd.open_workbook(path)\nsheets = xls.sheet_names\n'];['xls = pandas.ExcelFile(path)\nsheets = xls.sheet_names\nxls = xlrd.open_workbook(path)\nsheets = xls.sheet_names\n'];False;['import pandas as pd\nxls = pandas.ExcelFile(path)\nsheets = xls.sheet_names\nxls = xlrd.open_workbook(path)\nsheets = xls.sheet_names\n'];False;0;1;"[""No module named 'xlrd'""]";['ImportError'];0;1;"[""No module named 'xlrd'""]";['ImportError'];0;1;"[""No module named 'xlrd'""]";['ImportError']
86;86;86;86;5.0;3;12278347;;1;13;<python><r><pandas><data.table><hdf5>;How can I efficiently save a python pandas dataframe in hdf5 and open it as a dataframe in R?;10677.0;"['import pandas \nimport numpy\nd = pandas.HDFStore(\'data.h5\')\nd[\'testdata\'] = pandas.DataFrame({\'N\': numpy.random.randn(5)})\nd.close()\n> library(hdf5)\n> hdf5load(\'data.h5\')\nNULL\n> testdata\n$block0_values\n         [,1]      [,2]      [,3]       [,4]      [,5]\n[1,] 1.498147 0.8843877 -1.081656 0.08717049 -1.302641\nattr(,""CLASS"")\n[1] ""ARRAY""\nattr(,""VERSION"")\n[1] ""2.3""\nattr(,""TITLE"")\n[1] """"\nattr(,""FLAVOR"")\n[1] ""numpy""\n\n$block0_items\n[1] ""N""\nattr(,""CLASS"")\n[1] ""ARRAY""\nattr(,""VERSION"")\n[1] ""2.3""\nattr(,""TITLE"")\n[1] """"\nattr(,""FLAVOR"")\n[1] ""numpy""\nattr(,""kind"")\n[1] ""string""\nattr(,""name"")\n[1] ""N.""\n\n$axis1\n[1] 0 1 2 3 4\nattr(,""CLASS"")\n[1] ""ARRAY""\nattr(,""VERSION"")\n[1] ""2.3""\nattr(,""TITLE"")\n[1] """"\nattr(,""FLAVOR"")\n[1] ""numpy""\nattr(,""kind"")\n[1] ""integer""\nattr(,""name"")\n[1] ""N.""\n\n$axis0\n[1] ""N""\nattr(,""CLASS"")\n[1] ""ARRAY""\nattr(,""VERSION"")\n[1] ""2.3""\nattr(,""TITLE"")\n[1] """"\nattr(,""FLAVOR"")\n[1] ""numpy""\nattr(,""kind"")\n[1] ""string""\nattr(,""name"")\n[1] ""N.""\n\nattr(,""TITLE"")\n[1] """"\nattr(,""CLASS"")\n[1] ""GROUP""\nattr(,""VERSION"")\n[1] ""1.0""\nattr(,""ndim"")\n[1] 2\nattr(,""axis0_variety"")\n[1] ""regular""\nattr(,""axis1_variety"")\n[1] ""regular""\nattr(,""nblocks"")\n[1] 1\nattr(,""block0_items_variety"")\n[1] ""regular""\nattr(,""pandas_type"")\n[1] ""frame""\n']";"[""import pandas \nimport numpy\nd = pandas.HDFStore('data.h5')\nd['testdata'] = pandas.DataFrame({'N': numpy.random.randn(5)})\nd.close()\n"", '> library(hdf5)\n> hdf5load(\'data.h5\')\nNULL\n> testdata\n$block0_values\n         [,1]      [,2]      [,3]       [,4]      [,5]\n[1,] 1.498147 0.8843877 -1.081656 0.08717049 -1.302641\nattr(,""CLASS"")\n[1] ""ARRAY""\nattr(,""VERSION"")\n[1] ""2.3""\nattr(,""TITLE"")\n[1] """"\nattr(,""FLAVOR"")\n[1] ""numpy""\n\n$block0_items\n[1] ""N""\nattr(,""CLASS"")\n[1] ""ARRAY""\nattr(,""VERSION"")\n[1] ""2.3""\nattr(,""TITLE"")\n[1] """"\nattr(,""FLAVOR"")\n[1] ""numpy""\nattr(,""kind"")\n[1] ""string""\nattr(,""name"")\n[1] ""N.""\n\n$axis1\n[1] 0 1 2 3 4\nattr(,""CLASS"")\n[1] ""ARRAY""\nattr(,""VERSION"")\n[1] ""2.3""\nattr(,""TITLE"")\n[1] """"\nattr(,""FLAVOR"")\n[1] ""numpy""\nattr(,""kind"")\n[1] ""integer""\nattr(,""name"")\n[1] ""N.""\n\n$axis0\n[1] ""N""\nattr(,""CLASS"")\n[1] ""ARRAY""\nattr(,""VERSION"")\n[1] ""2.3""\nattr(,""TITLE"")\n[1] """"\nattr(,""FLAVOR"")\n[1] ""numpy""\nattr(,""kind"")\n[1] ""string""\nattr(,""name"")\n[1] ""N.""\n\nattr(,""TITLE"")\n[1] """"\nattr(,""CLASS"")\n[1] ""GROUP""\nattr(,""VERSION"")\n[1] ""1.0""\nattr(,""ndim"")\n[1] 2\nattr(,""axis0_variety"")\n[1] ""regular""\nattr(,""axis1_variety"")\n[1] ""regular""\nattr(,""nblocks"")\n[1] 1\nattr(,""block0_items_variety"")\n[1] ""regular""\nattr(,""pandas_type"")\n[1] ""frame""\n']";"[""import pandas \nimport numpy\nd = pandas.HDFStore('data.h5')\nd['testdata'] = pandas.DataFrame({'N': numpy.random.randn(5)})\nd.close()\n"", '> library(hdf5)\n> hdf5load(\'data.h5\')\nNULL\n> testdata\n$block0_values\n         [,1]      [,2]      [,3]       [,4]      [,5]\n[1,] 1.498147 0.8843877 -1.081656 0.08717049 -1.302641\nattr(,""CLASS"")\n[1] ""ARRAY""\nattr(,""VERSION"")\n[1] ""2.3""\nattr(,""TITLE"")\n[1] """"\nattr(,""FLAVOR"")\n[1] ""numpy""\n\n$block0_items\n[1] ""N""\nattr(,""CLASS"")\n[1] ""ARRAY""\nattr(,""VERSION"")\n[1] ""2.3""\nattr(,""TITLE"")\n[1] """"\nattr(,""FLAVOR"")\n[1] ""numpy""\nattr(,""kind"")\n[1] ""string""\nattr(,""name"")\n[1] ""N.""\n\n$axis1\n[1] 0 1 2 3 4\nattr(,""CLASS"")\n[1] ""ARRAY""\nattr(,""VERSION"")\n[1] ""2.3""\nattr(,""TITLE"")\n[1] """"\nattr(,""FLAVOR"")\n[1] ""numpy""\nattr(,""kind"")\n[1] ""integer""\nattr(,""name"")\n[1] ""N.""\n\n$axis0\n[1] ""N""\nattr(,""CLASS"")\n[1] ""ARRAY""\nattr(,""VERSION"")\n[1] ""2.3""\nattr(,""TITLE"")\n[1] """"\nattr(,""FLAVOR"")\n[1] ""numpy""\nattr(,""kind"")\n[1] ""string""\nattr(,""name"")\n[1] ""N.""\n\nattr(,""TITLE"")\n[1] """"\nattr(,""CLASS"")\n[1] ""GROUP""\nattr(,""VERSION"")\n[1] ""1.0""\nattr(,""ndim"")\n[1] 2\nattr(,""axis0_variety"")\n[1] ""regular""\nattr(,""axis1_variety"")\n[1] ""regular""\nattr(,""nblocks"")\n[1] 1\nattr(,""block0_items_variety"")\n[1] ""regular""\nattr(,""pandas_type"")\n[1] ""frame""\n']";"[""import pandas \nimport numpy\nd = pandas.HDFStore('data.h5')\nd['testdata'] = pandas.DataFrame({'N': numpy.random.randn(5)})\nd.close()\nNULL\n\n\n\n\n""]";"[""import pandas \nimport numpy\nd = pandas.HDFStore('data.h5')\nd['testdata'] = pandas.DataFrame({'N': numpy.random.randn(5)})\nd.close()\nNULL\n\n\n\n\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nimport pandas \nimport numpy\nd = pandas.HDFStore('data.h5')\nd['testdata'] = pandas.DataFrame({'N': numpy.random.randn(5)})\nd.close()\nNULL\n\n\n\n\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
87;87;87;87;3.0;1;12286607;;1;44;<python><pandas><heatmap>;python Making heatmap from DataFrame;34778.0;"[""import numpy as np \nfrom pandas import *\n\nIndex= ['aaa','bbb','ccc','ddd','eee']\nCols = ['A', 'B', 'C','D']\ndf = DataFrame(abs(np.random.randn(5, 4)), index= Index, columns=Cols)\n\n>>> df\n          A         B         C         D\naaa  2.431645  1.248688  0.267648  0.613826\nbbb  0.809296  1.671020  1.564420  0.347662\nccc  1.501939  1.126518  0.702019  1.596048\nddd  0.137160  0.147368  1.504663  0.202822\neee  0.134540  3.708104  0.309097  1.641090\n>>> \n""]";"[""import numpy as np \nfrom pandas import *\n\nIndex= ['aaa','bbb','ccc','ddd','eee']\nCols = ['A', 'B', 'C','D']\ndf = DataFrame(abs(np.random.randn(5, 4)), index= Index, columns=Cols)\n\n>>> df\n          A         B         C         D\naaa  2.431645  1.248688  0.267648  0.613826\nbbb  0.809296  1.671020  1.564420  0.347662\nccc  1.501939  1.126518  0.702019  1.596048\nddd  0.137160  0.147368  1.504663  0.202822\neee  0.134540  3.708104  0.309097  1.641090\n>>> \n""]";"[""import numpy as np \nfrom pandas import *\n\nIndex= ['aaa','bbb','ccc','ddd','eee']\nCols = ['A', 'B', 'C','D']\ndf = DataFrame(abs(np.random.randn(5, 4)), index= Index, columns=Cols)\n\n>>> df\n          A         B         C         D\naaa  2.431645  1.248688  0.267648  0.613826\nbbb  0.809296  1.671020  1.564420  0.347662\nccc  1.501939  1.126518  0.702019  1.596048\nddd  0.137160  0.147368  1.504663  0.202822\neee  0.134540  3.708104  0.309097  1.641090\n>>> \n""]";"[""import numpy as np \nfrom pandas import *\n\nIndex= ['aaa','bbb','ccc','ddd','eee']\nCols = ['A', 'B', 'C','D']\ndf = DataFrame(abs(np.random.randn(5, 4)), index= Index, columns=Cols)\n\n""]";"[""from pandas import DataFrame\nimport numpy as np \nfrom pandas import *\n\nIndex= ['aaa','bbb','ccc','ddd','eee']\nCols = ['A', 'B', 'C','D']\ndf = DataFrame(abs(np.random.randn(5, 4)), index= Index, columns=Cols)\n\n""]";True;"[""import pandas as pd\nimport numpy as np \nfrom pandas import *\n\nIndex= ['aaa','bbb','ccc','ddd','eee']\nCols = ['A', 'B', 'C','D']\ndf = DataFrame(abs(np.random.randn(5, 4)), index= Index, columns=Cols)\n\n""]";False;0;2;"[""No module named 'matplotlib'"", ""No module named 'seaborn'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'matplotlib'"", ""No module named 'seaborn'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'matplotlib'"", ""No module named 'seaborn'""]";['ImportError', 'ImportError']
88;88;88;88;3.0;0;12307099;;1;82;<python><pandas>;Modifying a subset of rows in a pandas dataframe;48481.0;"[""df['A'==0]['B'] = np.nan\ndf['A'==0]['B'].values.fill(np.nan)\n""]";"[""df['A'==0]['B'] = np.nan\n"", ""df['A'==0]['B'].values.fill(np.nan)\n""]";"[""df['A'==0]['B'] = np.nan\n"", ""df['A'==0]['B'].values.fill(np.nan)\n""]";"[""df['A'==0]['B'] = np.nan\ndf['A'==0]['B'].values.fill(np.nan)\n""]";"[""df['A'==0]['B'] = np.nan\ndf['A'==0]['B'].values.fill(np.nan)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf['A'==0]['B'] = np.nan\ndf['A'==0]['B'].values.fill(np.nan)\n""]";True;2;3;"[""name 'np' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess'];2;3;"[""name 'np' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess'];2;3;"[""name 'np' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess']
89;89;89;89;3.0;1;12322289;;1;11;<python><join><time-series><pandas><kdb>;KDB+ like asof join for timeseries data in pandas?;4799.0;"['q)5# t\ntime         sym  price size \n-----------------------------\n09:30:00.439 NVDA 13.42 60511\n09:30:00.439 NVDA 13.42 60511\n09:30:02.332 NVDA 13.42 100  \n09:30:02.332 NVDA 13.42 100  \n09:30:02.333 NVDA 13.41 100  \n\nq)5# q\ntime         sym  bid   ask   bsize asize\n-----------------------------------------\n09:30:00.026 NVDA 13.34 13.44 3     16   \n09:30:00.043 NVDA 13.34 13.44 3     17   \n09:30:00.121 NVDA 13.36 13.65 1     10   \n09:30:00.386 NVDA 13.36 13.52 21    1    \n09:30:00.440 NVDA 13.4  13.44 15    17\n\nq)5# aj[`time; t; q]\ntime         sym  price size  bid   ask   bsize asize\n-----------------------------------------------------\n09:30:00.439 NVDA 13.42 60511 13.36 13.52 21    1    \n09:30:00.439 NVDA 13.42 60511 13.36 13.52 21    1    \n09:30:02.332 NVDA 13.42 100   13.34 13.61 1     1    \n09:30:02.332 NVDA 13.42 100   13.34 13.61 1     1    \n09:30:02.333 NVDA 13.41 100   13.34 13.51 1     1  \nIn [55]: quotes.head()\nOut[55]: \n                              bid    ask  bsize  asize\n2012-09-06 09:30:00.026000  13.34  13.44      3     16\n2012-09-06 09:30:00.043000  13.34  13.44      3     17\n2012-09-06 09:30:00.121000  13.36  13.65      1     10\n2012-09-06 09:30:00.386000  13.36  13.52     21      1\n2012-09-06 09:30:00.440000  13.40  13.44     15     17\n\nIn [56]: trades.head()\nOut[56]: \n                            price   size\n2012-09-06 09:30:00.439000  13.42  60511\n2012-09-06 09:30:00.439000  13.42  60511\n2012-09-06 09:30:02.332000  13.42    100\n2012-09-06 09:30:02.332000  13.42    100\n2012-09-06 09:30:02.333000  13.41    100\n']";"['q)5# t\ntime         sym  price size \n-----------------------------\n09:30:00.439 NVDA 13.42 60511\n09:30:00.439 NVDA 13.42 60511\n09:30:02.332 NVDA 13.42 100  \n09:30:02.332 NVDA 13.42 100  \n09:30:02.333 NVDA 13.41 100  \n\nq)5# q\ntime         sym  bid   ask   bsize asize\n-----------------------------------------\n09:30:00.026 NVDA 13.34 13.44 3     16   \n09:30:00.043 NVDA 13.34 13.44 3     17   \n09:30:00.121 NVDA 13.36 13.65 1     10   \n09:30:00.386 NVDA 13.36 13.52 21    1    \n09:30:00.440 NVDA 13.4  13.44 15    17\n\nq)5# aj[`time; t; q]\ntime         sym  price size  bid   ask   bsize asize\n-----------------------------------------------------\n09:30:00.439 NVDA 13.42 60511 13.36 13.52 21    1    \n09:30:00.439 NVDA 13.42 60511 13.36 13.52 21    1    \n09:30:02.332 NVDA 13.42 100   13.34 13.61 1     1    \n09:30:02.332 NVDA 13.42 100   13.34 13.61 1     1    \n09:30:02.333 NVDA 13.41 100   13.34 13.51 1     1  \n', 'In [55]: quotes.head()\nOut[55]: \n                              bid    ask  bsize  asize\n2012-09-06 09:30:00.026000  13.34  13.44      3     16\n2012-09-06 09:30:00.043000  13.34  13.44      3     17\n2012-09-06 09:30:00.121000  13.36  13.65      1     10\n2012-09-06 09:30:00.386000  13.36  13.52     21      1\n2012-09-06 09:30:00.440000  13.40  13.44     15     17\n\nIn [56]: trades.head()\nOut[56]: \n                            price   size\n2012-09-06 09:30:00.439000  13.42  60511\n2012-09-06 09:30:00.439000  13.42  60511\n2012-09-06 09:30:02.332000  13.42    100\n2012-09-06 09:30:02.332000  13.42    100\n2012-09-06 09:30:02.333000  13.41    100\n']";"['q)5# t\ntime         sym  price size \n-----------------------------\n09:30:00.439 NVDA 13.42 60511\n09:30:00.439 NVDA 13.42 60511\n09:30:02.332 NVDA 13.42 100  \n09:30:02.332 NVDA 13.42 100  \n09:30:02.333 NVDA 13.41 100  \n\nq)5# q\ntime         sym  bid   ask   bsize asize\n-----------------------------------------\n09:30:00.026 NVDA 13.34 13.44 3     16   \n09:30:00.043 NVDA 13.34 13.44 3     17   \n09:30:00.121 NVDA 13.36 13.65 1     10   \n09:30:00.386 NVDA 13.36 13.52 21    1    \n09:30:00.440 NVDA 13.4  13.44 15    17\n\nq)5# aj[`time; t; q]\ntime         sym  price size  bid   ask   bsize asize\n-----------------------------------------------------\n09:30:00.439 NVDA 13.42 60511 13.36 13.52 21    1    \n09:30:00.439 NVDA 13.42 60511 13.36 13.52 21    1    \n09:30:02.332 NVDA 13.42 100   13.34 13.61 1     1    \n09:30:02.332 NVDA 13.42 100   13.34 13.61 1     1    \n09:30:02.333 NVDA 13.41 100   13.34 13.51 1     1  \n', 'In [55]: quotes.head()\nOut[55]: \n                              bid    ask  bsize  asize\n2012-09-06 09:30:00.026000  13.34  13.44      3     16\n2012-09-06 09:30:00.043000  13.34  13.44      3     17\n2012-09-06 09:30:00.121000  13.36  13.65      1     10\n2012-09-06 09:30:00.386000  13.36  13.52     21      1\n2012-09-06 09:30:00.440000  13.40  13.44     15     17\n\nIn [56]: trades.head()\nOut[56]: \n                            price   size\n2012-09-06 09:30:00.439000  13.42  60511\n2012-09-06 09:30:00.439000  13.42  60511\n2012-09-06 09:30:02.332000  13.42    100\n2012-09-06 09:30:02.332000  13.42    100\n2012-09-06 09:30:02.333000  13.41    100\n']";['quotes.head()\ntrades.head()\n'];['quotes.head()\ntrades.head()\n'];False;['import pandas as pd\nquotes.head()\ntrades.head()\n'];False;0;2;"[""name 'df1' is not defined"", ""name 'quotes' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df1' is not defined"", ""name 'quotes' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""name 'quotes' is not defined""]";['Sucess', 'NameError']
90;90;90;90;2.0;0;12322779;;1;22;<pandas>;Pandas: unique dataframe;26767.0;[''];[];['groupby'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'pandas' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'pandas' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'pandas' is not defined"", 'not enough values to unpack (expected 2, got 0)']";['NameError', 'ValueError']
91;91;91;91;5.0;0;12329853;;1;11;<python><pandas>;How to rearrange Pandas column sequence?;31812.0;"["">>> df =DataFrame({'a':[1,2,3,4],'b':[2,4,6,8]})\n>>> df['x']=df.a + df.b\n>>> df['y']=df.a - df.b\n>>> df\n   a  b   x  y\n0  1  2   3 -1\n1  2  4   6 -2\n2  3  6   9 -3\n3  4  8  12 -4\n>>> df = df[['x','y','a','b']]\n>>> df\n    x  y  a  b\n0   3 -1  1  2\n1   6 -2  2  4\n2   9 -3  3  6\n3  12 -4  4  8\n""]";"["">>> df =DataFrame({'a':[1,2,3,4],'b':[2,4,6,8]})\n>>> df['x']=df.a + df.b\n>>> df['y']=df.a - df.b\n>>> df\n   a  b   x  y\n0  1  2   3 -1\n1  2  4   6 -2\n2  3  6   9 -3\n3  4  8  12 -4\n"", "">>> df = df[['x','y','a','b']]\n>>> df\n    x  y  a  b\n0   3 -1  1  2\n1   6 -2  2  4\n2   9 -3  3  6\n3  12 -4  4  8\n""]";"["">>> df =DataFrame({'a':[1,2,3,4],'b':[2,4,6,8]})\n>>> df['x']=df.a + df.b\n>>> df['y']=df.a - df.b\n>>> df\n   a  b   x  y\n0  1  2   3 -1\n1  2  4   6 -2\n2  3  6   9 -3\n3  4  8  12 -4\n"", "">>> df = df[['x','y','a','b']]\n>>> df\n    x  y  a  b\n0   3 -1  1  2\n1   6 -2  2  4\n2   9 -3  3  6\n3  12 -4  4  8\n"", 'set_column_sequence(dataframe,col_name, seq)', ""set_column_sequence(df,'x',0)"", ""set_column_sequence(df,'y',1)""]";[''];[''];False;['import pandas as pd\n'];False;0;2;"[""'return' outside function (<ast>, line 6)"", ""name 'df' is not defined""]";['SyntaxError', 'NameError'];0;2;"[""'return' outside function (<ast>, line 6)"", ""name 'df' is not defined""]";['SyntaxError', 'NameError'];0;2;"[""'return' outside function (<ast>, line 8)"", ""name 'df' is not defined""]";['SyntaxError', 'NameError']
92;92;92;92;1.0;0;12356501;;1;68;<python><pandas>;Pandas: create two new columns in a dataframe with values calculated from a pre-existing column;45846.0;"[""def calculate(x):\n    ...operate...\n    return z, y\ndf['new_col']) = df['column_A'].map(a_function)\n(df['new_col_zetas'], df['new_col_ys']) = df['column_A'].map(calculate)\n""]";"['def calculate(x):\n    ...operate...\n    return z, y\n', ""df['new_col']) = df['column_A'].map(a_function)\n"", ""(df['new_col_zetas'], df['new_col_ys']) = df['column_A'].map(calculate)\n""]";"['df', 'def calculate(x):\n    ...operate...\n    return z, y\n', ""df['new_col']) = df['column_A'].map(a_function)\n"", ""(df['new_col_zetas'], df['new_col_ys']) = df['column_A'].map(calculate)\n"", ""df['column_A'].map(calculate)""]";"[""(df['new_col_zetas'], df['new_col_ys']) = df['column_A'].map(calculate)\n""]";"[""(df['new_col_zetas'], df['new_col_ys']) = df['column_A'].map(calculate)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\n(df['new_col_zetas'], df['new_col_ys']) = df['column_A'].map(calculate)\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'a'""]";['KeyError']
93;93;93;93;3.0;0;12358360;;1;13;<python><sorting><pandas>;Python pandas order column according to the values in a row;6706.0;"["">>> df = DataFrame(np.random.randn(10, 4), columns=['ddd', 'fff', 'aaa', 'ppp'])\n>>> df\n        ddd       fff       aaa       ppp\n0 -0.177438  0.102561 -1.318710  1.321252\n1  0.980348  0.786721  0.374506 -1.411019\n2  0.405112  0.514216  1.761983 -0.529482\n3  1.659710 -1.017048 -0.737615 -0.388145\n4 -0.472223  1.407655 -0.129119 -0.912974\n5  1.221324 -0.656599  0.563152 -0.900710\n6 -1.816420 -2.898094 -0.232047 -0.648904\n7  2.793261  0.568760 -0.850100  0.654704\n8 -2.180891  2.054178 -1.050897 -1.461458\n9 -1.123756  1.245987 -0.239863  0.359759\n""]";"["">>> df = DataFrame(np.random.randn(10, 4), columns=['ddd', 'fff', 'aaa', 'ppp'])\n>>> df\n        ddd       fff       aaa       ppp\n0 -0.177438  0.102561 -1.318710  1.321252\n1  0.980348  0.786721  0.374506 -1.411019\n2  0.405112  0.514216  1.761983 -0.529482\n3  1.659710 -1.017048 -0.737615 -0.388145\n4 -0.472223  1.407655 -0.129119 -0.912974\n5  1.221324 -0.656599  0.563152 -0.900710\n6 -1.816420 -2.898094 -0.232047 -0.648904\n7  2.793261  0.568760 -0.850100  0.654704\n8 -2.180891  2.054178 -1.050897 -1.461458\n9 -1.123756  1.245987 -0.239863  0.359759\n""]";"["">>> df = DataFrame(np.random.randn(10, 4), columns=['ddd', 'fff', 'aaa', 'ppp'])\n>>> df\n        ddd       fff       aaa       ppp\n0 -0.177438  0.102561 -1.318710  1.321252\n1  0.980348  0.786721  0.374506 -1.411019\n2  0.405112  0.514216  1.761983 -0.529482\n3  1.659710 -1.017048 -0.737615 -0.388145\n4 -0.472223  1.407655 -0.129119 -0.912974\n5  1.221324 -0.656599  0.563152 -0.900710\n6 -1.816420 -2.898094 -0.232047 -0.648904\n7  2.793261  0.568760 -0.850100  0.654704\n8 -2.180891  2.054178 -1.050897 -1.461458\n9 -1.123756  1.245987 -0.239863  0.359759\n""]";['6 -1.816420 -2.898094 -0.232047 -0.648904\n'];['6 -1.816420 -2.898094 -0.232047 -0.648904\n'];False;['import pandas as pd\n6 -1.816420 -2.898094 -0.232047 -0.648904\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError']
94;94;94;94;4.0;0;12376863;;1;51;<python><pandas>;Adding calculated column(s) to a dataframe in pandas;70320.0;"['<class \'pandas.core.frame.DataFrame\'>\nDatetimeIndex: 500047 entries, 1998-05-04 04:45:00 to 2012-08-07 00:15:00\nFreq: 15T\nData columns:\nClose    363152  non-null values\nHigh     363152  non-null values\nLow      363152  non-null values\nOpen     363152  non-null values\ndtypes: float64(4)\ndef closed_in_top_half_of_range(h,l,c):\n    return c > l + (h-1)/2\n\ndef lower_wick(o,l,c):\n    return min(o,c)-l\n\ndef real_body(o,c):\n    return abs(c-o)\n\ndef lower_wick_at_least_twice_real_body(o,l,c):\n    return lower_wick(o,l,c) >= 2 * real_body(o,c)\n\ndef is_hammer(row):\n    return lower_wick_at_least_twice_real_body(row[""Open""],row[""Low""],row[""Close""]) \\\n    and closed_in_top_half_of_range(row[""High""],row[""Low""],row[""Close""])\n']";"[""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 500047 entries, 1998-05-04 04:45:00 to 2012-08-07 00:15:00\nFreq: 15T\nData columns:\nClose    363152  non-null values\nHigh     363152  non-null values\nLow      363152  non-null values\nOpen     363152  non-null values\ndtypes: float64(4)\n"", 'def closed_in_top_half_of_range(h,l,c):\n    return c > l + (h-1)/2\n\ndef lower_wick(o,l,c):\n    return min(o,c)-l\n\ndef real_body(o,c):\n    return abs(c-o)\n\ndef lower_wick_at_least_twice_real_body(o,l,c):\n    return lower_wick(o,l,c) >= 2 * real_body(o,c)\n\ndef is_hammer(row):\n    return lower_wick_at_least_twice_real_body(row[""Open""],row[""Low""],row[""Close""]) \\\n    and closed_in_top_half_of_range(row[""High""],row[""Low""],row[""Close""])\n']";"[""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 500047 entries, 1998-05-04 04:45:00 to 2012-08-07 00:15:00\nFreq: 15T\nData columns:\nClose    363152  non-null values\nHigh     363152  non-null values\nLow      363152  non-null values\nOpen     363152  non-null values\ndtypes: float64(4)\n"", 'def closed_in_top_half_of_range(h,l,c):\n    return c > l + (h-1)/2\n\ndef lower_wick(o,l,c):\n    return min(o,c)-l\n\ndef real_body(o,c):\n    return abs(c-o)\n\ndef lower_wick_at_least_twice_real_body(o,l,c):\n    return lower_wick(o,l,c) >= 2 * real_body(o,c)\n\ndef is_hammer(row):\n    return lower_wick_at_least_twice_real_body(row[""Open""],row[""Low""],row[""Close""]) \\\n    and closed_in_top_half_of_range(row[""High""],row[""Low""],row[""Close""])\n']";['\n\n\n\n'];['\n\n\n\n'];False;['import pandas as pd\n\n\n\n\n'];False;0;2;"[""name 'is_hammer' is not defined"", ""name 'd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'is_hammer' is not defined"", ""name 'd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'is_hammer' is not defined"", ""name 'd' is not defined""]";['NameError', 'NameError']
95;95;95;95;1.0;3;12389898;;1;22;<python><group-by><transform><dataframe><pandas>;Python Pandas: how to add a totally new column to a data frame inside of a groupby/transform operation;18845.0;"['import pandas, numpy as np\ndfrm = pandas.DataFrame({\'A\':np.random.rand(100), \n                         \'B\':(50+np.random.randn(100)), \n                         \'C\':np.random.randint(low=0, high=3, size=(100,))})\nimport scipy.stats as st\ndef mark_quintiles(x, breakpoints):\n    # Assume this is filled in, using st.mstats.mquantiles.\n    # This returns an array the same shape as x, with an integer for which\n    # breakpoint-bucket that entry of x falls into.\ndef transformXtiles(dataFrame, inputColumnName, newColumnName, breaks):\n    dataFrame[newColumnName] = mark_quintiles(dataFrame[inputColumnName].values, \n                                              breaks)\n    return dataFrame\ndfrm.groupby(""C"").transform(lambda x: transformXtiles(x, ""A"", ""A_xtile"", [0.2, 0.4, 0.6, 0.8, 1.0]))\n']";"[""import pandas, numpy as np\ndfrm = pandas.DataFrame({'A':np.random.rand(100), \n                         'B':(50+np.random.randn(100)), \n                         'C':np.random.randint(low=0, high=3, size=(100,))})\n"", 'import scipy.stats as st\ndef mark_quintiles(x, breakpoints):\n    # Assume this is filled in, using st.mstats.mquantiles.\n    # This returns an array the same shape as x, with an integer for which\n    # breakpoint-bucket that entry of x falls into.\n', 'def transformXtiles(dataFrame, inputColumnName, newColumnName, breaks):\n    dataFrame[newColumnName] = mark_quintiles(dataFrame[inputColumnName].values, \n                                              breaks)\n    return dataFrame\n', 'dfrm.groupby(""C"").transform(lambda x: transformXtiles(x, ""A"", ""A_xtile"", [0.2, 0.4, 0.6, 0.8, 1.0]))\n']";"[""import pandas, numpy as np\ndfrm = pandas.DataFrame({'A':np.random.rand(100), \n                         'B':(50+np.random.randn(100)), \n                         'C':np.random.randint(low=0, high=3, size=(100,))})\n"", 'import scipy.stats as st\ndef mark_quintiles(x, breakpoints):\n    # Assume this is filled in, using st.mstats.mquantiles.\n    # This returns an array the same shape as x, with an integer for which\n    # breakpoint-bucket that entry of x falls into.\n', 'transform', 'def transformXtiles(dataFrame, inputColumnName, newColumnName, breaks):\n    dataFrame[newColumnName] = mark_quintiles(dataFrame[inputColumnName].values, \n                                              breaks)\n    return dataFrame\n', 'dfrm.groupby(""C"").transform(lambda x: transformXtiles(x, ""A"", ""A_xtile"", [0.2, 0.4, 0.6, 0.8, 1.0]))\n', 'apply']";"['import pandas, numpy as np\nimport scipy.stats as st\n    # Assume this is filled in, using st.mstats.mquantiles.\n    # This returns an array the same shape as x, with an integer for which\n    # breakpoint-bucket that entry of x falls into.\ndfrm.groupby(""C"").transform(lambda x: transformXtiles(x, ""A"", ""A_xtile"", [0.2, 0.4, 0.6, 0.8, 1.0]))\n']";"['import pandas, numpy as np\nimport scipy.stats as st\n    # Assume this is filled in, using st.mstats.mquantiles.\n    # This returns an array the same shape as x, with an integer for which\n    # breakpoint-bucket that entry of x falls into.\ndfrm.groupby(""C"").transform(lambda x: transformXtiles(x, ""A"", ""A_xtile"", [0.2, 0.4, 0.6, 0.8, 1.0]))\n']";False;"['import pandas as pd\ndf = pd.DataFrame()\nimport pandas, numpy as np\nimport scipy.stats as st\n    # Assume this is filled in, using st.mstats.mquantiles.\n    # This returns an array the same shape as x, with an integer for which\n    # breakpoint-bucket that entry of x falls into.\ndfrm.groupby(""C"").transform(lambda x: transformXtiles(x, ""A"", ""A_xtile"", [0.2, 0.4, 0.6, 0.8, 1.0]))\n']";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
96;96;96;96;3.0;1;12406162;;1;16;<numpy><matplotlib><pandas>;KeyError when plotting a sliced pandas dataframe with datetimes;10015.0;"['import numpy as np\nfrom pandas import DataFrame\nimport datetime\nfrom pylab import *\n\ntest = DataFrame({\'x\' : [datetime.datetime(2012,9,10) + datetime.timedelta(n) for n in range(10)], \n                  \'y\' : range(10)})\nplot(test[\'x\'][0:5])\nplot(test[\'x\'][5:10])\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n<ipython-input-7-aa076e3fc4e0> in <module>()\n----> 1 plot(test[\'x\'][5:10])\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\pyplot.pyc in plot(*args, **kwargs)\n   2456         ax.hold(hold)\n   2457     try:\n-> 2458         ret = ax.plot(*args, **kwargs)\n   2459         draw_if_interactive()\n   2460     finally:\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in plot(self, *args, **kwargs)\n   3846         lines = []\n   3847 \n-> 3848         for line in self._get_lines(*args, **kwargs):\n   3849             self.add_line(line)\n   3850             lines.append(line)\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in _grab_next_args(self, *args, **kwargs)\n    321                 return\n    322             if len(remaining) <= 3:\n--> 323                 for seg in self._plot_args(remaining, kwargs):\n    324                     yield seg\n    325                 return\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in _plot_args(self, tup, kwargs)\n    298             x = np.arange(y.shape[0], dtype=float)\n    299 \n--> 300         x, y = self._xy_from_xy(x, y)\n    301 \n    302         if self.command == \'plot\':\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in _xy_from_xy(self, x, y)\n    215         if self.axes.xaxis is not None and self.axes.yaxis is not None:\n    216             bx = self.axes.xaxis.update_units(x)\n--> 217             by = self.axes.yaxis.update_units(y)\n    218 \n    219             if self.command!=\'plot\':\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axis.pyc in update_units(self, data)\n   1277         neednew = self.converter!=converter\n   1278         self.converter = converter\n-> 1279         default = self.converter.default_units(data, self)\n   1280         #print \'update units: default=%s, units=%s\'%(default, self.units)\n   1281         if default is not None and self.units is None:\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\dates.pyc in default_units(x, axis)\n   1153         \'Return the tzinfo instance of *x* or of its first element, or None\'\n   1154         try:\n-> 1155             x = x[0]\n   1156         except (TypeError, IndexError):\n   1157             pass\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc in __getitem__(self, key)\n    374     def __getitem__(self, key):\n    375         try:\n--> 376             return self.index.get_value(self, key)\n    377         except InvalidIndexError:\n    378             pass\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\index.pyc in get_value(self, series, key)\n    529         """"""\n    530         try:\n--> 531             return self._engine.get_value(series, key)\n    532         except KeyError, e1:\n    533             if len(self) > 0 and self.inferred_type == \'integer\':\n\nC:\\Python27\\lib\\site-packages\\pandas\\_engines.pyd in pandas._engines.IndexEngine.get_value (pandas\\src\\engines.c:1479)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\_engines.pyd in pandas._engines.IndexEngine.get_value (pandas\\src\\engines.c:1374)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\_engines.pyd in pandas._engines.DictIndexEngine.get_loc (pandas\\src\\engines.c:2498)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\_engines.pyd in pandas._engines.DictIndexEngine.get_loc (pandas\\src\\engines.c:2460)()\n\nKeyError: 0\n']";"[""import numpy as np\nfrom pandas import DataFrame\nimport datetime\nfrom pylab import *\n\ntest = DataFrame({'x' : [datetime.datetime(2012,9,10) + datetime.timedelta(n) for n in range(10)], \n                  'y' : range(10)})\n"", ""plot(test['x'][0:5])\n"", ""plot(test['x'][5:10])\n"", '---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n<ipython-input-7-aa076e3fc4e0> in <module>()\n----> 1 plot(test[\'x\'][5:10])\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\pyplot.pyc in plot(*args, **kwargs)\n   2456         ax.hold(hold)\n   2457     try:\n-> 2458         ret = ax.plot(*args, **kwargs)\n   2459         draw_if_interactive()\n   2460     finally:\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in plot(self, *args, **kwargs)\n   3846         lines = []\n   3847 \n-> 3848         for line in self._get_lines(*args, **kwargs):\n   3849             self.add_line(line)\n   3850             lines.append(line)\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in _grab_next_args(self, *args, **kwargs)\n    321                 return\n    322             if len(remaining) <= 3:\n--> 323                 for seg in self._plot_args(remaining, kwargs):\n    324                     yield seg\n    325                 return\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in _plot_args(self, tup, kwargs)\n    298             x = np.arange(y.shape[0], dtype=float)\n    299 \n--> 300         x, y = self._xy_from_xy(x, y)\n    301 \n    302         if self.command == \'plot\':\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in _xy_from_xy(self, x, y)\n    215         if self.axes.xaxis is not None and self.axes.yaxis is not None:\n    216             bx = self.axes.xaxis.update_units(x)\n--> 217             by = self.axes.yaxis.update_units(y)\n    218 \n    219             if self.command!=\'plot\':\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axis.pyc in update_units(self, data)\n   1277         neednew = self.converter!=converter\n   1278         self.converter = converter\n-> 1279         default = self.converter.default_units(data, self)\n   1280         #print \'update units: default=%s, units=%s\'%(default, self.units)\n   1281         if default is not None and self.units is None:\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\dates.pyc in default_units(x, axis)\n   1153         \'Return the tzinfo instance of *x* or of its first element, or None\'\n   1154         try:\n-> 1155             x = x[0]\n   1156         except (TypeError, IndexError):\n   1157             pass\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc in __getitem__(self, key)\n    374     def __getitem__(self, key):\n    375         try:\n--> 376             return self.index.get_value(self, key)\n    377         except InvalidIndexError:\n    378             pass\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\index.pyc in get_value(self, series, key)\n    529         """"""\n    530         try:\n--> 531             return self._engine.get_value(series, key)\n    532         except KeyError, e1:\n    533             if len(self) > 0 and self.inferred_type == \'integer\':\n\nC:\\Python27\\lib\\site-packages\\pandas\\_engines.pyd in pandas._engines.IndexEngine.get_value (pandas\\src\\engines.c:1479)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\_engines.pyd in pandas._engines.IndexEngine.get_value (pandas\\src\\engines.c:1374)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\_engines.pyd in pandas._engines.DictIndexEngine.get_loc (pandas\\src\\engines.c:2498)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\_engines.pyd in pandas._engines.DictIndexEngine.get_loc (pandas\\src\\engines.c:2460)()\n\nKeyError: 0\n']";"[""import numpy as np\nfrom pandas import DataFrame\nimport datetime\nfrom pylab import *\n\ntest = DataFrame({'x' : [datetime.datetime(2012,9,10) + datetime.timedelta(n) for n in range(10)], \n                  'y' : range(10)})\n"", ""plot(test['x'][0:5])\n"", ""plot(test['x'][5:10])\n"", ""plot(test['y'][5:10])"", '---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n<ipython-input-7-aa076e3fc4e0> in <module>()\n----> 1 plot(test[\'x\'][5:10])\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\pyplot.pyc in plot(*args, **kwargs)\n   2456         ax.hold(hold)\n   2457     try:\n-> 2458         ret = ax.plot(*args, **kwargs)\n   2459         draw_if_interactive()\n   2460     finally:\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in plot(self, *args, **kwargs)\n   3846         lines = []\n   3847 \n-> 3848         for line in self._get_lines(*args, **kwargs):\n   3849             self.add_line(line)\n   3850             lines.append(line)\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in _grab_next_args(self, *args, **kwargs)\n    321                 return\n    322             if len(remaining) <= 3:\n--> 323                 for seg in self._plot_args(remaining, kwargs):\n    324                     yield seg\n    325                 return\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in _plot_args(self, tup, kwargs)\n    298             x = np.arange(y.shape[0], dtype=float)\n    299 \n--> 300         x, y = self._xy_from_xy(x, y)\n    301 \n    302         if self.command == \'plot\':\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in _xy_from_xy(self, x, y)\n    215         if self.axes.xaxis is not None and self.axes.yaxis is not None:\n    216             bx = self.axes.xaxis.update_units(x)\n--> 217             by = self.axes.yaxis.update_units(y)\n    218 \n    219             if self.command!=\'plot\':\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axis.pyc in update_units(self, data)\n   1277         neednew = self.converter!=converter\n   1278         self.converter = converter\n-> 1279         default = self.converter.default_units(data, self)\n   1280         #print \'update units: default=%s, units=%s\'%(default, self.units)\n   1281         if default is not None and self.units is None:\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\dates.pyc in default_units(x, axis)\n   1153         \'Return the tzinfo instance of *x* or of its first element, or None\'\n   1154         try:\n-> 1155             x = x[0]\n   1156         except (TypeError, IndexError):\n   1157             pass\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc in __getitem__(self, key)\n    374     def __getitem__(self, key):\n    375         try:\n--> 376             return self.index.get_value(self, key)\n    377         except InvalidIndexError:\n    378             pass\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\index.pyc in get_value(self, series, key)\n    529         """"""\n    530         try:\n--> 531             return self._engine.get_value(series, key)\n    532         except KeyError, e1:\n    533             if len(self) > 0 and self.inferred_type == \'integer\':\n\nC:\\Python27\\lib\\site-packages\\pandas\\_engines.pyd in pandas._engines.IndexEngine.get_value (pandas\\src\\engines.c:1479)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\_engines.pyd in pandas._engines.IndexEngine.get_value (pandas\\src\\engines.c:1374)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\_engines.pyd in pandas._engines.DictIndexEngine.get_loc (pandas\\src\\engines.c:2498)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\_engines.pyd in pandas._engines.DictIndexEngine.get_loc (pandas\\src\\engines.c:2460)()\n\nKeyError: 0\n']";"[""import numpy as np\nfrom pandas import DataFrame\nimport datetime\nfrom pylab import *\n\nplot(test['x'][0:5])\nplot(test['x'][5:10])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n""]";"[""import numpy as np\nfrom pandas import DataFrame\nimport datetime\nfrom pylab import *\n\nplot(test['x'][0:5])\nplot(test['x'][5:10])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n""]";False;"[""import pandas as pd\nimport numpy as np\nfrom pandas import DataFrame\nimport datetime\nfrom pylab import *\n\nplot(test['x'][0:5])\nplot(test['x'][5:10])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n""]";False;0;2;"[""name 'test' is not defined"", ""name 'plot' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'test' is not defined"", ""name 'plot' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'test' is not defined"", ""name 'plot' is not defined""]";['NameError', 'NameError']
97;97;97;97;4.0;1;12433076;;1;34;<pandas><finance><yahoo-finance><google-finance><stockquotes>;Download history stock prices automatically from yahoo finance in python;81673.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;3;"['Sucess', 'The pandas.io.data module is moved to a separate package (pandas-datareader). After installing the pandas-datareader package (https://github.com/pydata/pandas-datareader), you can change the import ``from pandas.io import data, wb`` to ``from pandas_datareader import data, wb``.', ""No module named 'pandas_datareader'""]";['Sucess', 'ImportError', 'ImportError'];1;3;"['Sucess', 'The pandas.io.data module is moved to a separate package (pandas-datareader). After installing the pandas-datareader package (https://github.com/pydata/pandas-datareader), you can change the import ``from pandas.io import data, wb`` to ``from pandas_datareader import data, wb``.', ""No module named 'pandas_datareader'""]";['Sucess', 'ImportError', 'ImportError'];1;3;"['Sucess', 'The pandas.io.data module is moved to a separate package (pandas-datareader). After installing the pandas-datareader package (https://github.com/pydata/pandas-datareader), you can change the import ``from pandas.io import data, wb`` to ``from pandas_datareader import data, wb``.', ""No module named 'pandas_datareader'""]";['Sucess', 'ImportError', 'ImportError']
98;98;98;98;9.0;7;12436979;;1;17;<python><numpy><pip><pandas><easy-install>;How to fix Python Numpy/Pandas installation?;65685.0;"['$ sudo easy_install pandas\nSearching for pandas\nReading http://pypi.python.org/simple/pandas/\nReading http://pandas.pydata.org\nReading http://pandas.sourceforge.net\nBest match: pandas 0.8.1\nDownloading http://pypi.python.org/packages/source/p/pandas/pandas-0.8.1.zip#md5=d2c5c5bea971cd760b0ae6f6850fcb74\nProcessing pandas-0.8.1.zip\nRunning pandas-0.8.1/setup.py -q bdist_egg --dist-dir /tmp/easy_install-ckAMym/pandas-0.8.1/egg-dist-tmp-0mlL7t\nerror: Setup script exited with pandas requires NumPy >= 1.6 due to datetime64 dependency\n$ sudo easy_install numpy\nSearching for numpy\nBest match: numpy 1.6.2\nAdding numpy 1.6.2 to easy-install.pth file\n\nUsing /Library/Python/2.6/site-packages\nProcessing dependencies for numpy\nFinished processing dependencies for numpy\n$ sudo easy_install pandas\nerror: Setup script exited with pandas requires NumPy >= 1.6 due to datetime64 dependency\n$ python\nPython 2.6.1 (r261:67515, Jun 24 2010, 21:47:49) \n[GCC 4.2.1 (Apple Inc. build 5646)] on darwin\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n>>> import numpy as np\n>>> np.__version__\n\'1.2.1\'\n$ sudo pip install numpy\nRequirement already satisfied (use --upgrade to upgrade): numpy in /Library/Python/2.6/site-packages\nCleaning up...\n$ sudo pip install numpy --upgrade\nRequirement already up-to-date: numpy in /Library/Python/2.6/site-packages\nCleaning up...\n\n$ sudo pip install pandas\nDownloading/unpacking pandas\n  Downloading pandas-0.8.1.zip (1.9MB): 1.9MB downloaded\n  Running setup.py egg_info for package pandas\n    pandas requires NumPy >= 1.6 due to datetime64 dependency\n    Complete output from command python setup.py egg_info:\n    pandas requires NumPy >= 1.6 due to datetime64 dependency\n\n----------------------------------------\nCommand python setup.py egg_info failed with error code 1 in /tmp/pip-build/pandas\nStoring complete log in /Users/MyUsername/Library/Logs/pip.log\n']";"['$ sudo easy_install pandas\nSearching for pandas\nReading http://pypi.python.org/simple/pandas/\nReading http://pandas.pydata.org\nReading http://pandas.sourceforge.net\nBest match: pandas 0.8.1\nDownloading http://pypi.python.org/packages/source/p/pandas/pandas-0.8.1.zip#md5=d2c5c5bea971cd760b0ae6f6850fcb74\nProcessing pandas-0.8.1.zip\nRunning pandas-0.8.1/setup.py -q bdist_egg --dist-dir /tmp/easy_install-ckAMym/pandas-0.8.1/egg-dist-tmp-0mlL7t\nerror: Setup script exited with pandas requires NumPy >= 1.6 due to datetime64 dependency\n', '$ sudo easy_install numpy\nSearching for numpy\nBest match: numpy 1.6.2\nAdding numpy 1.6.2 to easy-install.pth file\n\nUsing /Library/Python/2.6/site-packages\nProcessing dependencies for numpy\nFinished processing dependencies for numpy\n', '$ sudo easy_install pandas\n', 'error: Setup script exited with pandas requires NumPy >= 1.6 due to datetime64 dependency\n', '$ python\nPython 2.6.1 (r261:67515, Jun 24 2010, 21:47:49) \n[GCC 4.2.1 (Apple Inc. build 5646)] on darwin\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n>>> import numpy as np\n>>> np.__version__\n\'1.2.1\'\n', '$ sudo pip install numpy\nRequirement already satisfied (use --upgrade to upgrade): numpy in /Library/Python/2.6/site-packages\nCleaning up...\n', '$ sudo pip install numpy --upgrade\nRequirement already up-to-date: numpy in /Library/Python/2.6/site-packages\nCleaning up...\n\n$ sudo pip install pandas\nDownloading/unpacking pandas\n  Downloading pandas-0.8.1.zip (1.9MB): 1.9MB downloaded\n  Running setup.py egg_info for package pandas\n    pandas requires NumPy >= 1.6 due to datetime64 dependency\n    Complete output from command python setup.py egg_info:\n    pandas requires NumPy >= 1.6 due to datetime64 dependency\n\n----------------------------------------\nCommand python setup.py egg_info failed with error code 1 in /tmp/pip-build/pandas\nStoring complete log in /Users/MyUsername/Library/Logs/pip.log\n']";"['$ sudo easy_install pandas\nSearching for pandas\nReading http://pypi.python.org/simple/pandas/\nReading http://pandas.pydata.org\nReading http://pandas.sourceforge.net\nBest match: pandas 0.8.1\nDownloading http://pypi.python.org/packages/source/p/pandas/pandas-0.8.1.zip#md5=d2c5c5bea971cd760b0ae6f6850fcb74\nProcessing pandas-0.8.1.zip\nRunning pandas-0.8.1/setup.py -q bdist_egg --dist-dir /tmp/easy_install-ckAMym/pandas-0.8.1/egg-dist-tmp-0mlL7t\nerror: Setup script exited with pandas requires NumPy >= 1.6 due to datetime64 dependency\n', '$ sudo easy_install numpy\nSearching for numpy\nBest match: numpy 1.6.2\nAdding numpy 1.6.2 to easy-install.pth file\n\nUsing /Library/Python/2.6/site-packages\nProcessing dependencies for numpy\nFinished processing dependencies for numpy\n', '$ sudo easy_install pandas\n', 'error: Setup script exited with pandas requires NumPy >= 1.6 due to datetime64 dependency\n', '$ python\nPython 2.6.1 (r261:67515, Jun 24 2010, 21:47:49) \n[GCC 4.2.1 (Apple Inc. build 5646)] on darwin\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n>>> import numpy as np\n>>> np.__version__\n\'1.2.1\'\n', 'pip', 'easy_install', '$ sudo pip install numpy\nRequirement already satisfied (use --upgrade to upgrade): numpy in /Library/Python/2.6/site-packages\nCleaning up...\n', '--upgrade', '$ sudo pip install numpy --upgrade\nRequirement already up-to-date: numpy in /Library/Python/2.6/site-packages\nCleaning up...\n\n$ sudo pip install pandas\nDownloading/unpacking pandas\n  Downloading pandas-0.8.1.zip (1.9MB): 1.9MB downloaded\n  Running setup.py egg_info for package pandas\n    pandas requires NumPy >= 1.6 due to datetime64 dependency\n    Complete output from command python setup.py egg_info:\n    pandas requires NumPy >= 1.6 due to datetime64 dependency\n\n----------------------------------------\nCommand python setup.py egg_info failed with error code 1 in /tmp/pip-build/pandas\nStoring complete log in /Users/MyUsername/Library/Logs/pip.log\n']";"[""\nUsing /Library/Python/2.6/site-packages\n'1.2.1'\n\n\n""]";"[""\nUsing /Library/Python/2.6/site-packages\n'1.2.1'\n\n\n""]";False;"[""import pandas as pd\n\nUsing /Library/Python/2.6/site-packages\n'1.2.1'\n\n\n""]";False;3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess']
99;99;99;99;6.0;1;12497402;;1;43;<python><duplicates><pandas>;python pandas: Remove duplicates by columns A, keeping the row with the highest value in column B;38793.0;['A B\n1 10\n1 20\n2 30\n2 40\n3 10\nA B\n1 20\n2 40\n3 10\n'];['A B\n1 10\n1 20\n2 30\n2 40\n3 10\n', 'A B\n1 20\n2 40\n3 10\n'];['A B\n1 10\n1 20\n2 30\n2 40\n3 10\n', 'A B\n1 20\n2 40\n3 10\n'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'A'"", 'not enough values to unpack (expected 2, got 0)']";['KeyError', 'ValueError']
100;100;100;100;1.0;0;12504493;;1;14;<python><pandas>;Adding pandas Series with different indices without getting NaNs;5551.0;"[""Sr1 = pd.Series([1,2,3,4], index = ['A', 'B', 'C', 'D'])\nSr2 = pd.Series([5,6], index = ['A', 'C'])\nSr1        Sr2\nA     1    A     5\nB     2    C     6\nC     3\nD     4\nA     6\nB   NaN\nC     9\nD   NaN\nA     6\nB     2\nC     9\nD     4\n""]";"[""Sr1 = pd.Series([1,2,3,4], index = ['A', 'B', 'C', 'D'])\nSr2 = pd.Series([5,6], index = ['A', 'C'])\nSr1        Sr2\nA     1    A     5\nB     2    C     6\nC     3\nD     4\n"", 'A     6\nB   NaN\nC     9\nD   NaN\n', 'A     6\nB     2\nC     9\nD     4\n']";"[""Sr1 = pd.Series([1,2,3,4], index = ['A', 'B', 'C', 'D'])\nSr2 = pd.Series([5,6], index = ['A', 'C'])\nSr1        Sr2\nA     1    A     5\nB     2    C     6\nC     3\nD     4\n"", 'Sr1 + Sr2', 'Sr1.add(Sr2)', 'A     6\nB   NaN\nC     9\nD   NaN\n', 'A     6\nB     2\nC     9\nD     4\n', 'B', 'D', 'Sr1']";"[""Sr1 = pd.Series([1,2,3,4], index = ['A', 'B', 'C', 'D'])\nSr2 = pd.Series([5,6], index = ['A', 'C'])\n""]";"[""import pandas as pd\nSr1 = pd.Series([1,2,3,4], index = ['A', 'B', 'C', 'D'])\nSr2 = pd.Series([5,6], index = ['A', 'C'])\n""]";True;"[""import pandas as pd\nSr1 = pd.Series([1,2,3,4], index = ['A', 'B', 'C', 'D'])\nSr2 = pd.Series([5,6], index = ['A', 'C'])\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
101;101;101;101;4.0;2;12504951;;1;17;<python><ipython><pandas>;Save session in IPython like in MATLAB?;8203.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
102;102;102;102;3.0;0;12504976;;1;26;<python><string><pandas><split>;"Get last ""column"" after .str.split() operation on column in pandas DataFrame";14984.0;"[""import pandas as pd\ntemp = pd.DataFrame({'ticker' : ['spx 5/25/2001 p500', 'spx 5/25/2001 p600', 'spx 5/25/2001 p700']})\ntemp2 = temp.ticker.str.split(' ')\n0    ['spx', '5/25/2001', 'p500']\n1    ['spx', '5/25/2001', 'p600']\n2    ['spx', '5/25/2001', 'p700']\n""]";"[""import pandas as pd\ntemp = pd.DataFrame({'ticker' : ['spx 5/25/2001 p500', 'spx 5/25/2001 p600', 'spx 5/25/2001 p700']})\ntemp2 = temp.ticker.str.split(' ')\n"", ""0    ['spx', '5/25/2001', 'p500']\n1    ['spx', '5/25/2001', 'p600']\n2    ['spx', '5/25/2001', 'p700']\n""]";"[""DataFrame.str.split(' ')"", '.str.split()', ""import pandas as pd\ntemp = pd.DataFrame({'ticker' : ['spx 5/25/2001 p500', 'spx 5/25/2001 p600', 'spx 5/25/2001 p700']})\ntemp2 = temp.ticker.str.split(' ')\n"", ""0    ['spx', '5/25/2001', 'p500']\n1    ['spx', '5/25/2001', 'p600']\n2    ['spx', '5/25/2001', 'p700']\n"", 'temp2[0]', 'temp2[:][-1]']";"[""import pandas as pd\ntemp = pd.DataFrame({'ticker' : ['spx 5/25/2001 p500', 'spx 5/25/2001 p600', 'spx 5/25/2001 p700']})\ntemp2 = temp.ticker.str.split(' ')\n0    ['spx', '5/25/2001', 'p500']\n1    ['spx', '5/25/2001', 'p600']\n2    ['spx', '5/25/2001', 'p700']\n""]";"[""import pandas as pd\ntemp = pd.DataFrame({'ticker' : ['spx 5/25/2001 p500', 'spx 5/25/2001 p600', 'spx 5/25/2001 p700']})\ntemp2 = temp.ticker.str.split(' ')\n0    ['spx', '5/25/2001', 'p500']\n1    ['spx', '5/25/2001', 'p600']\n2    ['spx', '5/25/2001', 'p700']\n""]";False;"[""import pandas as pd\nimport pandas as pd\ntemp = pd.DataFrame({'ticker' : ['spx 5/25/2001 p500', 'spx 5/25/2001 p600', 'spx 5/25/2001 p700']})\ntemp2 = temp.ticker.str.split(' ')\n0    ['spx', '5/25/2001', 'p500']\n1    ['spx', '5/25/2001', 'p600']\n2    ['spx', '5/25/2001', 'p700']\n""]";False;0;2;"[""name 'd2' is not defined"", ""name 'temp2' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'd2' is not defined"", ""name 'temp2' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'd2' is not defined"", ""name 'temp2' is not defined""]";['NameError', 'NameError']
103;103;103;103;4.0;0;12525722;;1;64;<python><pandas><numpy>;Normalize data in pandas;84877.0;['df.apply(average) \ndf.apply(max) - df.apply(min)\n'];['df.apply(average) \n', 'df.apply(max) - df.apply(min)\n'];['df.apply(average) \n', 'df.apply(max) - df.apply(min)\n'];['df.apply(average) \ndf.apply(max) - df.apply(min)\n'];['df.apply(average) \ndf.apply(max) - df.apply(min)\n'];False;['import pandas as pd\ndf = pd.DataFrame()\ndf.apply(average) \ndf.apply(max) - df.apply(min)\n'];True;1;3;"[""name 'df' is not defined"", 'Sucess', ""No module named 'sklearn'""]";['NameError', 'Sucess', 'ImportError'];1;3;"[""name 'df' is not defined"", 'Sucess', ""No module named 'sklearn'""]";['NameError', 'Sucess', 'ImportError'];2;3;"['Sucess', 'Sucess', ""No module named 'sklearn'""]";['Sucess', 'Sucess', 'ImportError']
104;104;104;104;18.0;0;12555323;;1;350;<python><pandas><dataframe>;Adding new column to existing DataFrame in Python pandas;578606.0;['          a         b         c         d\n2  0.671399  0.101208 -0.181532  0.241273\n3  0.446172 -0.243316  0.051767  1.577318\n5  0.614758  0.075793 -0.451460 -0.012493\n0   -0.335485\n1   -1.166658\n2   -0.385571\ndtype: float64\n'];['          a         b         c         d\n2  0.671399  0.101208 -0.181532  0.241273\n3  0.446172 -0.243316  0.051767  1.577318\n5  0.614758  0.075793 -0.451460 -0.012493\n', '0   -0.335485\n1   -1.166658\n2   -0.385571\ndtype: float64\n'];"['          a         b         c         d\n2  0.671399  0.101208 -0.181532  0.241273\n3  0.446172 -0.243316  0.051767  1.577318\n5  0.614758  0.075793 -0.451460 -0.012493\n', ""'e'"", '0   -0.335485\n1   -1.166658\n2   -0.385571\ndtype: float64\n', 'join', 'append', 'merge', 'e']";['0   -0.335485\n1   -1.166658\n2   -0.385571\n'];['0   -0.335485\n1   -1.166658\n2   -0.385571\n'];False;['import pandas as pd\n0   -0.335485\n1   -1.166658\n2   -0.385571\n'];False;2;7;"[""name 'np' is not defined"", ""name 'Series' is not defined"", 'Sucess', ""name 'df' is not defined"", ""name 'e' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'NameError', 'Sucess', 'NameError', 'NameError', 'Sucess', 'NameError'];2;7;"[""name 'np' is not defined"", ""name 'Series' is not defined"", 'Sucess', ""name 'df' is not defined"", ""name 'e' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'NameError', 'Sucess', 'NameError', 'NameError', 'Sucess', 'NameError'];3;7;"[""name 'np' is not defined"", ""name 'Series' is not defined"", 'Sucess', ""name 'np' is not defined"", ""name 'e' is not defined"", 'Sucess', 'Sucess']";['NameError', 'NameError', 'Sucess', 'NameError', 'NameError', 'Sucess', 'Sucess']
105;105;105;105;2.0;4;12569730;;1;31;<python><wildcard><pandas>;Sum all columns with a wildcard name search using Python Pandas;6662.0;['Day P1S1 P1S2 P1S3 P2S1 P2S2 P2S3\n1   1    2    2    3    1    2\n2   2    2    3    5    4    2\n'];['Day P1S1 P1S2 P1S3 P2S1 P2S2 P2S3\n1   1    2    2    3    1    2\n2   2    2    3    5    4    2\n'];['Day P1S1 P1S2 P1S3 P2S1 P2S2 P2S3\n1   1    2    2    3    1    2\n2   2    2    3    5    4    2\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'data' is not defined""]";['NameError'];0;1;"[""name 'data' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
106;106;106;106;2.0;0;12589481;;1;30;<python><aggregate><pandas>;Python Pandas: Multiple aggregations of the same column;11477.0;"['df = pandas.DataFrame({\n                       ""date"":[datetime.date(2012,x,1) for x in range(1,11)], \n                       ""returns"":0.05*np.random.randn(10), \n                       ""dummy"":np.repeat(1,10) \n                      })\n# Assume `function1` and `function2` are defined for aggregating.\ndf.groupby(""dummy"").agg({""returns"":function1, ""returns"":function2})\n']";"['df = pandas.DataFrame({\n                       ""date"":[datetime.date(2012,x,1) for x in range(1,11)], \n                       ""returns"":0.05*np.random.randn(10), \n                       ""dummy"":np.repeat(1,10) \n                      })\n', '# Assume `function1` and `function2` are defined for aggregating.\ndf.groupby(""dummy"").agg({""returns"":function1, ""returns"":function2})\n']";"['df = pandas.DataFrame({\n                       ""date"":[datetime.date(2012,x,1) for x in range(1,11)], \n                       ""returns"":0.05*np.random.randn(10), \n                       ""dummy"":np.repeat(1,10) \n                      })\n', 'agg', '# Assume `function1` and `function2` are defined for aggregating.\ndf.groupby(""dummy"").agg({""returns"":function1, ""returns"":function2})\n', 'agg', '[(column, function)]']";"['# Assume `function1` and `function2` are defined for aggregating.\ndf.groupby(""dummy"").agg({""returns"":function1, ""returns"":function2})\n']";"['# Assume `function1` and `function2` are defined for aggregating.\ndf.groupby(""dummy"").agg({""returns"":function1, ""returns"":function2})\n']";False;"['import pandas as pd\ndf = pd.DataFrame()\n# Assume `function1` and `function2` are defined for aggregating.\ndf.groupby(""dummy"").agg({""returns"":function1, ""returns"":function2})\n']";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'dummy'""]";['KeyError']
107;107;107;107;1.0;2;12604909;;1;28;<python><database><pandas>;Pandas: how to change all the values of a column?;53708.0;"[""City     Date\nParis    01/04/2004\nLisbon   01/09/2004\nMadrid   2004\nPekin    31/2004\nCity     Date\nParis    2004\nLisbon   2004\nMadrid   2004\nPekin    2004\nfr61_70xls = pd.ExcelFile('AMADEUS FRANCE 1961-1970.xlsx')\n\n#Here we import the individual sheets and clean the sheets    \nyears=(['1961','1962','1963','1964','1965','1966','1967','1968','1969','1970'])\n\nfr={}\n\nheader=(['City','Country','NACE','Cons','Last_year','Op_Rev_EUR_Last_avail_yr','BvD_Indep_Indic','GUO_Name','Legal_status','Date_of_incorporation','Legal_status_date'])\n\nfor year in years:\n    # save every sheet in variable fr['1961'], fr['1962'] and so on\n    fr[year]=fr61_70xls.parse(year,header=0,parse_cols=10)\n    fr[year].columns=header\n    # drop the entire Legal status date column\n    fr[year]=fr[year].drop(['Legal_status_date','Date_of_incorporation'],axis=1)\n    # drop every row where GUO Name is empty\n    fr[year]=fr[year].dropna(axis=0,how='all',subset=[['GUO_Name']])\n    fr[year]=fr[year].set_index(['GUO_Name','Date_of_incorporation'])\n""]";"['City     Date\nParis    01/04/2004\nLisbon   01/09/2004\nMadrid   2004\nPekin    31/2004\n', 'City     Date\nParis    2004\nLisbon   2004\nMadrid   2004\nPekin    2004\n', ""fr61_70xls = pd.ExcelFile('AMADEUS FRANCE 1961-1970.xlsx')\n\n#Here we import the individual sheets and clean the sheets    \nyears=(['1961','1962','1963','1964','1965','1966','1967','1968','1969','1970'])\n\nfr={}\n\nheader=(['City','Country','NACE','Cons','Last_year','Op_Rev_EUR_Last_avail_yr','BvD_Indep_Indic','GUO_Name','Legal_status','Date_of_incorporation','Legal_status_date'])\n\nfor year in years:\n    # save every sheet in variable fr['1961'], fr['1962'] and so on\n    fr[year]=fr61_70xls.parse(year,header=0,parse_cols=10)\n    fr[year].columns=header\n    # drop the entire Legal status date column\n    fr[year]=fr[year].drop(['Legal_status_date','Date_of_incorporation'],axis=1)\n    # drop every row where GUO Name is empty\n    fr[year]=fr[year].dropna(axis=0,how='all',subset=[['GUO_Name']])\n    fr[year]=fr[year].set_index(['GUO_Name','Date_of_incorporation'])\n""]";"['""Date""', 'City     Date\nParis    01/04/2004\nLisbon   01/09/2004\nMadrid   2004\nPekin    31/2004\n', 'City     Date\nParis    2004\nLisbon   2004\nMadrid   2004\nPekin    2004\n', ""fr61_70xls = pd.ExcelFile('AMADEUS FRANCE 1961-1970.xlsx')\n\n#Here we import the individual sheets and clean the sheets    \nyears=(['1961','1962','1963','1964','1965','1966','1967','1968','1969','1970'])\n\nfr={}\n\nheader=(['City','Country','NACE','Cons','Last_year','Op_Rev_EUR_Last_avail_yr','BvD_Indep_Indic','GUO_Name','Legal_status','Date_of_incorporation','Legal_status_date'])\n\nfor year in years:\n    # save every sheet in variable fr['1961'], fr['1962'] and so on\n    fr[year]=fr61_70xls.parse(year,header=0,parse_cols=10)\n    fr[year].columns=header\n    # drop the entire Legal status date column\n    fr[year]=fr[year].drop(['Legal_status_date','Date_of_incorporation'],axis=1)\n    # drop every row where GUO Name is empty\n    fr[year]=fr[year].dropna(axis=0,how='all',subset=[['GUO_Name']])\n    fr[year]=fr[year].set_index(['GUO_Name','Date_of_incorporation'])\n"", ""fr['1961']"", 'Date_of_incorporation']";"[""fr61_70xls = pd.ExcelFile('AMADEUS FRANCE 1961-1970.xlsx')\n\n#Here we import the individual sheets and clean the sheets    \nyears=(['1961','1962','1963','1964','1965','1966','1967','1968','1969','1970'])\n\nfr={}\n\nheader=(['City','Country','NACE','Cons','Last_year','Op_Rev_EUR_Last_avail_yr','BvD_Indep_Indic','GUO_Name','Legal_status','Date_of_incorporation','Legal_status_date'])\n\n    # save every sheet in variable fr['1961'], fr['1962'] and so on\n    # drop the entire Legal status date column\n    # drop every row where GUO Name is empty\n""]";"[""import pandas as pd\nfr61_70xls = pd.ExcelFile('AMADEUS FRANCE 1961-1970.xlsx')\n\n#Here we import the individual sheets and clean the sheets    \nyears=(['1961','1962','1963','1964','1965','1966','1967','1968','1969','1970'])\n\nfr={}\n\nheader=(['City','Country','NACE','Cons','Last_year','Op_Rev_EUR_Last_avail_yr','BvD_Indep_Indic','GUO_Name','Legal_status','Date_of_incorporation','Legal_status_date'])\n\n    # save every sheet in variable fr['1961'], fr['1962'] and so on\n    # drop the entire Legal status date column\n    # drop every row where GUO Name is empty\n""]";True;"[""import pandas as pd\nfr61_70xls = pd.ExcelFile('AMADEUS FRANCE 1961-1970.xlsx')\n\n#Here we import the individual sheets and clean the sheets    \nyears=(['1961','1962','1963','1964','1965','1966','1967','1968','1969','1970'])\n\nfr={}\n\nheader=(['City','Country','NACE','Cons','Last_year','Op_Rev_EUR_Last_avail_yr','BvD_Indep_Indic','GUO_Name','Legal_status','Date_of_incorporation','Legal_status_date'])\n\n    # save every sheet in variable fr['1961'], fr['1962'] and so on\n    # drop the entire Legal status date column\n    # drop every row where GUO Name is empty\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'Date'""]";['KeyError']
108;108;108;108;1.0;0;12625650;;1;15;<grep><row><pandas>;Pandas: grep like function;3264.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'A'""]";['KeyError']
109;109;109;109;9.0;0;12680754;;1;38;<python><pandas><numpy><dataframe>;Split pandas dataframe string entry to separate rows;23089.0;"[""In [7]: a\nOut[7]: \n    var1  var2\n0  a,b,c     1\n1  d,e,f     2\n\nIn [8]: b\nOut[8]: \n  var1  var2\n0    a     1\n1    b     1\n2    c     1\n3    d     2\n4    e     2\n5    f     2\nfrom pandas import DataFrame\nimport numpy as np\na = DataFrame([{'var1': 'a,b,c', 'var2': 1},\n               {'var1': 'd,e,f', 'var2': 2}])\nb = DataFrame([{'var1': 'a', 'var2': 1},\n               {'var1': 'b', 'var2': 1},\n               {'var1': 'c', 'var2': 1},\n               {'var1': 'd', 'var2': 2},\n               {'var1': 'e', 'var2': 2},\n               {'var1': 'f', 'var2': 2}])\ndef fun(row):\n    letters = row['var1']\n    letters = letters.split(',')\n    out = np.array([row] * len(letters))\n    out['var1'] = letters\na['idx'] = range(a.shape[0])\nz = a.groupby('idx')\nz.transform(fun)\n""]";"['In [7]: a\nOut[7]: \n    var1  var2\n0  a,b,c     1\n1  d,e,f     2\n\nIn [8]: b\nOut[8]: \n  var1  var2\n0    a     1\n1    b     1\n2    c     1\n3    d     2\n4    e     2\n5    f     2\n', ""from pandas import DataFrame\nimport numpy as np\na = DataFrame([{'var1': 'a,b,c', 'var2': 1},\n               {'var1': 'd,e,f', 'var2': 2}])\nb = DataFrame([{'var1': 'a', 'var2': 1},\n               {'var1': 'b', 'var2': 1},\n               {'var1': 'c', 'var2': 1},\n               {'var1': 'd', 'var2': 2},\n               {'var1': 'e', 'var2': 2},\n               {'var1': 'f', 'var2': 2}])\n"", ""def fun(row):\n    letters = row['var1']\n    letters = letters.split(',')\n    out = np.array([row] * len(letters))\n    out['var1'] = letters\na['idx'] = range(a.shape[0])\nz = a.groupby('idx')\nz.transform(fun)\n""]";"['pandas dataframe', 'a', 'b', 'In [7]: a\nOut[7]: \n    var1  var2\n0  a,b,c     1\n1  d,e,f     2\n\nIn [8]: b\nOut[8]: \n  var1  var2\n0    a     1\n1    b     1\n2    c     1\n3    d     2\n4    e     2\n5    f     2\n', '.apply', '.transform', ""from pandas import DataFrame\nimport numpy as np\na = DataFrame([{'var1': 'a,b,c', 'var2': 1},\n               {'var1': 'd,e,f', 'var2': 2}])\nb = DataFrame([{'var1': 'a', 'var2': 1},\n               {'var1': 'b', 'var2': 1},\n               {'var1': 'c', 'var2': 1},\n               {'var1': 'd', 'var2': 2},\n               {'var1': 'e', 'var2': 2},\n               {'var1': 'f', 'var2': 2}])\n"", ""def fun(row):\n    letters = row['var1']\n    letters = letters.split(',')\n    out = np.array([row] * len(letters))\n    out['var1'] = letters\na['idx'] = range(a.shape[0])\nz = a.groupby('idx')\nz.transform(fun)\n""]";['a\nb\n'];['a\nb\n'];False;['import pandas as pd\na\nb\n'];False;2;5;"['Sucess', ""name 'DataFrame' is not defined"", 'Sucess', ""name 'tidy_split' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'Sucess', 'NameError', 'NameError'];2;5;"['Sucess', ""name 'a' is not defined"", 'Sucess', ""name 'tidy_split' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'Sucess', 'NameError', 'NameError'];2;5;"['Sucess', ""name 'DataFrame' is not defined"", 'Sucess', ""name 'tidy_split' is not defined"", ""name 'explode' is not defined""]";['Sucess', 'NameError', 'Sucess', 'NameError', 'NameError']
110;110;110;110;3.0;0;12704305;;1;14;<python><pandas><pyodbc>;return column names from pyodbc execute() statement;12842.0;"['from pandas import DataFrame\nimport pyodbc\n\ncnxn = pyodbc.connect(databasez)\ncursor.execute(""""""SELECT ID, NAME AS Nickname, ADDRESS AS Residence FROM tablez"""""")\nDF = DataFrame(cursor.fetchall())\nDF.columns = [\'ID\', \'Nickname\', \'Residence\']\n']";"['from pandas import DataFrame\nimport pyodbc\n\ncnxn = pyodbc.connect(databasez)\ncursor.execute(""""""SELECT ID, NAME AS Nickname, ADDRESS AS Residence FROM tablez"""""")\nDF = DataFrame(cursor.fetchall())\n', ""DF.columns = ['ID', 'Nickname', 'Residence']\n""]";"['from pandas import DataFrame\nimport pyodbc\n\ncnxn = pyodbc.connect(databasez)\ncursor.execute(""""""SELECT ID, NAME AS Nickname, ADDRESS AS Residence FROM tablez"""""")\nDF = DataFrame(cursor.fetchall())\n', ""DF.columns = ['ID', 'Nickname', 'Residence']\n""]";"['from pandas import DataFrame\nimport pyodbc\n\ncnxn = pyodbc.connect(databasez)\ncursor.execute(""""""SELECT ID, NAME AS Nickname, ADDRESS AS Residence FROM tablez"""""")\nDF = DataFrame(cursor.fetchall())\nDF.columns = [\'ID\', \'Nickname\', \'Residence\']\n']";"['from pandas import DataFrame\nimport pyodbc\n\ncnxn = pyodbc.connect(databasez)\ncursor.execute(""""""SELECT ID, NAME AS Nickname, ADDRESS AS Residence FROM tablez"""""")\nDF = DataFrame(cursor.fetchall())\nDF.columns = [\'ID\', \'Nickname\', \'Residence\']\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\nfrom pandas import DataFrame\nimport pyodbc\n\ncnxn = pyodbc.connect(databasez)\ncursor.execute(""""""SELECT ID, NAME AS Nickname, ADDRESS AS Residence FROM tablez"""""")\nDF = DataFrame(cursor.fetchall())\nDF.columns = [\'ID\', \'Nickname\', \'Residence\']\n']";True;1;2;"['Sucess', ""No module named 'pyodbc'""]";['Sucess', 'ImportError'];1;2;"['Sucess', ""No module named 'pyodbc'""]";['Sucess', 'ImportError'];1;2;"['Sucess', ""No module named 'pyodbc'""]";['Sucess', 'ImportError']
111;111;111;111;2.0;2;12725417;;1;21;<python><pandas>;Drop non-numeric columns from a pandas DataFrame;9057.0;['source = pandas.read_table(inputfile, index_col=0)\n'];['source = pandas.read_table(inputfile, index_col=0)\n'];['source = pandas.read_table(inputfile, index_col=0)\n'];['source = pandas.read_table(inputfile, index_col=0)\n'];['source = pandas.read_table(inputfile, index_col=0)\n'];False;['import pandas as pd\nsource = pandas.read_table(inputfile, index_col=0)\n'];False;1;2;"[""name 'source' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'source' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'source' is not defined"", 'Sucess']";['NameError', 'Sucess']
112;112;112;112;4.0;1;12726432;;1;16;<python><pandas><time-series><forecasting><statsmodels>;Package for time series analysis in python;21854.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;2;3;"['Sucess', ""name 'a' is not defined"", 'Sucess']";['Sucess', 'NameError', 'Sucess'];2;3;"['Sucess', ""name 'a' is not defined"", 'Sucess']";['Sucess', 'NameError', 'Sucess'];2;3;"['Sucess', ""name 'a' is not defined"", 'Sucess']";['Sucess', 'NameError', 'Sucess']
113;113;113;113;1.0;3;12741092;;1;22;<python><dataframe><pandas>;Pandas DataFrame: apply function to all columns;13636.0;"[""df=DataFrame({'a':[1,2,3,4,5,6],'b':[2,3,4,5,6,7]})\n\ndf['a']=df['a'].map(lambda x: x > 1)\ndf['a'],df['b']=df['a'].map(lambda x: x > 1),df['b'].map(lambda x: x > 1)\n""]";"[""df=DataFrame({'a':[1,2,3,4,5,6],'b':[2,3,4,5,6,7]})\n\ndf['a']=df['a'].map(lambda x: x > 1)\n"", ""df['a'],df['b']=df['a'].map(lambda x: x > 1),df['b'].map(lambda x: x > 1)\n""]";"['.map(func)', ""df=DataFrame({'a':[1,2,3,4,5,6],'b':[2,3,4,5,6,7]})\n\ndf['a']=df['a'].map(lambda x: x > 1)\n"", ""df['a'],df['b']=df['a'].map(lambda x: x > 1),df['b'].map(lambda x: x > 1)\n""]";"[""df=DataFrame({'a':[1,2,3,4,5,6],'b':[2,3,4,5,6,7]})\n\ndf['a']=df['a'].map(lambda x: x > 1)\ndf['a'],df['b']=df['a'].map(lambda x: x > 1),df['b'].map(lambda x: x > 1)\n""]";"[""df=DataFrame({'a':[1,2,3,4,5,6],'b':[2,3,4,5,6,7]})\n\ndf['a']=df['a'].map(lambda x: x > 1)\ndf['a'],df['b']=df['a'].map(lambda x: x > 1),df['b'].map(lambda x: x > 1)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf=DataFrame({'a':[1,2,3,4,5,6],'b':[2,3,4,5,6,7]})\n\ndf['a']=df['a'].map(lambda x: x > 1)\ndf['a'],df['b']=df['a'].map(lambda x: x > 1),df['b'].map(lambda x: x > 1)\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
114;114;114;114;2.0;0;12844529;;1;11;<python><pandas>;No numeric types to aggregate - change in groupby() behaviour?;14096.0;"['In [31]: data\nOut[31]: \n<class \'pandas.core.frame.DataFrame\'>\nDatetimeIndex: 2557 entries, 2004-01-01 00:00:00 to 2010-12-31 00:00:00\nFreq: <1 DateOffset>\nColumns: 360 entries, -89.75 to 89.75\ndtypes: object(360)\n\nIn [32]: latedges = linspace(-90., 90., 73)\n\nIn [33]: lats_new = linspace(-87.5, 87.5, 72)\n\nIn [34]: def _get_gridbox_label(x, bins, labels):\n   ....:             return labels[searchsorted(bins, x) - 1]\n   ....: \n\nIn [35]: lat_bucket = lambda x: _get_gridbox_label(x, latedges, lats_new)\n\nIn [36]: data.T.groupby(lat_bucket).mean()\n---------------------------------------------------------------------------\nDataError                                 Traceback (most recent call last)\n<ipython-input-36-ed9c538ac526> in <module>()\n----> 1 data.T.groupby(lat_bucket).mean()\n\n/usr/lib/python2.7/site-packages/pandas/core/groupby.py in mean(self)\n    295         """"""\n    296         try:\n--> 297             return self._cython_agg_general(\'mean\')\n    298         except DataError:\n    299             raise\n\n/usr/lib/python2.7/site-packages/pandas/core/groupby.py in _cython_agg_general(self, how, numeric_only)\n   1415 \n   1416     def _cython_agg_general(self, how, numeric_only=True):\n-> 1417         new_blocks = self._cython_agg_blocks(how, numeric_only=numeric_only)\n   1418         return self._wrap_agged_blocks(new_blocks)\n   1419 \n\n/usr/lib/python2.7/site-packages/pandas/core/groupby.py in _cython_agg_blocks(self, how, numeric_only)\n   1455 \n   1456         if len(new_blocks) == 0:\n-> 1457             raise DataError(\'No numeric types to aggregate\')\n   1458 \n   1459         return new_blocks\n\nDataError: No numeric types to aggregate\n']";"['In [31]: data\nOut[31]: \n<class \'pandas.core.frame.DataFrame\'>\nDatetimeIndex: 2557 entries, 2004-01-01 00:00:00 to 2010-12-31 00:00:00\nFreq: <1 DateOffset>\nColumns: 360 entries, -89.75 to 89.75\ndtypes: object(360)\n\nIn [32]: latedges = linspace(-90., 90., 73)\n\nIn [33]: lats_new = linspace(-87.5, 87.5, 72)\n\nIn [34]: def _get_gridbox_label(x, bins, labels):\n   ....:             return labels[searchsorted(bins, x) - 1]\n   ....: \n\nIn [35]: lat_bucket = lambda x: _get_gridbox_label(x, latedges, lats_new)\n\nIn [36]: data.T.groupby(lat_bucket).mean()\n---------------------------------------------------------------------------\nDataError                                 Traceback (most recent call last)\n<ipython-input-36-ed9c538ac526> in <module>()\n----> 1 data.T.groupby(lat_bucket).mean()\n\n/usr/lib/python2.7/site-packages/pandas/core/groupby.py in mean(self)\n    295         """"""\n    296         try:\n--> 297             return self._cython_agg_general(\'mean\')\n    298         except DataError:\n    299             raise\n\n/usr/lib/python2.7/site-packages/pandas/core/groupby.py in _cython_agg_general(self, how, numeric_only)\n   1415 \n   1416     def _cython_agg_general(self, how, numeric_only=True):\n-> 1417         new_blocks = self._cython_agg_blocks(how, numeric_only=numeric_only)\n   1418         return self._wrap_agged_blocks(new_blocks)\n   1419 \n\n/usr/lib/python2.7/site-packages/pandas/core/groupby.py in _cython_agg_blocks(self, how, numeric_only)\n   1455 \n   1456         if len(new_blocks) == 0:\n-> 1457             raise DataError(\'No numeric types to aggregate\')\n   1458 \n   1459         return new_blocks\n\nDataError: No numeric types to aggregate\n']";"['In [31]: data\nOut[31]: \n<class \'pandas.core.frame.DataFrame\'>\nDatetimeIndex: 2557 entries, 2004-01-01 00:00:00 to 2010-12-31 00:00:00\nFreq: <1 DateOffset>\nColumns: 360 entries, -89.75 to 89.75\ndtypes: object(360)\n\nIn [32]: latedges = linspace(-90., 90., 73)\n\nIn [33]: lats_new = linspace(-87.5, 87.5, 72)\n\nIn [34]: def _get_gridbox_label(x, bins, labels):\n   ....:             return labels[searchsorted(bins, x) - 1]\n   ....: \n\nIn [35]: lat_bucket = lambda x: _get_gridbox_label(x, latedges, lats_new)\n\nIn [36]: data.T.groupby(lat_bucket).mean()\n---------------------------------------------------------------------------\nDataError                                 Traceback (most recent call last)\n<ipython-input-36-ed9c538ac526> in <module>()\n----> 1 data.T.groupby(lat_bucket).mean()\n\n/usr/lib/python2.7/site-packages/pandas/core/groupby.py in mean(self)\n    295         """"""\n    296         try:\n--> 297             return self._cython_agg_general(\'mean\')\n    298         except DataError:\n    299             raise\n\n/usr/lib/python2.7/site-packages/pandas/core/groupby.py in _cython_agg_general(self, how, numeric_only)\n   1415 \n   1416     def _cython_agg_general(self, how, numeric_only=True):\n-> 1417         new_blocks = self._cython_agg_blocks(how, numeric_only=numeric_only)\n   1418         return self._wrap_agged_blocks(new_blocks)\n   1419 \n\n/usr/lib/python2.7/site-packages/pandas/core/groupby.py in _cython_agg_blocks(self, how, numeric_only)\n   1455 \n   1456         if len(new_blocks) == 0:\n-> 1457             raise DataError(\'No numeric types to aggregate\')\n   1458 \n   1459         return new_blocks\n\nDataError: No numeric types to aggregate\n']";['data\n'];['data\n'];False;['import pandas as pd\ndata = pd.DataFrame()\ndata\n'];True;0;1;"[""name 'data' is not defined""]";['NameError'];0;1;"[""name 'data' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
115;115;115;115;3.0;1;12850345;;1;17;<python><pandas>;how to combine two data frames in python pandas;45587.0;['A.label = 1\n'];['A.label = 1\n'];['A = D[D.label == k]', 'B = D[D.label != k]', 'A.label = 1\n', 'B.label = -1'];['A.label = 1\n'];['A.label = 1\n'];False;['import pandas as pd\nA.label = 1\n'];False;0;3;"[""name 'data1' is not defined"", ""name 'pd' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'data1' is not defined"", ""name 'data1' is not defined"", ""name 'df_a' is not defined""]";['NameError', 'NameError', 'NameError'];2;3;"['Sucess', 'Sucess', ""name 'df_a' is not defined""]";['Sucess', 'Sucess', 'NameError']
116;116;116;116;3.0;1;12860421;;1;24;<python><pandas><pivot-table>;Python Pandas : pivot table with aggfunc = count unique distinct;31611.0;"[""df2 = pd.DataFrame({'X' : ['X1', 'X1', 'X1', 'X1'], 'Y' : ['Y2','Y1','Y1','Y1'], 'Z' : ['Z3','Z1','Z1','Z2']})\n\n    X   Y   Z\n0  X1  Y2  Z3\n1  X1  Y1  Z1\n2  X1  Y1  Z1\n3  X1  Y1  Z2\n\ng=df2.groupby('X')\n\npd.pivot_table(g, values='X', rows='Y', cols='Z', margins=False, aggfunc='count')\nZ   Z1  Z2  Z3\nY             \nY1   1   1 NaN\nY2 NaN NaN   1\n""]";"[""df2 = pd.DataFrame({'X' : ['X1', 'X1', 'X1', 'X1'], 'Y' : ['Y2','Y1','Y1','Y1'], 'Z' : ['Z3','Z1','Z1','Z2']})\n\n    X   Y   Z\n0  X1  Y2  Z3\n1  X1  Y1  Z1\n2  X1  Y1  Z1\n3  X1  Y1  Z2\n\ng=df2.groupby('X')\n\npd.pivot_table(g, values='X', rows='Y', cols='Z', margins=False, aggfunc='count')\n"", 'Z   Z1  Z2  Z3\nY             \nY1   1   1 NaN\nY2 NaN NaN   1\n']";"[""df2 = pd.DataFrame({'X' : ['X1', 'X1', 'X1', 'X1'], 'Y' : ['Y2','Y1','Y1','Y1'], 'Z' : ['Z3','Z1','Z1','Z2']})\n\n    X   Y   Z\n0  X1  Y2  Z3\n1  X1  Y1  Z1\n2  X1  Y1  Z1\n3  X1  Y1  Z2\n\ng=df2.groupby('X')\n\npd.pivot_table(g, values='X', rows='Y', cols='Z', margins=False, aggfunc='count')\n"", 'aggfunc', 'np.bincount()', 'values_counts()', 'Z   Z1  Z2  Z3\nY             \nY1   1   1 NaN\nY2 NaN NaN   1\n']";"[""df2 = pd.DataFrame({'X' : ['X1', 'X1', 'X1', 'X1'], 'Y' : ['Y2','Y1','Y1','Y1'], 'Z' : ['Z3','Z1','Z1','Z2']})\n\n\ng=df2.groupby('X')\n\npd.pivot_table(g, values='X', rows='Y', cols='Z', margins=False, aggfunc='count')\nY             \n""]";"[""import pandas as pd\ndf2 = pd.DataFrame({'X' : ['X1', 'X1', 'X1', 'X1'], 'Y' : ['Y2','Y1','Y1','Y1'], 'Z' : ['Z3','Z1','Z1','Z2']})\n\n\ng=df2.groupby('X')\n\npd.pivot_table(g, values='X', rows='Y', cols='Z', margins=False, aggfunc='count')\nY             \n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\ndf2 = pd.DataFrame({'X' : ['X1', 'X1', 'X1', 'X1'], 'Y' : ['Y2','Y1','Y1','Y1'], 'Z' : ['Z3','Z1','Z1','Z2']})\n\n\ng=df2.groupby('X')\n\npd.pivot_table(g, values='X', rows='Y', cols='Z', margins=False, aggfunc='count')\nY             \n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
117;117;117;117;5.0;4;12867178;;1;26;<python><pandas>;pandas: count things;42829.0;"['mc = [ sum( male_trips[\'start_station_id\'] == id ) for id in stations[\'id\'] ]\nfrom timeit import Timer\nsetup = ""import pandas; male_trips=pandas.load(\'maletrips\')""\na  = ""male_trips.start_station_id.value_counts()""\nb = ""male_trips.groupby(\'start_station_id\').size()""\nTimer(a,setup).timeit(100)\nTimer(b,setup).timeit(100)\nIn [4]: Timer(a,setup).timeit(100) # <- this is value_counts\nOut[4]: 9.709594964981079\n\nIn [5]: Timer(b,setup).timeit(100) # <- this is groupby / size\nOut[5]: 1.5574288368225098\n']";"[""mc = [ sum( male_trips['start_station_id'] == id ) for id in stations['id'] ]\n"", 'from timeit import Timer\nsetup = ""import pandas; male_trips=pandas.load(\'maletrips\')""\na  = ""male_trips.start_station_id.value_counts()""\nb = ""male_trips.groupby(\'start_station_id\').size()""\nTimer(a,setup).timeit(100)\nTimer(b,setup).timeit(100)\n', 'In [4]: Timer(a,setup).timeit(100) # <- this is value_counts\nOut[4]: 9.709594964981079\n\nIn [5]: Timer(b,setup).timeit(100) # <- this is groupby / size\nOut[5]: 1.5574288368225098\n']";"[""mc = [ sum( male_trips['start_station_id'] == id ) for id in stations['id'] ]\n"", 'groupby()', 'size()', '.value_counts()', 'timeit', 'groupby', 'from timeit import Timer\nsetup = ""import pandas; male_trips=pandas.load(\'maletrips\')""\na  = ""male_trips.start_station_id.value_counts()""\nb = ""male_trips.groupby(\'start_station_id\').size()""\nTimer(a,setup).timeit(100)\nTimer(b,setup).timeit(100)\n', 'In [4]: Timer(a,setup).timeit(100) # <- this is value_counts\nOut[4]: 9.709594964981079\n\nIn [5]: Timer(b,setup).timeit(100) # <- this is groupby / size\nOut[5]: 1.5574288368225098\n']";['Timer(a,setup).timeit(100) # <- this is value_counts\nTimer(b,setup).timeit(100) # <- this is groupby / size\n'];['Timer(a,setup).timeit(100) # <- this is value_counts\nTimer(b,setup).timeit(100) # <- this is groupby / size\n'];False;['import pandas as pd\nTimer(a,setup).timeit(100) # <- this is value_counts\nTimer(b,setup).timeit(100) # <- this is groupby / size\n'];False;0;2;"[""name 'male_trips' is not defined"", ""name 'male_trips' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'male_trips' is not defined"", ""name 'male_trips' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'male_trips' is not defined"", ""name 'male_trips' is not defined""]";['NameError', 'NameError']
118;118;118;118;2.0;3;12877189;;1;21;<python><numpy><pandas>;float64 with pandas to_csv;18798.0;['Bob,0.085\nAlice,0.005\ndf = pd.read_csv(orig)\ndf.to_csv(pandasfile)\nBob,0.085000000000000006\nAlice,0.0050000000000000001\n'];['Bob,0.085\nAlice,0.005\n', 'df = pd.read_csv(orig)\ndf.to_csv(pandasfile)\n', 'Bob,0.085000000000000006\nAlice,0.0050000000000000001\n'];['Bob,0.085\nAlice,0.005\n', 'df = pd.read_csv(orig)\ndf.to_csv(pandasfile)\n', 'pandasfile', 'Bob,0.085000000000000006\nAlice,0.0050000000000000001\n'];['Bob,0.085\nAlice,0.005\ndf = pd.read_csv(orig)\ndf.to_csv(pandasfile)\nBob,0.085000000000000006\nAlice,0.0050000000000000001\n'];['import pandas as pd\nBob,0.085\nAlice,0.005\ndf = pd.read_csv(orig)\ndf.to_csv(pandasfile)\nBob,0.085000000000000006\nAlice,0.0050000000000000001\n'];True;['import pandas as pd\nBob,0.085\nAlice,0.005\ndf = pd.read_csv(orig)\ndf.to_csv(pandasfile)\nBob,0.085000000000000006\nAlice,0.0050000000000000001\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'Bob' is not defined""]";['NameError']
119;119;119;119;1.0;3;12945971;;1;70;<python><matplotlib><pandas>;Pandas timeseries plot setting x-axis major and minor ticks and labels;63942.0;"['import pandas\nprint \'pandas.__version__ is \', pandas.__version__\nprint \'matplotlib.__version__ is \', matplotlib.__version__    \n\ndStart = datetime.datetime(2011,5,1) # 1 May\ndEnd = datetime.datetime(2011,7,1) # 1 July    \n\ndateIndex = pandas.date_range(start=dStart, end=dEnd, freq=\'D\')\nprint ""1 May to 1 July 2011"", dateIndex      \n\ntestSeries = pandas.Series(data=np.random.randn(len(dateIndex)),\n                           index=dateIndex)    \n\nax = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\ntestSeries.plot(ax=ax, style=\'v-\', label=\'first line\')    \n\n# using MatPlotLib date time locators and formatters doesn\'t work with new\n# pandas datetime index\nax.xaxis.set_minor_locator(matplotlib.dates.WeekdayLocator(byweekday=(1),\n                                                           interval=1))\nax.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter(\'%d\\n%a\'))\nax.xaxis.grid(True, which=""minor"")\nax.xaxis.grid(False, which=""major"")\nax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\'\\n\\n\\n%b%Y\'))\nplt.show()    \n\n# set the major xticks and labels through pandas\nax2 = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\nxticks = pandas.date_range(start=dStart, end=dEnd, freq=\'W-Tue\')\nprint ""xticks: "", xticks\ntestSeries.plot(ax=ax2, style=\'-v\', label=\'second line\',\n                xticks=xticks.to_pydatetime())\nax2.set_xticklabels([x.strftime(\'%a\\n%d\\n%h\\n%Y\') for x in xticks]);\n# set the text of the first few minor ticks created by pandas.plot\n#    ax2.set_xticklabels([\'a\',\'b\',\'c\',\'d\',\'e\'], minor=True)\n# remove the minor xtick labels set by pandas.plot \nax2.set_xticklabels([], minor=True)\n# turn the minor ticks created by pandas.plot off \n# plt.minorticks_off()\nplt.show()\nprint testSeries[\'6/4/2011\':\'6/7/2011\']\npandas.__version__ is  0.9.1.dev-3de54ae\nmatplotlib.__version__ is  1.1.1\n1 May to 1 July 2011 <class \'pandas.tseries.index.DatetimeIndex\'>\n[2011-05-01 00:00:00, ..., 2011-07-01 00:00:00]\nLength: 62, Freq: D, Timezone: None\nxticks:  <class \'pandas.tseries.index.DatetimeIndex\'>\n[2011-05-03 00:00:00, ..., 2011-06-28 00:00:00]\nLength: 9, Freq: W-TUE, Timezone: None\n2011-06-04   -0.199393\n2011-06-05   -0.043118\n2011-06-06    0.477771\n2011-06-07   -0.033207\nFreq: D\n# only show month for first label in month\nmonth = dStart.month - 1\nxticklabels = []\nfor x in xticks:\n    if  month != x.month :\n        xticklabels.append(x.strftime(\'%d\\n%a\\n%h\'))\n        month = x.month\n    else:\n        xticklabels.append(x.strftime(\'%d\\n%a\'))\n']";"['import pandas\nprint \'pandas.__version__ is \', pandas.__version__\nprint \'matplotlib.__version__ is \', matplotlib.__version__    \n\ndStart = datetime.datetime(2011,5,1) # 1 May\ndEnd = datetime.datetime(2011,7,1) # 1 July    \n\ndateIndex = pandas.date_range(start=dStart, end=dEnd, freq=\'D\')\nprint ""1 May to 1 July 2011"", dateIndex      \n\ntestSeries = pandas.Series(data=np.random.randn(len(dateIndex)),\n                           index=dateIndex)    \n\nax = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\ntestSeries.plot(ax=ax, style=\'v-\', label=\'first line\')    \n\n# using MatPlotLib date time locators and formatters doesn\'t work with new\n# pandas datetime index\nax.xaxis.set_minor_locator(matplotlib.dates.WeekdayLocator(byweekday=(1),\n                                                           interval=1))\nax.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter(\'%d\\n%a\'))\nax.xaxis.grid(True, which=""minor"")\nax.xaxis.grid(False, which=""major"")\nax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\'\\n\\n\\n%b%Y\'))\nplt.show()    \n\n# set the major xticks and labels through pandas\nax2 = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\nxticks = pandas.date_range(start=dStart, end=dEnd, freq=\'W-Tue\')\nprint ""xticks: "", xticks\ntestSeries.plot(ax=ax2, style=\'-v\', label=\'second line\',\n                xticks=xticks.to_pydatetime())\nax2.set_xticklabels([x.strftime(\'%a\\n%d\\n%h\\n%Y\') for x in xticks]);\n# set the text of the first few minor ticks created by pandas.plot\n#    ax2.set_xticklabels([\'a\',\'b\',\'c\',\'d\',\'e\'], minor=True)\n# remove the minor xtick labels set by pandas.plot \nax2.set_xticklabels([], minor=True)\n# turn the minor ticks created by pandas.plot off \n# plt.minorticks_off()\nplt.show()\nprint testSeries[\'6/4/2011\':\'6/7/2011\']\n', ""pandas.__version__ is  0.9.1.dev-3de54ae\nmatplotlib.__version__ is  1.1.1\n1 May to 1 July 2011 <class 'pandas.tseries.index.DatetimeIndex'>\n[2011-05-01 00:00:00, ..., 2011-07-01 00:00:00]\nLength: 62, Freq: D, Timezone: None\n"", ""xticks:  <class 'pandas.tseries.index.DatetimeIndex'>\n[2011-05-03 00:00:00, ..., 2011-06-28 00:00:00]\nLength: 9, Freq: W-TUE, Timezone: None\n"", '2011-06-04   -0.199393\n2011-06-05   -0.043118\n2011-06-06    0.477771\n2011-06-07   -0.033207\nFreq: D\n', ""# only show month for first label in month\nmonth = dStart.month - 1\nxticklabels = []\nfor x in xticks:\n    if  month != x.month :\n        xticklabels.append(x.strftime('%d\\n%a\\n%h'))\n        month = x.month\n    else:\n        xticklabels.append(x.strftime('%d\\n%a'))\n""]";"['ax.xaxis.set_major_locator', 'ax.xaxis.set_major_formatter', 'import pandas\nprint \'pandas.__version__ is \', pandas.__version__\nprint \'matplotlib.__version__ is \', matplotlib.__version__    \n\ndStart = datetime.datetime(2011,5,1) # 1 May\ndEnd = datetime.datetime(2011,7,1) # 1 July    \n\ndateIndex = pandas.date_range(start=dStart, end=dEnd, freq=\'D\')\nprint ""1 May to 1 July 2011"", dateIndex      \n\ntestSeries = pandas.Series(data=np.random.randn(len(dateIndex)),\n                           index=dateIndex)    \n\nax = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\ntestSeries.plot(ax=ax, style=\'v-\', label=\'first line\')    \n\n# using MatPlotLib date time locators and formatters doesn\'t work with new\n# pandas datetime index\nax.xaxis.set_minor_locator(matplotlib.dates.WeekdayLocator(byweekday=(1),\n                                                           interval=1))\nax.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter(\'%d\\n%a\'))\nax.xaxis.grid(True, which=""minor"")\nax.xaxis.grid(False, which=""major"")\nax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\'\\n\\n\\n%b%Y\'))\nplt.show()    \n\n# set the major xticks and labels through pandas\nax2 = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\nxticks = pandas.date_range(start=dStart, end=dEnd, freq=\'W-Tue\')\nprint ""xticks: "", xticks\ntestSeries.plot(ax=ax2, style=\'-v\', label=\'second line\',\n                xticks=xticks.to_pydatetime())\nax2.set_xticklabels([x.strftime(\'%a\\n%d\\n%h\\n%Y\') for x in xticks]);\n# set the text of the first few minor ticks created by pandas.plot\n#    ax2.set_xticklabels([\'a\',\'b\',\'c\',\'d\',\'e\'], minor=True)\n# remove the minor xtick labels set by pandas.plot \nax2.set_xticklabels([], minor=True)\n# turn the minor ticks created by pandas.plot off \n# plt.minorticks_off()\nplt.show()\nprint testSeries[\'6/4/2011\':\'6/7/2011\']\n', ""pandas.__version__ is  0.9.1.dev-3de54ae\nmatplotlib.__version__ is  1.1.1\n1 May to 1 July 2011 <class 'pandas.tseries.index.DatetimeIndex'>\n[2011-05-01 00:00:00, ..., 2011-07-01 00:00:00]\nLength: 62, Freq: D, Timezone: None\n"", ""xticks:  <class 'pandas.tseries.index.DatetimeIndex'>\n[2011-05-03 00:00:00, ..., 2011-06-28 00:00:00]\nLength: 9, Freq: W-TUE, Timezone: None\n"", '2011-06-04   -0.199393\n2011-06-05   -0.043118\n2011-06-06    0.477771\n2011-06-07   -0.033207\nFreq: D\n', ""# only show month for first label in month\nmonth = dStart.month - 1\nxticklabels = []\nfor x in xticks:\n    if  month != x.month :\n        xticklabels.append(x.strftime('%d\\n%a\\n%h'))\n        month = x.month\n    else:\n        xticklabels.append(x.strftime('%d\\n%a'))\n"", 'ax.annotate']";"['import pandas\n\ndStart = datetime.datetime(2011,5,1) # 1 May\ndEnd = datetime.datetime(2011,7,1) # 1 July    \n\ndateIndex = pandas.date_range(start=dStart, end=dEnd, freq=\'D\')\n\n\nax = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\ntestSeries.plot(ax=ax, style=\'v-\', label=\'first line\')    \n\n# using MatPlotLib date time locators and formatters doesn\'t work with new\n# pandas datetime index\nax.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter(\'%d\\n%a\'))\nax.xaxis.grid(True, which=""minor"")\nax.xaxis.grid(False, which=""major"")\nax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\'\\n\\n\\n%b%Y\'))\nplt.show()    \n\n# set the major xticks and labels through pandas\nax2 = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\nxticks = pandas.date_range(start=dStart, end=dEnd, freq=\'W-Tue\')\nax2.set_xticklabels([x.strftime(\'%a\\n%d\\n%h\\n%Y\') for x in xticks]);\n# set the text of the first few minor ticks created by pandas.plot\n#    ax2.set_xticklabels([\'a\',\'b\',\'c\',\'d\',\'e\'], minor=True)\n# remove the minor xtick labels set by pandas.plot \nax2.set_xticklabels([], minor=True)\n# turn the minor ticks created by pandas.plot off \n# plt.minorticks_off()\nplt.show()\n# only show month for first label in month\nmonth = dStart.month - 1\nxticklabels = []\n']";"['import pandas\n\ndStart = datetime.datetime(2011,5,1) # 1 May\ndEnd = datetime.datetime(2011,7,1) # 1 July    \n\ndateIndex = pandas.date_range(start=dStart, end=dEnd, freq=\'D\')\n\n\nax = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\ntestSeries.plot(ax=ax, style=\'v-\', label=\'first line\')    \n\n# using MatPlotLib date time locators and formatters doesn\'t work with new\n# pandas datetime index\nax.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter(\'%d\\n%a\'))\nax.xaxis.grid(True, which=""minor"")\nax.xaxis.grid(False, which=""major"")\nax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\'\\n\\n\\n%b%Y\'))\nplt.show()    \n\n# set the major xticks and labels through pandas\nax2 = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\nxticks = pandas.date_range(start=dStart, end=dEnd, freq=\'W-Tue\')\nax2.set_xticklabels([x.strftime(\'%a\\n%d\\n%h\\n%Y\') for x in xticks]);\n# set the text of the first few minor ticks created by pandas.plot\n#    ax2.set_xticklabels([\'a\',\'b\',\'c\',\'d\',\'e\'], minor=True)\n# remove the minor xtick labels set by pandas.plot \nax2.set_xticklabels([], minor=True)\n# turn the minor ticks created by pandas.plot off \n# plt.minorticks_off()\nplt.show()\n# only show month for first label in month\nmonth = dStart.month - 1\nxticklabels = []\n']";False;"['import pandas as pd\nimport pandas\n\ndStart = datetime.datetime(2011,5,1) # 1 May\ndEnd = datetime.datetime(2011,7,1) # 1 July    \n\ndateIndex = pandas.date_range(start=dStart, end=dEnd, freq=\'D\')\n\n\nax = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\ntestSeries.plot(ax=ax, style=\'v-\', label=\'first line\')    \n\n# using MatPlotLib date time locators and formatters doesn\'t work with new\n# pandas datetime index\nax.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter(\'%d\\n%a\'))\nax.xaxis.grid(True, which=""minor"")\nax.xaxis.grid(False, which=""major"")\nax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\'\\n\\n\\n%b%Y\'))\nplt.show()    \n\n# set the major xticks and labels through pandas\nax2 = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\nxticks = pandas.date_range(start=dStart, end=dEnd, freq=\'W-Tue\')\nax2.set_xticklabels([x.strftime(\'%a\\n%d\\n%h\\n%Y\') for x in xticks]);\n# set the text of the first few minor ticks created by pandas.plot\n#    ax2.set_xticklabels([\'a\',\'b\',\'c\',\'d\',\'e\'], minor=True)\n# remove the minor xtick labels set by pandas.plot \nax2.set_xticklabels([], minor=True)\n# turn the minor ticks created by pandas.plot off \n# plt.minorticks_off()\nplt.show()\n# only show month for first label in month\nmonth = dStart.month - 1\nxticklabels = []\n']";False;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
120;120;120;120;2.0;1;12960574;;1;18;<python><pandas>;pandas read_csv index_col=None not working;15871.0;"[""import pandas as pd\nfec = pd.read_csv('P00000001-ALL.csv',nrows=10,index_col=None)\nIn [20]: fec\n\nOut[20]:\n<class 'pandas.core.frame.DataFrame'>\nIndex: 10 entries, C00410118 to C00410118\nData columns:\n...\ndtypes: float64(4), int64(3), object(11)\nIn [13]: fec = read_csv('P00000001-ALL.csv')\nIn [14]: fec\nOut[14]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1001731 entries, 0 to 1001730\n...\ndtypes: float64(1), int64(1), object(14)\n""]";"[""import pandas as pd\nfec = pd.read_csv('P00000001-ALL.csv',nrows=10,index_col=None)\n"", ""In [20]: fec\n\nOut[20]:\n<class 'pandas.core.frame.DataFrame'>\nIndex: 10 entries, C00410118 to C00410118\nData columns:\n...\ndtypes: float64(4), int64(3), object(11)\n"", ""In [13]: fec = read_csv('P00000001-ALL.csv')\nIn [14]: fec\nOut[14]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1001731 entries, 0 to 1001730\n...\ndtypes: float64(1), int64(1), object(14)\n""]";"[""import pandas as pd\nfec = pd.read_csv('P00000001-ALL.csv',nrows=10,index_col=None)\n"", ""In [20]: fec\n\nOut[20]:\n<class 'pandas.core.frame.DataFrame'>\nIndex: 10 entries, C00410118 to C00410118\nData columns:\n...\ndtypes: float64(4), int64(3), object(11)\n"", ""In [13]: fec = read_csv('P00000001-ALL.csv')\nIn [14]: fec\nOut[14]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1001731 entries, 0 to 1001730\n...\ndtypes: float64(1), int64(1), object(14)\n""]";"[""fec\n\nfec = read_csv('P00000001-ALL.csv')\n""]";"[""fec\n\nfec = read_csv('P00000001-ALL.csv')\n""]";False;"[""import pandas as pd\nfec\n\nfec = read_csv('P00000001-ALL.csv')\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
121;121;121;121;6.0;0;13003051;;1;21;<pandas>;How do I exclude a few columns from a DataFrame plot?;15239.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;['\n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated', '\n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated'];['DeprecationWarning', 'DeprecationWarning']
122;122;122;122;4.0;0;13019719;;1;11;<python><pandas>;Get business days between start and end date using pandas;13990.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;2;4;"[""name 's' is not defined"", ""name 'pd' is not defined"", 'Sucess', 'Sucess']";['NameError', 'NameError', 'Sucess', 'Sucess'];1;4;"[""name 's' is not defined"", ""name 'BDay' is not defined"", 'Sucess', 'Non-vectorized DateOffset being applied to Series or DatetimeIndex']";['NameError', 'NameError', 'Sucess', 'PerformanceWarning'];1;4;"[""name 's' is not defined"", ""name 'BDay' is not defined"", 'Sucess', 'Non-vectorized DateOffset being applied to Series or DatetimeIndex']";['NameError', 'NameError', 'Sucess', 'PerformanceWarning']
123;123;123;123;3.0;0;13021654;;1;54;<python><dataframe><pandas>;Retrieving column index from column name in python pandas;40205.0;['idx <- which(names(my_data)==my_colum_name)\n'];['idx <- which(names(my_data)==my_colum_name)\n'];['idx <- which(names(my_data)==my_colum_name)\n'];['idx <- which(names(my_data)==my_colum_name)\n'];['idx <- which(names(my_data)==my_colum_name)\n'];False;['import pandas as pd\ndata = pd.DataFrame()\nidx <- which(names(my_data)==my_colum_name)\n'];True;1;2;"[""name 'DataFrame' is not defined"", 'Sucess']";['NameError', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];1;2;"[""name 'DataFrame' is not defined"", 'Sucess']";['NameError', 'Sucess']
124;124;124;124;4.0;0;13035764;;1;90;<python><pandas>;Remove rows with duplicate indices (Pandas DataFrame and TimeSeries);67548.0;"[""                      Sta  Precip1hr  Precip5min  Temp  DewPnt  WindSpd  WindDir  AtmPress\nDate                                                                                      \n2001-01-01 00:00:00  KPDX          0           0     4       3        0        0     30.31\n2001-01-01 00:05:00  KPDX          0           0     4       3        0        0     30.30\n2001-01-01 00:10:00  KPDX          0           0     4       3        4       80     30.30\n2001-01-01 00:15:00  KPDX          0           0     3       2        5       90     30.30\n2001-01-01 00:20:00  KPDX          0           0     3       2       10      110     30.28\nimport pandas \nimport datetime\nstartdate = datetime.datetime(2001, 1, 1, 0, 0)\nenddate = datetime.datetime(2001, 1, 1, 5, 0)\nindex = pandas.DatetimeIndex(start=startdate, end=enddate, freq='H')\ndata = {'A' : range(6), 'B' : range(6)}\ndata1 = {'A' : [20, -30, 40], 'B' : [-50, 60, -70]}\ndf1 = pandas.DataFrame(data=data, index=index)\ndf2 = pandas.DataFrame(data=data1, index=index[:3])\ndf3 = df1.append(df2)\ndf3\n                       A   B\n2001-01-01 00:00:00   20 -50\n2001-01-01 01:00:00  -30  60\n2001-01-01 02:00:00   40 -70\n2001-01-01 03:00:00    3   3\n2001-01-01 04:00:00    4   4\n2001-01-01 05:00:00    5   5\n2001-01-01 00:00:00    0   0\n2001-01-01 01:00:00    1   1\n2001-01-01 02:00:00    2   2\n                       A   B\n2001-01-01 00:00:00    0   0\n2001-01-01 01:00:00    1   1\n2001-01-01 02:00:00    2   2\n2001-01-01 03:00:00    3   3\n2001-01-01 04:00:00    4   4\n2001-01-01 05:00:00    5   5\n""]";"['                      Sta  Precip1hr  Precip5min  Temp  DewPnt  WindSpd  WindDir  AtmPress\nDate                                                                                      \n2001-01-01 00:00:00  KPDX          0           0     4       3        0        0     30.31\n2001-01-01 00:05:00  KPDX          0           0     4       3        0        0     30.30\n2001-01-01 00:10:00  KPDX          0           0     4       3        4       80     30.30\n2001-01-01 00:15:00  KPDX          0           0     3       2        5       90     30.30\n2001-01-01 00:20:00  KPDX          0           0     3       2       10      110     30.28\n', ""import pandas \nimport datetime\nstartdate = datetime.datetime(2001, 1, 1, 0, 0)\nenddate = datetime.datetime(2001, 1, 1, 5, 0)\nindex = pandas.DatetimeIndex(start=startdate, end=enddate, freq='H')\ndata = {'A' : range(6), 'B' : range(6)}\ndata1 = {'A' : [20, -30, 40], 'B' : [-50, 60, -70]}\ndf1 = pandas.DataFrame(data=data, index=index)\ndf2 = pandas.DataFrame(data=data1, index=index[:3])\ndf3 = df1.append(df2)\ndf3\n                       A   B\n2001-01-01 00:00:00   20 -50\n2001-01-01 01:00:00  -30  60\n2001-01-01 02:00:00   40 -70\n2001-01-01 03:00:00    3   3\n2001-01-01 04:00:00    4   4\n2001-01-01 05:00:00    5   5\n2001-01-01 00:00:00    0   0\n2001-01-01 01:00:00    1   1\n2001-01-01 02:00:00    2   2\n"", '                       A   B\n2001-01-01 00:00:00    0   0\n2001-01-01 01:00:00    1   1\n2001-01-01 02:00:00    2   2\n2001-01-01 03:00:00    3   3\n2001-01-01 04:00:00    4   4\n2001-01-01 05:00:00    5   5\n']";"['                      Sta  Precip1hr  Precip5min  Temp  DewPnt  WindSpd  WindDir  AtmPress\nDate                                                                                      \n2001-01-01 00:00:00  KPDX          0           0     4       3        0        0     30.31\n2001-01-01 00:05:00  KPDX          0           0     4       3        0        0     30.30\n2001-01-01 00:10:00  KPDX          0           0     4       3        4       80     30.30\n2001-01-01 00:15:00  KPDX          0           0     3       2        5       90     30.30\n2001-01-01 00:20:00  KPDX          0           0     3       2       10      110     30.28\n', ""import pandas \nimport datetime\nstartdate = datetime.datetime(2001, 1, 1, 0, 0)\nenddate = datetime.datetime(2001, 1, 1, 5, 0)\nindex = pandas.DatetimeIndex(start=startdate, end=enddate, freq='H')\ndata = {'A' : range(6), 'B' : range(6)}\ndata1 = {'A' : [20, -30, 40], 'B' : [-50, 60, -70]}\ndf1 = pandas.DataFrame(data=data, index=index)\ndf2 = pandas.DataFrame(data=data1, index=index[:3])\ndf3 = df1.append(df2)\ndf3\n                       A   B\n2001-01-01 00:00:00   20 -50\n2001-01-01 01:00:00  -30  60\n2001-01-01 02:00:00   40 -70\n2001-01-01 03:00:00    3   3\n2001-01-01 04:00:00    4   4\n2001-01-01 05:00:00    5   5\n2001-01-01 00:00:00    0   0\n2001-01-01 01:00:00    1   1\n2001-01-01 02:00:00    2   2\n"", 'df3', '                       A   B\n2001-01-01 00:00:00    0   0\n2001-01-01 01:00:00    1   1\n2001-01-01 02:00:00    2   2\n2001-01-01 03:00:00    3   3\n2001-01-01 04:00:00    4   4\n2001-01-01 05:00:00    5   5\n', ""df3['rownum'] = range(df3.shape[0])"", 'DatetimeIndex', 'group_by', 'pivot']";"[""Date                                                                                      \nimport pandas \nimport datetime\nstartdate = datetime.datetime(2001, 1, 1, 0, 0)\nenddate = datetime.datetime(2001, 1, 1, 5, 0)\nindex = pandas.DatetimeIndex(start=startdate, end=enddate, freq='H')\ndata = {'A' : range(6), 'B' : range(6)}\ndata1 = {'A' : [20, -30, 40], 'B' : [-50, 60, -70]}\ndf1 = pandas.DataFrame(data=data, index=index)\ndf2 = pandas.DataFrame(data=data1, index=index[:3])\ndf3 = df1.append(df2)\ndf3\n""]";"[""Date                                                                                      \nimport pandas \nimport datetime\nstartdate = datetime.datetime(2001, 1, 1, 0, 0)\nenddate = datetime.datetime(2001, 1, 1, 5, 0)\nindex = pandas.DatetimeIndex(start=startdate, end=enddate, freq='H')\ndata = {'A' : range(6), 'B' : range(6)}\ndata1 = {'A' : [20, -30, 40], 'B' : [-50, 60, -70]}\ndf1 = pandas.DataFrame(data=data, index=index)\ndf2 = pandas.DataFrame(data=data1, index=index[:3])\ndf3 = df1.append(df2)\ndf3\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nDate                                                                                      \nimport pandas \nimport datetime\nstartdate = datetime.datetime(2001, 1, 1, 0, 0)\nenddate = datetime.datetime(2001, 1, 1, 5, 0)\nindex = pandas.DatetimeIndex(start=startdate, end=enddate, freq='H')\ndata = {'A' : range(6), 'B' : range(6)}\ndata1 = {'A' : [20, -30, 40], 'B' : [-50, 60, -70]}\ndf1 = pandas.DataFrame(data=data, index=index)\ndf2 = pandas.DataFrame(data=data1, index=index[:3])\ndf3 = df1.append(df2)\ndf3\n""]";True;0;3;"[""name 'df3' is not defined"", ""name 'df3' is not defined"", ""name 'df3' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df3' is not defined"", ""name 'df3' is not defined"", ""name 'df3' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df3' is not defined"", ""name 'df3' is not defined"", ""name 'df3' is not defined""]";['NameError', 'NameError', 'NameError']
125;125;125;125;3.0;1;13050003;;1;11;<python><pandas><data-analysis>;pandas: apply function to DataFrame that can return multiple rows;5107.0;"[""df = pd.DataFrame({'class': ['A', 'B', 'C'], 'count':[1,0,2]})\n\n  class  count\n0     A      1\n1     B      0\n2     C      2\n  class \n0     A   \n1     C   \n2     C \n""]";"[""df = pd.DataFrame({'class': ['A', 'B', 'C'], 'count':[1,0,2]})\n\n  class  count\n0     A      1\n1     B      0\n2     C      2\n"", '  class \n0     A   \n1     C   \n2     C \n']";"[""df = pd.DataFrame({'class': ['A', 'B', 'C'], 'count':[1,0,2]})\n\n  class  count\n0     A      1\n1     B      0\n2     C      2\n"", '  class \n0     A   \n1     C   \n2     C \n', 'DataFrame.applymap', 'apply', 'GroupBy']";"[""df = pd.DataFrame({'class': ['A', 'B', 'C'], 'count':[1,0,2]})\n\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'class': ['A', 'B', 'C'], 'count':[1,0,2]})\n\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame({'class': ['A', 'B', 'C'], 'count':[1,0,2]})\n\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'class'""]";['KeyError']
126;126;126;126;3.0;0;13052844;;1;17;<python><plot><matplotlib><pandas>;matplotlib: how to decrease density of tick labels in subplots?;24650.0;"['import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom StringIO import StringIO\ndata = """"""\\\n    a   b   c   d\nz   54.65   6.27    19.53   4.54\nw   -1.27   4.41    11.74   3.06\nd   5.51    3.39    22.98   2.29\nt   76284.53    -0.20   28394.93    0.28\n""""""\ndf = pd.read_csv(StringIO(data), sep=\'\\s+\')\ngs = gridspec.GridSpec(3, 1,height_ratios=[1,1,4] )\nax0 = plt.subplot(gs[0])\nax1 = plt.subplot(gs[1])\nax2 = plt.subplot(gs[2])\ndf.plot(kind=\'bar\', ax=ax0,color=(\'Blue\',\'DeepSkyBlue\',\'Red\',\'DarkOrange\'))\ndf.plot(kind=\'bar\', ax=ax1,color=(\'Blue\',\'DeepSkyBlue\',\'Red\',\'DarkOrange\'))\ndf.plot(kind=\'bar\', ax=ax2,color=(\'Blue\',\'DeepSkyBlue\',\'Red\',\'DarkOrange\'),rot=45)\nax0.set_ylim(69998, 78000)\nax1.set_ylim(19998, 29998)\nax2.set_ylim(-2, 28)\nax0.legend().set_visible(False)\nax1.legend().set_visible(False)\nax2.legend().set_visible(False)\nax0.spines[\'bottom\'].set_visible(False)\nax1.spines[\'bottom\'].set_visible(False)\nax1.spines[\'top\'].set_visible(False)\nax2.spines[\'top\'].set_visible(False)\nax0.xaxis.set_ticks_position(\'none\')\nax1.xaxis.set_ticks_position(\'none\')\nax0.xaxis.set_label_position(\'top\')\nax1.xaxis.set_label_position(\'top\')\nax0.tick_params(labeltop=\'off\')\nax1.tick_params(labeltop=\'off\', pad=15)\nax2.tick_params(pad=15)\nax2.xaxis.tick_bottom()\nd = .015\nkwargs = dict(transform=ax0.transAxes, color=\'k\', clip_on=False)\nax0.plot((-d,+d),(-d,+d), **kwargs)\nax0.plot((1-d,1+d),(-d,+d), **kwargs)\nkwargs.update(transform=ax1.transAxes)\nax1.plot((-d,+d),(1-d,1+d), **kwargs)\nax1.plot((1-d,1+d),(1-d,1+d), **kwargs)\nax1.plot((-d,+d),(-d,+d), **kwargs)\nax1.plot((1-d,1+d),(-d,+d), **kwargs)\nkwargs.update(transform=ax2.transAxes)\nax1.plot((-d,+d),(1-d/4,1+d/4), **kwargs)\nax1.plot((1-d,1+d),(1-d/4,1+d/4), **kwargs)\nplt.show()\n']";"['import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom StringIO import StringIO\ndata = """"""\\\n    a   b   c   d\nz   54.65   6.27    19.53   4.54\nw   -1.27   4.41    11.74   3.06\nd   5.51    3.39    22.98   2.29\nt   76284.53    -0.20   28394.93    0.28\n""""""\ndf = pd.read_csv(StringIO(data), sep=\'\\s+\')\ngs = gridspec.GridSpec(3, 1,height_ratios=[1,1,4] )\nax0 = plt.subplot(gs[0])\nax1 = plt.subplot(gs[1])\nax2 = plt.subplot(gs[2])\ndf.plot(kind=\'bar\', ax=ax0,color=(\'Blue\',\'DeepSkyBlue\',\'Red\',\'DarkOrange\'))\ndf.plot(kind=\'bar\', ax=ax1,color=(\'Blue\',\'DeepSkyBlue\',\'Red\',\'DarkOrange\'))\ndf.plot(kind=\'bar\', ax=ax2,color=(\'Blue\',\'DeepSkyBlue\',\'Red\',\'DarkOrange\'),rot=45)\nax0.set_ylim(69998, 78000)\nax1.set_ylim(19998, 29998)\nax2.set_ylim(-2, 28)\nax0.legend().set_visible(False)\nax1.legend().set_visible(False)\nax2.legend().set_visible(False)\nax0.spines[\'bottom\'].set_visible(False)\nax1.spines[\'bottom\'].set_visible(False)\nax1.spines[\'top\'].set_visible(False)\nax2.spines[\'top\'].set_visible(False)\nax0.xaxis.set_ticks_position(\'none\')\nax1.xaxis.set_ticks_position(\'none\')\nax0.xaxis.set_label_position(\'top\')\nax1.xaxis.set_label_position(\'top\')\nax0.tick_params(labeltop=\'off\')\nax1.tick_params(labeltop=\'off\', pad=15)\nax2.tick_params(pad=15)\nax2.xaxis.tick_bottom()\nd = .015\nkwargs = dict(transform=ax0.transAxes, color=\'k\', clip_on=False)\nax0.plot((-d,+d),(-d,+d), **kwargs)\nax0.plot((1-d,1+d),(-d,+d), **kwargs)\nkwargs.update(transform=ax1.transAxes)\nax1.plot((-d,+d),(1-d,1+d), **kwargs)\nax1.plot((1-d,1+d),(1-d,1+d), **kwargs)\nax1.plot((-d,+d),(-d,+d), **kwargs)\nax1.plot((1-d,1+d),(-d,+d), **kwargs)\nkwargs.update(transform=ax2.transAxes)\nax1.plot((-d,+d),(1-d/4,1+d/4), **kwargs)\nax1.plot((1-d,1+d),(1-d/4,1+d/4), **kwargs)\nplt.show()\n']";"['import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom StringIO import StringIO\ndata = """"""\\\n    a   b   c   d\nz   54.65   6.27    19.53   4.54\nw   -1.27   4.41    11.74   3.06\nd   5.51    3.39    22.98   2.29\nt   76284.53    -0.20   28394.93    0.28\n""""""\ndf = pd.read_csv(StringIO(data), sep=\'\\s+\')\ngs = gridspec.GridSpec(3, 1,height_ratios=[1,1,4] )\nax0 = plt.subplot(gs[0])\nax1 = plt.subplot(gs[1])\nax2 = plt.subplot(gs[2])\ndf.plot(kind=\'bar\', ax=ax0,color=(\'Blue\',\'DeepSkyBlue\',\'Red\',\'DarkOrange\'))\ndf.plot(kind=\'bar\', ax=ax1,color=(\'Blue\',\'DeepSkyBlue\',\'Red\',\'DarkOrange\'))\ndf.plot(kind=\'bar\', ax=ax2,color=(\'Blue\',\'DeepSkyBlue\',\'Red\',\'DarkOrange\'),rot=45)\nax0.set_ylim(69998, 78000)\nax1.set_ylim(19998, 29998)\nax2.set_ylim(-2, 28)\nax0.legend().set_visible(False)\nax1.legend().set_visible(False)\nax2.legend().set_visible(False)\nax0.spines[\'bottom\'].set_visible(False)\nax1.spines[\'bottom\'].set_visible(False)\nax1.spines[\'top\'].set_visible(False)\nax2.spines[\'top\'].set_visible(False)\nax0.xaxis.set_ticks_position(\'none\')\nax1.xaxis.set_ticks_position(\'none\')\nax0.xaxis.set_label_position(\'top\')\nax1.xaxis.set_label_position(\'top\')\nax0.tick_params(labeltop=\'off\')\nax1.tick_params(labeltop=\'off\', pad=15)\nax2.tick_params(pad=15)\nax2.xaxis.tick_bottom()\nd = .015\nkwargs = dict(transform=ax0.transAxes, color=\'k\', clip_on=False)\nax0.plot((-d,+d),(-d,+d), **kwargs)\nax0.plot((1-d,1+d),(-d,+d), **kwargs)\nkwargs.update(transform=ax1.transAxes)\nax1.plot((-d,+d),(1-d,1+d), **kwargs)\nax1.plot((1-d,1+d),(1-d,1+d), **kwargs)\nax1.plot((-d,+d),(-d,+d), **kwargs)\nax1.plot((1-d,1+d),(-d,+d), **kwargs)\nkwargs.update(transform=ax2.transAxes)\nax1.plot((-d,+d),(1-d/4,1+d/4), **kwargs)\nax1.plot((1-d,1+d),(1-d/4,1+d/4), **kwargs)\nplt.show()\n']";"[""import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom StringIO import StringIO\ndf = pd.read_csv(StringIO(data), sep='\\s+')\ngs = gridspec.GridSpec(3, 1,height_ratios=[1,1,4] )\nax0 = plt.subplot(gs[0])\nax1 = plt.subplot(gs[1])\nax2 = plt.subplot(gs[2])\ndf.plot(kind='bar', ax=ax0,color=('Blue','DeepSkyBlue','Red','DarkOrange'))\ndf.plot(kind='bar', ax=ax1,color=('Blue','DeepSkyBlue','Red','DarkOrange'))\ndf.plot(kind='bar', ax=ax2,color=('Blue','DeepSkyBlue','Red','DarkOrange'),rot=45)\nax0.set_ylim(69998, 78000)\nax1.set_ylim(19998, 29998)\nax2.set_ylim(-2, 28)\nax0.legend().set_visible(False)\nax1.legend().set_visible(False)\nax2.legend().set_visible(False)\nax0.spines['bottom'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\nax1.spines['top'].set_visible(False)\nax2.spines['top'].set_visible(False)\nax0.xaxis.set_ticks_position('none')\nax1.xaxis.set_ticks_position('none')\nax0.xaxis.set_label_position('top')\nax1.xaxis.set_label_position('top')\nax0.tick_params(labeltop='off')\nax1.tick_params(labeltop='off', pad=15)\nax2.tick_params(pad=15)\nax2.xaxis.tick_bottom()\nd = .015\nkwargs = dict(transform=ax0.transAxes, color='k', clip_on=False)\nax0.plot((-d,+d),(-d,+d), **kwargs)\nax0.plot((1-d,1+d),(-d,+d), **kwargs)\nkwargs.update(transform=ax1.transAxes)\nax1.plot((-d,+d),(1-d,1+d), **kwargs)\nax1.plot((1-d,1+d),(1-d,1+d), **kwargs)\nax1.plot((-d,+d),(-d,+d), **kwargs)\nax1.plot((1-d,1+d),(-d,+d), **kwargs)\nkwargs.update(transform=ax2.transAxes)\nax1.plot((-d,+d),(1-d/4,1+d/4), **kwargs)\nax1.plot((1-d,1+d),(1-d/4,1+d/4), **kwargs)\nplt.show()\n""]";"[""import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom StringIO import StringIO\ndf = pd.read_csv(StringIO(data), sep='\\s+')\ngs = gridspec.GridSpec(3, 1,height_ratios=[1,1,4] )\nax0 = plt.subplot(gs[0])\nax1 = plt.subplot(gs[1])\nax2 = plt.subplot(gs[2])\ndf.plot(kind='bar', ax=ax0,color=('Blue','DeepSkyBlue','Red','DarkOrange'))\ndf.plot(kind='bar', ax=ax1,color=('Blue','DeepSkyBlue','Red','DarkOrange'))\ndf.plot(kind='bar', ax=ax2,color=('Blue','DeepSkyBlue','Red','DarkOrange'),rot=45)\nax0.set_ylim(69998, 78000)\nax1.set_ylim(19998, 29998)\nax2.set_ylim(-2, 28)\nax0.legend().set_visible(False)\nax1.legend().set_visible(False)\nax2.legend().set_visible(False)\nax0.spines['bottom'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\nax1.spines['top'].set_visible(False)\nax2.spines['top'].set_visible(False)\nax0.xaxis.set_ticks_position('none')\nax1.xaxis.set_ticks_position('none')\nax0.xaxis.set_label_position('top')\nax1.xaxis.set_label_position('top')\nax0.tick_params(labeltop='off')\nax1.tick_params(labeltop='off', pad=15)\nax2.tick_params(pad=15)\nax2.xaxis.tick_bottom()\nd = .015\nkwargs = dict(transform=ax0.transAxes, color='k', clip_on=False)\nax0.plot((-d,+d),(-d,+d), **kwargs)\nax0.plot((1-d,1+d),(-d,+d), **kwargs)\nkwargs.update(transform=ax1.transAxes)\nax1.plot((-d,+d),(1-d,1+d), **kwargs)\nax1.plot((1-d,1+d),(1-d,1+d), **kwargs)\nax1.plot((-d,+d),(-d,+d), **kwargs)\nax1.plot((1-d,1+d),(-d,+d), **kwargs)\nkwargs.update(transform=ax2.transAxes)\nax1.plot((-d,+d),(1-d/4,1+d/4), **kwargs)\nax1.plot((1-d,1+d),(1-d/4,1+d/4), **kwargs)\nplt.show()\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom StringIO import StringIO\ndf = pd.read_csv(StringIO(data), sep='\\s+')\ngs = gridspec.GridSpec(3, 1,height_ratios=[1,1,4] )\nax0 = plt.subplot(gs[0])\nax1 = plt.subplot(gs[1])\nax2 = plt.subplot(gs[2])\ndf.plot(kind='bar', ax=ax0,color=('Blue','DeepSkyBlue','Red','DarkOrange'))\ndf.plot(kind='bar', ax=ax1,color=('Blue','DeepSkyBlue','Red','DarkOrange'))\ndf.plot(kind='bar', ax=ax2,color=('Blue','DeepSkyBlue','Red','DarkOrange'),rot=45)\nax0.set_ylim(69998, 78000)\nax1.set_ylim(19998, 29998)\nax2.set_ylim(-2, 28)\nax0.legend().set_visible(False)\nax1.legend().set_visible(False)\nax2.legend().set_visible(False)\nax0.spines['bottom'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\nax1.spines['top'].set_visible(False)\nax2.spines['top'].set_visible(False)\nax0.xaxis.set_ticks_position('none')\nax1.xaxis.set_ticks_position('none')\nax0.xaxis.set_label_position('top')\nax1.xaxis.set_label_position('top')\nax0.tick_params(labeltop='off')\nax1.tick_params(labeltop='off', pad=15)\nax2.tick_params(pad=15)\nax2.xaxis.tick_bottom()\nd = .015\nkwargs = dict(transform=ax0.transAxes, color='k', clip_on=False)\nax0.plot((-d,+d),(-d,+d), **kwargs)\nax0.plot((1-d,1+d),(-d,+d), **kwargs)\nkwargs.update(transform=ax1.transAxes)\nax1.plot((-d,+d),(1-d,1+d), **kwargs)\nax1.plot((1-d,1+d),(1-d,1+d), **kwargs)\nax1.plot((-d,+d),(-d,+d), **kwargs)\nax1.plot((1-d,1+d),(-d,+d), **kwargs)\nkwargs.update(transform=ax2.transAxes)\nax1.plot((-d,+d),(1-d/4,1+d/4), **kwargs)\nax1.plot((1-d,1+d),(1-d/4,1+d/4), **kwargs)\nplt.show()\n""]";True;0;3;"[""name 'ax0' is not defined"", ""No module named 'matplotlib'"", ""name 'plt' is not defined""]";['NameError', 'ImportError', 'NameError'];0;3;"[""name 'ax0' is not defined"", ""No module named 'matplotlib'"", ""name 'plt' is not defined""]";['NameError', 'ImportError', 'NameError'];0;3;"[""name 'ax0' is not defined"", ""No module named 'matplotlib'"", ""name 'plt' is not defined""]";['NameError', 'ImportError', 'NameError']
127;127;127;127;1.0;0;13085709;;1;14;<python><pandas>;df.head() sometimes doesn't work in Pandas, Python;14294.0;"["">>>df.head()  \n              X  Y       unixtime\n0  652f5e69fcb3  1  1346689910622\n1        400292  1  1346614723542\n2  1c9d02e4f14e  1  1346862070161\n3        610449  1  1346806384518\n4        207664  1  1346723370096\ndef unixTodate(unix):\n  day = dt.datetime.utcfromtimestamp(unix/1000).strftime('%Y-%m-%d')\n  return day\n\ndf['day'] = df['unixtime'].apply(unixTodate)\n>>>df.head()  \n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 5 entries, 190648 to 626582\nData columns:\nX              5  non-null values\nY              5  non-null values\nunixtime       5  non-null values\nday            5  non-null values\ndtypes: int64(3), object(5)\n""]";"['>>>df.head()  \n              X  Y       unixtime\n0  652f5e69fcb3  1  1346689910622\n1        400292  1  1346614723542\n2  1c9d02e4f14e  1  1346862070161\n3        610449  1  1346806384518\n4        207664  1  1346723370096\n', ""def unixTodate(unix):\n  day = dt.datetime.utcfromtimestamp(unix/1000).strftime('%Y-%m-%d')\n  return day\n\ndf['day'] = df['unixtime'].apply(unixTodate)\n"", "">>>df.head()  \n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 5 entries, 190648 to 626582\nData columns:\nX              5  non-null values\nY              5  non-null values\nunixtime       5  non-null values\nday            5  non-null values\ndtypes: int64(3), object(5)\n""]";"['>>>df.head()  \n              X  Y       unixtime\n0  652f5e69fcb3  1  1346689910622\n1        400292  1  1346614723542\n2  1c9d02e4f14e  1  1346862070161\n3        610449  1  1346806384518\n4        207664  1  1346723370096\n', ""def unixTodate(unix):\n  day = dt.datetime.utcfromtimestamp(unix/1000).strftime('%Y-%m-%d')\n  return day\n\ndf['day'] = df['unixtime'].apply(unixTodate)\n"", "">>>df.head()  \n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 5 entries, 190648 to 626582\nData columns:\nX              5  non-null values\nY              5  non-null values\nunixtime       5  non-null values\nday            5  non-null values\ndtypes: int64(3), object(5)\n""]";"[""\ndf['day'] = df['unixtime'].apply(unixTodate)\n\n""]";"[""\ndf['day'] = df['unixtime'].apply(unixTodate)\n\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\n\ndf['day'] = df['unixtime'].apply(unixTodate)\n\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
128;128;128;128;2.0;4;13114512;;1;24;<python><pandas>;Calculating difference between two rows in Python / Pandas;31121.0;"[""           Date   Close  Adj Close\n251  2011-01-03  147.48     143.25\n250  2011-01-04  147.64     143.41\n249  2011-01-05  147.05     142.83\n248  2011-01-06  148.66     144.40\n247  2011-01-07  147.93     143.69\nimport pandas\n\nurl = 'http://ichart.finance.yahoo.com/table.csv?s=IBM&a=00&b=1&c=2011&d=11&e=31&f=2011&g=d&ignore=.csv'\ndata = data = pandas.read_csv(url)\n\n## now I sorted the data frame ascending by date \ndata = data.sort(columns='Date')\n""]";"['           Date   Close  Adj Close\n251  2011-01-03  147.48     143.25\n250  2011-01-04  147.64     143.41\n249  2011-01-05  147.05     142.83\n248  2011-01-06  148.66     144.40\n247  2011-01-07  147.93     143.69\n', ""import pandas\n\nurl = 'http://ichart.finance.yahoo.com/table.csv?s=IBM&a=00&b=1&c=2011&d=11&e=31&f=2011&g=d&ignore=.csv'\ndata = data = pandas.read_csv(url)\n\n## now I sorted the data frame ascending by date \ndata = data.sort(columns='Date')\n""]";"['dataframes', 'pandas', '           Date   Close  Adj Close\n251  2011-01-03  147.48     143.25\n250  2011-01-04  147.64     143.41\n249  2011-01-05  147.05     142.83\n248  2011-01-06  148.66     144.40\n247  2011-01-07  147.93     143.69\n', ""import pandas\n\nurl = 'http://ichart.finance.yahoo.com/table.csv?s=IBM&a=00&b=1&c=2011&d=11&e=31&f=2011&g=d&ignore=.csv'\ndata = data = pandas.read_csv(url)\n\n## now I sorted the data frame ascending by date \ndata = data.sort(columns='Date')\n"", 'pandas', 'apply']";"[""import pandas\n\nurl = 'http://ichart.finance.yahoo.com/table.csv?s=IBM&a=00&b=1&c=2011&d=11&e=31&f=2011&g=d&ignore=.csv'\ndata = data = pandas.read_csv(url)\n\n## now I sorted the data frame ascending by date \ndata = data.sort(columns='Date')\n""]";"[""import pandas\n\nurl = 'http://ichart.finance.yahoo.com/table.csv?s=IBM&a=00&b=1&c=2011&d=11&e=31&f=2011&g=d&ignore=.csv'\ndata = data = pandas.read_csv(url)\n\n## now I sorted the data frame ascending by date \ndata = data.sort(columns='Date')\n""]";False;"[""import pandas as pd\nimport pandas\n\nurl = 'http://ichart.finance.yahoo.com/table.csv?s=IBM&a=00&b=1&c=2011&d=11&e=31&f=2011&g=d&ignore=.csv'\ndata = data = pandas.read_csv(url)\n\n## now I sorted the data frame ascending by date \ndata = data.sort(columns='Date')\n""]";False;0;1;"[""name 'data' is not defined""]";['NameError'];0;1;"[""name 'data' is not defined""]";['NameError'];0;1;"[""'Date'""]";['KeyError']
129;129;129;129;3.0;1;13129618;;1;33;<python><pandas><numpy><matplotlib>;Histogram values of a Pandas Series;23683.0;['In [1]: series = pd.Series([0.0,950.0,-70.0,812.0,0.0,-90.0,0.0,0.0,-90.0,0.0,-64.0,208.0,0.0,-90.0,0.0,-80.0,0.0,0.0,-80.0,-48.0,840.0,-100.0,190.0,130.0,-100.0,-100.0,0.0,-50.0,0.0,-100.0,-100.0,0.0,-90.0,0.0,-90.0,-90.0,63.0,-90.0,0.0,0.0,-90.0,-80.0,0.0,])\n\nIn [2]: series.min()\nOut[2]: -100.0\n\nIn [3]: series.max()\nOut[3]: 950.0\nlwb = range(-200,1000,50)\nupb = range(-150,1050,50)\n'];['In [1]: series = pd.Series([0.0,950.0,-70.0,812.0,0.0,-90.0,0.0,0.0,-90.0,0.0,-64.0,208.0,0.0,-90.0,0.0,-80.0,0.0,0.0,-80.0,-48.0,840.0,-100.0,190.0,130.0,-100.0,-100.0,0.0,-50.0,0.0,-100.0,-100.0,0.0,-90.0,0.0,-90.0,-90.0,63.0,-90.0,0.0,0.0,-90.0,-80.0,0.0,])\n\nIn [2]: series.min()\nOut[2]: -100.0\n\nIn [3]: series.max()\nOut[3]: 950.0\n', 'lwb = range(-200,1000,50)\n', 'upb = range(-150,1050,50)\n'];['In [1]: series = pd.Series([0.0,950.0,-70.0,812.0,0.0,-90.0,0.0,0.0,-90.0,0.0,-64.0,208.0,0.0,-90.0,0.0,-80.0,0.0,0.0,-80.0,-48.0,840.0,-100.0,190.0,130.0,-100.0,-100.0,0.0,-50.0,0.0,-100.0,-100.0,0.0,-90.0,0.0,-90.0,-90.0,63.0,-90.0,0.0,0.0,-90.0,-80.0,0.0,])\n\nIn [2]: series.min()\nOut[2]: -100.0\n\nIn [3]: series.max()\nOut[3]: 950.0\n', 'lwb = range(-200,1000,50)\n', 'upb = range(-150,1050,50)\n', 'cut'];['series = pd.Series([0.0,950.0,-70.0,812.0,0.0,-90.0,0.0,0.0,-90.0,0.0,-64.0,208.0,0.0,-90.0,0.0,-80.0,0.0,0.0,-80.0,-48.0,840.0,-100.0,190.0,130.0,-100.0,-100.0,0.0,-50.0,0.0,-100.0,-100.0,0.0,-90.0,0.0,-90.0,-90.0,63.0,-90.0,0.0,0.0,-90.0,-80.0,0.0,])\n\nseries.max()\n'];['import pandas as pd\nseries = pd.Series([0.0,950.0,-70.0,812.0,0.0,-90.0,0.0,0.0,-90.0,0.0,-64.0,208.0,0.0,-90.0,0.0,-80.0,0.0,0.0,-80.0,-48.0,840.0,-100.0,190.0,130.0,-100.0,-100.0,0.0,-50.0,0.0,-100.0,-100.0,0.0,-90.0,0.0,-90.0,-90.0,63.0,-90.0,0.0,0.0,-90.0,-80.0,0.0,])\n\nseries.max()\n'];True;['import pandas as pd\nseries = pd.Series([0.0,950.0,-70.0,812.0,0.0,-90.0,0.0,0.0,-90.0,0.0,-64.0,208.0,0.0,-90.0,0.0,-80.0,0.0,0.0,-80.0,-48.0,840.0,-100.0,190.0,130.0,-100.0,-100.0,0.0,-50.0,0.0,-100.0,-100.0,0.0,-90.0,0.0,-90.0,-90.0,63.0,-90.0,0.0,0.0,-90.0,-80.0,0.0,])\n\nseries.max()\n'];False;0;2;"[""name 'series' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'series' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'series' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError']
130;130;130;130;18.0;2;13148429;;1;286;<python><pandas>;How to change the order of DataFrame columns?;187524.0;"[""import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.rand(10, 5))\ndf['mean'] = df.mean(1)\n""]";"['import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.rand(10, 5))\n', ""df['mean'] = df.mean(1)\n""]";"['DataFrame', 'df', 'import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.rand(10, 5))\n', ""df['mean'] = df.mean(1)\n"", 'mean']";"[""import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.rand(10, 5))\ndf['mean'] = df.mean(1)\n""]";"[""import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.rand(10, 5))\ndf['mean'] = df.mean(1)\n""]";False;"[""import pandas as pd\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.rand(10, 5))\ndf['mean'] = df.mean(1)\n""]";False;1;9;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'order' is not defined"", 'Sucess', ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError', 'Sucess', 'NameError', 'NameError', 'NameError', 'NameError'];1;9;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'order' is not defined"", 'Sucess', ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError', 'Sucess', 'NameError', 'NameError', 'NameError', 'NameError'];2;9;"[""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined"", ""name 'order' is not defined"", 'Sucess', ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError', 'NameError', 'Sucess', 'NameError', 'NameError', 'NameError', 'NameError']
131;131;131;131;3.0;1;13166842;;1;16;<dataframe><pandas><multiplying>;pandas dataframe multiply with a series;14469.0;[''];[];['DataFrame', 'Series', 'repmat()', 'np.tile()'];[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'dataframe' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'dataframe' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'dataframe' is not defined""]";['Sucess', 'NameError']
132;132;132;132;2.0;5;13167391;;1;25;<python><pandas>;filtering grouped df in pandas;14412.0;"[""grouped[grouped.size > 1 ]\ndf = pandas.DataFrame({'A': ['foo','bar','foo','foo'],\n                       'B': range(4)})\ngrouped = df.groupby('A')\ngrouped[grouped.size() > 1]\nA\nfoo 0\n    2\n    3\n""]";"['grouped[grouped.size > 1 ]\n', ""df = pandas.DataFrame({'A': ['foo','bar','foo','foo'],\n                       'B': range(4)})\ngrouped = df.groupby('A')\n"", 'grouped[grouped.size() > 1]\n', 'A\nfoo 0\n    2\n    3\n']";"['groupby', 'DataFrame', 'grouped[grouped.size > 1 ]\n', 'DataFrame', 'grouped', ""'name'"", ""'foo'"", ""'bar'"", ""df = pandas.DataFrame({'A': ['foo','bar','foo','foo'],\n                       'B': range(4)})\ngrouped = df.groupby('A')\n"", 'groupby', 'grouped[grouped.size() > 1]\n', 'A\nfoo 0\n    2\n    3\n', 'grouped']";"[""grouped[grouped.size > 1 ]\ngrouped = df.groupby('A')\ngrouped[grouped.size() > 1]\nA\n""]";"[""grouped[grouped.size > 1 ]\ngrouped = df.groupby('A')\ngrouped[grouped.size() > 1]\nA\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ngrouped[grouped.size > 1 ]\ngrouped = df.groupby('A')\ngrouped[grouped.size() > 1]\nA\n""]";True;1;2;"[""name 'pd' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'grouped' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'grouped' is not defined"", 'Sucess']";['NameError', 'Sucess']
133;133;133;133;7.0;0;13187778;;1;92;<python><arrays><numpy><pandas><type-conversion>;Convert pandas dataframe to numpy array, preserving index;161037.0;"[""label   A    B    C\nID                                 \n1   NaN  0.2  NaN\n2   NaN  NaN  0.5\n3   NaN  0.2  0.5\n4   0.1  0.2  NaN\n5   0.1  0.2  0.5\n6   0.1  NaN  0.5\n7   0.1  NaN  NaN\narray([[ nan,  0.2,  nan],\n       [ nan,  nan,  0.5],\n       [ nan,  0.2,  0.5],\n       [ 0.1,  0.2,  nan],\n       [ 0.1,  0.2,  0.5],\n       [ 0.1,  nan,  0.5],\n       [ 0.1,  nan,  nan]])\narray([[ 1, nan,  0.2,  nan],\n       [ 2, nan,  nan,  0.5],\n       [ 3, nan,  0.2,  0.5],\n       [ 4, 0.1,  0.2,  nan],\n       [ 5, 0.1,  0.2,  0.5],\n       [ 6, 0.1,  nan,  0.5],\n       [ 7, 0.1,  nan,  nan]],\n     dtype=[('ID', '<i4'), ('A', '<f8'), ('B', '<f8'), ('B', '<f8')])\n""]";"['label   A    B    C\nID                                 \n1   NaN  0.2  NaN\n2   NaN  NaN  0.5\n3   NaN  0.2  0.5\n4   0.1  0.2  NaN\n5   0.1  0.2  0.5\n6   0.1  NaN  0.5\n7   0.1  NaN  NaN\n', 'array([[ nan,  0.2,  nan],\n       [ nan,  nan,  0.5],\n       [ nan,  0.2,  0.5],\n       [ 0.1,  0.2,  nan],\n       [ 0.1,  0.2,  0.5],\n       [ 0.1,  nan,  0.5],\n       [ 0.1,  nan,  nan]])\n', ""array([[ 1, nan,  0.2,  nan],\n       [ 2, nan,  nan,  0.5],\n       [ 3, nan,  0.2,  0.5],\n       [ 4, 0.1,  0.2,  nan],\n       [ 5, 0.1,  0.2,  0.5],\n       [ 6, 0.1,  nan,  0.5],\n       [ 7, 0.1,  nan,  nan]],\n     dtype=[('ID', '<i4'), ('A', '<f8'), ('B', '<f8'), ('B', '<f8')])\n""]";"['label   A    B    C\nID                                 \n1   NaN  0.2  NaN\n2   NaN  NaN  0.5\n3   NaN  0.2  0.5\n4   0.1  0.2  NaN\n5   0.1  0.2  0.5\n6   0.1  NaN  0.5\n7   0.1  NaN  NaN\n', 'array([[ nan,  0.2,  nan],\n       [ nan,  nan,  0.5],\n       [ nan,  0.2,  0.5],\n       [ 0.1,  0.2,  nan],\n       [ 0.1,  0.2,  0.5],\n       [ 0.1,  nan,  0.5],\n       [ 0.1,  nan,  nan]])\n', ""array([[ 1, nan,  0.2,  nan],\n       [ 2, nan,  nan,  0.5],\n       [ 3, nan,  0.2,  0.5],\n       [ 4, 0.1,  0.2,  nan],\n       [ 5, 0.1,  0.2,  0.5],\n       [ 6, 0.1,  nan,  0.5],\n       [ 7, 0.1,  nan,  nan]],\n     dtype=[('ID', '<i4'), ('A', '<f8'), ('B', '<f8'), ('B', '<f8')])\n""]";['ID                                 \n'];['ID                                 \n'];False;['import pandas as pd\nID                                 \n'];False;1;4;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess']";['NameError', 'NameError', 'NameError', 'Sucess'];1;4;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess']";['NameError', 'NameError', 'NameError', 'Sucess'];3;4;['Sucess', 'Cannot change data-type for object array.', 'Sucess', 'Sucess'];['Sucess', 'TypeError', 'Sucess', 'Sucess']
134;134;134;134;1.0;0;13226029;;1;29;<python><pandas><multi-index>;Benefits of panda's multiindex?;15935.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;['\n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated'];['DeprecationWarning']
135;135;135;135;7.0;1;13249135;;1;12;<python><pandas>;Installing Pandas on Mac OSX;40821.0;"[""$ sudo easy_install pandas\nSearching for pandas\nReading http://pypi.python.org/simple/pandas/\nReading http://pandas.pydata.org\nReading http://pandas.sourceforge.net\nBest match: pandas 0.9.0\nDownloading http://pypi.python.org/packages/source/p/pandas/pandas-\n0.9.0.zip#md5=04b1d8e11cc0fc30ae777499d89003ec\nProcessing pandas-0.9.0.zip\nWriting /tmp/easy_install-ixjbQO/pandas-0.9.0/setup.cfg\nRunning pandas-0.9.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-ixjbQO/pandas-\n0.9.0/egg-dist-tmp-EGREoT\nwarning: no files found matching 'setupegg.py'\nno previously-included directories found matching 'doc/build'\nwarning: no previously-included files matching '*.so' found anywhere in distribution\nwarning: no previously-included files matching '*.pyd' found anywhere in distribution\nwarning: no previously-included files matching '*.pyc' found anywhere in distribution\nwarning: no previously-included files matching '.git*' found anywhere in distribution\nwarning: no previously-included files matching '.DS_Store' found anywhere in distribution\nwarning: no previously-included files matching '*.png' found anywhere in distribution\nunable to execute gcc: No such file or directory\nerror: Setup script exited with error: command 'gcc' failed with exit status 1\n$ gcc\n-bash: gcc: command not found\n\n$ gcc-4.2\ni686-apple-darwin11-gcc-4.2.1: no input files\n""]";"['$ sudo easy_install pandas\n', ""Searching for pandas\nReading http://pypi.python.org/simple/pandas/\nReading http://pandas.pydata.org\nReading http://pandas.sourceforge.net\nBest match: pandas 0.9.0\nDownloading http://pypi.python.org/packages/source/p/pandas/pandas-\n0.9.0.zip#md5=04b1d8e11cc0fc30ae777499d89003ec\nProcessing pandas-0.9.0.zip\nWriting /tmp/easy_install-ixjbQO/pandas-0.9.0/setup.cfg\nRunning pandas-0.9.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-ixjbQO/pandas-\n0.9.0/egg-dist-tmp-EGREoT\nwarning: no files found matching 'setupegg.py'\nno previously-included directories found matching 'doc/build'\nwarning: no previously-included files matching '*.so' found anywhere in distribution\nwarning: no previously-included files matching '*.pyd' found anywhere in distribution\nwarning: no previously-included files matching '*.pyc' found anywhere in distribution\nwarning: no previously-included files matching '.git*' found anywhere in distribution\nwarning: no previously-included files matching '.DS_Store' found anywhere in distribution\nwarning: no previously-included files matching '*.png' found anywhere in distribution\nunable to execute gcc: No such file or directory\nerror: Setup script exited with error: command 'gcc' failed with exit status 1\n"", '$ gcc\n-bash: gcc: command not found\n\n$ gcc-4.2\ni686-apple-darwin11-gcc-4.2.1: no input files\n']";"['$ sudo easy_install pandas\n', ""Searching for pandas\nReading http://pypi.python.org/simple/pandas/\nReading http://pandas.pydata.org\nReading http://pandas.sourceforge.net\nBest match: pandas 0.9.0\nDownloading http://pypi.python.org/packages/source/p/pandas/pandas-\n0.9.0.zip#md5=04b1d8e11cc0fc30ae777499d89003ec\nProcessing pandas-0.9.0.zip\nWriting /tmp/easy_install-ixjbQO/pandas-0.9.0/setup.cfg\nRunning pandas-0.9.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-ixjbQO/pandas-\n0.9.0/egg-dist-tmp-EGREoT\nwarning: no files found matching 'setupegg.py'\nno previously-included directories found matching 'doc/build'\nwarning: no previously-included files matching '*.so' found anywhere in distribution\nwarning: no previously-included files matching '*.pyd' found anywhere in distribution\nwarning: no previously-included files matching '*.pyc' found anywhere in distribution\nwarning: no previously-included files matching '.git*' found anywhere in distribution\nwarning: no previously-included files matching '.DS_Store' found anywhere in distribution\nwarning: no previously-included files matching '*.png' found anywhere in distribution\nunable to execute gcc: No such file or directory\nerror: Setup script exited with error: command 'gcc' failed with exit status 1\n"", '$ gcc\n-bash: gcc: command not found\n\n$ gcc-4.2\ni686-apple-darwin11-gcc-4.2.1: no input files\n']";['\n'];['\n'];False;['import pandas as pd\n\n'];False;3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess']
136;136;136;136;3.0;1;13250046;;1;19;<python><pandas>;Pandas csv-import: Keep leading zeros in a column;6682.0;[''];[];['read_csv', 'int64'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'project_name' is not defined""]";['NameError'];0;1;"[""name 'project_name' is not defined""]";['NameError'];0;1;"[""name 'project_name' is not defined""]";['NameError']
137;137;137;137;4.0;0;13256917;;1;14;<python><pandas>;Pandas: Creating aggregated column in DataFrame;7817.0;"[""In [83]:\ndf = pd.DataFrame({'A':[1,1,2,2],'B':[1,2,1,2],'values':np.arange(10,30,5)})\ndf\nOut[83]:\n   A  B  values\n0  1  1      10\n1  1  2      15\n2  2  1      20\n3  2  2      25\nIn [84]:\ndf.groupby('A').sum()['values']\nOut[84]:\nA\n1    25\n2    45\nName: values\n   A  B  values  sum_values_A\n0  1  1      10            25\n1  1  2      15            25\n2  2  1      20            45\n3  2  2      25            45\n""]";"[""In [83]:\ndf = pd.DataFrame({'A':[1,1,2,2],'B':[1,2,1,2],'values':np.arange(10,30,5)})\ndf\nOut[83]:\n   A  B  values\n0  1  1      10\n1  1  2      15\n2  2  1      20\n3  2  2      25\n"", ""In [84]:\ndf.groupby('A').sum()['values']\nOut[84]:\nA\n1    25\n2    45\nName: values\n"", '   A  B  values  sum_values_A\n0  1  1      10            25\n1  1  2      15            25\n2  2  1      20            45\n3  2  2      25            45\n']";"[""In [83]:\ndf = pd.DataFrame({'A':[1,1,2,2],'B':[1,2,1,2],'values':np.arange(10,30,5)})\ndf\nOut[83]:\n   A  B  values\n0  1  1      10\n1  1  2      15\n2  2  1      20\n3  2  2      25\n"", 'values', 'A', ""In [84]:\ndf.groupby('A').sum()['values']\nOut[84]:\nA\n1    25\n2    45\nName: values\n"", '   A  B  values  sum_values_A\n0  1  1      10            25\n1  1  2      15            25\n2  2  1      20            45\n3  2  2      25            45\n']";"[""df = pd.DataFrame({'A':[1,1,2,2],'B':[1,2,1,2],'values':np.arange(10,30,5)})\ndf\ndf.groupby('A').sum()['values']\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'A':[1,1,2,2],'B':[1,2,1,2],'values':np.arange(10,30,5)})\ndf\ndf.groupby('A').sum()['values']\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame({'A':[1,1,2,2],'B':[1,2,1,2],'values':np.arange(10,30,5)})\ndf\ndf.groupby('A').sum()['values']\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
138;138;138;138;4.0;3;13269890;;1;30;<python><pandas>;cartesian product in pandas;18300.0;"[""from pandas import DataFrame\ndf1 = DataFrame({'col1':[1,2],'col2':[3,4]})\ndf2 = DataFrame({'col3':[5,6]})     \n#df1, df2 cartesian product\ndf_cartesian = DataFrame({'col1':[1,2,1,2],'col2':[3,4,3,4],'col3':[5,5,6,6]})\n""]";"[""from pandas import DataFrame\ndf1 = DataFrame({'col1':[1,2],'col2':[3,4]})\ndf2 = DataFrame({'col3':[5,6]})     \n"", ""#df1, df2 cartesian product\ndf_cartesian = DataFrame({'col1':[1,2,1,2],'col2':[3,4,3,4],'col3':[5,5,6,6]})\n""]";"[""from pandas import DataFrame\ndf1 = DataFrame({'col1':[1,2],'col2':[3,4]})\ndf2 = DataFrame({'col3':[5,6]})     \n"", ""#df1, df2 cartesian product\ndf_cartesian = DataFrame({'col1':[1,2,1,2],'col2':[3,4,3,4],'col3':[5,5,6,6]})\n""]";"[""from pandas import DataFrame\ndf1 = DataFrame({'col1':[1,2],'col2':[3,4]})\ndf2 = DataFrame({'col3':[5,6]})     \n#df1, df2 cartesian product\ndf_cartesian = DataFrame({'col1':[1,2,1,2],'col2':[3,4,3,4],'col3':[5,5,6,6]})\n""]";"[""from pandas import DataFrame\ndf1 = DataFrame({'col1':[1,2],'col2':[3,4]})\ndf2 = DataFrame({'col3':[5,6]})     \n#df1, df2 cartesian product\ndf_cartesian = DataFrame({'col1':[1,2,1,2],'col2':[3,4,3,4],'col3':[5,5,6,6]})\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nfrom pandas import DataFrame\ndf1 = DataFrame({'col1':[1,2],'col2':[3,4]})\ndf2 = DataFrame({'col3':[5,6]})     \n#df1, df2 cartesian product\ndf_cartesian = DataFrame({'col1':[1,2,1,2],'col2':[3,4,3,4],'col3':[5,5,6,6]})\n""]";True;1;2;"['Sucess', ""name 'pd' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'list_of_days' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'list_of_days' is not defined""]";['Sucess', 'NameError']
139;139;139;139;2.0;2;13293810;;1;32;<pandas>;Import pandas dataframe column as string not int;28863.0;"[""ID\n00013007854817840016671868\n00013007854817840016749251\n00013007854817840016754630\n00013007854817840016781876\n00013007854817840017028824\n00013007854817840017963235\n00013007854817840018860166\n\n\ndf = read_csv('sample.csv')\n\ndf.ID\n>>\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\nName: ID\ndf = read_csv('sample.csv', converters={'ID': str})\ndf.ID\n>>\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\nName: ID\n""]";"[""ID\n00013007854817840016671868\n00013007854817840016749251\n00013007854817840016754630\n00013007854817840016781876\n00013007854817840017028824\n00013007854817840017963235\n00013007854817840018860166\n\n\ndf = read_csv('sample.csv')\n\ndf.ID\n>>\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\nName: ID\n"", ""df = read_csv('sample.csv', converters={'ID': str})\ndf.ID\n>>\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\nName: ID\n""]";"[""ID\n00013007854817840016671868\n00013007854817840016749251\n00013007854817840016754630\n00013007854817840016781876\n00013007854817840017028824\n00013007854817840017963235\n00013007854817840018860166\n\n\ndf = read_csv('sample.csv')\n\ndf.ID\n>>\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\nName: ID\n"", ""df = read_csv('sample.csv', converters={'ID': str})\ndf.ID\n>>\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\nName: ID\n""]";"[""ID\n\n\ndf = read_csv('sample.csv')\n\ndf.ID\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\ndf = read_csv('sample.csv', converters={'ID': str})\ndf.ID\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\n""]";"[""ID\n\n\ndf = read_csv('sample.csv')\n\ndf.ID\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\ndf = read_csv('sample.csv', converters={'ID': str})\ndf.ID\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\n""]";False;"[""import pandas as pd\nID\n\n\ndf = read_csv('sample.csv')\n\ndf.ID\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\ndf = read_csv('sample.csv', converters={'ID': str})\ndf.ID\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\n""]";False;0;1;"[""name 'read_csv' is not defined""]";['NameError'];0;1;"[""name 'read_csv' is not defined""]";['NameError'];0;1;"[""name 'read_csv' is not defined""]";['NameError']
140;140;140;140;7.0;1;13295735;;1;145;<python><pandas>;How can I replace all the NaN values with Zero's in a column of a pandas dataframe;174458.0;['      itm Date                  Amount \n67    420 2012-09-30 00:00:00   65211\n68    421 2012-09-09 00:00:00   29424\n69    421 2012-09-16 00:00:00   29877\n70    421 2012-09-23 00:00:00   30990\n71    421 2012-09-30 00:00:00   61303\n72    485 2012-09-09 00:00:00   71781\n73    485 2012-09-16 00:00:00     NaN\n74    485 2012-09-23 00:00:00   11072\n75    485 2012-09-30 00:00:00  113702\n76    489 2012-09-09 00:00:00   64731\n77    489 2012-09-16 00:00:00     NaN\nValueError: cannot convert float NaN to integer\n'];['      itm Date                  Amount \n67    420 2012-09-30 00:00:00   65211\n68    421 2012-09-09 00:00:00   29424\n69    421 2012-09-16 00:00:00   29877\n70    421 2012-09-23 00:00:00   30990\n71    421 2012-09-30 00:00:00   61303\n72    485 2012-09-09 00:00:00   71781\n73    485 2012-09-16 00:00:00     NaN\n74    485 2012-09-23 00:00:00   11072\n75    485 2012-09-30 00:00:00  113702\n76    489 2012-09-09 00:00:00   64731\n77    489 2012-09-16 00:00:00     NaN\n', 'ValueError: cannot convert float NaN to integer\n'];['      itm Date                  Amount \n67    420 2012-09-30 00:00:00   65211\n68    421 2012-09-09 00:00:00   29424\n69    421 2012-09-16 00:00:00   29877\n70    421 2012-09-23 00:00:00   30990\n71    421 2012-09-30 00:00:00   61303\n72    485 2012-09-09 00:00:00   71781\n73    485 2012-09-16 00:00:00     NaN\n74    485 2012-09-23 00:00:00   11072\n75    485 2012-09-30 00:00:00  113702\n76    489 2012-09-09 00:00:00   64731\n77    489 2012-09-16 00:00:00     NaN\n', 'ValueError: cannot convert float NaN to integer\n'];[''];[''];False;['import pandas as pd\n'];False;0;4;"[""name 'df' is not defined"", ""name 'pd' is not defined"", ""File b'somefile.txt' does not exist"", ""name 'df' is not defined""]";['NameError', 'NameError', 'FileNotFoundError', 'NameError'];0;4;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""File b'somefile.txt' does not exist"", ""name 'df' is not defined""]";['NameError', 'NameError', 'FileNotFoundError', 'NameError'];0;4;"['1', ""name 'mask_1' is not defined"", ""File b'somefile.txt' does not exist"", ""'column'""]";['KeyError', 'NameError', 'FileNotFoundError', 'KeyError']
141;141;141;141;5.0;0;13331518;;1;26;<python><pandas>;How to add a single item to a Pandas Series;38577.0;['>> x = Series()\n>> N = 4\n>> for i in xrange(N):\n>>     x.some_appending_function(i**2)    \n>> print x\n\n0 | 0\n1 | 1\n2 | 4\n3 | 9\n'];['>> x = Series()\n>> N = 4\n>> for i in xrange(N):\n>>     x.some_appending_function(i**2)    \n>> print x\n\n0 | 0\n1 | 1\n2 | 4\n3 | 9\n'];['>> x = Series()\n>> N = 4\n>> for i in xrange(N):\n>>     x.some_appending_function(i**2)    \n>> print x\n\n0 | 0\n1 | 1\n2 | 4\n3 | 9\n'];['\n0 | 0\n1 | 1\n2 | 4\n3 | 9\n'];['\n0 | 0\n1 | 1\n2 | 4\n3 | 9\n'];False;['import pandas as pd\n\n0 | 0\n1 | 1\n2 | 4\n3 | 9\n'];False;0;3;"[""name 'p' is not defined"", ""name 'test' is not defined"", ""name 's' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'p' is not defined"", ""name 'test' is not defined"", ""name 's' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'p' is not defined"", ""name 'test' is not defined"", ""name 's' is not defined""]";['NameError', 'NameError', 'NameError']
142;142;142;142;8.0;4;13331698;;1;137;<python><pandas>;How to apply a function to two columns of Pandas dataframe;154207.0;"[""df['col_3'] = df[['col_1','col_2']].apply(f)  \n# Pandas gives : TypeError: ('<lambda>() takes exactly 2 arguments (1 given)'\nimport pandas as pd\n\ndf = pd.DataFrame({'ID':['1','2','3'], 'col_1': [0,2,3], 'col_2':[1,4,5]})\nmylist = ['a','b','c','d','e','f']\n\ndef get_sublist(sta,end):\n    return mylist[sta:end+1]\n\n#df['col_3'] = df[['col_1','col_2']].apply(get_sublist,axis=1)\n# expect above to output df as below \n\n  ID  col_1  col_2            col_3\n0  1      0      1       ['a', 'b']\n1  2      2      4  ['c', 'd', 'e']\n2  3      3      5  ['d', 'e', 'f']\n""]";"[""df['col_3'] = df[['col_1','col_2']].apply(f)  \n# Pandas gives : TypeError: ('<lambda>() takes exactly 2 arguments (1 given)'\n"", ""import pandas as pd\n\ndf = pd.DataFrame({'ID':['1','2','3'], 'col_1': [0,2,3], 'col_2':[1,4,5]})\nmylist = ['a','b','c','d','e','f']\n\ndef get_sublist(sta,end):\n    return mylist[sta:end+1]\n\n#df['col_3'] = df[['col_1','col_2']].apply(get_sublist,axis=1)\n# expect above to output df as below \n\n  ID  col_1  col_2            col_3\n0  1      0      1       ['a', 'b']\n1  2      2      4  ['c', 'd', 'e']\n2  3      3      5  ['d', 'e', 'f']\n""]";"['df', ""'ID', 'col_1', 'col_2'"", 'f = lambda x, y : my_function_expression', 'f', 'df', ""'col_1', 'col_2'"", ""'col_3'"", ""df['col_3'] = df[['col_1','col_2']].apply(f)  \n# Pandas gives : TypeError: ('<lambda>() takes exactly 2 arguments (1 given)'\n"", ""import pandas as pd\n\ndf = pd.DataFrame({'ID':['1','2','3'], 'col_1': [0,2,3], 'col_2':[1,4,5]})\nmylist = ['a','b','c','d','e','f']\n\ndef get_sublist(sta,end):\n    return mylist[sta:end+1]\n\n#df['col_3'] = df[['col_1','col_2']].apply(get_sublist,axis=1)\n# expect above to output df as below \n\n  ID  col_1  col_2            col_3\n0  1      0      1       ['a', 'b']\n1  2      2      4  ['c', 'd', 'e']\n2  3      3      5  ['d', 'e', 'f']\n""]";"[""df['col_3'] = df[['col_1','col_2']].apply(f)  \n# Pandas gives : TypeError: ('<lambda>() takes exactly 2 arguments (1 given)'\nimport pandas as pd\n\ndf = pd.DataFrame({'ID':['1','2','3'], 'col_1': [0,2,3], 'col_2':[1,4,5]})\nmylist = ['a','b','c','d','e','f']\n\n\n#df['col_3'] = df[['col_1','col_2']].apply(get_sublist,axis=1)\n# expect above to output df as below \n\n""]";"[""df['col_3'] = df[['col_1','col_2']].apply(f)  \n# Pandas gives : TypeError: ('<lambda>() takes exactly 2 arguments (1 given)'\nimport pandas as pd\n\ndf = pd.DataFrame({'ID':['1','2','3'], 'col_1': [0,2,3], 'col_2':[1,4,5]})\nmylist = ['a','b','c','d','e','f']\n\n\n#df['col_3'] = df[['col_1','col_2']].apply(get_sublist,axis=1)\n# expect above to output df as below \n\n""]";False;"[""import pandas as pd\ndf['col_3'] = df[['col_1','col_2']].apply(f)  \n# Pandas gives : TypeError: ('<lambda>() takes exactly 2 arguments (1 given)'\nimport pandas as pd\n\ndf = pd.DataFrame({'ID':['1','2','3'], 'col_1': [0,2,3], 'col_2':[1,4,5]})\nmylist = ['a','b','c','d','e','f']\n\n\n#df['col_3'] = df[['col_1','col_2']].apply(get_sublist,axis=1)\n# expect above to output df as below \n\n""]";False;1;5;"[""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined"", ""name 'sublst' is not defined"", ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError', 'NameError', 'NameError'];1;5;"[""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined"", ""name 'sublst' is not defined"", ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError', 'NameError', 'NameError'];2;5;"['Sucess', 'Sucess', ""'DataFrame' object has no attribute 'col_1'"", ""name 'sublst' is not defined"", '""[\'col_1\' \'col_2\'] not in index""']";['Sucess', 'Sucess', 'AttributeError', 'NameError', 'KeyError']
143;143;143;143;1.0;0;13370525;;1;13;<python><python-2.7><pandas>;Filling continuous pandas dataframe from sparse dataframe;6017.0;"[""df1 = pd.DataFrame(data=date_dict.values(),\n                   index=[datetime.datetime.combine(i, datetime.time()) \n                          for i in date_dict.keys()],\n                   columns=['Name'])\ndf1 = df1.sort(axis=0)\ndf1.head()\n             Name\nDate\n2003-06-24   2\n2003-08-13   1\n2003-08-19   2\n2003-08-22   1\n2003-08-24   5\ndf2 = pd.DataFrame(data=None,columns=['Empty'],\n                   index=pd.DateRange(min(date_dict.keys()),\n                                      max(date_dict.keys())))\ndf3 = df1.join(df2,how='right')\ndf3.head()\n            Name    Empty\n2003-06-24   2   NaN\n2003-06-25  NaN  NaN\n2003-06-26  NaN  NaN\n2003-06-27  NaN  NaN\n2003-06-30  NaN  NaN\n            Name\n2003-06-24   2\n2003-06-25   0\n2003-06-26   0\n2003-06-27   0\n2003-06-30   0\n""]";"[""df1 = pd.DataFrame(data=date_dict.values(),\n                   index=[datetime.datetime.combine(i, datetime.time()) \n                          for i in date_dict.keys()],\n                   columns=['Name'])\ndf1 = df1.sort(axis=0)\n"", 'df1.head()\n             Name\nDate\n2003-06-24   2\n2003-08-13   1\n2003-08-19   2\n2003-08-22   1\n2003-08-24   5\n', ""df2 = pd.DataFrame(data=None,columns=['Empty'],\n                   index=pd.DateRange(min(date_dict.keys()),\n                                      max(date_dict.keys())))\ndf3 = df1.join(df2,how='right')\ndf3.head()\n            Name    Empty\n2003-06-24   2   NaN\n2003-06-25  NaN  NaN\n2003-06-26  NaN  NaN\n2003-06-27  NaN  NaN\n2003-06-30  NaN  NaN\n"", '            Name\n2003-06-24   2\n2003-06-25   0\n2003-06-26   0\n2003-06-27   0\n2003-06-30   0\n']";"[""df1 = pd.DataFrame(data=date_dict.values(),\n                   index=[datetime.datetime.combine(i, datetime.time()) \n                          for i in date_dict.keys()],\n                   columns=['Name'])\ndf1 = df1.sort(axis=0)\n"", 'df1.head()\n             Name\nDate\n2003-06-24   2\n2003-08-13   1\n2003-08-19   2\n2003-08-22   1\n2003-08-24   5\n', ""df2 = pd.DataFrame(data=None,columns=['Empty'],\n                   index=pd.DateRange(min(date_dict.keys()),\n                                      max(date_dict.keys())))\ndf3 = df1.join(df2,how='right')\ndf3.head()\n            Name    Empty\n2003-06-24   2   NaN\n2003-06-25  NaN  NaN\n2003-06-26  NaN  NaN\n2003-06-27  NaN  NaN\n2003-06-30  NaN  NaN\n"", '            Name\n2003-06-24   2\n2003-06-25   0\n2003-06-26   0\n2003-06-27   0\n2003-06-30   0\n']";"[""df1 = df1.sort(axis=0)\ndf1.head()\nDate\ndf3 = df1.join(df2,how='right')\ndf3.head()\n""]";"[""df1 = df1.sort(axis=0)\ndf1.head()\nDate\ndf3 = df1.join(df2,how='right')\ndf3.head()\n""]";False;"[""import pandas as pd\ndf2 = pd.DataFrame()\ndf = pd.DataFrame()\ndf1 = df1.sort(axis=0)\ndf1.head()\nDate\ndf3 = df1.join(df2,how='right')\ndf3.head()\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'date_index' is not defined""]";['NameError'];0;1;"[""name 'date_index' is not defined""]";['NameError']
144;144;144;144;6.0;0;13385860;;1;28;<python><parsing><pandas>;How can I remove extra whitespace from strings when parsing a csv file in Pandas?;28532.0;"['    1997,Ford,E350\n    1997, Ford , E350\n    1997,Ford,E350,""Super, luxurious truck""\n    1997,Ford,E350,""Super """"luxurious"""" truck""\n    1997,Ford,E350,"" Super luxurious truck ""\n    ""1997"",Ford,E350\n    1997,Ford,E350\n    2000,Mercury,Cougar\n       Year     Make   Model              Description\n    0  1997     Ford    E350                     None\n    1  1997     Ford    E350                     None\n    2  1997     Ford    E350   Super, luxurious truck\n    3  1997     Ford    E350  Super ""luxurious"" truck\n    4  1997     Ford    E350    Super luxurious truck\n    5  1997     Ford    E350                     None\n    6  1997     Ford    E350                     None\n    7  2000  Mercury  Cougar                     None\n    pd.read_table(""data.csv"", sep=r\',\', names=[""Year"", ""Make"", ""Model"", ""Description""])\n    Year     Make   Model              Description\n 0  1997     Ford    E350                     None\n 1  1997    Ford     E350                     None\n 2  1997     Ford    E350   Super, luxurious truck\n 3  1997     Ford    E350  Super ""luxurious"" truck\n 4  1997     Ford    E350   Super luxurious truck \n 5  1997     Ford    E350                     None\n 6  1997     Ford    E350                     None\n 7  2000  Mercury  Cougar                     None\n']";"['    1997,Ford,E350\n    1997, Ford , E350\n    1997,Ford,E350,""Super, luxurious truck""\n    1997,Ford,E350,""Super """"luxurious"""" truck""\n    1997,Ford,E350,"" Super luxurious truck ""\n    ""1997"",Ford,E350\n    1997,Ford,E350\n    2000,Mercury,Cougar\n', '       Year     Make   Model              Description\n    0  1997     Ford    E350                     None\n    1  1997     Ford    E350                     None\n    2  1997     Ford    E350   Super, luxurious truck\n    3  1997     Ford    E350  Super ""luxurious"" truck\n    4  1997     Ford    E350    Super luxurious truck\n    5  1997     Ford    E350                     None\n    6  1997     Ford    E350                     None\n    7  2000  Mercury  Cougar                     None\n', '    pd.read_table(""data.csv"", sep=r\',\', names=[""Year"", ""Make"", ""Model"", ""Description""])\n', '    Year     Make   Model              Description\n 0  1997     Ford    E350                     None\n 1  1997    Ford     E350                     None\n 2  1997     Ford    E350   Super, luxurious truck\n 3  1997     Ford    E350  Super ""luxurious"" truck\n 4  1997     Ford    E350   Super luxurious truck \n 5  1997     Ford    E350                     None\n 6  1997     Ford    E350                     None\n 7  2000  Mercury  Cougar                     None\n']";"['    1997,Ford,E350\n    1997, Ford , E350\n    1997,Ford,E350,""Super, luxurious truck""\n    1997,Ford,E350,""Super """"luxurious"""" truck""\n    1997,Ford,E350,"" Super luxurious truck ""\n    ""1997"",Ford,E350\n    1997,Ford,E350\n    2000,Mercury,Cougar\n', '       Year     Make   Model              Description\n    0  1997     Ford    E350                     None\n    1  1997     Ford    E350                     None\n    2  1997     Ford    E350   Super, luxurious truck\n    3  1997     Ford    E350  Super ""luxurious"" truck\n    4  1997     Ford    E350    Super luxurious truck\n    5  1997     Ford    E350                     None\n    6  1997     Ford    E350                     None\n    7  2000  Mercury  Cougar                     None\n', '    pd.read_table(""data.csv"", sep=r\',\', names=[""Year"", ""Make"", ""Model"", ""Description""])\n', '    Year     Make   Model              Description\n 0  1997     Ford    E350                     None\n 1  1997    Ford     E350                     None\n 2  1997     Ford    E350   Super, luxurious truck\n 3  1997     Ford    E350  Super ""luxurious"" truck\n 4  1997     Ford    E350   Super luxurious truck \n 5  1997     Ford    E350                     None\n 6  1997     Ford    E350                     None\n 7  2000  Mercury  Cougar                     None\n']";[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'table' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'table' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'table' is not defined""]";['Sucess', 'NameError']
145;145;145;145;4.0;0;13389203;;1;11;<python><pandas>;pandas: slice a MultiIndex by range of secondary index;11982.0;"[""import numpy as np\nimport pandas as pd\n\nbuckets = np.repeat(['a','b','c'], [3,5,1])\nsequence = [0,1,5,0,1,2,4,50,0]\n\ns = pd.Series(\n    np.random.randn(len(sequence)), \n    index=pd.MultiIndex.from_tuples(zip(buckets, sequence))\n)\n\n# In [6]: s\n# Out[6]: \n# a  0    -1.106047\n#    1     1.665214\n#    5     0.279190\n# b  0     0.326364\n#    1     0.900439\n#    2    -0.653940\n#    4     0.082270\n#    50   -0.255482\n# c  0    -0.091730\ns['a':'b']\n# Out[109]: \n# bucket  value\n# a       0        1.828176\n#         1        0.160496\n#         5        0.401985\n# b       0       -1.514268\n#         1       -0.973915\n#         2        1.285553\n#         4       -0.194625\n#         5       -0.144112\ns['b'][1:10]\n\n# In [61]: s['b'][1:10]\n# Out[61]: \n# 1     0.900439\n# 2    -0.653940\n# 4     0.082270\n# 50   -0.255482\nIn [26]: s\nOut[26]: \n0   a   -0.126299\n1   a    1.810928\n5   a    0.571873\n0   b   -0.116108\n1   b   -0.712184\n2   b   -1.771264\n4   b    0.148961\n50  b    0.089683\n0   c   -0.582578\n\nIn [25]: s[0]['a':'b']\nOut[25]: \na   -0.126299\nb   -0.116108\n""]";"[""import numpy as np\nimport pandas as pd\n\nbuckets = np.repeat(['a','b','c'], [3,5,1])\nsequence = [0,1,5,0,1,2,4,50,0]\n\ns = pd.Series(\n    np.random.randn(len(sequence)), \n    index=pd.MultiIndex.from_tuples(zip(buckets, sequence))\n)\n\n# In [6]: s\n# Out[6]: \n# a  0    -1.106047\n#    1     1.665214\n#    5     0.279190\n# b  0     0.326364\n#    1     0.900439\n#    2    -0.653940\n#    4     0.082270\n#    50   -0.255482\n# c  0    -0.091730\n"", ""s['a':'b']\n# Out[109]: \n# bucket  value\n# a       0        1.828176\n#         1        0.160496\n#         5        0.401985\n# b       0       -1.514268\n#         1       -0.973915\n#         2        1.285553\n#         4       -0.194625\n#         5       -0.144112\n"", ""s['b'][1:10]\n\n# In [61]: s['b'][1:10]\n# Out[61]: \n# 1     0.900439\n# 2    -0.653940\n# 4     0.082270\n# 50   -0.255482\n"", ""In [26]: s\nOut[26]: \n0   a   -0.126299\n1   a    1.810928\n5   a    0.571873\n0   b   -0.116108\n1   b   -0.712184\n2   b   -1.771264\n4   b    0.148961\n50  b    0.089683\n0   c   -0.582578\n\nIn [25]: s[0]['a':'b']\nOut[25]: \na   -0.126299\nb   -0.116108\n""]";"[""import numpy as np\nimport pandas as pd\n\nbuckets = np.repeat(['a','b','c'], [3,5,1])\nsequence = [0,1,5,0,1,2,4,50,0]\n\ns = pd.Series(\n    np.random.randn(len(sequence)), \n    index=pd.MultiIndex.from_tuples(zip(buckets, sequence))\n)\n\n# In [6]: s\n# Out[6]: \n# a  0    -1.106047\n#    1     1.665214\n#    5     0.279190\n# b  0     0.326364\n#    1     0.900439\n#    2    -0.653940\n#    4     0.082270\n#    50   -0.255482\n# c  0    -0.091730\n"", 'sequence', ""s['a':'b']\n# Out[109]: \n# bucket  value\n# a       0        1.828176\n#         1        0.160496\n#         5        0.401985\n# b       0       -1.514268\n#         1       -0.973915\n#         2        1.285553\n#         4       -0.194625\n#         5       -0.144112\n"", ""s['b'][1:10]\n\n# In [61]: s['b'][1:10]\n# Out[61]: \n# 1     0.900439\n# 2    -0.653940\n# 4     0.082270\n# 50   -0.255482\n"", ""In [26]: s\nOut[26]: \n0   a   -0.126299\n1   a    1.810928\n5   a    0.571873\n0   b   -0.116108\n1   b   -0.712184\n2   b   -1.771264\n4   b    0.148961\n50  b    0.089683\n0   c   -0.582578\n\nIn [25]: s[0]['a':'b']\nOut[25]: \na   -0.126299\nb   -0.116108\n""]";"[""s\n# s['b'][1:10]\n# s\ns[0]['a':'b']\n""]";"[""s\n# s['b'][1:10]\n# s\ns[0]['a':'b']\n""]";False;"[""import pandas as pd\ns\n# s['b'][1:10]\n# s\ns[0]['a':'b']\n""]";False;0;1;"[""name 's' is not defined""]";['NameError'];0;1;"[""name 's' is not defined""]";['NameError'];0;1;"[""name 's' is not defined""]";['NameError']
146;146;146;146;2.0;0;13404468;;1;32;<python><pandas><scipy><statistics><hypothesis-test>;T-test in Pandas (Python);27308.0;"[""data = {'Category': ['cat2','cat1','cat2','cat1','cat2','cat1','cat2','cat1','cat1','cat1','cat2'],\n        'values': [1,2,3,1,2,3,1,2,3,5,1]}\nmy_data = DataFrame(data)\nmy_data.groupby('Category').mean()\n\nCategory:     values:   \ncat1     2.666667\ncat2     1.600000\n""]";"[""data = {'Category': ['cat2','cat1','cat2','cat1','cat2','cat1','cat2','cat1','cat1','cat1','cat2'],\n        'values': [1,2,3,1,2,3,1,2,3,5,1]}\nmy_data = DataFrame(data)\nmy_data.groupby('Category').mean()\n\nCategory:     values:   \ncat1     2.666667\ncat2     1.600000\n""]";"[""data = {'Category': ['cat2','cat1','cat2','cat1','cat2','cat1','cat2','cat1','cat1','cat1','cat2'],\n        'values': [1,2,3,1,2,3,1,2,3,5,1]}\nmy_data = DataFrame(data)\nmy_data.groupby('Category').mean()\n\nCategory:     values:   \ncat1     2.666667\ncat2     1.600000\n""]";"[""my_data = DataFrame(data)\nmy_data.groupby('Category').mean()\n\n""]";"[""from pandas import DataFrame\nmy_data = DataFrame(data)\nmy_data.groupby('Category').mean()\n\n""]";True;"[""import pandas as pd\nmy_data = DataFrame(data)\nmy_data.groupby('Category').mean()\n\n""]";False;0;1;"[""No module named 'scipy'""]";['ImportError'];0;1;"[""No module named 'scipy'""]";['ImportError'];0;1;"[""No module named 'scipy'""]";['ImportError']
147;147;147;147;11.0;0;13411544;;1;643;<python><pandas><design><dataframe><magic-methods>;Delete column from pandas DataFrame;684200.0;"[""del df['column_name']\ndel df.column_name\n""]";"[""del df['column_name']\n"", 'del df.column_name\n']";"[""del df['column_name']\n"", 'del df.column_name\n', 'df.column_name']";"[""del df['column_name']\ndel df.column_name\n""]";"[""del df['column_name']\ndel df.column_name\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndel df['column_name']\ndel df.column_name\n""]";True;4;10;"[""name 'DataFrame' is not defined"", 'Sucess', ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError', 'Sucess', 'NameError', 'NameError', 'Sucess', 'NameError', 'Sucess', 'NameError'];5;10;"['Sucess', 'Sucess', ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['Sucess', 'Sucess', 'NameError', 'Sucess', 'NameError', 'NameError', 'Sucess', 'NameError', 'Sucess', 'NameError'];6;10;"[""name 'DataFrame' is not defined"", 'Sucess', ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined"", 'Sucess', 'Sucess', 'Sucess', 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError', 'Sucess', 'NameError', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'NameError']
148;148;148;148;8.0;2;13413590;;1;272;<python><pandas><dataframe>;How to drop rows of Pandas DataFrame whose value in certain columns is NaN;280310.0;['>>> df\n                 STK_ID  EPS  cash\nSTK_ID RPT_Date                   \n601166 20111231  601166  NaN   NaN\n600036 20111231  600036  NaN    12\n600016 20111231  600016  4.3   NaN\n601009 20111231  601009  NaN   NaN\n601939 20111231  601939  2.5   NaN\n000001 20111231  000001  NaN   NaN\n                  STK_ID  EPS  cash\nSTK_ID RPT_Date                   \n600016 20111231  600016  4.3   NaN\n601939 20111231  601939  2.5   NaN\n'];['>>> df\n                 STK_ID  EPS  cash\nSTK_ID RPT_Date                   \n601166 20111231  601166  NaN   NaN\n600036 20111231  600036  NaN    12\n600016 20111231  600016  4.3   NaN\n601009 20111231  601009  NaN   NaN\n601939 20111231  601939  2.5   NaN\n000001 20111231  000001  NaN   NaN\n', '                  STK_ID  EPS  cash\nSTK_ID RPT_Date                   \n600016 20111231  600016  4.3   NaN\n601939 20111231  601939  2.5   NaN\n'];['DataFrame', '>>> df\n                 STK_ID  EPS  cash\nSTK_ID RPT_Date                   \n601166 20111231  601166  NaN   NaN\n600036 20111231  600036  NaN    12\n600016 20111231  600016  4.3   NaN\n601009 20111231  601009  NaN   NaN\n601939 20111231  601939  2.5   NaN\n000001 20111231  000001  NaN   NaN\n', 'EPS', 'NaN', 'df.drop(....)', '                  STK_ID  EPS  cash\nSTK_ID RPT_Date                   \n600016 20111231  600016  4.3   NaN\n601939 20111231  601939  2.5   NaN\n'];[''];[''];False;['import pandas as pd\n'];False;0;4;"[""name 'df' is not defined"", ""name 'pd' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""name 'df' is not defined"", ""name 'np' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""name 'df' is not defined"", ""name 'np' is not defined"", ""name 'df' is not defined"", ""'DataFrame' object has no attribute 'EPS'""]";['NameError', 'NameError', 'NameError', 'AttributeError']
149;149;149;149;3.0;0;13445174;;1;15;<datetime><time-series><pandas>;Date ranges in Pandas;19749.0;"[""from datetime import date\nimport pandas as pd\n\nstart = date('2012-01-15')\nend = date('2012-09-20')\n# 'M' is month-end, instead I need same-day-of-month\ndate_range(start, end, freq='M')\n2012-01-15\n2012-02-15\n2012-03-15\n...\n2012-09-15\n2012-01-31\n2012-02-29\n2012-03-31\n...\n2012-08-31\nrrule(freq=MONTHLY, dtstart=start, bymonthday=(start.day, -1), bysetpos=1)\n                total\n2012-01-10 00:01    50\n2012-01-15 01:01    55\n2012-03-11 00:01    60\n2012-04-28 00:01    80\n\n#Hypothetical usage\ndataframe.resample('total', how='sum', freq='M', start='2012-01-09', end='2012-04-15') \n                total\n2012-01-09          105 # Values summed\n2012-02-09          0   # Missing from dataframe\n2012-03-09          60\n2012-04-09          0   # Data past end date, not counted\n""]";"[""from datetime import date\nimport pandas as pd\n\nstart = date('2012-01-15')\nend = date('2012-09-20')\n# 'M' is month-end, instead I need same-day-of-month\ndate_range(start, end, freq='M')\n"", '2012-01-15\n2012-02-15\n2012-03-15\n...\n2012-09-15\n', '2012-01-31\n2012-02-29\n2012-03-31\n...\n2012-08-31\n', 'rrule(freq=MONTHLY, dtstart=start, bymonthday=(start.day, -1), bysetpos=1)\n', ""                total\n2012-01-10 00:01    50\n2012-01-15 01:01    55\n2012-03-11 00:01    60\n2012-04-28 00:01    80\n\n#Hypothetical usage\ndataframe.resample('total', how='sum', freq='M', start='2012-01-09', end='2012-04-15') \n"", '                total\n2012-01-09          105 # Values summed\n2012-02-09          0   # Missing from dataframe\n2012-03-09          60\n2012-04-09          0   # Data past end date, not counted\n']";"['date_range()', ""from datetime import date\nimport pandas as pd\n\nstart = date('2012-01-15')\nend = date('2012-09-20')\n# 'M' is month-end, instead I need same-day-of-month\ndate_range(start, end, freq='M')\n"", '2012-01-15\n2012-02-15\n2012-03-15\n...\n2012-09-15\n', '2012-01-31\n2012-02-29\n2012-03-31\n...\n2012-08-31\n', 'rrule(freq=MONTHLY, dtstart=start, bymonthday=(start.day, -1), bysetpos=1)\n', 'date_range()', 'period_range()', 'groupby', 'crosstab', 'resample', ""                total\n2012-01-10 00:01    50\n2012-01-15 01:01    55\n2012-03-11 00:01    60\n2012-04-28 00:01    80\n\n#Hypothetical usage\ndataframe.resample('total', how='sum', freq='M', start='2012-01-09', end='2012-04-15') \n"", '                total\n2012-01-09          105 # Values summed\n2012-02-09          0   # Missing from dataframe\n2012-03-09          60\n2012-04-09          0   # Data past end date, not counted\n']";"[""from datetime import date\nimport pandas as pd\n\nstart = date('2012-01-15')\nend = date('2012-09-20')\n# 'M' is month-end, instead I need same-day-of-month\ndate_range(start, end, freq='M')\n...\n...\nrrule(freq=MONTHLY, dtstart=start, bymonthday=(start.day, -1), bysetpos=1)\n\n#Hypothetical usage\ndataframe.resample('total', how='sum', freq='M', start='2012-01-09', end='2012-04-15') \n""]";"[""from datetime import date\nimport pandas as pd\n\nstart = date('2012-01-15')\nend = date('2012-09-20')\n# 'M' is month-end, instead I need same-day-of-month\ndate_range(start, end, freq='M')\n...\n...\nrrule(freq=MONTHLY, dtstart=start, bymonthday=(start.day, -1), bysetpos=1)\n\n#Hypothetical usage\ndataframe.resample('total', how='sum', freq='M', start='2012-01-09', end='2012-04-15') \n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nfrom datetime import date\nimport pandas as pd\n\nstart = date('2012-01-15')\nend = date('2012-09-20')\n# 'M' is month-end, instead I need same-day-of-month\ndate_range(start, end, freq='M')\n...\n...\nrrule(freq=MONTHLY, dtstart=start, bymonthday=(start.day, -1), bysetpos=1)\n\n#Hypothetical usage\ndataframe.resample('total', how='sum', freq='M', start='2012-01-09', end='2012-04-15') \n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'start' is not defined""]";['NameError'];0;1;"[""name 'start' is not defined""]";['NameError']
150;150;150;150;3.0;2;13445241;;1;57;<python><pandas>;Replacing blank values (white space) with NaN in pandas;50114.0;"[""                   A    B    C\n2000-01-01 -0.532681  foo    0\n2000-01-02  1.490752  bar    1\n2000-01-03 -1.387326  foo    2\n2000-01-04  0.814772  baz     \n2000-01-05 -0.222552         4\n2000-01-06 -1.176781  qux     \n                   A     B     C\n2000-01-01 -0.532681   foo     0\n2000-01-02  1.490752   bar     1\n2000-01-03 -1.387326   foo     2\n2000-01-04  0.814772   baz   NaN\n2000-01-05 -0.222552   NaN     4\n2000-01-06 -1.176781   qux   NaN\nfor i in df.columns:\n    df[i][df[i].apply(lambda i: True if re.search('^\\s*$', str(i)) else False)]=None\nif df[i].dtype == np.dtype('object')\n""]";"['                   A    B    C\n2000-01-01 -0.532681  foo    0\n2000-01-02  1.490752  bar    1\n2000-01-03 -1.387326  foo    2\n2000-01-04  0.814772  baz     \n2000-01-05 -0.222552         4\n2000-01-06 -1.176781  qux     \n', '                   A     B     C\n2000-01-01 -0.532681   foo     0\n2000-01-02  1.490752   bar     1\n2000-01-03 -1.387326   foo     2\n2000-01-04  0.814772   baz   NaN\n2000-01-05 -0.222552   NaN     4\n2000-01-06 -1.176781   qux   NaN\n', ""for i in df.columns:\n    df[i][df[i].apply(lambda i: True if re.search('^\\s*$', str(i)) else False)]=None\n"", ""if df[i].dtype == np.dtype('object')\n""]";"['                   A    B    C\n2000-01-01 -0.532681  foo    0\n2000-01-02  1.490752  bar    1\n2000-01-03 -1.387326  foo    2\n2000-01-04  0.814772  baz     \n2000-01-05 -0.222552         4\n2000-01-06 -1.176781  qux     \n', '                   A     B     C\n2000-01-01 -0.532681   foo     0\n2000-01-02  1.490752   bar     1\n2000-01-03 -1.387326   foo     2\n2000-01-04  0.814772   baz   NaN\n2000-01-05 -0.222552   NaN     4\n2000-01-06 -1.176781   qux   NaN\n', ""for i in df.columns:\n    df[i][df[i].apply(lambda i: True if re.search('^\\s*$', str(i)) else False)]=None\n"", ""if df[i].dtype == np.dtype('object')\n""]";[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'd' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'd' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'd' is not defined"", 'Sucess']";['NameError', 'Sucess']
151;151;151;151;4.0;1;13446480;;1;17;<python><numpy><python-2.7><pandas>;Python Pandas: remove entries based on the number of occurrences;6714.0;"[""pid   tag\n1     23    \n1     45\n1     62\n2     24\n2     45\n3     34\n3     25\n3     62\nbytag = data.groupby('tag').aggregate(np.count_nonzero)\n""]";"['pid   tag\n1     23    \n1     45\n1     62\n2     24\n2     45\n3     34\n3     25\n3     62\n', ""bytag = data.groupby('tag').aggregate(np.count_nonzero)\n""]";"['data', 'pid   tag\n1     23    \n1     45\n1     62\n2     24\n2     45\n3     34\n3     25\n3     62\n', ""bytag = data.groupby('tag').aggregate(np.count_nonzero)\n""]";"[""bytag = data.groupby('tag').aggregate(np.count_nonzero)\n""]";"[""bytag = data.groupby('tag').aggregate(np.count_nonzero)\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nbytag = data.groupby('tag').aggregate(np.count_nonzero)\n""]";True;0;2;"[""name 'data' is not defined"", ""name 'data' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'data' is not defined"", ""name 'data' is not defined""]";['NameError', 'NameError'];0;2;"[""'tag'"", ""'tag'""]";['KeyError', 'KeyError']
152;152;152;152;2.0;0;13575090;;1;29;<python><dataframe><pandas>;Construct pandas DataFrame from items in nested dictionary;27582.0;"['user_dict[12] = {\n    ""Category 1"": {""att_1"": 1, \n                   ""att_2"": ""whatever""},\n    ""Category 2"": {""att_1"": 23, \n                   ""att_2"": ""another""}}\ndf = pandas.DataFrame(users_summary)\n']";"['user_dict[12] = {\n    ""Category 1"": {""att_1"": 1, \n                   ""att_2"": ""whatever""},\n    ""Category 2"": {""att_1"": 23, \n                   ""att_2"": ""another""}}\n', 'df = pandas.DataFrame(users_summary)\n']";"['user_dict[12] = {\n    ""Category 1"": {""att_1"": 1, \n                   ""att_2"": ""whatever""},\n    ""Category 2"": {""att_1"": 23, \n                   ""att_2"": ""another""}}\n', 'df = pandas.DataFrame(users_summary)\n']";['df = pandas.DataFrame(users_summary)\n'];['df = pandas.DataFrame(users_summary)\n'];False;['import pandas as pd\ndf = pandas.DataFrame(users_summary)\n'];False;0;2;"[""name 'pd' is not defined"", ""name 'd' is not defined""]";['NameError', 'NameError'];0;2;"['No objects to concatenate', ""name 'd' is not defined""]";['ValueError', 'NameError'];0;2;"['No objects to concatenate', ""name 'd' is not defined""]";['ValueError', 'NameError']
153;153;153;153;1.0;0;13582449;;1;21;<python><pandas>;Convert DataFrameGroupBy object to DataFrame pandas;15555.0;"[""kl = ks.groupby('FIPS')\n\nkl.aggregate(np.sum)\n""]";"[""kl = ks.groupby('FIPS')\n\nkl.aggregate(np.sum)\n""]";"[""kl = ks.groupby('FIPS')\n\nkl.aggregate(np.sum)\n"", 'pandas.core.groupby.DataFrameGroupBy']";"[""kl = ks.groupby('FIPS')\n\nkl.aggregate(np.sum)\n""]";"[""kl = ks.groupby('FIPS')\n\nkl.aggregate(np.sum)\n""]";False;"[""import pandas as pd\nkl = ks.groupby('FIPS')\n\nkl.aggregate(np.sum)\n""]";False;0;1;"[""name 'A' is not defined""]";['NameError'];0;1;"[""name 'A' is not defined""]";['NameError'];0;1;"[""name 'A' is not defined""]";['NameError']
154;154;154;154;3.0;1;13611065;;1;46;<python><algorithm><pandas>;Efficient way to apply multiple filters to pandas DataFrame or Series;52152.0;"['relops = {\'>=\': [1], \'<=\': [1]}\n   def apply_relops(series, relops):\n        """"""\n        Pass dictionary of relational operators to perform on given series object\n        """"""\n        for op, vals in relops.iteritems():\n            op_func = ops[op]\n            for val in vals:\n                filtered = op_func(series, val)\n                series = series.reindex(series[filtered])\n        return series\n>>> df = pandas.DataFrame({\'col1\': [0, 1, 2], \'col2\': [10, 11, 12]})\n>>> print df\n>>> print df\n   col1  col2\n0     0    10\n1     1    11\n2     2    12\n\n>>> from operator import le, ge\n>>> ops ={\'>=\': ge, \'<=\': le}\n>>> apply_relops(df[\'col1\'], {\'>=\': [1]})\ncol1\n1       1\n2       2\nName: col1\n>>> apply_relops(df[\'col1\'], relops = {\'>=\': [1], \'<=\': [1]})\ncol1\n1       1\nName: col1\n']";"[""relops = {'>=': [1], '<=': [1]}\n"", '   def apply_relops(series, relops):\n        """"""\n        Pass dictionary of relational operators to perform on given series object\n        """"""\n        for op, vals in relops.iteritems():\n            op_func = ops[op]\n            for val in vals:\n                filtered = op_func(series, val)\n                series = series.reindex(series[filtered])\n        return series\n', "">>> df = pandas.DataFrame({'col1': [0, 1, 2], 'col2': [10, 11, 12]})\n>>> print df\n>>> print df\n   col1  col2\n0     0    10\n1     1    11\n2     2    12\n\n>>> from operator import le, ge\n>>> ops ={'>=': ge, '<=': le}\n>>> apply_relops(df['col1'], {'>=': [1]})\ncol1\n1       1\n2       2\nName: col1\n>>> apply_relops(df['col1'], relops = {'>=': [1], '<=': [1]})\ncol1\n1       1\nName: col1\n""]";"['reindex()', 'apply()', 'map()', ""relops = {'>=': [1], '<=': [1]}\n"", '   def apply_relops(series, relops):\n        """"""\n        Pass dictionary of relational operators to perform on given series object\n        """"""\n        for op, vals in relops.iteritems():\n            op_func = ops[op]\n            for val in vals:\n                filtered = op_func(series, val)\n                series = series.reindex(series[filtered])\n        return series\n', "">>> df = pandas.DataFrame({'col1': [0, 1, 2], 'col2': [10, 11, 12]})\n>>> print df\n>>> print df\n   col1  col2\n0     0    10\n1     1    11\n2     2    12\n\n>>> from operator import le, ge\n>>> ops ={'>=': ge, '<=': le}\n>>> apply_relops(df['col1'], {'>=': [1]})\ncol1\n1       1\n2       2\nName: col1\n>>> apply_relops(df['col1'], relops = {'>=': [1], '<=': [1]})\ncol1\n1       1\nName: col1\n""]";"[""relops = {'>=': [1], '<=': [1]}\n\ncol1\ncol1\n""]";"[""relops = {'>=': [1], '<=': [1]}\n\ncol1\ncol1\n""]";False;"[""import pandas as pd\nrelops = {'>=': [1], '<=': [1]}\n\ncol1\ncol1\n""]";False;0;2;"[""name 'df' is not defined"", ""name 'data' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'data' is not defined""]";['NameError', 'NameError'];0;2;"[""'col1'"", ""'DataFrame' object has no attribute 'col1'""]";['KeyError', 'AttributeError']
155;155;155;155;5.0;0;13636592;;1;23;<python><pandas>;How to sort a Pandas DataFrame according to multiple criteria?;50220.0;"[""                                          Song            Peak            Weeks\n76                            Paperback Writer               1               16\n117                               Lady Madonna               1                9\n118                                   Hey Jude               1               27\n22                           Can't Buy Me Love               1               17\n29                          A Hard Day's Night               1               14\n48                              Ticket To Ride               1               14\n56                                       Help!               1               17\n109                       All You Need Is Love               1               16\n173                The Ballad Of John And Yoko               1               13\n85                               Eleanor Rigby               1               14\n87                            Yellow Submarine               1               14\n20                    I Want To Hold Your Hand               1               24\n45                                 I Feel Fine               1               15\n60                                 Day Tripper               1               12\n61                          We Can Work It Out               1               12\n10                               She Loves You               1               36\n155                                   Get Back               1                6\n8                               From Me To You               1                7\n115                              Hello Goodbye               1                7\n2                             Please Please Me               2               20\n92                   Strawberry Fields Forever               2               12\n93                                  Penny Lane               2               13\n107                       Magical Mystery Tour               2               16\n176                                  Let It Be               2               14\n0                                   Love Me Do               4               26\n157                                  Something               4                9\n166                              Come Together               4               10\n58                                   Yesterday               8               21\n135                       Back In The U.S.S.R.              19                3\n164                         Here Comes The Sun              58               19\n96       Sgt. Pepper's Lonely Hearts Club Band              63               12\n105         With A Little Help From My Friends              63                7\n""]";"[""                                          Song            Peak            Weeks\n76                            Paperback Writer               1               16\n117                               Lady Madonna               1                9\n118                                   Hey Jude               1               27\n22                           Can't Buy Me Love               1               17\n29                          A Hard Day's Night               1               14\n48                              Ticket To Ride               1               14\n56                                       Help!               1               17\n109                       All You Need Is Love               1               16\n173                The Ballad Of John And Yoko               1               13\n85                               Eleanor Rigby               1               14\n87                            Yellow Submarine               1               14\n20                    I Want To Hold Your Hand               1               24\n45                                 I Feel Fine               1               15\n60                                 Day Tripper               1               12\n61                          We Can Work It Out               1               12\n10                               She Loves You               1               36\n155                                   Get Back               1                6\n8                               From Me To You               1                7\n115                              Hello Goodbye               1                7\n2                             Please Please Me               2               20\n92                   Strawberry Fields Forever               2               12\n93                                  Penny Lane               2               13\n107                       Magical Mystery Tour               2               16\n176                                  Let It Be               2               14\n0                                   Love Me Do               4               26\n157                                  Something               4                9\n166                              Come Together               4               10\n58                                   Yesterday               8               21\n135                       Back In The U.S.S.R.              19                3\n164                         Here Comes The Sun              58               19\n96       Sgt. Pepper's Lonely Hearts Club Band              63               12\n105         With A Little Help From My Friends              63                7\n""]";"[""                                          Song            Peak            Weeks\n76                            Paperback Writer               1               16\n117                               Lady Madonna               1                9\n118                                   Hey Jude               1               27\n22                           Can't Buy Me Love               1               17\n29                          A Hard Day's Night               1               14\n48                              Ticket To Ride               1               14\n56                                       Help!               1               17\n109                       All You Need Is Love               1               16\n173                The Ballad Of John And Yoko               1               13\n85                               Eleanor Rigby               1               14\n87                            Yellow Submarine               1               14\n20                    I Want To Hold Your Hand               1               24\n45                                 I Feel Fine               1               15\n60                                 Day Tripper               1               12\n61                          We Can Work It Out               1               12\n10                               She Loves You               1               36\n155                                   Get Back               1                6\n8                               From Me To You               1                7\n115                              Hello Goodbye               1                7\n2                             Please Please Me               2               20\n92                   Strawberry Fields Forever               2               12\n93                                  Penny Lane               2               13\n107                       Magical Mystery Tour               2               16\n176                                  Let It Be               2               14\n0                                   Love Me Do               4               26\n157                                  Something               4                9\n166                              Come Together               4               10\n58                                   Yesterday               8               21\n135                       Back In The U.S.S.R.              19                3\n164                         Here Comes The Sun              58               19\n96       Sgt. Pepper's Lonely Hearts Club Band              63               12\n105         With A Little Help From My Friends              63                7\n""]";[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'songs' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'songs' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'songs' is not defined"", ""'DataFrame' object has no attribute 'sort'"", ""'Peak'""]";['NameError', 'AttributeError', 'KeyError']
156;156;156;156;4.0;2;13636848;;1;24;<python><pandas>;is it possible to do fuzzy match merge with python pandas?;10322.0;"[""df1 = DataFrame([[1],[2],[3],[4],[5]], index=['one','two','three','four','five'], columns=['number'])\n\n       number\none         1\ntwo         2\nthree       3\nfour        4\nfive        5\n\ndf2 = DataFrame([['a'],['b'],['c'],['d'],['e']], index=['one','too','three','fours','five'], columns=['letter'])\n\n      letter\none        a\ntoo        b\nthree      c\nfours      d\nfive       e\n       number letter\none         1      a\ntwo         2      b\nthree       3      c\nfour        4      d\nfive        5      e\n""]";"[""df1 = DataFrame([[1],[2],[3],[4],[5]], index=['one','two','three','four','five'], columns=['number'])\n\n       number\none         1\ntwo         2\nthree       3\nfour        4\nfive        5\n\ndf2 = DataFrame([['a'],['b'],['c'],['d'],['e']], index=['one','too','three','fours','five'], columns=['letter'])\n\n      letter\none        a\ntoo        b\nthree      c\nfours      d\nfive       e\n"", '       number letter\none         1      a\ntwo         2      b\nthree       3      c\nfour        4      d\nfive        5      e\n']";"[""df1 = DataFrame([[1],[2],[3],[4],[5]], index=['one','two','three','four','five'], columns=['number'])\n\n       number\none         1\ntwo         2\nthree       3\nfour        4\nfive        5\n\ndf2 = DataFrame([['a'],['b'],['c'],['d'],['e']], index=['one','too','three','fours','five'], columns=['letter'])\n\n      letter\none        a\ntoo        b\nthree      c\nfours      d\nfive       e\n"", '       number letter\none         1      a\ntwo         2      b\nthree       3      c\nfour        4      d\nfive        5      e\n']";"[""df1 = DataFrame([[1],[2],[3],[4],[5]], index=['one','two','three','four','five'], columns=['number'])\n\n\ndf2 = DataFrame([['a'],['b'],['c'],['d'],['e']], index=['one','too','three','fours','five'], columns=['letter'])\n\n""]";"[""from pandas import DataFrame\ndf1 = DataFrame([[1],[2],[3],[4],[5]], index=['one','two','three','four','five'], columns=['number'])\n\n\ndf2 = DataFrame([['a'],['b'],['c'],['d'],['e']], index=['one','too','three','fours','five'], columns=['letter'])\n\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\ndf1 = DataFrame([[1],[2],[3],[4],[5]], index=['one','two','three','four','five'], columns=['number'])\n\n\ndf2 = DataFrame([['a'],['b'],['c'],['d'],['e']], index=['one','too','three','fours','five'], columns=['letter'])\n\n""]";True;0;1;"[""name 'df2' is not defined""]";['NameError'];0;1;"[""name 'df2' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
157;157;157;157;2.0;0;13651117;;1;39;<pandas>;pandas: filter lines on load in read_csv;17401.0;[''];[];['read_csv'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pandas' is not defined""]";['NameError'];0;1;"[""name 'pandas' is not defined""]";['NameError'];0;1;"[""name 'pandas' is not defined""]";['NameError']
158;158;158;158;1.0;0;13654699;;1;27;<python><datetime><python-2.7><pandas>;Reindexing pandas timeseries from object dtype to datetime dtype;23824.0;"[""In [1]: df = pd.read_csv('data.csv',index_col=0)\nIn [2]: print df['2008-02-27':'2008-03-02']\nOut[2]: \n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-02-30   0\n2008-02-31   0\n2008-03-01   0\n2008-03-02  17\n\nIn [3]: def clean_timestamps(df):\n    # remove invalid dates like '2008-02-30' and '2009-04-31'\n    to_drop = list()\n    for d in df.index:\n        try:\n            datetime.date(int(d[0:4]),int(d[5:7]),int(d[8:10]))\n        except ValueError:\n            to_drop.append(d)\n    df2 = df.drop(to_drop,axis=0)\n    return df2\n\nIn [4]: df2 = clean_timestamps(df)\nIn [5] :print df2['2008-02-27':'2008-03-02']\nOut[5]:\n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-03-01   0\n2008-03-02  17\nIn [6]: df2.index\nOut[6]: Index([2008-01-01, 2008-01-02, 2008-01-03, ..., 2012-11-27, 2012-11-28,\n   2012-11-29], dtype=object)\nIn [7]: i = pd.date_range(start=min(df2.index),end=max(df2.index))\nIn [8]: df3 = df2.reindex(index=i,columns=['count'])\nIn [9]: df3['2008-02-27':'2008-03-02']\nOut[9]: \n            count\n2008-02-27 NaN\n2008-02-28 NaN\n2008-02-29 NaN\n2008-03-01 NaN\n2008-03-02 NaN\nIn [10]: df3 = pd.DataFrame(columns=['count'],index=i)\nIn [11]: values = dict(df2['count'])\nIn [12]: for d in i:\n    try:\n        df3.set_value(index=d,col='count',value=values[d.isoformat()[0:10]])\n    except KeyError:\n        pass\nIn [13]: print df3['2008-02-27':'2008-03-02']\nOut[13]: \n\n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-03-01   0\n2008-03-02  17\n\nIn [14]: df3.index\nOut[14];\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2008-01-01 00:00:00, ..., 2012-11-29 00:00:00]\nLength: 1795, Freq: D, Timezone: None\n""]";"[""In [1]: df = pd.read_csv('data.csv',index_col=0)\nIn [2]: print df['2008-02-27':'2008-03-02']\nOut[2]: \n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-02-30   0\n2008-02-31   0\n2008-03-01   0\n2008-03-02  17\n\nIn [3]: def clean_timestamps(df):\n    # remove invalid dates like '2008-02-30' and '2009-04-31'\n    to_drop = list()\n    for d in df.index:\n        try:\n            datetime.date(int(d[0:4]),int(d[5:7]),int(d[8:10]))\n        except ValueError:\n            to_drop.append(d)\n    df2 = df.drop(to_drop,axis=0)\n    return df2\n\nIn [4]: df2 = clean_timestamps(df)\nIn [5] :print df2['2008-02-27':'2008-03-02']\nOut[5]:\n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-03-01   0\n2008-03-02  17\n"", 'In [6]: df2.index\nOut[6]: Index([2008-01-01, 2008-01-02, 2008-01-03, ..., 2012-11-27, 2012-11-28,\n   2012-11-29], dtype=object)\n', ""In [7]: i = pd.date_range(start=min(df2.index),end=max(df2.index))\nIn [8]: df3 = df2.reindex(index=i,columns=['count'])\nIn [9]: df3['2008-02-27':'2008-03-02']\nOut[9]: \n            count\n2008-02-27 NaN\n2008-02-28 NaN\n2008-02-29 NaN\n2008-03-01 NaN\n2008-03-02 NaN\n"", ""In [10]: df3 = pd.DataFrame(columns=['count'],index=i)\nIn [11]: values = dict(df2['count'])\nIn [12]: for d in i:\n    try:\n        df3.set_value(index=d,col='count',value=values[d.isoformat()[0:10]])\n    except KeyError:\n        pass\nIn [13]: print df3['2008-02-27':'2008-03-02']\nOut[13]: \n\n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-03-01   0\n2008-03-02  17\n\nIn [14]: df3.index\nOut[14];\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2008-01-01 00:00:00, ..., 2012-11-29 00:00:00]\nLength: 1795, Freq: D, Timezone: None\n""]";"[""In [1]: df = pd.read_csv('data.csv',index_col=0)\nIn [2]: print df['2008-02-27':'2008-03-02']\nOut[2]: \n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-02-30   0\n2008-02-31   0\n2008-03-01   0\n2008-03-02  17\n\nIn [3]: def clean_timestamps(df):\n    # remove invalid dates like '2008-02-30' and '2009-04-31'\n    to_drop = list()\n    for d in df.index:\n        try:\n            datetime.date(int(d[0:4]),int(d[5:7]),int(d[8:10]))\n        except ValueError:\n            to_drop.append(d)\n    df2 = df.drop(to_drop,axis=0)\n    return df2\n\nIn [4]: df2 = clean_timestamps(df)\nIn [5] :print df2['2008-02-27':'2008-03-02']\nOut[5]:\n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-03-01   0\n2008-03-02  17\n"", 'In [6]: df2.index\nOut[6]: Index([2008-01-01, 2008-01-02, 2008-01-03, ..., 2012-11-27, 2012-11-28,\n   2012-11-29], dtype=object)\n', ""In [7]: i = pd.date_range(start=min(df2.index),end=max(df2.index))\nIn [8]: df3 = df2.reindex(index=i,columns=['count'])\nIn [9]: df3['2008-02-27':'2008-03-02']\nOut[9]: \n            count\n2008-02-27 NaN\n2008-02-28 NaN\n2008-02-29 NaN\n2008-03-01 NaN\n2008-03-02 NaN\n"", ""In [10]: df3 = pd.DataFrame(columns=['count'],index=i)\nIn [11]: values = dict(df2['count'])\nIn [12]: for d in i:\n    try:\n        df3.set_value(index=d,col='count',value=values[d.isoformat()[0:10]])\n    except KeyError:\n        pass\nIn [13]: print df3['2008-02-27':'2008-03-02']\nOut[13]: \n\n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-03-01   0\n2008-03-02  17\n\nIn [14]: df3.index\nOut[14];\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2008-01-01 00:00:00, ..., 2012-11-29 00:00:00]\nLength: 1795, Freq: D, Timezone: None\n""]";"[""df = pd.read_csv('data.csv',index_col=0)\n    # remove invalid dates like '2008-02-30' and '2009-04-31'\n\ndf2.index\ni = pd.date_range(start=min(df2.index),end=max(df2.index))\ndf3 = pd.DataFrame(columns=['count'],index=i)\ndf3.index\n""]";"[""import pandas as pd\ndf = pd.read_csv('data.csv',index_col=0)\n    # remove invalid dates like '2008-02-30' and '2009-04-31'\n\ndf2.index\ni = pd.date_range(start=min(df2.index),end=max(df2.index))\ndf3 = pd.DataFrame(columns=['count'],index=i)\ndf3.index\n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\ndf2 = pd.DataFrame()\ndf = pd.read_csv('data.csv',index_col=0)\n    # remove invalid dates like '2008-02-30' and '2009-04-31'\n\ndf2.index\ni = pd.date_range(start=min(df2.index),end=max(df2.index))\ndf3 = pd.DataFrame(columns=['count'],index=i)\ndf3.index\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
159;159;159;159;1.0;0;13659881;;1;23;<pandas>;Count by unique pair of columns in pandas;12957.0;"[""d = pd.DataFrame({'ip': ['192.168.0.1', '192.168.0.1', '192.168.0.1', '192.168.0.2'], 'useragent': ['a', 'a', 'b', 'b']})\n\n     ip              useragent\n0    192.168.0.1     a\n1    192.168.0.1     a\n2    192.168.0.1     b\n3    192.168.0.2     b\nip           useragent  \n192.168.0.1  a           2\n192.168.0.1  b           1\n192.168.0.2  b           1\n""]";"[""d = pd.DataFrame({'ip': ['192.168.0.1', '192.168.0.1', '192.168.0.1', '192.168.0.2'], 'useragent': ['a', 'a', 'b', 'b']})\n\n     ip              useragent\n0    192.168.0.1     a\n1    192.168.0.1     a\n2    192.168.0.1     b\n3    192.168.0.2     b\n"", 'ip           useragent  \n192.168.0.1  a           2\n192.168.0.1  b           1\n192.168.0.2  b           1\n']";"[""d = pd.DataFrame({'ip': ['192.168.0.1', '192.168.0.1', '192.168.0.1', '192.168.0.2'], 'useragent': ['a', 'a', 'b', 'b']})\n\n     ip              useragent\n0    192.168.0.1     a\n1    192.168.0.1     a\n2    192.168.0.1     b\n3    192.168.0.2     b\n"", 'ip           useragent  \n192.168.0.1  a           2\n192.168.0.1  b           1\n192.168.0.2  b           1\n']";"[""d = pd.DataFrame({'ip': ['192.168.0.1', '192.168.0.1', '192.168.0.1', '192.168.0.2'], 'useragent': ['a', 'a', 'b', 'b']})\n\n""]";"[""import pandas as pd\nd = pd.DataFrame({'ip': ['192.168.0.1', '192.168.0.1', '192.168.0.1', '192.168.0.2'], 'useragent': ['a', 'a', 'b', 'b']})\n\n""]";True;"[""import pandas as pd\nd = pd.DataFrame({'ip': ['192.168.0.1', '192.168.0.1', '192.168.0.1', '192.168.0.2'], 'useragent': ['a', 'a', 'b', 'b']})\n\n""]";False;0;1;"[""name 'd' is not defined""]";['NameError'];0;1;"[""name 'd' is not defined""]";['NameError'];0;1;"[""name 'd' is not defined""]";['NameError']
160;160;160;160;6.0;0;13682044;;1;48;<python><dataframe><pandas>;Pandas DataFrame: remove unwanted parts from strings in a column;66744.0;['    time    result\n1    09:00   +52A\n2    10:00   +62B\n3    11:00   +44a\n4    12:00   +30b\n5    13:00   -110a\n    time    result\n1    09:00   52\n2    10:00   62\n3    11:00   44\n4    12:00   30\n5    13:00   110\nTypeError: wrapper() takes exactly 1 argument (2 given)\n'];['    time    result\n1    09:00   +52A\n2    10:00   +62B\n3    11:00   +44a\n4    12:00   +30b\n5    13:00   -110a\n', '    time    result\n1    09:00   52\n2    10:00   62\n3    11:00   44\n4    12:00   30\n5    13:00   110\n', 'TypeError: wrapper() takes exactly 1 argument (2 given)\n'];"['    time    result\n1    09:00   +52A\n2    10:00   +62B\n3    11:00   +44a\n4    12:00   +30b\n5    13:00   -110a\n', '    time    result\n1    09:00   52\n2    10:00   62\n3    11:00   44\n4    12:00   30\n5    13:00   110\n', "".str.lstrip('+-')"", ""str.rstrip('aAbBcC')"", 'TypeError: wrapper() takes exactly 1 argument (2 given)\n']";[''];[''];False;['import pandas as pd\n'];False;0;4;"[""name 'data' is not defined"", ""name 'df' is not defined"", ""name 'data' is not defined"", ""name 'data' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""name 'data' is not defined"", ""name 'df' is not defined"", ""name 'data' is not defined"", ""name 'data' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""'result'"", ""'result'"", ""'result'"", ""'result'""]";['KeyError', 'KeyError', 'KeyError', 'KeyError']
161;161;161;161;10.0;3;13703720;;1;140;<python><datetime><numpy><pandas>;Converting between datetime, Timestamp and datetime64;163489.0;"[""import datetime\nimport numpy as np\nimport pandas as pd\ndt = datetime.datetime(2012, 5, 1)\n# A strange way to extract a Timestamp object, there's surely a better way?\nts = pd.DatetimeIndex([dt])[0]\ndt64 = np.datetime64(dt)\n\nIn [7]: dt\nOut[7]: datetime.datetime(2012, 5, 1, 0, 0)\n\nIn [8]: ts\nOut[8]: <Timestamp: 2012-05-01 00:00:00>\n\nIn [9]: dt64\nOut[9]: numpy.datetime64('2012-05-01T01:00:00.000000+0100')\nIn [10]: ts.to_datetime()\nOut[10]: datetime.datetime(2012, 5, 1, 0, 0)\ndt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100')\n""]";"[""import datetime\nimport numpy as np\nimport pandas as pd\ndt = datetime.datetime(2012, 5, 1)\n# A strange way to extract a Timestamp object, there's surely a better way?\nts = pd.DatetimeIndex([dt])[0]\ndt64 = np.datetime64(dt)\n\nIn [7]: dt\nOut[7]: datetime.datetime(2012, 5, 1, 0, 0)\n\nIn [8]: ts\nOut[8]: <Timestamp: 2012-05-01 00:00:00>\n\nIn [9]: dt64\nOut[9]: numpy.datetime64('2012-05-01T01:00:00.000000+0100')\n"", 'In [10]: ts.to_datetime()\nOut[10]: datetime.datetime(2012, 5, 1, 0, 0)\n', ""dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100')\n""]";"['numpy.datetime64', 'datetime.datetime', 'Timestamp', ""import datetime\nimport numpy as np\nimport pandas as pd\ndt = datetime.datetime(2012, 5, 1)\n# A strange way to extract a Timestamp object, there's surely a better way?\nts = pd.DatetimeIndex([dt])[0]\ndt64 = np.datetime64(dt)\n\nIn [7]: dt\nOut[7]: datetime.datetime(2012, 5, 1, 0, 0)\n\nIn [8]: ts\nOut[8]: <Timestamp: 2012-05-01 00:00:00>\n\nIn [9]: dt64\nOut[9]: numpy.datetime64('2012-05-01T01:00:00.000000+0100')\n"", 'In [10]: ts.to_datetime()\nOut[10]: datetime.datetime(2012, 5, 1, 0, 0)\n', 'datetime', 'Timestamp', 'numpy.datetime64', 'dt64', ""dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100')\n"", 'datetime.datetime(2002, 6, 28, 1, 0)', '1025222400000000000L']";['dt\nts\ndt64\nts.to_datetime()\n'];['dt\nts\ndt64\nts.to_datetime()\n'];False;['import pandas as pd\ndt\nts\ndt64\nts.to_datetime()\n'];False;1;5;"[""name 'dt64' is not defined"", ""name 'datetime' is not defined"", ""name 'datetime' is not defined"", ""name 'Timestamp' is not defined"", 'Sucess']";['NameError', 'NameError', 'NameError', 'NameError', 'Sucess'];1;5;"[""name 'dt64' is not defined"", ""name 'datetime' is not defined"", ""name 'datetime' is not defined"", ""name 'Timestamp' is not defined"", 'Sucess']";['NameError', 'NameError', 'NameError', 'NameError', 'Sucess'];1;5;"[""name 'dt64' is not defined"", ""name 'datetime' is not defined"", ""name 'datetime' is not defined"", ""name 'Timestamp' is not defined"", 'Sucess']";['NameError', 'NameError', 'NameError', 'NameError', 'Sucess']
162;162;162;162;2.0;1;13740672;;1;13;<python><pandas>;in pandas how can I groupby weekday() for a datetime column?;10573.0;"['import datetime\nfrom pandas import *\n\ndata = read_csv(""data.csv"")\ndata.my_dt \n\nOut[52]:\n0     2012-10-01 02:00:39\n1     2012-10-01 02:00:38\n2     2012-10-01 02:01:05\n3     2012-10-01 02:01:07\n4     2012-10-01 02:02:03\n5     2012-10-01 02:02:09\n6     2012-10-01 02:02:03\n7     2012-10-01 02:02:35\n8     2012-10-01 02:02:33\n9     2012-10-01 02:03:01\n10    2012-10-01 02:08:53\n11    2012-10-01 02:09:04\n12    2012-10-01 02:09:09\n13    2012-10-01 02:10:20\n14    2012-10-01 02:10:45\n...\nweekdays_only = data[data.my_dt.weekday() < 5]\nmonday, 0-6, 7-12, 13-18, 19-23\ntuesday, 0-6, 7-12, 13-18, 19-23\n']";"['import datetime\nfrom pandas import *\n\ndata = read_csv(""data.csv"")\ndata.my_dt \n\nOut[52]:\n0     2012-10-01 02:00:39\n1     2012-10-01 02:00:38\n2     2012-10-01 02:01:05\n3     2012-10-01 02:01:07\n4     2012-10-01 02:02:03\n5     2012-10-01 02:02:09\n6     2012-10-01 02:02:03\n7     2012-10-01 02:02:35\n8     2012-10-01 02:02:33\n9     2012-10-01 02:03:01\n10    2012-10-01 02:08:53\n11    2012-10-01 02:09:04\n12    2012-10-01 02:09:09\n13    2012-10-01 02:10:20\n14    2012-10-01 02:10:45\n...\n', 'weekdays_only = data[data.my_dt.weekday() < 5]\n', 'monday, 0-6, 7-12, 13-18, 19-23\ntuesday, 0-6, 7-12, 13-18, 19-23\n']";"['import datetime\nfrom pandas import *\n\ndata = read_csv(""data.csv"")\ndata.my_dt \n\nOut[52]:\n0     2012-10-01 02:00:39\n1     2012-10-01 02:00:38\n2     2012-10-01 02:01:05\n3     2012-10-01 02:01:07\n4     2012-10-01 02:02:03\n5     2012-10-01 02:02:09\n6     2012-10-01 02:02:03\n7     2012-10-01 02:02:35\n8     2012-10-01 02:02:33\n9     2012-10-01 02:03:01\n10    2012-10-01 02:08:53\n11    2012-10-01 02:09:04\n12    2012-10-01 02:09:09\n13    2012-10-01 02:10:20\n14    2012-10-01 02:10:45\n...\n', 'weekdays_only = data[data.my_dt.weekday() < 5]\n', 'monday, 0-6, 7-12, 13-18, 19-23\ntuesday, 0-6, 7-12, 13-18, 19-23\n']";"['import datetime\nfrom pandas import *\n\ndata = read_csv(""data.csv"")\ndata.my_dt \n\n...\nweekdays_only = data[data.my_dt.weekday() < 5]\nmonday, 0-6, 7-12, 13-18, 19-23\ntuesday, 0-6, 7-12, 13-18, 19-23\n']";"['import datetime\nfrom pandas import *\n\ndata = read_csv(""data.csv"")\ndata.my_dt \n\n...\nweekdays_only = data[data.my_dt.weekday() < 5]\nmonday, 0-6, 7-12, 13-18, 19-23\ntuesday, 0-6, 7-12, 13-18, 19-23\n']";False;"['import pandas as pd\nimport datetime\nfrom pandas import *\n\ndata = read_csv(""data.csv"")\ndata.my_dt \n\n...\nweekdays_only = data[data.my_dt.weekday() < 5]\nmonday, 0-6, 7-12, 13-18, 19-23\ntuesday, 0-6, 7-12, 13-18, 19-23\n']";False;0;2;"[""name 'data' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'data' is not defined"", ""name 'data' is not defined""]";['NameError', 'NameError'];0;2;"[""'my_dt'"", ""'my_dt'""]";['KeyError', 'KeyError']
163;163;163;163;2.0;0;13757090;;1;16;<string><pandas>;Pandas column access w/column names containing spaces;12685.0;"[""df1 = DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],\n                 'data1': range(7)})\n\ndf1.data1\ndf2 = DataFrame({'key': ['a','b','d'],\n                 'data 2': range(3)})\n\ndf2.data 2      # <--- not the droid i'm looking for.\ndf2.xs('data 2', axis=1)\n""]";"[""df1 = DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],\n                 'data1': range(7)})\n\ndf1.data1\n"", ""df2 = DataFrame({'key': ['a','b','d'],\n                 'data 2': range(3)})\n\ndf2.data 2      # <--- not the droid i'm looking for.\n"", ""df2.xs('data 2', axis=1)\n""]";"[""df1 = DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],\n                 'data1': range(7)})\n\ndf1.data1\n"", ""df2 = DataFrame({'key': ['a','b','d'],\n                 'data 2': range(3)})\n\ndf2.data 2      # <--- not the droid i'm looking for.\n"", ""df2.xs('data 2', axis=1)\n""]";"[""\ndf1.data1\n\ndf2.xs('data 2', axis=1)\n""]";"[""\ndf1.data1\n\ndf2.xs('data 2', axis=1)\n""]";False;"[""import pandas as pd\ndata1 = pd.DataFrame()\ndata = pd.DataFrame()\ndf2 = pd.DataFrame()\ndf1 = pd.DataFrame()\ndf = pd.DataFrame()\n\ndf1.data1\n\ndf2.xs('data 2', axis=1)\n""]";True;0;2;"[""name 'df1' is not defined"", ""name 'df1' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df1' is not defined"", ""name 'df1' is not defined""]";['NameError', 'NameError'];1;2;"[""'dat a1'"", 'Sucess']";['KeyError', 'Sucess']
164;164;164;164;2.0;0;13784192;;1;131;<python><dataframe><pandas>;Creating an empty Pandas DataFrame, then filling it?;326681.0;"[""import datetime as dt\nimport pandas as pd\nimport scipy as s\n\nif __name__ == '__main__':\n    base = dt.datetime.today().date()\n    dates = [ base - dt.timedelta(days=x) for x in range(0,10) ]\n    dates.sort()\n\n    valdict = {}\n    symbols = ['A','B', 'C']\n    for symb in symbols:\n        valdict[symb] = pd.Series( s.zeros( len(dates)), dates )\n\n    for thedate in dates:\n        if thedate > dates[0]:\n            for symb in valdict:\n                valdict[symb][thedate] = 1+valdict[symb][thedate - dt.timedelta(days=1)]\n\n    print valdict\n""]";"[""import datetime as dt\nimport pandas as pd\nimport scipy as s\n\nif __name__ == '__main__':\n    base = dt.datetime.today().date()\n    dates = [ base - dt.timedelta(days=x) for x in range(0,10) ]\n    dates.sort()\n\n    valdict = {}\n    symbols = ['A','B', 'C']\n    for symb in symbols:\n        valdict[symb] = pd.Series( s.zeros( len(dates)), dates )\n\n    for thedate in dates:\n        if thedate > dates[0]:\n            for symb in valdict:\n                valdict[symb][thedate] = 1+valdict[symb][thedate - dt.timedelta(days=1)]\n\n    print valdict\n""]";"[""import datetime as dt\nimport pandas as pd\nimport scipy as s\n\nif __name__ == '__main__':\n    base = dt.datetime.today().date()\n    dates = [ base - dt.timedelta(days=x) for x in range(0,10) ]\n    dates.sort()\n\n    valdict = {}\n    symbols = ['A','B', 'C']\n    for symb in symbols:\n        valdict[symb] = pd.Series( s.zeros( len(dates)), dates )\n\n    for thedate in dates:\n        if thedate > dates[0]:\n            for symb in valdict:\n                valdict[symb][thedate] = 1+valdict[symb][thedate - dt.timedelta(days=1)]\n\n    print valdict\n""]";['import datetime as dt\nimport pandas as pd\nimport scipy as s\n\n\n\n\n'];['import datetime as dt\nimport pandas as pd\nimport scipy as s\n\n\n\n\n'];False;['import pandas as pd\nimport datetime as dt\nimport pandas as pd\nimport scipy as s\n\n\n\n\n'];False;0;2;"[""name 'pd' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'data' is not defined"", ""name 'oldDF' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'index' is not defined"", ""name 'oldDF' is not defined""]";['NameError', 'NameError']
165;165;165;165;4.0;4;13785932;;1;12;<date><datetime><numpy><pandas><date-format>;How to round a Pandas `DatetimeIndex`?;8474.0;"[""pd.date_range('2012-1-1 02:03:04.000',periods=3,freq='1ms')\n>>> [2012-01-01 02:03:04, ..., 2012-01-01 02:03:04.002000]\n[2012-01-01 02:03:04.000000, ..., 2012-01-01 02:03:04.000000]\nnp.array(['2012-01-02 00:00:00.001'],dtype='datetime64[ns]')\n""]";"[""pd.date_range('2012-1-1 02:03:04.000',periods=3,freq='1ms')\n>>> [2012-01-01 02:03:04, ..., 2012-01-01 02:03:04.002000]\n"", '[2012-01-01 02:03:04.000000, ..., 2012-01-01 02:03:04.000000]\n', ""np.array(['2012-01-02 00:00:00.001'],dtype='datetime64[ns]')\n""]";"['pandas.DatetimeIndex', ""pd.date_range('2012-1-1 02:03:04.000',periods=3,freq='1ms')\n>>> [2012-01-01 02:03:04, ..., 2012-01-01 02:03:04.002000]\n"", 'Timestamp', '[2012-01-01 02:03:04.000000, ..., 2012-01-01 02:03:04.000000]\n', 'datetime64[ns]', 'dtype', '[ns]', ""np.array(['2012-01-02 00:00:00.001'],dtype='datetime64[ns]')\n""]";"[""pd.date_range('2012-1-1 02:03:04.000',periods=3,freq='1ms')\nnp.array(['2012-01-02 00:00:00.001'],dtype='datetime64[ns]')\n""]";"[""import pandas as pd\npd.date_range('2012-1-1 02:03:04.000',periods=3,freq='1ms')\nnp.array(['2012-01-02 00:00:00.001'],dtype='datetime64[ns]')\n""]";True;"[""import pandas as pd\npd.date_range('2012-1-1 02:03:04.000',periods=3,freq='1ms')\nnp.array(['2012-01-02 00:00:00.001'],dtype='datetime64[ns]')\n""]";False;1;2;"[""name 't1' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 't1' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 't1' is not defined"", 'Sucess']";['NameError', 'Sucess']
166;166;166;166;3.0;1;13838405;;1;25;<python><pandas>;Custom sorting in pandas dataframe;17015.0;"[""custom_dict = {'March':0, 'April':1, 'Dec':3}  \n""]";"[""custom_dict = {'March':0, 'April':1, 'Dec':3}  \n""]";"[""custom_dict = {'March':0, 'April':1, 'Dec':3}  \n""]";"[""custom_dict = {'March':0, 'April':1, 'Dec':3}  \n""]";"[""custom_dict = {'March':0, 'April':1, 'Dec':3}  \n""]";False;"[""import pandas as pd\ncustom_dict = {'March':0, 'April':1, 'Dec':3}  \n""]";False;0;2;"[""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'m'"", ""name 'sort_by_custom_dict' is not defined""]";['KeyError', 'NameError']
167;167;167;167;8.0;3;13842088;;1;136;<python><pandas>;Set value for particular cell in pandas DataFrame;182280.0;"[""df=DataFrame(index=['A','B','C'], columns=['x','y'])\n\n    x    y\nA  NaN  NaN\nB  NaN  NaN\nC  NaN  NaN\n\n    x    y\nA  NaN  NaN\nB  NaN  NaN\nC  10  NaN\ndf.xs('C')['x']=10\n""]";"[""df=DataFrame(index=['A','B','C'], columns=['x','y'])\n"", '\n    x    y\nA  NaN  NaN\nB  NaN  NaN\nC  NaN  NaN\n', '\n    x    y\nA  NaN  NaN\nB  NaN  NaN\nC  10  NaN\n', ""df.xs('C')['x']=10\n""]";"[""df=DataFrame(index=['A','B','C'], columns=['x','y'])\n"", ""df.xs('C')['x']=10\n""]";"[""df=DataFrame(index=['A','B','C'], columns=['x','y'])\n\n\ndf.xs('C')['x']=10\n""]";"[""df=DataFrame(index=['A','B','C'], columns=['x','y'])\n\n\ndf.xs('C')['x']=10\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf=DataFrame(index=['A','B','C'], columns=['x','y'])\n\n\ndf.xs('C')['x']=10\n""]";True;3;5;"[""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess', 'Sucess', 'Sucess']";['NameError', 'NameError', 'Sucess', 'Sucess', 'Sucess'];3;5;"[""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess', 'Sucess', 'Sucess']";['NameError', 'NameError', 'Sucess', 'Sucess', 'Sucess'];3;5;"[""'C'"", '\n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated', 'Sucess', 'Sucess', 'Sucess']";['KeyError', 'DeprecationWarning', 'Sucess', 'Sucess', 'Sucess']
168;168;168;168;3.0;0;13851535;;1;48;<python><pandas>;How to delete rows from a pandas DataFrame based on a conditional expression;79345.0;"[""df[(len(df['column name']) < 2)]\nKeyError: u'no item named False'\n""]";"[""df[(len(df['column name']) < 2)]\n"", ""KeyError: u'no item named False'\n""]";"['df.dropna()', 'NaN', ""df[(len(df['column name']) < 2)]\n"", ""KeyError: u'no item named False'\n""]";"[""df[(len(df['column name']) < 2)]\n""]";"[""df[(len(df['column name']) < 2)]\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf[(len(df['column name']) < 2)]\n""]";True;1;3;"[""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""'column name'"", 'Sucess', ""'score'""]";['KeyError', 'Sucess', 'KeyError']
169;169;169;169;1.0;0;13854476;;1;14;<python><aggregate><pandas>;pandas' transform doesn't work sorting groupby output;4246.0;"[""In [119]:\n\ntips.head()\nOut[119]:\ntotal_bill  tip      sex     smoker    day   time    size  tip_pct\n0    16.99   1.01    Female  False   Sun     Dinner  2   0.059447\n1    10.34   1.66    Male    False   Sun     Dinner  3   0.160542\n2    21.01   3.50    Male    False   Sun     Dinner  3   0.166587\n3    23.68   3.31    Male    False   Sun     Dinner  2   0.139780\n4    24.59   3.61    Female  False   Sun     Dinner  4   0.146808\ndef top(df, n=5, column='tip_pct'): \n    return df.sort_index(by=column)[-n:]\n\nIn [101]:\n\ntips.groupby('smoker').apply(top)\nOut[101]:\n           total_bill   tip sex smoker  day time    size    tip_pct\nsmoker                                  \nFalse   88   24.71   5.85    Male    False   Thur    Lunch   2   0.236746\n185  20.69   5.00    Male    False   Sun     Dinner  5   0.241663\n51   10.29   2.60    Female  False   Sun     Dinner  2   0.252672\n149  7.51    2.00    Male    False   Thur    Lunch   2   0.266312\n232  11.61   3.39    Male    False   Sat     Dinner  2   0.291990\n\nTrue    109  14.31   4.00    Female  True    Sat     Dinner  2   0.279525\n183  23.17   6.50    Male    True    Sun     Dinner  4   0.280535\n67   3.07    1.00    Female  True    Sat     Dinner  1   0.325733\n178  9.60    4.00    Female  True    Sun     Dinner  2   0.416667\n172  7.25    5.15    Male    True    Sun     Dinner  2   0.710345\ndef top_all(df):\n    return df.sort_index(by='tip_pct')\n\ntips.groupby('smoker').transform(top_all)\nTypeError: Transform function invalid for data types\n""]";"['In [119]:\n\ntips.head()\nOut[119]:\ntotal_bill  tip      sex     smoker    day   time    size  tip_pct\n0    16.99   1.01    Female  False   Sun     Dinner  2   0.059447\n1    10.34   1.66    Male    False   Sun     Dinner  3   0.160542\n2    21.01   3.50    Male    False   Sun     Dinner  3   0.166587\n3    23.68   3.31    Male    False   Sun     Dinner  2   0.139780\n4    24.59   3.61    Female  False   Sun     Dinner  4   0.146808\n', ""def top(df, n=5, column='tip_pct'): \n    return df.sort_index(by=column)[-n:]\n\nIn [101]:\n\ntips.groupby('smoker').apply(top)\nOut[101]:\n           total_bill   tip sex smoker  day time    size    tip_pct\nsmoker                                  \nFalse   88   24.71   5.85    Male    False   Thur    Lunch   2   0.236746\n185  20.69   5.00    Male    False   Sun     Dinner  5   0.241663\n51   10.29   2.60    Female  False   Sun     Dinner  2   0.252672\n149  7.51    2.00    Male    False   Thur    Lunch   2   0.266312\n232  11.61   3.39    Male    False   Sat     Dinner  2   0.291990\n\nTrue    109  14.31   4.00    Female  True    Sat     Dinner  2   0.279525\n183  23.17   6.50    Male    True    Sun     Dinner  4   0.280535\n67   3.07    1.00    Female  True    Sat     Dinner  1   0.325733\n178  9.60    4.00    Female  True    Sun     Dinner  2   0.416667\n172  7.25    5.15    Male    True    Sun     Dinner  2   0.710345\n"", ""def top_all(df):\n    return df.sort_index(by='tip_pct')\n\ntips.groupby('smoker').transform(top_all)\n"", 'TypeError: Transform function invalid for data types\n']";"['In [119]:\n\ntips.head()\nOut[119]:\ntotal_bill  tip      sex     smoker    day   time    size  tip_pct\n0    16.99   1.01    Female  False   Sun     Dinner  2   0.059447\n1    10.34   1.66    Male    False   Sun     Dinner  3   0.160542\n2    21.01   3.50    Male    False   Sun     Dinner  3   0.166587\n3    23.68   3.31    Male    False   Sun     Dinner  2   0.139780\n4    24.59   3.61    Female  False   Sun     Dinner  4   0.146808\n', 'tip_pct', ""def top(df, n=5, column='tip_pct'): \n    return df.sort_index(by=column)[-n:]\n\nIn [101]:\n\ntips.groupby('smoker').apply(top)\nOut[101]:\n           total_bill   tip sex smoker  day time    size    tip_pct\nsmoker                                  \nFalse   88   24.71   5.85    Male    False   Thur    Lunch   2   0.236746\n185  20.69   5.00    Male    False   Sun     Dinner  5   0.241663\n51   10.29   2.60    Female  False   Sun     Dinner  2   0.252672\n149  7.51    2.00    Male    False   Thur    Lunch   2   0.266312\n232  11.61   3.39    Male    False   Sat     Dinner  2   0.291990\n\nTrue    109  14.31   4.00    Female  True    Sat     Dinner  2   0.279525\n183  23.17   6.50    Male    True    Sun     Dinner  4   0.280535\n67   3.07    1.00    Female  True    Sat     Dinner  1   0.325733\n178  9.60    4.00    Female  True    Sun     Dinner  2   0.416667\n172  7.25    5.15    Male    True    Sun     Dinner  2   0.710345\n"", ""def top_all(df):\n    return df.sort_index(by='tip_pct')\n\ntips.groupby('smoker').transform(top_all)\n"", 'TypeError: Transform function invalid for data types\n']";"[""tips.head()\ntips.groupby('smoker').apply(top)\n""]";"[""tips.head()\ntips.groupby('smoker').apply(top)\n""]";False;"[""import pandas as pd\ntips.head()\ntips.groupby('smoker').apply(top)\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
170;170;170;170;3.0;0;13872533;;1;31;<python><matplotlib><pandas>;Plot different DataFrames in the same figure;31002.0;['2012-04-12,16:13:09,20.6\n2012-04-12,17:13:09,20.9\n2012-04-12,18:13:09,20.6\n2007-05-12,19:13:09,5.4\n2007-05-12,20:13:09,20.6\n2007-05-12,20:13:09,20.6\n2005-08-11,11:13:09,20.6\n2005-08-11,11:13:09,17.5\n2005-08-13,07:13:09,20.6\n2006-04-13,01:13:09,20.6\n'];['2012-04-12,16:13:09,20.6\n2012-04-12,17:13:09,20.9\n2012-04-12,18:13:09,20.6\n2007-05-12,19:13:09,5.4\n2007-05-12,20:13:09,20.6\n2007-05-12,20:13:09,20.6\n2005-08-11,11:13:09,20.6\n2005-08-11,11:13:09,17.5\n2005-08-13,07:13:09,20.6\n2006-04-13,01:13:09,20.6\n'];['2012-04-12,16:13:09,20.6\n2012-04-12,17:13:09,20.9\n2012-04-12,18:13:09,20.6\n2007-05-12,19:13:09,5.4\n2007-05-12,20:13:09,20.6\n2007-05-12,20:13:09,20.6\n2005-08-11,11:13:09,20.6\n2005-08-11,11:13:09,17.5\n2005-08-13,07:13:09,20.6\n2006-04-13,01:13:09,20.6\n'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df1' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df1' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name '_converter' is not defined"", ""'Month'""]";['NameError', 'KeyError']
171;171;171;171;2.0;3;13888468;;1;30;<pandas>;Get unique values from index column in MultiIndex;21242.0;"[""        C\n A B     \n 0 one  3\n 1 one  2\n 2 two  1\ndf = df.reset_index()\nuniq_b = df.B.unique()\ndf = df.set_index(['A','B'])\n""]";"['        C\n A B     \n 0 one  3\n 1 one  2\n 2 two  1\n', ""df = df.reset_index()\nuniq_b = df.B.unique()\ndf = df.set_index(['A','B'])\n""]";"['DataFrame', '        C\n A B     \n 0 one  3\n 1 one  2\n 2 two  1\n', ""df = df.reset_index()\nuniq_b = df.B.unique()\ndf = df.set_index(['A','B'])\n""]";"[""df = df.reset_index()\nuniq_b = df.B.unique()\ndf = df.set_index(['A','B'])\n""]";"[""df = df.reset_index()\nuniq_b = df.B.unique()\ndf = df.set_index(['A','B'])\n""]";False;"[""import pandas as pd\ndf = df.reset_index()\nuniq_b = df.B.unique()\ndf = df.set_index(['A','B'])\n""]";False;0;2;"[""name 'df' is not defined"", ""name 'large' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'large' is not defined""]";['NameError', 'NameError'];0;2;"[""'Index' object has no attribute 'levels'"", ""name 'large' is not defined""]";['AttributeError', 'NameError']
172;172;172;172;1.0;0;13893227;;1;20;<python><pandas><vectorization>;Vectorized look-up of values in Pandas dataframe;12339.0;['              AAPL    GOOG     IBM    XOM\n2011-01-10  339.44  614.21  142.78  71.57\n2011-01-13  342.64  616.69  143.92  73.08\n2011-01-26  340.82  616.50  155.74  75.89\n2011-02-02  341.29  612.00  157.93  79.46\n2011-02-10  351.42  616.44  159.32  79.68\n2011-03-03  356.40  609.56  158.73  82.19\n2011-05-03  345.14  533.89  167.84  82.00\n2011-06-03  340.42  523.08  160.97  78.19\n2011-06-10  323.03  509.51  159.14  76.84\n2011-08-01  393.26  606.77  176.28  76.67\n2011-12-20  392.46  630.37  184.14  79.97\n           direction  size ticker  prices\n2011-01-10       Buy  1500   AAPL  339.44\n2011-01-13      Sell  1500   AAPL  342.64\n2011-01-13       Buy  4000    IBM  143.92\n2011-01-26       Buy  1000   GOOG  616.50\n2011-02-02      Sell  4000    XOM   79.46\n2011-02-10       Buy  4000    XOM   79.68\n2011-03-03      Sell  1000   GOOG  609.56\n2011-03-03      Sell  2200    IBM  158.73\n2011-06-03      Sell  3300    IBM  160.97\n2011-05-03       Buy  1500    IBM  167.84\n2011-06-10       Buy  1200   AAPL  323.03\n2011-08-01       Buy    55   GOOG  606.77\n2011-08-01      Sell    55   GOOG  606.77\n2011-12-20      Sell  1200   AAPL  392.46\n'];['              AAPL    GOOG     IBM    XOM\n2011-01-10  339.44  614.21  142.78  71.57\n2011-01-13  342.64  616.69  143.92  73.08\n2011-01-26  340.82  616.50  155.74  75.89\n2011-02-02  341.29  612.00  157.93  79.46\n2011-02-10  351.42  616.44  159.32  79.68\n2011-03-03  356.40  609.56  158.73  82.19\n2011-05-03  345.14  533.89  167.84  82.00\n2011-06-03  340.42  523.08  160.97  78.19\n2011-06-10  323.03  509.51  159.14  76.84\n2011-08-01  393.26  606.77  176.28  76.67\n2011-12-20  392.46  630.37  184.14  79.97\n', '           direction  size ticker  prices\n2011-01-10       Buy  1500   AAPL  339.44\n2011-01-13      Sell  1500   AAPL  342.64\n2011-01-13       Buy  4000    IBM  143.92\n2011-01-26       Buy  1000   GOOG  616.50\n2011-02-02      Sell  4000    XOM   79.46\n2011-02-10       Buy  4000    XOM   79.68\n2011-03-03      Sell  1000   GOOG  609.56\n2011-03-03      Sell  2200    IBM  158.73\n2011-06-03      Sell  3300    IBM  160.97\n2011-05-03       Buy  1500    IBM  167.84\n2011-06-10       Buy  1200   AAPL  323.03\n2011-08-01       Buy    55   GOOG  606.77\n2011-08-01      Sell    55   GOOG  606.77\n2011-12-20      Sell  1200   AAPL  392.46\n'];['              AAPL    GOOG     IBM    XOM\n2011-01-10  339.44  614.21  142.78  71.57\n2011-01-13  342.64  616.69  143.92  73.08\n2011-01-26  340.82  616.50  155.74  75.89\n2011-02-02  341.29  612.00  157.93  79.46\n2011-02-10  351.42  616.44  159.32  79.68\n2011-03-03  356.40  609.56  158.73  82.19\n2011-05-03  345.14  533.89  167.84  82.00\n2011-06-03  340.42  523.08  160.97  78.19\n2011-06-10  323.03  509.51  159.14  76.84\n2011-08-01  393.26  606.77  176.28  76.67\n2011-12-20  392.46  630.37  184.14  79.97\n', '           direction  size ticker  prices\n2011-01-10       Buy  1500   AAPL  339.44\n2011-01-13      Sell  1500   AAPL  342.64\n2011-01-13       Buy  4000    IBM  143.92\n2011-01-26       Buy  1000   GOOG  616.50\n2011-02-02      Sell  4000    XOM   79.46\n2011-02-10       Buy  4000    XOM   79.68\n2011-03-03      Sell  1000   GOOG  609.56\n2011-03-03      Sell  2200    IBM  158.73\n2011-06-03      Sell  3300    IBM  160.97\n2011-05-03       Buy  1500    IBM  167.84\n2011-06-10       Buy  1200   AAPL  323.03\n2011-08-01       Buy    55   GOOG  606.77\n2011-08-01      Sell    55   GOOG  606.77\n2011-12-20      Sell  1200   AAPL  392.46\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'prices' is not defined""]";['NameError'];0;1;"[""name 'prices' is not defined""]";['NameError'];0;1;"[""name 'prices' is not defined""]";['NameError']
173;173;173;173;2.0;0;13921647;;1;43;<python><r><pandas>;Python - Dimension of Data Frame;25371.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
174;174;174;174;2.0;0;13926089;;1;14;<python><pandas><hdfs>;Selecting columns from pandas.HDFStore table;13798.0;"[""tmp = pd.HDFStore('test.h5')\nchunker = pd.read_csv('cars.csv', iterator=True, chunksize=10, names=['make','model','drop'])\ntmp.append('df', pd.concat([chunk for chunk in chunker], ignore_index=True))\nIn [97]: tmp\nOut[97]:\n<class 'pandas.io.pytables.HDFStore'>\nFile path: test.h5\n/df     frame_table (typ->appendable,nrows->1930,indexers->[index])\n""]";"[""tmp = pd.HDFStore('test.h5')\nchunker = pd.read_csv('cars.csv', iterator=True, chunksize=10, names=['make','model','drop'])\ntmp.append('df', pd.concat([chunk for chunk in chunker], ignore_index=True))\n"", ""In [97]: tmp\nOut[97]:\n<class 'pandas.io.pytables.HDFStore'>\nFile path: test.h5\n/df     frame_table (typ->appendable,nrows->1930,indexers->[index])\n""]";"[""tmp = pd.HDFStore('test.h5')\nchunker = pd.read_csv('cars.csv', iterator=True, chunksize=10, names=['make','model','drop'])\ntmp.append('df', pd.concat([chunk for chunk in chunker], ignore_index=True))\n"", ""In [97]: tmp\nOut[97]:\n<class 'pandas.io.pytables.HDFStore'>\nFile path: test.h5\n/df     frame_table (typ->appendable,nrows->1930,indexers->[index])\n"", ""tmp['df']"", 'select()', 'Term']";['tmp\n'];['tmp\n'];False;['import pandas as pd\ntmp\n'];False;0;2;"[""name 'store' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'store' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;"[""name 'store' is not defined"", 'Sucess']";['NameError', 'Sucess']
175;175;175;175;1.0;0;13937022;;1;11;<python><pandas>;Using pandas to select rows using two different columns from dataframe?;14815.0;"[""import numpy as np\nfrom pandas import *\n\n\nd = {'one' : [1., 2., 3., 4] ,'two' : [5., 6., 7., 8.],'three' : [9., 16., 17., 18.]}\n\ndf = DataFrame(d)\nprint df\n\ncheckList = [1,7]\n\nprint df[df.one == 1 ]#1\nprint df[df.one == 7 ]#2\nprint df[df.two == 1 ]#3\nprint df[df.two == 7 ]#4\n\n#print df[df.one == 1 or df.two ==7]\nprint df[df.one.isin(checkList)]\n""]";"[""import numpy as np\nfrom pandas import *\n\n\nd = {'one' : [1., 2., 3., 4] ,'two' : [5., 6., 7., 8.],'three' : [9., 16., 17., 18.]}\n\ndf = DataFrame(d)\nprint df\n\ncheckList = [1,7]\n\nprint df[df.one == 1 ]#1\nprint df[df.one == 7 ]#2\nprint df[df.two == 1 ]#3\nprint df[df.two == 7 ]#4\n\n#print df[df.one == 1 or df.two ==7]\nprint df[df.one.isin(checkList)]\n""]";"[""import numpy as np\nfrom pandas import *\n\n\nd = {'one' : [1., 2., 3., 4] ,'two' : [5., 6., 7., 8.],'three' : [9., 16., 17., 18.]}\n\ndf = DataFrame(d)\nprint df\n\ncheckList = [1,7]\n\nprint df[df.one == 1 ]#1\nprint df[df.one == 7 ]#2\nprint df[df.two == 1 ]#3\nprint df[df.two == 7 ]#4\n\n#print df[df.one == 1 or df.two ==7]\nprint df[df.one.isin(checkList)]\n""]";"[""import numpy as np\nfrom pandas import *\n\n\nd = {'one' : [1., 2., 3., 4] ,'two' : [5., 6., 7., 8.],'three' : [9., 16., 17., 18.]}\n\ndf = DataFrame(d)\n\ncheckList = [1,7]\n\n\n#print df[df.one == 1 or df.two ==7]\n""]";"[""from pandas import DataFrame\nimport numpy as np\nfrom pandas import *\n\n\nd = {'one' : [1., 2., 3., 4] ,'two' : [5., 6., 7., 8.],'three' : [9., 16., 17., 18.]}\n\ndf = DataFrame(d)\n\ncheckList = [1,7]\n\n\n#print df[df.one == 1 or df.two ==7]\n""]";True;"[""import pandas as pd\nimport numpy as np\nfrom pandas import *\n\n\nd = {'one' : [1., 2., 3., 4] ,'two' : [5., 6., 7., 8.],'three' : [9., 16., 17., 18.]}\n\ndf = DataFrame(d)\n\ncheckList = [1,7]\n\n\n#print df[df.one == 1 or df.two ==7]\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'DataFrame' object has no attribute 'one'""]";['AttributeError']
176;176;176;176;1.0;2;13938704;;1;11;<python><pandas>;Apply function on Pandas dataframe;1837.0;['      A  B  C  D\n   0  8  3  5  8\n   1  9  4  0  4\n   2  5  4  3  8\n   3  4  8  5  1\n      A     B     C    D\n   0  8/9  3/8  5/5  8/8\n   1  9/9  4/8  0/5  4/8\n   2  5/9  4/8  3/5  8/8\n   3  4/9  8/8  5/5  1/8\n'];['      A  B  C  D\n   0  8  3  5  8\n   1  9  4  0  4\n   2  5  4  3  8\n   3  4  8  5  1\n', '      A     B     C    D\n   0  8/9  3/8  5/5  8/8\n   1  9/9  4/8  0/5  4/8\n   2  5/9  4/8  3/5  8/8\n   3  4/9  8/8  5/5  1/8\n'];['      A  B  C  D\n   0  8  3  5  8\n   1  9  4  0  4\n   2  5  4  3  8\n   3  4  8  5  1\n', '      A     B     C    D\n   0  8/9  3/8  5/5  8/8\n   1  9/9  4/8  0/5  4/8\n   2  5/9  4/8  3/5  8/8\n   3  4/9  8/8  5/5  1/8\n'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
177;177;177;177;3.0;4;13996302;;1;15;<python><pandas>;Python - rolling functions for GroupBy object;10359.0;"[""x = range(0, 6)\nid = ['a', 'a', 'a', 'b', 'b', 'b']\ndf = DataFrame(zip(id, x), columns = ['id', 'x'])\ndf.groupby('id').sum()\nid    x\na    3\nb   12\n  id  x\n0  a  0\n1  a  1\n2  a  3\n3  b  3\n4  b  7\n5  b  12\n""]";"[""x = range(0, 6)\nid = ['a', 'a', 'a', 'b', 'b', 'b']\ndf = DataFrame(zip(id, x), columns = ['id', 'x'])\ndf.groupby('id').sum()\nid    x\na    3\nb   12\n"", '  id  x\n0  a  0\n1  a  1\n2  a  3\n3  b  3\n4  b  7\n5  b  12\n']";"['grouped', '<pandas.core.groupby.SeriesGroupBy object at 0x03F1A9F0>', 'grouped.sum()', 'groupby', 'groupby', ""x = range(0, 6)\nid = ['a', 'a', 'a', 'b', 'b', 'b']\ndf = DataFrame(zip(id, x), columns = ['id', 'x'])\ndf.groupby('id').sum()\nid    x\na    3\nb   12\n"", '  id  x\n0  a  0\n1  a  1\n2  a  3\n3  b  3\n4  b  7\n5  b  12\n']";"[""x = range(0, 6)\nid = ['a', 'a', 'a', 'b', 'b', 'b']\ndf = DataFrame(zip(id, x), columns = ['id', 'x'])\ndf.groupby('id').sum()\n""]";"[""from pandas import DataFrame\nx = range(0, 6)\nid = ['a', 'a', 'a', 'b', 'b', 'b']\ndf = DataFrame(zip(id, x), columns = ['id', 'x'])\ndf.groupby('id').sum()\n""]";True;"[""import pandas as pd\nx = range(0, 6)\nid = ['a', 'a', 'a', 'b', 'b', 'b']\ndf = DataFrame(zip(id, x), columns = ['id', 'x'])\ndf.groupby('id').sum()\n""]";False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'id'"", ""'id'""]";['KeyError', 'KeyError']
178;178;178;178;2.0;0;13999850;;1;28;<python><pandas>;How to specify date format when using pandas.to_csv?;23394.0;['12/14/2012  12:00:00 AM\n20121214\n20121214,  084530\n'];['12/14/2012  12:00:00 AM\n', '20121214\n', '20121214,  084530\n'];['to_csv()', '12/14/2012  12:00:00 AM\n', '20121214\n', '20121214,  084530\n'];['20121214\n'];['20121214\n'];False;['import pandas as pd\n20121214\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'datetime'"", ""name 'filename' is not defined""]";['KeyError', 'NameError']
179;179;179;179;2.0;2;14016247;;1;49;<python><pandas>;Python - find integer index of rows with NaN in pandas;59715.0;['                    a         b\n2011-01-01 00:00:00 1.883381  -0.416629\n2011-01-01 01:00:00 0.149948  -1.782170\n2011-01-01 02:00:00 -0.407604 0.314168\n2011-01-01 03:00:00 1.452354  NaN\n2011-01-01 04:00:00 -1.224869 -0.947457\n2011-01-01 05:00:00 0.498326  0.070416\n2011-01-01 06:00:00 0.401665  NaN\n2011-01-01 07:00:00 -0.019766 0.533641\n2011-01-01 08:00:00 -1.101303 -1.408561\n2011-01-01 09:00:00 1.671795  -0.764629\n'];['                    a         b\n2011-01-01 00:00:00 1.883381  -0.416629\n2011-01-01 01:00:00 0.149948  -1.782170\n2011-01-01 02:00:00 -0.407604 0.314168\n2011-01-01 03:00:00 1.452354  NaN\n2011-01-01 04:00:00 -1.224869 -0.947457\n2011-01-01 05:00:00 0.498326  0.070416\n2011-01-01 06:00:00 0.401665  NaN\n2011-01-01 07:00:00 -0.019766 0.533641\n2011-01-01 08:00:00 -1.101303 -1.408561\n2011-01-01 09:00:00 1.671795  -0.764629\n'];['                    a         b\n2011-01-01 00:00:00 1.883381  -0.416629\n2011-01-01 01:00:00 0.149948  -1.782170\n2011-01-01 02:00:00 -0.407604 0.314168\n2011-01-01 03:00:00 1.452354  NaN\n2011-01-01 04:00:00 -1.224869 -0.947457\n2011-01-01 05:00:00 0.498326  0.070416\n2011-01-01 06:00:00 0.401665  NaN\n2011-01-01 07:00:00 -0.019766 0.533641\n2011-01-01 08:00:00 -1.101303 -1.408561\n2011-01-01 09:00:00 1.671795  -0.764629\n', '[3, 6]'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;"[""'b'"", 'Sucess']";['KeyError', 'Sucess']
180;180;180;180;1.0;2;14025879;;1;11;<python><pandas><hierarchical>;Assertion Error in columns in DataFrame with hierarchical indexing;1142.0;"[""In [51]:\nfrom pandas import DataFrame\nf = DataFrame({'a': ['1','2','3'], 'b': ['2','3','4']})\nf.columns = [['level1 item1', 'level1 item2'],['', 'level2 item2'], ['level3 item1', 'level3 item2']]\nf\nOut[51]:\n    level1 item1    level1 item2\n                    level2 item2\n    level3 item1    level3 item2\n0         1              2\n1         2              3\n2         3              4\nIn [58]: f['level1 item1']\nAssertionError: Index length did not match values\nfrom pandas import DataFrame\nf = DataFrame({'a': ['1','2','3'], 'b': ['2','3','4']})\nf.columns = [['level1 item1', 'level1 item2'],['', 'level1 item2']]\nf\nOut[59]:\n     level1 item1   level1 item2\n                    level1 item2\n0          1              2\n1          2              3\n2          3              4\nIn [63]:\nf['level1 item1']\nOut[63]:\n0    1\n1    2\n2    3\nName: level1 item1\nindex = [np.array(['Size and accumulated size of adjusted gross income', 'All returns', 'All returns', 'All returns', 'All returns', 'All returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns']),\nnp.array(['', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Taxable income', 'Taxable income', 'Taxable income', 'Income tax after credits', 'Income tax after credits', 'Income tax after credits', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax']),\nnp.array(['', '', '', '', '', '', '', '','', '', 'Number of returns', 'Amount', 'Percent of total', 'Number of returns', 'Amount', 'Percent of total', 'Amount', 'Percent of', 'Percent of', 'Percent of', 'Average total income tax (dollars)']),\nnp.array(['', '', '', 'Amount', 'Percent of total', 'Average (dollars)', 'Average (dollars)', 'Average (dollars)', 'Amount', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Total', 'Taxable income', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit'])]\n\ndf.columns = index\nIn [68]: df['Taxable returns']['Number of returns']\nAssertionError: Index length did not match values\nindex = [np.array(['Size and accumulated size of adjusted gross income', 'All returns', 'All returns', 'All returns', 'All returns', 'All returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns']),\nnp.array(['', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Taxable income', 'Taxable income', 'Taxable income', 'Income tax after credits', 'Income tax after credits', 'Income tax after credits', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax']),\nnp.array(['1', '2', '3', '4', '5', '6', '7', '8','9', '10', 'Number of returns', 'Amount', 'Percent of total', 'Number of returns', 'Amount', 'Percent of total', 'Amount', 'Percent of', 'Percent of', 'Percent of', 'Average total income tax (dollars)']),\nnp.array(['', '', '', 'Amount', 'Percent of total', 'Average (dollars)', 'Average (dollars)', 'Average (dollars)', 'Amount', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Total', 'Taxable income', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit'])]\n\ndf.columns = index\nIn [71]: df['Taxable returns']['Number of returns']\nOut[71]:\n7\nAverage (dollars)\n0    90,660,104\n1    3,495\n...\n""]";"[""In [51]:\nfrom pandas import DataFrame\nf = DataFrame({'a': ['1','2','3'], 'b': ['2','3','4']})\nf.columns = [['level1 item1', 'level1 item2'],['', 'level2 item2'], ['level3 item1', 'level3 item2']]\nf\nOut[51]:\n    level1 item1    level1 item2\n                    level2 item2\n    level3 item1    level3 item2\n0         1              2\n1         2              3\n2         3              4\n"", ""In [58]: f['level1 item1']\nAssertionError: Index length did not match values\n"", ""from pandas import DataFrame\nf = DataFrame({'a': ['1','2','3'], 'b': ['2','3','4']})\nf.columns = [['level1 item1', 'level1 item2'],['', 'level1 item2']]\nf\nOut[59]:\n     level1 item1   level1 item2\n                    level1 item2\n0          1              2\n1          2              3\n2          3              4\n"", ""In [63]:\nf['level1 item1']\nOut[63]:\n0    1\n1    2\n2    3\nName: level1 item1\n"", ""index = [np.array(['Size and accumulated size of adjusted gross income', 'All returns', 'All returns', 'All returns', 'All returns', 'All returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns']),\nnp.array(['', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Taxable income', 'Taxable income', 'Taxable income', 'Income tax after credits', 'Income tax after credits', 'Income tax after credits', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax']),\nnp.array(['', '', '', '', '', '', '', '','', '', 'Number of returns', 'Amount', 'Percent of total', 'Number of returns', 'Amount', 'Percent of total', 'Amount', 'Percent of', 'Percent of', 'Percent of', 'Average total income tax (dollars)']),\nnp.array(['', '', '', 'Amount', 'Percent of total', 'Average (dollars)', 'Average (dollars)', 'Average (dollars)', 'Amount', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Total', 'Taxable income', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit'])]\n\ndf.columns = index\n"", ""In [68]: df['Taxable returns']['Number of returns']\nAssertionError: Index length did not match values\n"", ""index = [np.array(['Size and accumulated size of adjusted gross income', 'All returns', 'All returns', 'All returns', 'All returns', 'All returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns']),\nnp.array(['', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Taxable income', 'Taxable income', 'Taxable income', 'Income tax after credits', 'Income tax after credits', 'Income tax after credits', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax']),\nnp.array(['1', '2', '3', '4', '5', '6', '7', '8','9', '10', 'Number of returns', 'Amount', 'Percent of total', 'Number of returns', 'Amount', 'Percent of total', 'Amount', 'Percent of', 'Percent of', 'Percent of', 'Average total income tax (dollars)']),\nnp.array(['', '', '', 'Amount', 'Percent of total', 'Average (dollars)', 'Average (dollars)', 'Average (dollars)', 'Amount', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Total', 'Taxable income', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit'])]\n\ndf.columns = index\n"", ""In [71]: df['Taxable returns']['Number of returns']\nOut[71]:\n7\nAverage (dollars)\n0    90,660,104\n1    3,495\n...\n""]";"[""In [51]:\nfrom pandas import DataFrame\nf = DataFrame({'a': ['1','2','3'], 'b': ['2','3','4']})\nf.columns = [['level1 item1', 'level1 item2'],['', 'level2 item2'], ['level3 item1', 'level3 item2']]\nf\nOut[51]:\n    level1 item1    level1 item2\n                    level2 item2\n    level3 item1    level3 item2\n0         1              2\n1         2              3\n2         3              4\n"", 'level1 item1', ""In [58]: f['level1 item1']\nAssertionError: Index length did not match values\n"", ""from pandas import DataFrame\nf = DataFrame({'a': ['1','2','3'], 'b': ['2','3','4']})\nf.columns = [['level1 item1', 'level1 item2'],['', 'level1 item2']]\nf\nOut[59]:\n     level1 item1   level1 item2\n                    level1 item2\n0          1              2\n1          2              3\n2          3              4\n"", ""In [63]:\nf['level1 item1']\nOut[63]:\n0    1\n1    2\n2    3\nName: level1 item1\n"", 'level1 item1', ""index = [np.array(['Size and accumulated size of adjusted gross income', 'All returns', 'All returns', 'All returns', 'All returns', 'All returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns']),\nnp.array(['', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Taxable income', 'Taxable income', 'Taxable income', 'Income tax after credits', 'Income tax after credits', 'Income tax after credits', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax']),\nnp.array(['', '', '', '', '', '', '', '','', '', 'Number of returns', 'Amount', 'Percent of total', 'Number of returns', 'Amount', 'Percent of total', 'Amount', 'Percent of', 'Percent of', 'Percent of', 'Average total income tax (dollars)']),\nnp.array(['', '', '', 'Amount', 'Percent of total', 'Average (dollars)', 'Average (dollars)', 'Average (dollars)', 'Amount', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Total', 'Taxable income', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit'])]\n\ndf.columns = index\n"", ""In [68]: df['Taxable returns']['Number of returns']\nAssertionError: Index length did not match values\n"", ""index = [np.array(['Size and accumulated size of adjusted gross income', 'All returns', 'All returns', 'All returns', 'All returns', 'All returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns', 'Taxable returns']),\nnp.array(['', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Taxable income', 'Taxable income', 'Taxable income', 'Income tax after credits', 'Income tax after credits', 'Income tax after credits', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax']),\nnp.array(['1', '2', '3', '4', '5', '6', '7', '8','9', '10', 'Number of returns', 'Amount', 'Percent of total', 'Number of returns', 'Amount', 'Percent of total', 'Amount', 'Percent of', 'Percent of', 'Percent of', 'Average total income tax (dollars)']),\nnp.array(['', '', '', 'Amount', 'Percent of total', 'Average (dollars)', 'Average (dollars)', 'Average (dollars)', 'Amount', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Percent of total', 'Total', 'Taxable income', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit'])]\n\ndf.columns = index\n"", ""In [71]: df['Taxable returns']['Number of returns']\nOut[71]:\n7\nAverage (dollars)\n0    90,660,104\n1    3,495\n...\n""]";"[""from pandas import DataFrame\nf = DataFrame({'a': ['1','2','3'], 'b': ['2','3','4']})\nf.columns = [['level1 item1', 'level1 item2'],['', 'level2 item2'], ['level3 item1', 'level3 item2']]\nf\nf['level1 item1']\nfrom pandas import DataFrame\nf = DataFrame({'a': ['1','2','3'], 'b': ['2','3','4']})\nf.columns = [['level1 item1', 'level1 item2'],['', 'level1 item2']]\nf\nf['level1 item1']\ndf['Taxable returns']['Number of returns']\nnp.array(['', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Taxable income', 'Taxable income', 'Taxable income', 'Income tax after credits', 'Income tax after credits', 'Income tax after credits', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax']),\nnp.array(['1', '2', '3', '4', '5', '6', '7', '8','9', '10', 'Number of returns', 'Amount', 'Percent of total', 'Number of returns', 'Amount', 'Percent of total', 'Amount', 'Percent of', 'Percent of', 'Percent of', 'Average total income tax (dollars)']),\n\ndf.columns = index\n""]";"[""from pandas import DataFrame\nf = DataFrame({'a': ['1','2','3'], 'b': ['2','3','4']})\nf.columns = [['level1 item1', 'level1 item2'],['', 'level2 item2'], ['level3 item1', 'level3 item2']]\nf\nf['level1 item1']\nfrom pandas import DataFrame\nf = DataFrame({'a': ['1','2','3'], 'b': ['2','3','4']})\nf.columns = [['level1 item1', 'level1 item2'],['', 'level1 item2']]\nf\nf['level1 item1']\ndf['Taxable returns']['Number of returns']\nnp.array(['', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Taxable income', 'Taxable income', 'Taxable income', 'Income tax after credits', 'Income tax after credits', 'Income tax after credits', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax']),\nnp.array(['1', '2', '3', '4', '5', '6', '7', '8','9', '10', 'Number of returns', 'Amount', 'Percent of total', 'Number of returns', 'Amount', 'Percent of total', 'Amount', 'Percent of', 'Percent of', 'Percent of', 'Average total income tax (dollars)']),\n\ndf.columns = index\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nfrom pandas import DataFrame\nf = DataFrame({'a': ['1','2','3'], 'b': ['2','3','4']})\nf.columns = [['level1 item1', 'level1 item2'],['', 'level2 item2'], ['level3 item1', 'level3 item2']]\nf\nf['level1 item1']\nfrom pandas import DataFrame\nf = DataFrame({'a': ['1','2','3'], 'b': ['2','3','4']})\nf.columns = [['level1 item1', 'level1 item2'],['', 'level1 item2']]\nf\nf['level1 item1']\ndf['Taxable returns']['Number of returns']\nnp.array(['', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Number of returns', 'Percent of total', 'Adjusted gross income less deficit', 'Adjusted gross income less deficit', 'Taxable income', 'Taxable income', 'Taxable income', 'Income tax after credits', 'Income tax after credits', 'Income tax after credits', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax', 'Total income tax']),\nnp.array(['1', '2', '3', '4', '5', '6', '7', '8','9', '10', 'Number of returns', 'Amount', 'Percent of total', 'Number of returns', 'Amount', 'Percent of total', 'Amount', 'Percent of', 'Percent of', 'Percent of', 'Average total income tax (dollars)']),\n\ndf.columns = index\n""]";True;0;1;"[""name 'paste' is not defined""]";['NameError'];0;1;"[""name 'paste' is not defined""]";['NameError'];0;1;"[""name 'paste' is not defined""]";['NameError']
181;181;181;181;4.0;0;14057007;;1;21;<python><filtering><pandas>;Remove rows not .isin('X');20970.0;[''];[];"[""isin('X')"", 'X', '!which(a %in% b)']";[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'pd' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'np' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'np' is not defined"", 'Sucess']";['NameError', 'Sucess']
182;182;182;182;5.0;0;14059094;;1;24;<python><python-2.7><pandas>;I want to multiply two columns in a pandas DataFrame and add the result into a new column;39480.0;"[""for i in orders_df.Action:\n if i  == 'Sell':\n  orders_df['Value'] = orders_df.Prices*orders_df.Amount\n elif i == 'Buy':\n  orders_df['Value'] = -orders_df.Prices*orders_df.Amount)\n""]";"[""for i in orders_df.Action:\n if i  == 'Sell':\n  orders_df['Value'] = orders_df.Prices*orders_df.Amount\n elif i == 'Buy':\n  orders_df['Value'] = -orders_df.Prices*orders_df.Amount)\n""]";"[""for i in orders_df.Action:\n if i  == 'Sell':\n  orders_df['Value'] = orders_df.Prices*orders_df.Amount\n elif i == 'Buy':\n  orders_df['Value'] = -orders_df.Prices*orders_df.Amount)\n""]";[''];[''];False;['import pandas as pd\n'];False;1;3;"['Sucess', ""name 'orders_df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name 'orders_df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name 'orders_df' is not defined"", ""'DataFrame' object has no attribute 'Prices'""]";['Sucess', 'NameError', 'AttributeError']
183;183;183;183;1.0;0;14110721;;1;12;<python><pandas>;How to change Pandas dataframe index value?;32137.0;['>>> df\n                   sales     cash\nSTK_ID RPT_Date                  \n000568 20120930   80.093   57.488\n000596 20120930   32.585   26.177\n000799 20120930   14.784    8.157\n>>> df\n                   sales     cash\nSTK_ID RPT_Date                  \n000999 20121231   80.093   57.488\n000596 20120930   32.585   26.177\n000799 20120930   14.784    8.157\n'];['>>> df\n                   sales     cash\nSTK_ID RPT_Date                  \n000568 20120930   80.093   57.488\n000596 20120930   32.585   26.177\n000799 20120930   14.784    8.157\n', '>>> df\n                   sales     cash\nSTK_ID RPT_Date                  \n000999 20121231   80.093   57.488\n000596 20120930   32.585   26.177\n000799 20120930   14.784    8.157\n'];"['df', '>>> df\n                   sales     cash\nSTK_ID RPT_Date                  \n000568 20120930   80.093   57.488\n000596 20120930   32.585   26.177\n000799 20120930   14.784    8.157\n', ""('000568','20120930')"", ""('000999','20121231')"", '>>> df\n                   sales     cash\nSTK_ID RPT_Date                  \n000999 20121231   80.093   57.488\n000596 20120930   32.585   26.177\n000799 20120930   14.784    8.157\n']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError']
184;184;184;184;1.0;2;14144867;;1;16;<python><google-app-engine><pandas>;Can Pandas run on Google App Engine for Python?;2894.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
185;185;185;185;4.0;1;14162723;;1;36;<numpy><pandas><mysql-python>;Replacing Pandas or Numpy Nan with a None to use with MysqlDB;22191.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'pd' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'np' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'np' is not defined""]";['Sucess', 'NameError']
186;186;186;186;3.0;0;14178194;;1;19;<python><plot><pandas>;Python pandas, Plotting options for multiple lines;22674.0;"[""testdataframe=pd.DataFrame(np.arange(12).reshape(4,3))\ntestdataframe.plot(style=['s-','o-','^-'],color=['b','r','y'],linewidth=[2,1,1])\n""]";"[""testdataframe=pd.DataFrame(np.arange(12).reshape(4,3))\ntestdataframe.plot(style=['s-','o-','^-'],color=['b','r','y'],linewidth=[2,1,1])\n""]";"[""testdataframe=pd.DataFrame(np.arange(12).reshape(4,3))\ntestdataframe.plot(style=['s-','o-','^-'],color=['b','r','y'],linewidth=[2,1,1])\n"", 'testdataframe[0].plot()', 'testdataframe[[0,1]].plot()', ""testdataframe[0].plot(style=['s-','o-','^-'],color=['b','r','y'])""]";"[""testdataframe=pd.DataFrame(np.arange(12).reshape(4,3))\ntestdataframe.plot(style=['s-','o-','^-'],color=['b','r','y'],linewidth=[2,1,1])\n""]";"[""import pandas as pd\ntestdataframe=pd.DataFrame(np.arange(12).reshape(4,3))\ntestdataframe.plot(style=['s-','o-','^-'],color=['b','r','y'],linewidth=[2,1,1])\n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\ntestdataframe=pd.DataFrame(np.arange(12).reshape(4,3))\ntestdataframe.plot(style=['s-','o-','^-'],color=['b','r','y'],linewidth=[2,1,1])\n""]";True;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
187;187;187;187;3.0;0;14189695;;1;17;<python><pandas><dataframe>;Reset a columns MultiIndex levels;9898.0;['In [704]: test\nOut[704]: \n           basic_amt               \nFaculty          NSW  QLD  VIC  All\nAll                1    1    2    4\nFull Time          0    1    0    1\nPart Time          1    0    2    3\n\nIn [705]: test.reset_index(level=0, drop=True)\nOut[705]: \n         basic_amt               \nFaculty        NSW  QLD  VIC  All\n0                1    1    2    4\n1                0    1    0    1\n2                1    0    2    3\n\nIn [711]: test.transpose().reset_index(level=0, drop=True).transpose()\nOut[711]: \nFaculty    NSW  QLD  VIC  All\nAll          1    1    2    4\nFull Time    0    1    0    1\nPart Time    1    0    2    3\n'];['In [704]: test\nOut[704]: \n           basic_amt               \nFaculty          NSW  QLD  VIC  All\nAll                1    1    2    4\nFull Time          0    1    0    1\nPart Time          1    0    2    3\n\nIn [705]: test.reset_index(level=0, drop=True)\nOut[705]: \n         basic_amt               \nFaculty        NSW  QLD  VIC  All\n0                1    1    2    4\n1                0    1    0    1\n2                1    0    2    3\n\nIn [711]: test.transpose().reset_index(level=0, drop=True).transpose()\nOut[711]: \nFaculty    NSW  QLD  VIC  All\nAll          1    1    2    4\nFull Time    0    1    0    1\nPart Time    1    0    2    3\n'];['basic_amt', 'In [704]: test\nOut[704]: \n           basic_amt               \nFaculty          NSW  QLD  VIC  All\nAll                1    1    2    4\nFull Time          0    1    0    1\nPart Time          1    0    2    3\n\nIn [705]: test.reset_index(level=0, drop=True)\nOut[705]: \n         basic_amt               \nFaculty        NSW  QLD  VIC  All\n0                1    1    2    4\n1                0    1    0    1\n2                1    0    2    3\n\nIn [711]: test.transpose().reset_index(level=0, drop=True).transpose()\nOut[711]: \nFaculty    NSW  QLD  VIC  All\nAll          1    1    2    4\nFull Time    0    1    0    1\nPart Time    1    0    2    3\n'];['test\ntest.reset_index(level=0, drop=True)\ntest.transpose().reset_index(level=0, drop=True).transpose()\n'];['test\ntest.reset_index(level=0, drop=True)\ntest.transpose().reset_index(level=0, drop=True).transpose()\n'];False;['import pandas as pd\ntest\ntest.reset_index(level=0, drop=True)\ntest.transpose().reset_index(level=0, drop=True).transpose()\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'Index' object has no attribute 'levels'"", ""name 'df' is not defined""]";['AttributeError', 'NameError']
188;188;188;188;2.0;2;14192741;;1;12;<python><pandas>;Understanding pandas dataframe indexing;5916.0;"[""df[df.key==1]['D'] = 1\ndf.D[df.key==1] = 1\nIn [1]: import pandas as pd\n\nIn [2]: from numpy.random import randn\n\nIn [4]: df = pd.DataFrame(randn(6,3),columns=list('ABC'))\n\nIn [5]: df\nOut[5]: \n          A         B         C\n0  1.438161 -0.210454 -1.983704\n1 -0.283780 -0.371773  0.017580\n2  0.552564 -0.610548  0.257276\n3  1.931332  0.649179 -1.349062\n4  1.656010 -1.373263  1.333079\n5  0.944862 -0.657849  1.526811\n\nIn [6]: df['D']=0.0\n\nIn [7]: df['key']=3*[1]+3*[2]\n\nIn [8]: df\nOut[8]: \n          A         B         C  D  key\n0  1.438161 -0.210454 -1.983704  0    1\n1 -0.283780 -0.371773  0.017580  0    1\n2  0.552564 -0.610548  0.257276  0    1\n3  1.931332  0.649179 -1.349062  0    2\n4  1.656010 -1.373263  1.333079  0    2\n5  0.944862 -0.657849  1.526811  0    2\nIn [9]: df[df.key==1]['D'] = 1\n\nIn [10]: df\nOut[10]: \n          A         B         C  D  key\n0  1.438161 -0.210454 -1.983704  0    1\n1 -0.283780 -0.371773  0.017580  0    1\n2  0.552564 -0.610548  0.257276  0    1\n3  1.931332  0.649179 -1.349062  0    2\n4  1.656010 -1.373263  1.333079  0    2\n5  0.944862 -0.657849  1.526811  0    2\nIn [11]: df.D[df.key==1] = 3.4\n\nIn [12]: df\nOut[12]: \n          A         B         C    D  key\n0  1.438161 -0.210454 -1.983704  3.4    1\n1 -0.283780 -0.371773  0.017580  3.4    1\n2  0.552564 -0.610548  0.257276  3.4    1\n3  1.931332  0.649179 -1.349062  0.0    2\n4  1.656010 -1.373263  1.333079  0.0    2\n5  0.944862 -0.657849  1.526811  0.0    2\n""]";"[""df[df.key==1]['D'] = 1\n"", 'df.D[df.key==1] = 1\n', ""In [1]: import pandas as pd\n\nIn [2]: from numpy.random import randn\n\nIn [4]: df = pd.DataFrame(randn(6,3),columns=list('ABC'))\n\nIn [5]: df\nOut[5]: \n          A         B         C\n0  1.438161 -0.210454 -1.983704\n1 -0.283780 -0.371773  0.017580\n2  0.552564 -0.610548  0.257276\n3  1.931332  0.649179 -1.349062\n4  1.656010 -1.373263  1.333079\n5  0.944862 -0.657849  1.526811\n\nIn [6]: df['D']=0.0\n\nIn [7]: df['key']=3*[1]+3*[2]\n\nIn [8]: df\nOut[8]: \n          A         B         C  D  key\n0  1.438161 -0.210454 -1.983704  0    1\n1 -0.283780 -0.371773  0.017580  0    1\n2  0.552564 -0.610548  0.257276  0    1\n3  1.931332  0.649179 -1.349062  0    2\n4  1.656010 -1.373263  1.333079  0    2\n5  0.944862 -0.657849  1.526811  0    2\n"", ""In [9]: df[df.key==1]['D'] = 1\n\nIn [10]: df\nOut[10]: \n          A         B         C  D  key\n0  1.438161 -0.210454 -1.983704  0    1\n1 -0.283780 -0.371773  0.017580  0    1\n2  0.552564 -0.610548  0.257276  0    1\n3  1.931332  0.649179 -1.349062  0    2\n4  1.656010 -1.373263  1.333079  0    2\n5  0.944862 -0.657849  1.526811  0    2\n"", 'In [11]: df.D[df.key==1] = 3.4\n\nIn [12]: df\nOut[12]: \n          A         B         C    D  key\n0  1.438161 -0.210454 -1.983704  3.4    1\n1 -0.283780 -0.371773  0.017580  3.4    1\n2  0.552564 -0.610548  0.257276  3.4    1\n3  1.931332  0.649179 -1.349062  0.0    2\n4  1.656010 -1.373263  1.333079  0.0    2\n5  0.944862 -0.657849  1.526811  0.0    2\n']";"[""df[df.key==1]['D'] = 1\n"", 'df.D[df.key==1] = 1\n', ""In [1]: import pandas as pd\n\nIn [2]: from numpy.random import randn\n\nIn [4]: df = pd.DataFrame(randn(6,3),columns=list('ABC'))\n\nIn [5]: df\nOut[5]: \n          A         B         C\n0  1.438161 -0.210454 -1.983704\n1 -0.283780 -0.371773  0.017580\n2  0.552564 -0.610548  0.257276\n3  1.931332  0.649179 -1.349062\n4  1.656010 -1.373263  1.333079\n5  0.944862 -0.657849  1.526811\n\nIn [6]: df['D']=0.0\n\nIn [7]: df['key']=3*[1]+3*[2]\n\nIn [8]: df\nOut[8]: \n          A         B         C  D  key\n0  1.438161 -0.210454 -1.983704  0    1\n1 -0.283780 -0.371773  0.017580  0    1\n2  0.552564 -0.610548  0.257276  0    1\n3  1.931332  0.649179 -1.349062  0    2\n4  1.656010 -1.373263  1.333079  0    2\n5  0.944862 -0.657849  1.526811  0    2\n"", ""In [9]: df[df.key==1]['D'] = 1\n\nIn [10]: df\nOut[10]: \n          A         B         C  D  key\n0  1.438161 -0.210454 -1.983704  0    1\n1 -0.283780 -0.371773  0.017580  0    1\n2  0.552564 -0.610548  0.257276  0    1\n3  1.931332  0.649179 -1.349062  0    2\n4  1.656010 -1.373263  1.333079  0    2\n5  0.944862 -0.657849  1.526811  0    2\n"", 'In [11]: df.D[df.key==1] = 3.4\n\nIn [12]: df\nOut[12]: \n          A         B         C    D  key\n0  1.438161 -0.210454 -1.983704  3.4    1\n1 -0.283780 -0.371773  0.017580  3.4    1\n2  0.552564 -0.610548  0.257276  3.4    1\n3  1.931332  0.649179 -1.349062  0.0    2\n4  1.656010 -1.373263  1.333079  0.0    2\n5  0.944862 -0.657849  1.526811  0.0    2\n', '.loc']";"[""import pandas as pd\n\n\n\ndf['D']=0.0\n\n\ndf[df.key==1]['D'] = 1\n\ndf.D[df.key==1] = 3.4\n\n""]";"[""import pandas as pd\n\n\n\ndf['D']=0.0\n\n\ndf[df.key==1]['D'] = 1\n\ndf.D[df.key==1] = 3.4\n\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\n\n\n\ndf['D']=0.0\n\n\ndf[df.key==1]['D'] = 1\n\ndf.D[df.key==1] = 3.4\n\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
189;189;189;189;6.0;2;14224172;;1;20;<python><pandas>;Equality in Pandas DataFrames - Column Order Matters?;14439.0;"[""import pandas\ndf1 = pandas.DataFrame(index = [1,2,3,4])\ndf2 = pandas.DataFrame(index = [1,2,3,4])\ndf1['A'] = [1,2,3,4]\ndf1['B'] = [2,3,4,5]\ndf2['B'] = [2,3,4,5]\ndf2['A'] = [1,2,3,4]\ndf1 == df2\nException: Can only compare identically-labeled DataFrame objects\n""]";"[""import pandas\ndf1 = pandas.DataFrame(index = [1,2,3,4])\ndf2 = pandas.DataFrame(index = [1,2,3,4])\ndf1['A'] = [1,2,3,4]\ndf1['B'] = [2,3,4,5]\ndf2['B'] = [2,3,4,5]\ndf2['A'] = [1,2,3,4]\ndf1 == df2\n"", 'Exception: Can only compare identically-labeled DataFrame objects\n']";"[""import pandas\ndf1 = pandas.DataFrame(index = [1,2,3,4])\ndf2 = pandas.DataFrame(index = [1,2,3,4])\ndf1['A'] = [1,2,3,4]\ndf1['B'] = [2,3,4,5]\ndf2['B'] = [2,3,4,5]\ndf2['A'] = [1,2,3,4]\ndf1 == df2\n"", 'Exception: Can only compare identically-labeled DataFrame objects\n', 'df1 == df2', 'True', '==']";"[""import pandas\ndf1 = pandas.DataFrame(index = [1,2,3,4])\ndf2 = pandas.DataFrame(index = [1,2,3,4])\ndf1['A'] = [1,2,3,4]\ndf1['B'] = [2,3,4,5]\ndf2['B'] = [2,3,4,5]\ndf2['A'] = [1,2,3,4]\ndf1 == df2\n""]";"[""import pandas\ndf1 = pandas.DataFrame(index = [1,2,3,4])\ndf2 = pandas.DataFrame(index = [1,2,3,4])\ndf1['A'] = [1,2,3,4]\ndf1['B'] = [2,3,4,5]\ndf2['B'] = [2,3,4,5]\ndf2['A'] = [1,2,3,4]\ndf1 == df2\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport pandas\ndf1 = pandas.DataFrame(index = [1,2,3,4])\ndf2 = pandas.DataFrame(index = [1,2,3,4])\ndf1['A'] = [1,2,3,4]\ndf1['B'] = [2,3,4,5]\ndf2['B'] = [2,3,4,5]\ndf2['A'] = [1,2,3,4]\ndf1 == df2\n""]";True;1;2;"[""name 'df1' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df1' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""'DataFrame' object has no attribute 'sort'"", 'Sucess']";['AttributeError', 'Sucess']
190;190;190;190;1.0;0;14225676;;1;43;<python><pandas><openpyxl>;Save list of DataFrames to multisheet Excel spreadsheet;23952.0;"[""from openpyxl.writer.excel import ExcelWriter\ndef save_xls(list_dfs, xls_path):\n    writer = ExcelWriter(xls_path)\n    for n, df in enumerate(list_dfs):\n        df.to_excel(writer,'sheet%s' % n)\n    writer.save()\nAttributeError: 'str' object has no attribute 'worksheets'\n""]";"[""from openpyxl.writer.excel import ExcelWriter\ndef save_xls(list_dfs, xls_path):\n    writer = ExcelWriter(xls_path)\n    for n, df in enumerate(list_dfs):\n        df.to_excel(writer,'sheet%s' % n)\n    writer.save()\n"", ""AttributeError: 'str' object has no attribute 'worksheets'\n""]";"['to_excel', ""writer = ExcelWriter('output.xlsx')"", ""df1.to_excel(writer, 'sheet1')"", ""df2.to_excel(writer, 'sheet2')"", 'writer.save()', ""from openpyxl.writer.excel import ExcelWriter\ndef save_xls(list_dfs, xls_path):\n    writer = ExcelWriter(xls_path)\n    for n, df in enumerate(list_dfs):\n        df.to_excel(writer,'sheet%s' % n)\n    writer.save()\n"", 'to_excel', ""AttributeError: 'str' object has no attribute 'worksheets'\n"", 'ExcelWriter']";['from openpyxl.writer.excel import ExcelWriter\n'];['from openpyxl.writer.excel import ExcelWriter\n'];False;['import pandas as pd\nfrom openpyxl.writer.excel import ExcelWriter\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
191;191;191;191;2.0;0;14247586;;1;60;<python><pandas><null><nan>;Python Pandas How to select rows with one or more nulls from a DataFrame without listing columns explicitly?;66974.0;['mask=False\nfor col in df.columns: mask = mask | df[col].isnull()\ndfnulls = df[mask]\ndf.ix[df.index[(df.T == np.nan).sum() > 1]]\n'];['mask=False\nfor col in df.columns: mask = mask | df[col].isnull()\ndfnulls = df[mask]\n', 'df.ix[df.index[(df.T == np.nan).sum() > 1]]\n'];['mask=False\nfor col in df.columns: mask = mask | df[col].isnull()\ndfnulls = df[mask]\n', 'df.ix[df.index[(df.T == np.nan).sum() > 1]]\n'];['mask=False\nfor col in df.columns: mask = mask | df[col].isnull()\ndfnulls = df[mask]\ndf.ix[df.index[(df.T == np.nan).sum() > 1]]\n'];['mask=False\nfor col in df.columns: mask = mask | df[col].isnull()\ndfnulls = df[mask]\ndf.ix[df.index[(df.T == np.nan).sum() > 1]]\n'];False;['import pandas as pd\ndf = pd.DataFrame()\nmask=False\nfor col in df.columns: mask = mask | df[col].isnull()\ndfnulls = df[mask]\ndf.ix[df.index[(df.T == np.nan).sum() > 1]]\n'];True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
192;192;192;192;1.0;0;14248706;;1;18;<pandas><histogram>;How can I plot a histogram in pandas using nominal values?;8485.0;"[""ser = Series(['one', 'two', 'three', 'two', 'two'])\n     X\n X   X   X\n-------------\none two three\nTypeError: cannot concatenate 'str' and 'float' objects\n""]";"[""ser = Series(['one', 'two', 'three', 'two', 'two'])\n"", '     X\n X   X   X\n-------------\none two three\n', ""TypeError: cannot concatenate 'str' and 'float' objects\n""]";"[""ser = Series(['one', 'two', 'three', 'two', 'two'])\n"", '     X\n X   X   X\n-------------\none two three\n', ""TypeError: cannot concatenate 'str' and 'float' objects\n""]";"[""ser = Series(['one', 'two', 'three', 'two', 'two'])\n""]";"[""ser = Series(['one', 'two', 'three', 'two', 'two'])\n""]";False;"[""import pandas as pd\nser = Series(['one', 'two', 'three', 'two', 'two'])\n""]";False;0;1;"[""name 'ser' is not defined""]";['NameError'];0;1;"[""name 'ser' is not defined""]";['NameError'];0;1;"[""name 'ser' is not defined""]";['NameError']
193;193;193;193;11.0;4;14262433;;1;577;<python><mongodb><pandas><large-data><hdf5>;"""Large data"" work flows using pandas";161470.0;[''];[];"['HDFStore', ""if var1 > 2 then newvar = 'A' elif var2 = 4 then newvar = 'B'""]";[''];[''];False;['import pandas as pd\n'];False;7;9;"['HDFStore requires PyTables, ""No module named \'tables\'"" problem importing', ""name 'aCollection' is not defined"", 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess']";['ImportError', 'NameError', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess'];7;9;"['HDFStore requires PyTables, ""No module named \'tables\'"" problem importing', ""name 'aCollection' is not defined"", 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess']";['ImportError', 'NameError', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess'];7;9;"['HDFStore requires PyTables, ""No module named \'tables\'"" problem importing', ""name 'aCollection' is not defined"", 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess']";['ImportError', 'NameError', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess']
194;194;194;194;1.0;2;14300137;;1;52;<python><matplotlib><plot><dataframe><pandas>;making matplotlib scatter plots from dataframes in Python's pandas;43938.0;"['import matplotlib.pylab as plt\n# df is a DataFrame: fetch col1 and col2 \n# and drop na rows if any of the columns are NA\nmydata = df[[""col1"", ""col2""]].dropna(how=""any"")\n# Now plot with matplotlib\nvals = mydata.values\nplt.scatter(vals[:, 0], vals[:, 1])\nmydata = df.dropna(how=""any"", subset=[""col1"", ""col2""])\n# plot a scatter of col1 by col2, with sizes according to col3\nscatter(mydata([""col1"", ""col2""]), s=mydata[""col3""])\nmydata = df.dropna(how=""any"", subset=[""col1"", ""col2""]) \nmyscatter = scatter(mydata[[""col1"", ""col2""]], s=1)\n# Plot in red, with smaller size, all the points that \n# have a col2 value greater than 0.5\nmyscatter.replot(mydata[""col2""] > 0.5, color=""red"", s=0.5)\nmydata = df.dropna(how=""any"", subset=[""col1"", ""col2"", ""col3"")\n']";"['import matplotlib.pylab as plt\n# df is a DataFrame: fetch col1 and col2 \n# and drop na rows if any of the columns are NA\nmydata = df[[""col1"", ""col2""]].dropna(how=""any"")\n# Now plot with matplotlib\nvals = mydata.values\nplt.scatter(vals[:, 0], vals[:, 1])\n', 'mydata = df.dropna(how=""any"", subset=[""col1"", ""col2""])\n# plot a scatter of col1 by col2, with sizes according to col3\nscatter(mydata([""col1"", ""col2""]), s=mydata[""col3""])\n', 'mydata = df.dropna(how=""any"", subset=[""col1"", ""col2""]) \nmyscatter = scatter(mydata[[""col1"", ""col2""]], s=1)\n# Plot in red, with smaller size, all the points that \n# have a col2 value greater than 0.5\nmyscatter.replot(mydata[""col2""] > 0.5, color=""red"", s=0.5)\n', 'mydata = df.dropna(how=""any"", subset=[""col1"", ""col2"", ""col3"")\n']";"['matplotlib', 'pandas', 'df', 'import matplotlib.pylab as plt\n# df is a DataFrame: fetch col1 and col2 \n# and drop na rows if any of the columns are NA\nmydata = df[[""col1"", ""col2""]].dropna(how=""any"")\n# Now plot with matplotlib\nvals = mydata.values\nplt.scatter(vals[:, 0], vals[:, 1])\n', 'col3', 'scatter', 'col1,col2', 'mydata = df.dropna(how=""any"", subset=[""col1"", ""col2""])\n# plot a scatter of col1 by col2, with sizes according to col3\nscatter(mydata([""col1"", ""col2""]), s=mydata[""col3""])\n', 'col1, col2', 'mydata = df.dropna(how=""any"", subset=[""col1"", ""col2""]) \nmyscatter = scatter(mydata[[""col1"", ""col2""]], s=1)\n# Plot in red, with smaller size, all the points that \n# have a col2 value greater than 0.5\nmyscatter.replot(mydata[""col2""] > 0.5, color=""red"", s=0.5)\n', 'subset_a', 'subset_b', 'col1,col2', 'col3', 'col1,col2,col3', 'col2', 'col3', 'col1', 'col3', 'dropna', 'mydata = df.dropna(how=""any"", subset=[""col1"", ""col2"", ""col3"")\n', 'mydata', 'col1,col2', 'col3', 'mydata', 'col1,col2', 'col3', 'mydata']";"['import matplotlib.pylab as plt\n# df is a DataFrame: fetch col1 and col2 \n# and drop na rows if any of the columns are NA\nmydata = df[[""col1"", ""col2""]].dropna(how=""any"")\n# Now plot with matplotlib\nvals = mydata.values\nplt.scatter(vals[:, 0], vals[:, 1])\nmydata = df.dropna(how=""any"", subset=[""col1"", ""col2""])\n# plot a scatter of col1 by col2, with sizes according to col3\nscatter(mydata([""col1"", ""col2""]), s=mydata[""col3""])\nmydata = df.dropna(how=""any"", subset=[""col1"", ""col2""]) \nmyscatter = scatter(mydata[[""col1"", ""col2""]], s=1)\n# Plot in red, with smaller size, all the points that \n# have a col2 value greater than 0.5\nmyscatter.replot(mydata[""col2""] > 0.5, color=""red"", s=0.5)\n']";"['from pandas import DataFrame\nimport matplotlib.pylab as plt\n# df is a DataFrame: fetch col1 and col2 \n# and drop na rows if any of the columns are NA\nmydata = df[[""col1"", ""col2""]].dropna(how=""any"")\n# Now plot with matplotlib\nvals = mydata.values\nplt.scatter(vals[:, 0], vals[:, 1])\nmydata = df.dropna(how=""any"", subset=[""col1"", ""col2""])\n# plot a scatter of col1 by col2, with sizes according to col3\nscatter(mydata([""col1"", ""col2""]), s=mydata[""col3""])\nmydata = df.dropna(how=""any"", subset=[""col1"", ""col2""]) \nmyscatter = scatter(mydata[[""col1"", ""col2""]], s=1)\n# Plot in red, with smaller size, all the points that \n# have a col2 value greater than 0.5\nmyscatter.replot(mydata[""col2""] > 0.5, color=""red"", s=0.5)\n']";True;"['import pandas as pd\ndf = pd.DataFrame()\nimport matplotlib.pylab as plt\n# df is a DataFrame: fetch col1 and col2 \n# and drop na rows if any of the columns are NA\nmydata = df[[""col1"", ""col2""]].dropna(how=""any"")\n# Now plot with matplotlib\nvals = mydata.values\nplt.scatter(vals[:, 0], vals[:, 1])\nmydata = df.dropna(how=""any"", subset=[""col1"", ""col2""])\n# plot a scatter of col1 by col2, with sizes according to col3\nscatter(mydata([""col1"", ""col2""]), s=mydata[""col3""])\nmydata = df.dropna(how=""any"", subset=[""col1"", ""col2""]) \nmyscatter = scatter(mydata[[""col1"", ""col2""]], s=1)\n# Plot in red, with smaller size, all the points that \n# have a col2 value greater than 0.5\nmyscatter.replot(mydata[""col2""] > 0.5, color=""red"", s=0.5)\n']";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
195;195;195;195;3.0;0;14300768;;1;16;<python><pandas>;pandas rolling computation with window based on values instead of counts;9850.0;"["">>> print d\n   RollBasis  ToRoll\n0          1       1\n1          1       4\n2          1      -5\n3          2       2\n4          3      -4\n5          5      -2\n6          8       0\n7         10     -13\n8         12      -2\n9         13      -5\n>>> d.roll_by(sum, 'RollBasis', 5)\n    1    -4    # sum of elements with 1 <= Rollbasis <= 5\n    2    -4    # sum of elements with 2 <= Rollbasis <= 6\n    3    -6    # sum of elements with 3 <= Rollbasis <= 7\n    4    -2    # sum of elements with 4 <= Rollbasis <= 8\n    # etc.\n""]";"['>>> print d\n   RollBasis  ToRoll\n0          1       1\n1          1       4\n2          1      -5\n3          2       2\n4          3      -4\n5          5      -2\n6          8       0\n7         10     -13\n8         12      -2\n9         13      -5\n', "">>> d.roll_by(sum, 'RollBasis', 5)\n    1    -4    # sum of elements with 1 <= Rollbasis <= 5\n    2    -4    # sum of elements with 2 <= Rollbasis <= 6\n    3    -6    # sum of elements with 3 <= Rollbasis <= 7\n    4    -2    # sum of elements with 4 <= Rollbasis <= 8\n    # etc.\n""]";"['rolling_*', 'pandas', '>>> print d\n   RollBasis  ToRoll\n0          1       1\n1          1       4\n2          1      -5\n3          2       2\n4          3      -4\n5          5      -2\n6          8       0\n7         10     -13\n8         12      -2\n9         13      -5\n', 'rolling_sum(d, 5)', 'RollBasis', ""d.roll_by(sum, 'RollBasis', 5)"", 'RollBasis', 'RollBasis', 'RollBasis', 'RollBasis', "">>> d.roll_by(sum, 'RollBasis', 5)\n    1    -4    # sum of elements with 1 <= Rollbasis <= 5\n    2    -4    # sum of elements with 2 <= Rollbasis <= 6\n    3    -6    # sum of elements with 3 <= Rollbasis <= 7\n    4    -2    # sum of elements with 4 <= Rollbasis <= 8\n    # etc.\n"", 'groupby', 'groupby']";['    # etc.\n'];['    # etc.\n'];False;['import pandas as pd\n    # etc.\n'];False;1;3;"[""name 'df' is not defined"", 'Sucess', ""name 'pd' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""name 'df' is not defined"", 'Sucess', ""name 'np' is not defined""]";['NameError', 'Sucess', 'NameError'];2;3;"['Sucess', 'Sucess', ""name 'np' is not defined""]";['Sucess', 'Sucess', 'NameError']
196;196;196;196;1.0;0;14301913;;1;11;<python><group-by><dataframe><pandas><multi-index>;Convert pandas group by object to multi-indexed Dataframe;4483.0;"["">>> df = pd.DataFrame({'Name': ['Bob'] * 3 + ['Alice'] * 3, \\\n'Destination': ['Athens', 'Rome'] * 3, 'Length': np.random.randint(1, 6, 6)}) \n>>> df    \n  Destination  Length   Name\n0      Athens       3    Bob\n1        Rome       5    Bob\n2      Athens       2    Bob\n3        Rome       1  Alice\n4      Athens       3  Alice\n5        Rome       5  Alice\n>>> grouped = df.groupby(['Name', 'Destination'])\n>>> for nm, gp in grouped:\n>>>     print nm\n>>>     print gp\n('Alice', 'Athens')\n  Destination  Length   Name\n4      Athens       3  Alice\n('Alice', 'Rome')\n  Destination  Length   Name\n3        Rome       1  Alice\n5        Rome       5  Alice\n('Bob', 'Athens')\n  Destination  Length Name\n0      Athens       3  Bob\n2      Athens       2  Bob\n('Bob', 'Rome')\n  Destination  Length Name\n1        Rome       5  Bob\n                Length\nAlice   Athens       3\n        Rome         1\n        Rome         5\nBob     Athens       3\n        Athens       2\n        Rome         5\n""]";"["">>> df = pd.DataFrame({'Name': ['Bob'] * 3 + ['Alice'] * 3, \\\n'Destination': ['Athens', 'Rome'] * 3, 'Length': np.random.randint(1, 6, 6)}) \n>>> df    \n  Destination  Length   Name\n0      Athens       3    Bob\n1        Rome       5    Bob\n2      Athens       2    Bob\n3        Rome       1  Alice\n4      Athens       3  Alice\n5        Rome       5  Alice\n"", "">>> grouped = df.groupby(['Name', 'Destination'])\n>>> for nm, gp in grouped:\n>>>     print nm\n>>>     print gp\n('Alice', 'Athens')\n  Destination  Length   Name\n4      Athens       3  Alice\n('Alice', 'Rome')\n  Destination  Length   Name\n3        Rome       1  Alice\n5        Rome       5  Alice\n('Bob', 'Athens')\n  Destination  Length Name\n0      Athens       3  Bob\n2      Athens       2  Bob\n('Bob', 'Rome')\n  Destination  Length Name\n1        Rome       5  Bob\n"", '                Length\nAlice   Athens       3\n        Rome         1\n        Rome         5\nBob     Athens       3\n        Athens       2\n        Rome         5\n']";"["">>> df = pd.DataFrame({'Name': ['Bob'] * 3 + ['Alice'] * 3, \\\n'Destination': ['Athens', 'Rome'] * 3, 'Length': np.random.randint(1, 6, 6)}) \n>>> df    \n  Destination  Length   Name\n0      Athens       3    Bob\n1        Rome       5    Bob\n2      Athens       2    Bob\n3        Rome       1  Alice\n4      Athens       3  Alice\n5        Rome       5  Alice\n"", "">>> grouped = df.groupby(['Name', 'Destination'])\n>>> for nm, gp in grouped:\n>>>     print nm\n>>>     print gp\n('Alice', 'Athens')\n  Destination  Length   Name\n4      Athens       3  Alice\n('Alice', 'Rome')\n  Destination  Length   Name\n3        Rome       1  Alice\n5        Rome       5  Alice\n('Bob', 'Athens')\n  Destination  Length Name\n0      Athens       3  Bob\n2      Athens       2  Bob\n('Bob', 'Rome')\n  Destination  Length Name\n1        Rome       5  Bob\n"", '                Length\nAlice   Athens       3\n        Rome         1\n        Rome         5\nBob     Athens       3\n        Athens       2\n        Rome         5\n', 'Dataframe(grouped)', 'PandasError']";"[""('Alice', 'Athens')\n('Alice', 'Rome')\n('Bob', 'Athens')\n('Bob', 'Rome')\n""]";"[""('Alice', 'Athens')\n('Alice', 'Rome')\n('Bob', 'Athens')\n('Bob', 'Rome')\n""]";False;"[""import pandas as pd\n('Alice', 'Athens')\n('Alice', 'Rome')\n('Bob', 'Athens')\n('Bob', 'Rome')\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'Name'""]";['KeyError']
197;197;197;197;2.0;0;14345739;;1;20;<python><csv><pandas>;Replacing part of string in python pandas dataframe;41978.0;"[""misc['product_desc'] = misc['product_desc'].strip('\\n')\n\nAttributeError: 'Series' object has no attribute 'strip'\nmisc['product_desc'] = misc['product_desc'].str.strip('\\n')\n\nTypeError: wrapper() takes exactly 1 argument (2 given)\n\nmisc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\\n'))\nmisc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\\n\\t'))\nmisc = misc.replace('\\n', '')\nmisc_id.to_csv('C:\\Users\\jlalonde\\Desktop\\misc_w_id.csv', sep=' ', na_rep='', index=False, encoding='utf-8')\n""]";"[""misc['product_desc'] = misc['product_desc'].strip('\\n')\n\nAttributeError: 'Series' object has no attribute 'strip'\n"", ""misc['product_desc'] = misc['product_desc'].str.strip('\\n')\n\nTypeError: wrapper() takes exactly 1 argument (2 given)\n\nmisc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\\n'))\nmisc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\\n\\t'))\n"", ""misc = misc.replace('\\n', '')\n"", ""misc_id.to_csv('C:\\Users\\jlalonde\\Desktop\\misc_w_id.csv', sep=' ', na_rep='', index=False, encoding='utf-8')\n""]";"[""misc['product_desc'] = misc['product_desc'].strip('\\n')\n\nAttributeError: 'Series' object has no attribute 'strip'\n"", ""misc['product_desc'] = misc['product_desc'].str.strip('\\n')\n\nTypeError: wrapper() takes exactly 1 argument (2 given)\n\nmisc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\\n'))\nmisc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\\n\\t'))\n"", ""misc = misc.replace('\\n', '')\n"", ""misc_id.to_csv('C:\\Users\\jlalonde\\Desktop\\misc_w_id.csv', sep=' ', na_rep='', index=False, encoding='utf-8')\n""]";"[""misc['product_desc'] = misc['product_desc'].strip('\\n')\n\nmisc['product_desc'] = misc['product_desc'].str.strip('\\n')\n\n\nmisc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\\n'))\nmisc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\\n\\t'))\nmisc = misc.replace('\\n', '')\n""]";"[""misc['product_desc'] = misc['product_desc'].strip('\\n')\n\nmisc['product_desc'] = misc['product_desc'].str.strip('\\n')\n\n\nmisc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\\n'))\nmisc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\\n\\t'))\nmisc = misc.replace('\\n', '')\n""]";False;"[""import pandas as pd\nmisc['product_desc'] = misc['product_desc'].strip('\\n')\n\nmisc['product_desc'] = misc['product_desc'].str.strip('\\n')\n\n\nmisc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\\n'))\nmisc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\\n\\t'))\nmisc = misc.replace('\\n', '')\n""]";False;0;2;"[""name 'misc' is not defined"", ""name 'misc' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'misc' is not defined"", ""name 'misc' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'misc' is not defined"", ""name 'misc' is not defined""]";['NameError', 'NameError']
198;198;198;198;7.0;5;14349055;;1;53;<python><r><matplotlib><plot><pandas>;making matplotlib graphs look like R by default?;21800.0;[''];[];['matplotlib', 'matplotlib', 'matplotlib', 'matplotlib', 'matplotlib'];[''];[''];False;['import pandas as pd\n'];False;1;4;"['Sucess', ""No module named 'matplotlib'"", ""No module named 'matplotlib'"", ""No module named 'matplotlib'""]";['Sucess', 'ImportError', 'ImportError', 'ImportError'];1;4;"['Sucess', ""No module named 'matplotlib'"", ""No module named 'matplotlib'"", ""No module named 'matplotlib'""]";['Sucess', 'ImportError', 'ImportError', 'ImportError'];1;4;"['Sucess', ""No module named 'matplotlib'"", ""No module named 'matplotlib'"", ""No module named 'matplotlib'""]";['Sucess', 'ImportError', 'ImportError', 'ImportError']
199;199;199;199;2.0;0;14358567;;1;14;<python><pandas>;Finding consecutive segments in a pandas data frame;9861.0;['1:  3\n2:  3\n3:  3\n4:  3\n5:  4\n6:  4\n7:  4\n8:  4\n9:  1\n10: 1\n11: 1\n12: 1\n13: 1\n[[1,2,3,4], [5,6,7,8], [9,10,11,12,13]]\n'];['1:  3\n2:  3\n3:  3\n4:  3\n5:  4\n6:  4\n7:  4\n8:  4\n9:  1\n10: 1\n11: 1\n12: 1\n13: 1\n', '[[1,2,3,4], [5,6,7,8], [9,10,11,12,13]]\n'];['1:  3\n2:  3\n3:  3\n4:  3\n5:  4\n6:  4\n7:  4\n8:  4\n9:  1\n10: 1\n11: 1\n12: 1\n13: 1\n', '[[1,2,3,4], [5,6,7,8], [9,10,11,12,13]]\n'];['[[1,2,3,4], [5,6,7,8], [9,10,11,12,13]]\n'];['[[1,2,3,4], [5,6,7,8], [9,10,11,12,13]]\n'];False;['import pandas as pd\n[[1,2,3,4], [5,6,7,8], [9,10,11,12,13]]\n'];False;0;2;"[""name 'np' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""'A'""]";['NameError', 'KeyError']
200;200;200;200;3.0;0;14363640;;1;17;<python><pandas>;Python Pandas - Deleting multiple series from a data frame in one command;25631.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError']
201;201;201;201;4.0;3;14365542;;1;35;<python><r><csv><pandas>;read csv file and return data.frame in Python;64795.0;"['Date,""price"",""factor_1"",""factor_2""\n2012-06-11,1600.20,1.255,1.548\n2012-06-12,1610.02,1.258,1.554\n2012-06-13,1618.07,1.249,1.552\n2012-06-14,1624.40,1.253,1.556\n2012-06-15,1626.15,1.258,1.552\n2012-06-16,1626.15,1.263,1.558\n2012-06-17,1626.15,1.264,1.572\nprice <- read.csv(""value.txt"")  \n> price <- read.csv(""value.txt"")\n> price\n     Date   price factor_1 factor_2\n1  2012-06-11 1600.20    1.255    1.548\n2  2012-06-12 1610.02    1.258    1.554\n3  2012-06-13 1618.07    1.249    1.552\n4  2012-06-14 1624.40    1.253    1.556\n5  2012-06-15 1626.15    1.258    1.552\n6  2012-06-16 1626.15    1.263    1.558\n7  2012-06-17 1626.15    1.264    1.572\n']";"['Date,""price"",""factor_1"",""factor_2""\n2012-06-11,1600.20,1.255,1.548\n2012-06-12,1610.02,1.258,1.554\n2012-06-13,1618.07,1.249,1.552\n2012-06-14,1624.40,1.253,1.556\n2012-06-15,1626.15,1.258,1.552\n2012-06-16,1626.15,1.263,1.558\n2012-06-17,1626.15,1.264,1.572\n', 'price <- read.csv(""value.txt"")  \n', '> price <- read.csv(""value.txt"")\n> price\n     Date   price factor_1 factor_2\n1  2012-06-11 1600.20    1.255    1.548\n2  2012-06-12 1610.02    1.258    1.554\n3  2012-06-13 1618.07    1.249    1.552\n4  2012-06-14 1624.40    1.253    1.556\n5  2012-06-15 1626.15    1.258    1.552\n6  2012-06-16 1626.15    1.263    1.558\n7  2012-06-17 1626.15    1.264    1.572\n']";"['""value.txt""', 'Date,""price"",""factor_1"",""factor_2""\n2012-06-11,1600.20,1.255,1.548\n2012-06-12,1610.02,1.258,1.554\n2012-06-13,1618.07,1.249,1.552\n2012-06-14,1624.40,1.253,1.556\n2012-06-15,1626.15,1.258,1.552\n2012-06-16,1626.15,1.263,1.558\n2012-06-17,1626.15,1.264,1.572\n', 'price <- read.csv(""value.txt"")  \n', '> price <- read.csv(""value.txt"")\n> price\n     Date   price factor_1 factor_2\n1  2012-06-11 1600.20    1.255    1.548\n2  2012-06-12 1610.02    1.258    1.554\n3  2012-06-13 1618.07    1.249    1.552\n4  2012-06-14 1624.40    1.253    1.556\n5  2012-06-15 1626.15    1.258    1.552\n6  2012-06-16 1626.15    1.263    1.558\n7  2012-06-17 1626.15    1.264    1.572\n']";"['Date,""price"",""factor_1"",""factor_2""\nprice <- read.csv(""value.txt"")  \n']";"['Date,""price"",""factor_1"",""factor_2""\nprice <- read.csv(""value.txt"")  \n']";False;"['import pandas as pd\nDate,""price"",""factor_1"",""factor_2""\nprice <- read.csv(""value.txt"")  \n']";False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
202;202;202;202;2.0;1;14380371;;1;30;<python><latex><dataframe><pandas>;Export a LaTeX table from pandas DataFrame;10522.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
203;203;203;203;4.0;2;14422976;;1;17;<python-3.x><pandas>;Importing pandas shows ImportError: cannot import name hashtable;29985.0;"['import csv\nimport pandas\nfrom pandas import DataFrame\n\ncsvdata = pandas.read_csv(\'datafile.csv\')\ndf = DataFrame(csvdata)\ncannot import name hashtable\nTraceback (most recent call last):\n  File ""C:\\Users\\document\\test4.py"", line 5, in <module>\n    import pandas\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\__init__.py"", line 6, in <module>\n    from . import hashtable, tslib, lib\nImportError: cannot import name hashtable\n']";"[""import csv\nimport pandas\nfrom pandas import DataFrame\n\ncsvdata = pandas.read_csv('datafile.csv')\ndf = DataFrame(csvdata)\n"", 'cannot import name hashtable\nTraceback (most recent call last):\n  File ""C:\\Users\\document\\test4.py"", line 5, in <module>\n    import pandas\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\__init__.py"", line 6, in <module>\n    from . import hashtable, tslib, lib\nImportError: cannot import name hashtable\n']";"[""import csv\nimport pandas\nfrom pandas import DataFrame\n\ncsvdata = pandas.read_csv('datafile.csv')\ndf = DataFrame(csvdata)\n"", 'cannot import name hashtable\nTraceback (most recent call last):\n  File ""C:\\Users\\document\\test4.py"", line 5, in <module>\n    import pandas\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\__init__.py"", line 6, in <module>\n    from . import hashtable, tslib, lib\nImportError: cannot import name hashtable\n']";"[""import csv\nimport pandas\nfrom pandas import DataFrame\n\ncsvdata = pandas.read_csv('datafile.csv')\ndf = DataFrame(csvdata)\n""]";"[""import csv\nimport pandas\nfrom pandas import DataFrame\n\ncsvdata = pandas.read_csv('datafile.csv')\ndf = DataFrame(csvdata)\n""]";False;"[""import pandas as pd\nimport csv\nimport pandas\nfrom pandas import DataFrame\n\ncsvdata = pandas.read_csv('datafile.csv')\ndf = DataFrame(csvdata)\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
204;204;204;204;2.0;1;14431646;;1;19;<python><sqlite3><pandas>;How to write Pandas dataframe to sqlite with Index;16793.0;"['             AAPL     GE\nDate\n2009-01-02  89.95  14.76\n2009-01-05  93.75  14.38\n2009-01-06  92.20  14.58\n2009-01-07  90.21  13.93\n2009-01-08  91.88  13.95\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame, Series\nimport sqlite3 as db\n\n# download data from yahoo\nall_data = {}\n\nfor ticker in [\'AAPL\', \'GE\']:\n    all_data[ticker] = pd.io.data.get_data_yahoo(ticker, \'1/1/2009\',\'12/31/2012\')\n\n# create a data frame\nprice = DataFrame({tic: data[\'Adj Close\'] for tic, data in all_data.iteritems()})\n\n# get output ready for database export\noutput = price.itertuples()\ndata = tuple(output)\n\n# connect to a test DB with one three-column table titled ""Demo""\ncon = db.connect(\'c:/Python27/test.db\')\nwildcards = \',\'.join([\'?\'] * 3)\ninsert_sql = \'INSERT INTO Demo VALUES (%s)\' % wildcards\ncon.executemany(insert_sql, data)\n---------------------------------------------------------------------------\nInterfaceError                            Traceback (most recent call last)\n<ipython-input-15-680cc9889c56> in <module>()\n----> 1 con.executemany(insert_sql, data)\n\nInterfaceError: Error binding parameter 0 - probably unsupported type.\n']";"['             AAPL     GE\nDate\n2009-01-02  89.95  14.76\n2009-01-05  93.75  14.38\n2009-01-06  92.20  14.58\n2009-01-07  90.21  13.93\n2009-01-08  91.88  13.95\n', 'import numpy as np\nimport pandas as pd\nfrom pandas import DataFrame, Series\nimport sqlite3 as db\n\n# download data from yahoo\nall_data = {}\n\nfor ticker in [\'AAPL\', \'GE\']:\n    all_data[ticker] = pd.io.data.get_data_yahoo(ticker, \'1/1/2009\',\'12/31/2012\')\n\n# create a data frame\nprice = DataFrame({tic: data[\'Adj Close\'] for tic, data in all_data.iteritems()})\n\n# get output ready for database export\noutput = price.itertuples()\ndata = tuple(output)\n\n# connect to a test DB with one three-column table titled ""Demo""\ncon = db.connect(\'c:/Python27/test.db\')\nwildcards = \',\'.join([\'?\'] * 3)\ninsert_sql = \'INSERT INTO Demo VALUES (%s)\' % wildcards\ncon.executemany(insert_sql, data)\n', '---------------------------------------------------------------------------\nInterfaceError                            Traceback (most recent call last)\n<ipython-input-15-680cc9889c56> in <module>()\n----> 1 con.executemany(insert_sql, data)\n\nInterfaceError: Error binding parameter 0 - probably unsupported type.\n']";"['             AAPL     GE\nDate\n2009-01-02  89.95  14.76\n2009-01-05  93.75  14.38\n2009-01-06  92.20  14.58\n2009-01-07  90.21  13.93\n2009-01-08  91.88  13.95\n', 'import numpy as np\nimport pandas as pd\nfrom pandas import DataFrame, Series\nimport sqlite3 as db\n\n# download data from yahoo\nall_data = {}\n\nfor ticker in [\'AAPL\', \'GE\']:\n    all_data[ticker] = pd.io.data.get_data_yahoo(ticker, \'1/1/2009\',\'12/31/2012\')\n\n# create a data frame\nprice = DataFrame({tic: data[\'Adj Close\'] for tic, data in all_data.iteritems()})\n\n# get output ready for database export\noutput = price.itertuples()\ndata = tuple(output)\n\n# connect to a test DB with one three-column table titled ""Demo""\ncon = db.connect(\'c:/Python27/test.db\')\nwildcards = \',\'.join([\'?\'] * 3)\ninsert_sql = \'INSERT INTO Demo VALUES (%s)\' % wildcards\ncon.executemany(insert_sql, data)\n', '---------------------------------------------------------------------------\nInterfaceError                            Traceback (most recent call last)\n<ipython-input-15-680cc9889c56> in <module>()\n----> 1 con.executemany(insert_sql, data)\n\nInterfaceError: Error binding parameter 0 - probably unsupported type.\n']";"['Date\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame, Series\nimport sqlite3 as db\n\n# download data from yahoo\nall_data = {}\n\n\n# create a data frame\nprice = DataFrame({tic: data[\'Adj Close\'] for tic, data in all_data.iteritems()})\n\n# get output ready for database export\noutput = price.itertuples()\ndata = tuple(output)\n\n# connect to a test DB with one three-column table titled ""Demo""\ncon = db.connect(\'c:/Python27/test.db\')\nwildcards = \',\'.join([\'?\'] * 3)\ninsert_sql = \'INSERT INTO Demo VALUES (%s)\' % wildcards\ncon.executemany(insert_sql, data)\n\n']";"['Date\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame, Series\nimport sqlite3 as db\n\n# download data from yahoo\nall_data = {}\n\n\n# create a data frame\nprice = DataFrame({tic: data[\'Adj Close\'] for tic, data in all_data.iteritems()})\n\n# get output ready for database export\noutput = price.itertuples()\ndata = tuple(output)\n\n# connect to a test DB with one three-column table titled ""Demo""\ncon = db.connect(\'c:/Python27/test.db\')\nwildcards = \',\'.join([\'?\'] * 3)\ninsert_sql = \'INSERT INTO Demo VALUES (%s)\' % wildcards\ncon.executemany(insert_sql, data)\n\n']";False;"['import pandas as pd\nDate\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame, Series\nimport sqlite3 as db\n\n# download data from yahoo\nall_data = {}\n\n\n# create a data frame\nprice = DataFrame({tic: data[\'Adj Close\'] for tic, data in all_data.iteritems()})\n\n# get output ready for database export\noutput = price.itertuples()\ndata = tuple(output)\n\n# connect to a test DB with one three-column table titled ""Demo""\ncon = db.connect(\'c:/Python27/test.db\')\nwildcards = \',\'.join([\'?\'] * 3)\ninsert_sql = \'INSERT INTO Demo VALUES (%s)\' % wildcards\ncon.executemany(insert_sql, data)\n\n']";False;0;1;"[""name 'price2' is not defined""]";['NameError'];0;1;"[""name 'price2' is not defined""]";['NameError'];0;1;"[""name 'price2' is not defined""]";['NameError']
205;205;205;205;1.0;0;14451185;;1;16;<python><pandas><binning>;Better binning in pandas;15236.0;['x = 5\ny = 17\nz = 33\nfilter_values = [x, y, z]\nfiltered_a = df[df.filtercol <= x]\na_count = filtered_a.filtercol.count()\n\nfiltered_b = df[df.filtercol > x]\nfiltered_b = filtered_b[filtered_b <= y]\nb_count = filtered_b.filtercol.count()\n\nfiltered_c = df[df.filtercol > y]\nc_count = filtered_c.filtercol.count()\n'];['x = 5\ny = 17\nz = 33\nfilter_values = [x, y, z]\nfiltered_a = df[df.filtercol <= x]\na_count = filtered_a.filtercol.count()\n\nfiltered_b = df[df.filtercol > x]\nfiltered_b = filtered_b[filtered_b <= y]\nb_count = filtered_b.filtercol.count()\n\nfiltered_c = df[df.filtercol > y]\nc_count = filtered_c.filtercol.count()\n'];['x = 5\ny = 17\nz = 33\nfilter_values = [x, y, z]\nfiltered_a = df[df.filtercol <= x]\na_count = filtered_a.filtercol.count()\n\nfiltered_b = df[df.filtercol > x]\nfiltered_b = filtered_b[filtered_b <= y]\nb_count = filtered_b.filtercol.count()\n\nfiltered_c = df[df.filtercol > y]\nc_count = filtered_c.filtercol.count()\n'];['x = 5\ny = 17\nz = 33\nfilter_values = [x, y, z]\nfiltered_a = df[df.filtercol <= x]\na_count = filtered_a.filtercol.count()\n\nfiltered_b = df[df.filtercol > x]\nfiltered_b = filtered_b[filtered_b <= y]\nb_count = filtered_b.filtercol.count()\n\nfiltered_c = df[df.filtercol > y]\nc_count = filtered_c.filtercol.count()\n'];['x = 5\ny = 17\nz = 33\nfilter_values = [x, y, z]\nfiltered_a = df[df.filtercol <= x]\na_count = filtered_a.filtercol.count()\n\nfiltered_b = df[df.filtercol > x]\nfiltered_b = filtered_b[filtered_b <= y]\nb_count = filtered_b.filtercol.count()\n\nfiltered_c = df[df.filtercol > y]\nc_count = filtered_c.filtercol.count()\n'];False;['import pandas as pd\ndf = pd.DataFrame()\nx = 5\ny = 17\nz = 33\nfilter_values = [x, y, z]\nfiltered_a = df[df.filtercol <= x]\na_count = filtered_a.filtercol.count()\n\nfiltered_b = df[df.filtercol > x]\nfiltered_b = filtered_b[filtered_b <= y]\nb_count = filtered_b.filtercol.count()\n\nfiltered_c = df[df.filtercol > y]\nc_count = filtered_c.filtercol.count()\n'];True;0;1;['DataFrame constructor not properly called!'];['ValueError'];0;1;['DataFrame constructor not properly called!'];['ValueError'];0;1;['DataFrame constructor not properly called!'];['ValueError']
206;206;206;206;10.0;2;14507794;;1;89;<python><pandas>;Python Pandas - How to flatten a hierarchical index in columns;47981.0;"[""     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf       \n                                     sum   sum   sum    sum   amax   amin\n0  702730  26451  1993      1    1     1     0    12     13  30.92  24.98\n1  702730  26451  1993      1    2     0     0    13     13  32.00  24.98\n2  702730  26451  1993      1    3     1    10     2     13  23.00   6.98\n3  702730  26451  1993      1    4     1     0    12     13  10.04   3.92\n4  702730  26451  1993      1    5     3     0    10     13  19.94  10.94\n     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf_amax  tmpf_amin   \n0  702730  26451  1993      1    1     1     0    12     13  30.92          24.98\n1  702730  26451  1993      1    2     0     0    13     13  32.00          24.98\n2  702730  26451  1993      1    3     1    10     2     13  23.00          6.98\n3  702730  26451  1993      1    4     1     0    12     13  10.04          3.92\n4  702730  26451  1993      1    5     3     0    10     13  19.94          10.94\n{('USAF', ''): {0: '702730',\n  1: '702730',\n  2: '702730',\n  3: '702730',\n  4: '702730'},\n ('WBAN', ''): {0: '26451', 1: '26451', 2: '26451', 3: '26451', 4: '26451'},\n ('day', ''): {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n ('month', ''): {0: 1, 1: 1, 2: 1, 3: 1, 4: 1},\n ('s_CD', 'sum'): {0: 12.0, 1: 13.0, 2: 2.0, 3: 12.0, 4: 10.0},\n ('s_CL', 'sum'): {0: 0.0, 1: 0.0, 2: 10.0, 3: 0.0, 4: 0.0},\n ('s_CNT', 'sum'): {0: 13.0, 1: 13.0, 2: 13.0, 3: 13.0, 4: 13.0},\n ('s_PC', 'sum'): {0: 1.0, 1: 0.0, 2: 1.0, 3: 1.0, 4: 3.0},\n ('tempf', 'amax'): {0: 30.920000000000002,\n  1: 32.0,\n  2: 23.0,\n  3: 10.039999999999999,\n  4: 19.939999999999998},\n ('tempf', 'amin'): {0: 24.98,\n  1: 24.98,\n  2: 6.9799999999999969,\n  3: 3.9199999999999982,\n  4: 10.940000000000001},\n ('year', ''): {0: 1993, 1: 1993, 2: 1993, 3: 1993, 4: 1993}}\n""]";"['     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf       \n                                     sum   sum   sum    sum   amax   amin\n0  702730  26451  1993      1    1     1     0    12     13  30.92  24.98\n1  702730  26451  1993      1    2     0     0    13     13  32.00  24.98\n2  702730  26451  1993      1    3     1    10     2     13  23.00   6.98\n3  702730  26451  1993      1    4     1     0    12     13  10.04   3.92\n4  702730  26451  1993      1    5     3     0    10     13  19.94  10.94\n', '     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf_amax  tmpf_amin   \n0  702730  26451  1993      1    1     1     0    12     13  30.92          24.98\n1  702730  26451  1993      1    2     0     0    13     13  32.00          24.98\n2  702730  26451  1993      1    3     1    10     2     13  23.00          6.98\n3  702730  26451  1993      1    4     1     0    12     13  10.04          3.92\n4  702730  26451  1993      1    5     3     0    10     13  19.94          10.94\n', ""{('USAF', ''): {0: '702730',\n  1: '702730',\n  2: '702730',\n  3: '702730',\n  4: '702730'},\n ('WBAN', ''): {0: '26451', 1: '26451', 2: '26451', 3: '26451', 4: '26451'},\n ('day', ''): {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n ('month', ''): {0: 1, 1: 1, 2: 1, 3: 1, 4: 1},\n ('s_CD', 'sum'): {0: 12.0, 1: 13.0, 2: 2.0, 3: 12.0, 4: 10.0},\n ('s_CL', 'sum'): {0: 0.0, 1: 0.0, 2: 10.0, 3: 0.0, 4: 0.0},\n ('s_CNT', 'sum'): {0: 13.0, 1: 13.0, 2: 13.0, 3: 13.0, 4: 13.0},\n ('s_PC', 'sum'): {0: 1.0, 1: 0.0, 2: 1.0, 3: 1.0, 4: 3.0},\n ('tempf', 'amax'): {0: 30.920000000000002,\n  1: 32.0,\n  2: 23.0,\n  3: 10.039999999999999,\n  4: 19.939999999999998},\n ('tempf', 'amin'): {0: 24.98,\n  1: 24.98,\n  2: 6.9799999999999969,\n  3: 3.9199999999999982,\n  4: 10.940000000000001},\n ('year', ''): {0: 1993, 1: 1993, 2: 1993, 3: 1993, 4: 1993}}\n""]";"['     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf       \n                                     sum   sum   sum    sum   amax   amin\n0  702730  26451  1993      1    1     1     0    12     13  30.92  24.98\n1  702730  26451  1993      1    2     0     0    13     13  32.00  24.98\n2  702730  26451  1993      1    3     1    10     2     13  23.00   6.98\n3  702730  26451  1993      1    4     1     0    12     13  10.04   3.92\n4  702730  26451  1993      1    5     3     0    10     13  19.94  10.94\n', '     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf_amax  tmpf_amin   \n0  702730  26451  1993      1    1     1     0    12     13  30.92          24.98\n1  702730  26451  1993      1    2     0     0    13     13  32.00          24.98\n2  702730  26451  1993      1    3     1    10     2     13  23.00          6.98\n3  702730  26451  1993      1    4     1     0    12     13  10.04          3.92\n4  702730  26451  1993      1    5     3     0    10     13  19.94          10.94\n', ""{('USAF', ''): {0: '702730',\n  1: '702730',\n  2: '702730',\n  3: '702730',\n  4: '702730'},\n ('WBAN', ''): {0: '26451', 1: '26451', 2: '26451', 3: '26451', 4: '26451'},\n ('day', ''): {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n ('month', ''): {0: 1, 1: 1, 2: 1, 3: 1, 4: 1},\n ('s_CD', 'sum'): {0: 12.0, 1: 13.0, 2: 2.0, 3: 12.0, 4: 10.0},\n ('s_CL', 'sum'): {0: 0.0, 1: 0.0, 2: 10.0, 3: 0.0, 4: 0.0},\n ('s_CNT', 'sum'): {0: 13.0, 1: 13.0, 2: 13.0, 3: 13.0, 4: 13.0},\n ('s_PC', 'sum'): {0: 1.0, 1: 0.0, 2: 1.0, 3: 1.0, 4: 3.0},\n ('tempf', 'amax'): {0: 30.920000000000002,\n  1: 32.0,\n  2: 23.0,\n  3: 10.039999999999999,\n  4: 19.939999999999998},\n ('tempf', 'amin'): {0: 24.98,\n  1: 24.98,\n  2: 6.9799999999999969,\n  3: 3.9199999999999982,\n  4: 10.940000000000001},\n ('year', ''): {0: 1993, 1: 1993, 2: 1993, 3: 1993, 4: 1993}}\n""]";[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess']
207;207;207;207;1.0;0;14529838;;1;59;<python><group-by><aggregate-functions><pandas>;Apply multiple functions to multiple groupby columns;34437.0;"[""In [563]: grouped['D'].agg({'result1' : np.sum,\n   .....:                   'result2' : np.mean})\n   .....:\nOut[563]: \n      result2   result1\nA                      \nbar -0.579846 -1.739537\nfoo -0.280588 -1.402938\ngrouped.agg({'C_sum' : lambda x: x['C'].sum(),\n             'C_std': lambda x: x['C'].std(),\n             'D_sum' : lambda x: x['D'].sum()},\n             'D_sumifC3': lambda x: x['D'][x['C'] == 3].sum(), ...)\n""]";"[""In [563]: grouped['D'].agg({'result1' : np.sum,\n   .....:                   'result2' : np.mean})\n   .....:\nOut[563]: \n      result2   result1\nA                      \nbar -0.579846 -1.739537\nfoo -0.280588 -1.402938\n"", ""grouped.agg({'C_sum' : lambda x: x['C'].sum(),\n             'C_std': lambda x: x['C'].std(),\n             'D_sum' : lambda x: x['D'].sum()},\n             'D_sumifC3': lambda x: x['D'][x['C'] == 3].sum(), ...)\n""]";"[""In [563]: grouped['D'].agg({'result1' : np.sum,\n   .....:                   'result2' : np.mean})\n   .....:\nOut[563]: \n      result2   result1\nA                      \nbar -0.579846 -1.739537\nfoo -0.280588 -1.402938\n"", ""grouped.agg({'C_sum' : lambda x: x['C'].sum(),\n             'C_std': lambda x: x['C'].std(),\n             'D_sum' : lambda x: x['D'].sum()},\n             'D_sumifC3': lambda x: x['D'][x['C'] == 3].sum(), ...)\n"", 'agg']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
208;208;208;208;2.0;0;14539992;;1;13;<python><pandas>;Pandas Drop Rows Outside of Time Range;10697.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'ts' is not defined""]";['NameError'];0;1;"[""name 'ts' is not defined""]";['NameError']
209;209;209;209;1.0;2;14568070;;1;13;<python><hashtable><pandas><cython><importerror>;Pandas installation on Mac OS X: ImportError (cannot import name hashtable);12158.0;"['(pandas)ems ~/.virtualenvs/pandas/localrepo/pandas> python\nPython 2.7.1 (r271:86832, Jul 31 2011, 19:30:53) \n[GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2335.15.00)] on darwin\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n>>> import pandas\n cannot import name hashtable\nTraceback (most recent call last):\nFile ""<stdin>"", line 1, in <module>\nFile ""pandas/__init__.py"", line 6, in <module>\nfrom . import hashtable, tslib, lib\nImportError: cannot import name hashtable\n(pandas)ems ~/.virtualenvs/pandas/localrepo/pandas> which cython\n/Users/EmilyChen/.virtualenvs/pandas/bin/cython\n']";"['(pandas)ems ~/.virtualenvs/pandas/localrepo/pandas> python\nPython 2.7.1 (r271:86832, Jul 31 2011, 19:30:53) \n[GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2335.15.00)] on darwin\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n>>> import pandas\n cannot import name hashtable\nTraceback (most recent call last):\nFile ""<stdin>"", line 1, in <module>\nFile ""pandas/__init__.py"", line 6, in <module>\nfrom . import hashtable, tslib, lib\nImportError: cannot import name hashtable\n', '(pandas)ems ~/.virtualenvs/pandas/localrepo/pandas> which cython\n/Users/EmilyChen/.virtualenvs/pandas/bin/cython\n']";"['mkvirtualenv --no-site-packages pandas', '(pandas)ems ~/.virtualenvs/pandas/localrepo/pandas> ~/anaconda/bin/python setup.py build_ext --inplace', '(pandas)ems ~/.virtualenvs/pandas/localrepo/pandas> ~/anaconda/bin/python setup.py build', '(pandas)ems ~/.virtualenvs/pandas/localrepo/pandas> python\nPython 2.7.1 (r271:86832, Jul 31 2011, 19:30:53) \n[GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2335.15.00)] on darwin\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n>>> import pandas\n cannot import name hashtable\nTraceback (most recent call last):\nFile ""<stdin>"", line 1, in <module>\nFile ""pandas/__init__.py"", line 6, in <module>\nfrom . import hashtable, tslib, lib\nImportError: cannot import name hashtable\n', '(pandas)ems ~/.virtualenvs/pandas/localrepo/pandas> which cython\n/Users/EmilyChen/.virtualenvs/pandas/bin/cython\n']";['from . import hashtable, tslib, lib\n'];['from . import hashtable, tslib, lib\n'];False;['import pandas as pd\nfrom . import hashtable, tslib, lib\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
210;210;210;210;2.0;2;14569223;;1;18;<group-by><dataframe><pandas>;TimeGrouper, pandas;20925.0;"[""6m_return = monthly_return.groupby(TimeGrouper(freq='6M')).aggregate(numpy.sum)\n2008-07-01    0.003626\n2008-08-01    0.001373\n2008-09-01    0.040192\n2008-10-01    0.027794\n2008-11-01    0.012590\n2008-12-01    0.026394\n2009-01-01    0.008564\n2009-02-01    0.007714\n2009-03-01   -0.019727\n2009-04-01    0.008888\n2009-05-01    0.039801\n2009-06-01    0.010042\n2009-07-01    0.020971\n2009-08-01    0.011926\n2009-09-01    0.024998\n2009-10-01    0.005213\n2009-11-01    0.016804\n2009-12-01    0.020724\n2010-01-01    0.006322\n2010-02-01    0.008971\n2010-03-01    0.003911\n2010-04-01    0.013928\n2010-05-01    0.004640\n2010-06-01    0.000744\n2010-07-01    0.004697\n2010-08-01    0.002553\n2010-09-01    0.002770\n2010-10-01    0.002834\n2010-11-01    0.002157\n2010-12-01    0.001034\n2008-07-31    0.003626\n2009-01-31    0.116907\n2009-07-31    0.067688\n2010-01-31    0.085986\n2010-07-31    0.036890\n2011-01-31    0.015283\n2008-12-31    ...\n2009-06-31    ...\n2009-12-31    ...\n2010-06-31    ...\n2010-12-31    ...\n""]";"[""6m_return = monthly_return.groupby(TimeGrouper(freq='6M')).aggregate(numpy.sum)\n"", '2008-07-01    0.003626\n2008-08-01    0.001373\n2008-09-01    0.040192\n2008-10-01    0.027794\n2008-11-01    0.012590\n2008-12-01    0.026394\n2009-01-01    0.008564\n2009-02-01    0.007714\n2009-03-01   -0.019727\n2009-04-01    0.008888\n2009-05-01    0.039801\n2009-06-01    0.010042\n2009-07-01    0.020971\n2009-08-01    0.011926\n2009-09-01    0.024998\n2009-10-01    0.005213\n2009-11-01    0.016804\n2009-12-01    0.020724\n2010-01-01    0.006322\n2010-02-01    0.008971\n2010-03-01    0.003911\n2010-04-01    0.013928\n2010-05-01    0.004640\n2010-06-01    0.000744\n2010-07-01    0.004697\n2010-08-01    0.002553\n2010-09-01    0.002770\n2010-10-01    0.002834\n2010-11-01    0.002157\n2010-12-01    0.001034\n', '2008-07-31    0.003626\n2009-01-31    0.116907\n2009-07-31    0.067688\n2010-01-31    0.085986\n2010-07-31    0.036890\n2011-01-31    0.015283\n', '2008-12-31    ...\n2009-06-31    ...\n2009-12-31    ...\n2010-06-31    ...\n2010-12-31    ...\n']";"['TimeGrouper', 'pandas.tseries.resample', ""6m_return = monthly_return.groupby(TimeGrouper(freq='6M')).aggregate(numpy.sum)\n"", 'monthly_return', '2008-07-01    0.003626\n2008-08-01    0.001373\n2008-09-01    0.040192\n2008-10-01    0.027794\n2008-11-01    0.012590\n2008-12-01    0.026394\n2009-01-01    0.008564\n2009-02-01    0.007714\n2009-03-01   -0.019727\n2009-04-01    0.008888\n2009-05-01    0.039801\n2009-06-01    0.010042\n2009-07-01    0.020971\n2009-08-01    0.011926\n2009-09-01    0.024998\n2009-10-01    0.005213\n2009-11-01    0.016804\n2009-12-01    0.020724\n2010-01-01    0.006322\n2010-02-01    0.008971\n2010-03-01    0.003911\n2010-04-01    0.013928\n2010-05-01    0.004640\n2010-06-01    0.000744\n2010-07-01    0.004697\n2010-08-01    0.002553\n2010-09-01    0.002770\n2010-10-01    0.002834\n2010-11-01    0.002157\n2010-12-01    0.001034\n', '2008-07-31    0.003626\n2009-01-31    0.116907\n2009-07-31    0.067688\n2010-01-31    0.085986\n2010-07-31    0.036890\n2011-01-31    0.015283\n', '6m_return', '2008-12-31    ...\n2009-06-31    ...\n2009-12-31    ...\n2010-06-31    ...\n2010-12-31    ...\n']";[''];[''];False;['import pandas as pd\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
211;211;211;211;3.0;7;14591855;;1;17;<python><pandas>;pandas HDFStore - how to reopen?;6267.0;"['store = pd.HDFStore(\'/home/.../data.h5\')\nstore[\'firstSet\'] = df1\nstore.close()\nstore = pd.HDFStore(\'/home/.../data.h5\')\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/misc/apps/linux/python-2.6.1/lib/python2.6/site-packages/pandas-0.10.0-py2.6-linux-x86_64.egg/pandas/io/pytables.py"", line 207, in __init__\n    self.open(mode=mode, warn=False)\n  File ""/misc/apps/linux/python-2.6.1/lib/python2.6/site-packages/pandas-0.10.0-py2.6-linux-x86_64.egg/pandas/io/pytables.py"", line 302, in open\n    self.handle = _tables().openFile(self.path, self.mode)\n  File ""/apps/linux/python-2.6.1/lib/python2.6/site-packages/tables/file.py"", line 230, in openFile\n    return File(filename, mode, title, rootUEP, filters, **kwargs)\n  File ""/apps/linux/python-2.6.1/lib/python2.6/site-packages/tables/file.py"", line 495, in __init__\n    self._g_new(filename, mode, **params)\n  File ""hdf5Extension.pyx"", line 317, in tables.hdf5Extension.File._g_new (tables/hdf5Extension.c:3039)\ntables.exceptions.HDF5ExtError: HDF5 error back trace\n\n  File ""H5F.c"", line 1582, in H5Fopen\n    unable to open file\n  File ""H5F.c"", line 1373, in H5F_open\n    unable to read superblock\n  File ""H5Fsuper.c"", line 334, in H5F_super_read\n    unable to find file signature\n  File ""H5Fsuper.c"", line 155, in H5F_locate_signature\n    unable to find a valid file signature\n\nEnd of HDF5 error back trace\n\nUnable to open/create file \'/home/.../data.h5\'\n']";"[""store = pd.HDFStore('/home/.../data.h5')\n"", ""store['firstSet'] = df1\nstore.close()\n"", ""store = pd.HDFStore('/home/.../data.h5')\n"", 'Traceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/misc/apps/linux/python-2.6.1/lib/python2.6/site-packages/pandas-0.10.0-py2.6-linux-x86_64.egg/pandas/io/pytables.py"", line 207, in __init__\n    self.open(mode=mode, warn=False)\n  File ""/misc/apps/linux/python-2.6.1/lib/python2.6/site-packages/pandas-0.10.0-py2.6-linux-x86_64.egg/pandas/io/pytables.py"", line 302, in open\n    self.handle = _tables().openFile(self.path, self.mode)\n  File ""/apps/linux/python-2.6.1/lib/python2.6/site-packages/tables/file.py"", line 230, in openFile\n    return File(filename, mode, title, rootUEP, filters, **kwargs)\n  File ""/apps/linux/python-2.6.1/lib/python2.6/site-packages/tables/file.py"", line 495, in __init__\n    self._g_new(filename, mode, **params)\n  File ""hdf5Extension.pyx"", line 317, in tables.hdf5Extension.File._g_new (tables/hdf5Extension.c:3039)\ntables.exceptions.HDF5ExtError: HDF5 error back trace\n\n  File ""H5F.c"", line 1582, in H5Fopen\n    unable to open file\n  File ""H5F.c"", line 1373, in H5F_open\n    unable to read superblock\n  File ""H5Fsuper.c"", line 334, in H5F_super_read\n    unable to find file signature\n  File ""H5Fsuper.c"", line 155, in H5F_locate_signature\n    unable to find a valid file signature\n\nEnd of HDF5 error back trace\n\nUnable to open/create file \'/home/.../data.h5\'\n']";"[""store = pd.HDFStore('/home/.../data.h5')\n"", ""store['firstSet'] = df1\nstore.close()\n"", ""store = pd.HDFStore('/home/.../data.h5')\n"", 'Traceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/misc/apps/linux/python-2.6.1/lib/python2.6/site-packages/pandas-0.10.0-py2.6-linux-x86_64.egg/pandas/io/pytables.py"", line 207, in __init__\n    self.open(mode=mode, warn=False)\n  File ""/misc/apps/linux/python-2.6.1/lib/python2.6/site-packages/pandas-0.10.0-py2.6-linux-x86_64.egg/pandas/io/pytables.py"", line 302, in open\n    self.handle = _tables().openFile(self.path, self.mode)\n  File ""/apps/linux/python-2.6.1/lib/python2.6/site-packages/tables/file.py"", line 230, in openFile\n    return File(filename, mode, title, rootUEP, filters, **kwargs)\n  File ""/apps/linux/python-2.6.1/lib/python2.6/site-packages/tables/file.py"", line 495, in __init__\n    self._g_new(filename, mode, **params)\n  File ""hdf5Extension.pyx"", line 317, in tables.hdf5Extension.File._g_new (tables/hdf5Extension.c:3039)\ntables.exceptions.HDF5ExtError: HDF5 error back trace\n\n  File ""H5F.c"", line 1582, in H5Fopen\n    unable to open file\n  File ""H5F.c"", line 1373, in H5F_open\n    unable to read superblock\n  File ""H5Fsuper.c"", line 334, in H5F_super_read\n    unable to find file signature\n  File ""H5Fsuper.c"", line 155, in H5F_locate_signature\n    unable to find a valid file signature\n\nEnd of HDF5 error back trace\n\nUnable to open/create file \'/home/.../data.h5\'\n']";"[""store = pd.HDFStore('/home/.../data.h5')\nstore['firstSet'] = df1\nstore.close()\nstore = pd.HDFStore('/home/.../data.h5')\n\n\n\n""]";"[""import pandas as pd\nstore = pd.HDFStore('/home/.../data.h5')\nstore['firstSet'] = df1\nstore.close()\nstore = pd.HDFStore('/home/.../data.h5')\n\n\n\n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\ndf1 = pd.DataFrame()\ndf = pd.DataFrame()\nstore = pd.HDFStore('/home/.../data.h5')\nstore['firstSet'] = df1\nstore.close()\nstore = pd.HDFStore('/home/.../data.h5')\n\n\n\n""]";True;0;0;[];[];0;0;[];[];0;0;[];[]
212;212;212;212;1.0;1;14627380;;1;21;<python><html><css><dataframe><pandas>;pandas: HTML output with conditional formatting;9977.0;"['    correlation  p-value\n0   0.5          0.1\n1   0.1          0.8\n2   0.9          *0.01*\nimport pandas as pd\nfrom pandas.core import format\nfrom StringIO import StringIO\nbuf = StringIO()\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\nfmt = format.DataFrameFormatter(df, \n          formatters={\'p_value\':lambda x: ""*%f*"" % x if x<0.05 else str(x)})\nformat.HTMLFormatter(fmt).write_result(buf)\n<table border=""1"" class=""dataframe"">\n  <thead>\n    <tr style=""text-align: right;"">\n      <th></th>\n      <th>correlation</th>\n      <th>p_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td> 0.5</td>\n      <td> 0.10</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td> 0.1</td>\n      <td> 0.80</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td> 0.9</td>\n      <td class=\'significant\'> 0.01</td>\n    </tr>\n  </tbody>\n</table>\nimport pandas as pd\nfrom StringIO import StringIO\nbuf = StringIO()\nsignificant = lambda x: \'<span class=""significant"">%f</span>\' % x if x<0.05 else str(x)\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\ndf.to_html(buf, formatters={\'p_value\': significant})\ndf.to_html(buf, formatters={\'p_value\': significant}, escape=False)\n']";"['    correlation  p-value\n0   0.5          0.1\n1   0.1          0.8\n2   0.9          *0.01*\n', 'import pandas as pd\nfrom pandas.core import format\nfrom StringIO import StringIO\nbuf = StringIO()\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\nfmt = format.DataFrameFormatter(df, \n          formatters={\'p_value\':lambda x: ""*%f*"" % x if x<0.05 else str(x)})\nformat.HTMLFormatter(fmt).write_result(buf)\n', '<table border=""1"" class=""dataframe"">\n  <thead>\n    <tr style=""text-align: right;"">\n      <th></th>\n      <th>correlation</th>\n      <th>p_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td> 0.5</td>\n      <td> 0.10</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td> 0.1</td>\n      <td> 0.80</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td> 0.9</td>\n      <td class=\'significant\'> 0.01</td>\n    </tr>\n  </tbody>\n</table>\n', 'import pandas as pd\nfrom StringIO import StringIO\nbuf = StringIO()\nsignificant = lambda x: \'<span class=""significant"">%f</span>\' % x if x<0.05 else str(x)\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\ndf.to_html(buf, formatters={\'p_value\': significant})\n', ""df.to_html(buf, formatters={'p_value': significant}, escape=False)\n""]";"['    correlation  p-value\n0   0.5          0.1\n1   0.1          0.8\n2   0.9          *0.01*\n', 'import pandas as pd\nfrom pandas.core import format\nfrom StringIO import StringIO\nbuf = StringIO()\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\nfmt = format.DataFrameFormatter(df, \n          formatters={\'p_value\':lambda x: ""*%f*"" % x if x<0.05 else str(x)})\nformat.HTMLFormatter(fmt).write_result(buf)\n', '<td>', '<table border=""1"" class=""dataframe"">\n  <thead>\n    <tr style=""text-align: right;"">\n      <th></th>\n      <th>correlation</th>\n      <th>p_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td> 0.5</td>\n      <td> 0.10</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td> 0.1</td>\n      <td> 0.80</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td> 0.9</td>\n      <td class=\'significant\'> 0.01</td>\n    </tr>\n  </tbody>\n</table>\n', '<span class=""signifcant"">...</span>', 'import pandas as pd\nfrom StringIO import StringIO\nbuf = StringIO()\nsignificant = lambda x: \'<span class=""significant"">%f</span>\' % x if x<0.05 else str(x)\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\ndf.to_html(buf, formatters={\'p_value\': significant})\n', ""df.to_html(buf, formatters={'p_value': significant}, escape=False)\n""]";"['import pandas as pd\nfrom pandas.core import format\nfrom StringIO import StringIO\nbuf = StringIO()\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\nformat.HTMLFormatter(fmt).write_result(buf)\nimport pandas as pd\nfrom StringIO import StringIO\nbuf = StringIO()\nsignificant = lambda x: \'<span class=""significant"">%f</span>\' % x if x<0.05 else str(x)\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\ndf.to_html(buf, formatters={\'p_value\': significant})\ndf.to_html(buf, formatters={\'p_value\': significant}, escape=False)\n']";"['import pandas as pd\nfrom pandas.core import format\nfrom StringIO import StringIO\nbuf = StringIO()\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\nformat.HTMLFormatter(fmt).write_result(buf)\nimport pandas as pd\nfrom StringIO import StringIO\nbuf = StringIO()\nsignificant = lambda x: \'<span class=""significant"">%f</span>\' % x if x<0.05 else str(x)\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\ndf.to_html(buf, formatters={\'p_value\': significant})\ndf.to_html(buf, formatters={\'p_value\': significant}, escape=False)\n']";False;"['import pandas as pd\nimport pandas as pd\nfrom pandas.core import format\nfrom StringIO import StringIO\nbuf = StringIO()\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\nformat.HTMLFormatter(fmt).write_result(buf)\nimport pandas as pd\nfrom StringIO import StringIO\nbuf = StringIO()\nsignificant = lambda x: \'<span class=""significant"">%f</span>\' % x if x<0.05 else str(x)\ndf = pd.DataFrame({\'correlation\':[0.5, 0.1,0.9], \'p_value\':[0.1,0.8,0.01]})\ndf.to_html(buf, formatters={\'p_value\': significant})\ndf.to_html(buf, formatters={\'p_value\': significant}, escape=False)\n']";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
213;213;213;213;3.0;3;14656852;;1;12;<python><numpy><pandas><ipython><rpy2>;How to use pandas dataframes and numpy arrays in Rpy2?;4058.0;"['In: r.hist(df.a)\nOut: \n...\nvectors.pyc in <genexpr>((x,))\n    293         if l < 7:\n    294             s = \'[\' + \\\n--> 295                 \', \'.join((p_str(x, max_width = math.floor(52 / l)) for x in self[ : 8])) +\\\n    296                 \']\'\n    297         else:\n\nvectors.pyc in p_str(x, max_width)\n    287                     res = x\n    288                 else:\n--> 289                     res = ""%s..."" % (str(x[ : (max_width - 3)]))\n    290             return res\n    291 \n\nTypeError: slice indices must be integers or None or have an __index__ method\ncom.convert_to_r_dataframe(mydf) # mydf is a pandas DataFrame\n----> 1 com.convert_to_r_dataframe(mydf)\nin convert_to_r_dataframe(df, strings_as_factors)\n    275     # FIXME: This doesn\'t handle MultiIndex\n    276 \n--> 277     for column in df:\n    278         value = df[column]\n    279         value_type = value.dtype.type\n\nTypeError: iteration over non-sequence\nimport rpy2\nfrom rpy2.robjects import r\nimport rpy2.robjects.numpy2ri\nimport pandas.rpy.common as com\nfrom rpy2.robjects.packages import importr\nfrom rpy2.robjects.lib import grid\nfrom rpy2.robjects.lib import ggplot2\nrpy2.robjects.numpy2ri.activate()\nfrom numpy import *\nimport scipy\n\n# load up pandas df\nimport pandas\ndata = pandas.read_table(""./test.txt"")\n# plotting a column fails\nprint ""data.c2: "", data.c2\nr.plot(data.c2)\n# Conversion and then plotting also fails\nr_df = com.convert_to_r_dataframe(data)\nr.plot(r_df)\nIn [12]: X = np.array([0,1,2,3,4])\n\nIn [13]: Y = np.array([3,5,4,6,7])\nIn [14]: import rpy2\n\nIn [15]: from rpy2.robjects import r\n\nIn [16]: import rpy2.robjects.numpy2ri\n\nIn [17]: import pandas.rpy.common as com\n\nIn [18]: from rpy2.robjects.packages import importr\n\nIn [19]: from rpy2.robjects.lib import grid\n\nIn [20]: from rpy2.robjects.lib import ggplot2\n\n\nIn [21]: rpy2.robjects.numpy2ri.activate()\n\nIn [22]: from numpy import *\n\nIn [23]: import scipy\n\nIn [24]: r.assign(""x"", X)\nOut[24]: \n<Array - Python:0x592ad88 / R:0x6110850>\n[       0,        1,        2,        3,        4]\n\nIn [25]: r.assign(""y"", Y)\n<Array - Python:0x592f5f0 / R:0x61109b8>\n[       3,        5,        4,        6,        7]\n\nIn [27]: %R plot(x,y)\n']";"['In: r.hist(df.a)\nOut: \n...\nvectors.pyc in <genexpr>((x,))\n    293         if l < 7:\n    294             s = \'[\' + \\\n--> 295                 \', \'.join((p_str(x, max_width = math.floor(52 / l)) for x in self[ : 8])) +\\\n    296                 \']\'\n    297         else:\n\nvectors.pyc in p_str(x, max_width)\n    287                     res = x\n    288                 else:\n--> 289                     res = ""%s..."" % (str(x[ : (max_width - 3)]))\n    290             return res\n    291 \n\nTypeError: slice indices must be integers or None or have an __index__ method\n', ""com.convert_to_r_dataframe(mydf) # mydf is a pandas DataFrame\n----> 1 com.convert_to_r_dataframe(mydf)\nin convert_to_r_dataframe(df, strings_as_factors)\n    275     # FIXME: This doesn't handle MultiIndex\n    276 \n--> 277     for column in df:\n    278         value = df[column]\n    279         value_type = value.dtype.type\n\nTypeError: iteration over non-sequence\n"", 'import rpy2\nfrom rpy2.robjects import r\nimport rpy2.robjects.numpy2ri\nimport pandas.rpy.common as com\nfrom rpy2.robjects.packages import importr\nfrom rpy2.robjects.lib import grid\nfrom rpy2.robjects.lib import ggplot2\nrpy2.robjects.numpy2ri.activate()\nfrom numpy import *\nimport scipy\n\n# load up pandas df\nimport pandas\ndata = pandas.read_table(""./test.txt"")\n# plotting a column fails\nprint ""data.c2: "", data.c2\nr.plot(data.c2)\n# Conversion and then plotting also fails\nr_df = com.convert_to_r_dataframe(data)\nr.plot(r_df)\n', 'In [12]: X = np.array([0,1,2,3,4])\n\nIn [13]: Y = np.array([3,5,4,6,7])\nIn [14]: import rpy2\n\nIn [15]: from rpy2.robjects import r\n\nIn [16]: import rpy2.robjects.numpy2ri\n\nIn [17]: import pandas.rpy.common as com\n\nIn [18]: from rpy2.robjects.packages import importr\n\nIn [19]: from rpy2.robjects.lib import grid\n\nIn [20]: from rpy2.robjects.lib import ggplot2\n\n\nIn [21]: rpy2.robjects.numpy2ri.activate()\n\nIn [22]: from numpy import *\n\nIn [23]: import scipy\n\nIn [24]: r.assign(""x"", X)\nOut[24]: \n<Array - Python:0x592ad88 / R:0x6110850>\n[       0,        1,        2,        3,        4]\n\nIn [25]: r.assign(""y"", Y)\n<Array - Python:0x592f5f0 / R:0x61109b8>\n[       3,        5,        4,        6,        7]\n\nIn [27]: %R plot(x,y)\n']";"['df', 'df', 'r.plot', 'In: r.plot(df.a, df.b) # df is pandas DataFrame', 'Out: rpy2.rinterface.NULL', 'a', 'df', 'b', 'r.hist', 'In: r.hist(df.a)\nOut: \n...\nvectors.pyc in <genexpr>((x,))\n    293         if l < 7:\n    294             s = \'[\' + \\\n--> 295                 \', \'.join((p_str(x, max_width = math.floor(52 / l)) for x in self[ : 8])) +\\\n    296                 \']\'\n    297         else:\n\nvectors.pyc in p_str(x, max_width)\n    287                     res = x\n    288                 else:\n--> 289                     res = ""%s..."" % (str(x[ : (max_width - 3)]))\n    290             return res\n    291 \n\nTypeError: slice indices must be integers or None or have an __index__ method\n', 'df', ""com.convert_to_r_dataframe(mydf) # mydf is a pandas DataFrame\n----> 1 com.convert_to_r_dataframe(mydf)\nin convert_to_r_dataframe(df, strings_as_factors)\n    275     # FIXME: This doesn't handle MultiIndex\n    276 \n--> 277     for column in df:\n    278         value = df[column]\n    279         value_type = value.dtype.type\n\nTypeError: iteration over non-sequence\n"", 'import rpy2\nfrom rpy2.robjects import r\nimport rpy2.robjects.numpy2ri\nimport pandas.rpy.common as com\nfrom rpy2.robjects.packages import importr\nfrom rpy2.robjects.lib import grid\nfrom rpy2.robjects.lib import ggplot2\nrpy2.robjects.numpy2ri.activate()\nfrom numpy import *\nimport scipy\n\n# load up pandas df\nimport pandas\ndata = pandas.read_table(""./test.txt"")\n# plotting a column fails\nprint ""data.c2: "", data.c2\nr.plot(data.c2)\n# Conversion and then plotting also fails\nr_df = com.convert_to_r_dataframe(data)\nr.plot(r_df)\n', 'activate()', 'data', 'test.txt', 'plot()', 'rmagic', 'In [12]: X = np.array([0,1,2,3,4])\n\nIn [13]: Y = np.array([3,5,4,6,7])\nIn [14]: import rpy2\n\nIn [15]: from rpy2.robjects import r\n\nIn [16]: import rpy2.robjects.numpy2ri\n\nIn [17]: import pandas.rpy.common as com\n\nIn [18]: from rpy2.robjects.packages import importr\n\nIn [19]: from rpy2.robjects.lib import grid\n\nIn [20]: from rpy2.robjects.lib import ggplot2\n\n\nIn [21]: rpy2.robjects.numpy2ri.activate()\n\nIn [22]: from numpy import *\n\nIn [23]: import scipy\n\nIn [24]: r.assign(""x"", X)\nOut[24]: \n<Array - Python:0x592ad88 / R:0x6110850>\n[       0,        1,        2,        3,        4]\n\nIn [25]: r.assign(""y"", Y)\n<Array - Python:0x592f5f0 / R:0x61109b8>\n[       3,        5,        4,        6,        7]\n\nIn [27]: %R plot(x,y)\n', 'rmagic']";['X = np.array([0,1,2,3,4])\n\n\n\n\n\n\n\n\n\n\n\n\n'];['X = np.array([0,1,2,3,4])\n\n\n\n\n\n\n\n\n\n\n\n\n'];False;['import pandas as pd\nX = np.array([0,1,2,3,4])\n\n\n\n\n\n\n\n\n\n\n\n\n'];False;0;2;"[""name 'r' is not defined"", ""name 'rpy2' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'r' is not defined"", ""name 'rpy2' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'r' is not defined"", ""name 'rpy2' is not defined""]";['NameError', 'NameError']
214;214;214;214;4.0;0;14657241;;1;14;<python><pandas>;How do I get a list of all the duplicate items using pandas in python?;15478.0;"['ID,ENROLLMENT_DATE,TRAINER_MANAGING,TRAINER_OPERATOR,FIRST_VISIT_DATE\n1536D,12-Feb-12,""06DA1B3-Lebanon NH"",,15-Feb-12\nF15D,18-May-12,""06405B2-Lebanon NH"",,25-Jul-12\n8096,8-Aug-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",25-Jun-12\nA036,1-Apr-12,""06CB8CF-Hanover NH"",""06CB8CF-Hanover NH"",9-Aug-12\n8944,19-Feb-12,""06D26AD-Hanover NH"",,4-Feb-12\n1004E,8-Jun-12,""06388B2-Lebanon NH"",,24-Dec-11\n11795,3-Jul-12,""0649597-White River VT"",""0649597-White River VT"",30-Mar-12\n30D7,11-Nov-12,""06D95A3-Hanover NH"",""06D95A3-Hanover NH"",30-Nov-11\n3AE2,21-Feb-12,""06405B2-Lebanon NH"",,26-Oct-12\nB0FE,17-Feb-12,""06D1B9D-Hartland VT"",,16-Feb-12\n127A1,11-Dec-11,""064456E-Hanover NH"",""064456E-Hanover NH"",11-Nov-12\n161FF,20-Feb-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",3-Jul-12\nA036,30-Nov-11,""063B208-Randolph VT"",""063B208-Randolph VT"",\n475B,25-Sep-12,""06D26AD-Hanover NH"",,5-Nov-12\n151A3,7-Mar-12,""06388B2-Lebanon NH"",,16-Nov-12\nCA62,3-Jan-12,,,\nD31B,18-Dec-11,""06405B2-Lebanon NH"",,9-Jan-12\n20F5,8-Jul-12,""0669C50-Randolph VT"",,3-Feb-12\n8096,19-Dec-11,""0649597-White River VT"",""0649597-White River VT"",9-Apr-12\n14E48,1-Aug-12,""06D3206-Hanover NH"",,\n177F8,20-Aug-12,""063B208-Randolph VT"",""063B208-Randolph VT"",5-May-12\n553E,11-Oct-12,""06D95A3-Hanover NH"",""06D95A3-Hanover NH"",8-Mar-12\n12D5F,18-Jul-12,""0649597-White River VT"",""0649597-White River VT"",2-Nov-12\nC6DC,13-Apr-12,""06388B2-Lebanon NH"",,\n11795,27-Feb-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",19-Jun-12\n17B43,11-Aug-12,,,22-Oct-12\nA036,11-Aug-12,""06D3206-Hanover NH"",,19-Jun-12\ndf_bigdata_duplicates = df_bigdata[df_bigdata.duplicated(cols=\'ID\')]\n']";"['ID,ENROLLMENT_DATE,TRAINER_MANAGING,TRAINER_OPERATOR,FIRST_VISIT_DATE\n1536D,12-Feb-12,""06DA1B3-Lebanon NH"",,15-Feb-12\nF15D,18-May-12,""06405B2-Lebanon NH"",,25-Jul-12\n8096,8-Aug-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",25-Jun-12\nA036,1-Apr-12,""06CB8CF-Hanover NH"",""06CB8CF-Hanover NH"",9-Aug-12\n8944,19-Feb-12,""06D26AD-Hanover NH"",,4-Feb-12\n1004E,8-Jun-12,""06388B2-Lebanon NH"",,24-Dec-11\n11795,3-Jul-12,""0649597-White River VT"",""0649597-White River VT"",30-Mar-12\n30D7,11-Nov-12,""06D95A3-Hanover NH"",""06D95A3-Hanover NH"",30-Nov-11\n3AE2,21-Feb-12,""06405B2-Lebanon NH"",,26-Oct-12\nB0FE,17-Feb-12,""06D1B9D-Hartland VT"",,16-Feb-12\n127A1,11-Dec-11,""064456E-Hanover NH"",""064456E-Hanover NH"",11-Nov-12\n161FF,20-Feb-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",3-Jul-12\nA036,30-Nov-11,""063B208-Randolph VT"",""063B208-Randolph VT"",\n475B,25-Sep-12,""06D26AD-Hanover NH"",,5-Nov-12\n151A3,7-Mar-12,""06388B2-Lebanon NH"",,16-Nov-12\nCA62,3-Jan-12,,,\nD31B,18-Dec-11,""06405B2-Lebanon NH"",,9-Jan-12\n20F5,8-Jul-12,""0669C50-Randolph VT"",,3-Feb-12\n8096,19-Dec-11,""0649597-White River VT"",""0649597-White River VT"",9-Apr-12\n14E48,1-Aug-12,""06D3206-Hanover NH"",,\n177F8,20-Aug-12,""063B208-Randolph VT"",""063B208-Randolph VT"",5-May-12\n553E,11-Oct-12,""06D95A3-Hanover NH"",""06D95A3-Hanover NH"",8-Mar-12\n12D5F,18-Jul-12,""0649597-White River VT"",""0649597-White River VT"",2-Nov-12\nC6DC,13-Apr-12,""06388B2-Lebanon NH"",,\n11795,27-Feb-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",19-Jun-12\n17B43,11-Aug-12,,,22-Oct-12\nA036,11-Aug-12,""06D3206-Hanover NH"",,19-Jun-12\n', ""df_bigdata_duplicates = df_bigdata[df_bigdata.duplicated(cols='ID')]\n""]";"['ID,ENROLLMENT_DATE,TRAINER_MANAGING,TRAINER_OPERATOR,FIRST_VISIT_DATE\n1536D,12-Feb-12,""06DA1B3-Lebanon NH"",,15-Feb-12\nF15D,18-May-12,""06405B2-Lebanon NH"",,25-Jul-12\n8096,8-Aug-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",25-Jun-12\nA036,1-Apr-12,""06CB8CF-Hanover NH"",""06CB8CF-Hanover NH"",9-Aug-12\n8944,19-Feb-12,""06D26AD-Hanover NH"",,4-Feb-12\n1004E,8-Jun-12,""06388B2-Lebanon NH"",,24-Dec-11\n11795,3-Jul-12,""0649597-White River VT"",""0649597-White River VT"",30-Mar-12\n30D7,11-Nov-12,""06D95A3-Hanover NH"",""06D95A3-Hanover NH"",30-Nov-11\n3AE2,21-Feb-12,""06405B2-Lebanon NH"",,26-Oct-12\nB0FE,17-Feb-12,""06D1B9D-Hartland VT"",,16-Feb-12\n127A1,11-Dec-11,""064456E-Hanover NH"",""064456E-Hanover NH"",11-Nov-12\n161FF,20-Feb-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",3-Jul-12\nA036,30-Nov-11,""063B208-Randolph VT"",""063B208-Randolph VT"",\n475B,25-Sep-12,""06D26AD-Hanover NH"",,5-Nov-12\n151A3,7-Mar-12,""06388B2-Lebanon NH"",,16-Nov-12\nCA62,3-Jan-12,,,\nD31B,18-Dec-11,""06405B2-Lebanon NH"",,9-Jan-12\n20F5,8-Jul-12,""0669C50-Randolph VT"",,3-Feb-12\n8096,19-Dec-11,""0649597-White River VT"",""0649597-White River VT"",9-Apr-12\n14E48,1-Aug-12,""06D3206-Hanover NH"",,\n177F8,20-Aug-12,""063B208-Randolph VT"",""063B208-Randolph VT"",5-May-12\n553E,11-Oct-12,""06D95A3-Hanover NH"",""06D95A3-Hanover NH"",8-Mar-12\n12D5F,18-Jul-12,""0649597-White River VT"",""0649597-White River VT"",2-Nov-12\nC6DC,13-Apr-12,""06388B2-Lebanon NH"",,\n11795,27-Feb-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",19-Jun-12\n17B43,11-Aug-12,,,22-Oct-12\nA036,11-Aug-12,""06D3206-Hanover NH"",,19-Jun-12\n', ""df_bigdata_duplicates = df_bigdata[df_bigdata.duplicated(cols='ID')]\n""]";"['ID,ENROLLMENT_DATE,TRAINER_MANAGING,TRAINER_OPERATOR,FIRST_VISIT_DATE\n8096,8-Aug-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",25-Jun-12\nA036,1-Apr-12,""06CB8CF-Hanover NH"",""06CB8CF-Hanover NH"",9-Aug-12\n11795,3-Jul-12,""0649597-White River VT"",""0649597-White River VT"",30-Mar-12\nA036,30-Nov-11,""063B208-Randolph VT"",""063B208-Randolph VT"",\n8096,19-Dec-11,""0649597-White River VT"",""0649597-White River VT"",9-Apr-12\n11795,27-Feb-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",19-Jun-12\ndf_bigdata_duplicates = df_bigdata[df_bigdata.duplicated(cols=\'ID\')]\n']";"['ID,ENROLLMENT_DATE,TRAINER_MANAGING,TRAINER_OPERATOR,FIRST_VISIT_DATE\n8096,8-Aug-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",25-Jun-12\nA036,1-Apr-12,""06CB8CF-Hanover NH"",""06CB8CF-Hanover NH"",9-Aug-12\n11795,3-Jul-12,""0649597-White River VT"",""0649597-White River VT"",30-Mar-12\nA036,30-Nov-11,""063B208-Randolph VT"",""063B208-Randolph VT"",\n8096,19-Dec-11,""0649597-White River VT"",""0649597-White River VT"",9-Apr-12\n11795,27-Feb-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",19-Jun-12\ndf_bigdata_duplicates = df_bigdata[df_bigdata.duplicated(cols=\'ID\')]\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\nID,ENROLLMENT_DATE,TRAINER_MANAGING,TRAINER_OPERATOR,FIRST_VISIT_DATE\n8096,8-Aug-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",25-Jun-12\nA036,1-Apr-12,""06CB8CF-Hanover NH"",""06CB8CF-Hanover NH"",9-Aug-12\n11795,3-Jul-12,""0649597-White River VT"",""0649597-White River VT"",30-Mar-12\nA036,30-Nov-11,""063B208-Randolph VT"",""063B208-Randolph VT"",\n8096,19-Dec-11,""0649597-White River VT"",""0649597-White River VT"",9-Apr-12\n11795,27-Feb-12,""0643D38-Hanover NH"",""0643D38-Hanover NH"",19-Jun-12\ndf_bigdata_duplicates = df_bigdata[df_bigdata.duplicated(cols=\'ID\')]\n']";True;1;3;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;['Sucess', 'not enough values to unpack (expected 2, got 0)', 'not enough values to unpack (expected 2, got 0)'];['Sucess', 'ValueError', 'ValueError']
215;215;215;215;5.0;0;14661701;;1;133;<python><pandas>;How to drop a list of rows from Pandas dataframe?;202455.0;['>>> df\n                  sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                     \n600141 20060331   2.709       NaN      2.709   2.245\n       20060630   6.590       NaN      6.590   5.291\n       20060930  10.103       NaN     10.103   7.981\n       20061231  15.915       NaN     15.915  12.686\n       20070331   3.196       NaN      3.196   2.710\n       20070630   7.907       NaN      7.907   6.459\n                  sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                     \n600141 20060331   2.709       NaN      2.709   2.245\n       20061231  15.915       NaN     15.915  12.686\n       20070630   7.907       NaN      7.907   6.459\n'];['>>> df\n                  sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                     \n600141 20060331   2.709       NaN      2.709   2.245\n       20060630   6.590       NaN      6.590   5.291\n       20060930  10.103       NaN     10.103   7.981\n       20061231  15.915       NaN     15.915  12.686\n       20070331   3.196       NaN      3.196   2.710\n       20070630   7.907       NaN      7.907   6.459\n', '                  sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                     \n600141 20060331   2.709       NaN      2.709   2.245\n       20061231  15.915       NaN     15.915  12.686\n       20070630   7.907       NaN      7.907   6.459\n'];['>>> df\n                  sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                     \n600141 20060331   2.709       NaN      2.709   2.245\n       20060630   6.590       NaN      6.590   5.291\n       20060930  10.103       NaN     10.103   7.981\n       20061231  15.915       NaN     15.915  12.686\n       20070331   3.196       NaN      3.196   2.710\n       20070630   7.907       NaN      7.907   6.459\n', '[1,2,4],', '                  sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                     \n600141 20060331   2.709       NaN      2.709   2.245\n       20061231  15.915       NaN     15.915  12.686\n       20070630   7.907       NaN      7.907   6.459\n'];[''];[''];False;['import pandas as pd\n'];False;1;4;"[""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'NameError', 'Sucess', 'NameError'];1;4;"[""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'NameError', 'Sucess', 'NameError'];1;4;"['index 1 is out of bounds for axis 1 with size 0', 'index 1 is out of bounds for axis 1 with size 0', 'Sucess', ""name 'indexes_to_drop' is not defined""]";['IndexError', 'IndexError', 'Sucess', 'NameError']
216;216;216;216;2.0;4;14663004;;1;53;<pandas>;How to get the last n row of pandas dataframe?;73792.0;['>>> df1\n    STK_ID  RPT_Date  TClose   sales  discount\n0   000568  20060331    3.69   5.975       NaN\n1   000568  20060630    9.14  10.143       NaN\n2   000568  20060930    9.49  13.854       NaN\n3   000568  20061231   15.84  19.262       NaN\n4   000568  20070331   17.00   6.803       NaN\n5   000568  20070630   26.31  12.940       NaN\n6   000568  20070930   39.12  19.977       NaN\n7   000568  20071231   45.94  29.269       NaN\n8   000568  20080331   38.75  12.668       NaN\n9   000568  20080630   30.09  21.102       NaN\n10  000568  20080930   26.00  30.769       NaN\n\n>>> df2\n                 TClose   sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                             \n000568 20060331    3.69   5.975       NaN      5.975   2.591\n       20060630    9.14  10.143       NaN     10.143   4.363\n       20060930    9.49  13.854       NaN     13.854   5.901\n       20061231   15.84  19.262       NaN     19.262   8.407\n       20070331   17.00   6.803       NaN      6.803   2.815\n       20070630   26.31  12.940       NaN     12.940   5.418\n       20070930   39.12  19.977       NaN     19.977   8.452\n       20071231   45.94  29.269       NaN     29.269  12.606\n       20080331   38.75  12.668       NaN     12.668   3.958\n       20080630   30.09  21.102       NaN     21.102   7.431\n>>> df2.ix[-3:]\n                 TClose   sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                             \n000568 20071231   45.94  29.269       NaN     29.269  12.606\n       20080331   38.75  12.668       NaN     12.668   3.958\n       20080630   30.09  21.102       NaN     21.102   7.431\n>>> df1.ix[-3:]\n    STK_ID  RPT_Date  TClose   sales  discount\n0   000568  20060331    3.69   5.975       NaN\n1   000568  20060630    9.14  10.143       NaN\n2   000568  20060930    9.49  13.854       NaN\n3   000568  20061231   15.84  19.262       NaN\n4   000568  20070331   17.00   6.803       NaN\n5   000568  20070630   26.31  12.940       NaN\n6   000568  20070930   39.12  19.977       NaN\n7   000568  20071231   45.94  29.269       NaN\n8   000568  20080331   38.75  12.668       NaN\n9   000568  20080630   30.09  21.102       NaN\n10  000568  20080930   26.00  30.769       NaN\n'];['>>> df1\n    STK_ID  RPT_Date  TClose   sales  discount\n0   000568  20060331    3.69   5.975       NaN\n1   000568  20060630    9.14  10.143       NaN\n2   000568  20060930    9.49  13.854       NaN\n3   000568  20061231   15.84  19.262       NaN\n4   000568  20070331   17.00   6.803       NaN\n5   000568  20070630   26.31  12.940       NaN\n6   000568  20070930   39.12  19.977       NaN\n7   000568  20071231   45.94  29.269       NaN\n8   000568  20080331   38.75  12.668       NaN\n9   000568  20080630   30.09  21.102       NaN\n10  000568  20080930   26.00  30.769       NaN\n\n>>> df2\n                 TClose   sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                             \n000568 20060331    3.69   5.975       NaN      5.975   2.591\n       20060630    9.14  10.143       NaN     10.143   4.363\n       20060930    9.49  13.854       NaN     13.854   5.901\n       20061231   15.84  19.262       NaN     19.262   8.407\n       20070331   17.00   6.803       NaN      6.803   2.815\n       20070630   26.31  12.940       NaN     12.940   5.418\n       20070930   39.12  19.977       NaN     19.977   8.452\n       20071231   45.94  29.269       NaN     29.269  12.606\n       20080331   38.75  12.668       NaN     12.668   3.958\n       20080630   30.09  21.102       NaN     21.102   7.431\n', '>>> df2.ix[-3:]\n                 TClose   sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                             \n000568 20071231   45.94  29.269       NaN     29.269  12.606\n       20080331   38.75  12.668       NaN     12.668   3.958\n       20080630   30.09  21.102       NaN     21.102   7.431\n', '>>> df1.ix[-3:]\n    STK_ID  RPT_Date  TClose   sales  discount\n0   000568  20060331    3.69   5.975       NaN\n1   000568  20060630    9.14  10.143       NaN\n2   000568  20060930    9.49  13.854       NaN\n3   000568  20061231   15.84  19.262       NaN\n4   000568  20070331   17.00   6.803       NaN\n5   000568  20070630   26.31  12.940       NaN\n6   000568  20070930   39.12  19.977       NaN\n7   000568  20071231   45.94  29.269       NaN\n8   000568  20080331   38.75  12.668       NaN\n9   000568  20080630   30.09  21.102       NaN\n10  000568  20080930   26.00  30.769       NaN\n'];['df1', 'df2', '>>> df1\n    STK_ID  RPT_Date  TClose   sales  discount\n0   000568  20060331    3.69   5.975       NaN\n1   000568  20060630    9.14  10.143       NaN\n2   000568  20060930    9.49  13.854       NaN\n3   000568  20061231   15.84  19.262       NaN\n4   000568  20070331   17.00   6.803       NaN\n5   000568  20070630   26.31  12.940       NaN\n6   000568  20070930   39.12  19.977       NaN\n7   000568  20071231   45.94  29.269       NaN\n8   000568  20080331   38.75  12.668       NaN\n9   000568  20080630   30.09  21.102       NaN\n10  000568  20080930   26.00  30.769       NaN\n\n>>> df2\n                 TClose   sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                             \n000568 20060331    3.69   5.975       NaN      5.975   2.591\n       20060630    9.14  10.143       NaN     10.143   4.363\n       20060930    9.49  13.854       NaN     13.854   5.901\n       20061231   15.84  19.262       NaN     19.262   8.407\n       20070331   17.00   6.803       NaN      6.803   2.815\n       20070630   26.31  12.940       NaN     12.940   5.418\n       20070930   39.12  19.977       NaN     19.977   8.452\n       20071231   45.94  29.269       NaN     29.269  12.606\n       20080331   38.75  12.668       NaN     12.668   3.958\n       20080630   30.09  21.102       NaN     21.102   7.431\n', '>>> df2.ix[-3:]\n                 TClose   sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                             \n000568 20071231   45.94  29.269       NaN     29.269  12.606\n       20080331   38.75  12.668       NaN     12.668   3.958\n       20080630   30.09  21.102       NaN     21.102   7.431\n', 'df1.ix[-3:]', '>>> df1.ix[-3:]\n    STK_ID  RPT_Date  TClose   sales  discount\n0   000568  20060331    3.69   5.975       NaN\n1   000568  20060630    9.14  10.143       NaN\n2   000568  20060930    9.49  13.854       NaN\n3   000568  20061231   15.84  19.262       NaN\n4   000568  20070331   17.00   6.803       NaN\n5   000568  20070630   26.31  12.940       NaN\n6   000568  20070930   39.12  19.977       NaN\n7   000568  20071231   45.94  29.269       NaN\n8   000568  20080331   38.75  12.668       NaN\n9   000568  20080630   30.09  21.102       NaN\n10  000568  20080930   26.00  30.769       NaN\n', 'df1'];['\n'];['\n'];False;['import pandas as pd\n\n'];False;1;2;"[""name 'df1' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df1' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""'DataFrame' object has no attribute 'irow'"", 'Sucess']";['AttributeError', 'Sucess']
217;217;217;217;5.0;0;14688306;;1;35;<python><pandas>;Adding meta-information/metadata to pandas DataFrame;9816.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess']
218;218;218;218;4.0;2;14714181;;1;14;<python><pandas>;Conditional Logic on Pandas DataFrame;37489.0;['   data desired_output\n0     1          False\n1     2          False\n2     3           True\n3     4           True\n'];['   data desired_output\n0     1          False\n1     2          False\n2     3           True\n3     4           True\n'];['   data desired_output\n0     1          False\n1     2          False\n2     3           True\n3     4           True\n'];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'df' is not defined"", ""name 'pandas' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'pandas' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""'data'"", ""name 'pandas' is not defined"", 'Wrong number of items passed 0, placement implies 1']";['KeyError', 'NameError', 'ValueError']
219;219;219;219;1.0;0;14733871;;1;42;<python><sorting><pandas><multi-index>;Multi Index Sorting in Pandas;17418.0;['    Group1    Group2\n    A B C     A B C\n1   1 0 3     2 5 7\n2   5 6 9     1 0 0\n3   7 0 2     0 3 5 \n    Group1    Group2\n    A B C     A B C\n 2  5 6 9     1 0 0\n 1  1 0 3     2 5 7\n 3  7 0 2     0 3 5 \n'];['    Group1    Group2\n    A B C     A B C\n1   1 0 3     2 5 7\n2   5 6 9     1 0 0\n3   7 0 2     0 3 5 \n', '    Group1    Group2\n    A B C     A B C\n 2  5 6 9     1 0 0\n 1  1 0 3     2 5 7\n 3  7 0 2     0 3 5 \n'];['    Group1    Group2\n    A B C     A B C\n1   1 0 3     2 5 7\n2   5 6 9     1 0 0\n3   7 0 2     0 3 5 \n', '    Group1    Group2\n    A B C     A B C\n 2  5 6 9     1 0 0\n 1  1 0 3     2 5 7\n 3  7 0 2     0 3 5 \n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'DataFrame' object has no attribute 'sort'""]";['AttributeError']
220;220;220;220;5.0;0;14734533;;1;71;<python><group-by><dataframe><pandas>;How to access pandas groupby dataframe by key;77177.0;"[""rand = np.random.RandomState(1)\ndf = pd.DataFrame({'A': ['foo', 'bar'] * 3,\n                   'B': rand.randn(6),\n                   'C': rand.randint(0, 20, 6)})\ngb = df.groupby(['A'])\nIn [11]: for k, gp in gb:\n             print 'key=' + str(k)\n             print gp\nkey=bar\n     A         B   C\n1  bar -0.611756  18\n3  bar -1.072969  10\n5  bar -2.301539  18\nkey=foo\n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\nIn [12]: gb['foo']\nOut[12]:  \n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\nIn [13]: def gb_df_key(gb, key, orig_df):\n             ix = gb.indices[key]\n             return orig_df.ix[ix]\n\n         gb_df_key(gb, 'foo', df)\nOut[13]:\n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14  \n""]";"[""rand = np.random.RandomState(1)\ndf = pd.DataFrame({'A': ['foo', 'bar'] * 3,\n                   'B': rand.randn(6),\n                   'C': rand.randint(0, 20, 6)})\ngb = df.groupby(['A'])\n"", ""In [11]: for k, gp in gb:\n             print 'key=' + str(k)\n             print gp\nkey=bar\n     A         B   C\n1  bar -0.611756  18\n3  bar -1.072969  10\n5  bar -2.301539  18\nkey=foo\n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n"", ""In [12]: gb['foo']\nOut[12]:  \n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n"", ""In [13]: def gb_df_key(gb, key, orig_df):\n             ix = gb.indices[key]\n             return orig_df.ix[ix]\n\n         gb_df_key(gb, 'foo', df)\nOut[13]:\n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14  \n""]";"[""rand = np.random.RandomState(1)\ndf = pd.DataFrame({'A': ['foo', 'bar'] * 3,\n                   'B': rand.randn(6),\n                   'C': rand.randint(0, 20, 6)})\ngb = df.groupby(['A'])\n"", ""In [11]: for k, gp in gb:\n             print 'key=' + str(k)\n             print gp\nkey=bar\n     A         B   C\n1  bar -0.611756  18\n3  bar -1.072969  10\n5  bar -2.301539  18\nkey=foo\n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n"", ""In [12]: gb['foo']\nOut[12]:  \n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n"", ""gb[('foo',)]"", 'pandas.core.groupby.DataFrameGroupBy', ""In [13]: def gb_df_key(gb, key, orig_df):\n             ix = gb.indices[key]\n             return orig_df.ix[ix]\n\n         gb_df_key(gb, 'foo', df)\nOut[13]:\n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14  \n""]";['key=bar\nkey=foo\n\n'];['key=bar\nkey=foo\n\n'];False;['import pandas as pd\nkey=bar\nkey=foo\n\n'];False;0;3;"[""name 'gb' is not defined"", ""name 'gb' is not defined"", ""name 'gb' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'gb' is not defined"", ""name 'gb' is not defined"", ""name 'gb' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'gb' is not defined"", ""name 'gb' is not defined"", ""name 'gb' is not defined""]";['NameError', 'NameError', 'NameError']
221;221;221;221;2.0;0;14744068;;1;22;<python><pandas>;Prepend a level to a pandas MultiIndex;16301.0;"[""import numpy as np\nimport pandas as p\nfrom numpy.random import randn\n\ndf = p.DataFrame({\n    'A' : ['a1', 'a1', 'a2', 'a3']\n  , 'B' : ['b1', 'b2', 'b3', 'b4']\n  , 'Vals' : randn(4)\n}).groupby(['A', 'B']).sum()\n\ndf\n\nOutput>            Vals\nOutput> A  B           \nOutput> a1 b1 -1.632460\nOutput>    b2  0.596027\nOutput> a2 b3 -0.619130\nOutput> a3 b4 -0.002009\nOutput>                       Vals\nOutput> FirstLevel A  B           \nOutput> Foo        a1 b1 -1.632460\nOutput>               b2  0.596027\nOutput>            a2 b3 -0.619130\nOutput>            a3 b4 -0.002009\n""]";"[""import numpy as np\nimport pandas as p\nfrom numpy.random import randn\n\ndf = p.DataFrame({\n    'A' : ['a1', 'a1', 'a2', 'a3']\n  , 'B' : ['b1', 'b2', 'b3', 'b4']\n  , 'Vals' : randn(4)\n}).groupby(['A', 'B']).sum()\n\ndf\n\nOutput>            Vals\nOutput> A  B           \nOutput> a1 b1 -1.632460\nOutput>    b2  0.596027\nOutput> a2 b3 -0.619130\nOutput> a3 b4 -0.002009\n"", 'Output>                       Vals\nOutput> FirstLevel A  B           \nOutput> Foo        a1 b1 -1.632460\nOutput>               b2  0.596027\nOutput>            a2 b3 -0.619130\nOutput>            a3 b4 -0.002009\n']";"[""import numpy as np\nimport pandas as p\nfrom numpy.random import randn\n\ndf = p.DataFrame({\n    'A' : ['a1', 'a1', 'a2', 'a3']\n  , 'B' : ['b1', 'b2', 'b3', 'b4']\n  , 'Vals' : randn(4)\n}).groupby(['A', 'B']).sum()\n\ndf\n\nOutput>            Vals\nOutput> A  B           \nOutput> a1 b1 -1.632460\nOutput>    b2  0.596027\nOutput> a2 b3 -0.619130\nOutput> a3 b4 -0.002009\n"", 'Output>                       Vals\nOutput> FirstLevel A  B           \nOutput> Foo        a1 b1 -1.632460\nOutput>               b2  0.596027\nOutput>            a2 b3 -0.619130\nOutput>            a3 b4 -0.002009\n']";['import numpy as np\nimport pandas as p\nfrom numpy.random import randn\n\n\ndf\n\nOutput>            Vals\nOutput>                       Vals\n'];['import numpy as np\nimport pandas as p\nfrom numpy.random import randn\n\n\ndf\n\nOutput>            Vals\nOutput>                       Vals\n'];False;['import pandas as pd\ndf = pd.DataFrame()\nimport numpy as np\nimport pandas as p\nfrom numpy.random import randn\n\n\ndf\n\nOutput>            Vals\nOutput>                       Vals\n'];True;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;"[""'Level A not found'"", 'Sucess']";['KeyError', 'Sucess']
222;222;222;222;4.0;1;14745022;;1;42;<python><dataframe><pandas>;Pandas DataFrame, how do i split a column into two;56622.0;['          row\n0    00000 UNITED STATES\n1    01000 ALABAMA\n2    01001 Autauga County, AL\n3    01003 Baldwin County, AL\n4    01005 Barbour County, AL\n         fips       row\n0    00000 UNITED STATES\n1    01000 ALABAMA \n2    01001 Autauga County, AL\n3    01003 Baldwin County, AL\n4    01005 Barbour County, AL\n'];['          row\n0    00000 UNITED STATES\n1    01000 ALABAMA\n2    01001 Autauga County, AL\n3    01003 Baldwin County, AL\n4    01005 Barbour County, AL\n', '         fips       row\n0    00000 UNITED STATES\n1    01000 ALABAMA \n2    01001 Autauga County, AL\n3    01003 Baldwin County, AL\n4    01005 Barbour County, AL\n'];"[""fips'"", ""'row'"", '          row\n0    00000 UNITED STATES\n1    01000 ALABAMA\n2    01001 Autauga County, AL\n3    01003 Baldwin County, AL\n4    01005 Barbour County, AL\n', 'df.row.str[:]', ""df['fips'] = hello"", '         fips       row\n0    00000 UNITED STATES\n1    01000 ALABAMA \n2    01001 Autauga County, AL\n3    01003 Baldwin County, AL\n4    01005 Barbour County, AL\n']";[''];[''];False;['import pandas as pd\n'];False;1;4;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError', 'NameError'];1;4;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError', 'NameError'];1;4;"['Sucess', ""'DataFrame' object has no attribute 'row'"", ""'row'"", ""'AB'""]";['Sucess', 'AttributeError', 'KeyError', 'KeyError']
223;223;223;223;2.0;1;14808945;;1;34;<python><pandas>;check if variable is dataframe;21988.0;"['def f(var):\nif var == pd.DataFrame():\n    print ""do stuff""\ndef f(var):\nif var.values != None:\n    print ""do stuff""\n']";"['def f(var):\nif var == pd.DataFrame():\n    print ""do stuff""\n', 'def f(var):\nif var.values != None:\n    print ""do stuff""\n']";"['def f(var):\nif var == pd.DataFrame():\n    print ""do stuff""\n', 'def f(var):\nif var.values != None:\n    print ""do stuff""\n']";[''];[''];False;['import pandas as pd\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
224;224;224;224;2.0;0;14813289;;1;11;<python><pandas><scikit-learn>;Keep pandas structure with numpy/scikit functions;2305.0;"['In [31]: data = pandas.read_csv(""lala.csv"", delimiter="","")\n\nIn [32]: data\nOut[32]: \n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nfrom sklearn import preprocessing\npreprocessing.scale(data)\n']";"['In [31]: data = pandas.read_csv(""lala.csv"", delimiter="","")\n\nIn [32]: data\nOut[32]: \n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\n', 'from sklearn import preprocessing\npreprocessing.scale(data)\n']";"['read_csv()', 'In [31]: data = pandas.read_csv(""lala.csv"", delimiter="","")\n\nIn [32]: data\nOut[32]: \n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\n', 'from sklearn import preprocessing\npreprocessing.scale(data)\n']";"['data = pandas.read_csv(""lala.csv"", delimiter="","")\n\n']";"['data = pandas.read_csv(""lala.csv"", delimiter="","")\n\n']";False;"['import pandas as pd\ndata = pandas.read_csv(""lala.csv"", delimiter="","")\n\n']";False;0;2;"[""name 'np' is not defined"", ""name 'preprocessing' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""name 'preprocessing' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""name 'preprocessing' is not defined""]";['NameError', 'NameError']
225;225;225;225;2.0;0;14822680;;1;14;<python><python-3.x><python-2.7><pandas>;convert python dataframe to list;54043.0;"[""  LogBlk    Page                                    BayFail       \n  0          0                                 [0, 1, 8, 9]  \n  1          16           [0, 1, 4, 5, 6, 8, 9, 12, 13, 14]  \n  2          32           [0, 1, 4, 5, 6, 8, 9, 12, 13, 14]  \n  3          48           [0, 1, 4, 5, 6, 8, 9, 12, 13, 14]  \ndf2 = df[ (df['Page'] == 16) & (df['LogBlk'] == 0) ]['BayFail']\n""]";"['  LogBlk    Page                                    BayFail       \n  0          0                                 [0, 1, 8, 9]  \n  1          16           [0, 1, 4, 5, 6, 8, 9, 12, 13, 14]  \n  2          32           [0, 1, 4, 5, 6, 8, 9, 12, 13, 14]  \n  3          48           [0, 1, 4, 5, 6, 8, 9, 12, 13, 14]  \n', ""df2 = df[ (df['Page'] == 16) & (df['LogBlk'] == 0) ]['BayFail']\n""]";"['  LogBlk    Page                                    BayFail       \n  0          0                                 [0, 1, 8, 9]  \n  1          16           [0, 1, 4, 5, 6, 8, 9, 12, 13, 14]  \n  2          32           [0, 1, 4, 5, 6, 8, 9, 12, 13, 14]  \n  3          48           [0, 1, 4, 5, 6, 8, 9, 12, 13, 14]  \n', ""df2 = df[ (df['Page'] == 16) & (df['LogBlk'] == 0) ]['BayFail']\n""]";"[""df2 = df[ (df['Page'] == 16) & (df['LogBlk'] == 0) ]['BayFail']\n""]";"[""df2 = df[ (df['Page'] == 16) & (df['LogBlk'] == 0) ]['BayFail']\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf2 = df[ (df['Page'] == 16) & (df['LogBlk'] == 0) ]['BayFail']\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
226;226;226;226;2.0;2;14824456;;1;17;<python><matplotlib><pandas><histogram><bar-chart>;Edit the width of bars using dataframe.plot() function in matplotlib;7537.0;"[""DataFrame.plot(kind='bar',stacked=True)\n""]";"[""DataFrame.plot(kind='bar',stacked=True)\n""]";"[""DataFrame.plot(kind='bar',stacked=True)\n""]";"[""DataFrame.plot(kind='bar',stacked=True)\n""]";"[""DataFrame.plot(kind='bar',stacked=True)\n""]";False;"[""import pandas as pd\nDataFrame.plot(kind='bar',stacked=True)\n""]";False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name '_converter' is not defined"", ""name '_converter' is not defined""]";['NameError', 'NameError']
227;227;227;227;1.0;1;14861023;;1;14;<pandas>;Resampling Minute data;17128.0;"[""# Extract data for regular trading hours (rth) from the 24 hour data set\nrth = data.between_time(start_time = '09:30:00', end_time = '16:15:00', include_end = False)\n\n# Extract data for extended trading hours (eth) from the 24 hour data set\neth = data.between_time(start_time = '16:30:00', end_time = '09:30:00', include_end = False)\n\n# Extract data for initial balance (rth) from the 24 hour data set\ninitial_balance = data.between_time(start_time = '09:30:00', end_time = '10:30:00', include_end =      False)\nconversion = {'Open' : 'first', 'High' : 'max', 'Low' : 'min', 'Close' : 'last', 'Volume' : 'sum'}\nsample = data.between_time(start_time = '09:30:00', end_time = '10:30:00', include_end = False)\nsample = sample.ix['2007-05-07']\nsample.tail()\n\nsample.resample('60Min', how = conversion) \n""]";"[""# Extract data for regular trading hours (rth) from the 24 hour data set\nrth = data.between_time(start_time = '09:30:00', end_time = '16:15:00', include_end = False)\n\n# Extract data for extended trading hours (eth) from the 24 hour data set\neth = data.between_time(start_time = '16:30:00', end_time = '09:30:00', include_end = False)\n\n# Extract data for initial balance (rth) from the 24 hour data set\ninitial_balance = data.between_time(start_time = '09:30:00', end_time = '10:30:00', include_end =      False)\n"", ""conversion = {'Open' : 'first', 'High' : 'max', 'Low' : 'min', 'Close' : 'last', 'Volume' : 'sum'}\nsample = data.between_time(start_time = '09:30:00', end_time = '10:30:00', include_end = False)\nsample = sample.ix['2007-05-07']\nsample.tail()\n\nsample.resample('60Min', how = conversion) \n""]";"[""# Extract data for regular trading hours (rth) from the 24 hour data set\nrth = data.between_time(start_time = '09:30:00', end_time = '16:15:00', include_end = False)\n\n# Extract data for extended trading hours (eth) from the 24 hour data set\neth = data.between_time(start_time = '16:30:00', end_time = '09:30:00', include_end = False)\n\n# Extract data for initial balance (rth) from the 24 hour data set\ninitial_balance = data.between_time(start_time = '09:30:00', end_time = '10:30:00', include_end =      False)\n"", ""conversion = {'Open' : 'first', 'High' : 'max', 'Low' : 'min', 'Close' : 'last', 'Volume' : 'sum'}\nsample = data.between_time(start_time = '09:30:00', end_time = '10:30:00', include_end = False)\nsample = sample.ix['2007-05-07']\nsample.tail()\n\nsample.resample('60Min', how = conversion) \n""]";"[""# Extract data for regular trading hours (rth) from the 24 hour data set\nrth = data.between_time(start_time = '09:30:00', end_time = '16:15:00', include_end = False)\n\n# Extract data for extended trading hours (eth) from the 24 hour data set\neth = data.between_time(start_time = '16:30:00', end_time = '09:30:00', include_end = False)\n\n# Extract data for initial balance (rth) from the 24 hour data set\ninitial_balance = data.between_time(start_time = '09:30:00', end_time = '10:30:00', include_end =      False)\nconversion = {'Open' : 'first', 'High' : 'max', 'Low' : 'min', 'Close' : 'last', 'Volume' : 'sum'}\nsample = data.between_time(start_time = '09:30:00', end_time = '10:30:00', include_end = False)\nsample = sample.ix['2007-05-07']\nsample.tail()\n\nsample.resample('60Min', how = conversion) \n""]";"[""# Extract data for regular trading hours (rth) from the 24 hour data set\nrth = data.between_time(start_time = '09:30:00', end_time = '16:15:00', include_end = False)\n\n# Extract data for extended trading hours (eth) from the 24 hour data set\neth = data.between_time(start_time = '16:30:00', end_time = '09:30:00', include_end = False)\n\n# Extract data for initial balance (rth) from the 24 hour data set\ninitial_balance = data.between_time(start_time = '09:30:00', end_time = '10:30:00', include_end =      False)\nconversion = {'Open' : 'first', 'High' : 'max', 'Low' : 'min', 'Close' : 'last', 'Volume' : 'sum'}\nsample = data.between_time(start_time = '09:30:00', end_time = '10:30:00', include_end = False)\nsample = sample.ix['2007-05-07']\nsample.tail()\n\nsample.resample('60Min', how = conversion) \n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\n# Extract data for regular trading hours (rth) from the 24 hour data set\nrth = data.between_time(start_time = '09:30:00', end_time = '16:15:00', include_end = False)\n\n# Extract data for extended trading hours (eth) from the 24 hour data set\neth = data.between_time(start_time = '16:30:00', end_time = '09:30:00', include_end = False)\n\n# Extract data for initial balance (rth) from the 24 hour data set\ninitial_balance = data.between_time(start_time = '09:30:00', end_time = '10:30:00', include_end =      False)\nconversion = {'Open' : 'first', 'High' : 'max', 'Low' : 'min', 'Close' : 'last', 'Volume' : 'sum'}\nsample = data.between_time(start_time = '09:30:00', end_time = '10:30:00', include_end = False)\nsample = sample.ix['2007-05-07']\nsample.tail()\n\nsample.resample('60Min', how = conversion) \n""]";True;0;1;"[""name 'sample' is not defined""]";['NameError'];0;1;"[""name 'sample' is not defined""]";['NameError'];0;1;"[""name 'sample' is not defined""]";['NameError']
228;228;228;228;3.0;0;14885895;;1;13;<matplotlib><pandas>;Color by Column Values in Matplotlib;16024.0;"['##ggplot scatterplot example with R dataframe, `df`, colored by col3\nggplot(data = df, aes(x=col1, y=col2, color=col3)) + geom_point()\n\n##ideal situation with pandas dataframe, \'df\', where colors are chosen by col3\ndf.plot(x=col1,y=col2,color=col3)\nimport pandas as pd\ndf = pd.DataFrame({\'Height\':np.random.normal(10),\n                   \'Weight\':np.random.normal(10),\n                   \'Gender\': [""Male"",""Male"",""Male"",""Male"",""Male"",\n                              ""Female"",""Female"",""Female"",""Female"",""Female""]})\n']";"[""##ggplot scatterplot example with R dataframe, `df`, colored by col3\nggplot(data = df, aes(x=col1, y=col2, color=col3)) + geom_point()\n\n##ideal situation with pandas dataframe, 'df', where colors are chosen by col3\ndf.plot(x=col1,y=col2,color=col3)\n"", 'import pandas as pd\ndf = pd.DataFrame({\'Height\':np.random.normal(10),\n                   \'Weight\':np.random.normal(10),\n                   \'Gender\': [""Male"",""Male"",""Male"",""Male"",""Male"",\n                              ""Female"",""Female"",""Female"",""Female"",""Female""]})\n']";"['ggplot2', ""##ggplot scatterplot example with R dataframe, `df`, colored by col3\nggplot(data = df, aes(x=col1, y=col2, color=col3)) + geom_point()\n\n##ideal situation with pandas dataframe, 'df', where colors are chosen by col3\ndf.plot(x=col1,y=col2,color=col3)\n"", 'import pandas as pd\ndf = pd.DataFrame({\'Height\':np.random.normal(10),\n                   \'Weight\':np.random.normal(10),\n                   \'Gender\': [""Male"",""Male"",""Male"",""Male"",""Male"",\n                              ""Female"",""Female"",""Female"",""Female"",""Female""]})\n']";"[""##ggplot scatterplot example with R dataframe, `df`, colored by col3\n\n##ideal situation with pandas dataframe, 'df', where colors are chosen by col3\ndf.plot(x=col1,y=col2,color=col3)\nimport pandas as pd\n""]";"[""##ggplot scatterplot example with R dataframe, `df`, colored by col3\n\n##ideal situation with pandas dataframe, 'df', where colors are chosen by col3\ndf.plot(x=col1,y=col2,color=col3)\nimport pandas as pd\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\n##ggplot scatterplot example with R dataframe, `df`, colored by col3\n\n##ideal situation with pandas dataframe, 'df', where colors are chosen by col3\ndf.plot(x=col1,y=col2,color=col3)\nimport pandas as pd\n""]";True;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
229;229;229;229;2.0;2;14888473;;1;11;<python><dataframe><pandas><subplot>;python pandas DataFrame subplot in columns and rows;6555.0;"[""df =pd.DataFrame(np.random.randn(6,4),index=pd.date_range('1/1/2000',periods=6, freq='1h'))\n df.plot(use_index=False, title=f, subplots=True, sharey=True, figsize=(8, 6))\n""]";"[""df =pd.DataFrame(np.random.randn(6,4),index=pd.date_range('1/1/2000',periods=6, freq='1h'))\n"", ' df.plot(use_index=False, title=f, subplots=True, sharey=True, figsize=(8, 6))\n']";"[""df =pd.DataFrame(np.random.randn(6,4),index=pd.date_range('1/1/2000',periods=6, freq='1h'))\n"", ' df.plot(use_index=False, title=f, subplots=True, sharey=True, figsize=(8, 6))\n']";"[""df =pd.DataFrame(np.random.randn(6,4),index=pd.date_range('1/1/2000',periods=6, freq='1h'))\n""]";"[""import pandas as pd\ndf =pd.DataFrame(np.random.randn(6,4),index=pd.date_range('1/1/2000',periods=6, freq='1h'))\n""]";True;"[""import pandas as pd\ndf =pd.DataFrame(np.random.randn(6,4),index=pd.date_range('1/1/2000',periods=6, freq='1h'))\n""]";False;1;2;"[""name 'plt' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'plt' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'plt' is not defined"", 'Sucess']";['NameError', 'Sucess']
230;230;230;230;1.0;0;14916358;;1;11;<python><numpy><pandas><scipy><multi-index>;Reshaping dataframes in pandas based on column labels;11117.0;"['In [23]: df = pandas.DataFrame({""s1_x"": scipy.randn(10), ""s1_y"": scipy.randn(10), ""s2_x"": scipy.randn(10), ""s2_y"": scipy.randn(10)})\nIn [24]: df\nOut[24]: \n       s1_x      s1_y      s2_x      s2_y\n0  0.913462  0.525590 -0.377640  0.700720\n1  0.723288 -0.691715  0.127153  0.180836\n2  0.181631 -1.090529 -1.392552  1.530669\n3  0.997414 -1.486094  1.207012  0.376120\n4 -0.319841  0.195289 -1.034683  0.286073\n5  1.085154 -0.619635  0.396867  0.623482\n6  1.867816 -0.928101 -0.491929 -0.955295\n7  0.920658 -1.132057  1.701582 -0.110299\n8 -0.241853 -0.129702 -0.809852  0.014802\n9 -0.019523 -0.578930  0.803688 -0.881875\n          x         y      sample\n0  0.913462  0.525590          s1\n1  0.723288 -0.691715          s1\n2  0.181631 -1.090529          s1\n3  0.997414 -1.486094          s1\n...\n5  0.396867  0.623482          s2\n...\nIn [636]: df = pandas.DataFrame({""s1_x"": scipy.randn(10), ""s1_y"": scipy.randn(10), ""s2_x"": scipy.randn(10), ""s2_y"": scipy.randn(10), ""names"": range(10)})\nIn [637]: df\nOut[637]: \n   names      s1_x      s1_y      s2_x      s2_y\n0      0  0.672298  0.415366  1.034770  0.556209\n1      1  0.067087 -0.851028  0.053608 -0.276461\n2      2 -0.674174 -0.099015  0.864148 -0.067240\n3      3  0.542996 -0.813018  2.283530  2.793727\n4      4  0.216633 -0.091870 -0.746411 -0.421852\n5      5  0.141301 -1.537721 -0.371601 -1.594634\n6      6  1.267148 -0.833120  0.369516 -0.671627\n7      7 -0.231163 -0.557398  1.123155  0.865140\n8      8  1.790570 -0.428563  0.668987  0.632409\n9      9 -0.820315 -0.894855  0.673247 -1.195831\nIn [638]: df.columns = pandas.MultiIndex.from_tuples([tuple(c.split(\'_\')) for c in df.columns])\n\nIn [639]: df.stack(0).reset_index(1)\nOut[639]: \n  level_1         x         y\n0      s1  0.672298  0.415366\n0      s2  1.034770  0.556209\n1      s1  0.067087 -0.851028\n1      s2  0.053608 -0.276461\n2      s1 -0.674174 -0.099015\n2      s2  0.864148 -0.067240\n3      s1  0.542996 -0.813018\n3      s2  2.283530  2.793727\n4      s1  0.216633 -0.091870\n4      s2 -0.746411 -0.421852\n5      s1  0.141301 -1.537721\n5      s2 -0.371601 -1.594634\n6      s1  1.267148 -0.833120\n6      s2  0.369516 -0.671627\n7      s1 -0.231163 -0.557398\n7      s2  1.123155  0.865140\n8      s1  1.790570 -0.428563\n8      s2  0.668987  0.632409\n9      s1 -0.820315 -0.894855\n9      s2  0.673247 -1.195831\n']";"['In [23]: df = pandas.DataFrame({""s1_x"": scipy.randn(10), ""s1_y"": scipy.randn(10), ""s2_x"": scipy.randn(10), ""s2_y"": scipy.randn(10)})\nIn [24]: df\nOut[24]: \n       s1_x      s1_y      s2_x      s2_y\n0  0.913462  0.525590 -0.377640  0.700720\n1  0.723288 -0.691715  0.127153  0.180836\n2  0.181631 -1.090529 -1.392552  1.530669\n3  0.997414 -1.486094  1.207012  0.376120\n4 -0.319841  0.195289 -1.034683  0.286073\n5  1.085154 -0.619635  0.396867  0.623482\n6  1.867816 -0.928101 -0.491929 -0.955295\n7  0.920658 -1.132057  1.701582 -0.110299\n8 -0.241853 -0.129702 -0.809852  0.014802\n9 -0.019523 -0.578930  0.803688 -0.881875\n', '          x         y      sample\n0  0.913462  0.525590          s1\n1  0.723288 -0.691715          s1\n2  0.181631 -1.090529          s1\n3  0.997414 -1.486094          s1\n...\n5  0.396867  0.623482          s2\n...\n', 'In [636]: df = pandas.DataFrame({""s1_x"": scipy.randn(10), ""s1_y"": scipy.randn(10), ""s2_x"": scipy.randn(10), ""s2_y"": scipy.randn(10), ""names"": range(10)})\nIn [637]: df\nOut[637]: \n   names      s1_x      s1_y      s2_x      s2_y\n0      0  0.672298  0.415366  1.034770  0.556209\n1      1  0.067087 -0.851028  0.053608 -0.276461\n2      2 -0.674174 -0.099015  0.864148 -0.067240\n3      3  0.542996 -0.813018  2.283530  2.793727\n4      4  0.216633 -0.091870 -0.746411 -0.421852\n5      5  0.141301 -1.537721 -0.371601 -1.594634\n6      6  1.267148 -0.833120  0.369516 -0.671627\n7      7 -0.231163 -0.557398  1.123155  0.865140\n8      8  1.790570 -0.428563  0.668987  0.632409\n9      9 -0.820315 -0.894855  0.673247 -1.195831\nIn [638]: df.columns = pandas.MultiIndex.from_tuples([tuple(c.split(\'_\')) for c in df.columns])\n\nIn [639]: df.stack(0).reset_index(1)\nOut[639]: \n  level_1         x         y\n0      s1  0.672298  0.415366\n0      s2  1.034770  0.556209\n1      s1  0.067087 -0.851028\n1      s2  0.053608 -0.276461\n2      s1 -0.674174 -0.099015\n2      s2  0.864148 -0.067240\n3      s1  0.542996 -0.813018\n3      s2  2.283530  2.793727\n4      s1  0.216633 -0.091870\n4      s2 -0.746411 -0.421852\n5      s1  0.141301 -1.537721\n5      s2 -0.371601 -1.594634\n6      s1  1.267148 -0.833120\n6      s2  0.369516 -0.671627\n7      s1 -0.231163 -0.557398\n7      s2  1.123155  0.865140\n8      s1  1.790570 -0.428563\n8      s2  0.668987  0.632409\n9      s1 -0.820315 -0.894855\n9      s2  0.673247 -1.195831\n']";"['df', 'x,y', 's1', 's2', 'In [23]: df = pandas.DataFrame({""s1_x"": scipy.randn(10), ""s1_y"": scipy.randn(10), ""s2_x"": scipy.randn(10), ""s2_y"": scipy.randn(10)})\nIn [24]: df\nOut[24]: \n       s1_x      s1_y      s2_x      s2_y\n0  0.913462  0.525590 -0.377640  0.700720\n1  0.723288 -0.691715  0.127153  0.180836\n2  0.181631 -1.090529 -1.392552  1.530669\n3  0.997414 -1.486094  1.207012  0.376120\n4 -0.319841  0.195289 -1.034683  0.286073\n5  1.085154 -0.619635  0.396867  0.623482\n6  1.867816 -0.928101 -0.491929 -0.955295\n7  0.920658 -1.132057  1.701582 -0.110299\n8 -0.241853 -0.129702 -0.809852  0.014802\n9 -0.019523 -0.578930  0.803688 -0.881875\n', 's1_x', 's1_y', 's2_x, s2_y', 'x', 'y', 'sample', 's1', 's2', '          x         y      sample\n0  0.913462  0.525590          s1\n1  0.723288 -0.691715          s1\n2  0.181631 -1.090529          s1\n3  0.997414 -1.486094          s1\n...\n5  0.396867  0.623482          s2\n...\n', 'In [636]: df = pandas.DataFrame({""s1_x"": scipy.randn(10), ""s1_y"": scipy.randn(10), ""s2_x"": scipy.randn(10), ""s2_y"": scipy.randn(10), ""names"": range(10)})\nIn [637]: df\nOut[637]: \n   names      s1_x      s1_y      s2_x      s2_y\n0      0  0.672298  0.415366  1.034770  0.556209\n1      1  0.067087 -0.851028  0.053608 -0.276461\n2      2 -0.674174 -0.099015  0.864148 -0.067240\n3      3  0.542996 -0.813018  2.283530  2.793727\n4      4  0.216633 -0.091870 -0.746411 -0.421852\n5      5  0.141301 -1.537721 -0.371601 -1.594634\n6      6  1.267148 -0.833120  0.369516 -0.671627\n7      7 -0.231163 -0.557398  1.123155  0.865140\n8      8  1.790570 -0.428563  0.668987  0.632409\n9      9 -0.820315 -0.894855  0.673247 -1.195831\nIn [638]: df.columns = pandas.MultiIndex.from_tuples([tuple(c.split(\'_\')) for c in df.columns])\n\nIn [639]: df.stack(0).reset_index(1)\nOut[639]: \n  level_1         x         y\n0      s1  0.672298  0.415366\n0      s2  1.034770  0.556209\n1      s1  0.067087 -0.851028\n1      s2  0.053608 -0.276461\n2      s1 -0.674174 -0.099015\n2      s2  0.864148 -0.067240\n3      s1  0.542996 -0.813018\n3      s2  2.283530  2.793727\n4      s1  0.216633 -0.091870\n4      s2 -0.746411 -0.421852\n5      s1  0.141301 -1.537721\n5      s2 -0.371601 -1.594634\n6      s1  1.267148 -0.833120\n6      s2  0.369516 -0.671627\n7      s1 -0.231163 -0.557398\n7      s2  1.123155  0.865140\n8      s1  1.790570 -0.428563\n8      s2  0.668987  0.632409\n9      s1 -0.820315 -0.894855\n9      s2  0.673247 -1.195831\n', '""names""', '""names""', '_', '""names""']";"['df = pandas.DataFrame({""s1_x"": scipy.randn(10), ""s1_y"": scipy.randn(10), ""s2_x"": scipy.randn(10), ""s2_y"": scipy.randn(10)})\ndf = pandas.DataFrame({""s1_x"": scipy.randn(10), ""s1_y"": scipy.randn(10), ""s2_x"": scipy.randn(10), ""s2_y"": scipy.randn(10), ""names"": range(10)})\ndf.columns = pandas.MultiIndex.from_tuples([tuple(c.split(\'_\')) for c in df.columns])\n\n']";"['df = pandas.DataFrame({""s1_x"": scipy.randn(10), ""s1_y"": scipy.randn(10), ""s2_x"": scipy.randn(10), ""s2_y"": scipy.randn(10)})\ndf = pandas.DataFrame({""s1_x"": scipy.randn(10), ""s1_y"": scipy.randn(10), ""s2_x"": scipy.randn(10), ""s2_y"": scipy.randn(10), ""names"": range(10)})\ndf.columns = pandas.MultiIndex.from_tuples([tuple(c.split(\'_\')) for c in df.columns])\n\n']";False;"['import pandas as pd\ndf = pandas.DataFrame({""s1_x"": scipy.randn(10), ""s1_y"": scipy.randn(10), ""s2_x"": scipy.randn(10), ""s2_y"": scipy.randn(10)})\ndf = pandas.DataFrame({""s1_x"": scipy.randn(10), ""s1_y"": scipy.randn(10), ""s2_x"": scipy.randn(10), ""s2_y"": scipy.randn(10), ""names"": range(10)})\ndf.columns = pandas.MultiIndex.from_tuples([tuple(c.split(\'_\')) for c in df.columns])\n\n']";False;0;1;"[""name 'pandas' is not defined""]";['NameError'];0;1;"[""name 'pandas' is not defined""]";['NameError'];0;1;"[""name 'pandas' is not defined""]";['NameError']
231;231;231;231;6.0;1;14940743;;1;84;<python><pandas>;Selecting/Excluding sets of columns in Pandas;77656.0;"['import numpy as np\nimport pandas as pd\n\n# Create a dataframe with columns A,B,C and D\ndf = pd.DataFrame(np.random.randn(100, 4), columns=list(\'ABCD\'))\n\n# Try to create a second dataframe df2 from df with all columns except \'B\' and D\nmy_cols = set(df.columns)\nmy_cols.remove(\'B\').remove(\'D\')\n\n# This returns an error (""unhashable type: set"")\ndf2 = df[my_cols]\n']";"['import numpy as np\nimport pandas as pd\n\n# Create a dataframe with columns A,B,C and D\ndf = pd.DataFrame(np.random.randn(100, 4), columns=list(\'ABCD\'))\n\n# Try to create a second dataframe df2 from df with all columns except \'B\' and D\nmy_cols = set(df.columns)\nmy_cols.remove(\'B\').remove(\'D\')\n\n# This returns an error (""unhashable type: set"")\ndf2 = df[my_cols]\n']";"['import numpy as np\nimport pandas as pd\n\n# Create a dataframe with columns A,B,C and D\ndf = pd.DataFrame(np.random.randn(100, 4), columns=list(\'ABCD\'))\n\n# Try to create a second dataframe df2 from df with all columns except \'B\' and D\nmy_cols = set(df.columns)\nmy_cols.remove(\'B\').remove(\'D\')\n\n# This returns an error (""unhashable type: set"")\ndf2 = df[my_cols]\n']";"['import numpy as np\nimport pandas as pd\n\n# Create a dataframe with columns A,B,C and D\ndf = pd.DataFrame(np.random.randn(100, 4), columns=list(\'ABCD\'))\n\n# Try to create a second dataframe df2 from df with all columns except \'B\' and D\nmy_cols = set(df.columns)\nmy_cols.remove(\'B\').remove(\'D\')\n\n# This returns an error (""unhashable type: set"")\ndf2 = df[my_cols]\n']";"['import numpy as np\nimport pandas as pd\n\n# Create a dataframe with columns A,B,C and D\ndf = pd.DataFrame(np.random.randn(100, 4), columns=list(\'ABCD\'))\n\n# Try to create a second dataframe df2 from df with all columns except \'B\' and D\nmy_cols = set(df.columns)\nmy_cols.remove(\'B\').remove(\'D\')\n\n# This returns an error (""unhashable type: set"")\ndf2 = df[my_cols]\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\nimport numpy as np\nimport pandas as pd\n\n# Create a dataframe with columns A,B,C and D\ndf = pd.DataFrame(np.random.randn(100, 4), columns=list(\'ABCD\'))\n\n# Try to create a second dataframe df2 from df with all columns except \'B\' and D\nmy_cols = set(df.columns)\nmy_cols.remove(\'B\').remove(\'D\')\n\n# This returns an error (""unhashable type: set"")\ndf2 = df[my_cols]\n']";True;1;4;"[""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'NameError', 'Sucess', 'NameError'];1;4;"[""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'NameError', 'Sucess', 'NameError'];4;4;['Sucess', 'Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess', 'Sucess']
232;232;232;232;5.0;1;14941097;;1;19;<python><pandas><indexing>;Selecting pandas column by location;26383.0;"[""df=pandas.DataFrame({'a':np.random.rand(5), 'b':np.random.rand(5)})\n""]";"[""df=pandas.DataFrame({'a':np.random.rand(5), 'b':np.random.rand(5)})\n""]";"['df.ix[3]', ""df=pandas.DataFrame({'a':np.random.rand(5), 'b':np.random.rand(5)})\n""]";"[""df=pandas.DataFrame({'a':np.random.rand(5), 'b':np.random.rand(5)})\n""]";"[""df=pandas.DataFrame({'a':np.random.rand(5), 'b':np.random.rand(5)})\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf=pandas.DataFrame({'a':np.random.rand(5), 'b':np.random.rand(5)})\n""]";True;1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'n' is not defined""]";['Sucess', 'NameError']
233;233;233;233;3.0;0;14941366;;1;43;<python><sorting><group-by><dataframe><pandas>;Pandas sort by group aggregate and column;70881.0;"[""In [31]: rand = np.random.RandomState(1)\n         df = pd.DataFrame({'A': ['foo', 'bar', 'baz'] * 2,\n                            'B': rand.randn(6),\n                            'C': rand.rand(6) > .5})\n\nIn [32]: df\nOut[32]:      A         B      C\n         0  foo  1.624345  False\n         1  bar -0.611756   True\n         2  baz -0.528172  False\n         3  foo -1.072969   True\n         4  bar  0.865408  False\n         5  baz -2.301539   True \nIn [28]: df.groupby('A').sum().sort('B')\nOut[28]:             B  C\n         A               \n         baz -2.829710  1\n         bar  0.253651  1\n         foo  0.551377  1\nIn [30]: df.ix[[5, 2, 1, 4, 3, 0]]\nOut[30]: A         B      C\n    5  baz -2.301539   True\n    2  baz -0.528172  False\n    1  bar -0.611756   True\n    4  bar  0.865408  False\n    3  foo -1.072969   True\n    0  foo  1.624345  False\n""]";"[""In [31]: rand = np.random.RandomState(1)\n         df = pd.DataFrame({'A': ['foo', 'bar', 'baz'] * 2,\n                            'B': rand.randn(6),\n                            'C': rand.rand(6) > .5})\n\nIn [32]: df\nOut[32]:      A         B      C\n         0  foo  1.624345  False\n         1  bar -0.611756   True\n         2  baz -0.528172  False\n         3  foo -1.072969   True\n         4  bar  0.865408  False\n         5  baz -2.301539   True \n"", ""In [28]: df.groupby('A').sum().sort('B')\nOut[28]:             B  C\n         A               \n         baz -2.829710  1\n         bar  0.253651  1\n         foo  0.551377  1\n"", 'In [30]: df.ix[[5, 2, 1, 4, 3, 0]]\nOut[30]: A         B      C\n    5  baz -2.301539   True\n    2  baz -0.528172  False\n    1  bar -0.611756   True\n    4  bar  0.865408  False\n    3  foo -1.072969   True\n    0  foo  1.624345  False\n']";"[""In [31]: rand = np.random.RandomState(1)\n         df = pd.DataFrame({'A': ['foo', 'bar', 'baz'] * 2,\n                            'B': rand.randn(6),\n                            'C': rand.rand(6) > .5})\n\nIn [32]: df\nOut[32]:      A         B      C\n         0  foo  1.624345  False\n         1  bar -0.611756   True\n         2  baz -0.528172  False\n         3  foo -1.072969   True\n         4  bar  0.865408  False\n         5  baz -2.301539   True \n"", 'A', 'B', 'C', 'A', ""In [28]: df.groupby('A').sum().sort('B')\nOut[28]:             B  C\n         A               \n         baz -2.829710  1\n         bar  0.253651  1\n         foo  0.551377  1\n"", 'In [30]: df.ix[[5, 2, 1, 4, 3, 0]]\nOut[30]: A         B      C\n    5  baz -2.301539   True\n    2  baz -0.528172  False\n    1  bar -0.611756   True\n    4  bar  0.865408  False\n    3  foo -1.072969   True\n    0  foo  1.624345  False\n']";"[""rand = np.random.RandomState(1)\n\ndf.groupby('A').sum().sort('B')\ndf.ix[[5, 2, 1, 4, 3, 0]]\n""]";"[""rand = np.random.RandomState(1)\n\ndf.groupby('A').sum().sort('B')\ndf.ix[[5, 2, 1, 4, 3, 0]]\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nrand = np.random.RandomState(1)\n\ndf.groupby('A').sum().sort('B')\ndf.ix[[5, 2, 1, 4, 3, 0]]\n""]";True;0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""'A'"", ""'A'"", ""'A'""]";['KeyError', 'KeyError', 'KeyError']
234;234;234;234;2.0;0;14964493;;1;21;<pandas>;MultiIndex-based indexing in pandas;10792.0;"[""import itertools\nimport pandas as pd\nimport numpy as np\na = ('A', 'B')\ni = (0, 1, 2)\nb = (True, False)\nidx = pd.MultiIndex.from_tuples(list(itertools.product(a, i, b)),\n                                names=('Alpha', 'Int', 'Bool'))\ndf = pd.DataFrame(np.random.randn(len(idx), 7), index=idx,\n                  columns=('I', 'II', 'III', 'IV', 'V', 'VI', 'VII'))\nIn [19]: df\nOut[19]: \n                        I        II       III        IV         V        VI       VII\nAlpha Int Bool                                                                       \nA     0   True  -0.462924  1.210442  0.306737  0.325116 -1.320084 -0.831699  0.892865\n          False -0.850570 -0.949779  0.022074 -0.205575 -0.684794 -0.214307 -1.133833\n      1   True   0.603602  1.387020 -0.830780 -1.242000 -0.321938  0.484271  0.171738\n          False -1.591730  1.282136  0.095159 -1.239882  0.760880 -0.606444 -0.485957\n      2   True  -1.346883  1.650247 -1.476443  2.092067  1.344689  0.177083  0.100844\n          False  0.001407 -1.127299 -0.417828  0.143595 -0.277838 -0.478262 -0.350906\nB     0   True   0.722781 -1.093182  0.237536  0.457614 -2.500885  0.338257  0.009128\n          False  0.321022  0.419357  1.161140 -1.371035  1.093696  0.250517 -1.125612\n      1   True   0.237441  1.739933  0.029653  0.327823 -0.384647  1.523628 -0.009053\n          False -0.459148 -0.598577 -0.593486 -0.607447  1.478399  0.504028 -0.329555\n      2   True  -0.583052 -0.986493 -0.057788 -0.639798  1.400311  0.076471 -0.212513\n          False  0.896755  2.583520  1.520151  2.367336 -1.084994 -1.233548 -2.414215\nIn [20]: df['VII']\nOut[20]: \nAlpha  Int  Bool \nA      0    True     0.892865\n            False   -1.133833\n       1    True     0.171738\n            False   -0.485957\n       2    True     0.100844\n            False   -0.350906\nB      0    True     0.009128\n            False   -1.125612\n       1    True    -0.009053\n            False   -0.329555\n       2    True    -0.212513\n            False   -2.414215\nName: VII\n""]";"[""import itertools\nimport pandas as pd\nimport numpy as np\na = ('A', 'B')\ni = (0, 1, 2)\nb = (True, False)\nidx = pd.MultiIndex.from_tuples(list(itertools.product(a, i, b)),\n                                names=('Alpha', 'Int', 'Bool'))\ndf = pd.DataFrame(np.random.randn(len(idx), 7), index=idx,\n                  columns=('I', 'II', 'III', 'IV', 'V', 'VI', 'VII'))\n"", 'In [19]: df\nOut[19]: \n                        I        II       III        IV         V        VI       VII\nAlpha Int Bool                                                                       \nA     0   True  -0.462924  1.210442  0.306737  0.325116 -1.320084 -0.831699  0.892865\n          False -0.850570 -0.949779  0.022074 -0.205575 -0.684794 -0.214307 -1.133833\n      1   True   0.603602  1.387020 -0.830780 -1.242000 -0.321938  0.484271  0.171738\n          False -1.591730  1.282136  0.095159 -1.239882  0.760880 -0.606444 -0.485957\n      2   True  -1.346883  1.650247 -1.476443  2.092067  1.344689  0.177083  0.100844\n          False  0.001407 -1.127299 -0.417828  0.143595 -0.277838 -0.478262 -0.350906\nB     0   True   0.722781 -1.093182  0.237536  0.457614 -2.500885  0.338257  0.009128\n          False  0.321022  0.419357  1.161140 -1.371035  1.093696  0.250517 -1.125612\n      1   True   0.237441  1.739933  0.029653  0.327823 -0.384647  1.523628 -0.009053\n          False -0.459148 -0.598577 -0.593486 -0.607447  1.478399  0.504028 -0.329555\n      2   True  -0.583052 -0.986493 -0.057788 -0.639798  1.400311  0.076471 -0.212513\n          False  0.896755  2.583520  1.520151  2.367336 -1.084994 -1.233548 -2.414215\n', ""In [20]: df['VII']\nOut[20]: \nAlpha  Int  Bool \nA      0    True     0.892865\n            False   -1.133833\n       1    True     0.171738\n            False   -0.485957\n       2    True     0.100844\n            False   -0.350906\nB      0    True     0.009128\n            False   -1.125612\n       1    True    -0.009053\n            False   -0.329555\n       2    True    -0.212513\n            False   -2.414215\nName: VII\n""]";"[""import itertools\nimport pandas as pd\nimport numpy as np\na = ('A', 'B')\ni = (0, 1, 2)\nb = (True, False)\nidx = pd.MultiIndex.from_tuples(list(itertools.product(a, i, b)),\n                                names=('Alpha', 'Int', 'Bool'))\ndf = pd.DataFrame(np.random.randn(len(idx), 7), index=idx,\n                  columns=('I', 'II', 'III', 'IV', 'V', 'VI', 'VII'))\n"", 'In [19]: df\nOut[19]: \n                        I        II       III        IV         V        VI       VII\nAlpha Int Bool                                                                       \nA     0   True  -0.462924  1.210442  0.306737  0.325116 -1.320084 -0.831699  0.892865\n          False -0.850570 -0.949779  0.022074 -0.205575 -0.684794 -0.214307 -1.133833\n      1   True   0.603602  1.387020 -0.830780 -1.242000 -0.321938  0.484271  0.171738\n          False -1.591730  1.282136  0.095159 -1.239882  0.760880 -0.606444 -0.485957\n      2   True  -1.346883  1.650247 -1.476443  2.092067  1.344689  0.177083  0.100844\n          False  0.001407 -1.127299 -0.417828  0.143595 -0.277838 -0.478262 -0.350906\nB     0   True   0.722781 -1.093182  0.237536  0.457614 -2.500885  0.338257  0.009128\n          False  0.321022  0.419357  1.161140 -1.371035  1.093696  0.250517 -1.125612\n      1   True   0.237441  1.739933  0.029653  0.327823 -0.384647  1.523628 -0.009053\n          False -0.459148 -0.598577 -0.593486 -0.607447  1.478399  0.504028 -0.329555\n      2   True  -0.583052 -0.986493 -0.057788 -0.639798  1.400311  0.076471 -0.212513\n          False  0.896755  2.583520  1.520151  2.367336 -1.084994 -1.233548 -2.414215\n', ""'VII'"", ""In [20]: df['VII']\nOut[20]: \nAlpha  Int  Bool \nA      0    True     0.892865\n            False   -1.133833\n       1    True     0.171738\n            False   -0.485957\n       2    True     0.100844\n            False   -0.350906\nB      0    True     0.009128\n            False   -1.125612\n       1    True    -0.009053\n            False   -0.329555\n       2    True    -0.212513\n            False   -2.414215\nName: VII\n"", ""Alpha=='B'"", ""Alpha=='B'"", 'Bool==False', ""Alpha=='B'"", 'Bool==False', ""'I'"", ""Alpha=='B'"", 'Bool==False', ""'I'"", ""'III'"", ""Alpha=='B'"", 'Bool==False', ""'I'"", ""'III'"", ""'V'"", 'Int']";"[""df\ndf['VII']\n""]";"[""df\ndf['VII']\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf\ndf['VII']\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'Index' object has no attribute 'get_loc_level'""]";['AttributeError']
235;235;235;235;4.0;3;14984119;;1;24;<python><pandas>;python pandas remove duplicate columns;23542.0;['import pandas as pd\n\ndf=pd.read_table(fname)\nTime, Time Relative, N2, Time, Time Relative, H2, etc...\nTime, Time Relative, N2, H2\ndf=df.T.drop_duplicates().T\nReindexing only valid with uniquely valued index objects\nTime    Time Relative [s]    N2[%]    Time    Time Relative [s]    H2[ppm]\n2/12/2013 9:20:55 AM    6.177    9.99268e+001    2/12/2013 9:20:55 AM    6.177    3.216293e-005    \n2/12/2013 9:21:06 AM    17.689    9.99296e+001    2/12/2013 9:21:06 AM    17.689    3.841667e-005    \n2/12/2013 9:21:18 AM    29.186    9.992954e+001    2/12/2013 9:21:18 AM    29.186    3.880365e-005    \n... etc ...\n2/12/2013 2:12:44 PM    17515.269    9.991756+001    2/12/2013 2:12:44 PM    17515.269    2.800279e-005    \n2/12/2013 2:12:55 PM    17526.769    9.991754e+001    2/12/2013 2:12:55 PM    17526.769    2.880386e-005\n2/12/2013 2:13:07 PM    17538.273    9.991797e+001    2/12/2013 2:13:07 PM    17538.273    3.131447e-005\n'];['import pandas as pd\n\ndf=pd.read_table(fname)\n', 'Time, Time Relative, N2, Time, Time Relative, H2, etc...\n', 'Time, Time Relative, N2, H2\n', 'df=df.T.drop_duplicates().T\n', 'Reindexing only valid with uniquely valued index objects\n', 'Time    Time Relative [s]    N2[%]    Time    Time Relative [s]    H2[ppm]\n2/12/2013 9:20:55 AM    6.177    9.99268e+001    2/12/2013 9:20:55 AM    6.177    3.216293e-005    \n2/12/2013 9:21:06 AM    17.689    9.99296e+001    2/12/2013 9:21:06 AM    17.689    3.841667e-005    \n2/12/2013 9:21:18 AM    29.186    9.992954e+001    2/12/2013 9:21:18 AM    29.186    3.880365e-005    \n... etc ...\n2/12/2013 2:12:44 PM    17515.269    9.991756+001    2/12/2013 2:12:44 PM    17515.269    2.800279e-005    \n2/12/2013 2:12:55 PM    17526.769    9.991754e+001    2/12/2013 2:12:55 PM    17526.769    2.880386e-005\n2/12/2013 2:13:07 PM    17538.273    9.991797e+001    2/12/2013 2:13:07 PM    17538.273    3.131447e-005\n'];['import pandas as pd\n\ndf=pd.read_table(fname)\n', 'Time, Time Relative, N2, Time, Time Relative, H2, etc...\n', 'Time, Time Relative, N2, H2\n', 'df=df.T.drop_duplicates().T\n', 'Reindexing only valid with uniquely valued index objects\n', 'Time    Time Relative [s]    N2[%]    Time    Time Relative [s]    H2[ppm]\n2/12/2013 9:20:55 AM    6.177    9.99268e+001    2/12/2013 9:20:55 AM    6.177    3.216293e-005    \n2/12/2013 9:21:06 AM    17.689    9.99296e+001    2/12/2013 9:21:06 AM    17.689    3.841667e-005    \n2/12/2013 9:21:18 AM    29.186    9.992954e+001    2/12/2013 9:21:18 AM    29.186    3.880365e-005    \n... etc ...\n2/12/2013 2:12:44 PM    17515.269    9.991756+001    2/12/2013 2:12:44 PM    17515.269    2.800279e-005    \n2/12/2013 2:12:55 PM    17526.769    9.991754e+001    2/12/2013 2:12:55 PM    17526.769    2.880386e-005\n2/12/2013 2:13:07 PM    17538.273    9.991797e+001    2/12/2013 2:13:07 PM    17538.273    3.131447e-005\n'];['import pandas as pd\n\ndf=pd.read_table(fname)\ndf=df.T.drop_duplicates().T\n'];['import pandas as pd\n\ndf=pd.read_table(fname)\ndf=df.T.drop_duplicates().T\n'];False;['import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\n\ndf=pd.read_table(fname)\ndf=df.T.drop_duplicates().T\n'];True;0;4;"[""name 'np' is not defined"", ""name 'duplicate_columns' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""name 'np' is not defined"", ""name 'duplicate_columns' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""name 'np' is not defined"", ""name 'duplicate_columns' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError']
236;236;236;236;2.0;2;14988480;;1;26;<python><r><dataframe><pandas>;Pandas version of rbind;17614.0;['        0         1       2        3          4          5        6                    7\n0   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43\n4   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44\n5  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44\n6   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45\n        0         1        2        3          4         5        6                    7       0         1       2        3          4          5        6                    7\n0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43\n4     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44\n5     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44\n6     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45\n0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  \n'];['        0         1       2        3          4          5        6                    7\n0   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43\n4   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44\n5  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44\n6   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45\n', '        0         1        2        3          4         5        6                    7       0         1       2        3          4          5        6                    7\n0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43\n4     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44\n5     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44\n6     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45\n0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  \n'];['        0         1       2        3          4          5        6                    7\n0   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43\n4   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44\n5  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44\n6   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45\n', '        0         1        2        3          4         5        6                    7       0         1       2        3          4          5        6                    7\n0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43\n4     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44\n5     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44\n6     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45\n0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  \n'];[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'Frame' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'Frame' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'Frame' is not defined"", 'Sucess']";['NameError', 'Sucess']
237;237;237;237;1.0;0;14991195;;1;23;<python><pandas>;How to remove rows with null values from kth column onward in python;33921.0;"[""df = DataFrame(np.random.randn(6, 5), index=['a', 'c', 'e', 'f', 'g','h'], columns=['one', 'two', 'three', 'four', 'five'])\n\ndf2 = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\ndf2.ix[1][0] = 111\ndf2.ix[1][1] = 222\n""]";"[""df = DataFrame(np.random.randn(6, 5), index=['a', 'c', 'e', 'f', 'g','h'], columns=['one', 'two', 'three', 'four', 'five'])\n\ndf2 = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\ndf2.ix[1][0] = 111\ndf2.ix[1][1] = 222\n""]";"[""df = DataFrame(np.random.randn(6, 5), index=['a', 'c', 'e', 'f', 'g','h'], columns=['one', 'two', 'three', 'four', 'five'])\n\ndf2 = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\ndf2.ix[1][0] = 111\ndf2.ix[1][1] = 222\n"", 'df.dropna()']";"[""df = DataFrame(np.random.randn(6, 5), index=['a', 'c', 'e', 'f', 'g','h'], columns=['one', 'two', 'three', 'four', 'five'])\n\ndf2 = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\ndf2.ix[1][0] = 111\ndf2.ix[1][1] = 222\n""]";"[""from pandas import DataFrame\ndf = DataFrame(np.random.randn(6, 5), index=['a', 'c', 'e', 'f', 'g','h'], columns=['one', 'two', 'three', 'four', 'five'])\n\ndf2 = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\ndf2.ix[1][0] = 111\ndf2.ix[1][1] = 222\n""]";True;"[""import pandas as pd\ndf = DataFrame(np.random.randn(6, 5), index=['a', 'c', 'e', 'f', 'g','h'], columns=['one', 'two', 'three', 'four', 'five'])\n\ndf2 = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\ndf2.ix[1][0] = 111\ndf2.ix[1][1] = 222\n""]";False;0;1;"[""name 'df2' is not defined""]";['NameError'];0;1;"[""name 'df2' is not defined""]";['NameError'];0;1;"[""['three', 'four', 'five']""]";['KeyError']
238;238;238;238;3.0;1;15001237;;1;16;<group-by><pandas>;"How to apply ""first"" and ""last"" functions to columns while using group by in pandas?";12399.0;"[""grouped['D'].agg({'result1' : np.sum, 'result2' : np.mean})\n""]";"[""grouped['D'].agg({'result1' : np.sum, 'result2' : np.mean})\n""]";"[""grouped = df.groupby(['ColumnName'])"", 'grouped.sum()', 'grouped.min()', ""grouped.agg({'ColumnName1':sum, 'ColumnName2':min})"", 'first', 'grouped.first()', ""grouped.agg({'ColumnName1':first, 'ColumnName2':first})"", ""NameError: name 'first' is not defined"", ""grouped['D'].agg({'result1' : np.sum, 'result2' : np.mean})\n"", 'np']";"[""grouped['D'].agg({'result1' : np.sum, 'result2' : np.mean})\n""]";"[""grouped['D'].agg({'result1' : np.sum, 'result2' : np.mean})\n""]";False;"[""import pandas as pd\ngrouped['D'].agg({'result1' : np.sum, 'result2' : np.mean})\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'g' is not defined""]";['NameError'];0;1;"[""name 'g' is not defined""]";['NameError']
239;239;239;239;1.0;5;15003828;;1;15;<python><group-by><pandas>;returning aggregated dataframe from pandas groupby;28764.0;"[""import pandas as pd\ndf = pd.DataFrame({'col1': ['A', 'A', 'B', 'B'], 'col2':[1.0, 2, 3, 4]})\n\nIn [3]: df\nOut[3]: \n  col1  col2\n0    A     1\n1    A     2\n2    B     3\n3    B     4\n\ndef func2(df):\n    dfout = pd.DataFrame({ 'col1' : df['col1'].unique() ,\n                           'someData': sum(df['col2']) })\n    return  dfout\n\nt = df.groupby('col1').apply(func2)\n\nIn [6]: t\nOut[6]: \n       col1  someData\ncol1                 \nA    0    A         3\nB    0    B         7\nIn [13]: import pandas as pd\n\nIn [14]: df = pd.DataFrame({'col1':['A','A','A','B','B','B'], 'col2':['C','D','D','D','C','C'], 'col3':[.1,.2,.4,.6,.8,1]})\n\nIn [15]: df\nOut[15]: \n  col1 col2  col3\n0    A    C   0.1\n1    A    D   0.2\n2    A    D   0.4\n3    B    D   0.6\n4    B    C   0.8\n5    B    C   1.0\n\nIn [16]: def func3(df):\n   ....:         dfout =  sum(df['col3']**2)\n   ....:         return  dfout\n   ....: \n\nIn [17]: t = df.groupby(['col1', 'col2']).apply(func3)\n\nIn [18]: t\nOut[18]: \ncol1  col2\nA     C       0.01\n      D       0.20\nB     C       1.64\n      D       0.36\n pd.DataFrame(t).reset_index()\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'col1': ['A', 'A', 'B', 'B'], 'col2':[1.0, 2, 3, 4]})\n\nIn [3]: df\nOut[3]: \n  col1  col2\n0    A     1\n1    A     2\n2    B     3\n3    B     4\n\ndef func2(df):\n    dfout = pd.DataFrame({ 'col1' : df['col1'].unique() ,\n                           'someData': sum(df['col2']) })\n    return  dfout\n\nt = df.groupby('col1').apply(func2)\n\nIn [6]: t\nOut[6]: \n       col1  someData\ncol1                 \nA    0    A         3\nB    0    B         7\n"", ""In [13]: import pandas as pd\n\nIn [14]: df = pd.DataFrame({'col1':['A','A','A','B','B','B'], 'col2':['C','D','D','D','C','C'], 'col3':[.1,.2,.4,.6,.8,1]})\n\nIn [15]: df\nOut[15]: \n  col1 col2  col3\n0    A    C   0.1\n1    A    D   0.2\n2    A    D   0.4\n3    B    D   0.6\n4    B    C   0.8\n5    B    C   1.0\n\nIn [16]: def func3(df):\n   ....:         dfout =  sum(df['col3']**2)\n   ....:         return  dfout\n   ....: \n\nIn [17]: t = df.groupby(['col1', 'col2']).apply(func3)\n\nIn [18]: t\nOut[18]: \ncol1  col2\nA     C       0.01\n      D       0.20\nB     C       1.64\n      D       0.36\n"", ' pd.DataFrame(t).reset_index()\n']";"[""import pandas as pd\ndf = pd.DataFrame({'col1': ['A', 'A', 'B', 'B'], 'col2':[1.0, 2, 3, 4]})\n\nIn [3]: df\nOut[3]: \n  col1  col2\n0    A     1\n1    A     2\n2    B     3\n3    B     4\n\ndef func2(df):\n    dfout = pd.DataFrame({ 'col1' : df['col1'].unique() ,\n                           'someData': sum(df['col2']) })\n    return  dfout\n\nt = df.groupby('col1').apply(func2)\n\nIn [6]: t\nOut[6]: \n       col1  someData\ncol1                 \nA    0    A         3\nB    0    B         7\n"", 'col1', 'col1', 'someData', ""In [13]: import pandas as pd\n\nIn [14]: df = pd.DataFrame({'col1':['A','A','A','B','B','B'], 'col2':['C','D','D','D','C','C'], 'col3':[.1,.2,.4,.6,.8,1]})\n\nIn [15]: df\nOut[15]: \n  col1 col2  col3\n0    A    C   0.1\n1    A    D   0.2\n2    A    D   0.4\n3    B    D   0.6\n4    B    C   0.8\n5    B    C   1.0\n\nIn [16]: def func3(df):\n   ....:         dfout =  sum(df['col3']**2)\n   ....:         return  dfout\n   ....: \n\nIn [17]: t = df.groupby(['col1', 'col2']).apply(func3)\n\nIn [18]: t\nOut[18]: \ncol1  col2\nA     C       0.01\n      D       0.20\nB     C       1.64\n      D       0.36\n"", 'apply()', 'df.groupby', ' pd.DataFrame(t).reset_index()\n']";['df\nt\nimport pandas as pd\n\n\n\n\n'];['df\nt\nimport pandas as pd\n\n\n\n\n'];False;['import pandas as pd\ndf = pd.DataFrame()\ndf\nt\nimport pandas as pd\n\n\n\n\n'];True;0;0;[];[];0;0;[];[];0;0;[];[]
240;240;240;240;7.0;4;15006298;;1;23;<python><pandas><dataframe><ipython><ipython-notebook>;How to preview a part of a large pandas DataFrame?;40523.0;"['In [27]:\n\nevaluation = readCSV(""evaluation_MO_without_VNS_quality.csv"").filter([""solver"", ""instance"", ""runtime"", ""objective""])\n\nIn [37]:\n\nevaluation\n\nOut[37]:\n\n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 333 entries, 0 to 332\nData columns:\nsolver       333  non-null values\ninstance     333  non-null values\nruntime      333  non-null values\nobjective    333  non-null values\ndtypes: int64(1), object(3)\n']";"['In [27]:\n\nevaluation = readCSV(""evaluation_MO_without_VNS_quality.csv"").filter([""solver"", ""instance"", ""runtime"", ""objective""])\n\nIn [37]:\n\nevaluation\n\nOut[37]:\n\n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 333 entries, 0 to 332\nData columns:\nsolver       333  non-null values\ninstance     333  non-null values\nruntime      333  non-null values\nobjective    333  non-null values\ndtypes: int64(1), object(3)\n']";"['DataFrame', 'DataFrame', 'In [27]:\n\nevaluation = readCSV(""evaluation_MO_without_VNS_quality.csv"").filter([""solver"", ""instance"", ""runtime"", ""objective""])\n\nIn [37]:\n\nevaluation\n\nOut[37]:\n\n<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 333 entries, 0 to 332\nData columns:\nsolver       333  non-null values\ninstance     333  non-null values\nruntime      333  non-null values\nobjective    333  non-null values\ndtypes: int64(1), object(3)\n']";"['evaluation = readCSV(""evaluation_MO_without_VNS_quality.csv"").filter([""solver"", ""instance"", ""runtime"", ""objective""])\n\n\nevaluation\n\n']";"['evaluation = readCSV(""evaluation_MO_without_VNS_quality.csv"").filter([""solver"", ""instance"", ""runtime"", ""objective""])\n\n\nevaluation\n\n']";False;"['import pandas as pd\nevaluation = readCSV(""evaluation_MO_without_VNS_quality.csv"").filter([""solver"", ""instance"", ""runtime"", ""objective""])\n\n\nevaluation\n\n']";False;1;3;"['Sucess', ""name '_sw' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name '_sw' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];2;3;"['Sucess', ""name '_sw' is not defined"", 'Sucess']";['Sucess', 'NameError', 'Sucess']
241;241;241;241;2.0;1;15008970;;1;24;<csv><dataframe><pandas>;Way to read first few lines for pandas dataframe;24174.0;"[""import pandas as pd\nfrom StringIO import StringIO\n\nn = 20\nwith open('big_file.csv', 'r') as f:\n    head = ''.join(f.readlines(n))\n\ndf = pd.read_csv(StringIO(head))\n""]";"[""import pandas as pd\nfrom StringIO import StringIO\n\nn = 20\nwith open('big_file.csv', 'r') as f:\n    head = ''.join(f.readlines(n))\n\ndf = pd.read_csv(StringIO(head))\n""]";"['read_csv', 'n', 'footer_lines = total_lines - n', 'skipfooter', 'n', ""import pandas as pd\nfrom StringIO import StringIO\n\nn = 20\nwith open('big_file.csv', 'r') as f:\n    head = ''.join(f.readlines(n))\n\ndf = pd.read_csv(StringIO(head))\n""]";['import pandas as pd\nfrom StringIO import StringIO\n\nn = 20\n\ndf = pd.read_csv(StringIO(head))\n'];['import pandas as pd\nfrom StringIO import StringIO\n\nn = 20\n\ndf = pd.read_csv(StringIO(head))\n'];False;['import pandas as pd\nimport pandas as pd\nfrom StringIO import StringIO\n\nn = 20\n\ndf = pd.read_csv(StringIO(head))\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
242;242;242;242;4.0;4;15017072;;1;37;<python><pandas>;pandas read_csv and filter columns with usecols;74330.0;"['import pandas as pd\ncsv = r""""""dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nbar,20090103,b,5""""""\nf = open(\'foo.csv\', \'w\')\nf.write(csv)\nf.close()\n\ndf1 = pd.read_csv(\'foo.csv\', \n        index_col=[""date"", ""loc""], \n        usecols=[""dummy"", ""date"", ""loc"", ""x""],\n        parse_dates=[""date""],\n        header=0,\n        names=[""dummy"", ""date"", ""loc"", ""x""])\nprint df1\n\n# Ignore the dummy columns\ndf2 = pd.read_csv(\'foo.csv\', \n        index_col=[""date"", ""loc""], \n        usecols=[""date"", ""loc"", ""x""], # <----------- Changed\n        parse_dates=[""date""],\n        header=0,\n        names=[""dummy"", ""date"", ""loc"", ""x""])\nprint df2\nIn [118]: %run test.py\n               dummy  x\ndate       loc\n2009-01-01 a     bar  1\n2009-01-02 a     bar  3\n2009-01-03 a     bar  5\n2009-01-01 b     bar  1\n2009-01-02 b     bar  3\n2009-01-03 b     bar  5\n              date\ndate loc\na    1    20090101\n     3    20090102\n     5    20090103\nb    1    20090101\n     3    20090102\n     5    20090103\n']";"['import pandas as pd\ncsv = r""""""dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nbar,20090103,b,5""""""\nf = open(\'foo.csv\', \'w\')\nf.write(csv)\nf.close()\n\ndf1 = pd.read_csv(\'foo.csv\', \n        index_col=[""date"", ""loc""], \n        usecols=[""dummy"", ""date"", ""loc"", ""x""],\n        parse_dates=[""date""],\n        header=0,\n        names=[""dummy"", ""date"", ""loc"", ""x""])\nprint df1\n\n# Ignore the dummy columns\ndf2 = pd.read_csv(\'foo.csv\', \n        index_col=[""date"", ""loc""], \n        usecols=[""date"", ""loc"", ""x""], # <----------- Changed\n        parse_dates=[""date""],\n        header=0,\n        names=[""dummy"", ""date"", ""loc"", ""x""])\nprint df2\n', 'In [118]: %run test.py\n               dummy  x\ndate       loc\n2009-01-01 a     bar  1\n2009-01-02 a     bar  3\n2009-01-03 a     bar  5\n2009-01-01 b     bar  1\n2009-01-02 b     bar  3\n2009-01-03 b     bar  5\n              date\ndate loc\na    1    20090101\n     3    20090102\n     5    20090103\nb    1    20090101\n     3    20090102\n     5    20090103\n']";"['pandas.read_csv', 'usecols', 'import pandas as pd\ncsv = r""""""dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nbar,20090103,b,5""""""\nf = open(\'foo.csv\', \'w\')\nf.write(csv)\nf.close()\n\ndf1 = pd.read_csv(\'foo.csv\', \n        index_col=[""date"", ""loc""], \n        usecols=[""dummy"", ""date"", ""loc"", ""x""],\n        parse_dates=[""date""],\n        header=0,\n        names=[""dummy"", ""date"", ""loc"", ""x""])\nprint df1\n\n# Ignore the dummy columns\ndf2 = pd.read_csv(\'foo.csv\', \n        index_col=[""date"", ""loc""], \n        usecols=[""date"", ""loc"", ""x""], # <----------- Changed\n        parse_dates=[""date""],\n        header=0,\n        names=[""dummy"", ""date"", ""loc"", ""x""])\nprint df2\n', 'In [118]: %run test.py\n               dummy  x\ndate       loc\n2009-01-01 a     bar  1\n2009-01-02 a     bar  3\n2009-01-03 a     bar  5\n2009-01-01 b     bar  1\n2009-01-02 b     bar  3\n2009-01-03 b     bar  5\n              date\ndate loc\na    1    20090101\n     3    20090102\n     5    20090103\nb    1    20090101\n     3    20090102\n     5    20090103\n']";"[""import pandas as pd\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nf = open('foo.csv', 'w')\nf.write(csv)\nf.close()\n\n\n# Ignore the dummy columns\n""]";"[""import pandas as pd\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nf = open('foo.csv', 'w')\nf.write(csv)\nf.close()\n\n\n# Ignore the dummy columns\n""]";False;"[""import pandas as pd\nimport pandas as pd\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nf = open('foo.csv', 'w')\nf.write(csv)\nf.close()\n\n\n# Ignore the dummy columns\n""]";False;0;3;"[""No module named 'StringIO'"", ""No module named 'StringIO'"", ""No module named 'StringIO'""]";['ImportError', 'ImportError', 'ImportError'];0;3;"[""No module named 'StringIO'"", ""No module named 'StringIO'"", ""No module named 'StringIO'""]";['ImportError', 'ImportError', 'ImportError'];0;3;"[""No module named 'StringIO'"", ""No module named 'StringIO'"", ""No module named 'StringIO'""]";['ImportError', 'ImportError', 'ImportError']
243;243;243;243;2.0;0;15026698;;1;30;<python><csv><pandas><dataframe><whitespace>;How to make separator in read_csv more flexible wrt whitespace?;25215.0;['for line in file(file_name):\n   fld = line.split()\n'];['for line in file(file_name):\n   fld = line.split()\n'];['read_csv', '\\t', 'for line in file(file_name):\n   fld = line.split()\n'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
244;244;244;244;1.0;1;15069814;;1;11;<python><latex><pandas>;Formatting latex (to_latex) output;4939.0;[''];[];['to_latex', 'to_latex', '&'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
245;245;245;245;3.0;0;15072005;;1;14;<python><pandas>;keep/slice specific columns in pandas;20039.0;[''];[];"['df2 = df[[""col1"", ""col2"", ""col3""]]', 'df2 = df.ix[:,0:2]', 'df', 'df2 = df.ix[:, [0:2, ""col5""]]']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'col_' is not defined""]";['NameError']
246;246;246;246;1.0;0;15072626;;1;13;<python><pandas><group-by>;Get group id back into pandas dataframe;5077.0;"[""In [2]: df = pd.DataFrame({'Name': ['foo', 'bar'] * 3,\n   ...:                    'Rank': np.random.randint(0,3,6),\n   ...:                    'Val': np.random.rand(6)})\n   ...: df\nOut[2]: \n  Name  Rank       Val\n0  foo     0  0.299397\n1  bar     0  0.909228\n2  foo     0  0.517700\n3  bar     0  0.929863\n4  foo     1  0.209324\n5  bar     2  0.381515\nIn [3]: group = df.groupby(['Name', 'Rank'])\nIn [4]: agg = group.agg(sum)\nIn [5]: agg\nOut[5]: \n                Val\nName Rank          \nbar  0     1.839091\n     2     0.381515\nfoo  0     0.817097\n     1     0.209324\nIn [13]: df['Group_id'] = [2, 0, 2, 0, 3, 1]\nIn [14]: df\nOut[14]: \n  Name  Rank       Val  Group_id\n0  foo     0  0.299397         2\n1  bar     0  0.909228         0\n2  foo     0  0.517700         2\n3  bar     0  0.929863         0\n4  foo     1  0.209324         3\n5  bar     2  0.381515         1\nIn [16]: from itertools import count\nIn [17]: c = count()\nIn [22]: group.transform(lambda x: c.next())\nOut[22]: \n   Val\n0    2\n1    0\n2    2\n3    0\n4    3\n5    1\n""]";"[""In [2]: df = pd.DataFrame({'Name': ['foo', 'bar'] * 3,\n   ...:                    'Rank': np.random.randint(0,3,6),\n   ...:                    'Val': np.random.rand(6)})\n   ...: df\nOut[2]: \n  Name  Rank       Val\n0  foo     0  0.299397\n1  bar     0  0.909228\n2  foo     0  0.517700\n3  bar     0  0.929863\n4  foo     1  0.209324\n5  bar     2  0.381515\n"", ""In [3]: group = df.groupby(['Name', 'Rank'])\nIn [4]: agg = group.agg(sum)\nIn [5]: agg\nOut[5]: \n                Val\nName Rank          \nbar  0     1.839091\n     2     0.381515\nfoo  0     0.817097\n     1     0.209324\n"", ""In [13]: df['Group_id'] = [2, 0, 2, 0, 3, 1]\nIn [14]: df\nOut[14]: \n  Name  Rank       Val  Group_id\n0  foo     0  0.299397         2\n1  bar     0  0.909228         0\n2  foo     0  0.517700         2\n3  bar     0  0.929863         0\n4  foo     1  0.209324         3\n5  bar     2  0.381515         1\n"", 'In [16]: from itertools import count\nIn [17]: c = count()\nIn [22]: group.transform(lambda x: c.next())\nOut[22]: \n   Val\n0    2\n1    0\n2    2\n3    0\n4    3\n5    1\n']";"[""In [2]: df = pd.DataFrame({'Name': ['foo', 'bar'] * 3,\n   ...:                    'Rank': np.random.randint(0,3,6),\n   ...:                    'Val': np.random.rand(6)})\n   ...: df\nOut[2]: \n  Name  Rank       Val\n0  foo     0  0.299397\n1  bar     0  0.909228\n2  foo     0  0.517700\n3  bar     0  0.929863\n4  foo     1  0.209324\n5  bar     2  0.381515\n"", ""In [3]: group = df.groupby(['Name', 'Rank'])\nIn [4]: agg = group.agg(sum)\nIn [5]: agg\nOut[5]: \n                Val\nName Rank          \nbar  0     1.839091\n     2     0.381515\nfoo  0     0.817097\n     1     0.209324\n"", 'df', ""In [13]: df['Group_id'] = [2, 0, 2, 0, 3, 1]\nIn [14]: df\nOut[14]: \n  Name  Rank       Val  Group_id\n0  foo     0  0.299397         2\n1  bar     0  0.909228         0\n2  foo     0  0.517700         2\n3  bar     0  0.929863         0\n4  foo     1  0.209324         3\n5  bar     2  0.381515         1\n"", 'In [16]: from itertools import count\nIn [17]: c = count()\nIn [22]: group.transform(lambda x: c.next())\nOut[22]: \n   Val\n0    2\n1    0\n2    2\n3    0\n4    3\n5    1\n']";"[""group = df.groupby(['Name', 'Rank'])\ndf['Group_id'] = [2, 0, 2, 0, 3, 1]\nfrom itertools import count\n""]";"[""group = df.groupby(['Name', 'Rank'])\ndf['Group_id'] = [2, 0, 2, 0, 3, 1]\nfrom itertools import count\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ngroup = df.groupby(['Name', 'Rank'])\ndf['Group_id'] = [2, 0, 2, 0, 3, 1]\nfrom itertools import count\n""]";True;0;1;"[""name 'grouped' is not defined""]";['NameError'];0;1;"[""name 'grouped' is not defined""]";['NameError'];0;1;"[""name 'grouped' is not defined""]";['NameError']
247;247;247;247;4.0;0;15079118;;1;11;<python><datatables><pandas>;JS dataTables from pandas;1890.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
248;248;248;248;1.0;0;15112234;;1;17;<python><dataframe><pandas>;converting dataframe into a list;30212.0;['2u    2s    4r     4n     4m   7h   7v\n0     1     1      0      0     0    1\n0     1     0      1      0     0    1\n1     0     0      1      0     1    0\n1     0     0      0      1     1    0\n1     0     1      0      0     1    0\n0     1     1      0      0     0    1\nX= [\n [0,0,1,1,1,0],\n [1,1,0,0,0,1],\n [1,0,0,0,1,1],\n [0,1,1,0,0,0],\n [0,0,0,1,0,0],\n [0,0,1,1,1,0],\n [1,1,0,0,0,1]\n ]\n'];['2u    2s    4r     4n     4m   7h   7v\n0     1     1      0      0     0    1\n0     1     0      1      0     0    1\n1     0     0      1      0     1    0\n1     0     0      0      1     1    0\n1     0     1      0      0     1    0\n0     1     1      0      0     0    1\n', 'X= [\n [0,0,1,1,1,0],\n [1,1,0,0,0,1],\n [1,0,0,0,1,1],\n [0,1,1,0,0,0],\n [0,0,0,1,0,0],\n [0,0,1,1,1,0],\n [1,1,0,0,0,1]\n ]\n'];['2u    2s    4r     4n     4m   7h   7v\n0     1     1      0      0     0    1\n0     1     0      1      0     0    1\n1     0     0      1      0     1    0\n1     0     0      0      1     1    0\n1     0     1      0      0     1    0\n0     1     1      0      0     0    1\n', 'X= [\n [0,0,1,1,1,0],\n [1,1,0,0,0,1],\n [1,0,0,0,1,1],\n [0,1,1,0,0,0],\n [0,0,0,1,0,0],\n [0,0,1,1,1,0],\n [1,1,0,0,0,1]\n ]\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
249;249;249;249;4.0;0;15118111;;1;24;<python><pandas>;Apply function to each row of pandas dataframe to create two new columns;61486.0;"['<class \'pandas.core.frame.DataFrame\'>\nDatetimeIndex: 53732 entries, 1993-01-07 12:23:58 to 2012-12-02 20:06:23\nData columns:\nDate(dd-mm-yy)_Time(hh-mm-ss)       53732  non-null values\nJulian_Day                          53732  non-null values\nAOT_1020                            53716  non-null values\nAOT_870                             53732  non-null values\nAOT_675                             53188  non-null values\nAOT_500                             51687  non-null values\nAOT_440                             53727  non-null values\nAOT_380                             51864  non-null values\nAOT_340                             52852  non-null values\nWater(cm)                           51687  non-null values\n%TripletVar_1020                    53710  non-null values\n%TripletVar_870                     53726  non-null values\n%TripletVar_675                     53182  non-null values\n%TripletVar_500                     51683  non-null values\n%TripletVar_440                     53721  non-null values\n%TripletVar_380                     51860  non-null values\n%TripletVar_340                     52846  non-null values\n440-870Angstrom                     53732  non-null values\n380-500Angstrom                     52253  non-null values\n440-675Angstrom                     53732  non-null values\n500-870Angstrom                     53732  non-null values\n340-440Angstrom                     53277  non-null values\nLast_Processing_Date(dd/mm/yyyy)    53732  non-null values\nSolar_Zenith_Angle                  53732  non-null values\ndtypes: datetime64[ns](1), float64(22), object(1)\ndef calculate(s):\n    a = s[\'path\'] + 2*s[\'row\'] # Simple calc for example\n    b = s[\'path\'] * 0.153\n    return (a, b)\nst.apply(calculate, axis=1)\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n<ipython-input-248-acb7a44054a7> in <module>()\n----> 1 st.apply(calculate, axis=1)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in apply(self, func, axis, broadcast, raw, args, **kwds)\n   4191                     return self._apply_raw(f, axis)\n   4192                 else:\n-> 4193                     return self._apply_standard(f, axis)\n   4194             else:\n   4195                 return self._apply_broadcast(f, axis)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _apply_standard(self, func, axis, ignore_failures)\n   4274                 index = None\n   4275 \n-> 4276             result = self._constructor(data=results, index=index)\n   4277             result.rename(columns=dict(zip(range(len(res_index)), res_index)),\n   4278                           inplace=True)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in __init__(self, data, index, columns, dtype, copy)\n    390             mgr = self._init_mgr(data, index, columns, dtype=dtype, copy=copy)\n    391         elif isinstance(data, dict):\n--> 392             mgr = self._init_dict(data, index, columns, dtype=dtype)\n    393         elif isinstance(data, ma.MaskedArray):\n    394             mask = ma.getmaskarray(data)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _init_dict(self, data, index, columns, dtype)\n    521 \n    522         return _arrays_to_mgr(arrays, data_names, index, columns,\n--> 523                               dtype=dtype)\n    524 \n    525     def _init_ndarray(self, values, index, columns, dtype=None,\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _arrays_to_mgr(arrays, arr_names, index, columns, dtype)\n   5411 \n   5412     # consolidate for now\n-> 5413     mgr = BlockManager(blocks, axes)\n   5414     return mgr.consolidate()\n   5415 \n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in __init__(self, blocks, axes, do_integrity_check)\n    802 \n    803         if do_integrity_check:\n--> 804             self._verify_integrity()\n    805 \n    806         self._consolidate_check()\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in _verify_integrity(self)\n    892                                      ""items"")\n    893             if block.values.shape[1:] != mgr_shape[1:]:\n--> 894                 raise AssertionError(\'Block shape incompatible with manager\')\n    895         tot_items = sum(len(x.items) for x in self.blocks)\n    896         if len(self.items) != tot_items:\n\nAssertionError: Block shape incompatible with manager\nst[\'a\'] = None\nst[\'b\'] = None\nfor i in st.index:\n    # do calc here\n    st.ix[i][\'a\'] = a\n    st.ix[i][\'b\'] = b\n']";"[""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 53732 entries, 1993-01-07 12:23:58 to 2012-12-02 20:06:23\nData columns:\nDate(dd-mm-yy)_Time(hh-mm-ss)       53732  non-null values\nJulian_Day                          53732  non-null values\nAOT_1020                            53716  non-null values\nAOT_870                             53732  non-null values\nAOT_675                             53188  non-null values\nAOT_500                             51687  non-null values\nAOT_440                             53727  non-null values\nAOT_380                             51864  non-null values\nAOT_340                             52852  non-null values\nWater(cm)                           51687  non-null values\n%TripletVar_1020                    53710  non-null values\n%TripletVar_870                     53726  non-null values\n%TripletVar_675                     53182  non-null values\n%TripletVar_500                     51683  non-null values\n%TripletVar_440                     53721  non-null values\n%TripletVar_380                     51860  non-null values\n%TripletVar_340                     52846  non-null values\n440-870Angstrom                     53732  non-null values\n380-500Angstrom                     52253  non-null values\n440-675Angstrom                     53732  non-null values\n500-870Angstrom                     53732  non-null values\n340-440Angstrom                     53277  non-null values\nLast_Processing_Date(dd/mm/yyyy)    53732  non-null values\nSolar_Zenith_Angle                  53732  non-null values\ndtypes: datetime64[ns](1), float64(22), object(1)\n"", ""def calculate(s):\n    a = s['path'] + 2*s['row'] # Simple calc for example\n    b = s['path'] * 0.153\n    return (a, b)\n"", 'st.apply(calculate, axis=1)\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n<ipython-input-248-acb7a44054a7> in <module>()\n----> 1 st.apply(calculate, axis=1)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in apply(self, func, axis, broadcast, raw, args, **kwds)\n   4191                     return self._apply_raw(f, axis)\n   4192                 else:\n-> 4193                     return self._apply_standard(f, axis)\n   4194             else:\n   4195                 return self._apply_broadcast(f, axis)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _apply_standard(self, func, axis, ignore_failures)\n   4274                 index = None\n   4275 \n-> 4276             result = self._constructor(data=results, index=index)\n   4277             result.rename(columns=dict(zip(range(len(res_index)), res_index)),\n   4278                           inplace=True)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in __init__(self, data, index, columns, dtype, copy)\n    390             mgr = self._init_mgr(data, index, columns, dtype=dtype, copy=copy)\n    391         elif isinstance(data, dict):\n--> 392             mgr = self._init_dict(data, index, columns, dtype=dtype)\n    393         elif isinstance(data, ma.MaskedArray):\n    394             mask = ma.getmaskarray(data)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _init_dict(self, data, index, columns, dtype)\n    521 \n    522         return _arrays_to_mgr(arrays, data_names, index, columns,\n--> 523                               dtype=dtype)\n    524 \n    525     def _init_ndarray(self, values, index, columns, dtype=None,\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _arrays_to_mgr(arrays, arr_names, index, columns, dtype)\n   5411 \n   5412     # consolidate for now\n-> 5413     mgr = BlockManager(blocks, axes)\n   5414     return mgr.consolidate()\n   5415 \n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in __init__(self, blocks, axes, do_integrity_check)\n    802 \n    803         if do_integrity_check:\n--> 804             self._verify_integrity()\n    805 \n    806         self._consolidate_check()\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in _verify_integrity(self)\n    892                                      ""items"")\n    893             if block.values.shape[1:] != mgr_shape[1:]:\n--> 894                 raise AssertionError(\'Block shape incompatible with manager\')\n    895         tot_items = sum(len(x.items) for x in self.blocks)\n    896         if len(self.items) != tot_items:\n\nAssertionError: Block shape incompatible with manager\n', ""st['a'] = None\nst['b'] = None\n"", ""for i in st.index:\n    # do calc here\n    st.ix[i]['a'] = a\n    st.ix[i]['b'] = b\n""]";"['st', ""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 53732 entries, 1993-01-07 12:23:58 to 2012-12-02 20:06:23\nData columns:\nDate(dd-mm-yy)_Time(hh-mm-ss)       53732  non-null values\nJulian_Day                          53732  non-null values\nAOT_1020                            53716  non-null values\nAOT_870                             53732  non-null values\nAOT_675                             53188  non-null values\nAOT_500                             51687  non-null values\nAOT_440                             53727  non-null values\nAOT_380                             51864  non-null values\nAOT_340                             52852  non-null values\nWater(cm)                           51687  non-null values\n%TripletVar_1020                    53710  non-null values\n%TripletVar_870                     53726  non-null values\n%TripletVar_675                     53182  non-null values\n%TripletVar_500                     51683  non-null values\n%TripletVar_440                     53721  non-null values\n%TripletVar_380                     51860  non-null values\n%TripletVar_340                     52846  non-null values\n440-870Angstrom                     53732  non-null values\n380-500Angstrom                     52253  non-null values\n440-675Angstrom                     53732  non-null values\n500-870Angstrom                     53732  non-null values\n340-440Angstrom                     53277  non-null values\nLast_Processing_Date(dd/mm/yyyy)    53732  non-null values\nSolar_Zenith_Angle                  53732  non-null values\ndtypes: datetime64[ns](1), float64(22), object(1)\n"", 'apply', 'apply', 'Series', ""def calculate(s):\n    a = s['path'] + 2*s['row'] # Simple calc for example\n    b = s['path'] * 0.153\n    return (a, b)\n"", 'st.apply(calculate, axis=1)\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n<ipython-input-248-acb7a44054a7> in <module>()\n----> 1 st.apply(calculate, axis=1)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in apply(self, func, axis, broadcast, raw, args, **kwds)\n   4191                     return self._apply_raw(f, axis)\n   4192                 else:\n-> 4193                     return self._apply_standard(f, axis)\n   4194             else:\n   4195                 return self._apply_broadcast(f, axis)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _apply_standard(self, func, axis, ignore_failures)\n   4274                 index = None\n   4275 \n-> 4276             result = self._constructor(data=results, index=index)\n   4277             result.rename(columns=dict(zip(range(len(res_index)), res_index)),\n   4278                           inplace=True)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in __init__(self, data, index, columns, dtype, copy)\n    390             mgr = self._init_mgr(data, index, columns, dtype=dtype, copy=copy)\n    391         elif isinstance(data, dict):\n--> 392             mgr = self._init_dict(data, index, columns, dtype=dtype)\n    393         elif isinstance(data, ma.MaskedArray):\n    394             mask = ma.getmaskarray(data)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _init_dict(self, data, index, columns, dtype)\n    521 \n    522         return _arrays_to_mgr(arrays, data_names, index, columns,\n--> 523                               dtype=dtype)\n    524 \n    525     def _init_ndarray(self, values, index, columns, dtype=None,\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _arrays_to_mgr(arrays, arr_names, index, columns, dtype)\n   5411 \n   5412     # consolidate for now\n-> 5413     mgr = BlockManager(blocks, axes)\n   5414     return mgr.consolidate()\n   5415 \n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in __init__(self, blocks, axes, do_integrity_check)\n    802 \n    803         if do_integrity_check:\n--> 804             self._verify_integrity()\n    805 \n    806         self._consolidate_check()\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in _verify_integrity(self)\n    892                                      ""items"")\n    893             if block.values.shape[1:] != mgr_shape[1:]:\n--> 894                 raise AssertionError(\'Block shape incompatible with manager\')\n    895         tot_items = sum(len(x.items) for x in self.blocks)\n    896         if len(self.items) != tot_items:\n\nAssertionError: Block shape incompatible with manager\n', 'apply', 'None', ""st['a'] = None\nst['b'] = None\n"", 'None', ""for i in st.index:\n    # do calc here\n    st.ix[i]['a'] = a\n    st.ix[i]['b'] = b\n""]";"[""st.apply(calculate, axis=1)\n\n\n\n\n\n\n\n\nst['a'] = None\nst['b'] = None\n    # do calc here\n""]";"[""st.apply(calculate, axis=1)\n\n\n\n\n\n\n\n\nst['a'] = None\nst['b'] = None\n    # do calc here\n""]";False;"[""import pandas as pd\nst.apply(calculate, axis=1)\n\n\n\n\n\n\n\n\nst['a'] = None\nst['b'] = None\n    # do calc here\n""]";False;0;2;"[""name 'a' is not defined"", ""name 'st' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'a' is not defined"", ""name 'st' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'a' is not defined"", ""name 'st' is not defined""]";['NameError', 'NameError']
250;250;250;250;3.0;2;15124439;;1;18;<python><r><pandas>;Closest equivalent of a factor variable in Python Pandas;4996.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'Series' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'Series' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'Series' is not defined""]";['Sucess', 'NameError']
251;251;251;251;3.0;1;15138973;;1;13;<pandas><frequency><series>;How to get the number of the most frequent value in a column?;16451.0;"[""items_counts = df['item'].value_counts()\nmax_item = items_counts.max()\nValueError: cannot convert float NaN to integer\n""]";"[""items_counts = df['item'].value_counts()\nmax_item = items_counts.max()\n"", 'ValueError: cannot convert float NaN to integer\n']";"[""items_counts = df['item'].value_counts()\nmax_item = items_counts.max()\n"", 'ValueError: cannot convert float NaN to integer\n']";"[""items_counts = df['item'].value_counts()\nmax_item = items_counts.max()\n""]";"[""items_counts = df['item'].value_counts()\nmax_item = items_counts.max()\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nitems_counts = df['item'].value_counts()\nmax_item = items_counts.max()\n""]";True;1;2;"['Sucess', ""No module named 'scipy'""]";['Sucess', 'ImportError'];1;2;"['Sucess', ""No module named 'scipy'""]";['Sucess', 'ImportError'];1;2;"['Sucess', ""No module named 'scipy'""]";['Sucess', 'ImportError']
252;252;252;252;3.0;0;15143842;;1;13;<python><dataframe><pandas>;boolean indexing that can produce a view to a large pandas dataframe?;1892.0;[''];[];['view', 'copy', '.ix', 'df.ix[]', 'df', 'C!=0, A==10, B<30,...'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'C'""]";['KeyError']
253;253;253;253;2.0;0;15203623;;1;30;<python><pandas>;Convert pandas DateTimeIndex to Unix Time?;20077.0;['[time.mktime(t.timetuple()) for t in my_data_frame.index.to_pydatetime()]\n'];['[time.mktime(t.timetuple()) for t in my_data_frame.index.to_pydatetime()]\n'];['[time.mktime(t.timetuple()) for t in my_data_frame.index.to_pydatetime()]\n'];['[time.mktime(t.timetuple()) for t in my_data_frame.index.to_pydatetime()]\n'];['[time.mktime(t.timetuple()) for t in my_data_frame.index.to_pydatetime()]\n'];False;['import pandas as pd\ndata = pd.DataFrame()\n[time.mktime(t.timetuple()) for t in my_data_frame.index.to_pydatetime()]\n'];True;0;2;"[""name 'pd' is not defined"", ""name 'index' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'time' is not defined"", ""name 'index' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'time' is not defined"", ""name 'index' is not defined""]";['NameError', 'NameError']
254;254;254;254;2.0;2;15210962;;1;34;<numpy><pandas>;Specifying dtype float32 with pandas.read_csv on pandas 0.10.1;52865.0;"["">>> cat test.out\na b\n0.76398 0.81394\n0.32136 0.91063\n>>> import pandas\n>>> import numpy\n>>> x = pandas.read_csv('test.out', dtype={'a': numpy.float32}, delim_whitespace=True)\n>>> x\n         a        b\n0  0.76398  0.81394\n1  0.32136  0.91063\n>>> x.a.dtype\ndtype('float64')\nAttributeError: 'NoneType' object has no attribute 'dtype'\n>>> !uname -a\nLinux ubuntu 3.0.0-13-generic #22-Ubuntu SMP Wed Nov 2 13:25:36 UTC 2011 i686 i686 i386 GNU/Linux\n>>> import platform\n>>> platform.architecture()\n('32bit', 'ELF')\n>>> pandas.__version__\n'0.10.1'\n""]";"["">>> cat test.out\na b\n0.76398 0.81394\n0.32136 0.91063\n>>> import pandas\n>>> import numpy\n>>> x = pandas.read_csv('test.out', dtype={'a': numpy.float32}, delim_whitespace=True)\n>>> x\n         a        b\n0  0.76398  0.81394\n1  0.32136  0.91063\n>>> x.a.dtype\ndtype('float64')\n"", ""AttributeError: 'NoneType' object has no attribute 'dtype'\n"", "">>> !uname -a\nLinux ubuntu 3.0.0-13-generic #22-Ubuntu SMP Wed Nov 2 13:25:36 UTC 2011 i686 i686 i386 GNU/Linux\n>>> import platform\n>>> platform.architecture()\n('32bit', 'ELF')\n>>> pandas.__version__\n'0.10.1'\n""]";"['read_csv', 'dtype', 'read_csv', 'converters', "">>> cat test.out\na b\n0.76398 0.81394\n0.32136 0.91063\n>>> import pandas\n>>> import numpy\n>>> x = pandas.read_csv('test.out', dtype={'a': numpy.float32}, delim_whitespace=True)\n>>> x\n         a        b\n0  0.76398  0.81394\n1  0.32136  0.91063\n>>> x.a.dtype\ndtype('float64')\n"", 'dtype', 'numpy.int32', 'numpy.int64', ""AttributeError: 'NoneType' object has no attribute 'dtype'\n"", 'AttributeError', "">>> !uname -a\nLinux ubuntu 3.0.0-13-generic #22-Ubuntu SMP Wed Nov 2 13:25:36 UTC 2011 i686 i686 i386 GNU/Linux\n>>> import platform\n>>> platform.architecture()\n('32bit', 'ELF')\n>>> pandas.__version__\n'0.10.1'\n""]";"[""dtype('float64')\n('32bit', 'ELF')\n'0.10.1'\n""]";"[""dtype('float64')\n('32bit', 'ELF')\n'0.10.1'\n""]";False;"[""import pandas as pd\ndtype('float64')\n('32bit', 'ELF')\n'0.10.1'\n""]";False;0;2;"[""name 'pd' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'StringIO' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'StringIO' is not defined"", ""'DataFrame' object has no attribute 'a'""]";['NameError', 'AttributeError']
255;255;255;255;3.0;0;15222754;;1;23;<python><pandas>;Group by pandas dataframe and select most common string factor;15902.0;"[""import pandas as pd\nfrom scipy import stats\n\nsource = pd.DataFrame({'Country' : ['USA', 'USA', 'Russia','USA'], \n                  'City' : ['New-York', 'New-York', 'Sankt-Petersburg', 'New-York'],\n                  'Short name' : ['NY','New','Spb','NY']})\n\nprint source.groupby(['Country','City']).agg(lambda x: stats.mode(x['Short name'])[0])\n""]";"[""import pandas as pd\nfrom scipy import stats\n\nsource = pd.DataFrame({'Country' : ['USA', 'USA', 'Russia','USA'], \n                  'City' : ['New-York', 'New-York', 'Sankt-Petersburg', 'New-York'],\n                  'Short name' : ['NY','New','Spb','NY']})\n\nprint source.groupby(['Country','City']).agg(lambda x: stats.mode(x['Short name'])[0])\n""]";"[""import pandas as pd\nfrom scipy import stats\n\nsource = pd.DataFrame({'Country' : ['USA', 'USA', 'Russia','USA'], \n                  'City' : ['New-York', 'New-York', 'Sankt-Petersburg', 'New-York'],\n                  'Short name' : ['NY','New','Spb','NY']})\n\nprint source.groupby(['Country','City']).agg(lambda x: stats.mode(x['Short name'])[0])\n""]";['import pandas as pd\nfrom scipy import stats\n\n\n'];['import pandas as pd\nfrom scipy import stats\n\n\n'];False;['import pandas as pd\nimport pandas as pd\nfrom scipy import stats\n\n\n'];False;0;2;"[""name 'source' is not defined"", ""name 'source' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'source' is not defined"", ""name 'source' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'source' is not defined"", ""name 'source' is not defined""]";['NameError', 'NameError']
256;256;256;256;3.0;0;15242746;;1;38;<python><pandas>;Handling Variable Number of Columns with Pandas - Python;15050.0;"[""1,2,3\n1,2,3,4\n1,2,3,4,5\n1,2\n1,2,3,4\n....\nimport pandas as pd\nmy_cols=['A','B','C','D','E']\nmy_df=pd.read_table(path,sep=',',header=None,names=my_cols)\n""]";"['1,2,3\n1,2,3,4\n1,2,3,4,5\n1,2\n1,2,3,4\n....\n', ""import pandas as pd\nmy_cols=['A','B','C','D','E']\nmy_df=pd.read_table(path,sep=',',header=None,names=my_cols)\n""]";"['1,2,3\n1,2,3,4\n1,2,3,4,5\n1,2\n1,2,3,4\n....\n', ""import pandas as pd\nmy_cols=['A','B','C','D','E']\nmy_df=pd.read_table(path,sep=',',header=None,names=my_cols)\n""]";"[""1,2,3\n1,2,3,4\n1,2,3,4,5\n1,2\n1,2,3,4\nimport pandas as pd\nmy_cols=['A','B','C','D','E']\nmy_df=pd.read_table(path,sep=',',header=None,names=my_cols)\n""]";"[""1,2,3\n1,2,3,4\n1,2,3,4,5\n1,2\n1,2,3,4\nimport pandas as pd\nmy_cols=['A','B','C','D','E']\nmy_df=pd.read_table(path,sep=',',header=None,names=my_cols)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\n1,2,3\n1,2,3,4\n1,2,3,4,5\n1,2\n1,2,3,4\nimport pandas as pd\nmy_cols=['A','B','C','D','E']\nmy_df=pd.read_table(path,sep=',',header=None,names=my_cols)\n""]";True;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
257;257;257;257;5.0;0;15247628;;1;19;<python><group-by><pandas>;How to find duplicate names using pandas?;25826.0;['funcs_groups = funcs.groupby(funcs.name)\nfuncs_groups[(funcs_groups.count().name>1)]\n'];['funcs_groups = funcs.groupby(funcs.name)\nfuncs_groups[(funcs_groups.count().name>1)]\n'];['pandas.DataFrame', 'name', 'funcs_groups = funcs.groupby(funcs.name)\nfuncs_groups[(funcs_groups.count().name>1)]\n'];['funcs_groups = funcs.groupby(funcs.name)\nfuncs_groups[(funcs_groups.count().name>1)]\n'];['funcs_groups = funcs.groupby(funcs.name)\nfuncs_groups[(funcs_groups.count().name>1)]\n'];False;['import pandas as pd\nfuncs_groups = funcs.groupby(funcs.name)\nfuncs_groups[(funcs_groups.count().name>1)]\n'];False;0;2;"[""name 'df' is not defined"", ""name 'x' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'x' is not defined""]";['NameError', 'NameError'];0;2;"['not enough values to unpack (expected 2, got 0)', ""name 'x' is not defined""]";['ValueError', 'NameError']
258;258;258;258;2.0;0;15259547;;1;12;<python><r><pandas><data.table>;conditional sums for pandas aggregate;4486.0;"['   A_id       B    C\n1:   a1    ""up""  100\n2:   a2  ""down""  102\n3:   a3    ""up""  100\n3:   a3    ""up""  250\n4:   a4  ""left""  100\n5:   a5 ""right""  102\n   A_id_grouped   sum_up   sum_down  ...  over_200_up\n1:           a1        1          0  ...            0\n2:           a2        0          1                 0\n3:           a3        2          0  ...            1\n4:           a4        0          0                 0\n5:           a5        0          0  ...            0\n>DT[ ,list(A_id_grouped, sum_up = sum(B == ""up""),\n+  sum_down = sum(B == ""down""), \n+  ...,\n+  over_200_up = sum(up == ""up"" & < 200), by=list(A)];\nDT.agg({""D"": [np.sum(DT[DT[""B""]==""up""]),np.sum(DT[DT[""B""]==""up""])], ...\n    ""C"": np.sum(DT[(DT[""B""]==""up"") & (DT[""C""]>200)])\n    })\n']";"['   A_id       B    C\n1:   a1    ""up""  100\n2:   a2  ""down""  102\n3:   a3    ""up""  100\n3:   a3    ""up""  250\n4:   a4  ""left""  100\n5:   a5 ""right""  102\n', '   A_id_grouped   sum_up   sum_down  ...  over_200_up\n1:           a1        1          0  ...            0\n2:           a2        0          1                 0\n3:           a3        2          0  ...            1\n4:           a4        0          0                 0\n5:           a5        0          0  ...            0\n', '>DT[ ,list(A_id_grouped, sum_up = sum(B == ""up""),\n+  sum_down = sum(B == ""down""), \n+  ...,\n+  over_200_up = sum(up == ""up"" & < 200), by=list(A)];\n', 'DT.agg({""D"": [np.sum(DT[DT[""B""]==""up""]),np.sum(DT[DT[""B""]==""up""])], ...\n    ""C"": np.sum(DT[(DT[""B""]==""up"") & (DT[""C""]>200)])\n    })\n']";"['   A_id       B    C\n1:   a1    ""up""  100\n2:   a2  ""down""  102\n3:   a3    ""up""  100\n3:   a3    ""up""  250\n4:   a4  ""left""  100\n5:   a5 ""right""  102\n', '   A_id_grouped   sum_up   sum_down  ...  over_200_up\n1:           a1        1          0  ...            0\n2:           a2        0          1                 0\n3:           a3        2          0  ...            1\n4:           a4        0          0                 0\n5:           a5        0          0  ...            0\n', '>DT[ ,list(A_id_grouped, sum_up = sum(B == ""up""),\n+  sum_down = sum(B == ""down""), \n+  ...,\n+  over_200_up = sum(up == ""up"" & < 200), by=list(A)];\n', 'DT.agg({""D"": [np.sum(DT[DT[""B""]==""up""]),np.sum(DT[DT[""B""]==""up""])], ...\n    ""C"": np.sum(DT[(DT[""B""]==""up"") & (DT[""C""]>200)])\n    })\n']";['+  ...,\n'];['+  ...,\n'];False;['import pandas as pd\n+  ...,\n'];False;0;1;"[""name 'A_id' is not defined""]";['NameError'];0;1;"[""name 'A_id' is not defined""]";['NameError'];0;1;"[""name 'A_id' is not defined""]";['NameError']
259;259;259;259;2.0;0;15297053;;1;15;<pandas><time-series><average><resampling>;How can I divide single values of a dataframe by monthly averages?;9735.0;"[""2014-01-01 00:15:00  1269.6      \n2014-01-01 00:30:00  1161.6      \n2014-01-01 00:45:00  1466.4      \n2014-01-01 01:00:00  1365.6      \n2014-01-01 01:15:00  1362.6      \n2014-01-01 01:30:00  1064.0      \n2014-01-01 01:45:00  1171.2      \n2014-01-01 02:00:00  1171.0      \n2014-01-01 02:15:00  1330.4      \n2014-01-01 02:30:00  1309.6      \n2014-01-01 02:45:00  1308.4      \n2014-01-01 03:00:00  1494.0    \ndata_Monthly = data.resample('1M', how='mean')\n""]";"['2014-01-01 00:15:00  1269.6      \n2014-01-01 00:30:00  1161.6      \n2014-01-01 00:45:00  1466.4      \n2014-01-01 01:00:00  1365.6      \n2014-01-01 01:15:00  1362.6      \n2014-01-01 01:30:00  1064.0      \n2014-01-01 01:45:00  1171.2      \n2014-01-01 02:00:00  1171.0      \n2014-01-01 02:15:00  1330.4      \n2014-01-01 02:30:00  1309.6      \n2014-01-01 02:45:00  1308.4      \n2014-01-01 03:00:00  1494.0    \n', ""data_Monthly = data.resample('1M', how='mean')\n""]";"['dataframe', '2014-01-01 00:15:00  1269.6      \n2014-01-01 00:30:00  1161.6      \n2014-01-01 00:45:00  1466.4      \n2014-01-01 01:00:00  1365.6      \n2014-01-01 01:15:00  1362.6      \n2014-01-01 01:30:00  1064.0      \n2014-01-01 01:45:00  1171.2      \n2014-01-01 02:00:00  1171.0      \n2014-01-01 02:15:00  1330.4      \n2014-01-01 02:30:00  1309.6      \n2014-01-01 02:45:00  1308.4      \n2014-01-01 03:00:00  1494.0    \n', 'resample', ""data_Monthly = data.resample('1M', how='mean')\n""]";"[""data_Monthly = data.resample('1M', how='mean')\n""]";"[""data_Monthly = data.resample('1M', how='mean')\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\ndata_Monthly = data.resample('1M', how='mean')\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
260;260;260;260;1.0;0;15315452;;1;77;<python><pandas>;Selecting with complex criteria from pandas.DataFrame;153283.0;"[""import pandas as pd\nfrom random import randint\n\ndf = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],\n                   'B': [randint(1, 9)*10 for x in xrange(10)],\n                   'C': [randint(1, 9)*100 for x in xrange(10)]})\n""]";"[""import pandas as pd\nfrom random import randint\n\ndf = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],\n                   'B': [randint(1, 9)*10 for x in xrange(10)],\n                   'C': [randint(1, 9)*100 for x in xrange(10)]})\n""]";"[""import pandas as pd\nfrom random import randint\n\ndf = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],\n                   'B': [randint(1, 9)*10 for x in xrange(10)],\n                   'C': [randint(1, 9)*100 for x in xrange(10)]})\n""]";['import pandas as pd\nfrom random import randint\n\n'];['import pandas as pd\nfrom random import randint\n\n'];False;['import pandas as pd\nimport pandas as pd\nfrom random import randint\n\n'];False;0;1;"[""name 'array' is not defined""]";['NameError'];0;1;"[""name 'array' is not defined""]";['NameError'];0;1;"[""name 'array' is not defined""]";['NameError']
261;261;261;261;2.0;0;15322632;;1;37;<python><group-by><pandas>;python pandas, DF.groupby().agg(), column reference in agg();68108.0;"[""     word  tag count\n0    a     S    30\n1    the   S    20\n2    a     T    60\n3    an    T    5\n4    the   T    10 \n     word  tag count\n1    the   S    20\n2    a     T    60\n3    an    T    5\nDF.groupby(['word']).agg(lambda x: x['tag'][ x['count'].argmax() ] )\n""]";"['     word  tag count\n0    a     S    30\n1    the   S    20\n2    a     T    60\n3    an    T    5\n4    the   T    10 \n', '     word  tag count\n1    the   S    20\n2    a     T    60\n3    an    T    5\n', ""DF.groupby(['word']).agg(lambda x: x['tag'][ x['count'].argmax() ] )\n""]";"['     word  tag count\n0    a     S    30\n1    the   S    20\n2    a     T    60\n3    an    T    5\n4    the   T    10 \n', '     word  tag count\n1    the   S    20\n2    a     T    60\n3    an    T    5\n', ""DF.groupby(['word']).agg(lambda x: x['tag'][ x['count'].argmax() ] )\n""]";"[""DF.groupby(['word']).agg(lambda x: x['tag'][ x['count'].argmax() ] )\n""]";"[""DF.groupby(['word']).agg(lambda x: x['tag'][ x['count'].argmax() ] )\n""]";False;"[""import pandas as pd\nDF.groupby(['word']).agg(lambda x: x['tag'][ x['count'].argmax() ] )\n""]";False;0;2;"[""name 'df2' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df2' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'tag'"", ""'word'""]";['KeyError', 'KeyError']
262;262;262;262;4.0;2;15325182;;1;56;<python><regex><pandas>;How to filter rows in pandas by regex;38880.0;"[""In [210]: foo = pd.DataFrame({'a' : [1,2,3,4], 'b' : ['hi', 'foo', 'fat', 'cat']})\nIn [211]: foo\nOut[211]: \n   a    b\n0  1   hi\n1  2  foo\n2  3  fat\n3  4  cat\nIn [213]: foo.b.str.match('f.*')\nOut[213]: \n0    []\n1    ()\n2    ()\n3    []\nIn [226]: foo.b.str.match('(f.*)').str.len() > 0\nOut[226]: \n0    False\n1     True\n2     True\n3    False\nName: b\nIn [229]: foo[foo.b.str.match('(f.*)').str.len() > 0]\nOut[229]: \n   a    b\n1  2  foo\n2  3  fat\n""]";"[""In [210]: foo = pd.DataFrame({'a' : [1,2,3,4], 'b' : ['hi', 'foo', 'fat', 'cat']})\nIn [211]: foo\nOut[211]: \n   a    b\n0  1   hi\n1  2  foo\n2  3  fat\n3  4  cat\n"", ""In [213]: foo.b.str.match('f.*')\nOut[213]: \n0    []\n1    ()\n2    ()\n3    []\n"", ""In [226]: foo.b.str.match('(f.*)').str.len() > 0\nOut[226]: \n0    False\n1     True\n2     True\n3    False\nName: b\n"", ""In [229]: foo[foo.b.str.match('(f.*)').str.len() > 0]\nOut[229]: \n   a    b\n1  2  foo\n2  3  fat\n""]";"[""In [210]: foo = pd.DataFrame({'a' : [1,2,3,4], 'b' : ['hi', 'foo', 'fat', 'cat']})\nIn [211]: foo\nOut[211]: \n   a    b\n0  1   hi\n1  2  foo\n2  3  fat\n3  4  cat\n"", 'f', ""In [213]: foo.b.str.match('f.*')\nOut[213]: \n0    []\n1    ()\n2    ()\n3    []\n"", ""In [226]: foo.b.str.match('(f.*)').str.len() > 0\nOut[226]: \n0    False\n1     True\n2     True\n3    False\nName: b\n"", ""In [229]: foo[foo.b.str.match('(f.*)').str.len() > 0]\nOut[229]: \n   a    b\n1  2  foo\n2  3  fat\n""]";"[""foo = pd.DataFrame({'a' : [1,2,3,4], 'b' : ['hi', 'foo', 'fat', 'cat']})\nfoo.b.str.match('f.*')\nfoo.b.str.match('(f.*)').str.len() > 0\nfoo[foo.b.str.match('(f.*)').str.len() > 0]\n""]";"[""import pandas as pd\nfoo = pd.DataFrame({'a' : [1,2,3,4], 'b' : ['hi', 'foo', 'fat', 'cat']})\nfoo.b.str.match('f.*')\nfoo.b.str.match('(f.*)').str.len() > 0\nfoo[foo.b.str.match('(f.*)').str.len() > 0]\n""]";True;"[""import pandas as pd\nfoo = pd.DataFrame({'a' : [1,2,3,4], 'b' : ['hi', 'foo', 'fat', 'cat']})\nfoo.b.str.match('f.*')\nfoo.b.str.match('(f.*)').str.len() > 0\nfoo[foo.b.str.match('(f.*)').str.len() > 0]\n""]";False;0;2;"[""name 'df' is not defined"", ""name 'frame' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'frame' is not defined""]";['NameError', 'NameError'];0;2;"[""'DataFrame' object has no attribute 'b'"", ""name 'frame' is not defined""]";['AttributeError', 'NameError']
263;263;263;263;4.0;0;15360925;;1;55;<python><dataframe><pandas><series>;How to get the first column of a pandas DataFrame as a Series?;81214.0;['x=pandas.DataFrame(...)\ns = x.take([0], axis=1)\n'];['x=pandas.DataFrame(...)\ns = x.take([0], axis=1)\n'];['x=pandas.DataFrame(...)\ns = x.take([0], axis=1)\n', 's'];['x=pandas.DataFrame(...)\ns = x.take([0], axis=1)\n'];['x=pandas.DataFrame(...)\ns = x.take([0], axis=1)\n'];False;['import pandas as pd\nx=pandas.DataFrame(...)\ns = x.take([0], axis=1)\n'];False;1;3;"[""name 'x' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""name 'x' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""name 'x' is not defined"", 'Sucess', 'single positional indexer is out-of-bounds']";['NameError', 'Sucess', 'IndexError']
264;264;264;264;3.0;0;15374597;;1;16;<python><pandas>;Apply function to pandas groupby;30097.0;"[""func = lambda x: x.size() / x.sum()\ndata = frame.groupby('my_labels').apply(func)\n""]";"[""func = lambda x: x.size() / x.sum()\ndata = frame.groupby('my_labels').apply(func)\n""]";"['my_labels', ""'A', 'B', 'C', 'D', 'E'"", ""func = lambda x: x.size() / x.sum()\ndata = frame.groupby('my_labels').apply(func)\n""]";"[""func = lambda x: x.size() / x.sum()\ndata = frame.groupby('my_labels').apply(func)\n""]";"[""func = lambda x: x.size() / x.sum()\ndata = frame.groupby('my_labels').apply(func)\n""]";False;"[""import pandas as pd\nfunc = lambda x: x.size() / x.sum()\ndata = frame.groupby('my_labels').apply(func)\n""]";False;0;1;"[""name 'get_count' is not defined""]";['NameError'];0;1;"[""name 'get_count' is not defined""]";['NameError'];0;1;"[""name 'get_count' is not defined""]";['NameError']
265;265;265;265;4.0;2;15411158;;1;87;<python><pandas><count><group-by><distinct>;Pandas count(distinct) equivalent;100056.0;"['YEARMONTH, CLIENTCODE, SIZE, .... etc etc\nSELECT count(distinct CLIENTCODE) FROM table GROUP BY YEARMONTH;\n201301    5000\n201302    13245\n']";"['YEARMONTH, CLIENTCODE, SIZE, .... etc etc\n', 'SELECT count(distinct CLIENTCODE) FROM table GROUP BY YEARMONTH;\n', '201301    5000\n201302    13245\n']";"['YEARMONTH, CLIENTCODE, SIZE, .... etc etc\n', 'SELECT count(distinct CLIENTCODE) FROM table GROUP BY YEARMONTH;\n', '201301    5000\n201302    13245\n']";[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'table' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'table' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'table' is not defined"", 'Sucess']";['NameError', 'Sucess']
266;266;266;266;1.0;3;15420672;;1;16;<pandas><ipython><ipython-notebook>;IPython Notebook: What is the default encoding?;9150.0;"[""UnicodeDecodeError                        Traceback (most recent call last)\n<ipython-input-13-92c0011919e7> in <module>()\n      3 ver = verif.VerificacaoNA()\n      4 comp, total = ver.executarCompRealFisica(DT_INI, DT_FIN)\n----> 5 comp\n\nc:\\Python27-32\\lib\\site-packages\\ipython-0.13.1-py2.7.egg\\IPython\\core\\displayhook.pyc in __call__(self, result)\n    240             self.update_user_ns(result)\n    241             self.log_output(format_dict)\n--> 242             self.finish_displayhook()\n    243 \n    244     def flush(self):\n\nc:\\Python27-32\\lib\\site-packages\\ipython-0.13.1-py2.7.egg\\IPython\\zmq\\displayhook.pyc in finish_displayhook(self)\n     59         sys.stdout.flush()\n     60         sys.stderr.flush()\n---> 61         self.session.send(self.pub_socket, self.msg, ident=self.topic)\n     62         self.msg = None\n     63 \n\nc:\\Python27-32\\lib\\site-packages\\ipython-0.13.1-py2.7.egg\\IPython\\zmq\\session.pyc in send(self, stream, msg_or_type, content, parent, ident, buffers, subheader, track, header)\n    557 \n    558         buffers = [] if buffers is None else buffers\n--> 559         to_send = self.serialize(msg, ident)\n    560         flag = 0\n    561         if buffers:\n\nc:\\Python27-32\\lib\\site-packages\\ipython-0.13.1-py2.7.egg\\IPython\\zmq\\session.pyc in serialize(self, msg, ident)\n    461             content = self.none\n    462         elif isinstance(content, dict):\n--> 463             content = self.pack(content)\n    464         elif isinstance(content, bytes):\n    465             # content is already packed, as in a relayed message\n\nc:\\Python27-32\\lib\\site-packages\\ipython-0.13.1-py2.7.egg\\IPython\\zmq\\session.pyc in <lambda>(obj)\n     76 \n     77 # ISO8601-ify datetime objects\n---> 78 json_packer = lambda obj: jsonapi.dumps(obj, default=date_default)\n     79 json_unpacker = lambda s: extract_dates(jsonapi.loads(s))\n     80 \n\nc:\\Python27-32\\lib\\site-packages\\pyzmq-13.0.0-py2.7-win32.egg\\zmq\\utils\\jsonapi.pyc in dumps(o, **kwargs)\n     70         kwargs['separators'] = (',', ':')\n     71 \n---> 72     return _squash_unicode(jsonmod.dumps(o, **kwargs))\n     73 \n     74 def loads(s, **kwargs):\n\nc:\\Python27-32\\lib\\json\\__init__.pyc in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, encoding, default, **kw)\n    236         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n    237         separators=separators, encoding=encoding, default=default,\n--> 238         **kw).encode(obj)\n    239 \n    240 \n\nc:\\Python27-32\\lib\\json\\encoder.pyc in encode(self, o)\n    199         # exceptions aren't as detailed.  The list call should be roughly\n    200         # equivalent to the PySequence_Fast that ''.join() would do.\n--> 201         chunks = self.iterencode(o, _one_shot=True)\n    202         if not isinstance(chunks, (list, tuple)):\n    203             chunks = list(chunks)\n\nc:\\Python27-32\\lib\\json\\encoder.pyc in iterencode(self, o, _one_shot)\n    262                 self.key_separator, self.item_separator, self.sort_keys,\n    263                 self.skipkeys, _one_shot)\n--> 264         return _iterencode(o, 0)\n    265 \n    266 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n\nUnicodeDecodeError: 'utf8' codec can't decode byte 0xe7 in position 199: invalid continuation byte\n""]";"[""UnicodeDecodeError                        Traceback (most recent call last)\n<ipython-input-13-92c0011919e7> in <module>()\n      3 ver = verif.VerificacaoNA()\n      4 comp, total = ver.executarCompRealFisica(DT_INI, DT_FIN)\n----> 5 comp\n\nc:\\Python27-32\\lib\\site-packages\\ipython-0.13.1-py2.7.egg\\IPython\\core\\displayhook.pyc in __call__(self, result)\n    240             self.update_user_ns(result)\n    241             self.log_output(format_dict)\n--> 242             self.finish_displayhook()\n    243 \n    244     def flush(self):\n\nc:\\Python27-32\\lib\\site-packages\\ipython-0.13.1-py2.7.egg\\IPython\\zmq\\displayhook.pyc in finish_displayhook(self)\n     59         sys.stdout.flush()\n     60         sys.stderr.flush()\n---> 61         self.session.send(self.pub_socket, self.msg, ident=self.topic)\n     62         self.msg = None\n     63 \n\nc:\\Python27-32\\lib\\site-packages\\ipython-0.13.1-py2.7.egg\\IPython\\zmq\\session.pyc in send(self, stream, msg_or_type, content, parent, ident, buffers, subheader, track, header)\n    557 \n    558         buffers = [] if buffers is None else buffers\n--> 559         to_send = self.serialize(msg, ident)\n    560         flag = 0\n    561         if buffers:\n\nc:\\Python27-32\\lib\\site-packages\\ipython-0.13.1-py2.7.egg\\IPython\\zmq\\session.pyc in serialize(self, msg, ident)\n    461             content = self.none\n    462         elif isinstance(content, dict):\n--> 463             content = self.pack(content)\n    464         elif isinstance(content, bytes):\n    465             # content is already packed, as in a relayed message\n\nc:\\Python27-32\\lib\\site-packages\\ipython-0.13.1-py2.7.egg\\IPython\\zmq\\session.pyc in <lambda>(obj)\n     76 \n     77 # ISO8601-ify datetime objects\n---> 78 json_packer = lambda obj: jsonapi.dumps(obj, default=date_default)\n     79 json_unpacker = lambda s: extract_dates(jsonapi.loads(s))\n     80 \n\nc:\\Python27-32\\lib\\site-packages\\pyzmq-13.0.0-py2.7-win32.egg\\zmq\\utils\\jsonapi.pyc in dumps(o, **kwargs)\n     70         kwargs['separators'] = (',', ':')\n     71 \n---> 72     return _squash_unicode(jsonmod.dumps(o, **kwargs))\n     73 \n     74 def loads(s, **kwargs):\n\nc:\\Python27-32\\lib\\json\\__init__.pyc in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, encoding, default, **kw)\n    236         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n    237         separators=separators, encoding=encoding, default=default,\n--> 238         **kw).encode(obj)\n    239 \n    240 \n\nc:\\Python27-32\\lib\\json\\encoder.pyc in encode(self, o)\n    199         # exceptions aren't as detailed.  The list call should be roughly\n    200         # equivalent to the PySequence_Fast that ''.join() would do.\n--> 201         chunks = self.iterencode(o, _one_shot=True)\n    202         if not isinstance(chunks, (list, tuple)):\n    203             chunks = list(chunks)\n\nc:\\Python27-32\\lib\\json\\encoder.pyc in iterencode(self, o, _one_shot)\n    262                 self.key_separator, self.item_separator, self.sort_keys,\n    263                 self.skipkeys, _one_shot)\n--> 264         return _iterencode(o, 0)\n    265 \n    266 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n\nUnicodeDecodeError: 'utf8' codec can't decode byte 0xe7 in position 199: invalid continuation byte\n""]";"['DataFrame', ""'utf8' codec can't decode byte 0xe7"", ""UnicodeDecodeError                        Traceback (most recent call last)\n<ipython-input-13-92c0011919e7> in <module>()\n      3 ver = verif.VerificacaoNA()\n      4 comp, total = ver.executarCompRealFisica(DT_INI, DT_FIN)\n----> 5 comp\n\nc:\\Python27-32\\lib\\site-packages\\ipython-0.13.1-py2.7.egg\\IPython\\core\\displayhook.pyc in __call__(self, result)\n    240             self.update_user_ns(result)\n    241             self.log_output(format_dict)\n--> 242             self.finish_displayhook()\n    243 \n    244     def flush(self):\n\nc:\\Python27-32\\lib\\site-packages\\ipython-0.13.1-py2.7.egg\\IPython\\zmq\\displayhook.pyc in finish_displayhook(self)\n     59         sys.stdout.flush()\n     60         sys.stderr.flush()\n---> 61         self.session.send(self.pub_socket, self.msg, ident=self.topic)\n     62         self.msg = None\n     63 \n\nc:\\Python27-32\\lib\\site-packages\\ipython-0.13.1-py2.7.egg\\IPython\\zmq\\session.pyc in send(self, stream, msg_or_type, content, parent, ident, buffers, subheader, track, header)\n    557 \n    558         buffers = [] if buffers is None else buffers\n--> 559         to_send = self.serialize(msg, ident)\n    560         flag = 0\n    561         if buffers:\n\nc:\\Python27-32\\lib\\site-packages\\ipython-0.13.1-py2.7.egg\\IPython\\zmq\\session.pyc in serialize(self, msg, ident)\n    461             content = self.none\n    462         elif isinstance(content, dict):\n--> 463             content = self.pack(content)\n    464         elif isinstance(content, bytes):\n    465             # content is already packed, as in a relayed message\n\nc:\\Python27-32\\lib\\site-packages\\ipython-0.13.1-py2.7.egg\\IPython\\zmq\\session.pyc in <lambda>(obj)\n     76 \n     77 # ISO8601-ify datetime objects\n---> 78 json_packer = lambda obj: jsonapi.dumps(obj, default=date_default)\n     79 json_unpacker = lambda s: extract_dates(jsonapi.loads(s))\n     80 \n\nc:\\Python27-32\\lib\\site-packages\\pyzmq-13.0.0-py2.7-win32.egg\\zmq\\utils\\jsonapi.pyc in dumps(o, **kwargs)\n     70         kwargs['separators'] = (',', ':')\n     71 \n---> 72     return _squash_unicode(jsonmod.dumps(o, **kwargs))\n     73 \n     74 def loads(s, **kwargs):\n\nc:\\Python27-32\\lib\\json\\__init__.pyc in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, encoding, default, **kw)\n    236         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n    237         separators=separators, encoding=encoding, default=default,\n--> 238         **kw).encode(obj)\n    239 \n    240 \n\nc:\\Python27-32\\lib\\json\\encoder.pyc in encode(self, o)\n    199         # exceptions aren't as detailed.  The list call should be roughly\n    200         # equivalent to the PySequence_Fast that ''.join() would do.\n--> 201         chunks = self.iterencode(o, _one_shot=True)\n    202         if not isinstance(chunks, (list, tuple)):\n    203             chunks = list(chunks)\n\nc:\\Python27-32\\lib\\json\\encoder.pyc in iterencode(self, o, _one_shot)\n    262                 self.key_separator, self.item_separator, self.sort_keys,\n    263                 self.skipkeys, _one_shot)\n--> 264         return _iterencode(o, 0)\n    265 \n    266 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n\nUnicodeDecodeError: 'utf8' codec can't decode byte 0xe7 in position 199: invalid continuation byte\n""]";['\n\n\n\n\n\n\n\n\n\n'];['\n\n\n\n\n\n\n\n\n\n'];False;['import pandas as pd\n\n\n\n\n\n\n\n\n\n\n'];False;0;1;"[""name 'reload' is not defined""]";['NameError'];0;1;"[""name 'reload' is not defined""]";['NameError'];0;1;"[""name 'reload' is not defined""]";['NameError']
267;267;267;267;2.0;0;15432659;;1;13;<python><row><pandas><sequence><dataframe>;How to rearrange a python pandas dataframe?;5962.0;['> Date           h1 h2  h3  h4 ... h24\n> 14.03.2013    60  50  52  49 ... 73\n>Date/Time            Value\n>14.03.2013 00:00:00  60\n>14.03.2013 01:00:00  50\n>14.03.2013 02:00:00  52\n>14.03.2013 03:00:00  49\n>.\n>.\n>.\n>14.03.2013 23:00:00  73\n'];['> Date           h1 h2  h3  h4 ... h24\n> 14.03.2013    60  50  52  49 ... 73\n', '>Date/Time            Value\n>14.03.2013 00:00:00  60\n>14.03.2013 01:00:00  50\n>14.03.2013 02:00:00  52\n>14.03.2013 03:00:00  49\n>.\n>.\n>.\n>14.03.2013 23:00:00  73\n'];['> Date           h1 h2  h3  h4 ... h24\n> 14.03.2013    60  50  52  49 ... 73\n', '>Date/Time            Value\n>14.03.2013 00:00:00  60\n>14.03.2013 01:00:00  50\n>14.03.2013 02:00:00  52\n>14.03.2013 03:00:00  49\n>.\n>.\n>.\n>14.03.2013 23:00:00  73\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""File b'hourmelt.csv' does not exist""]";['FileNotFoundError'];0;1;"[""File b'hourmelt.csv' does not exist""]";['FileNotFoundError'];0;1;"[""File b'hourmelt.csv' does not exist""]";['FileNotFoundError']
268;268;268;268;1.0;0;15455388;;1;12;<python><json><dictionary><pandas>;Dict of dicts of dicts to DataFrame;4558.0;"['d = {\n  ""col1"": {\n    ""row1"": {\n      ""data1"": ""0.87"", \n      ""data2"": ""Title col1"", \n      ""data3"": ""14.4878"", \n      ""data4"": ""Title row1""\n    }, \n    ""row2"": {\n      ""data1"": ""15352.3"", \n      ""data2"": ""Title col1"", \n      ""data3"": ""14.9561"", \n      ""data4"": ""Title row2""\n    }, \n    ""row3"": {\n      ""data1"": ""0"", \n      ""data2"": ""Title col1"", \n      ""data3"": ""16.8293"", \n      ""data4"": ""Title row3""\n    }\n  }, \n  ""col2"": {\n    ""row1"": {\n      ""data1"": ""0.87"", \n      ""data2"": ""Title col2"", \n      ""data3"": ""24.4878"", \n      ""data4"": ""Title row1""\n    }, \n    ""row2"": {\n      ""data1"": ""15352.3"", \n      ""data2"": ""Title col2"", \n      ""data3"": ""24.9561"", \n      ""data4"": ""Title row2""\n    }, \n    ""row3"": {\n      ""data1"": ""0"", \n      ""data2"": ""Title col2"", \n      ""data3"": ""26.8293"", \n      ""data4"": ""Title row3""\n    }\n  }\n}\nimport pandas as pd\ndf=pd.DataFrame(d)\nIn [1]: df\nOut[1]: \n                                                   col1                                               col2\nrow1  {\'data4\': \'Title col1\', \'data1\': \'0.87\', \'data3\':  {\'data4\': \'Title col1\', \'data1\': \'0.87\', \'data3\':\nrow2  {\'data4\': \'Title col2\', \'data1\': \'15352.3\', \'data  {\'data4\': \'Title col2\', \'data1\': \'15352.3\', \'data\nrow3  {\'data4\': \'Title col3\', \'data1\': \'0\', \'data3\': \'1  {\'data4\': \'Title col3\', \'data1\': \'0\', \'data3\': \'2\n']";"['d = {\n  ""col1"": {\n    ""row1"": {\n      ""data1"": ""0.87"", \n      ""data2"": ""Title col1"", \n      ""data3"": ""14.4878"", \n      ""data4"": ""Title row1""\n    }, \n    ""row2"": {\n      ""data1"": ""15352.3"", \n      ""data2"": ""Title col1"", \n      ""data3"": ""14.9561"", \n      ""data4"": ""Title row2""\n    }, \n    ""row3"": {\n      ""data1"": ""0"", \n      ""data2"": ""Title col1"", \n      ""data3"": ""16.8293"", \n      ""data4"": ""Title row3""\n    }\n  }, \n  ""col2"": {\n    ""row1"": {\n      ""data1"": ""0.87"", \n      ""data2"": ""Title col2"", \n      ""data3"": ""24.4878"", \n      ""data4"": ""Title row1""\n    }, \n    ""row2"": {\n      ""data1"": ""15352.3"", \n      ""data2"": ""Title col2"", \n      ""data3"": ""24.9561"", \n      ""data4"": ""Title row2""\n    }, \n    ""row3"": {\n      ""data1"": ""0"", \n      ""data2"": ""Title col2"", \n      ""data3"": ""26.8293"", \n      ""data4"": ""Title row3""\n    }\n  }\n}\n', 'import pandas as pd\ndf=pd.DataFrame(d)\n', ""In [1]: df\nOut[1]: \n                                                   col1                                               col2\nrow1  {'data4': 'Title col1', 'data1': '0.87', 'data3':  {'data4': 'Title col1', 'data1': '0.87', 'data3':\nrow2  {'data4': 'Title col2', 'data1': '15352.3', 'data  {'data4': 'Title col2', 'data1': '15352.3', 'data\nrow3  {'data4': 'Title col3', 'data1': '0', 'data3': '1  {'data4': 'Title col3', 'data1': '0', 'data3': '2\n""]";"['d = {\n  ""col1"": {\n    ""row1"": {\n      ""data1"": ""0.87"", \n      ""data2"": ""Title col1"", \n      ""data3"": ""14.4878"", \n      ""data4"": ""Title row1""\n    }, \n    ""row2"": {\n      ""data1"": ""15352.3"", \n      ""data2"": ""Title col1"", \n      ""data3"": ""14.9561"", \n      ""data4"": ""Title row2""\n    }, \n    ""row3"": {\n      ""data1"": ""0"", \n      ""data2"": ""Title col1"", \n      ""data3"": ""16.8293"", \n      ""data4"": ""Title row3""\n    }\n  }, \n  ""col2"": {\n    ""row1"": {\n      ""data1"": ""0.87"", \n      ""data2"": ""Title col2"", \n      ""data3"": ""24.4878"", \n      ""data4"": ""Title row1""\n    }, \n    ""row2"": {\n      ""data1"": ""15352.3"", \n      ""data2"": ""Title col2"", \n      ""data3"": ""24.9561"", \n      ""data4"": ""Title row2""\n    }, \n    ""row3"": {\n      ""data1"": ""0"", \n      ""data2"": ""Title col2"", \n      ""data3"": ""26.8293"", \n      ""data4"": ""Title row3""\n    }\n  }\n}\n', 'import pandas as pd\ndf=pd.DataFrame(d)\n', ""In [1]: df\nOut[1]: \n                                                   col1                                               col2\nrow1  {'data4': 'Title col1', 'data1': '0.87', 'data3':  {'data4': 'Title col1', 'data1': '0.87', 'data3':\nrow2  {'data4': 'Title col2', 'data1': '15352.3', 'data  {'data4': 'Title col2', 'data1': '15352.3', 'data\nrow3  {'data4': 'Title col3', 'data1': '0', 'data3': '1  {'data4': 'Title col3', 'data1': '0', 'data3': '2\n""]";['df\n'];['df\n'];False;['import pandas as pd\ndf = pd.DataFrame()\ndf\n'];True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'd' is not defined""]";['NameError'];0;1;"[""name 'd' is not defined""]";['NameError']
269;269;269;269;4.0;0;15463729;;1;14;<pandas>;Select a multiple-key cross section from a DataFrame;2361.0;"[""\n\n                          tod    last     bid      ask      volume\n    time        ticker                  \n    2013-02-01  SPY       1600   149.70   150.14   150.17   1300\n                SLV       1600   30.44    30.38    30.43    3892\n                GLD       1600   161.20   161.19   161.21   3860\n\n\n\n    df.xs('SPY', level=1)\n\n\n\n    df.xs(['SPY', 'GLD'], level=1)\n\n""]";"['\n\n                          tod    last     bid      ask      volume\n    time        ticker                  \n    2013-02-01  SPY       1600   149.70   150.14   150.17   1300\n                SLV       1600   30.44    30.38    30.43    3892\n                GLD       1600   161.20   161.19   161.21   3860\n\n', ""\n\n    df.xs('SPY', level=1)\n\n"", ""\n\n    df.xs(['SPY', 'GLD'], level=1)\n\n""]";[];['\n\n\n\n\n\n\n\n\n'];['\n\n\n\n\n\n\n\n\n'];False;['import pandas as pd\n\n\n\n\n\n\n\n\n\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
270;270;270;270;1.0;1;15465645;;1;35;<python><matplotlib><group-by><pandas><data-analysis>;Plotting results of Pandas GroupBy;31249.0;[''];[];['A=True', 'time=t', 't-5', 't', 'A=True'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
271;271;271;271;1.0;0;15557542;;1;20;<pandas><dataframe>;Reindexing dataframes;22266.0;['import pandas as pd\nimport numpy as np\n\njjarray = np.array(range(5))\neq2 = jjarray == 2\nneq2 = np.logical_not(eq2)\n\njjdf = pd.DataFrame(jjarray)\njjdfno2 = jjdf[neq2]\n\njjdfno2\n  0\n0 0\n1 1\n3 3\n4 4\n  0\n0 0\n1 1\n2 3\n3 4\n'];['import pandas as pd\nimport numpy as np\n\njjarray = np.array(range(5))\neq2 = jjarray == 2\nneq2 = np.logical_not(eq2)\n\njjdf = pd.DataFrame(jjarray)\njjdfno2 = jjdf[neq2]\n\njjdfno2\n', '  0\n0 0\n1 1\n3 3\n4 4\n', '  0\n0 0\n1 1\n2 3\n3 4\n'];['import pandas as pd\nimport numpy as np\n\njjarray = np.array(range(5))\neq2 = jjarray == 2\nneq2 = np.logical_not(eq2)\n\njjdf = pd.DataFrame(jjarray)\njjdfno2 = jjdf[neq2]\n\njjdfno2\n', '  0\n0 0\n1 1\n3 3\n4 4\n', '  0\n0 0\n1 1\n2 3\n3 4\n'];['import pandas as pd\nimport numpy as np\n\njjarray = np.array(range(5))\neq2 = jjarray == 2\nneq2 = np.logical_not(eq2)\n\njjdf = pd.DataFrame(jjarray)\njjdfno2 = jjdf[neq2]\n\njjdfno2\n'];['import pandas as pd\nimport numpy as np\n\njjarray = np.array(range(5))\neq2 = jjarray == 2\nneq2 = np.logical_not(eq2)\n\njjdf = pd.DataFrame(jjarray)\njjdfno2 = jjdf[neq2]\n\njjdfno2\n'];False;['import pandas as pd\nimport pandas as pd\nimport numpy as np\n\njjarray = np.array(range(5))\neq2 = jjarray == 2\nneq2 = np.logical_not(eq2)\n\njjdf = pd.DataFrame(jjarray)\njjdfno2 = jjdf[neq2]\n\njjdfno2\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
272;272;272;272;4.0;0;15570099;;1;26;<python><pandas><pivot-table>;Pandas Pivot tables row subtotals;17499.0;"[""Date       State   City    SalesToday  SalesMTD  SalesYTD\n20130320     stA    ctA            20       400      1000\n20130320     stA    ctB            30       500      1100\n20130320     stB    ctC            10       500       900\n20130320     stB    ctD            40       200      1300\n20130320     stC    ctF            30       300       800\nState   City  SalesToday  SalesMTD  SalesYTD\n  stA    ALL          50       900      2100\n  stA    ctA          20       400      1000\n  stA    ctB          30       500      1100\ntable = pivot_table(df, values=['SalesToday', 'SalesMTD','SalesYTD'],\\\n                     rows=['State','City'], aggfunc=np.sum, margins=True)\n""]";"['Date       State   City    SalesToday  SalesMTD  SalesYTD\n20130320     stA    ctA            20       400      1000\n20130320     stA    ctB            30       500      1100\n20130320     stB    ctC            10       500       900\n20130320     stB    ctD            40       200      1300\n20130320     stC    ctF            30       300       800\n', 'State   City  SalesToday  SalesMTD  SalesYTD\n  stA    ALL          50       900      2100\n  stA    ctA          20       400      1000\n  stA    ctB          30       500      1100\n', ""table = pivot_table(df, values=['SalesToday', 'SalesMTD','SalesYTD'],\\\n                     rows=['State','City'], aggfunc=np.sum, margins=True)\n""]";"['Date       State   City    SalesToday  SalesMTD  SalesYTD\n20130320     stA    ctA            20       400      1000\n20130320     stA    ctB            30       500      1100\n20130320     stB    ctC            10       500       900\n20130320     stB    ctD            40       200      1300\n20130320     stC    ctF            30       300       800\n', 'State   City  SalesToday  SalesMTD  SalesYTD\n  stA    ALL          50       900      2100\n  stA    ctA          20       400      1000\n  stA    ctB          30       500      1100\n', ""table = pivot_table(df, values=['SalesToday', 'SalesMTD','SalesYTD'],\\\n                     rows=['State','City'], aggfunc=np.sum, margins=True)\n""]";[''];[''];False;['import pandas as pd\n'];False;1;2;"[""No module named 'StringIO'"", 'Sucess']";['ImportError', 'Sucess'];1;2;"[""No module named 'StringIO'"", 'Sucess']";['ImportError', 'Sucess'];1;2;"[""No module named 'StringIO'"", 'Sucess']";['ImportError', 'Sucess']
273;273;273;273;4.0;3;15589354;;1;11;<python><group-by><pandas>;Frequency tables in pandas (like plyr in R);10305.0;"['d1 = pd.DataFrame( {\'StudentID\': [""x1"", ""x10"", ""x2"",""x3"", ""x4"", ""x5"", ""x6"",   ""x7"",     ""x8"", ""x9""],\n                       \'StudentGender\' : [\'F\', \'M\', \'F\', \'M\', \'F\', \'M\', \'F\', \'M\', \'M\', \'M\'],\n                 \'ExamenYear\': [\'2007\',\'2007\',\'2007\',\'2008\',\'2008\',\'2008\',\'2008\',\'2009\',\'2009\',\'2009\'],\n                 \'Exam\': [\'algebra\', \'stats\', \'bio\', \'algebra\', \'algebra\', \'stats\', \'stats\', \'algebra\', \'bio\', \'bio\'],\n                 \'Participated\': [\'no\',\'yes\',\'yes\',\'yes\',\'no\',\'yes\',\'yes\',\'yes\',\'yes\',\'yes\'],\n                  \'Passed\': [\'no\',\'yes\',\'yes\',\'yes\',\'no\',\'yes\',\'yes\',\'yes\',\'no\',\'yes\']},\n                  columns = [\'StudentID\', \'StudentGender\', \'ExamenYear\', \'Exam\', \'Participated\', \'Passed\'])\n             Participated  OfWhichpassed\n ExamenYear                             \n2007                   3              2\n2008                   4              3\n2009                   3              2\nt1 = d1.pivot_table(values = \'StudentID\', rows=[\'ExamenYear\'], cols = [\'Participated\'], aggfunc = len)\nt2 = d1.pivot_table(values = \'StudentID\', rows=[\'ExamenYear\'], cols = [\'Passed\'], aggfunc = len)\ntx = pd.concat([t1, t2] , axis = 1)\n\nRes1 = tx[\'yes\']\nimport collections\ndg = d1.groupby(\'ExamenYear\')\nRes2 = dg.agg({\'Participated\': len,\'Passed\': lambda x : collections.Counter(x == \'yes\')[True]})\n\n Res2.columns = [\'Participated\', \'OfWhichpassed\']\n t1 = d1.pivot_table(values = \'StudentID\', rows=[\'ExamenYear\'], aggfunc = len)\n t2 = d1.pivot_table(values = \'StudentID\', rows=[\'ExamenYear\'], cols = [\'Participated\'], aggfunc = len)\n t3 = d1.pivot_table(values = \'StudentID\', rows=[\'ExamenYear\'], cols = [\'Passed\'], aggfunc = len)\n\n Res1 = pd.DataFrame( {\'All\': t1,\n                       \'OfWhichParticipated\': t2[\'yes\'],\n                     \'OfWhichPassed\': t3[\'yes\']})\n             All  OfWhichParticipated  OfWhichPassed\nExamenYear                                         \n2007          3                    2              2\n2008          4                    3              3\n2009          3                    3              2\nRes2 = d1.groupby(\'ExamenYear\').agg({\'StudentID\': len,\n                                 \'Participated\': lambda x: x.value_counts()[\'yes\'],\n                                 \'Passed\': lambda x: x.value_counts()[\'yes\']})\n\nRes2.columns = [\'All\', \'OfWgichParticipated\', \'OfWhichPassed\']\n']";"['d1 = pd.DataFrame( {\'StudentID\': [""x1"", ""x10"", ""x2"",""x3"", ""x4"", ""x5"", ""x6"",   ""x7"",     ""x8"", ""x9""],\n                       \'StudentGender\' : [\'F\', \'M\', \'F\', \'M\', \'F\', \'M\', \'F\', \'M\', \'M\', \'M\'],\n                 \'ExamenYear\': [\'2007\',\'2007\',\'2007\',\'2008\',\'2008\',\'2008\',\'2008\',\'2009\',\'2009\',\'2009\'],\n                 \'Exam\': [\'algebra\', \'stats\', \'bio\', \'algebra\', \'algebra\', \'stats\', \'stats\', \'algebra\', \'bio\', \'bio\'],\n                 \'Participated\': [\'no\',\'yes\',\'yes\',\'yes\',\'no\',\'yes\',\'yes\',\'yes\',\'yes\',\'yes\'],\n                  \'Passed\': [\'no\',\'yes\',\'yes\',\'yes\',\'no\',\'yes\',\'yes\',\'yes\',\'no\',\'yes\']},\n                  columns = [\'StudentID\', \'StudentGender\', \'ExamenYear\', \'Exam\', \'Participated\', \'Passed\'])\n', '             Participated  OfWhichpassed\n ExamenYear                             \n2007                   3              2\n2008                   4              3\n2009                   3              2\n', ""t1 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Participated'], aggfunc = len)\nt2 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Passed'], aggfunc = len)\ntx = pd.concat([t1, t2] , axis = 1)\n\nRes1 = tx['yes']\n"", ""import collections\ndg = d1.groupby('ExamenYear')\nRes2 = dg.agg({'Participated': len,'Passed': lambda x : collections.Counter(x == 'yes')[True]})\n\n Res2.columns = ['Participated', 'OfWhichpassed']\n"", "" t1 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], aggfunc = len)\n t2 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Participated'], aggfunc = len)\n t3 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Passed'], aggfunc = len)\n\n Res1 = pd.DataFrame( {'All': t1,\n                       'OfWhichParticipated': t2['yes'],\n                     'OfWhichPassed': t3['yes']})\n"", '             All  OfWhichParticipated  OfWhichPassed\nExamenYear                                         \n2007          3                    2              2\n2008          4                    3              3\n2009          3                    3              2\n', ""Res2 = d1.groupby('ExamenYear').agg({'StudentID': len,\n                                 'Participated': lambda x: x.value_counts()['yes'],\n                                 'Passed': lambda x: x.value_counts()['yes']})\n\nRes2.columns = ['All', 'OfWgichParticipated', 'OfWhichPassed']\n""]";"['d1 = pd.DataFrame( {\'StudentID\': [""x1"", ""x10"", ""x2"",""x3"", ""x4"", ""x5"", ""x6"",   ""x7"",     ""x8"", ""x9""],\n                       \'StudentGender\' : [\'F\', \'M\', \'F\', \'M\', \'F\', \'M\', \'F\', \'M\', \'M\', \'M\'],\n                 \'ExamenYear\': [\'2007\',\'2007\',\'2007\',\'2008\',\'2008\',\'2008\',\'2008\',\'2009\',\'2009\',\'2009\'],\n                 \'Exam\': [\'algebra\', \'stats\', \'bio\', \'algebra\', \'algebra\', \'stats\', \'stats\', \'algebra\', \'bio\', \'bio\'],\n                 \'Participated\': [\'no\',\'yes\',\'yes\',\'yes\',\'no\',\'yes\',\'yes\',\'yes\',\'yes\',\'yes\'],\n                  \'Passed\': [\'no\',\'yes\',\'yes\',\'yes\',\'no\',\'yes\',\'yes\',\'yes\',\'no\',\'yes\']},\n                  columns = [\'StudentID\', \'StudentGender\', \'ExamenYear\', \'Exam\', \'Participated\', \'Passed\'])\n', '             Participated  OfWhichpassed\n ExamenYear                             \n2007                   3              2\n2008                   4              3\n2009                   3              2\n', ""t1 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Participated'], aggfunc = len)\nt2 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Passed'], aggfunc = len)\ntx = pd.concat([t1, t2] , axis = 1)\n\nRes1 = tx['yes']\n"", ""import collections\ndg = d1.groupby('ExamenYear')\nRes2 = dg.agg({'Participated': len,'Passed': lambda x : collections.Counter(x == 'yes')[True]})\n\n Res2.columns = ['Participated', 'OfWhichpassed']\n"", "" t1 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], aggfunc = len)\n t2 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Participated'], aggfunc = len)\n t3 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Passed'], aggfunc = len)\n\n Res1 = pd.DataFrame( {'All': t1,\n                       'OfWhichParticipated': t2['yes'],\n                     'OfWhichPassed': t3['yes']})\n"", '             All  OfWhichParticipated  OfWhichPassed\nExamenYear                                         \n2007          3                    2              2\n2008          4                    3              3\n2009          3                    3              2\n', ""Res2 = d1.groupby('ExamenYear').agg({'StudentID': len,\n                                 'Participated': lambda x: x.value_counts()['yes'],\n                                 'Passed': lambda x: x.value_counts()['yes']})\n\nRes2.columns = ['All', 'OfWgichParticipated', 'OfWhichPassed']\n""]";"[""t1 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Participated'], aggfunc = len)\nt2 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Passed'], aggfunc = len)\ntx = pd.concat([t1, t2] , axis = 1)\n\nRes1 = tx['yes']\nimport collections\ndg = d1.groupby('ExamenYear')\nRes2 = dg.agg({'Participated': len,'Passed': lambda x : collections.Counter(x == 'yes')[True]})\n\n\nExamenYear                                         \n\nRes2.columns = ['All', 'OfWgichParticipated', 'OfWhichPassed']\n""]";"[""import pandas as pd\nt1 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Participated'], aggfunc = len)\nt2 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Passed'], aggfunc = len)\ntx = pd.concat([t1, t2] , axis = 1)\n\nRes1 = tx['yes']\nimport collections\ndg = d1.groupby('ExamenYear')\nRes2 = dg.agg({'Participated': len,'Passed': lambda x : collections.Counter(x == 'yes')[True]})\n\n\nExamenYear                                         \n\nRes2.columns = ['All', 'OfWgichParticipated', 'OfWhichPassed']\n""]";True;"[""import pandas as pd\nt1 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Participated'], aggfunc = len)\nt2 = d1.pivot_table(values = 'StudentID', rows=['ExamenYear'], cols = ['Passed'], aggfunc = len)\ntx = pd.concat([t1, t2] , axis = 1)\n\nRes1 = tx['yes']\nimport collections\ndg = d1.groupby('ExamenYear')\nRes2 = dg.agg({'Participated': len,'Passed': lambda x : collections.Counter(x == 'yes')[True]})\n\n\nExamenYear                                         \n\nRes2.columns = ['All', 'OfWgichParticipated', 'OfWhichPassed']\n""]";False;1;2;"['Sucess', ""name 'ExamenYear' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'ExamenYear' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'ExamenYear' is not defined""]";['Sucess', 'NameError']
274;274;274;274;3.0;1;15653688;;1;15;<python><pandas>;Preserving column order in Python Pandas DataFrame;18663.0;['import pandas as pd\n\ndata = pd.read_csv(filename)\ndata.to_csv(filename)\n'];['import pandas as pd\n\ndata = pd.read_csv(filename)\ndata.to_csv(filename)\n'];['import pandas as pd\n\ndata = pd.read_csv(filename)\ndata.to_csv(filename)\n'];['import pandas as pd\n\ndata = pd.read_csv(filename)\ndata.to_csv(filename)\n'];['import pandas as pd\n\ndata = pd.read_csv(filename)\ndata.to_csv(filename)\n'];False;['import pandas as pd\nimport pandas as pd\n\ndata = pd.read_csv(filename)\ndata.to_csv(filename)\n'];False;0;1;"[""to_csv() got an unexpected keyword argument 'cols'""]";['TypeError'];0;1;"[""to_csv() got an unexpected keyword argument 'cols'""]";['TypeError'];0;1;"[""to_csv() got an unexpected keyword argument 'cols'""]";['TypeError']
275;275;275;275;5.0;5;15705630;;1;40;<python><pandas>;Python : Getting the Row which has the max value in groups using groupby;46906.0;[' Sp  Mt Value  count\n0  MM1  S1   a      **3**\n1  MM1  S1   n      2\n2  MM1  S3   cb     5\n3  MM2  S3   mk      **8**\n4  MM2  S4   bg     **10**\n5  MM2  S4   dgd      1\n6  MM4  S2  rd     2\n7  MM4  S2   cb      2\n8  MM4  S2   uyi      **7**\n0  MM1  S1   a      **3**\n1 3  MM2  S3   mk      **8**\n4  MM2  S4   bg     **10** \n8  MM4  S2   uyi      **7**\n   Sp   Mt   Value  count\n4  MM2  S4   bg     10\n5  MM2  S4   dgd    1\n6  MM4  S2   rd     2\n7  MM4  S2   cb     8\n8  MM4  S2   uyi    8\nMM2  S4   bg     10\nMM4  S2   cb     8\nMM4  S2   uyi    8\n'];[' Sp  Mt Value  count\n0  MM1  S1   a      **3**\n1  MM1  S1   n      2\n2  MM1  S3   cb     5\n3  MM2  S3   mk      **8**\n4  MM2  S4   bg     **10**\n5  MM2  S4   dgd      1\n6  MM4  S2  rd     2\n7  MM4  S2   cb      2\n8  MM4  S2   uyi      **7**\n', '0  MM1  S1   a      **3**\n1 3  MM2  S3   mk      **8**\n4  MM2  S4   bg     **10** \n8  MM4  S2   uyi      **7**\n', '   Sp   Mt   Value  count\n4  MM2  S4   bg     10\n5  MM2  S4   dgd    1\n6  MM4  S2   rd     2\n7  MM4  S2   cb     8\n8  MM4  S2   uyi    8\n', 'MM2  S4   bg     10\nMM4  S2   cb     8\nMM4  S2   uyi    8\n'];[' Sp  Mt Value  count\n0  MM1  S1   a      **3**\n1  MM1  S1   n      2\n2  MM1  S3   cb     5\n3  MM2  S3   mk      **8**\n4  MM2  S4   bg     **10**\n5  MM2  S4   dgd      1\n6  MM4  S2  rd     2\n7  MM4  S2   cb      2\n8  MM4  S2   uyi      **7**\n', '0  MM1  S1   a      **3**\n1 3  MM2  S3   mk      **8**\n4  MM2  S4   bg     **10** \n8  MM4  S2   uyi      **7**\n', '   Sp   Mt   Value  count\n4  MM2  S4   bg     10\n5  MM2  S4   dgd    1\n6  MM4  S2   rd     2\n7  MM4  S2   cb     8\n8  MM4  S2   uyi    8\n', 'MM2  S4   bg     10\nMM4  S2   cb     8\nMM4  S2   uyi    8\n'];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""'Mt'"", ""name 'df' is not defined"", ""'count'""]";['KeyError', 'NameError', 'KeyError']
276;276;276;276;3.0;0;15723628;;1;30;<python><pandas>;Pandas - make a column dtype object or Factor;21102.0;[''];[];['as.factor()', 'pandas.Factor', 'pandas.Categorical'];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'df' is not defined"", ""name 'pd' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", 'Categorical.from_array is deprecated, use Categorical instead', ""name 'df' is not defined""]";['NameError', 'FutureWarning', 'NameError'];0;3;"[""name 'df' is not defined"", 'Categorical.from_array is deprecated, use Categorical instead', ""'DataFrame' object has no attribute 'b'""]";['NameError', 'FutureWarning', 'AttributeError']
277;277;277;277;3.0;0;15741759;;1;26;<python><pandas>;Find maximum value of a column and return the corresponding row values using Pandas;51057.0;"[""data.groupby(['Country','Place'])['Value'].max()\n""]";"[""data.groupby(['Country','Place'])['Value'].max()\n""]";"[""data.groupby(['Country','Place'])['Value'].max()\n""]";"[""data.groupby(['Country','Place'])['Value'].max()\n""]";"[""data.groupby(['Country','Place'])['Value'].max()\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\ndata.groupby(['Country','Place'])['Value'].max()\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'Value'""]";['KeyError']
278;278;278;278;3.0;1;15752422;;1;13;<python><dataframe><pandas>;Python Pandas - Date Column to Column index;18886.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'datetime' is not defined""]";['NameError'];0;1;"[""name 'datetime' is not defined""]";['NameError']
279;279;279;279;2.0;0;15755057;;1;11;<python><group-by><pandas>;Using cumsum in pandas on group();9433.0;"["" data1=pd.DataFrame({'Dir':['E','E','W','W','E','W','W','E'], 'Bool':['Y','N','Y','N','Y','N','Y','N'], 'Data':[4,5,6,7,8,9,10,11]}, index=pd.DatetimeIndex(['12/30/2000','12/30/2000','12/30/2000','1/2/2001','1/3/2001','1/3/2001','12/30/2000','12/30/2000']))\ndata1\nOut[1]: \n           Bool  Data Dir\n2000-12-30    Y     4   E\n2000-12-30    N     5   E\n2000-12-30    Y     6   W\n2001-01-02    N     7   W\n2001-01-03    Y     8   E\n2001-01-03    N     9   W\n2000-12-30    Y    10   W\n2000-12-30    N    11   E\nBool Dir Date        running_sum\nN    E   2000-12-30           16\n     W   2001-01-02            7\n         2001-01-03           16\nY    E   2000-12-30            4\n         2001-01-03           12\n     W   2000-12-30           16\n""]";"["" data1=pd.DataFrame({'Dir':['E','E','W','W','E','W','W','E'], 'Bool':['Y','N','Y','N','Y','N','Y','N'], 'Data':[4,5,6,7,8,9,10,11]}, index=pd.DatetimeIndex(['12/30/2000','12/30/2000','12/30/2000','1/2/2001','1/3/2001','1/3/2001','12/30/2000','12/30/2000']))\ndata1\nOut[1]: \n           Bool  Data Dir\n2000-12-30    Y     4   E\n2000-12-30    N     5   E\n2000-12-30    Y     6   W\n2001-01-02    N     7   W\n2001-01-03    Y     8   E\n2001-01-03    N     9   W\n2000-12-30    Y    10   W\n2000-12-30    N    11   E\n"", 'Bool Dir Date        running_sum\nN    E   2000-12-30           16\n     W   2001-01-02            7\n         2001-01-03           16\nY    E   2000-12-30            4\n         2001-01-03           12\n     W   2000-12-30           16\n']";"["" data1=pd.DataFrame({'Dir':['E','E','W','W','E','W','W','E'], 'Bool':['Y','N','Y','N','Y','N','Y','N'], 'Data':[4,5,6,7,8,9,10,11]}, index=pd.DatetimeIndex(['12/30/2000','12/30/2000','12/30/2000','1/2/2001','1/3/2001','1/3/2001','12/30/2000','12/30/2000']))\ndata1\nOut[1]: \n           Bool  Data Dir\n2000-12-30    Y     4   E\n2000-12-30    N     5   E\n2000-12-30    Y     6   W\n2001-01-02    N     7   W\n2001-01-03    Y     8   E\n2001-01-03    N     9   W\n2000-12-30    Y    10   W\n2000-12-30    N    11   E\n"", ""running_sum=data1.groupby(['Bool','Dir']).cumsum()"", 'Bool Dir Date        running_sum\nN    E   2000-12-30           16\n     W   2001-01-02            7\n         2001-01-03           16\nY    E   2000-12-30            4\n         2001-01-03           12\n     W   2000-12-30           16\n']";['data1\n'];['data1\n'];False;['import pandas as pd\ndata1 = pd.DataFrame()\ndata = pd.DataFrame()\ndata1\n'];True;0;2;"[""name 'data1' is not defined"", ""name 'data1' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'data1' is not defined"", ""name 'data1' is not defined""]";['NameError', 'NameError'];0;2;"[""'Bool'"", ""'Bool'""]";['KeyError', 'KeyError']
280;280;280;280;7.0;3;15771472;;1;41;<python><pandas><time-series>;Pandas: rolling mean by time interval;47293.0;['polls_subset.tail(20)\nOut[185]: \n            favorable  unfavorable  other\n\nenddate                                  \n2012-10-25       0.48         0.49   0.03\n2012-10-25       0.51         0.48   0.02\n2012-10-27       0.51         0.47   0.02\n2012-10-26       0.56         0.40   0.04\n2012-10-28       0.48         0.49   0.04\n2012-10-28       0.46         0.46   0.09\n2012-10-28       0.48         0.49   0.03\n2012-10-28       0.49         0.48   0.03\n2012-10-30       0.53         0.45   0.02\n2012-11-01       0.49         0.49   0.03\n2012-11-01       0.47         0.47   0.05\n2012-11-01       0.51         0.45   0.04\n2012-11-03       0.49         0.45   0.06\n2012-11-04       0.53         0.39   0.00\n2012-11-04       0.47         0.44   0.08\n2012-11-04       0.49         0.48   0.03\n2012-11-04       0.52         0.46   0.01\n2012-11-04       0.50         0.47   0.03\n2012-11-05       0.51         0.46   0.02\n2012-11-07       0.51         0.41   0.00\n'];['polls_subset.tail(20)\nOut[185]: \n            favorable  unfavorable  other\n\nenddate                                  \n2012-10-25       0.48         0.49   0.03\n2012-10-25       0.51         0.48   0.02\n2012-10-27       0.51         0.47   0.02\n2012-10-26       0.56         0.40   0.04\n2012-10-28       0.48         0.49   0.04\n2012-10-28       0.46         0.46   0.09\n2012-10-28       0.48         0.49   0.03\n2012-10-28       0.49         0.48   0.03\n2012-10-30       0.53         0.45   0.02\n2012-11-01       0.49         0.49   0.03\n2012-11-01       0.47         0.47   0.05\n2012-11-01       0.51         0.45   0.04\n2012-11-03       0.49         0.45   0.06\n2012-11-04       0.53         0.39   0.00\n2012-11-04       0.47         0.44   0.08\n2012-11-04       0.49         0.48   0.03\n2012-11-04       0.52         0.46   0.01\n2012-11-04       0.50         0.47   0.03\n2012-11-05       0.51         0.46   0.02\n2012-11-07       0.51         0.41   0.00\n'];['polls_subset.tail(20)\nOut[185]: \n            favorable  unfavorable  other\n\nenddate                                  \n2012-10-25       0.48         0.49   0.03\n2012-10-25       0.51         0.48   0.02\n2012-10-27       0.51         0.47   0.02\n2012-10-26       0.56         0.40   0.04\n2012-10-28       0.48         0.49   0.04\n2012-10-28       0.46         0.46   0.09\n2012-10-28       0.48         0.49   0.03\n2012-10-28       0.49         0.48   0.03\n2012-10-30       0.53         0.45   0.02\n2012-11-01       0.49         0.49   0.03\n2012-11-01       0.47         0.47   0.05\n2012-11-01       0.51         0.45   0.04\n2012-11-03       0.49         0.45   0.06\n2012-11-04       0.53         0.39   0.00\n2012-11-04       0.47         0.44   0.08\n2012-11-04       0.49         0.48   0.03\n2012-11-04       0.52         0.46   0.01\n2012-11-04       0.50         0.47   0.03\n2012-11-05       0.51         0.46   0.02\n2012-11-07       0.51         0.41   0.00\n'];['polls_subset.tail(20)\n\nenddate                                  \n'];['polls_subset.tail(20)\n\nenddate                                  \n'];False;['import pandas as pd\npolls_subset.tail(20)\n\nenddate                                  \n'];False;0;2;"[""name 'pd' is not defined"", ""name 'idx' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'idx' is not defined""]";['NameError', 'NameError'];0;2;"[""Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'Index'"", ""name 'idx' is not defined""]";['TypeError', 'NameError']
281;281;281;281;9.0;0;15772009;;1;45;<python><numpy><pandas>;shuffling/permutating a DataFrame in pandas;40329.0;"[""for 1...n:\n  for each col in df: shuffle column\nreturn new_df\ndef shuffle(df, n, axis=0):\n        shuffled_df = df.copy()\n        for k in range(n):\n            shuffled_df.apply(np.random.shuffle(shuffled_df.values),axis=axis)\n        return shuffled_df\n\ndf = pandas.DataFrame({'A':range(10), 'B':range(10)})\nshuffle(df, 5)\n""]";"['for 1...n:\n  for each col in df: shuffle column\nreturn new_df\n', ""def shuffle(df, n, axis=0):\n        shuffled_df = df.copy()\n        for k in range(n):\n            shuffled_df.apply(np.random.shuffle(shuffled_df.values),axis=axis)\n        return shuffled_df\n\ndf = pandas.DataFrame({'A':range(10), 'B':range(10)})\nshuffle(df, 5)\n""]";"['shuffle(df, n, axis=0)', 'n', 'axis=0', 'axis=1', 'n', 'df.index', 'df', 'a', 'b', 'a', 'b', 'for 1...n:\n  for each col in df: shuffle column\nreturn new_df\n', ""def shuffle(df, n, axis=0):\n        shuffled_df = df.copy()\n        for k in range(n):\n            shuffled_df.apply(np.random.shuffle(shuffled_df.values),axis=axis)\n        return shuffled_df\n\ndf = pandas.DataFrame({'A':range(10), 'B':range(10)})\nshuffle(df, 5)\n""]";"[""return new_df\n\ndf = pandas.DataFrame({'A':range(10), 'B':range(10)})\nshuffle(df, 5)\n""]";"[""return new_df\n\ndf = pandas.DataFrame({'A':range(10), 'B':range(10)})\nshuffle(df, 5)\n""]";False;"[""import pandas as pd\nreturn new_df\n\ndf = pandas.DataFrame({'A':range(10), 'B':range(10)})\nshuffle(df, 5)\n""]";False;1;4;"[""name 'pd' is not defined"", 'Sucess', ""name 'df' is not defined"", ""No module named 'sklearn'""]";['NameError', 'Sucess', 'NameError', 'ImportError'];1;4;"[""name 'np' is not defined"", 'Sucess', ""name 'df' is not defined"", ""No module named 'sklearn'""]";['NameError', 'Sucess', 'NameError', 'ImportError'];1;4;"[""name 'np' is not defined"", 'Sucess', 'a must be greater than 0', ""No module named 'sklearn'""]";['NameError', 'Sucess', 'ValueError', 'ImportError']
282;282;282;282;1.0;0;15777951;;1;25;<pandas><suppress-warnings>;How to suppress Pandas Future warning ?;7622.0;"['D:\\Python\\lib\\site-packages\\pandas\\core\\frame.py:3581: FutureWarning: rename with inplace=True  will return None from pandas 0.11 onward\n  "" from pandas 0.11 onward"", FutureWarning) \n']";"['D:\\Python\\lib\\site-packages\\pandas\\core\\frame.py:3581: FutureWarning: rename with inplace=True  will return None from pandas 0.11 onward\n  "" from pandas 0.11 onward"", FutureWarning) \n']";"['D:\\Python\\lib\\site-packages\\pandas\\core\\frame.py:3581: FutureWarning: rename with inplace=True  will return None from pandas 0.11 onward\n  "" from pandas 0.11 onward"", FutureWarning) \n']";[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
283;283;283;283;1.0;6;15798209;;1;17;<python><pandas><pytables>;"Pandas ""Group By"" Query on Large Data in HDFStore?";5672.0;[''];[];['HDFStore', 'DataFrame', 'DataFrame', 'HDFStore'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""[Errno 2] No such file or directory: 'groupby.h5'""]";['FileNotFoundError'];0;1;"[""[Errno 2] No such file or directory: 'groupby.h5'""]";['FileNotFoundError'];0;1;"[""[Errno 2] No such file or directory: 'groupby.h5'""]";['FileNotFoundError']
284;284;284;284;6.0;0;15799162;;1;15;<python><pandas><time-series><hierarchical-data>;Resampling Within a Pandas MultiIndex;8485.0;"['df = pandas.DataFrame(\n    {\'value_a\': values_a, \'value_b\': values_b},\n    index=[states, cities, dates])\ndf.index.names = [\'State\', \'City\', \'Date\']\ndf\n\n                               value_a  value_b\nState   City       Date                        \nGeorgia Atlanta    2012-01-01        0       10\n                   2012-01-02        1       11\n                   2012-01-03        2       12\n                   2012-01-04        3       13\n        Savanna    2012-01-01        4       14\n                   2012-01-02        5       15\n                   2012-01-03        6       16\n                   2012-01-04        7       17\nAlabama Mobile     2012-01-01        8       18\n                   2012-01-02        9       19\n                   2012-01-03       10       20\n                   2012-01-04       11       21\n        Montgomery 2012-01-01       12       22\n                   2012-01-02       13       23\n                   2012-01-03       14       24\n                   2012-01-04       15       25\ndf.resample(""2D"", how=""sum"")\n                             value_a  value_b\nState   City       Date                        \nGeorgia Atlanta    2012-01-01        1       21\n                   2012-01-03        5       25\n        Savanna    2012-01-01        9       29\n                   2012-01-03       13       33\nAlabama Mobile     2012-01-01       17       37\n                   2012-01-03       21       41\n        Montgomery 2012-01-01       25       45\n                   2012-01-03       29       49\nTypeError: Only valid with DatetimeIndex or PeriodIndex\n>>> df.swaplevel(\'Date\', \'State\').resample(\'2D\', how=\'sum\')\nTypeError: Only valid with DatetimeIndex or PeriodIndex\n']";"[""df = pandas.DataFrame(\n    {'value_a': values_a, 'value_b': values_b},\n    index=[states, cities, dates])\ndf.index.names = ['State', 'City', 'Date']\ndf\n\n                               value_a  value_b\nState   City       Date                        \nGeorgia Atlanta    2012-01-01        0       10\n                   2012-01-02        1       11\n                   2012-01-03        2       12\n                   2012-01-04        3       13\n        Savanna    2012-01-01        4       14\n                   2012-01-02        5       15\n                   2012-01-03        6       16\n                   2012-01-04        7       17\nAlabama Mobile     2012-01-01        8       18\n                   2012-01-02        9       19\n                   2012-01-03       10       20\n                   2012-01-04       11       21\n        Montgomery 2012-01-01       12       22\n                   2012-01-02       13       23\n                   2012-01-03       14       24\n                   2012-01-04       15       25\n"", 'df.resample(""2D"", how=""sum"")\n', '                             value_a  value_b\nState   City       Date                        \nGeorgia Atlanta    2012-01-01        1       21\n                   2012-01-03        5       25\n        Savanna    2012-01-01        9       29\n                   2012-01-03       13       33\nAlabama Mobile     2012-01-01       17       37\n                   2012-01-03       21       41\n        Montgomery 2012-01-01       25       45\n                   2012-01-03       29       49\n', 'TypeError: Only valid with DatetimeIndex or PeriodIndex\n', "">>> df.swaplevel('Date', 'State').resample('2D', how='sum')\nTypeError: Only valid with DatetimeIndex or PeriodIndex\n""]";"[""df = pandas.DataFrame(\n    {'value_a': values_a, 'value_b': values_b},\n    index=[states, cities, dates])\ndf.index.names = ['State', 'City', 'Date']\ndf\n\n                               value_a  value_b\nState   City       Date                        \nGeorgia Atlanta    2012-01-01        0       10\n                   2012-01-02        1       11\n                   2012-01-03        2       12\n                   2012-01-04        3       13\n        Savanna    2012-01-01        4       14\n                   2012-01-02        5       15\n                   2012-01-03        6       16\n                   2012-01-04        7       17\nAlabama Mobile     2012-01-01        8       18\n                   2012-01-02        9       19\n                   2012-01-03       10       20\n                   2012-01-04       11       21\n        Montgomery 2012-01-01       12       22\n                   2012-01-02       13       23\n                   2012-01-03       14       24\n                   2012-01-04       15       25\n"", 'df.resample(""2D"", how=""sum"")\n', '                             value_a  value_b\nState   City       Date                        \nGeorgia Atlanta    2012-01-01        1       21\n                   2012-01-03        5       25\n        Savanna    2012-01-01        9       29\n                   2012-01-03       13       33\nAlabama Mobile     2012-01-01       17       37\n                   2012-01-03       21       41\n        Montgomery 2012-01-01       25       45\n                   2012-01-03       29       49\n', ""df.resample('2D', how='sum')"", 'TypeError: Only valid with DatetimeIndex or PeriodIndex\n', "">>> df.swaplevel('Date', 'State').resample('2D', how='sum')\nTypeError: Only valid with DatetimeIndex or PeriodIndex\n""]";"['df.index.names = [\'State\', \'City\', \'Date\']\ndf\n\ndf.resample(""2D"", how=""sum"")\n']";"['df.index.names = [\'State\', \'City\', \'Date\']\ndf\n\ndf.resample(""2D"", how=""sum"")\n']";False;"['import pandas as pd\ndf = pd.DataFrame()\ndf.index.names = [\'State\', \'City\', \'Date\']\ndf\n\ndf.resample(""2D"", how=""sum"")\n']";True;0;3;"[""name 'make_df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'make_df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'make_df' is not defined"", 'Too many levels: Index has only 1 level, not 2', 'multiple levels only valid with MultiIndex']";['NameError', 'IndexError', 'ValueError']
285;285;285;285;1.0;0;15819050;;1;22;<python><pandas>;Pandas DataFrame concat vs append;33712.0;"[""data\n\n[<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 35228 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-03-28 18:59:20.357000+02:00\nData columns:\nPrice       4040  non-null values\nVolume      4040  non-null values\nBidQty      35228  non-null values\nBidPrice    35228  non-null values\nAskPrice    35228  non-null values\nAskQty      35228  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 33088 entries, 2013-04-01 00:03:17.047000+02:00 to 2013-04-01 18:59:58.175000+02:00\nData columns:\nPrice       3969  non-null values\nVolume      3969  non-null values\nBidQty      33088  non-null values\nBidPrice    33088  non-null values\nAskPrice    33088  non-null values\nAskQty      33088  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 50740 entries, 2013-04-02 00:03:27.470000+02:00 to 2013-04-02 18:59:58.172000+02:00\nData columns:\nPrice       7326  non-null values\nVolume      7326  non-null values\nBidQty      50740  non-null values\nBidPrice    50740  non-null values\nAskPrice    50740  non-null values\nAskQty      50740  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 60799 entries, 2013-04-03 00:03:06.994000+02:00 to 2013-04-03 18:59:58.180000+02:00\nData columns:\nPrice       8258  non-null values\nVolume      8258  non-null values\nBidQty      60799  non-null values\nBidPrice    60799  non-null values\nAskPrice    60799  non-null values\nAskQty      60799  non-null values\ndtypes: float64(6)]\npd.DataFrame().append(data)\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 179855 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-04-03 18:59:58.180000+02:00\nData columns:\nAskPrice    179855  non-null values\nAskQty      179855  non-null values\nBidPrice    179855  non-null values\nBidQty      179855  non-null values\nPrice       23593  non-null values\nVolume      23593  non-null values\ndtypes: float64(6)\npd.concat(data)\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 179855 entries, 2013-03-27 22:00:07.089000+02:00 to 2013-04-03 16:59:58.180000+02:00\nData columns:\nPrice       23593  non-null values\nVolume      23593  non-null values\nBidQty      179855  non-null values\nBidPrice    179855  non-null values\nAskPrice    179855  non-null values\nAskQty      179855  non-null values\ndtypes: float64(6)\n""]";"[""data\n\n[<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 35228 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-03-28 18:59:20.357000+02:00\nData columns:\nPrice       4040  non-null values\nVolume      4040  non-null values\nBidQty      35228  non-null values\nBidPrice    35228  non-null values\nAskPrice    35228  non-null values\nAskQty      35228  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 33088 entries, 2013-04-01 00:03:17.047000+02:00 to 2013-04-01 18:59:58.175000+02:00\nData columns:\nPrice       3969  non-null values\nVolume      3969  non-null values\nBidQty      33088  non-null values\nBidPrice    33088  non-null values\nAskPrice    33088  non-null values\nAskQty      33088  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 50740 entries, 2013-04-02 00:03:27.470000+02:00 to 2013-04-02 18:59:58.172000+02:00\nData columns:\nPrice       7326  non-null values\nVolume      7326  non-null values\nBidQty      50740  non-null values\nBidPrice    50740  non-null values\nAskPrice    50740  non-null values\nAskQty      50740  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 60799 entries, 2013-04-03 00:03:06.994000+02:00 to 2013-04-03 18:59:58.180000+02:00\nData columns:\nPrice       8258  non-null values\nVolume      8258  non-null values\nBidQty      60799  non-null values\nBidPrice    60799  non-null values\nAskPrice    60799  non-null values\nAskQty      60799  non-null values\ndtypes: float64(6)]\n"", ""pd.DataFrame().append(data)\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 179855 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-04-03 18:59:58.180000+02:00\nData columns:\nAskPrice    179855  non-null values\nAskQty      179855  non-null values\nBidPrice    179855  non-null values\nBidQty      179855  non-null values\nPrice       23593  non-null values\nVolume      23593  non-null values\ndtypes: float64(6)\n"", ""pd.concat(data)\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 179855 entries, 2013-03-27 22:00:07.089000+02:00 to 2013-04-03 16:59:58.180000+02:00\nData columns:\nPrice       23593  non-null values\nVolume      23593  non-null values\nBidQty      179855  non-null values\nBidPrice    179855  non-null values\nAskPrice    179855  non-null values\nAskQty      179855  non-null values\ndtypes: float64(6)\n""]";"[""data\n\n[<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 35228 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-03-28 18:59:20.357000+02:00\nData columns:\nPrice       4040  non-null values\nVolume      4040  non-null values\nBidQty      35228  non-null values\nBidPrice    35228  non-null values\nAskPrice    35228  non-null values\nAskQty      35228  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 33088 entries, 2013-04-01 00:03:17.047000+02:00 to 2013-04-01 18:59:58.175000+02:00\nData columns:\nPrice       3969  non-null values\nVolume      3969  non-null values\nBidQty      33088  non-null values\nBidPrice    33088  non-null values\nAskPrice    33088  non-null values\nAskQty      33088  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 50740 entries, 2013-04-02 00:03:27.470000+02:00 to 2013-04-02 18:59:58.172000+02:00\nData columns:\nPrice       7326  non-null values\nVolume      7326  non-null values\nBidQty      50740  non-null values\nBidPrice    50740  non-null values\nAskPrice    50740  non-null values\nAskQty      50740  non-null values\ndtypes: float64(6),\n<class 'pandas.core.frame.DataFrame'>\n\nDatetimeIndex: 60799 entries, 2013-04-03 00:03:06.994000+02:00 to 2013-04-03 18:59:58.180000+02:00\nData columns:\nPrice       8258  non-null values\nVolume      8258  non-null values\nBidQty      60799  non-null values\nBidPrice    60799  non-null values\nAskPrice    60799  non-null values\nAskQty      60799  non-null values\ndtypes: float64(6)]\n"", 'append', ""pd.DataFrame().append(data)\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 179855 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-04-03 18:59:58.180000+02:00\nData columns:\nAskPrice    179855  non-null values\nAskQty      179855  non-null values\nBidPrice    179855  non-null values\nBidQty      179855  non-null values\nPrice       23593  non-null values\nVolume      23593  non-null values\ndtypes: float64(6)\n"", 'concat', ""pd.concat(data)\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 179855 entries, 2013-03-27 22:00:07.089000+02:00 to 2013-04-03 16:59:58.180000+02:00\nData columns:\nPrice       23593  non-null values\nVolume      23593  non-null values\nBidQty      179855  non-null values\nBidPrice    179855  non-null values\nAskPrice    179855  non-null values\nAskQty      179855  non-null values\ndtypes: float64(6)\n"", 'concat', 'concat', 'append', 'concat']";['data\n\n\n\n\npd.DataFrame().append(data)\n\npd.concat(data)\n\n'];['import pandas as pd\ndata\n\n\n\n\npd.DataFrame().append(data)\n\npd.concat(data)\n\n'];True;['import pandas as pd\ndata = pd.DataFrame()\ndata\n\n\n\n\npd.DataFrame().append(data)\n\npd.concat(data)\n\n'];True;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
286;286;286;286;1.0;0;15854878;;1;28;<python><pandas>;Correlation between columns in DataFrame;45152.0;['     a     b\n0  0.5  0.75\n1  0.5  0.75\n2  0.5  0.75\n3  0.5  0.75\n4  0.5  0.75\n    a   b\na NaN NaN\nb NaN NaN\n'];['     a     b\n0  0.5  0.75\n1  0.5  0.75\n2  0.5  0.75\n3  0.5  0.75\n4  0.5  0.75\n', '    a   b\na NaN NaN\nb NaN NaN\n'];"['     a     b\n0  0.5  0.75\n1  0.5  0.75\n2  0.5  0.75\n3  0.5  0.75\n4  0.5  0.75\n', 'df.corr()', '    a   b\na NaN NaN\nb NaN NaN\n', 'np.correlate(df[""a""], df[""b""])', '1.875', 'corr()', 'NaN']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
287;287;287;287;2.0;0;15862034;;1;21;<python><pandas>;Access index of last element in data frame;31919.0;"[""df.ix[0]['date']\ndatetime.datetime(2011, 1, 10, 16, 0)\ndf[-1:]['date']\nmyIndex\n13         2011-12-20 16:00:00\nName: mydate\ndf.ix[df.tail(1)['IndexCopy']]['mydate']\n""]";"[""df.ix[0]['date']\n"", 'datetime.datetime(2011, 1, 10, 16, 0)\n', ""df[-1:]['date']\n"", 'myIndex\n13         2011-12-20 16:00:00\nName: mydate\n', ""df.ix[df.tail(1)['IndexCopy']]['mydate']\n""]";"[""df.ix[0]['date']\n"", 'datetime.datetime(2011, 1, 10, 16, 0)\n', ""df[-1:]['date']\n"", 'myIndex\n13         2011-12-20 16:00:00\nName: mydate\n', ""df.ix[df.tail(1)['IndexCopy']]['mydate']\n""]";"[""df.ix[0]['date']\ndatetime.datetime(2011, 1, 10, 16, 0)\ndf[-1:]['date']\nmyIndex\ndf.ix[df.tail(1)['IndexCopy']]['mydate']\n""]";"[""df.ix[0]['date']\ndatetime.datetime(2011, 1, 10, 16, 0)\ndf[-1:]['date']\nmyIndex\ndf.ix[df.tail(1)['IndexCopy']]['mydate']\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf.ix[0]['date']\ndatetime.datetime(2011, 1, 10, 16, 0)\ndf[-1:]['date']\nmyIndex\ndf.ix[df.tail(1)['IndexCopy']]['mydate']\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
288;288;288;288;1.0;0;15888648;;1;16;<python><pandas>;Is it possible to insert a row at an arbitrary position in a dataframe using pandas?;6025.0;['       onset    length\n1      2.215    1.3\n2     23.107    1.3\n3     41.815    1.3\n4     61.606    1.3\n...\n       onset    length\n1      2.215    1.3\n2     23.107    1.3\n3     30.000    1.3  # new row\n4     41.815    1.3\n5     61.606    1.3\n...\n'];['       onset    length\n1      2.215    1.3\n2     23.107    1.3\n3     41.815    1.3\n4     61.606    1.3\n...\n', '       onset    length\n1      2.215    1.3\n2     23.107    1.3\n3     30.000    1.3  # new row\n4     41.815    1.3\n5     61.606    1.3\n...\n'];['       onset    length\n1      2.215    1.3\n2     23.107    1.3\n3     41.815    1.3\n4     61.606    1.3\n...\n', '       onset    length\n1      2.215    1.3\n2     23.107    1.3\n3     30.000    1.3  # new row\n4     41.815    1.3\n5     61.606    1.3\n...\n'];['...\n...\n'];['...\n...\n'];False;['import pandas as pd\n...\n...\n'];False;0;1;"[""name 'DataFrame' is not defined""]";['NameError'];0;1;"[""name 'concat' is not defined""]";['NameError'];0;1;"[""name 'DataFrame' is not defined""]";['NameError']
289;289;289;289;4.0;5;15891038;;1;257;<python><pandas><dataframe><types><casting>;Pandas: change data type of columns;406291.0;"[""a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\ndf = pd.DataFrame(a)\n""]";"[""a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\ndf = pd.DataFrame(a)\n""]";"[""a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\ndf = pd.DataFrame(a)\n""]";"[""a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\ndf = pd.DataFrame(a)\n""]";"[""import pandas as pd\na = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\ndf = pd.DataFrame(a)\n""]";True;"[""import pandas as pd\na = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\ndf = pd.DataFrame(a)\n""]";False;0;2;"[""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', '""[\'col2\' \'col3\'] not in index""']";['Sucess', 'KeyError']
290;290;290;290;4.0;0;15910019;;1;13;<matplotlib><pandas>;Annotate data points while plotting from Pandas DataFrame;8786.0;['ax = plt.figure().add_subplot(1, 1, 1)\ndf.plot(ax = ax)\nplt.show()\n'];['ax = plt.figure().add_subplot(1, 1, 1)\ndf.plot(ax = ax)\nplt.show()\n'];['ax = plt.figure().add_subplot(1, 1, 1)\ndf.plot(ax = ax)\nplt.show()\n'];['ax = plt.figure().add_subplot(1, 1, 1)\ndf.plot(ax = ax)\nplt.show()\n'];['ax = plt.figure().add_subplot(1, 1, 1)\ndf.plot(ax = ax)\nplt.show()\n'];False;['import pandas as pd\ndf = pd.DataFrame()\nax = plt.figure().add_subplot(1, 1, 1)\ndf.plot(ax = ax)\nplt.show()\n'];True;0;2;"[""name 'df' is not defined"", ""No module named 'matplotlib'""]";['NameError', 'ImportError'];0;2;"[""name 'df' is not defined"", ""No module named 'matplotlib'""]";['NameError', 'ImportError'];1;2;"['Sucess', ""No module named 'matplotlib'""]";['Sucess', 'ImportError']
291;291;291;291;1.0;0;15916612;;1;20;<pandas><division>;How to divide the value of pandas columns by the other column;25171.0;['>>> dt\n                   COL000   COL001   QT\nSTK_ID  RPT_Date                       \nSTK000  20120331   2.6151   2.1467    1\n        20120630   4.0589   2.3442    2\n        20120930   4.4547   3.9204    3\n        20121231   4.1360   3.8559    4\nSTK001  20120331  -0.2178   0.9184    1\n        20120630  -1.9639   0.7900    2\n        20120930  -2.9147   1.0189    3\n        20121231  -2.5648   2.3743    4\nSTK002  20120331  -0.6426   0.9543    1\n        20120630  -0.3575   1.6085    2\n        20120930  -2.3549   0.7174    3\n        20121231  -3.4860   1.6324    4\ndt =  dt/dt.QT     # pandas does not accept this syntax\nSTK_ID  RPT_Date        COL000       COL001  QT\nSTK000  20120331   2.615110188  2.146655745   1\n        20120630   2.029447265  1.172093561   1\n        20120930   1.484909881  1.306795608   1\n        20121231   1.034008443  0.963970609   1\nSTK001  20120331  -0.217808111  0.918355842   1\n        20120630  -0.981974837  0.394977675   1\n        20120930  -0.97157148   0.339633733   1\n        20121231  -0.641203355  0.593569537   1\nSTK002  20120331  -0.642567516  0.954323016   1\n        20120630  -0.178759288  0.804230898   1\n        20120930  -0.784982521  0.239117442   1\n        20121231  -0.871501505  0.408094317   1\n'];['>>> dt\n                   COL000   COL001   QT\nSTK_ID  RPT_Date                       \nSTK000  20120331   2.6151   2.1467    1\n        20120630   4.0589   2.3442    2\n        20120930   4.4547   3.9204    3\n        20121231   4.1360   3.8559    4\nSTK001  20120331  -0.2178   0.9184    1\n        20120630  -1.9639   0.7900    2\n        20120930  -2.9147   1.0189    3\n        20121231  -2.5648   2.3743    4\nSTK002  20120331  -0.6426   0.9543    1\n        20120630  -0.3575   1.6085    2\n        20120930  -2.3549   0.7174    3\n        20121231  -3.4860   1.6324    4\n', 'dt =  dt/dt.QT     # pandas does not accept this syntax\n', 'STK_ID  RPT_Date        COL000       COL001  QT\nSTK000  20120331   2.615110188  2.146655745   1\n        20120630   2.029447265  1.172093561   1\n        20120930   1.484909881  1.306795608   1\n        20121231   1.034008443  0.963970609   1\nSTK001  20120331  -0.217808111  0.918355842   1\n        20120630  -0.981974837  0.394977675   1\n        20120930  -0.97157148   0.339633733   1\n        20121231  -0.641203355  0.593569537   1\nSTK002  20120331  -0.642567516  0.954323016   1\n        20120630  -0.178759288  0.804230898   1\n        20120930  -0.784982521  0.239117442   1\n        20121231  -0.871501505  0.408094317   1\n'];['>>> dt\n                   COL000   COL001   QT\nSTK_ID  RPT_Date                       \nSTK000  20120331   2.6151   2.1467    1\n        20120630   4.0589   2.3442    2\n        20120930   4.4547   3.9204    3\n        20121231   4.1360   3.8559    4\nSTK001  20120331  -0.2178   0.9184    1\n        20120630  -1.9639   0.7900    2\n        20120930  -2.9147   1.0189    3\n        20121231  -2.5648   2.3743    4\nSTK002  20120331  -0.6426   0.9543    1\n        20120630  -0.3575   1.6085    2\n        20120930  -2.3549   0.7174    3\n        20121231  -3.4860   1.6324    4\n', 'dt =  dt/dt.QT     # pandas does not accept this syntax\n', 'STK_ID  RPT_Date        COL000       COL001  QT\nSTK000  20120331   2.615110188  2.146655745   1\n        20120630   2.029447265  1.172093561   1\n        20120930   1.484909881  1.306795608   1\n        20121231   1.034008443  0.963970609   1\nSTK001  20120331  -0.217808111  0.918355842   1\n        20120630  -0.981974837  0.394977675   1\n        20120930  -0.97157148   0.339633733   1\n        20121231  -0.641203355  0.593569537   1\nSTK002  20120331  -0.642567516  0.954323016   1\n        20120630  -0.178759288  0.804230898   1\n        20120930  -0.784982521  0.239117442   1\n        20121231  -0.871501505  0.408094317   1\n'];['dt =  dt/dt.QT     # pandas does not accept this syntax\n'];['dt =  dt/dt.QT     # pandas does not accept this syntax\n'];False;['import pandas as pd\ndt =  dt/dt.QT     # pandas does not accept this syntax\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError']
292;292;292;292;5.0;0;15923826;;1;33;<python><pandas>;Random row selection in Pandas dataframe;26646.0;[''];[];['some(x, n)', 'df.sample(n)'];[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'data' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'data' is not defined""]";['Sucess', 'NameError'];1;2;['Sucess', 'a must be greater than 0'];['Sucess', 'ValueError']
293;293;293;293;3.0;0;15930885;;1;12;<python><pandas><time-series>;Converting irregularly time stamped measurements into equally spaced, time-weighted averages;1459.0;"[""23:00:00.100     10\n23:00:01.200      8\n23:00:01.600      0\n23:00:06.300      4\n23:00:01     NaN ( the first 100ms are missing )\n23:00:02     5.2 ( 10*0.2 + 8*0.4 + 0*0.4 )\n23:00:03       0\n23:00:04       0\n23:00:05       0\n23:00:06     2.8 ( 0*0.3 + 4*0.7 )\ndata.resample('S', fill_method='pad')          # forming a series of seconds\ndata = data.resample('L', fill_method='pad')   # forming a series of milliseconds\ndata.resample('S')\nimport pandas as pa\nimport numpy as np\nfrom datetime import datetime\nfrom datetime import timedelta\n\ntime_stamps=[datetime(2013,04,11,23,00,00,100000), \n             datetime(2013,04,11,23,00,1,200000),\n             datetime(2013,04,11,23,00,1,600000),\n             datetime(2013,04,11,23,00,6,300000)]\nvalues = [10, 8, 0, 4]\nraw = pa.TimeSeries(index=time_stamps, data=values)\n\ndef round_down_to_second(dt):\n    return datetime(year=dt.year, month=dt.month, day=dt.day, \n                    hour=dt.hour, minute=dt.minute, second=dt.second)\n\ndef round_up_to_second(dt):\n    return round_down_to_second(dt) + timedelta(seconds=1)\n\ndef time_weighted_average(data):\n    end = pa.DatetimeIndex([round_up_to_second(data.index[-1])])\n    return np.average(data, weights=np.diff(data.index.append(end).asi8))\n\nstart = round_down_to_second(time_stamps[0])\nend = round_down_to_second(time_stamps[-1])\nrange = pa.date_range(start, end, freq='S')\ndata = raw.reindex(raw.index + range)\ndata = data.ffill()\n\ndata = data.resample('S', how=time_weighted_average)\n""]";"['23:00:00.100     10\n23:00:01.200      8\n23:00:01.600      0\n23:00:06.300      4\n', '23:00:01     NaN ( the first 100ms are missing )\n23:00:02     5.2 ( 10*0.2 + 8*0.4 + 0*0.4 )\n23:00:03       0\n23:00:04       0\n23:00:05       0\n23:00:06     2.8 ( 0*0.3 + 4*0.7 )\n', ""data.resample('S', fill_method='pad')          # forming a series of seconds\n"", ""data = data.resample('L', fill_method='pad')   # forming a series of milliseconds\ndata.resample('S')\n"", ""import pandas as pa\nimport numpy as np\nfrom datetime import datetime\nfrom datetime import timedelta\n\ntime_stamps=[datetime(2013,04,11,23,00,00,100000), \n             datetime(2013,04,11,23,00,1,200000),\n             datetime(2013,04,11,23,00,1,600000),\n             datetime(2013,04,11,23,00,6,300000)]\nvalues = [10, 8, 0, 4]\nraw = pa.TimeSeries(index=time_stamps, data=values)\n\ndef round_down_to_second(dt):\n    return datetime(year=dt.year, month=dt.month, day=dt.day, \n                    hour=dt.hour, minute=dt.minute, second=dt.second)\n\ndef round_up_to_second(dt):\n    return round_down_to_second(dt) + timedelta(seconds=1)\n\ndef time_weighted_average(data):\n    end = pa.DatetimeIndex([round_up_to_second(data.index[-1])])\n    return np.average(data, weights=np.diff(data.index.append(end).asi8))\n\nstart = round_down_to_second(time_stamps[0])\nend = round_down_to_second(time_stamps[-1])\nrange = pa.date_range(start, end, freq='S')\ndata = raw.reindex(raw.index + range)\ndata = data.ffill()\n\ndata = data.resample('S', how=time_weighted_average)\n""]";"['23:00:00.100     10\n23:00:01.200      8\n23:00:01.600      0\n23:00:06.300      4\n', '23:00:01     NaN ( the first 100ms are missing )\n23:00:02     5.2 ( 10*0.2 + 8*0.4 + 0*0.4 )\n23:00:03       0\n23:00:04       0\n23:00:05       0\n23:00:06     2.8 ( 0*0.3 + 4*0.7 )\n', ""data.resample('S', fill_method='pad')          # forming a series of seconds\n"", ""data = data.resample('L', fill_method='pad')   # forming a series of milliseconds\ndata.resample('S')\n"", ""import pandas as pa\nimport numpy as np\nfrom datetime import datetime\nfrom datetime import timedelta\n\ntime_stamps=[datetime(2013,04,11,23,00,00,100000), \n             datetime(2013,04,11,23,00,1,200000),\n             datetime(2013,04,11,23,00,1,600000),\n             datetime(2013,04,11,23,00,6,300000)]\nvalues = [10, 8, 0, 4]\nraw = pa.TimeSeries(index=time_stamps, data=values)\n\ndef round_down_to_second(dt):\n    return datetime(year=dt.year, month=dt.month, day=dt.day, \n                    hour=dt.hour, minute=dt.minute, second=dt.second)\n\ndef round_up_to_second(dt):\n    return round_down_to_second(dt) + timedelta(seconds=1)\n\ndef time_weighted_average(data):\n    end = pa.DatetimeIndex([round_up_to_second(data.index[-1])])\n    return np.average(data, weights=np.diff(data.index.append(end).asi8))\n\nstart = round_down_to_second(time_stamps[0])\nend = round_down_to_second(time_stamps[-1])\nrange = pa.date_range(start, end, freq='S')\ndata = raw.reindex(raw.index + range)\ndata = data.ffill()\n\ndata = data.resample('S', how=time_weighted_average)\n""]";"[""data.resample('S', fill_method='pad')          # forming a series of seconds\ndata = data.resample('L', fill_method='pad')   # forming a series of milliseconds\ndata.resample('S')\nimport pandas as pa\nimport numpy as np\nfrom datetime import datetime\nfrom datetime import timedelta\n\nvalues = [10, 8, 0, 4]\nraw = pa.TimeSeries(index=time_stamps, data=values)\n\n\n\n\nstart = round_down_to_second(time_stamps[0])\nend = round_down_to_second(time_stamps[-1])\nrange = pa.date_range(start, end, freq='S')\ndata = raw.reindex(raw.index + range)\ndata = data.ffill()\n\ndata = data.resample('S', how=time_weighted_average)\n""]";"[""data.resample('S', fill_method='pad')          # forming a series of seconds\ndata = data.resample('L', fill_method='pad')   # forming a series of milliseconds\ndata.resample('S')\nimport pandas as pa\nimport numpy as np\nfrom datetime import datetime\nfrom datetime import timedelta\n\nvalues = [10, 8, 0, 4]\nraw = pa.TimeSeries(index=time_stamps, data=values)\n\n\n\n\nstart = round_down_to_second(time_stamps[0])\nend = round_down_to_second(time_stamps[-1])\nrange = pa.date_range(start, end, freq='S')\ndata = raw.reindex(raw.index + range)\ndata = data.ffill()\n\ndata = data.resample('S', how=time_weighted_average)\n""]";False;"[""import pandas as pd\ndata.resample('S', fill_method='pad')          # forming a series of seconds\ndata = data.resample('L', fill_method='pad')   # forming a series of milliseconds\ndata.resample('S')\nimport pandas as pa\nimport numpy as np\nfrom datetime import datetime\nfrom datetime import timedelta\n\nvalues = [10, 8, 0, 4]\nraw = pa.TimeSeries(index=time_stamps, data=values)\n\n\n\n\nstart = round_down_to_second(time_stamps[0])\nend = round_down_to_second(time_stamps[-1])\nrange = pa.date_range(start, end, freq='S')\ndata = raw.reindex(raw.index + range)\ndata = data.ffill()\n\ndata = data.resample('S', how=time_weighted_average)\n""]";False;0;0;[];[];0;0;[];[];0;0;[];[]
294;294;294;294;7.0;3;15943769;;1;260;<python><pandas><dataframe>;How do I get the row count of a Pandas dataframe?;373069.0;"[""total_rows = df.count\nprint total_rows +1\ntotal_rows = df['First_columnn_label'].count\nprint total_rows +1\nlen(df.index)\n""]";"['total_rows = df.count\nprint total_rows +1\n', ""total_rows = df['First_columnn_label'].count\nprint total_rows +1\n"", 'len(df.index)\n']";"['total_rows = df.count\nprint total_rows +1\n', ""total_rows = df['First_columnn_label'].count\nprint total_rows +1\n"", 'len(df.index)\n']";"[""total_rows = df.count\ntotal_rows = df['First_columnn_label'].count\nlen(df.index)\n""]";"[""total_rows = df.count\ntotal_rows = df['First_columnn_label'].count\nlen(df.index)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ntotal_rows = df.count\ntotal_rows = df['First_columnn_label'].count\nlen(df.index)\n""]";True;1;5;"[""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError', 'NameError', 'NameError'];1;5;"[""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError', 'NameError', 'NameError'];5;5;['Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess', 'Sucess', 'Sucess']
295;295;295;295;1.0;1;15989281;;1;15;<python><pandas>;python/pandas: how to combine two dataframes into one with hierarchical column index?;3076.0;['>>> df1\n              A    B\n2000-01-01  1.4  1.4\n2000-01-02  1.7 -1.9\n2000-01-03 -0.2 -0.8\n\n>>> df2\n              A    B\n2000-01-01  0.6 -0.3\n2000-01-02 -0.4  0.6\n2000-01-03  1.1 -1.0\n            df1       df2\n              A    B    A    B\n2000-01-01  1.4  1.4  0.6 -0.3\n2000-01-02  1.7 -1.9 -0.4  0.6\n2000-01-03 -0.2 -0.8  1.1 -1.0\n'];['>>> df1\n              A    B\n2000-01-01  1.4  1.4\n2000-01-02  1.7 -1.9\n2000-01-03 -0.2 -0.8\n\n>>> df2\n              A    B\n2000-01-01  0.6 -0.3\n2000-01-02 -0.4  0.6\n2000-01-03  1.1 -1.0\n', '            df1       df2\n              A    B    A    B\n2000-01-01  1.4  1.4  0.6 -0.3\n2000-01-02  1.7 -1.9 -0.4  0.6\n2000-01-03 -0.2 -0.8  1.1 -1.0\n'];['>>> df1\n              A    B\n2000-01-01  1.4  1.4\n2000-01-02  1.7 -1.9\n2000-01-03 -0.2 -0.8\n\n>>> df2\n              A    B\n2000-01-01  0.6 -0.3\n2000-01-02 -0.4  0.6\n2000-01-03  1.1 -1.0\n', '            df1       df2\n              A    B    A    B\n2000-01-01  1.4  1.4  0.6 -0.3\n2000-01-02  1.7 -1.9 -0.4  0.6\n2000-01-03 -0.2 -0.8  1.1 -1.0\n'];['\n'];['\n'];False;['import pandas as pd\n\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
296;296;296;296;4.0;0;15998188;;1;80;<python><pandas><boolean-logic>;How can I obtain the element-wise logical NOT of a pandas Series?;30530.0;['True\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n'];['True\nTrue\nTrue\nFalse\n', 'False\nFalse\nFalse\nTrue\n'];['Series', 'NOT', 'True\nTrue\nTrue\nFalse\n', 'False\nFalse\nFalse\nTrue\n'];['True\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n'];['True\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n'];False;['import pandas as pd\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n'];False;0;2;"[""name 'Series' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];1;2;"[""name 'Series' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'Series' is not defined"", 'Sucess']";['NameError', 'Sucess']
297;297;297;297;2.0;0;16031056;;1;38;<python><dataframe><pandas><tuples>;How to form tuple column from two columns in Pandas;23199.0;"[""<class 'pandas.core.frame.DataFrame'>\nInt64Index: 205482 entries, 0 to 209018\nData columns:\nMonth           205482  non-null values\nReported by     205482  non-null values\nFalls within    205482  non-null values\nEasting         205482  non-null values\nNorthing        205482  non-null values\nLocation        205482  non-null values\nCrime type      205482  non-null values\nlong            205482  non-null values\nlat             205482  non-null values\ndtypes: float64(4), object(5)\ndef merge_two_cols(series): \n    return (series['lat'], series['long'])\n\nsample['lat_long'] = sample.apply(merge_two_cols, axis=1)\n---------------------------------------------------------------------------\n AssertionError                            Traceback (most recent call last)\n<ipython-input-261-e752e52a96e6> in <module>()\n      2     return (series['lat'], series['long'])\n      3 \n----> 4 sample['lat_long'] = sample.apply(merge_two_cols, axis=1)\n      5\nAssertionError: Block shape incompatible with manager \n""]";"[""<class 'pandas.core.frame.DataFrame'>\nInt64Index: 205482 entries, 0 to 209018\nData columns:\nMonth           205482  non-null values\nReported by     205482  non-null values\nFalls within    205482  non-null values\nEasting         205482  non-null values\nNorthing        205482  non-null values\nLocation        205482  non-null values\nCrime type      205482  non-null values\nlong            205482  non-null values\nlat             205482  non-null values\ndtypes: float64(4), object(5)\n"", ""def merge_two_cols(series): \n    return (series['lat'], series['long'])\n\nsample['lat_long'] = sample.apply(merge_two_cols, axis=1)\n"", ""---------------------------------------------------------------------------\n AssertionError                            Traceback (most recent call last)\n<ipython-input-261-e752e52a96e6> in <module>()\n      2     return (series['lat'], series['long'])\n      3 \n----> 4 sample['lat_long'] = sample.apply(merge_two_cols, axis=1)\n      5\n"", 'AssertionError: Block shape incompatible with manager \n']";"[""<class 'pandas.core.frame.DataFrame'>\nInt64Index: 205482 entries, 0 to 209018\nData columns:\nMonth           205482  non-null values\nReported by     205482  non-null values\nFalls within    205482  non-null values\nEasting         205482  non-null values\nNorthing        205482  non-null values\nLocation        205482  non-null values\nCrime type      205482  non-null values\nlong            205482  non-null values\nlat             205482  non-null values\ndtypes: float64(4), object(5)\n"", ""def merge_two_cols(series): \n    return (series['lat'], series['long'])\n\nsample['lat_long'] = sample.apply(merge_two_cols, axis=1)\n"", ""---------------------------------------------------------------------------\n AssertionError                            Traceback (most recent call last)\n<ipython-input-261-e752e52a96e6> in <module>()\n      2     return (series['lat'], series['long'])\n      3 \n----> 4 sample['lat_long'] = sample.apply(merge_two_cols, axis=1)\n      5\n"", 'AssertionError: Block shape incompatible with manager \n']";"[""\nsample['lat_long'] = sample.apply(merge_two_cols, axis=1)\n""]";"[""\nsample['lat_long'] = sample.apply(merge_two_cols, axis=1)\n""]";False;"[""import pandas as pd\n\nsample['lat_long'] = sample.apply(merge_two_cols, axis=1)\n""]";False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"['""[\'lat\' \'long\'] not in index""', ""'DataFrame' object has no attribute 'lat'""]";['KeyError', 'AttributeError']
298;298;298;298;3.0;1;16074392;;1;38;<python><matplotlib><pandas>;Getting vertical gridlines to appear in line plot in matplotlib;50935.0;"[""data.plot()\ngrid('on')\nax = plt.axes()        \nax.yaxis.grid() # horizontal lines\nax.xaxis.grid() # vertical lines\n""]";"[""data.plot()\ngrid('on')\n"", 'ax = plt.axes()        \nax.yaxis.grid() # horizontal lines\nax.xaxis.grid() # vertical lines\n']";"['pandas.DataFrame', ""data.plot()\ngrid('on')\n"", 'ax = plt.axes()        \nax.yaxis.grid() # horizontal lines\nax.xaxis.grid() # vertical lines\n']";"[""data.plot()\ngrid('on')\nax = plt.axes()        \nax.yaxis.grid() # horizontal lines\nax.xaxis.grid() # vertical lines\n""]";"[""data.plot()\ngrid('on')\nax = plt.axes()        \nax.yaxis.grid() # horizontal lines\nax.xaxis.grid() # vertical lines\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\ndata.plot()\ngrid('on')\nax = plt.axes()        \nax.yaxis.grid() # horizontal lines\nax.xaxis.grid() # vertical lines\n""]";True;1;2;"[""name 'plt' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'plt' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'plt' is not defined"", 'Sucess']";['NameError', 'Sucess']
299;299;299;299;2.0;0;16088741;;1;22;<pandas><multi-index>;Pandas: add a column to a multiindex column dataframe;10377.0;"[""In [151]: df\nOut[151]: \nfirst        bar                 baz           \nsecond       one       two       one       two \nA       0.487880 -0.487661 -1.030176  0.100813 \nB       0.267913  1.918923  0.132791  0.178503\nC       1.550526 -0.312235 -1.177689 -0.081596 \nIn [152]: df['bar']['three'] = [0, 1, 2]\n\nIn [153]: df\nOut[153]: \nfirst        bar                 baz           \nsecond       one       two       one       two \nA       0.487880 -0.487661 -1.030176  0.100813\nB       0.267913  1.918923  0.132791  0.178503\nC       1.550526 -0.312235 -1.177689 -0.081596\n""]";"['In [151]: df\nOut[151]: \nfirst        bar                 baz           \nsecond       one       two       one       two \nA       0.487880 -0.487661 -1.030176  0.100813 \nB       0.267913  1.918923  0.132791  0.178503\nC       1.550526 -0.312235 -1.177689 -0.081596 \n', ""In [152]: df['bar']['three'] = [0, 1, 2]\n\nIn [153]: df\nOut[153]: \nfirst        bar                 baz           \nsecond       one       two       one       two \nA       0.487880 -0.487661 -1.030176  0.100813\nB       0.267913  1.918923  0.132791  0.178503\nC       1.550526 -0.312235 -1.177689 -0.081596\n""]";"['In [151]: df\nOut[151]: \nfirst        bar                 baz           \nsecond       one       two       one       two \nA       0.487880 -0.487661 -1.030176  0.100813 \nB       0.267913  1.918923  0.132791  0.178503\nC       1.550526 -0.312235 -1.177689 -0.081596 \n', ""In [152]: df['bar']['three'] = [0, 1, 2]\n\nIn [153]: df\nOut[153]: \nfirst        bar                 baz           \nsecond       one       two       one       two \nA       0.487880 -0.487661 -1.030176  0.100813\nB       0.267913  1.918923  0.132791  0.178503\nC       1.550526 -0.312235 -1.177689 -0.081596\n""]";"[""df\ndf['bar']['three'] = [0, 1, 2]\n\n""]";"[""df\ndf['bar']['three'] = [0, 1, 2]\n\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf\ndf['bar']['three'] = [0, 1, 2]\n\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError']
300;300;300;300;5.0;1;16096627;;1;166;<pandas>;Pandas select row of data frame by integer index;227892.0;['In [26]: df.ix[2]\nOut[26]: \nA    1.027680\nB    1.514210\nC   -1.466963\nD   -0.162339\nName: 2000-01-03 00:00:00\n\nIn [27]: df[2:3]\nOut[27]: \n                  A        B         C         D\n2000-01-03  1.02768  1.51421 -1.466963 -0.162339\n'];['In [26]: df.ix[2]\nOut[26]: \nA    1.027680\nB    1.514210\nC   -1.466963\nD   -0.162339\nName: 2000-01-03 00:00:00\n\nIn [27]: df[2:3]\nOut[27]: \n                  A        B         C         D\n2000-01-03  1.02768  1.51421 -1.466963 -0.162339\n'];['df[2]', 'df.ix[2]', 'df[2:3]', 'In [26]: df.ix[2]\nOut[26]: \nA    1.027680\nB    1.514210\nC   -1.466963\nD   -0.162339\nName: 2000-01-03 00:00:00\n\nIn [27]: df[2:3]\nOut[27]: \n                  A        B         C         D\n2000-01-03  1.02768  1.51421 -1.466963 -0.162339\n', 'df[2]', 'df[2:3]'];['df.ix[2]\ndf[2:3]\n'];['df.ix[2]\ndf[2:3]\n'];False;['import pandas as pd\ndf = pd.DataFrame()\ndf.ix[2]\ndf[2:3]\n'];True;1;3;"['Sucess', ""name 'DataFrame' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name 'randn' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name 'DataFrame' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError']
301;301;301;301;5.0;0;16103238;;1;11;<python><numpy><timestamp><pandas>;Pandas Timedelta in Days;26655.0;['internal_quote_id\n2                    15685977 days, 23:54:30.457856\n3                    11651985 days, 23:49:15.359744\n4                     9491988 days, 23:39:55.621376\n7                     11907004 days, 0:10:30.196224\n9                    15282164 days, 23:30:30.196224\n15                  15282227 days, 23:50:40.261632  \n'];['internal_quote_id\n2                    15685977 days, 23:54:30.457856\n3                    11651985 days, 23:49:15.359744\n4                     9491988 days, 23:39:55.621376\n7                     11907004 days, 0:10:30.196224\n9                    15282164 days, 23:30:30.196224\n15                  15282227 days, 23:50:40.261632  \n'];['internal_quote_id\n2                    15685977 days, 23:54:30.457856\n3                    11651985 days, 23:49:15.359744\n4                     9491988 days, 23:39:55.621376\n7                     11907004 days, 0:10:30.196224\n9                    15282164 days, 23:30:30.196224\n15                  15282227 days, 23:50:40.261632  \n'];['internal_quote_id\n'];['internal_quote_id\n'];False;['import pandas as pd\ninternal_quote_id\n'];False;1;3;"[""name 'DataFrame' is not defined"", ""name 'pd' is not defined"", 'Sucess']";['NameError', 'NameError', 'Sucess'];1;3;"[""name 'Timestamp' is not defined"", '\n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated', 'Sucess']";['NameError', 'DeprecationWarning', 'Sucess'];1;3;"[""name 'DataFrame' is not defined"", '\n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated', 'Sucess']";['NameError', 'DeprecationWarning', 'Sucess']
302;302;302;302;1.0;0;16167829;;1;17;<python><pandas>;In pandas, how can I reset index without adding a new column?;20242.0;['In [37]: df = pd.DataFrame([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]])\n\nIn [38]: df2 = pd.concat([df, df])\n\nIn [39]: df2.reset_index()\nOut[39]: \n   index  0  1  2  3\n0      0  1  2  3  4\n1      1  2  3  4  5\n2      2  3  4  5  6\n3      0  1  2  3  4\n4      1  2  3  4  5\n5      2  3  4  5  6\n'];['In [37]: df = pd.DataFrame([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]])\n\nIn [38]: df2 = pd.concat([df, df])\n\nIn [39]: df2.reset_index()\nOut[39]: \n   index  0  1  2  3\n0      0  1  2  3  4\n1      1  2  3  4  5\n2      2  3  4  5  6\n3      0  1  2  3  4\n4      1  2  3  4  5\n5      2  3  4  5  6\n'];['In [37]: df = pd.DataFrame([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]])\n\nIn [38]: df2 = pd.concat([df, df])\n\nIn [39]: df2.reset_index()\nOut[39]: \n   index  0  1  2  3\n0      0  1  2  3  4\n1      1  2  3  4  5\n2      2  3  4  5  6\n3      0  1  2  3  4\n4      1  2  3  4  5\n5      2  3  4  5  6\n', 'reset_index', 'index'];['df = pd.DataFrame([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]])\n\n\n'];['import pandas as pd\ndf = pd.DataFrame([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]])\n\n\n'];True;['import pandas as pd\ndf = pd.DataFrame([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]])\n\n\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
303;303;303;303;3.0;0;16175874;;1;29;<python><dataframe><pandas>;python pandas dataframe slicing by date conditions;43773.0;"["">>> data\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 252 entries, 2010-12-31 00:00:00 to 2010-04-01 00:00:00\nData columns:\nAdj Close    252  non-null values\ndtypes: float64(1)\n\n>>> st = datetime.datetime(2010, 12, 31, 0, 0)\n>>> en = datetime.datetime(2010, 12, 28, 0, 0)\n\n>>> data[st:en]\n            Adj Close\nDate                 \n2010-12-31     593.97\n2010-12-30     598.86\n2010-12-29     601.00\n2010-12-28     598.92\n""]";"["">>> data\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 252 entries, 2010-12-31 00:00:00 to 2010-04-01 00:00:00\nData columns:\nAdj Close    252  non-null values\ndtypes: float64(1)\n\n>>> st = datetime.datetime(2010, 12, 31, 0, 0)\n>>> en = datetime.datetime(2010, 12, 28, 0, 0)\n\n>>> data[st:en]\n            Adj Close\nDate                 \n2010-12-31     593.97\n2010-12-30     598.86\n2010-12-29     601.00\n2010-12-28     598.92\n""]";"["">>> data\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 252 entries, 2010-12-31 00:00:00 to 2010-04-01 00:00:00\nData columns:\nAdj Close    252  non-null values\ndtypes: float64(1)\n\n>>> st = datetime.datetime(2010, 12, 31, 0, 0)\n>>> en = datetime.datetime(2010, 12, 28, 0, 0)\n\n>>> data[st:en]\n            Adj Close\nDate                 \n2010-12-31     593.97\n2010-12-30     598.86\n2010-12-29     601.00\n2010-12-28     598.92\n""]";['\n\nDate                 \n'];['\n\nDate                 \n'];False;['import pandas as pd\n\n\nDate                 \n'];False;0;2;"[""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'dt' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;"[""name 'dt' is not defined"", 'Sucess']";['NameError', 'Sucess']
304;304;304;304;4.0;6;16176996;;1;33;<python><datetime><pandas>;Keep only date part when using pandas.to_datetime;31231.0;['[dt.to_datetime().date() for dt in df.dates]\n'];['[dt.to_datetime().date() for dt in df.dates]\n'];['pandas.to_datetime', 'datetime64[ns]', 'datetime.date', 'datetime64[D]', '00:00:00', '[dt.to_datetime().date() for dt in df.dates]\n', 'pandas.to_datetime', 'dtype', 'pandas.to_datetime'];['[dt.to_datetime().date() for dt in df.dates]\n'];['[dt.to_datetime().date() for dt in df.dates]\n'];False;['import pandas as pd\ndf = pd.DataFrame()\n[dt.to_datetime().date() for dt in df.dates]\n'];True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'dates'""]";['KeyError']
305;305;305;305;2.0;1;16202711;;1;14;<python><pandas>;Adding two pandas.series objects;6014.0;"[""    a = Series([35000,71000,16000,5000],index=['Ohio','Texas','Oregon','Utah'])\n    b = Series([NaN,71000,16000,35000],index=['California', 'Texas', 'Oregon', 'Ohio'])\n    In [63]: a\n    Out[63]: Ohio          35000\n             Texas         71000\n             Oregon        16000\n             Utah           5000\n    In [64]: b\n    Out[64]: California      NaN\n             Texas         71000\n             Oregon        16000\n             Ohio          35000\n    In [65]: a+b\n    Out[65]: California       NaN\n             Ohio           70000\n             Oregon         32000\n             Texas         142000\n             Utah             NaN\n    In [92]: # fill NaN with zero\n             b = b.fillna(0)\n             b\n    Out[92]: California        0\n             Texas         71000\n             Oregon        16000\n             Ohio          35000\n\n    In [93]: a\n    Out[93]: Ohio      35000\n             Texas     71000\n             Oregon    16000\n             Utah       5000\n\n    In [94]: # a is still good\n             a+b\n    Out[94]: California       NaN\n             Ohio           70000\n             Oregon         32000\n             Texas         142000 \n             Utah             NaN\nIn [95]: a.add(b, fill_value=0)\nOut[95]: California         0\n         Ohio           70000\n         Oregon         32000\n         Texas         142000\n         Utah            5000\n""]";"[""    a = Series([35000,71000,16000,5000],index=['Ohio','Texas','Oregon','Utah'])\n    b = Series([NaN,71000,16000,35000],index=['California', 'Texas', 'Oregon', 'Ohio'])\n"", '    In [63]: a\n    Out[63]: Ohio          35000\n             Texas         71000\n             Oregon        16000\n             Utah           5000\n    In [64]: b\n    Out[64]: California      NaN\n             Texas         71000\n             Oregon        16000\n             Ohio          35000\n', '    In [65]: a+b\n    Out[65]: California       NaN\n             Ohio           70000\n             Oregon         32000\n             Texas         142000\n             Utah             NaN\n', '    In [92]: # fill NaN with zero\n             b = b.fillna(0)\n             b\n    Out[92]: California        0\n             Texas         71000\n             Oregon        16000\n             Ohio          35000\n\n    In [93]: a\n    Out[93]: Ohio      35000\n             Texas     71000\n             Oregon    16000\n             Utah       5000\n\n    In [94]: # a is still good\n             a+b\n    Out[94]: California       NaN\n             Ohio           70000\n             Oregon         32000\n             Texas         142000 \n             Utah             NaN\n', 'In [95]: a.add(b, fill_value=0)\nOut[95]: California         0\n         Ohio           70000\n         Oregon         32000\n         Texas         142000\n         Utah            5000\n']";"[""    a = Series([35000,71000,16000,5000],index=['Ohio','Texas','Oregon','Utah'])\n    b = Series([NaN,71000,16000,35000],index=['California', 'Texas', 'Oregon', 'Ohio'])\n"", '    In [63]: a\n    Out[63]: Ohio          35000\n             Texas         71000\n             Oregon        16000\n             Utah           5000\n    In [64]: b\n    Out[64]: California      NaN\n             Texas         71000\n             Oregon        16000\n             Ohio          35000\n', '    In [65]: a+b\n    Out[65]: California       NaN\n             Ohio           70000\n             Oregon         32000\n             Texas         142000\n             Utah             NaN\n', '    In [92]: # fill NaN with zero\n             b = b.fillna(0)\n             b\n    Out[92]: California        0\n             Texas         71000\n             Oregon        16000\n             Ohio          35000\n\n    In [93]: a\n    Out[93]: Ohio      35000\n             Texas     71000\n             Oregon    16000\n             Utah       5000\n\n    In [94]: # a is still good\n             a+b\n    Out[94]: California       NaN\n             Ohio           70000\n             Oregon         32000\n             Texas         142000 \n             Utah             NaN\n', 'In [95]: a.add(b, fill_value=0)\nOut[95]: California         0\n         Ohio           70000\n         Oregon         32000\n         Texas         142000\n         Utah            5000\n']";['a\n    # fill NaN with zero\n    # a is still good\n'];['a\n    # fill NaN with zero\n    # a is still good\n'];False;['import pandas as pd\na\n    # fill NaN with zero\n    # a is still good\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
306;306;306;306;5.0;4;16236684;;1;75;<merge><pandas><multiple-columns><return-type>;Apply pandas function to column to create multiple new columns?;33681.0;[''];[];['extract_text_features', 'df.ix[: ,10:16] = df.textcol.map(extract_text_features)', 'df.iterrows()', 'df.iterrows()', '.map(lambda ...)'];[''];[''];False;['import pandas as pd\n'];False;2;3;"[""name 'pd' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess'];2;3;"[""name 'np' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess'];2;3;"[""name 'np' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess']
307;307;307;307;8.0;1;16249736;;1;38;<python><mongodb><pandas><pymongo>;How to import data from mongodb to pandas?;24496.0;"['{\n""_cls"" : ""SensorReport"",\n""_id"" : ObjectId(""515a963b78f6a035d9fa531b""),\n""_types"" : [\n    ""SensorReport""\n],\n""Readings"" : [\n    {\n        ""a"" : 0.958069536790466,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:26:35.297Z""),\n        ""b"" : 6.296118156595,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.95574014778624,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:27:09.963Z""),\n        ""b"" : 6.29651468650064,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.953648289182713,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:27:37.545Z""),\n        ""b"" : 7.29679823731148,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.955931884300997,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:28:21.369Z""),\n        ""b"" : 6.29642922525632,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.95821381,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:41:20.801Z""),\n        ""b"" : 7.28956613,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 4.95821335,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:41:36.931Z""),\n        ""b"" : 6.28956574,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 9.95821341,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:42:09.971Z""),\n        ""b"" : 0.28956488,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 1.95667927,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:43:55.463Z""),\n        ""b"" : 0.29115237,\n        ""_cls"" : ""Reading""\n    }\n],\n""latestReportTime"" : ISODate(""2013-04-02T08:43:55.463Z""),\n""sensorName"" : ""56847890-0"",\n""reportCount"" : 8\n}\n']";"['{\n""_cls"" : ""SensorReport"",\n""_id"" : ObjectId(""515a963b78f6a035d9fa531b""),\n""_types"" : [\n    ""SensorReport""\n],\n""Readings"" : [\n    {\n        ""a"" : 0.958069536790466,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:26:35.297Z""),\n        ""b"" : 6.296118156595,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.95574014778624,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:27:09.963Z""),\n        ""b"" : 6.29651468650064,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.953648289182713,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:27:37.545Z""),\n        ""b"" : 7.29679823731148,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.955931884300997,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:28:21.369Z""),\n        ""b"" : 6.29642922525632,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.95821381,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:41:20.801Z""),\n        ""b"" : 7.28956613,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 4.95821335,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:41:36.931Z""),\n        ""b"" : 6.28956574,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 9.95821341,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:42:09.971Z""),\n        ""b"" : 0.28956488,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 1.95667927,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:43:55.463Z""),\n        ""b"" : 0.29115237,\n        ""_cls"" : ""Reading""\n    }\n],\n""latestReportTime"" : ISODate(""2013-04-02T08:43:55.463Z""),\n""sensorName"" : ""56847890-0"",\n""reportCount"" : 8\n}\n']";"['{\n""_cls"" : ""SensorReport"",\n""_id"" : ObjectId(""515a963b78f6a035d9fa531b""),\n""_types"" : [\n    ""SensorReport""\n],\n""Readings"" : [\n    {\n        ""a"" : 0.958069536790466,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:26:35.297Z""),\n        ""b"" : 6.296118156595,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.95574014778624,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:27:09.963Z""),\n        ""b"" : 6.29651468650064,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.953648289182713,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:27:37.545Z""),\n        ""b"" : 7.29679823731148,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.955931884300997,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:28:21.369Z""),\n        ""b"" : 6.29642922525632,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 0.95821381,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:41:20.801Z""),\n        ""b"" : 7.28956613,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 4.95821335,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:41:36.931Z""),\n        ""b"" : 6.28956574,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 9.95821341,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:42:09.971Z""),\n        ""b"" : 0.28956488,\n        ""_cls"" : ""Reading""\n    },\n    {\n        ""a"" : 1.95667927,\n        ""_types"" : [\n            ""Reading""\n        ],\n        ""ReadingUpdatedDate"" : ISODate(""2013-04-02T08:43:55.463Z""),\n        ""b"" : 0.29115237,\n        ""_cls"" : ""Reading""\n    }\n],\n""latestReportTime"" : ISODate(""2013-04-02T08:43:55.463Z""),\n""sensorName"" : ""56847890-0"",\n""reportCount"" : 8\n}\n']";[''];[''];False;['import pandas as pd\n'];False;1;3;"[""No module named 'pymongo'"", 'Sucess', ""No module named 'pymongo'""]";['ImportError', 'Sucess', 'ImportError'];1;3;"[""No module named 'pymongo'"", 'Sucess', ""No module named 'pymongo'""]";['ImportError', 'Sucess', 'ImportError'];1;3;"[""No module named 'pymongo'"", 'Sucess', ""No module named 'pymongo'""]";['ImportError', 'Sucess', 'ImportError']
308;308;308;308;3.0;2;16266019;;1;24;<python><date><pandas>;Python Pandas: Group datetime column into hour and minute aggregations;20761.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'pd' is not defined"", ""name 'data' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'data' is not defined"", ""name 'data' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""'DataFrame' object has no attribute 'timestamp_col'"", ""'DataFrame' object has no attribute 'datetime_col'"", ""'DataFrame' object has no attribute 'datetime_col'""]";['AttributeError', 'AttributeError', 'AttributeError']
309;309;309;309;1.0;1;16311793;;1;11;<python><pandas>;Why does pandas groupby().transform() require a unique index?;10302.0;"[""df = pd.DataFrame([[1,1],\n                  [1,2],\n                  [2,3],\n                  [3,4],\n                  [3,5]], \n                  columns='a b'.split())\ndf['partials'] = df.groupby('a')['b'].transform(np.cumsum)\ndf\n     a   b   partials\n0    1   1   1\n1    1   2   3\n2    2   3   3\n3    3   4   4\n4    3   5   9\ndf = df.set_index('a')\ndf['partials'] = df.groupby(level=0)['b'].transform(np.cumsum)\ndf\n\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n<ipython-input-146-d0c35a4ba053> in <module>()\n      3 \n      4 df = df.set_index('a')\n----> 5 df.groupby(level=0)['b'].transform(np.cumsum)\n\n/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/groupby.pyc in transform(self, func, *args, **kwargs)\n   1542             res = wrapper(group)\n   1543             # result[group.index] = res\n-> 1544             indexer = self.obj.index.get_indexer(group.index)\n   1545             np.put(result, indexer, res)\n   1546 \n\n/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/index.pyc in get_indexer(self, target, method, limit)\n    847 \n    848         if not self.is_unique:\n--> 849             raise Exception('Reindexing only valid with uniquely valued Index '\n    850                             'objects')\n    851 \n\nException: Reindexing only valid with uniquely valued Index objects\ndf['b'].groupby(level=0).transform(np.cumsum)\ndf.groupby(level=0).transform(np.cumsum)\ndf.groupby(level=0)[['b']].transform(np.cumsum)\n""]";"[""df = pd.DataFrame([[1,1],\n                  [1,2],\n                  [2,3],\n                  [3,4],\n                  [3,5]], \n                  columns='a b'.split())\ndf['partials'] = df.groupby('a')['b'].transform(np.cumsum)\ndf\n"", '     a   b   partials\n0    1   1   1\n1    1   2   3\n2    2   3   3\n3    3   4   4\n4    3   5   9\n', ""df = df.set_index('a')\ndf['partials'] = df.groupby(level=0)['b'].transform(np.cumsum)\ndf\n\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n<ipython-input-146-d0c35a4ba053> in <module>()\n      3 \n      4 df = df.set_index('a')\n----> 5 df.groupby(level=0)['b'].transform(np.cumsum)\n\n/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/groupby.pyc in transform(self, func, *args, **kwargs)\n   1542             res = wrapper(group)\n   1543             # result[group.index] = res\n-> 1544             indexer = self.obj.index.get_indexer(group.index)\n   1545             np.put(result, indexer, res)\n   1546 \n\n/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/index.pyc in get_indexer(self, target, method, limit)\n    847 \n    848         if not self.is_unique:\n--> 849             raise Exception('Reindexing only valid with uniquely valued Index '\n    850                             'objects')\n    851 \n\nException: Reindexing only valid with uniquely valued Index objects\n"", ""df['b'].groupby(level=0).transform(np.cumsum)\n"", 'df.groupby(level=0).transform(np.cumsum)\n', ""df.groupby(level=0)[['b']].transform(np.cumsum)\n""]";"[""df = pd.DataFrame([[1,1],\n                  [1,2],\n                  [2,3],\n                  [3,4],\n                  [3,5]], \n                  columns='a b'.split())\ndf['partials'] = df.groupby('a')['b'].transform(np.cumsum)\ndf\n"", '     a   b   partials\n0    1   1   1\n1    1   2   3\n2    2   3   3\n3    3   4   4\n4    3   5   9\n', ""df = df.set_index('a')\ndf['partials'] = df.groupby(level=0)['b'].transform(np.cumsum)\ndf\n\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n<ipython-input-146-d0c35a4ba053> in <module>()\n      3 \n      4 df = df.set_index('a')\n----> 5 df.groupby(level=0)['b'].transform(np.cumsum)\n\n/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/groupby.pyc in transform(self, func, *args, **kwargs)\n   1542             res = wrapper(group)\n   1543             # result[group.index] = res\n-> 1544             indexer = self.obj.index.get_indexer(group.index)\n   1545             np.put(result, indexer, res)\n   1546 \n\n/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/index.pyc in get_indexer(self, target, method, limit)\n    847 \n    848         if not self.is_unique:\n--> 849             raise Exception('Reindexing only valid with uniquely valued Index '\n    850                             'objects')\n    851 \n\nException: Reindexing only valid with uniquely valued Index objects\n"", ""df['b'].groupby(level=0).transform(np.cumsum)\n"", 'df.groupby(level=0).transform(np.cumsum)\n', ""df.groupby(level=0)[['b']].transform(np.cumsum)\n""]";"[""df['partials'] = df.groupby('a')['b'].transform(np.cumsum)\ndf\ndf = df.set_index('a')\ndf['partials'] = df.groupby(level=0)['b'].transform(np.cumsum)\ndf\n\n\n\n\ndf['b'].groupby(level=0).transform(np.cumsum)\ndf.groupby(level=0).transform(np.cumsum)\ndf.groupby(level=0)[['b']].transform(np.cumsum)\n""]";"[""df['partials'] = df.groupby('a')['b'].transform(np.cumsum)\ndf\ndf = df.set_index('a')\ndf['partials'] = df.groupby(level=0)['b'].transform(np.cumsum)\ndf\n\n\n\n\ndf['b'].groupby(level=0).transform(np.cumsum)\ndf.groupby(level=0).transform(np.cumsum)\ndf.groupby(level=0)[['b']].transform(np.cumsum)\n""]";False;"[""import pandas as pd\ndf['partials'] = df.groupby('a')['b'].transform(np.cumsum)\ndf\ndf = df.set_index('a')\ndf['partials'] = df.groupby(level=0)['b'].transform(np.cumsum)\ndf\n\n\n\n\ndf['b'].groupby(level=0).transform(np.cumsum)\ndf.groupby(level=0).transform(np.cumsum)\ndf.groupby(level=0)[['b']].transform(np.cumsum)\n""]";False;0;0;[];[];0;0;[];[];0;0;[];[]
310;310;310;310;5.0;1;16327055;;1;73;<python><pandas>;How to add an empty column to a dataframe?;65645.0;"[""df['foo'] = df.apply(lambda _: '', axis=1)\n""]";"[""df['foo'] = df.apply(lambda _: '', axis=1)\n""]";"['DataFrame', ""df['foo'] = df.apply(lambda _: '', axis=1)\n""]";"[""df['foo'] = df.apply(lambda _: '', axis=1)\n""]";"[""df['foo'] = df.apply(lambda _: '', axis=1)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf['foo'] = df.apply(lambda _: '', axis=1)\n""]";True;1;2;"['Sucess', ""name 'mydf' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'mydf' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'mydf' is not defined""]";['Sucess', 'NameError']
311;311;311;311;1.0;0;16345583;;1;11;<python><pandas><nan><missing-data><data-cleansing>;Fill in missing pandas data with previous non-missing value, grouped by key;4190.0;['   id    x\n0   1   10\n1   1   20\n2   2  100\n3   2  200\n4   1  NaN\n5   2  NaN\n6   1  300\n7   1  NaN\n   id    x\n0   1   10\n1   1   20\n2   2  100\n3   2  200\n4   1   20\n5   2  200\n6   1  300\n7   1  300\n'];['   id    x\n0   1   10\n1   1   20\n2   2  100\n3   2  200\n4   1  NaN\n5   2  NaN\n6   1  300\n7   1  NaN\n', '   id    x\n0   1   10\n1   1   20\n2   2  100\n3   2  200\n4   1   20\n5   2  200\n6   1  300\n7   1  300\n'];['   id    x\n0   1   10\n1   1   20\n2   2  100\n3   2  200\n4   1  NaN\n5   2  NaN\n6   1  300\n7   1  NaN\n', '   id    x\n0   1   10\n1   1   20\n2   2  100\n3   2  200\n4   1   20\n5   2  200\n6   1  300\n7   1  300\n'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
312;312;312;312;3.0;0;16349389;;1;15;<python-2.7><pandas>;Grouping data by value ranges;7593.0;"["">56\n>35 and <= 56\n>14 and <= 35\n>0 and <=14\n{'Red':'>56,'Amber':'>35 and <= 56','Yellow':'>14 and <= 35','White':'>0 and <=14'}\n        Red  Amber  Yellow  White\nSTRSUB  56   60     74      40\nBOTDWG  20   67     87      34\n""]";"['>56\n>35 and <= 56\n>14 and <= 35\n>0 and <=14\n', ""{'Red':'>56,'Amber':'>35 and <= 56','Yellow':'>14 and <= 35','White':'>0 and <=14'}\n"", '        Red  Amber  Yellow  White\nSTRSUB  56   60     74      40\nBOTDWG  20   67     87      34\n']";"['>56\n>35 and <= 56\n>14 and <= 35\n>0 and <=14\n', ""{'Red':'>56,'Amber':'>35 and <= 56','Yellow':'>14 and <= 35','White':'>0 and <=14'}\n"", '        Red  Amber  Yellow  White\nSTRSUB  56   60     74      40\nBOTDWG  20   67     87      34\n']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError']
313;313;313;313;2.0;0;16353729;;1;84;<python><python-2.7><pandas><dataframe><apply>;Pandas: How to use apply function to multiple columns;136478.0;"['df = DataFrame ({\'a\' : np.random.randn(6),\n             \'b\' : [\'foo\', \'bar\'] * 3,\n             \'c\' : np.random.randn(6)})\ndef my_test(a, b):\n    return a % b\ndf[\'Value\'] = df.apply(lambda row: my_test(row[a], row[c]), axis=1)\nNameError: (""global name \'a\' is not defined"", u\'occurred at index 0\')\ndef my_test(a):\n    cum_diff = 0\n    for ix in df.index():\n        cum_diff = cum_diff + (a - df[\'a\'][ix])\n    return cum_diff \n']";"[""df = DataFrame ({'a' : np.random.randn(6),\n             'b' : ['foo', 'bar'] * 3,\n             'c' : np.random.randn(6)})\n"", 'def my_test(a, b):\n    return a % b\n', ""df['Value'] = df.apply(lambda row: my_test(row[a], row[c]), axis=1)\n"", 'NameError: (""global name \'a\' is not defined"", u\'occurred at index 0\')\n', ""def my_test(a):\n    cum_diff = 0\n    for ix in df.index():\n        cum_diff = cum_diff + (a - df['a'][ix])\n    return cum_diff \n""]";"[""df = DataFrame ({'a' : np.random.randn(6),\n             'b' : ['foo', 'bar'] * 3,\n             'c' : np.random.randn(6)})\n"", 'def my_test(a, b):\n    return a % b\n', ""df['Value'] = df.apply(lambda row: my_test(row[a], row[c]), axis=1)\n"", 'NameError: (""global name \'a\' is not defined"", u\'occurred at index 0\')\n', ""def my_test(a):\n    cum_diff = 0\n    for ix in df.index():\n        cum_diff = cum_diff + (a - df['a'][ix])\n    return cum_diff \n""]";"[""df['Value'] = df.apply(lambda row: my_test(row[a], row[c]), axis=1)\n""]";"[""df['Value'] = df.apply(lambda row: my_test(row[a], row[c]), axis=1)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf['Value'] = df.apply(lambda row: my_test(row[a], row[c]), axis=1)\n""]";True;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'a'"", 'Wrong number of items passed 0, placement implies 1']";['KeyError', 'ValueError']
314;314;314;314;2.0;0;16376159;;1;12;<python><python-2.7><numpy><scipy><pandas>;Plotting a Pandas DataSeries.GroupBy;27727.0;"[""data[40:76].groupby('ModelID').plot()\n""]";"[""data[40:76].groupby('ModelID').plot()\n""]";"['DataFrame', 'DataFrame', 'ModelID', 'saledate', 'MeanToDate', ""data[40:76].groupby('ModelID').plot()\n""]";"[""data[40:76].groupby('ModelID').plot()\n""]";"[""data[40:76].groupby('ModelID').plot()\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\ndata[40:76].groupby('ModelID').plot()\n""]";True;0;2;"[""No module named 'matplotlib'"", ""name 'df' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'matplotlib'"", ""name 'df' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'matplotlib'"", ""'model'""]";['ImportError', 'KeyError']
315;315;315;315;4.0;0;16392921;;1;47;<python><pandas><ipython><ipython-notebook>;Make more than one chart in same IPython Notebook cell;38376.0;"[""ipython notebook --pylab inline\ndf['korisnika'].plot()\ndf['osiguranika'].plot()\n""]";"['ipython notebook --pylab inline\n', ""df['korisnika'].plot()\ndf['osiguranika'].plot()\n""]";"['ipython notebook --pylab inline\n', ""df['korisnika'].plot()\ndf['osiguranika'].plot()\n""]";"[""df['korisnika'].plot()\ndf['osiguranika'].plot()\n""]";"[""df['korisnika'].plot()\ndf['osiguranika'].plot()\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf['korisnika'].plot()\ndf['osiguranika'].plot()\n""]";True;1;2;"[""name 'plt' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'plt' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'plt' is not defined"", 'Sucess']";['NameError', 'Sucess']
316;316;316;316;5.0;0;16396903;;1;58;<pandas>;Delete the first three rows of a dataframe in pandas;65041.0;[''];[];['df.ix[:-1]'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df1' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df1' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""name 'label' is not defined""]";['Sucess', 'NameError']
317;317;317;317;4.0;3;16412099;;1;20;<python><python-2.7><numpy><scipy><pandas>;Parse a Pandas column to Datetime;29568.0;"['import dateutil\n\ndf[\'date\'] = dateutil.parser.parse(df[\'date\'])\nAttributeError                            Traceback (most recent call last)\n<ipython-input-636-9b19aa5f989c> in <module>()\n     15 \n     16 # Parse \'Date\' Column to Datetime\n---> 17 df[\'date\'] = dateutil.parser.parse(df[\'date\'])\n     18 \n     19 # SELECT RECENT SALES\n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in parse(timestr, parserinfo, **kwargs)\n    695         return parser(parserinfo).parse(timestr, **kwargs)\n    696     else:\n--> 697         return DEFAULTPARSER.parse(timestr, **kwargs)\n    698 \n    699 \n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in parse(self, timestr, default, ignoretz, tzinfos, **kwargs)\n    299             default = datetime.datetime.now().replace(hour=0, minute=0,\n    300                                                       second=0, microsecond=0)\n--> 301         res = self._parse(timestr, **kwargs)\n    302         if res is None:\n    303             raise ValueError, ""unknown string format""\n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in _parse(self, timestr, dayfirst, yearfirst, fuzzy)\n    347             yearfirst = info.yearfirst\n    348         res = self._result()\n--> 349         l = _timelex.split(timestr)\n    350         try:\n    351 \n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in split(cls, s)\n    141 \n    142     def split(cls, s):\n--> 143         return list(cls(s))\n    144     split = classmethod(split)\n    145 \n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in next(self)\n    135 \n    136     def next(self):\n--> 137         token = self.get_token()\n    138         if token is None:\n    139             raise StopIteration\n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in get_token(self)\n     66                 nextchar = self.charstack.pop(0)\n     67             else:\n---> 68                 nextchar = self.instream.read(1)\n     69                 while nextchar == \'\\x00\':\n     70                     nextchar = self.instream.read(1)\n\nAttributeError: \'Series\' object has no attribute \'read\'\n']";"[""import dateutil\n\ndf['date'] = dateutil.parser.parse(df['date'])\n"", 'AttributeError                            Traceback (most recent call last)\n<ipython-input-636-9b19aa5f989c> in <module>()\n     15 \n     16 # Parse \'Date\' Column to Datetime\n---> 17 df[\'date\'] = dateutil.parser.parse(df[\'date\'])\n     18 \n     19 # SELECT RECENT SALES\n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in parse(timestr, parserinfo, **kwargs)\n    695         return parser(parserinfo).parse(timestr, **kwargs)\n    696     else:\n--> 697         return DEFAULTPARSER.parse(timestr, **kwargs)\n    698 \n    699 \n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in parse(self, timestr, default, ignoretz, tzinfos, **kwargs)\n    299             default = datetime.datetime.now().replace(hour=0, minute=0,\n    300                                                       second=0, microsecond=0)\n--> 301         res = self._parse(timestr, **kwargs)\n    302         if res is None:\n    303             raise ValueError, ""unknown string format""\n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in _parse(self, timestr, dayfirst, yearfirst, fuzzy)\n    347             yearfirst = info.yearfirst\n    348         res = self._result()\n--> 349         l = _timelex.split(timestr)\n    350         try:\n    351 \n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in split(cls, s)\n    141 \n    142     def split(cls, s):\n--> 143         return list(cls(s))\n    144     split = classmethod(split)\n    145 \n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in next(self)\n    135 \n    136     def next(self):\n--> 137         token = self.get_token()\n    138         if token is None:\n    139             raise StopIteration\n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in get_token(self)\n     66                 nextchar = self.charstack.pop(0)\n     67             else:\n---> 68                 nextchar = self.instream.read(1)\n     69                 while nextchar == \'\\x00\':\n     70                     nextchar = self.instream.read(1)\n\nAttributeError: \'Series\' object has no attribute \'read\'\n']";"['DataFrame', 'date', 'DateTime', 'sql.read_frame()', 'date', '2013-04-04', '2013-04-01', '2013-04-04', ""'Series' object has no attribute 'read'"", ""import dateutil\n\ndf['date'] = dateutil.parser.parse(df['date'])\n"", 'AttributeError                            Traceback (most recent call last)\n<ipython-input-636-9b19aa5f989c> in <module>()\n     15 \n     16 # Parse \'Date\' Column to Datetime\n---> 17 df[\'date\'] = dateutil.parser.parse(df[\'date\'])\n     18 \n     19 # SELECT RECENT SALES\n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in parse(timestr, parserinfo, **kwargs)\n    695         return parser(parserinfo).parse(timestr, **kwargs)\n    696     else:\n--> 697         return DEFAULTPARSER.parse(timestr, **kwargs)\n    698 \n    699 \n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in parse(self, timestr, default, ignoretz, tzinfos, **kwargs)\n    299             default = datetime.datetime.now().replace(hour=0, minute=0,\n    300                                                       second=0, microsecond=0)\n--> 301         res = self._parse(timestr, **kwargs)\n    302         if res is None:\n    303             raise ValueError, ""unknown string format""\n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in _parse(self, timestr, dayfirst, yearfirst, fuzzy)\n    347             yearfirst = info.yearfirst\n    348         res = self._result()\n--> 349         l = _timelex.split(timestr)\n    350         try:\n    351 \n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in split(cls, s)\n    141 \n    142     def split(cls, s):\n--> 143         return list(cls(s))\n    144     split = classmethod(split)\n    145 \n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in next(self)\n    135 \n    136     def next(self):\n--> 137         token = self.get_token()\n    138         if token is None:\n    139             raise StopIteration\n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in get_token(self)\n     66                 nextchar = self.charstack.pop(0)\n     67             else:\n---> 68                 nextchar = self.instream.read(1)\n     69                 while nextchar == \'\\x00\':\n     70                     nextchar = self.instream.read(1)\n\nAttributeError: \'Series\' object has no attribute \'read\'\n', ""df['date'].apply(dateutil.parser.parse)"", ""AttributeError: 'datetime.date' object has no attribute 'read'"", ""df['date'].truncate(after='2013/04/01')"", ""TypeError: can't compare datetime.datetime to long"", ""df['date'].dtype"", ""dtype('O')"", 'datetime']";"[""import dateutil\n\ndf['date'] = dateutil.parser.parse(df['date'])\n\n\n\n\n\n\n\n""]";"[""import dateutil\n\ndf['date'] = dateutil.parser.parse(df['date'])\n\n\n\n\n\n\n\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport dateutil\n\ndf['date'] = dateutil.parser.parse(df['date'])\n\n\n\n\n\n\n\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'date'""]";['KeyError']
318;318;318;318;3.0;8;16424493;;1;44;<python><formatting><pandas><ipython-notebook>;Pandas: Setting no. of max rows;28777.0;"[""n = 100\nfoo = DataFrame(index=range(n))\nfoo['floats'] = np.random.randn(n)\nfoo\npd.set_option('display.max_rows', 500)\n""]";"[""n = 100\nfoo = DataFrame(index=range(n))\nfoo['floats'] = np.random.randn(n)\nfoo\n"", ""pd.set_option('display.max_rows', 500)\n""]";"['DataFrame', ""n = 100\nfoo = DataFrame(index=range(n))\nfoo['floats'] = np.random.randn(n)\nfoo\n"", ""pd.set_option('display.max_rows', 500)\n""]";"[""n = 100\nfoo = DataFrame(index=range(n))\nfoo['floats'] = np.random.randn(n)\nfoo\npd.set_option('display.max_rows', 500)\n""]";"[""from pandas import DataFrame\nimport pandas as pd\nn = 100\nfoo = DataFrame(index=range(n))\nfoo['floats'] = np.random.randn(n)\nfoo\npd.set_option('display.max_rows', 500)\n""]";True;"[""import pandas as pd\nn = 100\nfoo = DataFrame(index=range(n))\nfoo['floats'] = np.random.randn(n)\nfoo\npd.set_option('display.max_rows', 500)\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
319;319;319;319;3.0;0;16476413;;1;13;<python><mysql><pandas><mysql-python>;How to insert pandas dataframe via mysqldb into database?;33749.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'con' is not defined""]";['NameError']
320;320;320;320;8.0;2;16476924;;1;379;<python><pandas><rows><dataframe>;How to iterate over rows in a DataFrame in Pandas?;384371.0;"[""import pandas as pd\ninp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]\ndf = pd.DataFrame(inp)\nprint df\n   c1   c2\n0  10  100\n1  11  110\n2  12  120\nfor row in df.rows:\n   print row['c1'], row['c2']\nfor date, row in df.T.iteritems():\nfor row in df.iterrows():\n""]";"[""import pandas as pd\ninp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]\ndf = pd.DataFrame(inp)\nprint df\n"", '   c1   c2\n0  10  100\n1  11  110\n2  12  120\n', ""for row in df.rows:\n   print row['c1'], row['c2']\n"", 'for date, row in df.T.iteritems():\n', 'for row in df.iterrows():\n']";"[""import pandas as pd\ninp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]\ndf = pd.DataFrame(inp)\nprint df\n"", '   c1   c2\n0  10  100\n1  11  110\n2  12  120\n', ""for row in df.rows:\n   print row['c1'], row['c2']\n"", 'for date, row in df.T.iteritems():\n', 'for row in df.iterrows():\n', 'row']";"[""import pandas as pd\ninp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]\ndf = pd.DataFrame(inp)\n""]";"[""import pandas as pd\ninp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]\ndf = pd.DataFrame(inp)\n""]";False;"[""import pandas as pd\nimport pandas as pd\ninp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]\ndf = pd.DataFrame(inp)\n""]";False;2;5;"['Sucess', ""name 'df' is not defined"", ""name 'pd' is not defined"", 'Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError', 'Sucess', 'NameError'];2;5;"['Sucess', ""name 'df' is not defined"", ""name 'randn' is not defined"", 'Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError', 'Sucess', 'NameError'];2;5;"['Sucess', 'Wrong number of items passed 0, placement implies 1', ""name 'randn' is not defined"", 'Sucess', ""name 'df' is not defined""]";['Sucess', 'ValueError', 'NameError', 'Sucess', 'NameError']
321;321;321;321;2.0;2;16522380;;1;52;<python><matplotlib><pandas>;Matplotlib plot is a no-show;30493.0;"[""import pandas as pd\nimport numpy as np\ndef add_prop(group):\n    births = group.births.astype(float)\n    group['prop'] = births/births.sum()\n    return group\n\npieces = []\ncolumns = ['name', 'sex', 'births']\n\nfor year in range(1880, 2012):\n    path = 'yob%d.txt' % year\n    frame = pd.read_csv(path, names = columns)\n    frame['year'] = year\n    pieces.append(frame)\n    names = pd.concat(pieces, ignore_index = True)\n\ntotal_births = names.pivot_table('births', rows = 'year', cols = 'sex', aggfunc = sum)\ntotal_births.plot(title = 'Total Births by sex and year')\n""]";"[""import pandas as pd\nimport numpy as np\ndef add_prop(group):\n    births = group.births.astype(float)\n    group['prop'] = births/births.sum()\n    return group\n\npieces = []\ncolumns = ['name', 'sex', 'births']\n\nfor year in range(1880, 2012):\n    path = 'yob%d.txt' % year\n    frame = pd.read_csv(path, names = columns)\n    frame['year'] = year\n    pieces.append(frame)\n    names = pd.concat(pieces, ignore_index = True)\n\ntotal_births = names.pivot_table('births', rows = 'year', cols = 'sex', aggfunc = sum)\ntotal_births.plot(title = 'Total Births by sex and year')\n""]";"[""import pandas as pd\nimport numpy as np\ndef add_prop(group):\n    births = group.births.astype(float)\n    group['prop'] = births/births.sum()\n    return group\n\npieces = []\ncolumns = ['name', 'sex', 'births']\n\nfor year in range(1880, 2012):\n    path = 'yob%d.txt' % year\n    frame = pd.read_csv(path, names = columns)\n    frame['year'] = year\n    pieces.append(frame)\n    names = pd.concat(pieces, ignore_index = True)\n\ntotal_births = names.pivot_table('births', rows = 'year', cols = 'sex', aggfunc = sum)\ntotal_births.plot(title = 'Total Births by sex and year')\n""]";"[""import pandas as pd\nimport numpy as np\n\npieces = []\ncolumns = ['name', 'sex', 'births']\n\n\ntotal_births = names.pivot_table('births', rows = 'year', cols = 'sex', aggfunc = sum)\ntotal_births.plot(title = 'Total Births by sex and year')\n""]";"[""import pandas as pd\nimport numpy as np\n\npieces = []\ncolumns = ['name', 'sex', 'births']\n\n\ntotal_births = names.pivot_table('births', rows = 'year', cols = 'sex', aggfunc = sum)\ntotal_births.plot(title = 'Total Births by sex and year')\n""]";False;"[""import pandas as pd\nimport pandas as pd\nimport numpy as np\n\npieces = []\ncolumns = ['name', 'sex', 'births']\n\n\ntotal_births = names.pivot_table('births', rows = 'year', cols = 'sex', aggfunc = sum)\ntotal_births.plot(title = 'Total Births by sex and year')\n""]";False;1;2;"[""No module named 'matplotlib'"", 'Sucess']";['ImportError', 'Sucess'];1;2;"[""No module named 'matplotlib'"", 'Sucess']";['ImportError', 'Sucess'];1;2;"[""No module named 'matplotlib'"", 'Sucess']";['ImportError', 'Sucess']
322;322;322;322;1.0;0;16575868;;1;14;<python><pandas><dataframe>;Efficiently creating additional columns in a pandas DataFrame using .map();11896.0;"[""   abc1  abc2  abc3  xyz1  xyz2  xyz3\n0     1     2     2     2     1     2\n1     2     1     1     2     1     1\n2     2     2     1     2     2     2\n3     1     2     1     1     1     1\n4     1     1     2     1     2     1\nabc_columns = ['abc1', 'abc2', 'abc3']\nxyz_columns = ['xyz1', 'xyz2', 'xyz3']\nabc_category_columns = ['abc1_category', 'abc2_category', 'abc3_category']\ncategories = {1: 'Good', 2: 'Bad', 3: 'Ugly'}\n\nfor i in range(len(abc_category_columns)):\n    df3[abc_category_columns[i]] = df3[abc_columns[i]].map(categories)\n\nprint df3\n   abc1  abc2  abc3  xyz1  xyz2  xyz3 abc1_category abc2_category abc3_category\n0     1     2     2     2     1     2          Good           Bad           Bad\n1     2     1     1     2     1     1           Bad          Good          Good\n2     2     2     1     2     2     2           Bad           Bad          Good\n3     1     2     1     1     1     1          Good           Bad          Good\n4     1     1     2     1     2     1          Good          Good           Bad\n""]";"['   abc1  abc2  abc3  xyz1  xyz2  xyz3\n0     1     2     2     2     1     2\n1     2     1     1     2     1     1\n2     2     2     1     2     2     2\n3     1     2     1     1     1     1\n4     1     1     2     1     2     1\n', ""abc_columns = ['abc1', 'abc2', 'abc3']\nxyz_columns = ['xyz1', 'xyz2', 'xyz3']\nabc_category_columns = ['abc1_category', 'abc2_category', 'abc3_category']\ncategories = {1: 'Good', 2: 'Bad', 3: 'Ugly'}\n\nfor i in range(len(abc_category_columns)):\n    df3[abc_category_columns[i]] = df3[abc_columns[i]].map(categories)\n\nprint df3\n"", '   abc1  abc2  abc3  xyz1  xyz2  xyz3 abc1_category abc2_category abc3_category\n0     1     2     2     2     1     2          Good           Bad           Bad\n1     2     1     1     2     1     1           Bad          Good          Good\n2     2     2     1     2     2     2           Bad           Bad          Good\n3     1     2     1     1     1     1          Good           Bad          Good\n4     1     1     2     1     2     1          Good          Good           Bad\n']";"['   abc1  abc2  abc3  xyz1  xyz2  xyz3\n0     1     2     2     2     1     2\n1     2     1     1     2     1     1\n2     2     2     1     2     2     2\n3     1     2     1     1     1     1\n4     1     1     2     1     2     1\n', ""abc_columns = ['abc1', 'abc2', 'abc3']\nxyz_columns = ['xyz1', 'xyz2', 'xyz3']\nabc_category_columns = ['abc1_category', 'abc2_category', 'abc3_category']\ncategories = {1: 'Good', 2: 'Bad', 3: 'Ugly'}\n\nfor i in range(len(abc_category_columns)):\n    df3[abc_category_columns[i]] = df3[abc_columns[i]].map(categories)\n\nprint df3\n"", '   abc1  abc2  abc3  xyz1  xyz2  xyz3 abc1_category abc2_category abc3_category\n0     1     2     2     2     1     2          Good           Bad           Bad\n1     2     1     1     2     1     1           Bad          Good          Good\n2     2     2     1     2     2     2           Bad           Bad          Good\n3     1     2     1     1     1     1          Good           Bad          Good\n4     1     1     2     1     2     1          Good          Good           Bad\n', 'for', 'lambda']";"[""abc_columns = ['abc1', 'abc2', 'abc3']\nxyz_columns = ['xyz1', 'xyz2', 'xyz3']\nabc_category_columns = ['abc1_category', 'abc2_category', 'abc3_category']\ncategories = {1: 'Good', 2: 'Bad', 3: 'Ugly'}\n\n\n""]";"[""abc_columns = ['abc1', 'abc2', 'abc3']\nxyz_columns = ['xyz1', 'xyz2', 'xyz3']\nabc_category_columns = ['abc1_category', 'abc2_category', 'abc3_category']\ncategories = {1: 'Good', 2: 'Bad', 3: 'Ugly'}\n\n\n""]";False;"[""import pandas as pd\nabc_columns = ['abc1', 'abc2', 'abc3']\nxyz_columns = ['xyz1', 'xyz2', 'xyz3']\nabc_category_columns = ['abc1_category', 'abc2_category', 'abc3_category']\ncategories = {1: 'Good', 2: 'Bad', 3: 'Ugly'}\n\n\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'abc_columns' is not defined""]";['NameError']
323;323;323;323;3.0;1;16597265;;1;84;<python><pandas>;Appending to an empty data frame in Pandas?;97800.0;"[""df = pd.DataFrame()\ndata = ['some kind of data here' --> I have checked the type already, and it is a dataframe]\ndf.append(data)\nEmpty DataFrame\nColumns: []\nIndex: []\n""]";"[""df = pd.DataFrame()\ndata = ['some kind of data here' --> I have checked the type already, and it is a dataframe]\ndf.append(data)\n"", 'Empty DataFrame\nColumns: []\nIndex: []\n']";"[""df = pd.DataFrame()\ndata = ['some kind of data here' --> I have checked the type already, and it is a dataframe]\ndf.append(data)\n"", 'Empty DataFrame\nColumns: []\nIndex: []\n']";['df = pd.DataFrame()\ndf.append(data)\n'];['import pandas as pd\ndf = pd.DataFrame()\ndf.append(data)\n'];True;['import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\ndf.append(data)\n'];True;1;2;"['Sucess', ""name 'pd' is not defined""]";['Sucess', 'NameError'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
324;324;324;324;2.0;0;16616141;;1;15;<python><pandas>;Deleting all columns except a few python-pandas;8381.0;['    1  2  3  4  5  6 ..  n\nA   x  x  x  x  x  x ..  x\nB   x  x  x  x  x  x ..  x\nC   x  x  x  x  x  x ..  x\n'];['    1  2  3  4  5  6 ..  n\nA   x  x  x  x  x  x ..  x\nB   x  x  x  x  x  x ..  x\nC   x  x  x  x  x  x ..  x\n'];['    1  2  3  4  5  6 ..  n\nA   x  x  x  x  x  x ..  x\nB   x  x  x  x  x  x ..  x\nC   x  x  x  x  x  x ..  x\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
325;325;325;325;1.0;0;16626058;;1;18;<python><performance><indexing><pandas><binary-search>;What is the performance impact of non-unique indexes in pandas?;3430.0;"[""In [23]: import numpy as np\nIn [24]: import pandas as pd\nIn [25]: x = np.random.randint(0, 10**7, 10**7)\nIn [26]: df1 = pd.DataFrame({'x':x})\nIn [27]: df2 = df1.set_index('x', drop=False)\nIn [28]: %timeit df2.ix[0]\n1 loops, best of 3: 402 ms per loop\nIn [29]: %timeit df1.ix[0]\n10000 loops, best of 3: 123 us per loop\n""]";"[""In [23]: import numpy as np\nIn [24]: import pandas as pd\nIn [25]: x = np.random.randint(0, 10**7, 10**7)\nIn [26]: df1 = pd.DataFrame({'x':x})\nIn [27]: df2 = df1.set_index('x', drop=False)\nIn [28]: %timeit df2.ix[0]\n1 loops, best of 3: 402 ms per loop\nIn [29]: %timeit df1.ix[0]\n10000 loops, best of 3: 123 us per loop\n""]";"['ix', ""In [23]: import numpy as np\nIn [24]: import pandas as pd\nIn [25]: x = np.random.randint(0, 10**7, 10**7)\nIn [26]: df1 = pd.DataFrame({'x':x})\nIn [27]: df2 = df1.set_index('x', drop=False)\nIn [28]: %timeit df2.ix[0]\n1 loops, best of 3: 402 ms per loop\nIn [29]: %timeit df1.ix[0]\n10000 loops, best of 3: 123 us per loop\n"", 'ix', 'ix']";[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
326;326;326;326;2.0;2;16628329;;1;39;<python><sqlite><pandas><hdf5>;HDF5 - concurrency, compression & I/O performance;17032.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
327;327;327;327;4.0;4;16628819;;1;22;<python><pandas>;Convert pandas timezone-aware DateTimeIndex to naive timestamp, but in certain timezone;16527.0;"['In [82]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=10, freq=\'s\', tz=""Europe/Brussels"")\n\nIn [83]: t\nOut[83]: \n<class \'pandas.tseries.index.DatetimeIndex\'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: S, Timezone: Europe/Brussels\nIn [86]: t.tz = None\n\nIn [87]: t\nOut[87]: \n<class \'pandas.tseries.index.DatetimeIndex\'>\n[2013-05-18 10:00:00, ..., 2013-05-18 10:00:09]\nLength: 10, Freq: S, Timezone: None\nIn [119]: d = pd.Timestamp(""2013-05-18 12:00:00"", tz=""Europe/Brussels"")\n\nIn [120]: d\nOut[120]: <Timestamp: 2013-05-18 12:00:00+0200 CEST, tz=Europe/Brussels>\n\nIn [121]: d.replace(tzinfo=None)\nOut[121]: <Timestamp: 2013-05-18 12:00:00> \nIn [124]: t\nOut[124]: \n<class \'pandas.tseries.index.DatetimeIndex\'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: S, Timezone: Europe/Brussels\n\nIn [125]: pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])\nOut[125]: \n<class \'pandas.tseries.index.DatetimeIndex\'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: None, Timezone: None\n']";"['In [82]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=10, freq=\'s\', tz=""Europe/Brussels"")\n\nIn [83]: t\nOut[83]: \n<class \'pandas.tseries.index.DatetimeIndex\'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: S, Timezone: Europe/Brussels\n', ""In [86]: t.tz = None\n\nIn [87]: t\nOut[87]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-05-18 10:00:00, ..., 2013-05-18 10:00:09]\nLength: 10, Freq: S, Timezone: None\n"", 'In [119]: d = pd.Timestamp(""2013-05-18 12:00:00"", tz=""Europe/Brussels"")\n\nIn [120]: d\nOut[120]: <Timestamp: 2013-05-18 12:00:00+0200 CEST, tz=Europe/Brussels>\n\nIn [121]: d.replace(tzinfo=None)\nOut[121]: <Timestamp: 2013-05-18 12:00:00> \n', ""In [124]: t\nOut[124]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: S, Timezone: Europe/Brussels\n\nIn [125]: pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])\nOut[125]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: None, Timezone: None\n""]";"['tz_localize', 'In [82]: t = pd.date_range(start=""2013-05-18 12:00:00"", periods=10, freq=\'s\', tz=""Europe/Brussels"")\n\nIn [83]: t\nOut[83]: \n<class \'pandas.tseries.index.DatetimeIndex\'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: S, Timezone: Europe/Brussels\n', ""In [86]: t.tz = None\n\nIn [87]: t\nOut[87]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-05-18 10:00:00, ..., 2013-05-18 10:00:09]\nLength: 10, Freq: S, Timezone: None\n"", 'In [119]: d = pd.Timestamp(""2013-05-18 12:00:00"", tz=""Europe/Brussels"")\n\nIn [120]: d\nOut[120]: <Timestamp: 2013-05-18 12:00:00+0200 CEST, tz=Europe/Brussels>\n\nIn [121]: d.replace(tzinfo=None)\nOut[121]: <Timestamp: 2013-05-18 12:00:00> \n', ""In [124]: t\nOut[124]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: S, Timezone: Europe/Brussels\n\nIn [125]: pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])\nOut[125]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: None, Timezone: None\n""]";"['t = pd.date_range(start=""2013-05-18 12:00:00"", periods=10, freq=\'s\', tz=""Europe/Brussels"")\n\nt.tz = None\n\nd = pd.Timestamp(""2013-05-18 12:00:00"", tz=""Europe/Brussels"")\n\nd.replace(tzinfo=None)\nt\npd.DatetimeIndex([i.replace(tzinfo=None) for i in t])\n']";"['import pandas as pd\nt = pd.date_range(start=""2013-05-18 12:00:00"", periods=10, freq=\'s\', tz=""Europe/Brussels"")\n\nt.tz = None\n\nd = pd.Timestamp(""2013-05-18 12:00:00"", tz=""Europe/Brussels"")\n\nd.replace(tzinfo=None)\nt\npd.DatetimeIndex([i.replace(tzinfo=None) for i in t])\n']";True;"['import pandas as pd\nt = pd.date_range(start=""2013-05-18 12:00:00"", periods=10, freq=\'s\', tz=""Europe/Brussels"")\n\nt.tz = None\n\nd = pd.Timestamp(""2013-05-18 12:00:00"", tz=""Europe/Brussels"")\n\nd.replace(tzinfo=None)\nt\npd.DatetimeIndex([i.replace(tzinfo=None) for i in t])\n']";False;0;2;"[""name 'pd' is not defined"", ""name 't' is not defined""]";['NameError', 'NameError'];0;2;"[""name 't' is not defined"", ""name 't' is not defined""]";['NameError', 'NameError'];0;2;"[""name 't' is not defined"", ""name 't' is not defined""]";['NameError', 'NameError']
328;328;328;328;1.0;0;16637171;;1;11;<python><pandas><category><vectorization>;Pandas: reshaping data;1544.0;['14    [Yellow, Pizza, Restaurants]\n...\n160920                  [Automotive, Auto Parts & Supplies]\n160921       [Lighting Fixtures & Equipment, Home Services]\n160922                 [Food, Pizza, Candy Stores]\n160923           [Hair Removal, Nail Salons, Beauty & Spas]\n160924           [Hair Removal, Nail Salons, Beauty & Spas]\n      Yellow  Automotive  Pizza\n14       1         0        1\n\x85           \n160920   0         1        0\n160921   0         0        0\n160922   0         0        1\n160923   0         0        0\n160924   0         0        0\n'];['14    [Yellow, Pizza, Restaurants]\n...\n160920                  [Automotive, Auto Parts & Supplies]\n160921       [Lighting Fixtures & Equipment, Home Services]\n160922                 [Food, Pizza, Candy Stores]\n160923           [Hair Removal, Nail Salons, Beauty & Spas]\n160924           [Hair Removal, Nail Salons, Beauty & Spas]\n', '      Yellow  Automotive  Pizza\n14       1         0        1\n\x85           \n160920   0         1        0\n160921   0         0        0\n160922   0         0        1\n160923   0         0        0\n160924   0         0        0\n'];['14    [Yellow, Pizza, Restaurants]\n...\n160920                  [Automotive, Auto Parts & Supplies]\n160921       [Lighting Fixtures & Equipment, Home Services]\n160922                 [Food, Pizza, Candy Stores]\n160923           [Hair Removal, Nail Salons, Beauty & Spas]\n160924           [Hair Removal, Nail Salons, Beauty & Spas]\n', '      Yellow  Automotive  Pizza\n14       1         0        1\n\x85           \n160920   0         1        0\n160921   0         0        0\n160922   0         0        1\n160923   0         0        0\n160924   0         0        0\n'];['14    [Yellow, Pizza, Restaurants]\n...\n'];['14    [Yellow, Pizza, Restaurants]\n...\n'];False;['import pandas as pd\n14    [Yellow, Pizza, Restaurants]\n...\n'];False;0;1;"[""name 'Series' is not defined""]";['NameError'];0;1;"[""name 'Series' is not defined""]";['NameError'];0;1;"[""name 'Series' is not defined""]";['NameError']
329;329;329;329;2.0;0;16637271;;1;20;<python><io><pandas><hdf5><pytables>;Iteratively writing to HDF5 Stores in Pandas;12287.0;"[""In [1142]: store = HDFStore('store.h5')\n\nIn [1143]: index = date_range('1/1/2000', periods=8)\n\nIn [1144]: s = Series(randn(5), index=['a', 'b', 'c', 'd', 'e'])\n\nIn [1145]: df = DataFrame(randn(8, 3), index=index,\n   ......:                columns=['A', 'B', 'C'])\n   ......:\n\nIn [1146]: wp = Panel(randn(2, 5, 4), items=['Item1', 'Item2'],\n   ......:            major_axis=date_range('1/1/2000', periods=5),\n   ......:            minor_axis=['A', 'B', 'C', 'D'])\n   ......:\nIn [1147]: store['s'] = s\n\nIn [1148]: store['df'] = df\n\nIn [1149]: store['wp'] = wp\nIn [1150]: store\nOut[1150]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n/df            frame        (shape->[8,3])  \n/s             series       (shape->[5])    \n/wp            wide         (shape->[2,5,4])\nIn [1151]: store.close()\n""]";"[""In [1142]: store = HDFStore('store.h5')\n\nIn [1143]: index = date_range('1/1/2000', periods=8)\n\nIn [1144]: s = Series(randn(5), index=['a', 'b', 'c', 'd', 'e'])\n\nIn [1145]: df = DataFrame(randn(8, 3), index=index,\n   ......:                columns=['A', 'B', 'C'])\n   ......:\n\nIn [1146]: wp = Panel(randn(2, 5, 4), items=['Item1', 'Item2'],\n   ......:            major_axis=date_range('1/1/2000', periods=5),\n   ......:            minor_axis=['A', 'B', 'C', 'D'])\n   ......:\n"", ""In [1147]: store['s'] = s\n\nIn [1148]: store['df'] = df\n\nIn [1149]: store['wp'] = wp\n"", ""In [1150]: store\nOut[1150]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n/df            frame        (shape->[8,3])  \n/s             series       (shape->[5])    \n/wp            wide         (shape->[2,5,4])\n"", 'In [1151]: store.close()\n']";"['Series', 'DataFrames', 'Panels', ""In [1142]: store = HDFStore('store.h5')\n\nIn [1143]: index = date_range('1/1/2000', periods=8)\n\nIn [1144]: s = Series(randn(5), index=['a', 'b', 'c', 'd', 'e'])\n\nIn [1145]: df = DataFrame(randn(8, 3), index=index,\n   ......:                columns=['A', 'B', 'C'])\n   ......:\n\nIn [1146]: wp = Panel(randn(2, 5, 4), items=['Item1', 'Item2'],\n   ......:            major_axis=date_range('1/1/2000', periods=5),\n   ......:            minor_axis=['A', 'B', 'C', 'D'])\n   ......:\n"", ""In [1147]: store['s'] = s\n\nIn [1148]: store['df'] = df\n\nIn [1149]: store['wp'] = wp\n"", ""In [1150]: store\nOut[1150]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n/df            frame        (shape->[8,3])  \n/s             series       (shape->[5])    \n/wp            wide         (shape->[2,5,4])\n"", 'In [1151]: store.close()\n', '.csv', '.h5', '.h5']";"[""store = HDFStore('store.h5')\n\n\n\n\n\n\n""]";"[""store = HDFStore('store.h5')\n\n\n\n\n\n\n""]";False;"[""import pandas as pd\nstore = HDFStore('store.h5')\n\n\n\n\n\n\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""mode ['index>100'] is not allowed while performing a read. Allowed modes are r, r+ and a.""]";['ValueError'];0;1;"[""mode ['index>100'] is not allowed while performing a read. Allowed modes are r, r+ and a.""]";['ValueError']
330;330;330;330;1.0;5;16639877;;1;20;<python><pandas><hdf5><pytables>;HDF5 taking more space than CSV?;6173.0;"[""import string\nimport random\nimport pandas as pd\n\nmatrix = np.random.random((100, 3000))\nmy_cols = [random.choice(string.ascii_uppercase) for x in range(matrix.shape[1])]\nmydf = pd.DataFrame(matrix, columns=my_cols)\nmydf['something'] = 'hello_world'\nstore = pd.HDFStore('myfile.h5',complevel=9, complib='bzip2')\nstore['mydf'] = mydf\nstore.close()\nmydf.to_csv('myfile.csv', sep=':')\n""]";"[""import string\nimport random\nimport pandas as pd\n\nmatrix = np.random.random((100, 3000))\nmy_cols = [random.choice(string.ascii_uppercase) for x in range(matrix.shape[1])]\nmydf = pd.DataFrame(matrix, columns=my_cols)\nmydf['something'] = 'hello_world'\n"", ""store = pd.HDFStore('myfile.h5',complevel=9, complib='bzip2')\nstore['mydf'] = mydf\nstore.close()\n"", ""mydf.to_csv('myfile.csv', sep=':')\n""]";"[""import string\nimport random\nimport pandas as pd\n\nmatrix = np.random.random((100, 3000))\nmy_cols = [random.choice(string.ascii_uppercase) for x in range(matrix.shape[1])]\nmydf = pd.DataFrame(matrix, columns=my_cols)\nmydf['something'] = 'hello_world'\n"", ""store = pd.HDFStore('myfile.h5',complevel=9, complib='bzip2')\nstore['mydf'] = mydf\nstore.close()\n"", ""mydf.to_csv('myfile.csv', sep=':')\n"", 'myfile.csv', 'myfile.h5']";"[""import string\nimport random\nimport pandas as pd\n\nmatrix = np.random.random((100, 3000))\nmy_cols = [random.choice(string.ascii_uppercase) for x in range(matrix.shape[1])]\nmydf = pd.DataFrame(matrix, columns=my_cols)\nmydf['something'] = 'hello_world'\nstore = pd.HDFStore('myfile.h5',complevel=9, complib='bzip2')\nstore['mydf'] = mydf\nstore.close()\nmydf.to_csv('myfile.csv', sep=':')\n""]";"[""import string\nimport random\nimport pandas as pd\n\nmatrix = np.random.random((100, 3000))\nmy_cols = [random.choice(string.ascii_uppercase) for x in range(matrix.shape[1])]\nmydf = pd.DataFrame(matrix, columns=my_cols)\nmydf['something'] = 'hello_world'\nstore = pd.HDFStore('myfile.h5',complevel=9, complib='bzip2')\nstore['mydf'] = mydf\nstore.close()\nmydf.to_csv('myfile.csv', sep=':')\n""]";False;"[""import pandas as pd\nimport string\nimport random\nimport pandas as pd\n\nmatrix = np.random.random((100, 3000))\nmy_cols = [random.choice(string.ascii_uppercase) for x in range(matrix.shape[1])]\nmydf = pd.DataFrame(matrix, columns=my_cols)\nmydf['something'] = 'hello_world'\nstore = pd.HDFStore('myfile.h5',complevel=9, complib='bzip2')\nstore['mydf'] = mydf\nstore.close()\nmydf.to_csv('myfile.csv', sep=':')\n""]";False;0;1;"[""name 'DataFrame' is not defined""]";['NameError'];0;1;"[""name 'randn' is not defined""]";['NameError'];0;1;"[""name 'DataFrame' is not defined""]";['NameError']
331;331;331;331;3.0;0;16643695;;1;11;<python><pandas>;pandas convert strings to float for multiple columns in dataframe;20295.0;"[""for column in ['field1', 'field2', 'field3']:\n    data[column] = data[column].str.rstrip('%').astype('float64') / 100\n""]";"[""for column in ['field1', 'field2', 'field3']:\n    data[column] = data[column].str.rstrip('%').astype('float64') / 100\n""]";"[""for column in ['field1', 'field2', 'field3']:\n    data[column] = data[column].str.rstrip('%').astype('float64') / 100\n""]";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'DataFrame' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];0;1;"[""name 'DataFrame' is not defined""]";['NameError']
332;332;332;332;2.0;3;16650945;;1;18;<python><pandas>;Merge on single level of MultiIndex;8389.0;"[""dynamic.reset_index().merge(static, left_on=['ObjectID'], right_index=True)\n""]";"[""dynamic.reset_index().merge(static, left_on=['ObjectID'], right_index=True)\n""]";"[""dynamic.reset_index().merge(static, left_on=['ObjectID'], right_index=True)\n""]";"[""dynamic.reset_index().merge(static, left_on=['ObjectID'], right_index=True)\n""]";"[""dynamic.reset_index().merge(static, left_on=['ObjectID'], right_index=True)\n""]";False;"[""import pandas as pd\ndynamic.reset_index().merge(static, left_on=['ObjectID'], right_index=True)\n""]";False;0;1;"[""name 'df1' is not defined""]";['NameError'];0;1;"[""name 'df1' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
333;333;333;333;2.0;7;16672237;;1;16;<python><datetime><pandas>;Specifying date format when converting with pandas.to_datetime;36583.0;"[""12/01/2012\n30/01/2012\nimport pandas as pd\n...\ncpts.Date = cpts.Date.apply(pd.to_datetime)\ncpts.Date = cpts.Date.apply(pd.to_datetime, format='%d/%m/%Y')\n""]";"['12/01/2012\n30/01/2012\n', 'import pandas as pd\n...\ncpts.Date = cpts.Date.apply(pd.to_datetime)\n', ""cpts.Date = cpts.Date.apply(pd.to_datetime, format='%d/%m/%Y')\n""]";"['%d/%m/%Y', '12/01/2012\n30/01/2012\n', 'import pandas as pd\n...\ncpts.Date = cpts.Date.apply(pd.to_datetime)\n', ""cpts.Date = cpts.Date.apply(pd.to_datetime, format='%d/%m/%Y')\n""]";"[""import pandas as pd\n...\ncpts.Date = cpts.Date.apply(pd.to_datetime)\ncpts.Date = cpts.Date.apply(pd.to_datetime, format='%d/%m/%Y')\n""]";"[""import pandas as pd\n...\ncpts.Date = cpts.Date.apply(pd.to_datetime)\ncpts.Date = cpts.Date.apply(pd.to_datetime, format='%d/%m/%Y')\n""]";False;"[""import pandas as pd\nimport pandas as pd\n...\ncpts.Date = cpts.Date.apply(pd.to_datetime)\ncpts.Date = cpts.Date.apply(pd.to_datetime, format='%d/%m/%Y')\n""]";False;0;2;"[""name 'date' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];1;2;"[""name 'date' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'date' is not defined"", 'Sucess']";['NameError', 'Sucess']
334;334;334;334;3.0;0;16683701;;1;12;<indexing><pandas>;In PANDAS, how to get the index of a known value?;28655.0;"[""In [148]: a = pd.DataFrame(np.arange(10).reshape(5,2),columns=['c1','c2'])\nIn [149]: a\nOut[149]:   \n   c1  c2\n0   0   1\n1   2   3\n2   4   5\n........\nIn [151]: a.ix[0,1]    In [152]: a.c2[0]   In [154]: a.c2.ix[0]   <--  use index\nOut[151]: 1            Out[152]: 1         Out[154]: 1            <--  get value\n""]";"[""In [148]: a = pd.DataFrame(np.arange(10).reshape(5,2),columns=['c1','c2'])\nIn [149]: a\nOut[149]:   \n   c1  c2\n0   0   1\n1   2   3\n2   4   5\n........\n"", 'In [151]: a.ix[0,1]    In [152]: a.c2[0]   In [154]: a.c2.ix[0]   <--  use index\nOut[151]: 1            Out[152]: 1         Out[154]: 1            <--  get value\n']";"[""In [148]: a = pd.DataFrame(np.arange(10).reshape(5,2),columns=['c1','c2'])\nIn [149]: a\nOut[149]:   \n   c1  c2\n0   0   1\n1   2   3\n2   4   5\n........\n"", 'In [151]: a.ix[0,1]    In [152]: a.c2[0]   In [154]: a.c2.ix[0]   <--  use index\nOut[151]: 1            Out[152]: 1         Out[154]: 1            <--  get value\n']";"[""a = pd.DataFrame(np.arange(10).reshape(5,2),columns=['c1','c2'])\n""]";"[""import pandas as pd\na = pd.DataFrame(np.arange(10).reshape(5,2),columns=['c1','c2'])\n""]";True;"[""import pandas as pd\na = pd.DataFrame(np.arange(10).reshape(5,2),columns=['c1','c2'])\n""]";False;0;1;"[""name 'a' is not defined""]";['NameError'];0;1;"[""name 'a' is not defined""]";['NameError'];0;1;"[""name 'a' is not defined""]";['NameError']
335;335;335;335;1.0;2;16689514;;1;21;<python><pandas><dataframe>;how to get the average of dataframe column values;52385.0;['                    A        B\nDATE                 \n2013-05-01        473077    71333\n2013-05-02         35131    62441\n2013-05-03           727    27381\n2013-05-04           481     1206\n2013-05-05           226     1733\n2013-05-06           NaN     4064\n2013-05-07           NaN    41151\n2013-05-08           NaN     8144\n2013-05-09           NaN       23\n2013-05-10           NaN       10\ndf.sum(axis=1) / len(df.columns)\n'];['                    A        B\nDATE                 \n2013-05-01        473077    71333\n2013-05-02         35131    62441\n2013-05-03           727    27381\n2013-05-04           481     1206\n2013-05-05           226     1733\n2013-05-06           NaN     4064\n2013-05-07           NaN    41151\n2013-05-08           NaN     8144\n2013-05-09           NaN       23\n2013-05-10           NaN       10\n', 'df.sum(axis=1) / len(df.columns)\n'];['                    A        B\nDATE                 \n2013-05-01        473077    71333\n2013-05-02         35131    62441\n2013-05-03           727    27381\n2013-05-04           481     1206\n2013-05-05           226     1733\n2013-05-06           NaN     4064\n2013-05-07           NaN    41151\n2013-05-08           NaN     8144\n2013-05-09           NaN       23\n2013-05-10           NaN       10\n', 'df.sum(axis=1) / len(df.columns)\n'];['DATE                 \ndf.sum(axis=1) / len(df.columns)\n'];['DATE                 \ndf.sum(axis=1) / len(df.columns)\n'];False;['import pandas as pd\ndf = pd.DataFrame()\nDATE                 \ndf.sum(axis=1) / len(df.columns)\n'];True;0;1;"[""name 'DATE' is not defined""]";['NameError'];0;1;"[""name 'DATE' is not defined""]";['NameError'];0;1;"[""name 'DATE' is not defined""]";['NameError']
336;336;336;336;4.0;0;16729483;;1;39;<python><pandas>;Converting strings to floats in a DataFrame;88771.0;[''];[];['NaN'];[''];[''];False;['import pandas as pd\n'];False;1;4;"[""name 'df' is not defined"", ""name 'DataFrame' is not defined"", ""name 'df' is not defined"", 'Sucess']";['NameError', 'NameError', 'NameError', 'Sucess'];1;4;"[""name 'df' is not defined"", ""name 'Series' is not defined"", ""name 'df' is not defined"", 'Sucess']";['NameError', 'NameError', 'NameError', 'Sucess'];1;4;"[""'DataFrame' object has no attribute 'a'"", ""name 'DataFrame' is not defined"", ""'MyColumnName'"", 'Sucess']";['AttributeError', 'NameError', 'KeyError', 'Sucess']
337;337;337;337;5.0;0;16729574;;1;86;<python><pandas><dataframe>;How to get a value from a cell of a data frame?;161920.0;"[""d2 = df[(df['l_ext']==l_ext) & (df['item']==item) & (df['wn']==wn) & (df['wd']==1)]\nval = d2['col_name']\n""]";"[""d2 = df[(df['l_ext']==l_ext) & (df['item']==item) & (df['wn']==wn) & (df['wd']==1)]\n"", ""val = d2['col_name']\n""]";"[""d2 = df[(df['l_ext']==l_ext) & (df['item']==item) & (df['wn']==wn) & (df['wd']==1)]\n"", ""val = d2['col_name']\n""]";"[""d2 = df[(df['l_ext']==l_ext) & (df['item']==item) & (df['wn']==wn) & (df['wd']==1)]\nval = d2['col_name']\n""]";"[""d2 = df[(df['l_ext']==l_ext) & (df['item']==item) & (df['wn']==wn) & (df['wd']==1)]\nval = d2['col_name']\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nd2 = df[(df['l_ext']==l_ext) & (df['item']==item) & (df['wn']==wn) & (df['wd']==1)]\nval = d2['col_name']\n""]";True;0;2;"[""name 'sub_df' is not defined"", ""name 'DataFrame' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'sub_df' is not defined"", ""name 'randn' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'sub_df' is not defined"", ""name 'DataFrame' is not defined""]";['NameError', 'NameError']
338;338;338;338;2.0;6;16740887;;1;46;<python><pandas>;How to handle incoming real time data with python pandas;7890.0;"[""{'time' :'2013-01-01 00:00:00', 'stock' : 'BLAH',\n 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}\nIn [1]: index = pd.DatetimeIndex(start='2013-01-01 00:00:00', freq='S', periods=5)\n\nIn [2]: columns = ['high', 'low', 'open', 'close']\n\nIn [3]: df = pd.DataFrame(index=t, columns=columns)\n\nIn [4]: df\nOut[4]: \n                    high  low open close\n2013-01-01 00:00:00  NaN  NaN  NaN   NaN\n2013-01-01 00:00:01  NaN  NaN  NaN   NaN\n2013-01-01 00:00:02  NaN  NaN  NaN   NaN\n2013-01-01 00:00:03  NaN  NaN  NaN   NaN\n2013-01-01 00:00:04  NaN  NaN  NaN   NaN\n\nIn [5]: data = {'time' :'2013-01-01 00:00:02', 'stock' : 'BLAH', 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}\n\nIn [6]: data_ = pd.Series(data)\n\nIn [7]: df.loc[data['time']] = data_\n\nIn [8]: df\nOut[8]: \n                    high  low open close\n2013-01-01 00:00:00  NaN  NaN  NaN   NaN\n2013-01-01 00:00:01  NaN  NaN  NaN   NaN\n2013-01-01 00:00:02    4    3    2     1\n2013-01-01 00:00:03  NaN  NaN  NaN   NaN\n2013-01-01 00:00:04  NaN  NaN  NaN   NaN\nIn [9]: ls = []\n\nIn [10]: for n in range(5):\n   .....:     # Naive stuff ahead =)\n   .....:     time = '2013-01-01 00:00:0' + str(n)\n   .....:     d = {'time' : time, 'stock' : 'BLAH', 'high' : np.random.rand()*10, 'low' : np.random.rand()*10, 'open' : np.random.rand()*10, 'close' : np.random.rand()*10}\n   .....:     ls.append(d)\n\nIn [11]: df = pd.DataFrame(ls[1:3]).set_index('time')\n\nIn [12]: df\nOut[12]: \n                        close      high       low      open stock\ntime                                                             \n2013-01-01 00:00:01  3.270078  1.008289  7.486118  2.180683  BLAH\n2013-01-01 00:00:02  3.883586  2.215645  0.051799  2.310823  BLAH\n""]";"[""{'time' :'2013-01-01 00:00:00', 'stock' : 'BLAH',\n 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}\n"", ""In [1]: index = pd.DatetimeIndex(start='2013-01-01 00:00:00', freq='S', periods=5)\n\nIn [2]: columns = ['high', 'low', 'open', 'close']\n\nIn [3]: df = pd.DataFrame(index=t, columns=columns)\n\nIn [4]: df\nOut[4]: \n                    high  low open close\n2013-01-01 00:00:00  NaN  NaN  NaN   NaN\n2013-01-01 00:00:01  NaN  NaN  NaN   NaN\n2013-01-01 00:00:02  NaN  NaN  NaN   NaN\n2013-01-01 00:00:03  NaN  NaN  NaN   NaN\n2013-01-01 00:00:04  NaN  NaN  NaN   NaN\n\nIn [5]: data = {'time' :'2013-01-01 00:00:02', 'stock' : 'BLAH', 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}\n\nIn [6]: data_ = pd.Series(data)\n\nIn [7]: df.loc[data['time']] = data_\n\nIn [8]: df\nOut[8]: \n                    high  low open close\n2013-01-01 00:00:00  NaN  NaN  NaN   NaN\n2013-01-01 00:00:01  NaN  NaN  NaN   NaN\n2013-01-01 00:00:02    4    3    2     1\n2013-01-01 00:00:03  NaN  NaN  NaN   NaN\n2013-01-01 00:00:04  NaN  NaN  NaN   NaN\n"", ""In [9]: ls = []\n\nIn [10]: for n in range(5):\n   .....:     # Naive stuff ahead =)\n   .....:     time = '2013-01-01 00:00:0' + str(n)\n   .....:     d = {'time' : time, 'stock' : 'BLAH', 'high' : np.random.rand()*10, 'low' : np.random.rand()*10, 'open' : np.random.rand()*10, 'close' : np.random.rand()*10}\n   .....:     ls.append(d)\n\nIn [11]: df = pd.DataFrame(ls[1:3]).set_index('time')\n\nIn [12]: df\nOut[12]: \n                        close      high       low      open stock\ntime                                                             \n2013-01-01 00:00:01  3.270078  1.008289  7.486118  2.180683  BLAH\n2013-01-01 00:00:02  3.883586  2.215645  0.051799  2.310823  BLAH\n""]";"[""{'time' :'2013-01-01 00:00:00', 'stock' : 'BLAH',\n 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}\n"", ""In [1]: index = pd.DatetimeIndex(start='2013-01-01 00:00:00', freq='S', periods=5)\n\nIn [2]: columns = ['high', 'low', 'open', 'close']\n\nIn [3]: df = pd.DataFrame(index=t, columns=columns)\n\nIn [4]: df\nOut[4]: \n                    high  low open close\n2013-01-01 00:00:00  NaN  NaN  NaN   NaN\n2013-01-01 00:00:01  NaN  NaN  NaN   NaN\n2013-01-01 00:00:02  NaN  NaN  NaN   NaN\n2013-01-01 00:00:03  NaN  NaN  NaN   NaN\n2013-01-01 00:00:04  NaN  NaN  NaN   NaN\n\nIn [5]: data = {'time' :'2013-01-01 00:00:02', 'stock' : 'BLAH', 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}\n\nIn [6]: data_ = pd.Series(data)\n\nIn [7]: df.loc[data['time']] = data_\n\nIn [8]: df\nOut[8]: \n                    high  low open close\n2013-01-01 00:00:00  NaN  NaN  NaN   NaN\n2013-01-01 00:00:01  NaN  NaN  NaN   NaN\n2013-01-01 00:00:02    4    3    2     1\n2013-01-01 00:00:03  NaN  NaN  NaN   NaN\n2013-01-01 00:00:04  NaN  NaN  NaN   NaN\n"", ""In [9]: ls = []\n\nIn [10]: for n in range(5):\n   .....:     # Naive stuff ahead =)\n   .....:     time = '2013-01-01 00:00:0' + str(n)\n   .....:     d = {'time' : time, 'stock' : 'BLAH', 'high' : np.random.rand()*10, 'low' : np.random.rand()*10, 'open' : np.random.rand()*10, 'close' : np.random.rand()*10}\n   .....:     ls.append(d)\n\nIn [11]: df = pd.DataFrame(ls[1:3]).set_index('time')\n\nIn [12]: df\nOut[12]: \n                        close      high       low      open stock\ntime                                                             \n2013-01-01 00:00:01  3.270078  1.008289  7.486118  2.180683  BLAH\n2013-01-01 00:00:02  3.883586  2.215645  0.051799  2.310823  BLAH\n""]";"[""index = pd.DatetimeIndex(start='2013-01-01 00:00:00', freq='S', periods=5)\n\n\n\ndata = {'time' :'2013-01-01 00:00:02', 'stock' : 'BLAH', 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}\n\n\n\nls = []\n\n\n\n""]";"[""import pandas as pd\nindex = pd.DatetimeIndex(start='2013-01-01 00:00:00', freq='S', periods=5)\n\n\n\ndata = {'time' :'2013-01-01 00:00:02', 'stock' : 'BLAH', 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}\n\n\n\nls = []\n\n\n\n""]";True;"[""import pandas as pd\nindex = pd.DatetimeIndex(start='2013-01-01 00:00:00', freq='S', periods=5)\n\n\n\ndata = {'time' :'2013-01-01 00:00:02', 'stock' : 'BLAH', 'high' : 4.0, 'low' : 3.0, 'open' : 2.0, 'close' : 1.0}\n\n\n\nls = []\n\n\n\n""]";False;1;2;"['Sucess', ""name 'get_latest' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'get_latest' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'get_latest' is not defined""]";['Sucess', 'NameError']
339;339;339;339;2.0;1;16777570;;1;22;<python><dataframe><pandas>;Calculate time difference between Pandas Dataframe indices;19835.0;['time                 value\n\n2012-03-16 23:50:00      1\n2012-03-16 23:56:00      2\n2012-03-17 00:08:00      3\n2012-03-17 00:10:00      4\n2012-03-17 00:12:00      5\n2012-03-17 00:20:00      6\n2012-03-20 00:43:00      7\ntime                 value  deltaT\n\n2012-03-16 23:50:00      1       0\n2012-03-16 23:56:00      2       6\n2012-03-17 00:08:00      3      12\n2012-03-17 00:10:00      4       2\n2012-03-17 00:12:00      5       2\n2012-03-17 00:20:00      6       8\n2012-03-20 00:43:00      7      23\n'];['time                 value\n\n2012-03-16 23:50:00      1\n2012-03-16 23:56:00      2\n2012-03-17 00:08:00      3\n2012-03-17 00:10:00      4\n2012-03-17 00:12:00      5\n2012-03-17 00:20:00      6\n2012-03-20 00:43:00      7\n', 'time                 value  deltaT\n\n2012-03-16 23:50:00      1       0\n2012-03-16 23:56:00      2       6\n2012-03-17 00:08:00      3      12\n2012-03-17 00:10:00      4       2\n2012-03-17 00:12:00      5       2\n2012-03-17 00:20:00      6       8\n2012-03-20 00:43:00      7      23\n'];['time                 value\n\n2012-03-16 23:50:00      1\n2012-03-16 23:56:00      2\n2012-03-17 00:08:00      3\n2012-03-17 00:10:00      4\n2012-03-17 00:12:00      5\n2012-03-17 00:20:00      6\n2012-03-20 00:43:00      7\n', 'time                 value  deltaT\n\n2012-03-16 23:50:00      1       0\n2012-03-16 23:56:00      2       6\n2012-03-17 00:08:00      3      12\n2012-03-17 00:10:00      4       2\n2012-03-17 00:12:00      5       2\n2012-03-17 00:20:00      6       8\n2012-03-20 00:43:00      7      23\n'];['\n\n'];['\n\n'];False;['import pandas as pd\n\n\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'delta'"", 'Can only use .dt accessor with datetimelike values']";['KeyError', 'AttributeError']
340;340;340;340;1.0;1;16782323;;1;33;<python><pandas>;Python pandas: Keep selected column as DataFrame instead of Series;15889.0;[''];[];"['df.iloc[:, 0]', ""df['A']"", 'df.A', 'pd.DataFrame(df.iloc[:, 0])']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
341;341;341;341;2.0;4;16822996;;1;14;<python><pandas><time-series>;How to convert a pandas DataFrame into a TimeSeries?;15070.0;"[""In [20]: import pandas as pd\n\nIn [21]: import numpy as np\n\nIn [22]: dates = pd.date_range('20130101',periods=6)\n\nIn [23]: df = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list('ABCD'))\n\nIn [24]: df\nOut[24]:\n                   A         B         C         D\n2013-01-01 -0.119230  1.892838  0.843414 -0.482739\n2013-01-02  1.204884 -0.942299 -0.521808  0.446309\n2013-01-03  1.899832  0.460871 -1.491727 -0.647614\n2013-01-04  1.126043  0.818145  0.159674 -1.490958\n2013-01-05  0.113360  0.190421 -0.618656  0.976943\n2013-01-06 -0.537863 -0.078802  0.197864 -1.414924\n\nIn [25]: pd.Series(df)\nOut[25]:\n0    A\n1    B\n2    C\n3    D\ndtype: object\n""]";"[""In [20]: import pandas as pd\n\nIn [21]: import numpy as np\n\nIn [22]: dates = pd.date_range('20130101',periods=6)\n\nIn [23]: df = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list('ABCD'))\n\nIn [24]: df\nOut[24]:\n                   A         B         C         D\n2013-01-01 -0.119230  1.892838  0.843414 -0.482739\n2013-01-02  1.204884 -0.942299 -0.521808  0.446309\n2013-01-03  1.899832  0.460871 -1.491727 -0.647614\n2013-01-04  1.126043  0.818145  0.159674 -1.490958\n2013-01-05  0.113360  0.190421 -0.618656  0.976943\n2013-01-06 -0.537863 -0.078802  0.197864 -1.414924\n\nIn [25]: pd.Series(df)\nOut[25]:\n0    A\n1    B\n2    C\n3    D\ndtype: object\n""]";"[""In [20]: import pandas as pd\n\nIn [21]: import numpy as np\n\nIn [22]: dates = pd.date_range('20130101',periods=6)\n\nIn [23]: df = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list('ABCD'))\n\nIn [24]: df\nOut[24]:\n                   A         B         C         D\n2013-01-01 -0.119230  1.892838  0.843414 -0.482739\n2013-01-02  1.204884 -0.942299 -0.521808  0.446309\n2013-01-03  1.899832  0.460871 -1.491727 -0.647614\n2013-01-04  1.126043  0.818145  0.159674 -1.490958\n2013-01-05  0.113360  0.190421 -0.618656  0.976943\n2013-01-06 -0.537863 -0.078802  0.197864 -1.414924\n\nIn [25]: pd.Series(df)\nOut[25]:\n0    A\n1    B\n2    C\n3    D\ndtype: object\n""]";['import pandas as pd\n\n\n\n\npd.Series(df)\n'];['import pandas as pd\n\n\n\n\npd.Series(df)\n'];False;['import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\n\n\n\n\npd.Series(df)\n'];True;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""'DateTime'""]";['Sucess', 'KeyError']
342;342;342;342;2.0;0;16824607;;1;18;<python><pandas>;Pandas: Appending a row to a dataframe and specify its index label;23799.0;"[""In [1301]: df = DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])\n\nIn [1302]: df\nOut[1302]: \n          A         B         C         D\n0 -1.137707 -0.891060 -0.693921  1.613616\n1  0.464000  0.227371 -0.496922  0.306389\n2 -2.290613 -1.134623 -1.561819 -0.260838\n3  0.281957  1.523962 -0.902937  0.068159\n4 -0.057873 -0.368204 -1.144073  0.861209\n5  0.800193  0.782098 -1.069094 -1.099248\n6  0.255269  0.009750  0.661084  0.379319\n7 -0.008434  1.952541 -1.056652  0.533946\n\nIn [1303]: s = df.xs(3)\n\nIn [1304]: df.append(s, ignore_index=True)\nOut[1304]: \n          A         B         C         D\n0 -1.137707 -0.891060 -0.693921  1.613616\n1  0.464000  0.227371 -0.496922  0.306389\n2 -2.290613 -1.134623 -1.561819 -0.260838\n3  0.281957  1.523962 -0.902937  0.068159\n4 -0.057873 -0.368204 -1.144073  0.861209\n5  0.800193  0.782098 -1.069094 -1.099248\n6  0.255269  0.009750  0.661084  0.379319\n7 -0.008434  1.952541 -1.056652  0.533946\n8  0.281957  1.523962 -0.902937  0.068159\n""]";"[""In [1301]: df = DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])\n\nIn [1302]: df\nOut[1302]: \n          A         B         C         D\n0 -1.137707 -0.891060 -0.693921  1.613616\n1  0.464000  0.227371 -0.496922  0.306389\n2 -2.290613 -1.134623 -1.561819 -0.260838\n3  0.281957  1.523962 -0.902937  0.068159\n4 -0.057873 -0.368204 -1.144073  0.861209\n5  0.800193  0.782098 -1.069094 -1.099248\n6  0.255269  0.009750  0.661084  0.379319\n7 -0.008434  1.952541 -1.056652  0.533946\n\nIn [1303]: s = df.xs(3)\n\nIn [1304]: df.append(s, ignore_index=True)\nOut[1304]: \n          A         B         C         D\n0 -1.137707 -0.891060 -0.693921  1.613616\n1  0.464000  0.227371 -0.496922  0.306389\n2 -2.290613 -1.134623 -1.561819 -0.260838\n3  0.281957  1.523962 -0.902937  0.068159\n4 -0.057873 -0.368204 -1.144073  0.861209\n5  0.800193  0.782098 -1.069094 -1.099248\n6  0.255269  0.009750  0.661084  0.379319\n7 -0.008434  1.952541 -1.056652  0.533946\n8  0.281957  1.523962 -0.902937  0.068159\n""]";"[""In [1301]: df = DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])\n\nIn [1302]: df\nOut[1302]: \n          A         B         C         D\n0 -1.137707 -0.891060 -0.693921  1.613616\n1  0.464000  0.227371 -0.496922  0.306389\n2 -2.290613 -1.134623 -1.561819 -0.260838\n3  0.281957  1.523962 -0.902937  0.068159\n4 -0.057873 -0.368204 -1.144073  0.861209\n5  0.800193  0.782098 -1.069094 -1.099248\n6  0.255269  0.009750  0.661084  0.379319\n7 -0.008434  1.952541 -1.056652  0.533946\n\nIn [1303]: s = df.xs(3)\n\nIn [1304]: df.append(s, ignore_index=True)\nOut[1304]: \n          A         B         C         D\n0 -1.137707 -0.891060 -0.693921  1.613616\n1  0.464000  0.227371 -0.496922  0.306389\n2 -2.290613 -1.134623 -1.561819 -0.260838\n3  0.281957  1.523962 -0.902937  0.068159\n4 -0.057873 -0.368204 -1.144073  0.861209\n5  0.800193  0.782098 -1.069094 -1.099248\n6  0.255269  0.009750  0.661084  0.379319\n7 -0.008434  1.952541 -1.056652  0.533946\n8  0.281957  1.523962 -0.902937  0.068159\n""]";"[""df = DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])\n\ns = df.xs(3)\n\n""]";"[""from pandas import DataFrame\ndf = DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])\n\ns = df.xs(3)\n\n""]";True;"[""import pandas as pd\ndf = DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])\n\ns = df.xs(3)\n\n""]";False;1;2;"[""name 'pd' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'np' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'np' is not defined"", 'Sucess']";['NameError', 'Sucess']
343;343;343;343;2.0;3;16826711;;1;19;<python><matplotlib><pandas><legend><legend-properties>;Is it possible to add a string as a legend item in matplotlib;12597.0;[''];[];"[""ax.legend(['0-10','10-100','100-500','500+'],loc='best')""]";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
344;344;344;344;1.0;5;16833842;;1;15;<python><pandas><multi-index><dataframe>;Assign new values to slice from MultiIndex DataFrame;7430.0;"[""In [1]: arrays = [np.array(['bar', 'bar', 'baz', 'qux', 'qux', 'bar']),\n                  np.array(['one', 'two', 'one', 'one', 'two', 'one']),\n                  np.arange(0, 6, 1)]\nIn [2]: df = pd.DataFrame(randn(6, 3), index=arrays, columns=['A', 'B', 'C'])\n\nIn [3]: df\n                  A         B         C\nbar one 0 -0.088671  1.902021 -0.540959\n    two 1  0.782919 -0.733581 -0.824522\nbaz one 2 -0.827128 -0.849712  0.072431\nqux one 3 -0.328493  1.456945  0.587793\n    two 4 -1.466625  0.720638  0.976438\nbar one 5 -0.456558  1.163404  0.464295\nIn [4]: df.ix['bar', 'two', :]['A']\nOut[4]:\n1    0.782919\nName: A, dtype: float64\n\nIn [5]: df.ix['bar', 'two', :]['A'] = 9999\n# df is unchanged\nIn [6]: df.ix['bar', 'one', :]['A'] = [999, 888]\n# again df remains unchanged\n""]";"[""In [1]: arrays = [np.array(['bar', 'bar', 'baz', 'qux', 'qux', 'bar']),\n                  np.array(['one', 'two', 'one', 'one', 'two', 'one']),\n                  np.arange(0, 6, 1)]\nIn [2]: df = pd.DataFrame(randn(6, 3), index=arrays, columns=['A', 'B', 'C'])\n\nIn [3]: df\n                  A         B         C\nbar one 0 -0.088671  1.902021 -0.540959\n    two 1  0.782919 -0.733581 -0.824522\nbaz one 2 -0.827128 -0.849712  0.072431\nqux one 3 -0.328493  1.456945  0.587793\n    two 4 -1.466625  0.720638  0.976438\nbar one 5 -0.456558  1.163404  0.464295\n"", ""In [4]: df.ix['bar', 'two', :]['A']\nOut[4]:\n1    0.782919\nName: A, dtype: float64\n\nIn [5]: df.ix['bar', 'two', :]['A'] = 9999\n# df is unchanged\n"", ""In [6]: df.ix['bar', 'one', :]['A'] = [999, 888]\n# again df remains unchanged\n""]";"['df', 'df', ""In [1]: arrays = [np.array(['bar', 'bar', 'baz', 'qux', 'qux', 'bar']),\n                  np.array(['one', 'two', 'one', 'one', 'two', 'one']),\n                  np.arange(0, 6, 1)]\nIn [2]: df = pd.DataFrame(randn(6, 3), index=arrays, columns=['A', 'B', 'C'])\n\nIn [3]: df\n                  A         B         C\nbar one 0 -0.088671  1.902021 -0.540959\n    two 1  0.782919 -0.733581 -0.824522\nbaz one 2 -0.827128 -0.849712  0.072431\nqux one 3 -0.328493  1.456945  0.587793\n    two 4 -1.466625  0.720638  0.976438\nbar one 5 -0.456558  1.163404  0.464295\n"", 'df', ""In [4]: df.ix['bar', 'two', :]['A']\nOut[4]:\n1    0.782919\nName: A, dtype: float64\n\nIn [5]: df.ix['bar', 'two', :]['A'] = 9999\n# df is unchanged\n"", ""In [6]: df.ix['bar', 'one', :]['A'] = [999, 888]\n# again df remains unchanged\n""]";['\n'];['\n'];False;['import pandas as pd\n\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'randn' is not defined""]";['NameError'];0;1;"[""name 'randn' is not defined""]";['NameError']
345;345;345;345;4.0;0;16852911;;1;42;<python><date><pandas>;How do I convert dates in a Pandas data frame to a 'date' data type?;54111.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;2;4;"[""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess', 'Sucess']";['NameError', 'NameError', 'Sucess', 'Sucess'];2;4;"[""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess', 'Sucess']";['NameError', 'NameError', 'Sucess', 'Sucess'];2;4;"[""'time'"", ""'time'"", 'Sucess', 'Sucess']";['KeyError', 'KeyError', 'Sucess', 'Sucess']
346;346;346;346;1.0;1;16888888;;1;28;<python><pandas><ipython><ipython-notebook><dataframe>;How to read a .xlsx file using the pandas Library in iPython?;25755.0;"['import pandas as pd\ndata = pd.ExcelFile(""*File Name*"")\n']";"['import pandas as pd\ndata = pd.ExcelFile(""*File Name*"")\n']";"['import pandas as pd\ndata = pd.ExcelFile(""*File Name*"")\n']";"['import pandas as pd\ndata = pd.ExcelFile(""*File Name*"")\n']";"['import pandas as pd\ndata = pd.ExcelFile(""*File Name*"")\n']";False;"['import pandas as pd\nimport pandas as pd\ndata = pd.ExcelFile(""*File Name*"")\n']";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'file_name' is not defined""]";['NameError'];0;1;"[""name 'file_name' is not defined""]";['NameError']
347;347;347;347;5.0;0;16923281;;1;214;<python><csv><pandas><dataframe>;Pandas writing dataframe to CSV file;296632.0;"[""df.to_csv('out.csv')\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\u03b1' in position 20: ordinal not in range(128)\n""]";"[""df.to_csv('out.csv')\n"", ""UnicodeEncodeError: 'ascii' codec can't encode character u'\\u03b1' in position 20: ordinal not in range(128)\n""]";"[""df.to_csv('out.csv')\n"", ""UnicodeEncodeError: 'ascii' codec can't encode character u'\\u03b1' in position 20: ordinal not in range(128)\n""]";"[""df.to_csv('out.csv')\n""]";"[""df.to_csv('out.csv')\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf.to_csv('out.csv')\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'file_name' is not defined""]";['NameError']
348;348;348;348;3.0;0;16947336;;1;31;<python><numpy><pandas>;binning a dataframe in pandas in Python;34913.0;"['import numpy as np\ndf = pandas.DataFrame({""a"": np.random.random(100), ""b"": np.random.random(100), ""id"": np.arange(100)})\na = np.random.random(20)\ndf = pandas.DataFrame({""a"": a, ""b"": a + 10})\n# bins for df.a\nbins = np.linspace(0, 1, 10)\n# bin df according to a\ngroups = df.groupby(np.digitize(df.a,bins))\n# Get the mean of a in each group\nprint groups.mean()\n## But how to get the mean of b for each group of a?\n# ...\n']";"['import numpy as np\ndf = pandas.DataFrame({""a"": np.random.random(100), ""b"": np.random.random(100), ""id"": np.arange(100)})\n', 'a = np.random.random(20)\ndf = pandas.DataFrame({""a"": a, ""b"": a + 10})\n# bins for df.a\nbins = np.linspace(0, 1, 10)\n# bin df according to a\ngroups = df.groupby(np.digitize(df.a,bins))\n# Get the mean of a in each group\nprint groups.mean()\n## But how to get the mean of b for each group of a?\n# ...\n']";"['import numpy as np\ndf = pandas.DataFrame({""a"": np.random.random(100), ""b"": np.random.random(100), ""id"": np.arange(100)})\n', 'id', 'a', 'b', 'a', 'b', 'a', 'b', 'df', 'NaN', 'a', 'b', 'df', 'a = np.random.random(20)\ndf = pandas.DataFrame({""a"": a, ""b"": a + 10})\n# bins for df.a\nbins = np.linspace(0, 1, 10)\n# bin df according to a\ngroups = df.groupby(np.digitize(df.a,bins))\n# Get the mean of a in each group\nprint groups.mean()\n## But how to get the mean of b for each group of a?\n# ...\n']";"['import numpy as np\ndf = pandas.DataFrame({""a"": np.random.random(100), ""b"": np.random.random(100), ""id"": np.arange(100)})\na = np.random.random(20)\ndf = pandas.DataFrame({""a"": a, ""b"": a + 10})\n# bins for df.a\nbins = np.linspace(0, 1, 10)\n# bin df according to a\ngroups = df.groupby(np.digitize(df.a,bins))\n# Get the mean of a in each group\n## But how to get the mean of b for each group of a?\n# ...\n']";"['import numpy as np\ndf = pandas.DataFrame({""a"": np.random.random(100), ""b"": np.random.random(100), ""id"": np.arange(100)})\na = np.random.random(20)\ndf = pandas.DataFrame({""a"": a, ""b"": a + 10})\n# bins for df.a\nbins = np.linspace(0, 1, 10)\n# bin df according to a\ngroups = df.groupby(np.digitize(df.a,bins))\n# Get the mean of a in each group\n## But how to get the mean of b for each group of a?\n# ...\n']";False;"['import pandas as pd\nimport numpy as np\ndf = pandas.DataFrame({""a"": np.random.random(100), ""b"": np.random.random(100), ""id"": np.arange(100)})\na = np.random.random(20)\ndf = pandas.DataFrame({""a"": a, ""b"": a + 10})\n# bins for df.a\nbins = np.linspace(0, 1, 10)\n# bin df according to a\ngroups = df.groupby(np.digitize(df.a,bins))\n# Get the mean of a in each group\n## But how to get the mean of b for each group of a?\n# ...\n']";False;0;3;"[""name 'df' is not defined"", ""name 'DataFrame' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'np' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""'DataFrame' object has no attribute 'a'"", ""name 'DataFrame' is not defined"", ""name 'pandas' is not defined""]";['AttributeError', 'NameError', 'NameError']
349;349;349;349;3.0;0;16958499;;1;11;<pandas><dataframe>;Sort Pandas dataframe and print highest n values;20978.0;"[""0       Bytes    Client Ip                \n0       1000      192.168.10.2    \n1       2000      192.168.10.12    \n2       500       192.168.10.4     \n3       159       192.168.10.56 \nprint df['Bytes'].argmax()\n""]";"['0       Bytes    Client Ip                \n0       1000      192.168.10.2    \n1       2000      192.168.10.12    \n2       500       192.168.10.4     \n3       159       192.168.10.56 \n', ""print df['Bytes'].argmax()\n""]";"['0       Bytes    Client Ip                \n0       1000      192.168.10.2    \n1       2000      192.168.10.12    \n2       500       192.168.10.4     \n3       159       192.168.10.56 \n', ""print df['Bytes'].argmax()\n""]";[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""'DataFrame' object has no attribute 'sort'"", 'Sucess']";['AttributeError', 'Sucess']
350;350;350;350;1.0;0;16988526;;1;16;<python><pandas>;Pandas reading csv as string type;25336.0;"[""df = pd.DataFrame(np.random.rand(2,2),\n                  index=['1A', '1B'],\n                  columns=['A', 'B'])\ndf.to_csv(savefile)\n           A         B\n1A  0.209059  0.275554\n1B  0.742666  0.721165\ndf_read = pd.read_csv(savefile, dtype=str, index_col=0)\n   A  B\nB  (  <\n""]";"[""df = pd.DataFrame(np.random.rand(2,2),\n                  index=['1A', '1B'],\n                  columns=['A', 'B'])\ndf.to_csv(savefile)\n"", '           A         B\n1A  0.209059  0.275554\n1B  0.742666  0.721165\n', 'df_read = pd.read_csv(savefile, dtype=str, index_col=0)\n', '   A  B\nB  (  <\n']";"[""df = pd.DataFrame(np.random.rand(2,2),\n                  index=['1A', '1B'],\n                  columns=['A', 'B'])\ndf.to_csv(savefile)\n"", '           A         B\n1A  0.209059  0.275554\n1B  0.742666  0.721165\n', 'df_read = pd.read_csv(savefile, dtype=str, index_col=0)\n', '   A  B\nB  (  <\n']";['df.to_csv(savefile)\ndf_read = pd.read_csv(savefile, dtype=str, index_col=0)\n'];['import pandas as pd\ndf.to_csv(savefile)\ndf_read = pd.read_csv(savefile, dtype=str, index_col=0)\n'];True;['import pandas as pd\ndf = pd.DataFrame()\ndf.to_csv(savefile)\ndf_read = pd.read_csv(savefile, dtype=str, index_col=0)\n'];True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""File b'a' does not exist""]";['FileNotFoundError'];0;1;"[""File b'a' does not exist""]";['FileNotFoundError']
351;351;351;351;2.0;1;16989946;;1;15;<python><pandas>;Creating an element-wise minimum Series from two other Series in Python Pandas;9408.0;['In [1]:\nimport pandas as pd\ns1 = pd.Series(data=[1,1,1], index=[1,2,3])\ns2 = pd.Series(data=[1,2,2,1], index=[1,2,3,4])\ns1.add(s2)    \nOut[1]:\n1     2\n2     3\n3     3\n4   NaN\ndtype: float64\nIn [2]:\ns1 = pd.Series(data=[1,1,1], index=[1,2,3])\ns2 = pd.Series(data=[1,2,2,1], index=[1,2,3,4])\ns1.combine(s2, min, 0)\nOut[2]:\n1    1\n2    1\n3    1\n4    0\ndtype: int64\n'];['In [1]:\nimport pandas as pd\ns1 = pd.Series(data=[1,1,1], index=[1,2,3])\ns2 = pd.Series(data=[1,2,2,1], index=[1,2,3,4])\ns1.add(s2)    \nOut[1]:\n1     2\n2     3\n3     3\n4   NaN\ndtype: float64\n', 'In [2]:\ns1 = pd.Series(data=[1,1,1], index=[1,2,3])\ns2 = pd.Series(data=[1,2,2,1], index=[1,2,3,4])\ns1.combine(s2, min, 0)\nOut[2]:\n1    1\n2    1\n3    1\n4    0\ndtype: int64\n'];['In [1]:\nimport pandas as pd\ns1 = pd.Series(data=[1,1,1], index=[1,2,3])\ns2 = pd.Series(data=[1,2,2,1], index=[1,2,3,4])\ns1.add(s2)    \nOut[1]:\n1     2\n2     3\n3     3\n4   NaN\ndtype: float64\n', 'In [2]:\ns1 = pd.Series(data=[1,1,1], index=[1,2,3])\ns2 = pd.Series(data=[1,2,2,1], index=[1,2,3,4])\ns1.combine(s2, min, 0)\nOut[2]:\n1    1\n2    1\n3    1\n4    0\ndtype: int64\n'];['import pandas as pd\ns1 = pd.Series(data=[1,1,1], index=[1,2,3])\ns2 = pd.Series(data=[1,2,2,1], index=[1,2,3,4])\ns1.add(s2)    \ns1 = pd.Series(data=[1,1,1], index=[1,2,3])\ns2 = pd.Series(data=[1,2,2,1], index=[1,2,3,4])\ns1.combine(s2, min, 0)\n'];['import pandas as pd\ns1 = pd.Series(data=[1,1,1], index=[1,2,3])\ns2 = pd.Series(data=[1,2,2,1], index=[1,2,3,4])\ns1.add(s2)    \ns1 = pd.Series(data=[1,1,1], index=[1,2,3])\ns2 = pd.Series(data=[1,2,2,1], index=[1,2,3,4])\ns1.combine(s2, min, 0)\n'];False;['import pandas as pd\ndata = pd.DataFrame()\nimport pandas as pd\ns1 = pd.Series(data=[1,1,1], index=[1,2,3])\ns2 = pd.Series(data=[1,2,2,1], index=[1,2,3,4])\ns1.add(s2)    \ns1 = pd.Series(data=[1,1,1], index=[1,2,3])\ns2 = pd.Series(data=[1,2,2,1], index=[1,2,3,4])\ns1.combine(s2, min, 0)\n'];True;1;2;"['Sucess', ""name 'pd' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 's1' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 's1' is not defined""]";['Sucess', 'NameError']
352;352;352;352;2.0;6;16990996;;1;12;<python><numpy><pandas><scikit-learn>;Multidimensional Scaling Fitting in Numpy, Pandas and Sklearn (ValueError);7435.0;"['import numpy as np\nimport pandas as pd\nfrom sklearn import manifold\nfrom sklearn.metrics import euclidean_distances\n\nseed = np.random.RandomState(seed=3)\ndata = pd.read_csv(\'data/big-file.csv\')\n\n#  start small dont take all the data, \n#  its about 200k records\nsubset = data[:10000]\nsimilarities = euclidean_distances(subset)\n\nmds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, \n      random_state=seed, dissimilarity=""precomputed"", n_jobs=1)\n\npos = mds.fit(similarities).embedding_\nTraceback (most recent call last):\n  File ""demo/mds-demo.py"", line 18, in <module>\n    pos = mds.fit(similarities).embedding_\n  File ""/Users/dwilliams/Desktop/Anaconda/lib/python2.7/site-packages/sklearn/manifold/mds.py"", line 360, in fit\n    self.fit_transform(X, init=init)\n  File ""/Users/dwilliams/Desktop/Anaconda/lib/python2.7/site-packages/sklearn/manifold/mds.py"", line 395, in fit_transform\neps=self.eps, random_state=self.random_state)\n  File ""/Users/dwilliams/Desktop/Anaconda/lib/python2.7/site-packages/sklearn/manifold/mds.py"", line 242, in smacof\neps=eps, random_state=random_state)\n  File ""/Users/dwilliams/Desktop/Anaconda/lib/python2.7/site-packages/sklearn/manifold/mds.py"", line 73, in _smacof_single\nraise ValueError(""similarities must be symmetric"")\nValueError: similarities must be symmetric\n']";"['import numpy as np\nimport pandas as pd\nfrom sklearn import manifold\nfrom sklearn.metrics import euclidean_distances\n\nseed = np.random.RandomState(seed=3)\ndata = pd.read_csv(\'data/big-file.csv\')\n\n#  start small dont take all the data, \n#  its about 200k records\nsubset = data[:10000]\nsimilarities = euclidean_distances(subset)\n\nmds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, \n      random_state=seed, dissimilarity=""precomputed"", n_jobs=1)\n\npos = mds.fit(similarities).embedding_\n', 'Traceback (most recent call last):\n  File ""demo/mds-demo.py"", line 18, in <module>\n    pos = mds.fit(similarities).embedding_\n  File ""/Users/dwilliams/Desktop/Anaconda/lib/python2.7/site-packages/sklearn/manifold/mds.py"", line 360, in fit\n    self.fit_transform(X, init=init)\n  File ""/Users/dwilliams/Desktop/Anaconda/lib/python2.7/site-packages/sklearn/manifold/mds.py"", line 395, in fit_transform\neps=self.eps, random_state=self.random_state)\n  File ""/Users/dwilliams/Desktop/Anaconda/lib/python2.7/site-packages/sklearn/manifold/mds.py"", line 242, in smacof\neps=eps, random_state=random_state)\n  File ""/Users/dwilliams/Desktop/Anaconda/lib/python2.7/site-packages/sklearn/manifold/mds.py"", line 73, in _smacof_single\nraise ValueError(""similarities must be symmetric"")\nValueError: similarities must be symmetric\n']";"['import numpy as np\nimport pandas as pd\nfrom sklearn import manifold\nfrom sklearn.metrics import euclidean_distances\n\nseed = np.random.RandomState(seed=3)\ndata = pd.read_csv(\'data/big-file.csv\')\n\n#  start small dont take all the data, \n#  its about 200k records\nsubset = data[:10000]\nsimilarities = euclidean_distances(subset)\n\nmds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, \n      random_state=seed, dissimilarity=""precomputed"", n_jobs=1)\n\npos = mds.fit(similarities).embedding_\n', 'Traceback (most recent call last):\n  File ""demo/mds-demo.py"", line 18, in <module>\n    pos = mds.fit(similarities).embedding_\n  File ""/Users/dwilliams/Desktop/Anaconda/lib/python2.7/site-packages/sklearn/manifold/mds.py"", line 360, in fit\n    self.fit_transform(X, init=init)\n  File ""/Users/dwilliams/Desktop/Anaconda/lib/python2.7/site-packages/sklearn/manifold/mds.py"", line 395, in fit_transform\neps=self.eps, random_state=self.random_state)\n  File ""/Users/dwilliams/Desktop/Anaconda/lib/python2.7/site-packages/sklearn/manifold/mds.py"", line 242, in smacof\neps=eps, random_state=random_state)\n  File ""/Users/dwilliams/Desktop/Anaconda/lib/python2.7/site-packages/sklearn/manifold/mds.py"", line 73, in _smacof_single\nraise ValueError(""similarities must be symmetric"")\nValueError: similarities must be symmetric\n']";"['import numpy as np\nimport pandas as pd\nfrom sklearn import manifold\nfrom sklearn.metrics import euclidean_distances\n\nseed = np.random.RandomState(seed=3)\ndata = pd.read_csv(\'data/big-file.csv\')\n\n#  start small dont take all the data, \n#  its about 200k records\nsubset = data[:10000]\nsimilarities = euclidean_distances(subset)\n\n\npos = mds.fit(similarities).embedding_\nraise ValueError(""similarities must be symmetric"")\n']";"['import numpy as np\nimport pandas as pd\nfrom sklearn import manifold\nfrom sklearn.metrics import euclidean_distances\n\nseed = np.random.RandomState(seed=3)\ndata = pd.read_csv(\'data/big-file.csv\')\n\n#  start small dont take all the data, \n#  its about 200k records\nsubset = data[:10000]\nsimilarities = euclidean_distances(subset)\n\n\npos = mds.fit(similarities).embedding_\nraise ValueError(""similarities must be symmetric"")\n']";False;"['import pandas as pd\nimport numpy as np\nimport pandas as pd\nfrom sklearn import manifold\nfrom sklearn.metrics import euclidean_distances\n\nseed = np.random.RandomState(seed=3)\ndata = pd.read_csv(\'data/big-file.csv\')\n\n#  start small dont take all the data, \n#  its about 200k records\nsubset = data[:10000]\nsimilarities = euclidean_distances(subset)\n\n\npos = mds.fit(similarities).embedding_\nraise ValueError(""similarities must be symmetric"")\n']";False;0;2;"[""No module named 'sklearn'"", ""No module named 'scipy'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'sklearn'"", ""No module named 'scipy'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'sklearn'"", ""No module named 'scipy'""]";['ImportError', 'ImportError']
353;353;353;353;1.0;1;16997048;;1;12;<python><indexing><pandas><dataframe><hdfstore>;How does one append large amounts of data to a Pandas HDFStore and get a natural unique index?;9423.0;"['if hd_file_name:\n        """"""\n        HDF5 output file specified.\n        """"""\n\n        hdf_output = pd.HDFStore(hd_file_name, complib=\'blosc\')\n        print hdf_output\n\n        columns = [\'source\', \'ip\', \'unknown\', \'user\', \'timestamp\', \'http_verb\', \'path\', \'protocol\', \'http_result\', \n                   \'response_size\', \'referrer\', \'user_agent\', \'response_time\']\n\n        source_name = str(log_file.name.rsplit(\'/\')[-1])   # HDF5 Tables don\'t play nice with unicode so explicit str(). :(\n\n        batch = []\n\n        for count, line in enumerate(log_file,1):\n            data = parse_line(line, rejected_output = reject_output)\n\n            # Add our source file name to the beginning.\n            data.insert(0, source_name )    \n            batch.append(data)\n\n            if not (count % 10):\n                df = pd.DataFrame( batch, columns = columns )\n                hdf_output.append(KEY_NAME, df)\n                batch = []\n\n        if (count % 10):\n            df = pd.DataFrame( batch, columns = columns )\n            hdf_output.append(KEY_NAME, df)\n']";"['if hd_file_name:\n        """"""\n        HDF5 output file specified.\n        """"""\n\n        hdf_output = pd.HDFStore(hd_file_name, complib=\'blosc\')\n        print hdf_output\n\n        columns = [\'source\', \'ip\', \'unknown\', \'user\', \'timestamp\', \'http_verb\', \'path\', \'protocol\', \'http_result\', \n                   \'response_size\', \'referrer\', \'user_agent\', \'response_time\']\n\n        source_name = str(log_file.name.rsplit(\'/\')[-1])   # HDF5 Tables don\'t play nice with unicode so explicit str(). :(\n\n        batch = []\n\n        for count, line in enumerate(log_file,1):\n            data = parse_line(line, rejected_output = reject_output)\n\n            # Add our source file name to the beginning.\n            data.insert(0, source_name )    \n            batch.append(data)\n\n            if not (count % 10):\n                df = pd.DataFrame( batch, columns = columns )\n                hdf_output.append(KEY_NAME, df)\n                batch = []\n\n        if (count % 10):\n            df = pd.DataFrame( batch, columns = columns )\n            hdf_output.append(KEY_NAME, df)\n']";"['if hd_file_name:\n        """"""\n        HDF5 output file specified.\n        """"""\n\n        hdf_output = pd.HDFStore(hd_file_name, complib=\'blosc\')\n        print hdf_output\n\n        columns = [\'source\', \'ip\', \'unknown\', \'user\', \'timestamp\', \'http_verb\', \'path\', \'protocol\', \'http_result\', \n                   \'response_size\', \'referrer\', \'user_agent\', \'response_time\']\n\n        source_name = str(log_file.name.rsplit(\'/\')[-1])   # HDF5 Tables don\'t play nice with unicode so explicit str(). :(\n\n        batch = []\n\n        for count, line in enumerate(log_file,1):\n            data = parse_line(line, rejected_output = reject_output)\n\n            # Add our source file name to the beginning.\n            data.insert(0, source_name )    \n            batch.append(data)\n\n            if not (count % 10):\n                df = pd.DataFrame( batch, columns = columns )\n                hdf_output.append(KEY_NAME, df)\n                batch = []\n\n        if (count % 10):\n            df = pd.DataFrame( batch, columns = columns )\n            hdf_output.append(KEY_NAME, df)\n']";['\n\n\n\n\n\n            # Add our source file name to the beginning.\n\n\n'];['\n\n\n\n\n\n            # Add our source file name to the beginning.\n\n\n'];False;['import pandas as pd\n\n\n\n\n\n\n            # Add our source file name to the beginning.\n\n\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;['File test.h5 does not exist'];['FileNotFoundError'];0;1;['File test.h5 does not exist'];['FileNotFoundError']
354;354;354;354;3.0;2;17001389;;1;113;<python><documentation><pandas>;pandas resample documentation;59119.0;[''];[];"['resample', ""'D'"", ""'xMin'"", ""'xL'"", ""'first'"", 'np.max', ""'last'"", ""'mean'"", ""'n1n2n3n4...nx'"", 'pandas.resample']";[''];[''];False;['import pandas as pd\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
355;355;355;355;1.0;0;17004985;;1;14;<python><pandas>;How do I create pandas DataFrame (with index or multiindex) from list of namedtuple instances?;17744.0;"["">>> from collections import namedtuple\n>>> import pandas\n\n>>> Price = namedtuple('Price', 'ticker date price')\n>>> a = Price('GE', '2010-01-01', 30.00)\n>>> b = Price('GE', '2010-01-02', 31.00)\n>>> l = [a, b]\n>>> df = pandas.DataFrame.from_records(l, index='ticker')\nTraceback (most recent call last)\n...\nKeyError: 'ticker'\n>>> df2 = pandas.DataFrame.from_records(l, index=['ticker', 'date'])\n>>> df2\n\n         0           1   2\nticker  GE  2010-01-01  30\ndate    GE  2010-01-02  31\n""]";"["">>> from collections import namedtuple\n>>> import pandas\n\n>>> Price = namedtuple('Price', 'ticker date price')\n>>> a = Price('GE', '2010-01-01', 30.00)\n>>> b = Price('GE', '2010-01-02', 31.00)\n>>> l = [a, b]\n>>> df = pandas.DataFrame.from_records(l, index='ticker')\nTraceback (most recent call last)\n...\nKeyError: 'ticker'\n"", "">>> df2 = pandas.DataFrame.from_records(l, index=['ticker', 'date'])\n>>> df2\n\n         0           1   2\nticker  GE  2010-01-01  30\ndate    GE  2010-01-02  31\n""]";"["">>> from collections import namedtuple\n>>> import pandas\n\n>>> Price = namedtuple('Price', 'ticker date price')\n>>> a = Price('GE', '2010-01-01', 30.00)\n>>> b = Price('GE', '2010-01-02', 31.00)\n>>> l = [a, b]\n>>> df = pandas.DataFrame.from_records(l, index='ticker')\nTraceback (most recent call last)\n...\nKeyError: 'ticker'\n"", "">>> df2 = pandas.DataFrame.from_records(l, index=['ticker', 'date'])\n>>> df2\n\n         0           1   2\nticker  GE  2010-01-01  30\ndate    GE  2010-01-02  31\n"", ""['ticker', 'date']"", 'set_index']";['\n...\n\n'];['\n...\n\n'];False;['import pandas as pd\n\n...\n\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'a' is not defined""]";['NameError'];0;1;"[""name 'a' is not defined""]";['NameError']
356;356;356;356;4.0;0;17063458;;1;56;<python><python-2.7><pandas>;Reading an Excel file in python using pandas;114839.0;['newFile = pd.ExcelFile(PATH\\FileName.xlsx)\nParsedData = pd.io.parsers.ExcelFile.parse(newFile)\n'];['newFile = pd.ExcelFile(PATH\\FileName.xlsx)\nParsedData = pd.io.parsers.ExcelFile.parse(newFile)\n'];['newFile = pd.ExcelFile(PATH\\FileName.xlsx)\nParsedData = pd.io.parsers.ExcelFile.parse(newFile)\n'];['ParsedData = pd.io.parsers.ExcelFile.parse(newFile)\n'];['import pandas as pd\nParsedData = pd.io.parsers.ExcelFile.parse(newFile)\n'];True;['import pandas as pd\nParsedData = pd.io.parsers.ExcelFile.parse(newFile)\n'];False;0;3;"[""name 'Index' is not defined"", ""name 'xlsx' is not defined"", ""[Errno 2] No such file or directory: 'your_xls_xlsx_filename'""]";['NameError', 'NameError', 'FileNotFoundError'];0;3;"[""name 'Index' is not defined"", ""name 'xlsx' is not defined"", ""[Errno 2] No such file or directory: 'your_xls_xlsx_filename'""]";['NameError', 'NameError', 'FileNotFoundError'];0;3;"[""name 'Index' is not defined"", ""name 'xlsx' is not defined"", ""[Errno 2] No such file or directory: 'your_xls_xlsx_filename'""]";['NameError', 'NameError', 'FileNotFoundError']
357;357;357;357;3.0;2;17068269;;1;18;<python><pandas>;Return max of zero or value for a pandas DataFrame column;6301.0;"[""df['value'][df['value'] < 0] = 0\n""]";"[""df['value'][df['value'] < 0] = 0\n""]";"[""df['value'][df['value'] < 0] = 0\n""]";"[""df['value'][df['value'] < 0] = 0\n""]";"[""df['value'][df['value'] < 0] = 0\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf['value'][df['value'] < 0] = 0\n""]";True;1;3;"[""name 'DataFrame' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""name 'randn' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""name 'DataFrame' is not defined"", 'Sucess', ""'value'""]";['NameError', 'Sucess', 'KeyError']
358;358;358;358;9.0;0;17071871;;1;375;<python><pandas><dataframe>;Select rows from a DataFrame based on values in a column in pandas;420943.0;['select * from table where colume_name = some_value. \n'];['select * from table where colume_name = some_value. \n'];['select * from table where colume_name = some_value. \n'];[''];[''];False;['import pandas as pd\n'];False;0;5;"[""name 'df' is not defined"", ""name 'd' is not defined"", ""name 'df' is not defined"", ""name 'table' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError', 'NameError'];0;5;"[""name 'df' is not defined"", ""name 'd' is not defined"", ""name 'df' is not defined"", ""name 'table' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError', 'NameError'];0;5;"[""name 'df' is not defined"", ""name 'd' is not defined"", ""name 'df' is not defined"", ""name 'table' is not defined"", ""'DataFrame' object has no attribute 'a'""]";['NameError', 'NameError', 'NameError', 'NameError', 'AttributeError']
359;359;359;359;1.0;0;17073150;;1;11;<date><pandas>;filtering a pandas Dataframe based on date value;5012.0;[' instrument         type   from_date  to_date   \n0   96000001    W/D & V/L  19951227  19960102\n1   96000002   DEED TRUST  19951227  19960102\n2   96000003  WARNTY DEED  19951228  19960102\n3   96000004   DEED TRUST  19951228  19960102\n4   96000005    W/D & V/L  19951228  19960102\n'];[' instrument         type   from_date  to_date   \n0   96000001    W/D & V/L  19951227  19960102\n1   96000002   DEED TRUST  19951227  19960102\n2   96000003  WARNTY DEED  19951228  19960102\n3   96000004   DEED TRUST  19951228  19960102\n4   96000005    W/D & V/L  19951228  19960102\n'];['read_csv', ' instrument         type   from_date  to_date   \n0   96000001    W/D & V/L  19951227  19960102\n1   96000002   DEED TRUST  19951227  19960102\n2   96000003  WARNTY DEED  19951228  19960102\n3   96000004   DEED TRUST  19951228  19960102\n4   96000005    W/D & V/L  19951228  19960102\n', '19951227', 'from_date', 'from_date', '19951227', 'to_date', '19960102'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'from_date'""]";['KeyError']
360;360;360;360;2.0;0;17084579;;1;19;<pandas>;How to remove levels from a multi-indexed dataframe?;18302.0;"[""In [1]: df = pd.DataFrame([8, 9],\n                          index=pd.MultiIndex.from_tuples([(1, 1, 1),\n                                                           (1, 3, 2)]),\n                          columns=['A'])\n\nIn [2] df\nOut[2]: \n       A\n1 1 1  8\n  3 2  9\nIn [3]: pd.DataFrame(df.values,\n                     index=df.index.droplevel(2),\n                     columns=df.columns)\nOut[3]: \n     A\n1 1  8\n  3  9\n""]";"[""In [1]: df = pd.DataFrame([8, 9],\n                          index=pd.MultiIndex.from_tuples([(1, 1, 1),\n                                                           (1, 3, 2)]),\n                          columns=['A'])\n\nIn [2] df\nOut[2]: \n       A\n1 1 1  8\n  3 2  9\n"", 'In [3]: pd.DataFrame(df.values,\n                     index=df.index.droplevel(2),\n                     columns=df.columns)\nOut[3]: \n     A\n1 1  8\n  3  9\n']";"[""In [1]: df = pd.DataFrame([8, 9],\n                          index=pd.MultiIndex.from_tuples([(1, 1, 1),\n                                                           (1, 3, 2)]),\n                          columns=['A'])\n\nIn [2] df\nOut[2]: \n       A\n1 1 1  8\n  3 2  9\n"", 'In [3]: pd.DataFrame(df.values,\n                     index=df.index.droplevel(2),\n                     columns=df.columns)\nOut[3]: \n     A\n1 1  8\n  3  9\n']";['\n'];['\n'];False;['import pandas as pd\n\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'Index' object has no attribute 'droplevel'"", 'Too many levels: Index has only 1 level, not 3']";['AttributeError', 'IndexError']
361;361;361;361;4.0;0;17091769;;1;54;<python><dataframe><row><pandas>;Python pandas: fill a dataframe row by row;53561.0;"["">>> df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'])\n>>> df\n     a    b    c    d\nx  NaN  NaN  NaN  NaN\ny  NaN  NaN  NaN  NaN\nz  NaN  NaN  NaN  NaN\n>>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df['y'] = y\nAssertionError: Length of values does not match length of index\n>>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.join(y)\nAttributeError: 'builtin_function_or_method' object has no attribute 'is_unique'\n>>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.set_value(index='y', value=y)\nTypeError: set_value() takes exactly 4 arguments (3 given)\n>>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.append(y)\nException: Can only append a Series if ignore_index=True\n>>> df.append(y, ignore_index=True)\n     a    b    c    d\n0  NaN  NaN  NaN  NaN\n1  NaN  NaN  NaN  NaN\n2  NaN  NaN  NaN  NaN\n3    1    5    2    3\n>>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.ix['y'] = y\n>>> df\n                                  a                                 b  \\\nx                               NaN                               NaN\ny  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}\nz                               NaN                               NaN\n\n                                  c                                 d\nx                               NaN                               NaN\ny  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}\nz                               NaN                               NaN\n""]";"["">>> df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'])\n>>> df\n     a    b    c    d\nx  NaN  NaN  NaN  NaN\ny  NaN  NaN  NaN  NaN\nz  NaN  NaN  NaN  NaN\n"", "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df['y'] = y\nAssertionError: Length of values does not match length of index\n"", "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.join(y)\nAttributeError: 'builtin_function_or_method' object has no attribute 'is_unique'\n"", "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.set_value(index='y', value=y)\nTypeError: set_value() takes exactly 4 arguments (3 given)\n"", "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.append(y)\nException: Can only append a Series if ignore_index=True\n"", '>>> df.append(y, ignore_index=True)\n     a    b    c    d\n0  NaN  NaN  NaN  NaN\n1  NaN  NaN  NaN  NaN\n2  NaN  NaN  NaN  NaN\n3    1    5    2    3\n', "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.ix['y'] = y\n>>> df\n                                  a                                 b  \\\nx                               NaN                               NaN\ny  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}\nz                               NaN                               NaN\n\n                                  c                                 d\nx                               NaN                               NaN\ny  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}\nz                               NaN                               NaN\n""]";"['pandas.DataFrame', "">>> df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'])\n>>> df\n     a    b    c    d\nx  NaN  NaN  NaN  NaN\ny  NaN  NaN  NaN  NaN\nz  NaN  NaN  NaN  NaN\n"", 'pandas.Series', "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df['y'] = y\nAssertionError: Length of values does not match length of index\n"", "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.join(y)\nAttributeError: 'builtin_function_or_method' object has no attribute 'is_unique'\n"", "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.set_value(index='y', value=y)\nTypeError: set_value() takes exactly 4 arguments (3 given)\n"", "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.append(y)\nException: Can only append a Series if ignore_index=True\n"", '>>> df.append(y, ignore_index=True)\n     a    b    c    d\n0  NaN  NaN  NaN  NaN\n1  NaN  NaN  NaN  NaN\n2  NaN  NaN  NaN  NaN\n3    1    5    2    3\n', "">>> y = {'a':1, 'b':5, 'c':2, 'd':3} \n>>> df.ix['y'] = y\n>>> df\n                                  a                                 b  \\\nx                               NaN                               NaN\ny  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}\nz                               NaN                               NaN\n\n                                  c                                 d\nx                               NaN                               NaN\ny  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}\nz                               NaN                               NaN\n""]";['\n'];['\n'];False;['import pandas as pd\n\n'];False;1;3;"[""name 'pandas' is not defined"", 'Sucess', ""name 'pd' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""name 'pandas' is not defined"", 'Sucess', ""module 'pandas' has no attribute 'Dataframe'""]";['NameError', 'Sucess', 'AttributeError'];1;3;"[""name 'pandas' is not defined"", 'Sucess', ""module 'pandas' has no attribute 'Dataframe'""]";['NameError', 'Sucess', 'AttributeError']
362;362;362;362;3.0;5;17092671;;1;14;<python><csv><dataframe><pandas>;Python pandas: output dataframe to csv with integers;16184.0;"[""df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'], dtype=int)\nx = pandas.Series([10,10,10], index=['a','b','d'], dtype=int)\ny = pandas.Series([1,5,2,3], index=['a','b','c','d'], dtype=int)\nz = pandas.Series([1,2,3,4], index=['a','b','c','d'], dtype=int)\ndf.loc['x']=x; df.loc['y']=y; df.loc['z']=z\n>>> df\n    a   b    c   d\nx  10  10  NaN  10\ny   1   5    2   3\nz   1   2    3   4\n>>> df.to_csv('test.csv', sep='\\t', na_rep='0', dtype=int)\n>>> for l in open('test.csv'): print l.strip('\\n')\n        a       b       c       d\nx       10.0    10.0    0       10.0\ny       1       5       2       3\nz       1       2       3       4\ndef lines_as_integer(path):\n    handle = open(path)\n    yield handle.next()\n    for line in handle:\n        line = line.split()\n        label = line[0]\n        values = map(float, line[1:])\n        values = map(int, values)\n        yield label + '\\t' + '\\t'.join(map(str,values)) + '\\n'\nhandle = open(path_table_int, 'w')\nhandle.writelines(lines_as_integer(path_table_float))\nhandle.close()\n""]";"[""df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'], dtype=int)\nx = pandas.Series([10,10,10], index=['a','b','d'], dtype=int)\ny = pandas.Series([1,5,2,3], index=['a','b','c','d'], dtype=int)\nz = pandas.Series([1,2,3,4], index=['a','b','c','d'], dtype=int)\ndf.loc['x']=x; df.loc['y']=y; df.loc['z']=z\n"", '>>> df\n    a   b    c   d\nx  10  10  NaN  10\ny   1   5    2   3\nz   1   2    3   4\n', "">>> df.to_csv('test.csv', sep='\\t', na_rep='0', dtype=int)\n>>> for l in open('test.csv'): print l.strip('\\n')\n        a       b       c       d\nx       10.0    10.0    0       10.0\ny       1       5       2       3\nz       1       2       3       4\n"", ""def lines_as_integer(path):\n    handle = open(path)\n    yield handle.next()\n    for line in handle:\n        line = line.split()\n        label = line[0]\n        values = map(float, line[1:])\n        values = map(int, values)\n        yield label + '\\t' + '\\t'.join(map(str,values)) + '\\n'\nhandle = open(path_table_int, 'w')\nhandle.writelines(lines_as_integer(path_table_float))\nhandle.close()\n""]";"['pandas.DataFrame', 'float', 'int', ""df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'], dtype=int)\nx = pandas.Series([10,10,10], index=['a','b','d'], dtype=int)\ny = pandas.Series([1,5,2,3], index=['a','b','c','d'], dtype=int)\nz = pandas.Series([1,2,3,4], index=['a','b','c','d'], dtype=int)\ndf.loc['x']=x; df.loc['y']=y; df.loc['z']=z\n"", '>>> df\n    a   b    c   d\nx  10  10  NaN  10\ny   1   5    2   3\nz   1   2    3   4\n', "">>> df.to_csv('test.csv', sep='\\t', na_rep='0', dtype=int)\n>>> for l in open('test.csv'): print l.strip('\\n')\n        a       b       c       d\nx       10.0    10.0    0       10.0\ny       1       5       2       3\nz       1       2       3       4\n"", ""def lines_as_integer(path):\n    handle = open(path)\n    yield handle.next()\n    for line in handle:\n        line = line.split()\n        label = line[0]\n        values = map(float, line[1:])\n        values = map(int, values)\n        yield label + '\\t' + '\\t'.join(map(str,values)) + '\\n'\nhandle = open(path_table_int, 'w')\nhandle.writelines(lines_as_integer(path_table_float))\nhandle.close()\n""]";"[""df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'], dtype=int)\nx = pandas.Series([10,10,10], index=['a','b','d'], dtype=int)\ny = pandas.Series([1,5,2,3], index=['a','b','c','d'], dtype=int)\nz = pandas.Series([1,2,3,4], index=['a','b','c','d'], dtype=int)\ndf.loc['x']=x; df.loc['y']=y; df.loc['z']=z\nhandle = open(path_table_int, 'w')\nhandle.writelines(lines_as_integer(path_table_float))\nhandle.close()\n""]";"[""df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'], dtype=int)\nx = pandas.Series([10,10,10], index=['a','b','d'], dtype=int)\ny = pandas.Series([1,5,2,3], index=['a','b','c','d'], dtype=int)\nz = pandas.Series([1,2,3,4], index=['a','b','c','d'], dtype=int)\ndf.loc['x']=x; df.loc['y']=y; df.loc['z']=z\nhandle = open(path_table_int, 'w')\nhandle.writelines(lines_as_integer(path_table_float))\nhandle.close()\n""]";False;"[""import pandas as pd\ndf = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'], dtype=int)\nx = pandas.Series([10,10,10], index=['a','b','d'], dtype=int)\ny = pandas.Series([1,5,2,3], index=['a','b','c','d'], dtype=int)\nz = pandas.Series([1,2,3,4], index=['a','b','c','d'], dtype=int)\ndf.loc['x']=x; df.loc['y']=y; df.loc['z']=z\nhandle = open(path_table_int, 'w')\nhandle.writelines(lines_as_integer(path_table_float))\nhandle.close()\n""]";False;2;3;"['Sucess', ""name 'df' is not defined"", 'Sucess']";['Sucess', 'NameError', 'Sucess'];2;3;"['Sucess', ""name 'df' is not defined"", 'Sucess']";['Sucess', 'NameError', 'Sucess'];2;3;"['Sucess', ""name 'convert' is not defined"", 'Sucess']";['Sucess', 'NameError', 'Sucess']
363;363;363;363;8.0;1;17095101;;1;48;<python><html><dataframe><pandas>;Outputting difference in two Pandas dataframes side by side - highlighting the difference;51910.0;"['""StudentRoster Jan-1"":\nid   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.11                     False                Graduated\n113  Zoe    4.12                     True       \n\n""StudentRoster Jan-2"":\nid   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.21                     False                Graduated\n113  Zoe    4.12                     False                On vacation\n""StudentRoster Difference Jan-1 - Jan-2"":  \nid   Name   score                    isEnrolled           Comment\n112  Nick   was 1.11| now 1.21       False                Graduated\n113  Zoe    4.12                     was True | now False was """" | now   ""On   vacation""\n']";"['""StudentRoster Jan-1"":\nid   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.11                     False                Graduated\n113  Zoe    4.12                     True       \n\n""StudentRoster Jan-2"":\nid   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.21                     False                Graduated\n113  Zoe    4.12                     False                On vacation\n', '""StudentRoster Difference Jan-1 - Jan-2"":  \nid   Name   score                    isEnrolled           Comment\n112  Nick   was 1.11| now 1.21       False                Graduated\n113  Zoe    4.12                     was True | now False was """" | now   ""On   vacation""\n']";"['""StudentRoster Jan-1"":\nid   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.11                     False                Graduated\n113  Zoe    4.12                     True       \n\n""StudentRoster Jan-2"":\nid   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.21                     False                Graduated\n113  Zoe    4.12                     False                On vacation\n', '""StudentRoster Difference Jan-1 - Jan-2"":  \nid   Name   score                    isEnrolled           Comment\n112  Nick   was 1.11| now 1.21       False                Graduated\n113  Zoe    4.12                     was True | now False was """" | now   ""On   vacation""\n']";['\n'];['\n'];False;['import pandas as pd\n\n'];False;0;4;"[""name 'df1' is not defined"", ""name 'texts' is not defined"", ""name 'texts' is not defined"", ""name 'DF1' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""name 'df1' is not defined"", ""name 'texts' is not defined"", ""name 'texts' is not defined"", ""name 'DF1' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""name 'np' is not defined"", ""name 'texts' is not defined"", ""name 'texts' is not defined"", ""name 'DF1' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError']
364;364;364;364;2.0;5;17097236;;1;29;<python><replace><pandas><nan><nonetype>;How to replace values with None in Pandas data frame in Python?;53211.0;"[""df = DataFrame(['-',3,2,5,1,-5,-1,'-',9])\ndf.replace('-', 0)\ndf.replace('-', None)\n0\n0   - // this isn't replaced\n1   3\n2   2\n3   5\n4   1\n5  -5\n6  -1\n7  -1 // this is changed to `-1`...\n8   9\n""]";"[""df = DataFrame(['-',3,2,5,1,-5,-1,'-',9])\ndf.replace('-', 0)\n"", ""df.replace('-', None)\n"", ""0\n0   - // this isn't replaced\n1   3\n2   2\n3   5\n4   1\n5  -5\n6  -1\n7  -1 // this is changed to `-1`...\n8   9\n""]";"['None', ""df.replace('pre', 'post')"", 'None', ""df = DataFrame(['-',3,2,5,1,-5,-1,'-',9])\ndf.replace('-', 0)\n"", ""df.replace('-', None)\n"", ""0\n0   - // this isn't replaced\n1   3\n2   2\n3   5\n4   1\n5  -5\n6  -1\n7  -1 // this is changed to `-1`...\n8   9\n"", 'NaN', 'None', ""'-'"", 'NaN', 'NaN', 'None']";"[""df = DataFrame(['-',3,2,5,1,-5,-1,'-',9])\ndf.replace('-', 0)\ndf.replace('-', None)\n0\n5  -5\n6  -1\n""]";"[""from pandas import DataFrame\ndf = DataFrame(['-',3,2,5,1,-5,-1,'-',9])\ndf.replace('-', 0)\ndf.replace('-', None)\n0\n5  -5\n6  -1\n""]";True;"[""import pandas as pd\ndf = DataFrame(['-',3,2,5,1,-5,-1,'-',9])\ndf.replace('-', 0)\ndf.replace('-', None)\n0\n5  -5\n6  -1\n""]";False;0;2;"[""name 'df' is not defined"", ""name 'data' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'data' is not defined""]";['NameError', 'NameError'];1;2;"[""name 'np' is not defined"", 'Sucess']";['NameError', 'Sucess']
365;365;365;365;3.0;0;17097643;;1;23;<python><pandas><contains>;"search for ""does-not-contain"" on a dataframe in pandas";13033.0;[''];[];"['df[""col""].str.contains(word)', '!(df[""col""].str.contains(word))', 'DataFrame']";[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""'col'"", 'Sucess']";['KeyError', 'Sucess']
366;366;366;366;5.0;0;17098654;;1;117;<python><pandas><dataframe>;How to store a dataframe using Pandas;78448.0;[''];[];['CSV'];[''];[''];False;['import pandas as pd\n'];False;2;4;"[""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess', 'NameError', 'Sucess'];2;4;"[""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess', 'NameError', 'Sucess'];3;4;"[""name 'df' is not defined"", 'Sucess', 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess', 'Sucess']
367;367;367;367;4.0;0;17114904;;1;11;<python><replace><dataframe><pandas>;python pandas replacing strings in dataframe with numbers;14060.0;['ds_r\n  respondent  brand engine  country  aware  aware_2  aware_3  age tesst   set\n0          a  volvo      p      swe      1        0        1   23   set   set\n1          b  volvo   None      swe      0        0        1   45   set   set\n2          c    bmw      p       us      0        0        1   56  test  test\n3          d    bmw      p       us      0        1        1   43  test  test\n4          e    bmw      d  germany      1        0        1   34   set   set\n5          f   audi      d  germany      1        0        1   59   set   set\n6          g  volvo      d      swe      1        0        0   65  test   set\n7          h   audi      d      swe      1        0        0   78  test   set\n8          i  volvo      d       us      1        1        1   32   set   set\n ds_r\n  respondent  brand engine  country  aware  aware_2  aware_3  age  tesst  set\n0          a  volvo      p      swe      1        0        1   23      1    1\n1          b  volvo   None      swe      0        0        1   45      1    1\n2          c    bmw      p       us      0        0        1   56      2    2\n3          d    bmw      p       us      0        1        1   43      2    2\n4          e    bmw      d  germany      1        0        1   34      1    1\n5          f   audi      d  germany      1        0        1   59      1    1\n6          g  volvo      d      swe      1        0        0   65      2    1\n7          h   audi      d      swe      1        0        0   78      2    1\n8          i  volvo      d       us      1        1        1   32      1    1\n'];['ds_r\n  respondent  brand engine  country  aware  aware_2  aware_3  age tesst   set\n0          a  volvo      p      swe      1        0        1   23   set   set\n1          b  volvo   None      swe      0        0        1   45   set   set\n2          c    bmw      p       us      0        0        1   56  test  test\n3          d    bmw      p       us      0        1        1   43  test  test\n4          e    bmw      d  germany      1        0        1   34   set   set\n5          f   audi      d  germany      1        0        1   59   set   set\n6          g  volvo      d      swe      1        0        0   65  test   set\n7          h   audi      d      swe      1        0        0   78  test   set\n8          i  volvo      d       us      1        1        1   32   set   set\n', ' ds_r\n  respondent  brand engine  country  aware  aware_2  aware_3  age  tesst  set\n0          a  volvo      p      swe      1        0        1   23      1    1\n1          b  volvo   None      swe      0        0        1   45      1    1\n2          c    bmw      p       us      0        0        1   56      2    2\n3          d    bmw      p       us      0        1        1   43      2    2\n4          e    bmw      d  germany      1        0        1   34      1    1\n5          f   audi      d  germany      1        0        1   59      1    1\n6          g  volvo      d      swe      1        0        0   65      2    1\n7          h   audi      d      swe      1        0        0   78      2    1\n8          i  volvo      d       us      1        1        1   32      1    1\n'];['ds_r\n  respondent  brand engine  country  aware  aware_2  aware_3  age tesst   set\n0          a  volvo      p      swe      1        0        1   23   set   set\n1          b  volvo   None      swe      0        0        1   45   set   set\n2          c    bmw      p       us      0        0        1   56  test  test\n3          d    bmw      p       us      0        1        1   43  test  test\n4          e    bmw      d  germany      1        0        1   34   set   set\n5          f   audi      d  germany      1        0        1   59   set   set\n6          g  volvo      d      swe      1        0        0   65  test   set\n7          h   audi      d      swe      1        0        0   78  test   set\n8          i  volvo      d       us      1        1        1   32   set   set\n', ' ds_r\n  respondent  brand engine  country  aware  aware_2  aware_3  age  tesst  set\n0          a  volvo      p      swe      1        0        1   23      1    1\n1          b  volvo   None      swe      0        0        1   45      1    1\n2          c    bmw      p       us      0        0        1   56      2    2\n3          d    bmw      p       us      0        1        1   43      2    2\n4          e    bmw      d  germany      1        0        1   34      1    1\n5          f   audi      d  germany      1        0        1   59      1    1\n6          g  volvo      d      swe      1        0        0   65      2    1\n7          h   audi      d      swe      1        0        0   78      2    1\n8          i  volvo      d       us      1        1        1   32      1    1\n'];['ds_r\n'];['ds_r\n'];False;['import pandas as pd\nds_r\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
368;368;368;368;3.0;0;17116814;;1;95;<python><pandas><dataframe>;pandas: How do I split text in a column into multiple rows?;70528.0;['CustNum  CustomerName     ItemQty  Item   Seatblocks                 ItemExt\n32363    McCartney, Paul      3     F04    2:218:10:4,6                   60\n31316    Lennon, John        25     F01    1:13:36:1,12 1:13:37:1,13     300\n'];['CustNum  CustomerName     ItemQty  Item   Seatblocks                 ItemExt\n32363    McCartney, Paul      3     F04    2:218:10:4,6                   60\n31316    Lennon, John        25     F01    1:13:36:1,12 1:13:37:1,13     300\n'];"['CustNum  CustomerName     ItemQty  Item   Seatblocks                 ItemExt\n32363    McCartney, Paul      3     F04    2:218:10:4,6                   60\n31316    Lennon, John        25     F01    1:13:36:1,12 1:13:37:1,13     300\n', ""(' ')"", ""(':')"", 'Seatblocks', 'Seatblocks', 'text-to-columns']";[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'df' is not defined"", ""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""No module named 'scipy'"", ""name 'df' is not defined""]";['NameError', 'ImportError', 'NameError'];0;3;"[""'Seatblocks'"", ""No module named 'scipy'"", ""name 'df' is not defined""]";['KeyError', 'ImportError', 'NameError']
369;369;369;369;1.0;0;17128302;;1;14;<python><pandas>;Python/pandas idiom for if/then/else;17587.0;"[""df['var1000'][df['type']==7] = 0\ndf['var1001'][df['type']==7] = 0\ndf['var1002'][df['type']==7] = 0\n...\ndf['var1099'][df['type']==7] = 0\nif (df['type']==7):\n    df['var1000'] = 0\n    df['var1001'] = 0\n    df['var1002'] = 0\n    ...\n    df['var1099'] = 0\n""]";"[""df['var1000'][df['type']==7] = 0\ndf['var1001'][df['type']==7] = 0\ndf['var1002'][df['type']==7] = 0\n...\ndf['var1099'][df['type']==7] = 0\n"", ""if (df['type']==7):\n    df['var1000'] = 0\n    df['var1001'] = 0\n    df['var1002'] = 0\n    ...\n    df['var1099'] = 0\n""]";"[""df['var1000'][df['type']==7] = 0\ndf['var1001'][df['type']==7] = 0\ndf['var1002'][df['type']==7] = 0\n...\ndf['var1099'][df['type']==7] = 0\n"", ""if (df['type']==7):\n    df['var1000'] = 0\n    df['var1001'] = 0\n    df['var1002'] = 0\n    ...\n    df['var1099'] = 0\n""]";"[""df['var1000'][df['type']==7] = 0\ndf['var1001'][df['type']==7] = 0\ndf['var1002'][df['type']==7] = 0\n...\ndf['var1099'][df['type']==7] = 0\n""]";"[""df['var1000'][df['type']==7] = 0\ndf['var1001'][df['type']==7] = 0\ndf['var1002'][df['type']==7] = 0\n...\ndf['var1099'][df['type']==7] = 0\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf['var1000'][df['type']==7] = 0\ndf['var1001'][df['type']==7] = 0\ndf['var1002'][df['type']==7] = 0\n...\ndf['var1099'][df['type']==7] = 0\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;['\n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated'];['DeprecationWarning']
370;370;370;370;2.0;0;17134716;;1;92;<python><pandas><dataframe>;Convert DataFrame column type from string to datetime;85223.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
371;371;371;371;4.0;0;17134942;;1;12;<python><csv><pandas><dataframe>;pandas DataFrame output end of csv;16440.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'f' is not defined"", ""name 'file_name' is not defined""]";['NameError', 'NameError']
372;372;372;372;2.0;1;17141558;;1;65;<python><python-2.7><pandas><data-analysis>;How to sort a dataFrame in python pandas by two or more columns?;73105.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""'b'""]";['NameError', 'KeyError']
373;373;373;373;2.0;0;17142304;;1;33;<python><replace><dataframe><pandas>;replace string/value in entire dataframe;35525.0;['data\n   resp          A          B          C\n0     1       poor       poor       good\n1     2       good       poor       good\n2     3  very good  very good  very good\n3     4       bad        poor       bad \n4     5   very bad   very bad   very bad\n5     6       poor       good   very bad\n6     7       good       good       good\n7     8  very good  very good  very good\n8     9       bad        bad    very bad\n9    10   very bad   very bad   very bad\n data\n   resp  A  B  C\n0      1  3  3  4\n1     2  4  3  4\n2     3  5  5  5\n3     4  2  3  2\n4     5  1  1  1\n5     6  3  4  1\n6     7  4  4  4\n7     8  5  5  5\n8     9  2  2  1\n9    10  1  1  1\n'];['data\n   resp          A          B          C\n0     1       poor       poor       good\n1     2       good       poor       good\n2     3  very good  very good  very good\n3     4       bad        poor       bad \n4     5   very bad   very bad   very bad\n5     6       poor       good   very bad\n6     7       good       good       good\n7     8  very good  very good  very good\n8     9       bad        bad    very bad\n9    10   very bad   very bad   very bad\n', ' data\n   resp  A  B  C\n0      1  3  3  4\n1     2  4  3  4\n2     3  5  5  5\n3     4  2  3  2\n4     5  1  1  1\n5     6  3  4  1\n6     7  4  4  4\n7     8  5  5  5\n8     9  2  2  1\n9    10  1  1  1\n'];['data\n   resp          A          B          C\n0     1       poor       poor       good\n1     2       good       poor       good\n2     3  very good  very good  very good\n3     4       bad        poor       bad \n4     5   very bad   very bad   very bad\n5     6       poor       good   very bad\n6     7       good       good       good\n7     8  very good  very good  very good\n8     9       bad        bad    very bad\n9    10   very bad   very bad   very bad\n', ' data\n   resp  A  B  C\n0      1  3  3  4\n1     2  4  3  4\n2     3  5  5  5\n3     4  2  3  2\n4     5  1  1  1\n5     6  3  4  1\n6     7  4  4  4\n7     8  5  5  5\n8     9  2  2  1\n9    10  1  1  1\n'];['data\n'];['data\n'];False;['import pandas as pd\ndata = pd.DataFrame()\ndata\n'];True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
374;374;374;374;3.0;0;17148787;;1;15;<pandas><histogram><series>;Are there functions to retrieve the histogram counts of a Series in pandas?;6490.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'pd' is not defined"", ""name 'Series' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""name 'Series' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'Series' is not defined""]";['Sucess', 'NameError']
375;375;375;375;3.0;0;17156084;;1;12;<sql><pandas>;unpacking a sql select into a pandas dataframe;6474.0;"[""select instrument, price, date from my_prices;\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: ...\nData columns (total 2 columns):\ninst_1    ...\ninst_2    ...\ndtypes: float64(1), object(1) \n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: ...\nData columns (total 2 columns):\ninstrument    ...\nprice         ...\ndtypes: float64(1), object(1)\n""]";"['select instrument, price, date from my_prices;\n', ""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: ...\nData columns (total 2 columns):\ninst_1    ...\ninst_2    ...\ndtypes: float64(1), object(1) \n"", ""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: ...\nData columns (total 2 columns):\ninstrument    ...\nprice         ...\ndtypes: float64(1), object(1)\n""]";"['select instrument, price, date from my_prices;\n', ""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: ...\nData columns (total 2 columns):\ninst_1    ...\ninst_2    ...\ndtypes: float64(1), object(1) \n"", ""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: ...\nData columns (total 2 columns):\ninstrument    ...\nprice         ...\ndtypes: float64(1), object(1)\n""]";[''];[''];False;['import pandas as pd\n'];False;0;2;"[""No module named 'psycopg2'"", ""No module named 'sqlalchemy'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'psycopg2'"", ""No module named 'sqlalchemy'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'psycopg2'"", ""No module named 'sqlalchemy'""]";['ImportError', 'ImportError']
376;376;376;376;2.0;0;17158382;;1;13;<python><matplotlib><pandas>;Centering x-tick labels between tick marks in matplotlib;5901.0;"[""day_fmt = '%d'   \nmyFmt = mdates.DateFormatter(day_fmt)\nax.xaxis.set_major_formatter(myFmt)    \nax.xaxis.set_major_locator(matplotlib.dates.DayLocator(interval=1))     \n\nfor tick in ax.xaxis.get_major_ticks():\n    tick.tick1line.set_markersize(0)\n    tick.tick2line.set_markersize(0)\n    tick.label1.set_horizontalalignment('center')\n""]";"[""day_fmt = '%d'   \nmyFmt = mdates.DateFormatter(day_fmt)\nax.xaxis.set_major_formatter(myFmt)    \nax.xaxis.set_major_locator(matplotlib.dates.DayLocator(interval=1))     \n\nfor tick in ax.xaxis.get_major_ticks():\n    tick.tick1line.set_markersize(0)\n    tick.tick2line.set_markersize(0)\n    tick.label1.set_horizontalalignment('center')\n""]";"[""day_fmt = '%d'   \nmyFmt = mdates.DateFormatter(day_fmt)\nax.xaxis.set_major_formatter(myFmt)    \nax.xaxis.set_major_locator(matplotlib.dates.DayLocator(interval=1))     \n\nfor tick in ax.xaxis.get_major_ticks():\n    tick.tick1line.set_markersize(0)\n    tick.tick2line.set_markersize(0)\n    tick.label1.set_horizontalalignment('center')\n""]";"[""day_fmt = '%d'   \nmyFmt = mdates.DateFormatter(day_fmt)\nax.xaxis.set_major_formatter(myFmt)    \nax.xaxis.set_major_locator(matplotlib.dates.DayLocator(interval=1))     \n\n""]";"[""day_fmt = '%d'   \nmyFmt = mdates.DateFormatter(day_fmt)\nax.xaxis.set_major_formatter(myFmt)    \nax.xaxis.set_major_locator(matplotlib.dates.DayLocator(interval=1))     \n\n""]";False;"[""import pandas as pd\nday_fmt = '%d'   \nmyFmt = mdates.DateFormatter(day_fmt)\nax.xaxis.set_major_formatter(myFmt)    \nax.xaxis.set_major_locator(matplotlib.dates.DayLocator(interval=1))     \n\n""]";False;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
377;377;377;377;4.0;3;17159207;;1;17;<python><timezone><dataframe><pandas><multi-index>;Change timezone of date-time column in pandas and add as hierarchical index;18265.0;"['>>> import pandas as pd\n>>> dat = pd.DataFrame({\'label\':[\'a\', \'a\', \'a\', \'b\', \'b\', \'b\'], \'datetime\':[\'2011-07-19 07:00:00\', \'2011-07-19 08:00:00\', \'2011-07-19 09:00:00\', \'2011-07-19 07:00:00\', \'2011-07-19 08:00:00\', \'2011-07-19 09:00:00\'], \'value\':range(6)})\n>>> dat.dtypes\n#datetime    object\n#label       object\n#value        int64\n#dtype: object\n>>> times = pd.to_datetime(dat[\'datetime\'])\n>>> times.tz_localize(\'UTC\')\n#Traceback (most recent call last):\n#  File ""<stdin>"", line 1, in <module>\n#  File ""/Users/erikshilts/workspace/schedule-detection/python/pysched/env/lib/python2.7/site-packages/pandas/core/series.py"", line 3170, in tz_localize\n#    raise Exception(\'Cannot tz-localize non-time series\')\n#Exception: Cannot tz-localize non-time series\n>>> times_index = pd.Index(times)\n>>> times_index_pacific = times_index.tz_localize(\'UTC\').tz_convert(\'US/Pacific\')\n>>> times_index_pacific\n#<class \'pandas.tseries.index.DatetimeIndex\'>\n#[2011-07-19 00:00:00, ..., 2011-07-19 02:00:00]\n#Length: 6, Freq: None, Timezone: US/Pacific\n>>> dat_index = dat.set_index([dat[\'label\'], times_index_pacific])\n>>> dat_index\n#                                      datetime label  value\n#label                                                      \n#a     2011-07-19 07:00:00  2011-07-19 07:00:00     a      0\n#      2011-07-19 08:00:00  2011-07-19 08:00:00     a      1\n#      2011-07-19 09:00:00  2011-07-19 09:00:00     a      2\n#b     2011-07-19 07:00:00  2011-07-19 07:00:00     b      3\n#      2011-07-19 08:00:00  2011-07-19 08:00:00     b      4\n#      2011-07-19 09:00:00  2011-07-19 09:00:00     b      5\n']";"["">>> import pandas as pd\n>>> dat = pd.DataFrame({'label':['a', 'a', 'a', 'b', 'b', 'b'], 'datetime':['2011-07-19 07:00:00', '2011-07-19 08:00:00', '2011-07-19 09:00:00', '2011-07-19 07:00:00', '2011-07-19 08:00:00', '2011-07-19 09:00:00'], 'value':range(6)})\n>>> dat.dtypes\n#datetime    object\n#label       object\n#value        int64\n#dtype: object\n"", '>>> times = pd.to_datetime(dat[\'datetime\'])\n>>> times.tz_localize(\'UTC\')\n#Traceback (most recent call last):\n#  File ""<stdin>"", line 1, in <module>\n#  File ""/Users/erikshilts/workspace/schedule-detection/python/pysched/env/lib/python2.7/site-packages/pandas/core/series.py"", line 3170, in tz_localize\n#    raise Exception(\'Cannot tz-localize non-time series\')\n#Exception: Cannot tz-localize non-time series\n', "">>> times_index = pd.Index(times)\n>>> times_index_pacific = times_index.tz_localize('UTC').tz_convert('US/Pacific')\n>>> times_index_pacific\n#<class 'pandas.tseries.index.DatetimeIndex'>\n#[2011-07-19 00:00:00, ..., 2011-07-19 02:00:00]\n#Length: 6, Freq: None, Timezone: US/Pacific\n"", "">>> dat_index = dat.set_index([dat['label'], times_index_pacific])\n>>> dat_index\n#                                      datetime label  value\n#label                                                      \n#a     2011-07-19 07:00:00  2011-07-19 07:00:00     a      0\n#      2011-07-19 08:00:00  2011-07-19 08:00:00     a      1\n#      2011-07-19 09:00:00  2011-07-19 09:00:00     a      2\n#b     2011-07-19 07:00:00  2011-07-19 07:00:00     b      3\n#      2011-07-19 08:00:00  2011-07-19 08:00:00     b      4\n#      2011-07-19 09:00:00  2011-07-19 09:00:00     b      5\n""]";"["">>> import pandas as pd\n>>> dat = pd.DataFrame({'label':['a', 'a', 'a', 'b', 'b', 'b'], 'datetime':['2011-07-19 07:00:00', '2011-07-19 08:00:00', '2011-07-19 09:00:00', '2011-07-19 07:00:00', '2011-07-19 08:00:00', '2011-07-19 09:00:00'], 'value':range(6)})\n>>> dat.dtypes\n#datetime    object\n#label       object\n#value        int64\n#dtype: object\n"", '>>> times = pd.to_datetime(dat[\'datetime\'])\n>>> times.tz_localize(\'UTC\')\n#Traceback (most recent call last):\n#  File ""<stdin>"", line 1, in <module>\n#  File ""/Users/erikshilts/workspace/schedule-detection/python/pysched/env/lib/python2.7/site-packages/pandas/core/series.py"", line 3170, in tz_localize\n#    raise Exception(\'Cannot tz-localize non-time series\')\n#Exception: Cannot tz-localize non-time series\n', "">>> times_index = pd.Index(times)\n>>> times_index_pacific = times_index.tz_localize('UTC').tz_convert('US/Pacific')\n>>> times_index_pacific\n#<class 'pandas.tseries.index.DatetimeIndex'>\n#[2011-07-19 00:00:00, ..., 2011-07-19 02:00:00]\n#Length: 6, Freq: None, Timezone: US/Pacific\n"", "">>> dat_index = dat.set_index([dat['label'], times_index_pacific])\n>>> dat_index\n#                                      datetime label  value\n#label                                                      \n#a     2011-07-19 07:00:00  2011-07-19 07:00:00     a      0\n#      2011-07-19 08:00:00  2011-07-19 08:00:00     a      1\n#      2011-07-19 09:00:00  2011-07-19 09:00:00     a      2\n#b     2011-07-19 07:00:00  2011-07-19 07:00:00     b      3\n#      2011-07-19 08:00:00  2011-07-19 08:00:00     b      4\n#      2011-07-19 09:00:00  2011-07-19 09:00:00     b      5\n""]";"['#datetime    object\n#label       object\n#value        int64\n#dtype: object\n#Traceback (most recent call last):\n#  File ""<stdin>"", line 1, in <module>\n#  File ""/Users/erikshilts/workspace/schedule-detection/python/pysched/env/lib/python2.7/site-packages/pandas/core/series.py"", line 3170, in tz_localize\n#    raise Exception(\'Cannot tz-localize non-time series\')\n#Exception: Cannot tz-localize non-time series\n#<class \'pandas.tseries.index.DatetimeIndex\'>\n#[2011-07-19 00:00:00, ..., 2011-07-19 02:00:00]\n#Length: 6, Freq: None, Timezone: US/Pacific\n#                                      datetime label  value\n#label                                                      \n#a     2011-07-19 07:00:00  2011-07-19 07:00:00     a      0\n#      2011-07-19 08:00:00  2011-07-19 08:00:00     a      1\n#      2011-07-19 09:00:00  2011-07-19 09:00:00     a      2\n#b     2011-07-19 07:00:00  2011-07-19 07:00:00     b      3\n#      2011-07-19 08:00:00  2011-07-19 08:00:00     b      4\n#      2011-07-19 09:00:00  2011-07-19 09:00:00     b      5\n']";"['#datetime    object\n#label       object\n#value        int64\n#dtype: object\n#Traceback (most recent call last):\n#  File ""<stdin>"", line 1, in <module>\n#  File ""/Users/erikshilts/workspace/schedule-detection/python/pysched/env/lib/python2.7/site-packages/pandas/core/series.py"", line 3170, in tz_localize\n#    raise Exception(\'Cannot tz-localize non-time series\')\n#Exception: Cannot tz-localize non-time series\n#<class \'pandas.tseries.index.DatetimeIndex\'>\n#[2011-07-19 00:00:00, ..., 2011-07-19 02:00:00]\n#Length: 6, Freq: None, Timezone: US/Pacific\n#                                      datetime label  value\n#label                                                      \n#a     2011-07-19 07:00:00  2011-07-19 07:00:00     a      0\n#      2011-07-19 08:00:00  2011-07-19 08:00:00     a      1\n#      2011-07-19 09:00:00  2011-07-19 09:00:00     a      2\n#b     2011-07-19 07:00:00  2011-07-19 07:00:00     b      3\n#      2011-07-19 08:00:00  2011-07-19 08:00:00     b      4\n#      2011-07-19 09:00:00  2011-07-19 09:00:00     b      5\n']";False;"['import pandas as pd\n#datetime    object\n#label       object\n#value        int64\n#dtype: object\n#Traceback (most recent call last):\n#  File ""<stdin>"", line 1, in <module>\n#  File ""/Users/erikshilts/workspace/schedule-detection/python/pysched/env/lib/python2.7/site-packages/pandas/core/series.py"", line 3170, in tz_localize\n#    raise Exception(\'Cannot tz-localize non-time series\')\n#Exception: Cannot tz-localize non-time series\n#<class \'pandas.tseries.index.DatetimeIndex\'>\n#[2011-07-19 00:00:00, ..., 2011-07-19 02:00:00]\n#Length: 6, Freq: None, Timezone: US/Pacific\n#                                      datetime label  value\n#label                                                      \n#a     2011-07-19 07:00:00  2011-07-19 07:00:00     a      0\n#      2011-07-19 08:00:00  2011-07-19 08:00:00     a      1\n#      2011-07-19 09:00:00  2011-07-19 09:00:00     a      2\n#b     2011-07-19 07:00:00  2011-07-19 07:00:00     b      3\n#      2011-07-19 08:00:00  2011-07-19 08:00:00     b      4\n#      2011-07-19 09:00:00  2011-07-19 09:00:00     b      5\n']";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'dat' is not defined""]";['NameError'];0;1;"[""name 'dat' is not defined""]";['NameError']
378;378;378;378;1.0;7;17165340;;1;15;<python><numpy><pandas>;Using Pandas to create DataFrame with Series, resulting in memory error;47971.0;"["">>> prcpSeries.shape\n(12626172,)\nd = {'prcp': pd.Series(prcpSeries),\n     'tmax': pd.Series(tmaxSeries),\n     'tmin': pd.Series(tminSeries),\n     'ndvi': pd.Series(ndviSeries),\n     'lstm': pd.Series(lstmSeries),\n     'evtm': pd.Series(evtmSeries)}\n\ndf = pd.DataFrame(d)\noutFile ='F:/data/output/run1/_'+str(i)+'.out'\ndf.to_csv(outFile, header = False, chunksize = 1000)\nd = None\ndf = None\ndf = pd.DataFrame(d)\n""]";"['>>> prcpSeries.shape\n(12626172,)\n', ""d = {'prcp': pd.Series(prcpSeries),\n     'tmax': pd.Series(tmaxSeries),\n     'tmin': pd.Series(tminSeries),\n     'ndvi': pd.Series(ndviSeries),\n     'lstm': pd.Series(lstmSeries),\n     'evtm': pd.Series(evtmSeries)}\n\ndf = pd.DataFrame(d)\noutFile ='F:/data/output/run1/_'+str(i)+'.out'\ndf.to_csv(outFile, header = False, chunksize = 1000)\nd = None\ndf = None\n"", 'df = pd.DataFrame(d)\n']";"['>>> prcpSeries.shape\n(12626172,)\n', ""d = {'prcp': pd.Series(prcpSeries),\n     'tmax': pd.Series(tmaxSeries),\n     'tmin': pd.Series(tminSeries),\n     'ndvi': pd.Series(ndviSeries),\n     'lstm': pd.Series(lstmSeries),\n     'evtm': pd.Series(evtmSeries)}\n\ndf = pd.DataFrame(d)\noutFile ='F:/data/output/run1/_'+str(i)+'.out'\ndf.to_csv(outFile, header = False, chunksize = 1000)\nd = None\ndf = None\n"", 'df = pd.DataFrame(d)\n']";"[""(12626172,)\n\ndf = pd.DataFrame(d)\noutFile ='F:/data/output/run1/_'+str(i)+'.out'\ndf.to_csv(outFile, header = False, chunksize = 1000)\nd = None\ndf = None\ndf = pd.DataFrame(d)\n""]";"[""import pandas as pd\n(12626172,)\n\ndf = pd.DataFrame(d)\noutFile ='F:/data/output/run1/_'+str(i)+'.out'\ndf.to_csv(outFile, header = False, chunksize = 1000)\nd = None\ndf = None\ndf = pd.DataFrame(d)\n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\n(12626172,)\n\ndf = pd.DataFrame(d)\noutFile ='F:/data/output/run1/_'+str(i)+'.out'\ndf.to_csv(outFile, header = False, chunksize = 1000)\nd = None\ndf = None\ndf = pd.DataFrame(d)\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
379;379;379;379;2.0;0;17193850;;1;11;<python><pandas>;How to get column by number in Pandas?;21660.0;"[""Maand['P_Sanyo_Gesloten']\nOut[119]: \nTime\n2012-08-01 00:00:11    0\n2012-08-01 00:05:10    0\n2012-08-01 00:10:11    0\n2012-08-01 00:20:10    0\n2012-08-01 00:25:10    0\n2012-08-01 00:30:09    0\n2012-08-01 00:40:10    0\n2012-08-01 00:50:09    0\n2012-08-01 01:05:10    0\n2012-08-01 01:10:10    0\n2012-08-01 01:15:10    0\n2012-08-01 01:25:10    0\n2012-08-01 01:30:10    0\n2012-08-01 01:35:09    0\n2012-08-01 01:40:10    0\n...\n2012-08-30 22:35:09    0\n2012-08-30 22:45:10    0\n2012-08-30 22:50:09    0\n2012-08-30 22:55:10    0\n2012-08-30 23:00:09    0\n2012-08-30 23:05:10    0\n2012-08-30 23:10:09    0\n2012-08-30 23:15:10    0\n2012-08-30 23:20:09    0\n2012-08-30 23:25:10    0\n2012-08-30 23:35:09    0\n2012-08-30 23:40:10    0\n2012-08-30 23:45:09    0\n2012-08-30 23:50:10    0\n2012-08-30 23:55:11    0\nName: P_Sanyo_Gesloten, Length: 7413, dtype: int64\nMaand[[1]]\nOut[120]: \n&ltclass 'pandas.core.frame.DataFrame'&gt\nDatetimeIndex: 7413 entries, 2012-08-01 00:00:11 to 2012-08-30 23:55:11\nData columns (total 1 columns):\nP_Sanyo_Gesloten    7413  non-null values\ndtypes: int64(1)\n""]";"[""Maand['P_Sanyo_Gesloten']\nOut[119]: \nTime\n2012-08-01 00:00:11    0\n2012-08-01 00:05:10    0\n2012-08-01 00:10:11    0\n2012-08-01 00:20:10    0\n2012-08-01 00:25:10    0\n2012-08-01 00:30:09    0\n2012-08-01 00:40:10    0\n2012-08-01 00:50:09    0\n2012-08-01 01:05:10    0\n2012-08-01 01:10:10    0\n2012-08-01 01:15:10    0\n2012-08-01 01:25:10    0\n2012-08-01 01:30:10    0\n2012-08-01 01:35:09    0\n2012-08-01 01:40:10    0\n...\n2012-08-30 22:35:09    0\n2012-08-30 22:45:10    0\n2012-08-30 22:50:09    0\n2012-08-30 22:55:10    0\n2012-08-30 23:00:09    0\n2012-08-30 23:05:10    0\n2012-08-30 23:10:09    0\n2012-08-30 23:15:10    0\n2012-08-30 23:20:09    0\n2012-08-30 23:25:10    0\n2012-08-30 23:35:09    0\n2012-08-30 23:40:10    0\n2012-08-30 23:45:09    0\n2012-08-30 23:50:10    0\n2012-08-30 23:55:11    0\nName: P_Sanyo_Gesloten, Length: 7413, dtype: int64\n"", ""Maand[[1]]\nOut[120]: \n&ltclass 'pandas.core.frame.DataFrame'&gt\nDatetimeIndex: 7413 entries, 2012-08-01 00:00:11 to 2012-08-30 23:55:11\nData columns (total 1 columns):\nP_Sanyo_Gesloten    7413  non-null values\ndtypes: int64(1)\n""]";"[""Maand['P_Sanyo_Gesloten']\nOut[119]: \nTime\n2012-08-01 00:00:11    0\n2012-08-01 00:05:10    0\n2012-08-01 00:10:11    0\n2012-08-01 00:20:10    0\n2012-08-01 00:25:10    0\n2012-08-01 00:30:09    0\n2012-08-01 00:40:10    0\n2012-08-01 00:50:09    0\n2012-08-01 01:05:10    0\n2012-08-01 01:10:10    0\n2012-08-01 01:15:10    0\n2012-08-01 01:25:10    0\n2012-08-01 01:30:10    0\n2012-08-01 01:35:09    0\n2012-08-01 01:40:10    0\n...\n2012-08-30 22:35:09    0\n2012-08-30 22:45:10    0\n2012-08-30 22:50:09    0\n2012-08-30 22:55:10    0\n2012-08-30 23:00:09    0\n2012-08-30 23:05:10    0\n2012-08-30 23:10:09    0\n2012-08-30 23:15:10    0\n2012-08-30 23:20:09    0\n2012-08-30 23:25:10    0\n2012-08-30 23:35:09    0\n2012-08-30 23:40:10    0\n2012-08-30 23:45:09    0\n2012-08-30 23:50:10    0\n2012-08-30 23:55:11    0\nName: P_Sanyo_Gesloten, Length: 7413, dtype: int64\n"", ""Maand[[1]]\nOut[120]: \n&ltclass 'pandas.core.frame.DataFrame'&gt\nDatetimeIndex: 7413 entries, 2012-08-01 00:00:11 to 2012-08-30 23:55:11\nData columns (total 1 columns):\nP_Sanyo_Gesloten    7413  non-null values\ndtypes: int64(1)\n""]";"[""Maand['P_Sanyo_Gesloten']\nTime\n...\nMaand[[1]]\n""]";"[""Maand['P_Sanyo_Gesloten']\nTime\n...\nMaand[[1]]\n""]";False;"[""import pandas as pd\nMaand['P_Sanyo_Gesloten']\nTime\n...\nMaand[[1]]\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""'[1] not in index'""]";['KeyError'];0;1;"[""'[1] not in index'""]";['KeyError']
380;380;380;380;3.0;1;17208567;;1;12;<python><debugging><warnings><pandas>;How to find out where a Python Warning is from;2105.0;"[""[...]\\pandas\\core\\index.py:756: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\nreturn self._engine.get_loc(key)\ntry:\n    return self._engine.get_loc(key)\nexcept UnicodeWarning:\n    warnings.warn('Oh Non', stacklevel=2)\n""]";"['[...]\\pandas\\core\\index.py:756: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\nreturn self._engine.get_loc(key)\n', ""try:\n    return self._engine.get_loc(key)\nexcept UnicodeWarning:\n    warnings.warn('Oh Non', stacklevel=2)\n""]";"['[...]\\pandas\\core\\index.py:756: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\nreturn self._engine.get_loc(key)\n', ""try:\n    return self._engine.get_loc(key)\nexcept UnicodeWarning:\n    warnings.warn('Oh Non', stacklevel=2)\n""]";['return self._engine.get_loc(key)\n'];['return self._engine.get_loc(key)\n'];False;['import pandas as pd\nreturn self._engine.get_loc(key)\n'];False;0;1;['nothing to repeat at position 0'];['error'];0;1;['nothing to repeat at position 0'];['error'];0;1;['nothing to repeat at position 0'];['error']
381;381;381;381;2.0;0;17216153;;1;12;<python><pandas><dataframe>;Python Pandas: Boolean indexing on multiple columns;19384.0;"['>>> d = pd.DataFrame({\'x\':[1, 2, 3, 4, 5], \'y\':[4, 5, 6, 7, 8]})\n>>> d\n   x  y\n0  1  4\n1  2  5\n2  3  6\n3  4  7\n4  5  8\n>>> d[d[\'x\']>2] # This works fine\n   x  y\n2  3  6\n3  4  7\n4  5  8\n>>> d[d[\'x\']>2 & d[\'y\']>7] # I had expected this to work, but it doesn\'t\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n>>> d[d[\'x\']>2][d[\'y\']>7]\n']";"['>>> d = pd.DataFrame({\'x\':[1, 2, 3, 4, 5], \'y\':[4, 5, 6, 7, 8]})\n>>> d\n   x  y\n0  1  4\n1  2  5\n2  3  6\n3  4  7\n4  5  8\n>>> d[d[\'x\']>2] # This works fine\n   x  y\n2  3  6\n3  4  7\n4  5  8\n>>> d[d[\'x\']>2 & d[\'y\']>7] # I had expected this to work, but it doesn\'t\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n', "">>> d[d['x']>2][d['y']>7]\n""]";"['pandas', 'SELECT', '>>> d = pd.DataFrame({\'x\':[1, 2, 3, 4, 5], \'y\':[4, 5, 6, 7, 8]})\n>>> d\n   x  y\n0  1  4\n1  2  5\n2  3  6\n3  4  7\n4  5  8\n>>> d[d[\'x\']>2] # This works fine\n   x  y\n2  3  6\n3  4  7\n4  5  8\n>>> d[d[\'x\']>2 & d[\'y\']>7] # I had expected this to work, but it doesn\'t\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n', "">>> d[d['x']>2][d['y']>7]\n""]";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'd' is not defined""]";['NameError'];0;1;"[""name 'd' is not defined""]";['NameError'];0;1;"[""name 'd' is not defined""]";['NameError']
382;382;382;382;4.0;0;17241004;;1;105;<python><pandas>;Pandas - how to get the data frame index as an array;158874.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;4;"['Sucess', ""name 'pd' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError', 'NameError'];2;4;"['Sucess', 'Sucess', ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'Sucess', 'NameError', 'NameError'];3;4;"['Sucess', 'Sucess', 'Sucess', ""'Level name_sub_index must be same as name (None)'""]";['Sucess', 'Sucess', 'Sucess', 'KeyError']
383;383;383;383;4.0;1;17242970;;1;12;<python><sorting><pandas><multi-index>;Multi-Index Sorting in Pandas;10275.0;"[""         Date Manufacturer Product Name Product Launch Date  Sales\n0  2013-01-01        Apple         iPod          2001-10-23     12\n1  2013-01-01        Apple         iPad          2010-04-03     13\n2  2013-01-01      Samsung       Galaxy          2009-04-27     14\n3  2013-01-01      Samsung   Galaxy Tab          2010-09-02     15\n4  2013-01-02        Apple         iPod          2001-10-23     22\n5  2013-01-02        Apple         iPad          2010-04-03     17\n6  2013-01-02      Samsung       Galaxy          2009-04-27     10\n7  2013-01-02      Samsung   Galaxy Tab          2010-09-02      7\n> grouped = df.groupby(['Manufacturer', 'Product Name', 'Product Launch Date']).sum()\n                                               Sales\nManufacturer Product Name Product Launch Date       \nApple        iPad         2010-04-03              30\n             iPod         2001-10-23              34\nSamsung      Galaxy       2009-04-27              24\n             Galaxy Tab   2010-09-02              22\n                                               Sales\nManufacturer Product Name Product Launch Date       \nApple        iPod         2001-10-23              34\n             iPad         2010-04-03              30\nSamsung      Galaxy       2009-04-27              24\n             Galaxy Tab   2010-09-02              22\n> grouped.sortlevel('Product Launch Date')\n                                               Sales\nManufacturer Product Name Product Launch Date       \nApple        iPod         2001-10-23              34\nSamsung      Galaxy       2009-04-27              24\nApple        iPad         2010-04-03              30\nSamsung      Galaxy Tab   2010-09-02              22\ngrouped.sort(['Manufacturer','Product Launch Date'])\nKeyError: u'no item named Manufacturer'\n\ngrouped.sort_index(by=['Manufacturer','Product Launch Date'])\nKeyError: u'no item named Manufacturer'\ndata = {\n  'Date': ['2013-01-01', '2013-01-01', '2013-01-01', '2013-01-01', '2013-01-02', '2013-01-02', '2013-01-02', '2013-01-02'],\n  'Manufacturer' : ['Apple', 'Apple', 'Samsung', 'Samsung', 'Apple', 'Apple', 'Samsung', 'Samsung',],\n  'Product Name' : ['iPod', 'iPad', 'Galaxy', 'Galaxy Tab', 'iPod', 'iPad', 'Galaxy', 'Galaxy Tab'], \n  'Product Launch Date' : ['2001-10-23', '2010-04-03', '2009-04-27', '2010-09-02','2001-10-23', '2010-04-03', '2009-04-27', '2010-09-02'],\n  'Sales' : [12, 13, 14, 15, 22, 17, 10, 7]\n}\ndf = DataFrame(data, columns=['Date', 'Manufacturer', 'Product Name', 'Product Launch Date', 'Sales'])\n""]";"['         Date Manufacturer Product Name Product Launch Date  Sales\n0  2013-01-01        Apple         iPod          2001-10-23     12\n1  2013-01-01        Apple         iPad          2010-04-03     13\n2  2013-01-01      Samsung       Galaxy          2009-04-27     14\n3  2013-01-01      Samsung   Galaxy Tab          2010-09-02     15\n4  2013-01-02        Apple         iPod          2001-10-23     22\n5  2013-01-02        Apple         iPad          2010-04-03     17\n6  2013-01-02      Samsung       Galaxy          2009-04-27     10\n7  2013-01-02      Samsung   Galaxy Tab          2010-09-02      7\n', ""> grouped = df.groupby(['Manufacturer', 'Product Name', 'Product Launch Date']).sum()\n                                               Sales\nManufacturer Product Name Product Launch Date       \nApple        iPad         2010-04-03              30\n             iPod         2001-10-23              34\nSamsung      Galaxy       2009-04-27              24\n             Galaxy Tab   2010-09-02              22\n"", '                                               Sales\nManufacturer Product Name Product Launch Date       \nApple        iPod         2001-10-23              34\n             iPad         2010-04-03              30\nSamsung      Galaxy       2009-04-27              24\n             Galaxy Tab   2010-09-02              22\n', ""> grouped.sortlevel('Product Launch Date')\n                                               Sales\nManufacturer Product Name Product Launch Date       \nApple        iPod         2001-10-23              34\nSamsung      Galaxy       2009-04-27              24\nApple        iPad         2010-04-03              30\nSamsung      Galaxy Tab   2010-09-02              22\n"", ""grouped.sort(['Manufacturer','Product Launch Date'])\nKeyError: u'no item named Manufacturer'\n\ngrouped.sort_index(by=['Manufacturer','Product Launch Date'])\nKeyError: u'no item named Manufacturer'\n"", ""data = {\n  'Date': ['2013-01-01', '2013-01-01', '2013-01-01', '2013-01-01', '2013-01-02', '2013-01-02', '2013-01-02', '2013-01-02'],\n  'Manufacturer' : ['Apple', 'Apple', 'Samsung', 'Samsung', 'Apple', 'Apple', 'Samsung', 'Samsung',],\n  'Product Name' : ['iPod', 'iPad', 'Galaxy', 'Galaxy Tab', 'iPod', 'iPad', 'Galaxy', 'Galaxy Tab'], \n  'Product Launch Date' : ['2001-10-23', '2010-04-03', '2009-04-27', '2010-09-02','2001-10-23', '2010-04-03', '2009-04-27', '2010-09-02'],\n  'Sales' : [12, 13, 14, 15, 22, 17, 10, 7]\n}\ndf = DataFrame(data, columns=['Date', 'Manufacturer', 'Product Name', 'Product Launch Date', 'Sales'])\n""]";"['         Date Manufacturer Product Name Product Launch Date  Sales\n0  2013-01-01        Apple         iPod          2001-10-23     12\n1  2013-01-01        Apple         iPad          2010-04-03     13\n2  2013-01-01      Samsung       Galaxy          2009-04-27     14\n3  2013-01-01      Samsung   Galaxy Tab          2010-09-02     15\n4  2013-01-02        Apple         iPod          2001-10-23     22\n5  2013-01-02        Apple         iPad          2010-04-03     17\n6  2013-01-02      Samsung       Galaxy          2009-04-27     10\n7  2013-01-02      Samsung   Galaxy Tab          2010-09-02      7\n', ""> grouped = df.groupby(['Manufacturer', 'Product Name', 'Product Launch Date']).sum()\n                                               Sales\nManufacturer Product Name Product Launch Date       \nApple        iPad         2010-04-03              30\n             iPod         2001-10-23              34\nSamsung      Galaxy       2009-04-27              24\n             Galaxy Tab   2010-09-02              22\n"", '                                               Sales\nManufacturer Product Name Product Launch Date       \nApple        iPod         2001-10-23              34\n             iPad         2010-04-03              30\nSamsung      Galaxy       2009-04-27              24\n             Galaxy Tab   2010-09-02              22\n', ""> grouped.sortlevel('Product Launch Date')\n                                               Sales\nManufacturer Product Name Product Launch Date       \nApple        iPod         2001-10-23              34\nSamsung      Galaxy       2009-04-27              24\nApple        iPad         2010-04-03              30\nSamsung      Galaxy Tab   2010-09-02              22\n"", ""grouped.sort(['Manufacturer','Product Launch Date'])\nKeyError: u'no item named Manufacturer'\n\ngrouped.sort_index(by=['Manufacturer','Product Launch Date'])\nKeyError: u'no item named Manufacturer'\n"", ""data = {\n  'Date': ['2013-01-01', '2013-01-01', '2013-01-01', '2013-01-01', '2013-01-02', '2013-01-02', '2013-01-02', '2013-01-02'],\n  'Manufacturer' : ['Apple', 'Apple', 'Samsung', 'Samsung', 'Apple', 'Apple', 'Samsung', 'Samsung',],\n  'Product Name' : ['iPod', 'iPad', 'Galaxy', 'Galaxy Tab', 'iPod', 'iPad', 'Galaxy', 'Galaxy Tab'], \n  'Product Launch Date' : ['2001-10-23', '2010-04-03', '2009-04-27', '2010-09-02','2001-10-23', '2010-04-03', '2009-04-27', '2010-09-02'],\n  'Sales' : [12, 13, 14, 15, 22, 17, 10, 7]\n}\ndf = DataFrame(data, columns=['Date', 'Manufacturer', 'Product Name', 'Product Launch Date', 'Sales'])\n""]";"[""grouped.sort(['Manufacturer','Product Launch Date'])\n\ngrouped.sort_index(by=['Manufacturer','Product Launch Date'])\ndf = DataFrame(data, columns=['Date', 'Manufacturer', 'Product Name', 'Product Launch Date', 'Sales'])\n""]";"[""from pandas import DataFrame\ngrouped.sort(['Manufacturer','Product Launch Date'])\n\ngrouped.sort_index(by=['Manufacturer','Product Launch Date'])\ndf = DataFrame(data, columns=['Date', 'Manufacturer', 'Product Name', 'Product Launch Date', 'Sales'])\n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\ngrouped.sort(['Manufacturer','Product Launch Date'])\n\ngrouped.sort_index(by=['Manufacturer','Product Launch Date'])\ndf = DataFrame(data, columns=['Date', 'Manufacturer', 'Product Name', 'Product Launch Date', 'Sales'])\n""]";True;0;1;"[""name 'g' is not defined""]";['NameError'];0;1;"[""name 'g' is not defined""]";['NameError'];0;1;"[""name 'g' is not defined""]";['NameError']
384;384;384;384;2.0;2;17244049;;1;11;<python><pandas>;Finding label location in a DataFrame Index;14603.0;"[""import pandas as pnd\nd = pnd.Timestamp('2013-01-01 16:00')\ndates = pnd.bdate_range(start=d, end = d+pnd.DateOffset(days=10), normalize = False)\n\ndf = pnd.DataFrame(index=dates, columns=['a'])\ndf['a'] = 6\n\nprint(df)\n                     a\n2013-01-01 16:00:00  6\n2013-01-02 16:00:00  6\n2013-01-03 16:00:00  6\n2013-01-04 16:00:00  6\n2013-01-07 16:00:00  6\n2013-01-08 16:00:00  6\n2013-01-09 16:00:00  6\n2013-01-10 16:00:00  6\n2013-01-11 16:00:00  6\nds = pnd.Timestamp('2013-01-02 16:00')\n""]";"[""import pandas as pnd\nd = pnd.Timestamp('2013-01-01 16:00')\ndates = pnd.bdate_range(start=d, end = d+pnd.DateOffset(days=10), normalize = False)\n\ndf = pnd.DataFrame(index=dates, columns=['a'])\ndf['a'] = 6\n\nprint(df)\n                     a\n2013-01-01 16:00:00  6\n2013-01-02 16:00:00  6\n2013-01-03 16:00:00  6\n2013-01-04 16:00:00  6\n2013-01-07 16:00:00  6\n2013-01-08 16:00:00  6\n2013-01-09 16:00:00  6\n2013-01-10 16:00:00  6\n2013-01-11 16:00:00  6\n"", ""ds = pnd.Timestamp('2013-01-02 16:00')\n""]";"[""import pandas as pnd\nd = pnd.Timestamp('2013-01-01 16:00')\ndates = pnd.bdate_range(start=d, end = d+pnd.DateOffset(days=10), normalize = False)\n\ndf = pnd.DataFrame(index=dates, columns=['a'])\ndf['a'] = 6\n\nprint(df)\n                     a\n2013-01-01 16:00:00  6\n2013-01-02 16:00:00  6\n2013-01-03 16:00:00  6\n2013-01-04 16:00:00  6\n2013-01-07 16:00:00  6\n2013-01-08 16:00:00  6\n2013-01-09 16:00:00  6\n2013-01-10 16:00:00  6\n2013-01-11 16:00:00  6\n"", ""ds = pnd.Timestamp('2013-01-02 16:00')\n""]";"[""import pandas as pnd\nd = pnd.Timestamp('2013-01-01 16:00')\ndates = pnd.bdate_range(start=d, end = d+pnd.DateOffset(days=10), normalize = False)\n\ndf = pnd.DataFrame(index=dates, columns=['a'])\ndf['a'] = 6\n\nprint(df)\nds = pnd.Timestamp('2013-01-02 16:00')\n""]";"[""import pandas as pnd\nd = pnd.Timestamp('2013-01-01 16:00')\ndates = pnd.bdate_range(start=d, end = d+pnd.DateOffset(days=10), normalize = False)\n\ndf = pnd.DataFrame(index=dates, columns=['a'])\ndf['a'] = 6\n\nprint(df)\nds = pnd.Timestamp('2013-01-02 16:00')\n""]";False;"[""import pandas as pd\nimport pandas as pnd\nd = pnd.Timestamp('2013-01-01 16:00')\ndates = pnd.bdate_range(start=d, end = d+pnd.DateOffset(days=10), normalize = False)\n\ndf = pnd.DataFrame(index=dates, columns=['a'])\ndf['a'] = 6\n\nprint(df)\nds = pnd.Timestamp('2013-01-02 16:00')\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'ds' is not defined""]";['NameError']
385;385;385;385;1.0;0;17286672;;1;17;<python><pandas>;Creating percentile buckets in pandas;8866.0;"[""a = pnd.DataFrame(index = ['a','b','c','d','e','f','g','h','i','j'], columns=['data'])\na.data = np.random.randn(10)\nprint a\nprint '\\nthese are ranked as shown'\nprint a.rank()\n\n       data\na -0.310188\nb -0.191582\nc  0.860467\nd -0.458017\ne  0.858653\nf -1.640166\ng -1.969908\nh  0.649781\ni  0.218000\nj  1.887577\n\nthese are ranked as shown\n   data\na     4\nb     5\nc     9\nd     3\ne     8\nf     2\ng     1\nh     7\ni     6\nj    10\ndesired result : ['c','j']\n""]";"[""a = pnd.DataFrame(index = ['a','b','c','d','e','f','g','h','i','j'], columns=['data'])\na.data = np.random.randn(10)\nprint a\nprint '\\nthese are ranked as shown'\nprint a.rank()\n\n       data\na -0.310188\nb -0.191582\nc  0.860467\nd -0.458017\ne  0.858653\nf -1.640166\ng -1.969908\nh  0.649781\ni  0.218000\nj  1.887577\n\nthese are ranked as shown\n   data\na     4\nb     5\nc     9\nd     3\ne     8\nf     2\ng     1\nh     7\ni     6\nj    10\n"", ""desired result : ['c','j']\n""]";"[""a = pnd.DataFrame(index = ['a','b','c','d','e','f','g','h','i','j'], columns=['data'])\na.data = np.random.randn(10)\nprint a\nprint '\\nthese are ranked as shown'\nprint a.rank()\n\n       data\na -0.310188\nb -0.191582\nc  0.860467\nd -0.458017\ne  0.858653\nf -1.640166\ng -1.969908\nh  0.649781\ni  0.218000\nj  1.887577\n\nthese are ranked as shown\n   data\na     4\nb     5\nc     9\nd     3\ne     8\nf     2\ng     1\nh     7\ni     6\nj    10\n"", ""desired result : ['c','j']\n""]";"[""a = pnd.DataFrame(index = ['a','b','c','d','e','f','g','h','i','j'], columns=['data'])\na.data = np.random.randn(10)\n\na -0.310188\nb -0.191582\nd -0.458017\nf -1.640166\ng -1.969908\n\n""]";"[""a = pnd.DataFrame(index = ['a','b','c','d','e','f','g','h','i','j'], columns=['data'])\na.data = np.random.randn(10)\n\na -0.310188\nb -0.191582\nd -0.458017\nf -1.640166\ng -1.969908\n\n""]";False;"[""import pandas as pd\na = pnd.DataFrame(index = ['a','b','c','d','e','f','g','h','i','j'], columns=['data'])\na.data = np.random.randn(10)\n\na -0.310188\nb -0.191582\nd -0.458017\nf -1.640166\ng -1.969908\n\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;['need at least one array to concatenate'];['ValueError']
386;386;386;386;1.0;0;17298313;;1;19;<python><pandas>;Python Pandas: Convert Rows as Column headers;10395.0;['Year    Country          medal    no of medals\n1896    Afghanistan      Gold        5\n1896    Afghanistan      Silver      4\n1896    Afghanistan      Bronze      3\n1896    Algeria          Gold        1\n1896    Algeria          Silver      2\n1896    Algeria          Bronze      3\nYear    Country      Gold   Silver   Bronze\n1896    Afghanistan    5      4         3\n1896    Algeria        1      2         3\n'];['Year    Country          medal    no of medals\n1896    Afghanistan      Gold        5\n1896    Afghanistan      Silver      4\n1896    Afghanistan      Bronze      3\n1896    Algeria          Gold        1\n1896    Algeria          Silver      2\n1896    Algeria          Bronze      3\n', 'Year    Country      Gold   Silver   Bronze\n1896    Afghanistan    5      4         3\n1896    Algeria        1      2         3\n'];['Year    Country          medal    no of medals\n1896    Afghanistan      Gold        5\n1896    Afghanistan      Silver      4\n1896    Afghanistan      Bronze      3\n1896    Algeria          Gold        1\n1896    Algeria          Silver      2\n1896    Algeria          Bronze      3\n', 'Year    Country      Gold   Silver   Bronze\n1896    Afghanistan    5      4         3\n1896    Algeria        1      2         3\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'no of medals'""]";['KeyError']
387;387;387;387;4.0;0;17315737;;1;15;<python><pandas>;Split a large pandas dataframe;20454.0;['for item in np.split(df, 4):\n    print item\n'];['for item in np.split(df, 4):\n    print item\n'];['ValueError: array split does not result in an equal division', 'for item in np.split(df, 4):\n    print item\n'];[''];[''];False;['import pandas as pd\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
388;388;388;388;3.0;4;17315881;;1;17;<python><pandas>;How can I check if a Pandas dataframe's index is sorted;3592.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
389;389;389;389;1.0;0;17322109;;1;11;<python><pandas>;get dataframe row count based on conditions;16817.0;"['print df[(df.IP == head.idxmax()) & (df.Method == \'HEAD\') & (df.Referrer == \'""-""\')].count()\nIP          57\nTime        57\nMethod      57\nResource    57\nStatus      57\nBytes       57\nReferrer    57\nAgent       57\ndtype: int64\n']";"['print df[(df.IP == head.idxmax()) & (df.Method == \'HEAD\') & (df.Referrer == \'""-""\')].count()\n', 'IP          57\nTime        57\nMethod      57\nResource    57\nStatus      57\nBytes       57\nReferrer    57\nAgent       57\ndtype: int64\n']";"['print df[(df.IP == head.idxmax()) & (df.Method == \'HEAD\') & (df.Referrer == \'""-""\')].count()\n', 'IP          57\nTime        57\nMethod      57\nResource    57\nStatus      57\nBytes       57\nReferrer    57\nAgent       57\ndtype: int64\n']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'DataFrame' is not defined""]";['NameError'];0;1;"[""name 'randn' is not defined""]";['NameError'];0;1;"[""name 'DataFrame' is not defined""]";['NameError']
390;390;390;390;4.0;4;17326973;;1;21;<python><excel><pandas><openpyxl>;Is there a way to auto-adjust Excel column widths with pandas.ExcelWriter?;11512.0;"['writer = pd.ExcelWriter(excel_file_path)\ndf.to_excel(writer, sheet_name=""Summary"")\n']";"['writer = pd.ExcelWriter(excel_file_path)\ndf.to_excel(writer, sheet_name=""Summary"")\n']";"['writer = pd.ExcelWriter(excel_file_path)\ndf.to_excel(writer, sheet_name=""Summary"")\n']";"['writer = pd.ExcelWriter(excel_file_path)\ndf.to_excel(writer, sheet_name=""Summary"")\n']";"['import pandas as pd\nwriter = pd.ExcelWriter(excel_file_path)\ndf.to_excel(writer, sheet_name=""Summary"")\n']";True;"['import pandas as pd\ndf = pd.DataFrame()\nwriter = pd.ExcelWriter(excel_file_path)\ndf.to_excel(writer, sheet_name=""Summary"")\n']";True;0;2;"[""name 'writer' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'writer' is not defined"", ""name 'filename' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'writer' is not defined"", ""name 'filename' is not defined""]";['NameError', 'NameError']
391;391;391;391;4.0;1;17383094;;1;30;<python><numpy><pandas>;python pandas/numpy True/False to 1/0 mapping;27133.0;[''];[];['dtype=object', 'np_values    = np.array(df.values, dtype = np.float64)'];[''];[''];False;['import pandas as pd\n'];False;3;4;"['Sucess', ""name 'DataFrame' is not defined"", 'Sucess', 'Sucess']";['Sucess', 'NameError', 'Sucess', 'Sucess'];4;4;['Sucess', 'Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess', 'Sucess'];3;4;"['Sucess', ""name 'DataFrame' is not defined"", 'Sucess', 'Sucess']";['Sucess', 'NameError', 'Sucess', 'Sucess']
392;392;392;392;2.0;0;17426292;;1;19;<python><dictionary><pandas><dataframe>;What is the most efficient way to create a dictionary of two pandas Dataframe columns?;12430.0;['Position    Letter\n1           a\n2           b\n3           c\n4           d\n5           e\n'];['Position    Letter\n1           a\n2           b\n3           c\n4           d\n5           e\n'];"['Position    Letter\n1           a\n2           b\n3           c\n4           d\n5           e\n', ""alphabet[1 : 'a', 2 : 'b', 3 : 'c', 4 : 'd', 5 : 'e']""]";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'Series' is not defined""]";['NameError'];0;1;"[""name 'Series' is not defined""]";['NameError'];0;1;"[""name 'Series' is not defined""]";['NameError']
393;393;393;393;2.0;1;17438906;;1;30;<python><pandas>;Combining rows in pandas;26878.0;['city_id    val1 val2 val3\nhouston,tx    1    2    0\nhouston,tx    0    0    1\nhouston,tx    2    1    1\ncity_id    val1 val2 val3\nhouston,tx    3    3    2\n'];['city_id    val1 val2 val3\nhouston,tx    1    2    0\nhouston,tx    0    0    1\nhouston,tx    2    1    1\n', 'city_id    val1 val2 val3\nhouston,tx    3    3    2\n'];['city_id', '[city],[state]', 'new york,ny', 'city_id', 'groupby()', 'city_id    val1 val2 val3\nhouston,tx    1    2    0\nhouston,tx    0    0    1\nhouston,tx    2    1    1\n', 'city_id    val1 val2 val3\nhouston,tx    3    3    2\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'city_id' is not defined""]";['NameError'];0;1;"[""name 'city_id' is not defined""]";['NameError'];0;1;"[""name 'city_id' is not defined""]";['NameError']
394;394;394;394;4.0;0;17465045;;1;40;<python><date><types><dataframe><pandas>;Can pandas automatically recognize dates?;49798.0;"['df = pandas.read_csv(\'test.dat\', delimiter=r""\\s+"", names=[\'col1\',\'col2\',\'col3\'])\nfor i, r in df.iterrows():\n    print type(r[\'col1\']), type(r[\'col2\']), type(r[\'col3\'])\n']";"['df = pandas.read_csv(\'test.dat\', delimiter=r""\\s+"", names=[\'col1\',\'col2\',\'col3\'])\n', ""for i, r in df.iterrows():\n    print type(r['col1']), type(r['col2']), type(r['col3'])\n""]";"['df = pandas.read_csv(\'test.dat\', delimiter=r""\\s+"", names=[\'col1\',\'col2\',\'col3\'])\n', ""for i, r in df.iterrows():\n    print type(r['col1']), type(r['col2']), type(r['col3'])\n"", '2013-6-4']";"['df = pandas.read_csv(\'test.dat\', delimiter=r""\\s+"", names=[\'col1\',\'col2\',\'col3\'])\n']";"['df = pandas.read_csv(\'test.dat\', delimiter=r""\\s+"", names=[\'col1\',\'col2\',\'col3\'])\n']";False;"['import pandas as pd\ndf = pandas.read_csv(\'test.dat\', delimiter=r""\\s+"", names=[\'col1\',\'col2\',\'col3\'])\n']";False;1;3;"[""name 'pd' is not defined"", 'Sucess', ""name 'pd' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""name 'infile' is not defined"", 'Sucess', ""name 'infile' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""name 'infile' is not defined"", 'Sucess', ""name 'infile' is not defined""]";['NameError', 'Sucess', 'NameError']
395;395;395;395;4.0;0;17477979;;1;60;<python><numpy><scipy><pandas>;dropping infinite values from dataframes in pandas?;45786.0;"['df.dropna(subset=[""col1"", ""col2""], how=""all"", with_inf=True)\n']";"['df.dropna(subset=[""col1"", ""col2""], how=""all"", with_inf=True)\n']";"['mode.use_inf_as_null', 'subset', 'how', 'dropna', 'inf', 'df.dropna(subset=[""col1"", ""col2""], how=""all"", with_inf=True)\n', 'dropna', 'inf']";"['df.dropna(subset=[""col1"", ""col2""], how=""all"", with_inf=True)\n']";"['df.dropna(subset=[""col1"", ""col2""], how=""all"", with_inf=True)\n']";False;"['import pandas as pd\ndf = pd.DataFrame()\ndf.dropna(subset=[""col1"", ""col2""], how=""all"", with_inf=True)\n']";True;0;2;"[""name 'pd' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError']
396;396;396;396;4.0;1;17530542;;1;55;<csv><pandas>;How to add pandas data to an existing csv file?;48770.0;[''];[];['to_csv()'];[''];[''];False;['import pandas as pd\n'];False;2;3;"[""name 'pd' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess'];2;3;"[""File b'foo.csv' does not exist"", 'Sucess', 'Sucess']";['FileNotFoundError', 'Sucess', 'Sucess'];2;3;"[""File b'foo.csv' does not exist"", 'Sucess', 'Sucess']";['FileNotFoundError', 'Sucess', 'Sucess']
397;397;397;397;4.0;3;17534106;;1;43;<python><numpy><pandas><nan>;What is the difference between NaN and None?;23404.0;['for k, v in my_dict.iteritems():\n    if np.isnan(v):\n'];['for k, v in my_dict.iteritems():\n    if np.isnan(v):\n'];['readcsv()', 'None', 'nan', 'None', 'nan', 'None', 'nan', 'nan', 'None', 'numpy.isnan()', 'for k, v in my_dict.iteritems():\n    if np.isnan(v):\n', 'v', 'v', 'nan'];[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 's_bad' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 's_bad' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 's_bad' is not defined""]";['Sucess', 'NameError']
398;398;398;398;5.0;17;17557074;;1;61;<python><windows><pandas>;Memory error when using pandas read_csv;17699.0;"['data = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2)\nTraceback (most recent call last):\n  File ""F:\\QA ALM\\Python\\new WIM data\\new WIM data\\new_WIM_data.py"", line 25, in\n <module>\n    wimdata = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2\n)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 401, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 216, in _read\n    return parser.read()\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 643, in read\n    df = DataFrame(col_dict, columns=columns, index=index)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 394, in __init__\n    mgr = self._init_dict(data, index, columns, dtype=dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 525, in _init_dict\n    dtype=dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 5338, in _arrays_to_mgr\n    return create_block_manager_from_arrays(arrays, arr_names, axes)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1820, in create_block_manager_from_arrays\n    blocks = form_blocks(arrays, names, axes)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1872, in form_blocks\n    float_blocks = _multi_blockify(float_items, items)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1930, in _multi_blockify\n    block_items, values = _stack_arrays(list(tup_block), ref_items, dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1962, in _stack_arrays\n    stacked = np.empty(shape, dtype=dtype)\nMemoryError\nPress any key to continue . . .\ndata <- read.table(paste(INPUTDIR,config[i,]$TOEXTRACT,sep=""""), HASHEADER, DELIMITER,skip=2,fill=TRUE)\n']";"['data = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2)\n', 'Traceback (most recent call last):\n  File ""F:\\QA ALM\\Python\\new WIM data\\new WIM data\\new_WIM_data.py"", line 25, in\n <module>\n    wimdata = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2\n)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 401, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 216, in _read\n    return parser.read()\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 643, in read\n    df = DataFrame(col_dict, columns=columns, index=index)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 394, in __init__\n    mgr = self._init_dict(data, index, columns, dtype=dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 525, in _init_dict\n    dtype=dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 5338, in _arrays_to_mgr\n    return create_block_manager_from_arrays(arrays, arr_names, axes)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1820, in create_block_manager_from_arrays\n    blocks = form_blocks(arrays, names, axes)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1872, in form_blocks\n    float_blocks = _multi_blockify(float_items, items)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1930, in _multi_blockify\n    block_items, values = _stack_arrays(list(tup_block), ref_items, dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1962, in _stack_arrays\n    stacked = np.empty(shape, dtype=dtype)\nMemoryError\nPress any key to continue . . .\n', 'data <- read.table(paste(INPUTDIR,config[i,]$TOEXTRACT,sep=""""), HASHEADER, DELIMITER,skip=2,fill=TRUE)\n']";"['data = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2)\n', 'MemoryError', 'Traceback (most recent call last):\n  File ""F:\\QA ALM\\Python\\new WIM data\\new WIM data\\new_WIM_data.py"", line 25, in\n <module>\n    wimdata = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2\n)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 401, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 216, in _read\n    return parser.read()\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py""\n, line 643, in read\n    df = DataFrame(col_dict, columns=columns, index=index)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 394, in __init__\n    mgr = self._init_dict(data, index, columns, dtype=dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 525, in _init_dict\n    dtype=dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py""\n, line 5338, in _arrays_to_mgr\n    return create_block_manager_from_arrays(arrays, arr_names, axes)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1820, in create_block_manager_from_arrays\n    blocks = form_blocks(arrays, names, axes)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1872, in form_blocks\n    float_blocks = _multi_blockify(float_items, items)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1930, in _multi_blockify\n    block_items, values = _stack_arrays(list(tup_block), ref_items, dtype)\n  File ""C:\\Program Files\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\n.py"", line 1962, in _stack_arrays\n    stacked = np.empty(shape, dtype=dtype)\nMemoryError\nPress any key to continue . . .\n', 'data <- read.table(paste(INPUTDIR,config[i,]$TOEXTRACT,sep=""""), HASHEADER, DELIMITER,skip=2,fill=TRUE)\n']";['data = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2)\nMemoryError\n'];['data = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2)\nMemoryError\n'];False;['import pandas as pd\ndata = pandas.read_csv(filepath, header = 0, sep = DELIMITER,skiprows = 2)\nMemoryError\n'];False;0;1;"[""name 'name' is not defined""]";['NameError'];0;1;"[""name 'name' is not defined""]";['NameError'];0;1;"[""name 'name' is not defined""]";['NameError']
399;399;399;399;2.0;1;17591104;;1;16;<python><pandas>;In pandas, can I deeply copy a DataFrame including its index and column?;14921.0;['In [61]: import pandas as pd\nIn [62]: df = pd.DataFrame([[1], [2], [3]])\nIn [64]: id(df), id(df2)\nOut[64]: (4385185040, 4385183312)\nIn [65]: id(df.index), id(df2.index)\nOut[65]: (4385175264, 4385175264)\n'];['In [61]: import pandas as pd\nIn [62]: df = pd.DataFrame([[1], [2], [3]])\n', 'In [64]: id(df), id(df2)\nOut[64]: (4385185040, 4385183312)\n', 'In [65]: id(df.index), id(df2.index)\nOut[65]: (4385175264, 4385175264)\n'];['In [61]: import pandas as pd\nIn [62]: df = pd.DataFrame([[1], [2], [3]])\n', 'copy', 'DataFrame', 'In [64]: id(df), id(df2)\nOut[64]: (4385185040, 4385183312)\n', 'index', 'In [65]: id(df.index), id(df2.index)\nOut[65]: (4385175264, 4385175264)\n'];['import pandas as pd\nid(df.index), id(df2.index)\n'];['import pandas as pd\nid(df.index), id(df2.index)\n'];False;['import pandas as pd\ndf2 = pd.DataFrame()\ndf = pd.DataFrame()\nimport pandas as pd\nid(df.index), id(df2.index)\n'];True;1;2;"[""name 'df2' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df2' is not defined"", 'Sucess']";['NameError', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
400;400;400;400;6.0;1;17618981;;1;35;<python><sorting><dataframe><pandas>;How to sort pandas data frame using values from several columns?;82354.0;"[""df = pandas.DataFrame([{'c1':3,'c2':10},{'c1':2, 'c2':30},{'c1':1,'c2':20},{'c1':2,'c2':15},{'c1':2,'c2':100}])\n   c1   c2\n0   3   10\n1   2   30\n2   1   20\n3   2   15\n4   2  100\ndf.sort(['c1','c2'], ascending=False)\n   c1   c2\n0   3   10\n4   2  100\n1   2   30\n3   2   15\n2   1   20\ndf.sort(['c1','c2'], ascending=[False,True])\n   c1   c2\n2   1   20\n3   2   15\n1   2   30\n4   2  100\n0   3   10\n>>> df.sort(['c1','c2'], ascending=[False,True])\n   c1   c2\n2   1   20\n3   2   15\n1   2   30\n4   2  100\n0   3   10\n""]";"[""df = pandas.DataFrame([{'c1':3,'c2':10},{'c1':2, 'c2':30},{'c1':1,'c2':20},{'c1':2,'c2':15},{'c1':2,'c2':100}])\n"", '   c1   c2\n0   3   10\n1   2   30\n2   1   20\n3   2   15\n4   2  100\n', ""df.sort(['c1','c2'], ascending=False)\n"", '   c1   c2\n0   3   10\n4   2  100\n1   2   30\n3   2   15\n2   1   20\n', ""df.sort(['c1','c2'], ascending=[False,True])\n"", '   c1   c2\n2   1   20\n3   2   15\n1   2   30\n4   2  100\n0   3   10\n', "">>> df.sort(['c1','c2'], ascending=[False,True])\n   c1   c2\n2   1   20\n3   2   15\n1   2   30\n4   2  100\n0   3   10\n""]";"[""df = pandas.DataFrame([{'c1':3,'c2':10},{'c1':2, 'c2':30},{'c1':1,'c2':20},{'c1':2,'c2':15},{'c1':2,'c2':100}])\n"", '   c1   c2\n0   3   10\n1   2   30\n2   1   20\n3   2   15\n4   2  100\n', ""df.sort(['c1','c2'], ascending=False)\n"", '   c1   c2\n0   3   10\n4   2  100\n1   2   30\n3   2   15\n2   1   20\n', ""df.sort(['c1','c2'], ascending=[False,True])\n"", '   c1   c2\n2   1   20\n3   2   15\n1   2   30\n4   2  100\n0   3   10\n', "">>> df.sort(['c1','c2'], ascending=[False,True])\n   c1   c2\n2   1   20\n3   2   15\n1   2   30\n4   2  100\n0   3   10\n""]";"[""df = pandas.DataFrame([{'c1':3,'c2':10},{'c1':2, 'c2':30},{'c1':1,'c2':20},{'c1':2,'c2':15},{'c1':2,'c2':100}])\ndf.sort(['c1','c2'], ascending=False)\ndf.sort(['c1','c2'], ascending=[False,True])\n""]";"[""df = pandas.DataFrame([{'c1':3,'c2':10},{'c1':2, 'c2':30},{'c1':1,'c2':20},{'c1':2,'c2':15},{'c1':2,'c2':100}])\ndf.sort(['c1','c2'], ascending=False)\ndf.sort(['c1','c2'], ascending=[False,True])\n""]";False;"[""import pandas as pd\ndf = pandas.DataFrame([{'c1':3,'c2':10},{'c1':2, 'c2':30},{'c1':1,'c2':20},{'c1':2,'c2':15},{'c1':2,'c2':100}])\ndf.sort(['c1','c2'], ascending=False)\ndf.sort(['c1','c2'], ascending=[False,True])\n""]";False;1;3;"['Sucess', ""name 'df' is not defined"", ""name 'f' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name 'df' is not defined"", ""name 'f' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name 'df' is not defined"", ""name 'f' is not defined""]";['Sucess', 'NameError', 'NameError']
401;401;401;401;5.0;3;17627219;;1;29;<python><numpy><pandas><similarity><cosine-similarity>;What's the fastest way in Python to calculate cosine similarity given sparse matrix data?;33178.0;"['A= \n[0 1 0 0 1\n 0 0 1 1 1\n 1 1 0 1 0]\nA = \n0, 1\n0, 4\n1, 2\n1, 3\n1, 4\n2, 0\n2, 1\n2, 3\nimport numpy as np\nfrom sklearn.metrics import pairwise_distances\nfrom scipy.spatial.distance import cosine\n\nA = np.array(\n[[0, 1, 0, 0, 1],\n[0, 0, 1, 1, 1],\n[1, 1, 0, 1, 0]])\n\ndist_out = 1-pairwise_distances(A, metric=""cosine"")\ndist_out\narray([[ 1.        ,  0.40824829,  0.40824829],\n       [ 0.40824829,  1.        ,  0.33333333],\n       [ 0.40824829,  0.33333333,  1.        ]])\n']";"['A= \n[0 1 0 0 1\n 0 0 1 1 1\n 1 1 0 1 0]\n', 'A = \n0, 1\n0, 4\n1, 2\n1, 3\n1, 4\n2, 0\n2, 1\n2, 3\n', 'import numpy as np\nfrom sklearn.metrics import pairwise_distances\nfrom scipy.spatial.distance import cosine\n\nA = np.array(\n[[0, 1, 0, 0, 1],\n[0, 0, 1, 1, 1],\n[1, 1, 0, 1, 0]])\n\ndist_out = 1-pairwise_distances(A, metric=""cosine"")\ndist_out\n', 'array([[ 1.        ,  0.40824829,  0.40824829],\n       [ 0.40824829,  1.        ,  0.33333333],\n       [ 0.40824829,  0.33333333,  1.        ]])\n']";"['A= \n[0 1 0 0 1\n 0 0 1 1 1\n 1 1 0 1 0]\n', 'A = \n0, 1\n0, 4\n1, 2\n1, 3\n1, 4\n2, 0\n2, 1\n2, 3\n', 'import numpy as np\nfrom sklearn.metrics import pairwise_distances\nfrom scipy.spatial.distance import cosine\n\nA = np.array(\n[[0, 1, 0, 0, 1],\n[0, 0, 1, 1, 1],\n[1, 1, 0, 1, 0]])\n\ndist_out = 1-pairwise_distances(A, metric=""cosine"")\ndist_out\n', 'array([[ 1.        ,  0.40824829,  0.40824829],\n       [ 0.40824829,  1.        ,  0.33333333],\n       [ 0.40824829,  0.33333333,  1.        ]])\n']";"['0, 1\n0, 4\n1, 2\n1, 3\n1, 4\n2, 0\n2, 1\n2, 3\nimport numpy as np\nfrom sklearn.metrics import pairwise_distances\nfrom scipy.spatial.distance import cosine\n\n[0, 0, 1, 1, 1],\n\ndist_out = 1-pairwise_distances(A, metric=""cosine"")\ndist_out\n']";"['0, 1\n0, 4\n1, 2\n1, 3\n1, 4\n2, 0\n2, 1\n2, 3\nimport numpy as np\nfrom sklearn.metrics import pairwise_distances\nfrom scipy.spatial.distance import cosine\n\n[0, 0, 1, 1, 1],\n\ndist_out = 1-pairwise_distances(A, metric=""cosine"")\ndist_out\n']";False;"['import pandas as pd\n0, 1\n0, 4\n1, 2\n1, 3\n1, 4\n2, 0\n2, 1\n2, 3\nimport numpy as np\nfrom sklearn.metrics import pairwise_distances\nfrom scipy.spatial.distance import cosine\n\n[0, 0, 1, 1, 1],\n\ndist_out = 1-pairwise_distances(A, metric=""cosine"")\ndist_out\n']";False;0;2;"[""name 'numpy' is not defined"", ""No module named 'sklearn'""]";['NameError', 'ImportError'];0;2;"[""name 'numpy' is not defined"", ""No module named 'sklearn'""]";['NameError', 'ImportError'];0;2;"[""name 'numpy' is not defined"", ""No module named 'sklearn'""]";['NameError', 'ImportError']
402;402;402;402;1.0;0;17644100;;1;11;<python><time><pandas><range>;Create hourly/minutely time range using pandas;10401.0;"['pandas.time_range(""11:00"", ""21:30"", freq=""30min"")\n']";"['pandas.time_range(""11:00"", ""21:30"", freq=""30min"")\n']";"['pandas.time_range(""11:00"", ""21:30"", freq=""30min"")\n']";"['pandas.time_range(""11:00"", ""21:30"", freq=""30min"")\n']";"['pandas.time_range(""11:00"", ""21:30"", freq=""30min"")\n']";False;"['import pandas as pd\npandas.time_range(""11:00"", ""21:30"", freq=""30min"")\n']";False;0;1;"[""name 'pandas' is not defined""]";['NameError'];0;1;"[""name 'pandas' is not defined""]";['NameError'];0;1;"[""name 'pandas' is not defined""]";['NameError']
403;403;403;403;3.0;0;17666075;;1;14;<python><group-by><pandas>;python pandas groupby() result;9112.0;"[""df = pd.DataFrame( {\n   'A': [1,1,1,1,2,2,2,3,3,4,4,4],\n   'B': [5,5,6,7,5,6,6,7,7,6,7,7],\n   'C': [1,1,1,1,1,1,1,1,1,1,1,1]\n    } );\n\ndf\n    A  B  C\n0   1  5  1\n1   1  5  1\n2   1  6  1\n3   1  7  1\n4   2  5  1\n5   2  6  1\n6   2  6  1\n7   3  7  1\n8   3  7  1\n9   4  6  1\n10  4  7  1\n11  4  7  1\n    A  B  C  D\n0   1  5  1  2\n1   1  5  1  2\n2   1  6  1  1\n3   1  7  1  1\n4   2  5  1  1\n5   2  6  1  2\n6   2  6  1  2\n7   3  7  1  2\n8   3  7  1  2\n9   4  6  1  1\n10  4  7  1  2\n11  4  7  1  2\nres = {}\nfor a, group_by_A in df.groupby('A'):\n    group_by_B = group_by_A.groupby('B', as_index = False)\n    res[a] = group_by_B['C'].sum()\n""]";"[""df = pd.DataFrame( {\n   'A': [1,1,1,1,2,2,2,3,3,4,4,4],\n   'B': [5,5,6,7,5,6,6,7,7,6,7,7],\n   'C': [1,1,1,1,1,1,1,1,1,1,1,1]\n    } );\n\ndf\n    A  B  C\n0   1  5  1\n1   1  5  1\n2   1  6  1\n3   1  7  1\n4   2  5  1\n5   2  6  1\n6   2  6  1\n7   3  7  1\n8   3  7  1\n9   4  6  1\n10  4  7  1\n11  4  7  1\n"", '    A  B  C  D\n0   1  5  1  2\n1   1  5  1  2\n2   1  6  1  1\n3   1  7  1  1\n4   2  5  1  1\n5   2  6  1  2\n6   2  6  1  2\n7   3  7  1  2\n8   3  7  1  2\n9   4  6  1  1\n10  4  7  1  2\n11  4  7  1  2\n', ""res = {}\nfor a, group_by_A in df.groupby('A'):\n    group_by_B = group_by_A.groupby('B', as_index = False)\n    res[a] = group_by_B['C'].sum()\n""]";"[""df = pd.DataFrame( {\n   'A': [1,1,1,1,2,2,2,3,3,4,4,4],\n   'B': [5,5,6,7,5,6,6,7,7,6,7,7],\n   'C': [1,1,1,1,1,1,1,1,1,1,1,1]\n    } );\n\ndf\n    A  B  C\n0   1  5  1\n1   1  5  1\n2   1  6  1\n3   1  7  1\n4   2  5  1\n5   2  6  1\n6   2  6  1\n7   3  7  1\n8   3  7  1\n9   4  6  1\n10  4  7  1\n11  4  7  1\n"", '    A  B  C  D\n0   1  5  1  2\n1   1  5  1  2\n2   1  6  1  1\n3   1  7  1  1\n4   2  5  1  1\n5   2  6  1  2\n6   2  6  1  2\n7   3  7  1  2\n8   3  7  1  2\n9   4  6  1  1\n10  4  7  1  2\n11  4  7  1  2\n', 'groupby', ""res = {}\nfor a, group_by_A in df.groupby('A'):\n    group_by_B = group_by_A.groupby('B', as_index = False)\n    res[a] = group_by_B['C'].sum()\n"", 'res', 'df']";['\ndf\nres = {}\n'];['\ndf\nres = {}\n'];False;['import pandas as pd\ndf = pd.DataFrame()\n\ndf\nres = {}\n'];True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'A'""]";['KeyError']
404;404;404;404;5.0;1;17679089;;1;41;<python><pandas><dataframe>;Pandas DataFrame Groupby two columns and get counts;65459.0;"[""df = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T\ndf.columns = ['col1','col2','col3','col4','col5']\n   col1 col2 col3     col4 col5\n0   1.1    A  1.1    x/y/z    1\n1   1.1    A  1.7      x/y    3\n2   1.1    A  2.5  x/y/z/n    3\n3   2.6    B  2.6      x/u    2\n4   2.5    B  3.3        x    4\n5   3.4    B  3.8    x/u/v    2\n6   2.6    B    4    x/y/z    5\n7   2.6    A  4.2        x    3\n8   3.4    B  4.3  x/u/v/b    6\n9   3.4    C  4.5        -    3\n10  2.6    B  4.6      x/y    5\n11  1.1    D  4.7    x/y/z    1\n12  1.1    D  4.7        x    1\n13  3.3    D  4.8  x/u/v/w    1\ndf.groupby(['col5','col2']).reset_index()\n             index col1 col2 col3     col4 col5\ncol5 col2                                      \n1    A    0      0  1.1    A  1.1    x/y/z    1\n     D    0     11  1.1    D  4.7    x/y/z    1\n          1     12  1.1    D  4.7        x    1\n          2     13  3.3    D  4.8  x/u/v/w    1\n2    B    0      3  2.6    B  2.6      x/u    2\n          1      5  3.4    B  3.8    x/u/v    2\n3    A    0      1  1.1    A  1.7      x/y    3\n          1      2  1.1    A  2.5  x/y/z/n    3\n          2      7  2.6    A  4.2        x    3\n     C    0      9  3.4    C  4.5        -    3\n4    B    0      4  2.5    B  3.3        x    4\n5    B    0      6  2.6    B    4    x/y/z    5\n          1     10  2.6    B  4.6      x/y    5\n6    B    0      8  3.4    B  4.3  x/u/v/b    6\ncol5 col2 count\n1    A      1\n     D      3\n2    B      2\netc...\n""]";"[""df = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T\ndf.columns = ['col1','col2','col3','col4','col5']\n"", '   col1 col2 col3     col4 col5\n0   1.1    A  1.1    x/y/z    1\n1   1.1    A  1.7      x/y    3\n2   1.1    A  2.5  x/y/z/n    3\n3   2.6    B  2.6      x/u    2\n4   2.5    B  3.3        x    4\n5   3.4    B  3.8    x/u/v    2\n6   2.6    B    4    x/y/z    5\n7   2.6    A  4.2        x    3\n8   3.4    B  4.3  x/u/v/b    6\n9   3.4    C  4.5        -    3\n10  2.6    B  4.6      x/y    5\n11  1.1    D  4.7    x/y/z    1\n12  1.1    D  4.7        x    1\n13  3.3    D  4.8  x/u/v/w    1\n', ""df.groupby(['col5','col2']).reset_index()\n"", '             index col1 col2 col3     col4 col5\ncol5 col2                                      \n1    A    0      0  1.1    A  1.1    x/y/z    1\n     D    0     11  1.1    D  4.7    x/y/z    1\n          1     12  1.1    D  4.7        x    1\n          2     13  3.3    D  4.8  x/u/v/w    1\n2    B    0      3  2.6    B  2.6      x/u    2\n          1      5  3.4    B  3.8    x/u/v    2\n3    A    0      1  1.1    A  1.7      x/y    3\n          1      2  1.1    A  2.5  x/y/z/n    3\n          2      7  2.6    A  4.2        x    3\n     C    0      9  3.4    C  4.5        -    3\n4    B    0      4  2.5    B  3.3        x    4\n5    B    0      6  2.6    B    4    x/y/z    5\n          1     10  2.6    B  4.6      x/y    5\n6    B    0      8  3.4    B  4.3  x/u/v/b    6\n', 'col5 col2 count\n1    A      1\n     D      3\n2    B      2\netc...\n']";"[""df = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T\ndf.columns = ['col1','col2','col3','col4','col5']\n"", '   col1 col2 col3     col4 col5\n0   1.1    A  1.1    x/y/z    1\n1   1.1    A  1.7      x/y    3\n2   1.1    A  2.5  x/y/z/n    3\n3   2.6    B  2.6      x/u    2\n4   2.5    B  3.3        x    4\n5   3.4    B  3.8    x/u/v    2\n6   2.6    B    4    x/y/z    5\n7   2.6    A  4.2        x    3\n8   3.4    B  4.3  x/u/v/b    6\n9   3.4    C  4.5        -    3\n10  2.6    B  4.6      x/y    5\n11  1.1    D  4.7    x/y/z    1\n12  1.1    D  4.7        x    1\n13  3.3    D  4.8  x/u/v/w    1\n', ""df.groupby(['col5','col2']).reset_index()\n"", '             index col1 col2 col3     col4 col5\ncol5 col2                                      \n1    A    0      0  1.1    A  1.1    x/y/z    1\n     D    0     11  1.1    D  4.7    x/y/z    1\n          1     12  1.1    D  4.7        x    1\n          2     13  3.3    D  4.8  x/u/v/w    1\n2    B    0      3  2.6    B  2.6      x/u    2\n          1      5  3.4    B  3.8    x/u/v    2\n3    A    0      1  1.1    A  1.7      x/y    3\n          1      2  1.1    A  2.5  x/y/z/n    3\n          2      7  2.6    A  4.2        x    3\n     C    0      9  3.4    C  4.5        -    3\n4    B    0      4  2.5    B  3.3        x    4\n5    B    0      6  2.6    B    4    x/y/z    5\n          1     10  2.6    B  4.6      x/y    5\n6    B    0      8  3.4    B  4.3  x/u/v/b    6\n', 'col5 col2 count\n1    A      1\n     D      3\n2    B      2\netc...\n']";"[""df = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T\ndf.columns = ['col1','col2','col3','col4','col5']\ndf.groupby(['col5','col2']).reset_index()\n""]";"[""import pandas as pd\ndf = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T\ndf.columns = ['col1','col2','col3','col4','col5']\ndf.groupby(['col5','col2']).reset_index()\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T\ndf.columns = ['col1','col2','col3','col4','col5']\ndf.groupby(['col5','col2']).reset_index()\n""]";False;0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""Grouper for 'Alphabet' not 1-dimensional""]";['NameError', 'NameError', 'ValueError'];0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""Grouper for 'Alphabet' not 1-dimensional""]";['NameError', 'NameError', 'ValueError'];0;3;"[""'col5'"", ""'col5'"", ""Grouper for 'Alphabet' not 1-dimensional""]";['KeyError', 'KeyError', 'ValueError']
405;405;405;405;3.0;0;17682613;;1;41;<python><arrays><numpy><pandas><scikit-learn>;How to convert a pandas DataFrame subset of columns AND rows into a numpy array?;99966.0;"[""\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint df\n\n          a         b         c         d         e\n0  0.945686  0.000710  0.909158  0.892892  0.326670\n1  0.919359  0.667057  0.462478  0.008204  0.473096\n2  0.976163  0.621712  0.208423  0.980471  0.048334\n3  0.459039  0.788318  0.309892  0.100539  0.753992\n\nlocs = [df.columns.get_loc(_) for _ in ['a', 'd']]\nprint df[df.c > 0.5][locs]\n\n          a         d\n0  0.945686  0.892892\n\ntraining_set = array(df[df.c > 0.5][locs])\n""]";"[""\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\nprint df\n\n          a         b         c         d         e\n0  0.945686  0.000710  0.909158  0.892892  0.326670\n1  0.919359  0.667057  0.462478  0.008204  0.473096\n2  0.976163  0.621712  0.208423  0.980471  0.048334\n3  0.459039  0.788318  0.309892  0.100539  0.753992\n"", ""\nlocs = [df.columns.get_loc(_) for _ in ['a', 'd']]\nprint df[df.c > 0.5][locs]\n\n          a         d\n0  0.945686  0.892892\n"", '\ntraining_set = array(df[df.c > 0.5][locs])\n']";[];"[""\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\n\n\nlocs = [df.columns.get_loc(_) for _ in ['a', 'd']]\n\n\ntraining_set = array(df[df.c > 0.5][locs])\n""]";"[""from pandas import DataFrame\n\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\n\n\nlocs = [df.columns.get_loc(_) for _ in ['a', 'd']]\n\n\ntraining_set = array(df[df.c > 0.5][locs])\n""]";True;"[""import pandas as pd\n\ndf = DataFrame(np.random.rand(4,5), columns = list('abcde'))\n\n\nlocs = [df.columns.get_loc(_) for _ in ['a', 'd']]\n\n\ntraining_set = array(df[df.c > 0.5][locs])\n""]";False;1;3;"[""name 'df' is not defined"", 'Sucess', ""name 'DataFrame' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""name 'df' is not defined"", 'Sucess', ""name 'np' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""'DataFrame' object has no attribute 'c'"", 'Sucess', ""name 'DataFrame' is not defined""]";['AttributeError', 'Sucess', 'NameError']
406;406;406;406;2.0;0;17690738;;1;41;<python><datetime><pandas>;In Pandas how do I convert a string of date strings to datetime objects and put them in a DataFrame?;50026.0;"[""import pandas as pd\ndate_stngs = ('2008-12-20','2008-12-21','2008-12-22','2008-12-23')\n\na = pd.Series(range(4),index = (range(4)))\n\nfor idx, date in enumerate(date_stngs):\n    a[idx]= pd.to_datetime(date)\n""]";"[""import pandas as pd\ndate_stngs = ('2008-12-20','2008-12-21','2008-12-22','2008-12-23')\n\na = pd.Series(range(4),index = (range(4)))\n\nfor idx, date in enumerate(date_stngs):\n    a[idx]= pd.to_datetime(date)\n""]";"[""import pandas as pd\ndate_stngs = ('2008-12-20','2008-12-21','2008-12-22','2008-12-23')\n\na = pd.Series(range(4),index = (range(4)))\n\nfor idx, date in enumerate(date_stngs):\n    a[idx]= pd.to_datetime(date)\n"", 'DateTime']";"[""import pandas as pd\ndate_stngs = ('2008-12-20','2008-12-21','2008-12-22','2008-12-23')\n\na = pd.Series(range(4),index = (range(4)))\n\n""]";"[""import pandas as pd\ndate_stngs = ('2008-12-20','2008-12-21','2008-12-22','2008-12-23')\n\na = pd.Series(range(4),index = (range(4)))\n\n""]";False;"[""import pandas as pd\nimport pandas as pd\ndate_stngs = ('2008-12-20','2008-12-21','2008-12-22','2008-12-23')\n\na = pd.Series(range(4),index = (range(4)))\n\n""]";False;1;2;"['Sucess', ""name 'pd' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'date_stngs' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'date_stngs' is not defined""]";['Sucess', 'NameError']
407;407;407;407;2.0;0;17691447;;1;22;<python><pandas><dataframe>;Get count of values across columns-Pandas DataFrame;21387.0;['               A              B              C\n0   192.168.2.85   192.168.2.85  124.43.113.22\n1  192.248.8.183  192.248.8.183   192.168.2.85\n2  192.168.2.161            NaN  192.248.8.183\n3   66.249.74.52            NaN  192.168.2.161\n4            NaN            NaN   66.249.74.52\nIP          Count\n192.168.2.85 3 #Since this value is there in all coulmns\n192.248.8.183 3\n192.168.2.161 2\n66.249.74.52 2\n124.43.113.22 1\n'];['               A              B              C\n0   192.168.2.85   192.168.2.85  124.43.113.22\n1  192.248.8.183  192.248.8.183   192.168.2.85\n2  192.168.2.161            NaN  192.248.8.183\n3   66.249.74.52            NaN  192.168.2.161\n4            NaN            NaN   66.249.74.52\n', 'IP          Count\n192.168.2.85 3 #Since this value is there in all coulmns\n192.248.8.183 3\n192.168.2.161 2\n66.249.74.52 2\n124.43.113.22 1\n'];['               A              B              C\n0   192.168.2.85   192.168.2.85  124.43.113.22\n1  192.248.8.183  192.248.8.183   192.168.2.85\n2  192.168.2.161            NaN  192.248.8.183\n3   66.249.74.52            NaN  192.168.2.161\n4            NaN            NaN   66.249.74.52\n', 'IP          Count\n192.168.2.85 3 #Since this value is there in all coulmns\n192.248.8.183 3\n192.168.2.161 2\n66.249.74.52 2\n124.43.113.22 1\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
408;408;408;408;3.0;0;17702272;;1;11;<python><pandas><boolean><type-conversion><series>;Convert Pandas series containing string to boolean;8843.0;"[""  Order Number       Status\n1         1668  Undelivered\n2        19771  Undelivered\n3    100032108  Undelivered\n4         2229    Delivered\n5        00056  Undelivered\nd = {\n  'Delivered': True,\n  'Undelivered': False\n}\n""]";"['  Order Number       Status\n1         1668  Undelivered\n2        19771  Undelivered\n3    100032108  Undelivered\n4         2229    Delivered\n5        00056  Undelivered\n', ""d = {\n  'Delivered': True,\n  'Undelivered': False\n}\n""]";"['df', '  Order Number       Status\n1         1668  Undelivered\n2        19771  Undelivered\n3    100032108  Undelivered\n4         2229    Delivered\n5        00056  Undelivered\n', 'Status', 'True', 'False', 'NotANumber', ""d = {\n  'Delivered': True,\n  'Undelivered': False\n}\n"", 'True', 'False']";[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
409;409;409;409;2.0;1;17709270;;1;20;<python><merge><pandas>;I want to create a column of value_counts in my pandas dataframe;29277.0;['Color Value\nRed   100\nRed   150\nBlue  50\nColor Value Counts\nRed   100   2\nRed   150   2 \nBlue  50    1\n'];['Color Value\nRed   100\nRed   150\nBlue  50\n', 'Color Value Counts\nRed   100   2\nRed   150   2 \nBlue  50    1\n'];['Color Value\nRed   100\nRed   150\nBlue  50\n', 'Color Value Counts\nRed   100   2\nRed   150   2 \nBlue  50    1\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
410;410;410;410;10.0;7;17709641;;1;79;<python><numpy><install><pandas><statsmodels>;ValueError: numpy.dtype has the wrong size, try recompiling;64914.0;"['numpy.dtype has the wrong size, try recompiling\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\formula\\__init__.py"",\nline 4, in <module>\n    from formulatools import handle_formula_data\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\formula\\formulatools.p\ny"", line 1, in <module>\n    import statsmodels.tools.data as data_util\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\tools\\__init__.py"", li\nne 1, in <module>\n    from tools import add_constant, categorical\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\tools\\tools.py"", line\n14, in <module>\n    from pandas import DataFrame\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\pandas\\__init__.py"", line 6, in <module>\n    from . import hashtable, tslib, lib\n  File ""numpy.pxd"", line 157, in init pandas.tslib (pandas\\tslib.c:49133)\nValueError: numpy.dtype has the wrong size, try recompiling\n']";"['numpy.dtype has the wrong size, try recompiling\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\formula\\__init__.py"",\nline 4, in <module>\n    from formulatools import handle_formula_data\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\formula\\formulatools.p\ny"", line 1, in <module>\n    import statsmodels.tools.data as data_util\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\tools\\__init__.py"", li\nne 1, in <module>\n    from tools import add_constant, categorical\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\tools\\tools.py"", line\n14, in <module>\n    from pandas import DataFrame\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\pandas\\__init__.py"", line 6, in <module>\n    from . import hashtable, tslib, lib\n  File ""numpy.pxd"", line 157, in init pandas.tslib (pandas\\tslib.c:49133)\nValueError: numpy.dtype has the wrong size, try recompiling\n']";"['numpy.dtype has the wrong size, try recompiling\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\formula\\__init__.py"",\nline 4, in <module>\n    from formulatools import handle_formula_data\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\formula\\formulatools.p\ny"", line 1, in <module>\n    import statsmodels.tools.data as data_util\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\tools\\__init__.py"", li\nne 1, in <module>\n    from tools import add_constant, categorical\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\statsmodels-0.5.0-py2.7-win32.egg\\statsmodels\\tools\\tools.py"", line\n14, in <module>\n    from pandas import DataFrame\n  File ""C:\\analytics\\ext\\python27\\lib\\site-packages\\pandas\\__init__.py"", line 6, in <module>\n    from . import hashtable, tslib, lib\n  File ""numpy.pxd"", line 157, in init pandas.tslib (pandas\\tslib.c:49133)\nValueError: numpy.dtype has the wrong size, try recompiling\n']";[''];[''];False;['import pandas as pd\n'];False;3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess']
411;411;411;411;1.0;0;17712163;;1;11;<python><pandas>;Pandas: Sorting columns by their mean value;2276.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
412;412;412;412;2.0;0;17729853;;1;20;<python><pandas>;Set value for a selected cell in pandas DataFrame;29305.0;"["">>> d = pd.DataFrame({'year':[2008,2008,2008,2008,2009,2009,2009,2009], \n...                   'flavour':['strawberry','strawberry','banana','banana',\n...                   'strawberry','strawberry','banana','banana'],\n...                   'day':['sat','sun','sat','sun','sat','sun','sat','sun'],\n...                   'sales':[10,12,22,23,11,13,23,24]})\n\n>>> d\n   day     flavour  sales  year\n0  sat  strawberry     10  2008\n1  sun  strawberry     12  2008\n2  sat      banana     22  2008\n3  sun      banana     23  2008\n4  sat  strawberry     11  2009\n5  sun  strawberry     13  2009\n6  sat      banana     23  2009\n7  sun      banana     24  2009\n\n>>> d[d.sales==24]\n   day flavour  sales  year\n7  sun  banana     24  2009\n\n>>> d[d.sales==24].sales = 100\n>>> d\n   day     flavour  sales  year\n0  sat  strawberry     10  2008\n1  sun  strawberry     12  2008\n2  sat      banana     22  2008\n3  sun      banana     23  2008\n4  sat  strawberry     11  2009\n5  sun  strawberry     13  2009\n6  sat      banana     23  2009\n7  sun      banana     24  2009\n""]";"["">>> d = pd.DataFrame({'year':[2008,2008,2008,2008,2009,2009,2009,2009], \n...                   'flavour':['strawberry','strawberry','banana','banana',\n...                   'strawberry','strawberry','banana','banana'],\n...                   'day':['sat','sun','sat','sun','sat','sun','sat','sun'],\n...                   'sales':[10,12,22,23,11,13,23,24]})\n\n>>> d\n   day     flavour  sales  year\n0  sat  strawberry     10  2008\n1  sun  strawberry     12  2008\n2  sat      banana     22  2008\n3  sun      banana     23  2008\n4  sat  strawberry     11  2009\n5  sun  strawberry     13  2009\n6  sat      banana     23  2009\n7  sun      banana     24  2009\n\n>>> d[d.sales==24]\n   day flavour  sales  year\n7  sun  banana     24  2009\n\n>>> d[d.sales==24].sales = 100\n>>> d\n   day     flavour  sales  year\n0  sat  strawberry     10  2008\n1  sun  strawberry     12  2008\n2  sat      banana     22  2008\n3  sun      banana     23  2008\n4  sat  strawberry     11  2009\n5  sun  strawberry     13  2009\n6  sat      banana     23  2009\n7  sun      banana     24  2009\n""]";"["">>> d = pd.DataFrame({'year':[2008,2008,2008,2008,2009,2009,2009,2009], \n...                   'flavour':['strawberry','strawberry','banana','banana',\n...                   'strawberry','strawberry','banana','banana'],\n...                   'day':['sat','sun','sat','sun','sat','sun','sat','sun'],\n...                   'sales':[10,12,22,23,11,13,23,24]})\n\n>>> d\n   day     flavour  sales  year\n0  sat  strawberry     10  2008\n1  sun  strawberry     12  2008\n2  sat      banana     22  2008\n3  sun      banana     23  2008\n4  sat  strawberry     11  2009\n5  sun  strawberry     13  2009\n6  sat      banana     23  2009\n7  sun      banana     24  2009\n\n>>> d[d.sales==24]\n   day flavour  sales  year\n7  sun  banana     24  2009\n\n>>> d[d.sales==24].sales = 100\n>>> d\n   day     flavour  sales  year\n0  sat  strawberry     10  2008\n1  sun  strawberry     12  2008\n2  sat      banana     22  2008\n3  sun      banana     23  2008\n4  sat  strawberry     11  2009\n5  sun  strawberry     13  2009\n6  sat      banana     23  2009\n7  sun      banana     24  2009\n""]";['\n\n\n'];['\n\n\n'];False;['import pandas as pd\n\n\n\n'];False;0;2;"[""name 'd' is not defined"", ""name 'd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'd' is not defined"", ""name 'd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'd' is not defined"", ""name 'd' is not defined""]";['NameError', 'NameError']
413;413;413;413;2.0;0;17758023;;1;12;<python><python-2.7><pandas>;return rows in a dataframe closest to a user-defined number;7922.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
414;414;414;414;3.0;2;17759128;;1;15;<python><pandas>;Python pandas stuck at version 0.7.0;12366.0;"['Ubuntu: 12.04.2 LTS Desktop (virtual workstation on VMWare)\nsudo apt-get update, sudo apt-get upgrade, and sudo apt-get dist-upgrade all current\nPython: 2.7.3 (default, April 10 2013, 06:20:15) /n [GCC 4.6.3] on Linux2\n$ ""which python"" only show a single instance: /usr/bin/python\npandas.__version__ = 0.7.0\nnumpy.__version__ = 1.6.1\n']";"['Ubuntu: 12.04.2 LTS Desktop (virtual workstation on VMWare)\nsudo apt-get update, sudo apt-get upgrade, and sudo apt-get dist-upgrade all current\nPython: 2.7.3 (default, April 10 2013, 06:20:15) /n [GCC 4.6.3] on Linux2\n$ ""which python"" only show a single instance: /usr/bin/python\npandas.__version__ = 0.7.0\nnumpy.__version__ = 1.6.1\n']";"['Ubuntu: 12.04.2 LTS Desktop (virtual workstation on VMWare)\nsudo apt-get update, sudo apt-get upgrade, and sudo apt-get dist-upgrade all current\nPython: 2.7.3 (default, April 10 2013, 06:20:15) /n [GCC 4.6.3] on Linux2\n$ ""which python"" only show a single instance: /usr/bin/python\npandas.__version__ = 0.7.0\nnumpy.__version__ = 1.6.1\n']";[''];[''];False;['import pandas as pd\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
415;415;415;415;3.0;1;17775935;;1;11;<python><pandas><dataframe>;SQL-like window functions in PANDAS: Row Numbering in Python Pandas Dataframe;8442.0;"[""df = pd.DataFrame({'key1' : ['a','a','a','b','a'],\n           'data1' : [1,2,2,3,3],\n           'data2' : [1,10,2,3,30]})\ndf\n     data1        data2     key1    \n0    1            1         a           \n1    2            10        a        \n2    2            2         a       \n3    3            3         b       \n4    3            30        a        \nRN = ROW_NUMBER() OVER (PARTITION BY Key1, Key2 ORDER BY Data1 ASC, Data2 DESC)\n\n\n    data1        data2     key1    RN\n0    1            1         a       1    \n1    2            10        a       2 \n2    2            2         a       3\n3    3            3         b       1\n4    3            30        a       4\ndef row_number(frame,orderby_columns, orderby_direction,name):\n    frame.sort_index(by = orderby_columns, ascending = orderby_direction, inplace = True)\n    frame[name] = list(xrange(len(frame.index)))\ndf1 = df.groupby('key1').apply(lambda t: t.sort_index(by=['data1', 'data2'], ascending=[True, False], inplace = True)).reset_index()\n\ndef nf(x):\n    x['rn'] = list(xrange(len(x.index)))\n\ndf1['rn1'] = df1.groupby('key1').apply(nf)\n""]";"[""df = pd.DataFrame({'key1' : ['a','a','a','b','a'],\n           'data1' : [1,2,2,3,3],\n           'data2' : [1,10,2,3,30]})\ndf\n     data1        data2     key1    \n0    1            1         a           \n1    2            10        a        \n2    2            2         a       \n3    3            3         b       \n4    3            30        a        \n"", 'RN = ROW_NUMBER() OVER (PARTITION BY Key1, Key2 ORDER BY Data1 ASC, Data2 DESC)\n\n\n    data1        data2     key1    RN\n0    1            1         a       1    \n1    2            10        a       2 \n2    2            2         a       3\n3    3            3         b       1\n4    3            30        a       4\n', 'def row_number(frame,orderby_columns, orderby_direction,name):\n    frame.sort_index(by = orderby_columns, ascending = orderby_direction, inplace = True)\n    frame[name] = list(xrange(len(frame.index)))\n', ""df1 = df.groupby('key1').apply(lambda t: t.sort_index(by=['data1', 'data2'], ascending=[True, False], inplace = True)).reset_index()\n\ndef nf(x):\n    x['rn'] = list(xrange(len(x.index)))\n\ndf1['rn1'] = df1.groupby('key1').apply(nf)\n""]";"[""df = pd.DataFrame({'key1' : ['a','a','a','b','a'],\n           'data1' : [1,2,2,3,3],\n           'data2' : [1,10,2,3,30]})\ndf\n     data1        data2     key1    \n0    1            1         a           \n1    2            10        a        \n2    2            2         a       \n3    3            3         b       \n4    3            30        a        \n"", 'RN = ROW_NUMBER() OVER (PARTITION BY Key1, Key2 ORDER BY Data1 ASC, Data2 DESC)\n\n\n    data1        data2     key1    RN\n0    1            1         a       1    \n1    2            10        a       2 \n2    2            2         a       3\n3    3            3         b       1\n4    3            30        a       4\n', 'def row_number(frame,orderby_columns, orderby_direction,name):\n    frame.sort_index(by = orderby_columns, ascending = orderby_direction, inplace = True)\n    frame[name] = list(xrange(len(frame.index)))\n', ""df1 = df.groupby('key1').apply(lambda t: t.sort_index(by=['data1', 'data2'], ascending=[True, False], inplace = True)).reset_index()\n\ndef nf(x):\n    x['rn'] = list(xrange(len(x.index)))\n\ndf1['rn1'] = df1.groupby('key1').apply(nf)\n""]";"[""df\n\n\ndf1 = df.groupby('key1').apply(lambda t: t.sort_index(by=['data1', 'data2'], ascending=[True, False], inplace = True)).reset_index()\n\n\ndf1['rn1'] = df1.groupby('key1').apply(nf)\n""]";"[""df\n\n\ndf1 = df.groupby('key1').apply(lambda t: t.sort_index(by=['data1', 'data2'], ascending=[True, False], inplace = True)).reset_index()\n\n\ndf1['rn1'] = df1.groupby('key1').apply(nf)\n""]";False;"[""import pandas as pd\ndata2 = pd.DataFrame()\ndata1 = pd.DataFrame()\ndata = pd.DataFrame()\ndf = pd.DataFrame()\ndf\n\n\ndf1 = df.groupby('key1').apply(lambda t: t.sort_index(by=['data1', 'data2'], ascending=[True, False], inplace = True)).reset_index()\n\n\ndf1['rn1'] = df1.groupby('key1').apply(nf)\n""]";True;0;1;['invalid syntax (<unknown>, line 2)'];['SyntaxError'];0;1;['invalid syntax (<unknown>, line 2)'];['SyntaxError'];0;1;['invalid syntax (<unknown>, line 7)'];['SyntaxError']
416;416;416;416;3.0;0;17778394;;1;14;<python><pandas>;List Highest Correlation Pairs from a Large Correlation Matrix in Pandas?;8078.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""'Series' object has no attribute 'order'""]";['AttributeError'];0;1;"[""'Series' object has no attribute 'order'""]";['AttributeError'];0;1;"[""'Series' object has no attribute 'order'""]";['AttributeError']
417;417;417;417;2.0;0;17812978;;1;30;<python><matplotlib><plot><pandas><dataframe>;How to plot two columns of a pandas data frame using points?;74379.0;"[""df.plot(x='col_name_1', y='col_name_2')\n""]";"[""df.plot(x='col_name_1', y='col_name_2')\n""]";"['plot', ""df.plot(x='col_name_1', y='col_name_2')\n"", 'kind']";"[""df.plot(x='col_name_1', y='col_name_2')\n""]";"[""df.plot(x='col_name_1', y='col_name_2')\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf.plot(x='col_name_1', y='col_name_2')\n""]";True;0;2;"[""name 'df' is not defined"", ""No module named 'matplotlib'""]";['NameError', 'ImportError'];0;2;"[""name 'df' is not defined"", ""No module named 'matplotlib'""]";['NameError', 'ImportError'];0;2;"[""name 'df' is not defined"", ""No module named 'matplotlib'""]";['NameError', 'ImportError']
418;418;418;418;3.0;1;17818783;;1;25;<python><numpy><scipy><pandas><sparse-matrix>;Populate a Pandas SparseDataFrame from a SciPy Sparse Matrix;9661.0;['return DataFrame(matrix.toarray(), columns=features, index=observations)\n'];['return DataFrame(matrix.toarray(), columns=features, index=observations)\n'];['DataFrame()', 'return DataFrame(matrix.toarray(), columns=features, index=observations)\n', 'SparseDataFrame()', 'scipy.sparse.csc_matrix()', 'csr_matrix()'];['return DataFrame(matrix.toarray(), columns=features, index=observations)\n'];['from pandas import DataFrame\nreturn DataFrame(matrix.toarray(), columns=features, index=observations)\n'];True;['import pandas as pd\nreturn DataFrame(matrix.toarray(), columns=features, index=observations)\n'];False;0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
419;419;419;419;7.0;0;17834995;;1;19;<python><pandas><libreoffice><dataframe><opendocument>;How to convert OpenDocument spreadsheets to a pandas DataFrame?;6003.0;[''];[];['pandas.DataFrame', 'pandas.read_excel(file)', 'pandas.read_excel'];[''];[''];False;['import pandas as pd\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
420;420;420;420;6.0;0;17839973;;1;64;<python><pandas><dataframe>;construct pandas DataFrame from values in variables;53479.0;"[""a = 2\nb = 3\ndf2 = pd.DataFrame({'A':a,'B':b})\ndf2 = (pd.DataFrame({'a':a,'b':b})).reset_index()\n""]";"['a = 2\nb = 3\n', ""df2 = pd.DataFrame({'A':a,'B':b})\n"", ""df2 = (pd.DataFrame({'a':a,'b':b})).reset_index()\n""]";"['a = 2\nb = 3\n', ""df2 = pd.DataFrame({'A':a,'B':b})\n"", ""df2 = (pd.DataFrame({'a':a,'b':b})).reset_index()\n""]";"[""a = 2\nb = 3\ndf2 = pd.DataFrame({'A':a,'B':b})\ndf2 = (pd.DataFrame({'a':a,'b':b})).reset_index()\n""]";"[""import pandas as pd\na = 2\nb = 3\ndf2 = pd.DataFrame({'A':a,'B':b})\ndf2 = (pd.DataFrame({'a':a,'b':b})).reset_index()\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\na = 2\nb = 3\ndf2 = pd.DataFrame({'A':a,'B':b})\ndf2 = (pd.DataFrame({'a':a,'b':b})).reset_index()\n""]";True;1;3;"['Sucess', ""name 'pd' is not defined"", ""name 'pd' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name 'a' is not defined"", ""name 'a' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name 'a' is not defined"", ""name 'a' is not defined""]";['Sucess', 'NameError', 'NameError']
421;421;421;421;4.0;0;17841149;;1;37;<python><pandas>;Pandas groupby: How to get a union of strings;24290.0;"['   A         B       C\n0  1  0.749065    This\n1  2  0.301084      is\n2  3  0.463468       a\n3  4  0.643961  random\n4  1  0.866521  string\n5  2  0.120737       !\nIn [10]: print df.groupby(""A"")[""B""].sum()\nA\n1    1.615586\n2    0.421821\n3    0.463468\n4    0.643961\nA\n1    {This, string}\n2    {is, !}\n3    {a}\n4    {random}\ndf.groupby(""A"")[""B""]\npandas.core.groupby.SeriesGroupBy object\n']";"['   A         B       C\n0  1  0.749065    This\n1  2  0.301084      is\n2  3  0.463468       a\n3  4  0.643961  random\n4  1  0.866521  string\n5  2  0.120737       !\n', 'In [10]: print df.groupby(""A"")[""B""].sum()\n', 'A\n1    1.615586\n2    0.421821\n3    0.463468\n4    0.643961\n', 'A\n1    {This, string}\n2    {is, !}\n3    {a}\n4    {random}\n', 'df.groupby(""A"")[""B""]\n', 'pandas.core.groupby.SeriesGroupBy object\n']";"['   A         B       C\n0  1  0.749065    This\n1  2  0.301084      is\n2  3  0.463468       a\n3  4  0.643961  random\n4  1  0.866521  string\n5  2  0.120737       !\n', 'In [10]: print df.groupby(""A"")[""B""].sum()\n', 'A\n1    1.615586\n2    0.421821\n3    0.463468\n4    0.643961\n', 'A\n1    {This, string}\n2    {is, !}\n3    {a}\n4    {random}\n', 'df.groupby(""A"")[""B""]\n', 'pandas.core.groupby.SeriesGroupBy object\n']";"['A\nA\ndf.groupby(""A"")[""B""]\n']";"['A\nA\ndf.groupby(""A"")[""B""]\n']";False;"['import pandas as pd\ndf = pd.DataFrame()\nA\nA\ndf.groupby(""A"")[""B""]\n']";True;0;2;"[""name 'read_csv' is not defined"", ""name 'A' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'read_csv' is not defined"", ""name 'A' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'read_csv' is not defined"", ""name 'A' is not defined""]";['NameError', 'NameError']
422;422;422;422;4.0;3;17874063;;1;22;<python><pandas><matplotlib>;Is there a parameter in matplotlib/pandas to have the Y axis of a histogram as percentage?;15779.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'np' is not defined"", ""name '_converter' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'np' is not defined"", ""name '_converter' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'np' is not defined"", ""name '_converter' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError', 'NameError']
423;423;423;423;3.0;3;17921010;;1;19;<python><pandas><indexing><slice><multi-index>;How to query MultiIndex index columns values in pandas;36303.0;"[""In [171]: A = np.array([1.1, 1.1, 3.3, 3.3, 5.5, 6.6])\n\nIn [172]: B = np.array([111, 222, 222, 333, 333, 777])\n\nIn [173]: C = randint(10, 99, 6)\n\nIn [174]: df = pd.DataFrame(zip(A, B, C), columns=['A', 'B', 'C'])\n\nIn [175]: df.set_index(['A', 'B'], inplace=True)\n\nIn [176]: df\nOut[176]: \n          C\nA   B      \n1.1 111  20\n    222  31\n3.3 222  24\n    333  65\n5.5 333  22\n6.6 777  74 \n""]";"[""In [171]: A = np.array([1.1, 1.1, 3.3, 3.3, 5.5, 6.6])\n\nIn [172]: B = np.array([111, 222, 222, 333, 333, 777])\n\nIn [173]: C = randint(10, 99, 6)\n\nIn [174]: df = pd.DataFrame(zip(A, B, C), columns=['A', 'B', 'C'])\n\nIn [175]: df.set_index(['A', 'B'], inplace=True)\n\nIn [176]: df\nOut[176]: \n          C\nA   B      \n1.1 111  20\n    222  31\n3.3 222  24\n    333  65\n5.5 333  22\n6.6 777  74 \n""]";"[""In [171]: A = np.array([1.1, 1.1, 3.3, 3.3, 5.5, 6.6])\n\nIn [172]: B = np.array([111, 222, 222, 333, 333, 777])\n\nIn [173]: C = randint(10, 99, 6)\n\nIn [174]: df = pd.DataFrame(zip(A, B, C), columns=['A', 'B', 'C'])\n\nIn [175]: df.set_index(['A', 'B'], inplace=True)\n\nIn [176]: df\nOut[176]: \n          C\nA   B      \n1.1 111  20\n    222  31\n3.3 222  24\n    333  65\n5.5 333  22\n6.6 777  74 \n""]";['A = np.array([1.1, 1.1, 3.3, 3.3, 5.5, 6.6])\n\n\n\n\n\n'];['A = np.array([1.1, 1.1, 3.3, 3.3, 5.5, 6.6])\n\n\n\n\n\n'];False;['import pandas as pd\nA = np.array([1.1, 1.1, 3.3, 3.3, 5.5, 6.6])\n\n\n\n\n\n'];False;0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""'DataFrame' object has no attribute 'A'"", ""name 'df' is not defined"", ""name 'A' is not defined""]";['AttributeError', 'NameError', 'UndefinedVariableError']
424;424;424;424;2.0;1;17926273;;1;19;<python><group-by><pandas>;How to count distinct values in a column of a pandas group by object?;37568.0;['1  1  1\n1  1  1\n1  1  2\n1  2  3\n1  2  3\n1  2  3\n2  1  1\n2  1  2\n2  1  3\n2  2  3\n2  2  3\n2  2  3\n1  1  2\n1  2  1\n2  1  3\n2  2  1\n'];['1  1  1\n1  1  1\n1  1  2\n1  2  3\n1  2  3\n1  2  3\n2  1  1\n2  1  2\n2  1  3\n2  2  3\n2  2  3\n2  2  3\n', '1  1  2\n1  2  1\n2  1  3\n2  2  1\n'];['col1', 'col2', 'col1', 'col2', 'col3', '1  1  1\n1  1  1\n1  1  2\n1  2  3\n1  2  3\n1  2  3\n2  1  1\n2  1  2\n2  1  3\n2  2  3\n2  2  3\n2  2  3\n', '1  1  2\n1  2  1\n2  1  3\n2  2  1\n'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'col1'"", '0']";['KeyError', 'KeyError']
425;425;425;425;4.0;4;17933282;;1;20;<python><numpy><pandas><genfromtxt>;Using numpy.genfromtxt to read a csv file with strings containing commas;21881.0;"['2012, ""Louisville KY"", 3.5\n2011, ""Lexington, KY"", 4.0\nnp.genfromtxt(\'t.csv\', delimiter=\',\')\narray([[\'2012\', \'Louisville KY\', \'3.5\'],\n       [\'2011\', \'Lexington, KY\', \'4.0\']], \n      dtype=\'|S13\')\n']";"['2012, ""Louisville KY"", 3.5\n2011, ""Lexington, KY"", 4.0\n', ""np.genfromtxt('t.csv', delimiter=',')\n"", ""array([['2012', 'Louisville KY', '3.5'],\n       ['2011', 'Lexington, KY', '4.0']], \n      dtype='|S13')\n""]";"['numpy.genfromtxt', '2012, ""Louisville KY"", 3.5\n2011, ""Lexington, KY"", 4.0\n', ""np.genfromtxt('t.csv', delimiter=',')\n"", ""array([['2012', 'Louisville KY', '3.5'],\n       ['2011', 'Lexington, KY', '4.0']], \n      dtype='|S13')\n"", 'csv']";"['2012, ""Louisville KY"", 3.5\n2011, ""Lexington, KY"", 4.0\nnp.genfromtxt(\'t.csv\', delimiter=\',\')\n']";"['2012, ""Louisville KY"", 3.5\n2011, ""Lexington, KY"", 4.0\nnp.genfromtxt(\'t.csv\', delimiter=\',\')\n']";False;"['import pandas as pd\n2012, ""Louisville KY"", 3.5\n2011, ""Lexington, KY"", 4.0\nnp.genfromtxt(\'t.csv\', delimiter=\',\')\n']";False;1;2;"[""[Errno 2] No such file or directory: 'myfile.csv'"", 'Sucess']";['FileNotFoundError', 'Sucess'];1;2;"[""[Errno 2] No such file or directory: 'myfile.csv'"", 'Sucess']";['FileNotFoundError', 'Sucess'];1;2;"[""[Errno 2] No such file or directory: 'myfile.csv'"", 'Sucess']";['FileNotFoundError', 'Sucess']
426;426;426;426;3.0;1;17950374;;1;38;<string><int><pandas>;Converting a column within pandas dataframe from int to string;71902.0;"[""mtrx['X.3'] = mtrx.to_string(columns = ['X.3'])\nmtrx['X.3'] = mtrx['X.3'].astype(str)\n""]";"[""mtrx['X.3'] = mtrx.to_string(columns = ['X.3'])\n"", ""mtrx['X.3'] = mtrx['X.3'].astype(str)\n""]";"['int', 'str', ""mtrx['X.3'] = mtrx.to_string(columns = ['X.3'])\n"", ""mtrx['X.3'] = mtrx['X.3'].astype(str)\n"", 'str']";"[""mtrx['X.3'] = mtrx.to_string(columns = ['X.3'])\nmtrx['X.3'] = mtrx['X.3'].astype(str)\n""]";"[""mtrx['X.3'] = mtrx.to_string(columns = ['X.3'])\nmtrx['X.3'] = mtrx['X.3'].astype(str)\n""]";False;"[""import pandas as pd\nmtrx['X.3'] = mtrx.to_string(columns = ['X.3'])\nmtrx['X.3'] = mtrx['X.3'].astype(str)\n""]";False;1;2;"[""name 'DataFrame' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'np' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'DataFrame' is not defined"", 'Sucess']";['NameError', 'Sucess']
427;427;427;427;1.0;1;17957890;;1;18;<python><numpy><pandas>;pandas select from Dataframe using startswith;13690.0;"[""table2=table[table['SUBDIVISION'] =='INVERNESS']\ncriteria = table['SUBDIVISION'].map(lambda x: x.startswith('INVERNESS'))\ntable2 = table[criteria]\ntable[[x.startswith('INVERNESS') for x in table['SUBDIVISION']]]\n""]";"[""table2=table[table['SUBDIVISION'] =='INVERNESS']\n"", ""criteria = table['SUBDIVISION'].map(lambda x: x.startswith('INVERNESS'))\ntable2 = table[criteria]\n"", ""table[[x.startswith('INVERNESS') for x in table['SUBDIVISION']]]\n""]";"[""table2=table[table['SUBDIVISION'] =='INVERNESS']\n"", ""criteria = table['SUBDIVISION'].map(lambda x: x.startswith('INVERNESS'))\ntable2 = table[criteria]\n"", ""table[[x.startswith('INVERNESS') for x in table['SUBDIVISION']]]\n""]";"[""table2=table[table['SUBDIVISION'] =='INVERNESS']\ncriteria = table['SUBDIVISION'].map(lambda x: x.startswith('INVERNESS'))\ntable2 = table[criteria]\ntable[[x.startswith('INVERNESS') for x in table['SUBDIVISION']]]\n""]";"[""table2=table[table['SUBDIVISION'] =='INVERNESS']\ncriteria = table['SUBDIVISION'].map(lambda x: x.startswith('INVERNESS'))\ntable2 = table[criteria]\ntable[[x.startswith('INVERNESS') for x in table['SUBDIVISION']]]\n""]";False;"[""import pandas as pd\ntable2=table[table['SUBDIVISION'] =='INVERNESS']\ncriteria = table['SUBDIVISION'].map(lambda x: x.startswith('INVERNESS'))\ntable2 = table[criteria]\ntable[[x.startswith('INVERNESS') for x in table['SUBDIVISION']]]\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
428;428;428;428;1.0;1;17960511;;1;13;<python><pandas>;Pandas: Subindexing dataframes: Copies vs views;5342.0;['import pandas as pd\nimport numpy as np\nfoo = pd.DataFrame(np.random.random((10,5)))\nbar = foo.iloc[3:5,1:4]\n'];['import pandas as pd\nimport numpy as np\nfoo = pd.DataFrame(np.random.random((10,5)))\n', 'bar = foo.iloc[3:5,1:4]\n'];['import pandas as pd\nimport numpy as np\nfoo = pd.DataFrame(np.random.random((10,5)))\n', 'bar = foo.iloc[3:5,1:4]\n', 'bar', 'foo', 'view'];['import pandas as pd\nimport numpy as np\nfoo = pd.DataFrame(np.random.random((10,5)))\nbar = foo.iloc[3:5,1:4]\n'];['import pandas as pd\nimport numpy as np\nfoo = pd.DataFrame(np.random.random((10,5)))\nbar = foo.iloc[3:5,1:4]\n'];False;['import pandas as pd\nimport pandas as pd\nimport numpy as np\nfoo = pd.DataFrame(np.random.random((10,5)))\nbar = foo.iloc[3:5,1:4]\n'];False;0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
429;429;429;429;2.0;0;17972938;;1;21;<python><python-2.7><pandas>;check if string in pandas dataframe column is in list;27027.0;"['frame = pd.DataFrame({\'a\' : [\'the cat is blue\', \'the sky is green\', \'the dog is black\']})\nframe[\'b\'] = frame.a.str.contains(""dog"") | frame.a.str.contains(""cat"") | frame.a.str.contains(""fish"")\nTrue\nFalse\nTrue\nmylist =[\'dog\', \'cat\', \'fish\']\n']";"[""frame = pd.DataFrame({'a' : ['the cat is blue', 'the sky is green', 'the dog is black']})\n"", 'frame[\'b\'] = frame.a.str.contains(""dog"") | frame.a.str.contains(""cat"") | frame.a.str.contains(""fish"")\n', 'True\nFalse\nTrue\n', ""mylist =['dog', 'cat', 'fish']\n""]";"[""frame = pd.DataFrame({'a' : ['the cat is blue', 'the sky is green', 'the dog is black']})\n"", 'frame[\'b\'] = frame.a.str.contains(""dog"") | frame.a.str.contains(""cat"") | frame.a.str.contains(""fish"")\n', ""frame['b']"", 'True\nFalse\nTrue\n', ""mylist =['dog', 'cat', 'fish']\n""]";"['frame = pd.DataFrame({\'a\' : [\'the cat is blue\', \'the sky is green\', \'the dog is black\']})\nframe[\'b\'] = frame.a.str.contains(""dog"") | frame.a.str.contains(""cat"") | frame.a.str.contains(""fish"")\nTrue\nFalse\nTrue\nmylist =[\'dog\', \'cat\', \'fish\']\n']";"['import pandas as pd\nframe = pd.DataFrame({\'a\' : [\'the cat is blue\', \'the sky is green\', \'the dog is black\']})\nframe[\'b\'] = frame.a.str.contains(""dog"") | frame.a.str.contains(""cat"") | frame.a.str.contains(""fish"")\nTrue\nFalse\nTrue\nmylist =[\'dog\', \'cat\', \'fish\']\n']";True;"['import pandas as pd\nframe = pd.DataFrame({\'a\' : [\'the cat is blue\', \'the sky is green\', \'the dog is black\']})\nframe[\'b\'] = frame.a.str.contains(""dog"") | frame.a.str.contains(""cat"") | frame.a.str.contains(""fish"")\nTrue\nFalse\nTrue\nmylist =[\'dog\', \'cat\', \'fish\']\n']";False;0;1;"[""name 'mylist' is not defined""]";['NameError'];0;1;"[""name 'mylist' is not defined""]";['NameError'];0;1;"[""name 'mylist' is not defined""]";['NameError']
430;430;430;430;2.0;0;17977540;;1;32;<python><pandas>;Pandas: Looking up the list of sheets in an excel file;16531.0;"[""read_excel('path_to_file.xls', 'Sheet1', index_col=None, na_values=['NA'])\n""]";"[""read_excel('path_to_file.xls', 'Sheet1', index_col=None, na_values=['NA'])\n""]";"[""read_excel('path_to_file.xls', 'Sheet1', index_col=None, na_values=['NA'])\n"", 'N']";"[""read_excel('path_to_file.xls', 'Sheet1', index_col=None, na_values=['NA'])\n""]";"[""read_excel('path_to_file.xls', 'Sheet1', index_col=None, na_values=['NA'])\n""]";False;"[""import pandas as pd\nread_excel('path_to_file.xls', 'Sheet1', index_col=None, na_values=['NA'])\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;['Install xlrd >= 0.9.0 for Excel support'];['ImportError'];0;1;['Install xlrd >= 0.9.0 for Excel support'];['ImportError']
431;431;431;431;4.0;1;17978092;;1;30;<python><pandas>;Combine Date and Time columns using python pandas;16717.0;['Date              Time\n01-06-2013      23:00:00\n02-06-2013      01:00:00\n02-06-2013      21:00:00\n02-06-2013      22:00:00\n02-06-2013      23:00:00\n03-06-2013      01:00:00\n03-06-2013      21:00:00\n03-06-2013      22:00:00\n03-06-2013      23:00:00\n04-06-2013      01:00:00\nDate\n01-06-2013 23:00:00\n02-06-2013 01:00:00\n02-06-2013 21:00:00\n02-06-2013 22:00:00\n02-06-2013 23:00:00\n03-06-2013 01:00:00\n03-06-2013 21:00:00\n03-06-2013 22:00:00\n03-06-2013 23:00:00\n04-06-2013 01:00:00\n'];['Date              Time\n01-06-2013      23:00:00\n02-06-2013      01:00:00\n02-06-2013      21:00:00\n02-06-2013      22:00:00\n02-06-2013      23:00:00\n03-06-2013      01:00:00\n03-06-2013      21:00:00\n03-06-2013      22:00:00\n03-06-2013      23:00:00\n04-06-2013      01:00:00\n', 'Date\n01-06-2013 23:00:00\n02-06-2013 01:00:00\n02-06-2013 21:00:00\n02-06-2013 22:00:00\n02-06-2013 23:00:00\n03-06-2013 01:00:00\n03-06-2013 21:00:00\n03-06-2013 22:00:00\n03-06-2013 23:00:00\n04-06-2013 01:00:00\n'];['Date              Time\n01-06-2013      23:00:00\n02-06-2013      01:00:00\n02-06-2013      21:00:00\n02-06-2013      22:00:00\n02-06-2013      23:00:00\n03-06-2013      01:00:00\n03-06-2013      21:00:00\n03-06-2013      22:00:00\n03-06-2013      23:00:00\n04-06-2013      01:00:00\n', 'pd.to_datetime', 'Date\n01-06-2013 23:00:00\n02-06-2013 01:00:00\n02-06-2013 21:00:00\n02-06-2013 22:00:00\n02-06-2013 23:00:00\n03-06-2013 01:00:00\n03-06-2013 21:00:00\n03-06-2013 22:00:00\n03-06-2013 23:00:00\n04-06-2013 01:00:00\n'];['Date\n'];['Date\n'];False;['import pandas as pd\nDate\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;"[""'Date'"", 'Sucess']";['KeyError', 'Sucess']
432;432;432;432;4.0;1;17978133;;1;24;<python><merge><pandas>;Python Pandas merge only certain columns;15297.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df2' is not defined""]";['NameError'];0;1;"[""name 'df2' is not defined""]";['NameError'];0;1;"['""[\'x\' \'a\' \'b\'] not in index""']";['KeyError']
433;433;433;433;3.0;0;17995024;;1;17;<pandas>;How to assign a name to the a size() column?;4313.0;"[""grpd = df.groupby(['A','B'])\ngrpd['size'] = grpd.size()\ngrpd\n""]";"[""grpd = df.groupby(['A','B'])\ngrpd['size'] = grpd.size()\ngrpd\n""]";"[""grpd = df.groupby(['A','B'])\ngrpd['size'] = grpd.size()\ngrpd\n""]";"[""grpd = df.groupby(['A','B'])\ngrpd['size'] = grpd.size()\ngrpd\n""]";"[""import pandas as pd\ngrpd = df.groupby(['A','B'])\ngrpd['size'] = grpd.size()\ngrpd\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\ngrpd = df.groupby(['A','B'])\ngrpd['size'] = grpd.size()\ngrpd\n""]";True;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'A'"", ""'A'""]";['KeyError', 'KeyError']
434;434;434;434;1.0;0;18012505;;1;18;<python><dictionary><data-conversion><dataframe>;python pandas dataframe columns convert to dict key and value;19984.0;['           area  count\nco tp\nDE Lake      10      7\nForest       20      5\nFR Lake      30      2\nForest       40      3\n'];['           area  count\nco tp\nDE Lake      10      7\nForest       20      5\nFR Lake      30      2\nForest       40      3\n'];['           area  count\nco tp\nDE Lake      10      7\nForest       20      5\nFR Lake      30      2\nForest       40      3\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'lakes' is not defined""]";['NameError'];0;1;"[""name 'lakes' is not defined""]";['NameError'];0;1;"[""name 'lakes' is not defined""]";['NameError']
435;435;435;435;6.0;0;18022845;;1;100;<python><pandas><dataframe><columnname>;Pandas index column title or name;114634.0;"['             Column 1\nIndex Title          \nApples              1\nOranges             2\nPuppies             3\nDucks               4  \nimport pandas as pd\ndata = {\'Column 1\'     : [1., 2., 3., 4.],\n        \'Index Title\'  : [""Apples"", ""Oranges"", ""Puppies"", ""Ducks""]}\ndf = pd.DataFrame(data)\ndf.index = df[""Index Title""]\ndel df[""Index Title""]\nprint df\n']";"['             Column 1\nIndex Title          \nApples              1\nOranges             2\nPuppies             3\nDucks               4  \n', 'import pandas as pd\ndata = {\'Column 1\'     : [1., 2., 3., 4.],\n        \'Index Title\'  : [""Apples"", ""Oranges"", ""Puppies"", ""Ducks""]}\ndf = pd.DataFrame(data)\ndf.index = df[""Index Title""]\ndel df[""Index Title""]\nprint df\n']";"['             Column 1\nIndex Title          \nApples              1\nOranges             2\nPuppies             3\nDucks               4  \n', 'import pandas as pd\ndata = {\'Column 1\'     : [1., 2., 3., 4.],\n        \'Index Title\'  : [""Apples"", ""Oranges"", ""Puppies"", ""Ducks""]}\ndf = pd.DataFrame(data)\ndf.index = df[""Index Title""]\ndel df[""Index Title""]\nprint df\n']";"['import pandas as pd\ndf = pd.DataFrame(data)\ndf.index = df[""Index Title""]\ndel df[""Index Title""]\n']";"['import pandas as pd\ndf = pd.DataFrame(data)\ndf.index = df[""Index Title""]\ndel df[""Index Title""]\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\nimport pandas as pd\ndf = pd.DataFrame(data)\ndf.index = df[""Index Title""]\ndel df[""Index Title""]\n']";True;2;5;"[""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined"", 'Sucess', ""name 'foo' is not defined""]";['NameError', 'Sucess', 'NameError', 'Sucess', 'NameError'];2;5;"[""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined"", 'Sucess', ""name 'foo' is not defined""]";['NameError', 'Sucess', 'NameError', 'Sucess', 'NameError'];4;5;"['Sucess', 'Sucess', 'Sucess', 'Sucess', ""name 'foo' is not defined""]";['Sucess', 'Sucess', 'Sucess', 'Sucess', 'NameError']
436;436;436;436;11.0;1;18039057;;1;84;<python><csv><pandas>;Python Pandas Error tokenizing data;84465.0;"[""pandas.parser.CParserError: Error tokenizing data. C error: Expected 2 fields in line 3,  saw 12\npath = 'GOOG Key Ratios.csv'\n#print(open(path).read())\ndata = pd.read_csv(path)\n""]";"['pandas.parser.CParserError: Error tokenizing data. C error: Expected 2 fields in line 3,  saw 12\n', ""path = 'GOOG Key Ratios.csv'\n#print(open(path).read())\ndata = pd.read_csv(path)\n""]";"['pandas.parser.CParserError: Error tokenizing data. C error: Expected 2 fields in line 3,  saw 12\n', ""path = 'GOOG Key Ratios.csv'\n#print(open(path).read())\ndata = pd.read_csv(path)\n"", 'csv']";"[""path = 'GOOG Key Ratios.csv'\n#print(open(path).read())\ndata = pd.read_csv(path)\n""]";"[""import pandas as pd\npath = 'GOOG Key Ratios.csv'\n#print(open(path).read())\ndata = pd.read_csv(path)\n""]";True;"[""import pandas as pd\npath = 'GOOG Key Ratios.csv'\n#print(open(path).read())\ndata = pd.read_csv(path)\n""]";False;1;5;"['Sucess', ""name 'pd' is not defined"", ""name 'pandas' is not defined"", ""name 'pd' is not defined"", ""name 'pd' is not defined""]";['Sucess', 'NameError', 'NameError', 'NameError', 'NameError'];1;5;"['Sucess', 'No columns to parse from file', ""name 'pandas' is not defined"", ""File b'File_path' does not exist"", 'No columns to parse from file']";['Sucess', 'EmptyDataError', 'NameError', 'FileNotFoundError', 'EmptyDataError'];1;5;"['Sucess', 'No columns to parse from file', ""name 'pandas' is not defined"", ""File b'File_path' does not exist"", 'No columns to parse from file']";['Sucess', 'EmptyDataError', 'NameError', 'FileNotFoundError', 'EmptyDataError']
437;437;437;437;1.0;0;18060619;;1;17;<python><pandas>;Difference between asfreq and resample;4888.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'date_range' is not defined""]";['NameError'];0;1;"[""name 'date_range' is not defined""]";['NameError'];0;1;"[""name 'date_range' is not defined""]";['NameError']
438;438;438;438;5.0;0;18062135;;1;113;<python><pandas><series><dataframe>;Combining two Series into a DataFrame in pandas;90070.0;[''];[];['s1', 's2', 's1', 's2'];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'Series' is not defined"", ""name 'pd' is not defined"", ""name 'a' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'Series' is not defined"", ""name 's2' is not defined"", ""name 'a' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'Series' is not defined"", ""name 's2' is not defined"", ""name 'a' is not defined""]";['NameError', 'NameError', 'NameError']
439;439;439;439;2.0;0;18067073;;1;13;<python><numpy><pandas><virtualenv><pip>;Speedup virtualenv creation with numpy and pandas;2062.0;['pip install my_precompiled_numpy \n'];['pip install my_precompiled_numpy \n'];['pip install my_precompiled_numpy \n'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
440;440;440;440;5.0;0;18079563;;1;22;<python><pandas><series>;Finding the intersection between two series in Pandas;23715.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'pd' is not defined"", ""name 'pd' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 's1' is not defined"", ""name 'np' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 's1' is not defined"", ""name 'np' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError', 'NameError']
441;441;441;441;6.0;4;18089667;;1;42;<python><pandas>;How to estimate how much memory a Pandas' DataFrame will need?;17587.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;2;4;"[""name 'df' is not defined"", ""name 'DataFrame' is not defined"", 'Sucess', 'Sucess']";['NameError', 'NameError', 'Sucess', 'Sucess'];2;4;"[""name 'df' is not defined"", ""name 'DataFrame' is not defined"", 'Sucess', 'Sucess']";['NameError', 'NameError', 'Sucess', 'Sucess'];2;4;"['as_blocks is deprecated and will be removed in a future version', ""name 'DataFrame' is not defined"", 'Sucess', 'Sucess']";['FutureWarning', 'NameError', 'Sucess', 'Sucess']
442;442;442;442;3.0;0;18096748;;1;16;<python><html><pandas>;Pandas Dataframes to_html: Highlighting table rows;14996.0;"['<table border=""1"">\n  <tr style=""background-color:#FF0000"">\n    <th>Month</th>\n    <th>Savings</th>\n  </tr>\n  <tr>\n    <td>January</td>\n    <td>$100</td>\n  </tr>\n</table>\n']";"['<table border=""1"">\n  <tr style=""background-color:#FF0000"">\n    <th>Month</th>\n    <th>Savings</th>\n  </tr>\n  <tr>\n    <td>January</td>\n    <td>$100</td>\n  </tr>\n</table>\n']";"['<table border=""1"">\n  <tr style=""background-color:#FF0000"">\n    <th>Month</th>\n    <th>Savings</th>\n  </tr>\n  <tr>\n    <td>January</td>\n    <td>$100</td>\n  </tr>\n</table>\n', '<tr style=""""background-color:#FF0000"">', '<tr>']";[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'highlight_last_row' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'highlight_last_row' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'highlight_last_row' is not defined""]";['NameError', 'NameError']
443;443;443;443;3.0;4;18107953;;1;15;<python><sql><pandas><bigdata>;How to create a large pandas dataframe from an sql query without running out of memory?;17209.0;"['import pandas.io.sql as psql\nsql = ""SELECT TOP 1000000 * FROM MyTable"" \ndata = psql.read_frame(sql, cnxn)\nsql = ""SELECT TOP 2000000 * FROM MyTable"" \ndata = psql.read_frame(sql, cnxn)\nFile ""inference.pyx"", line 931, in pandas.lib.to_object_array_tuples\n(pandas\\lib.c:42733) Memory Error\nread_csv(\'exp4326.csv\', iterator=True, chunksize=1000)\n']";"['import pandas.io.sql as psql\nsql = ""SELECT TOP 1000000 * FROM MyTable"" \ndata = psql.read_frame(sql, cnxn)\n', 'sql = ""SELECT TOP 2000000 * FROM MyTable"" \ndata = psql.read_frame(sql, cnxn)\n', 'File ""inference.pyx"", line 931, in pandas.lib.to_object_array_tuples\n(pandas\\lib.c:42733) Memory Error\n', ""read_csv('exp4326.csv', iterator=True, chunksize=1000)\n""]";"['import pandas.io.sql as psql\nsql = ""SELECT TOP 1000000 * FROM MyTable"" \ndata = psql.read_frame(sql, cnxn)\n', 'sql = ""SELECT TOP 2000000 * FROM MyTable"" \ndata = psql.read_frame(sql, cnxn)\n', 'File ""inference.pyx"", line 931, in pandas.lib.to_object_array_tuples\n(pandas\\lib.c:42733) Memory Error\n', ""read_csv('exp4326.csv', iterator=True, chunksize=1000)\n""]";"['import pandas.io.sql as psql\nsql = ""SELECT TOP 1000000 * FROM MyTable"" \ndata = psql.read_frame(sql, cnxn)\nsql = ""SELECT TOP 2000000 * FROM MyTable"" \ndata = psql.read_frame(sql, cnxn)\nread_csv(\'exp4326.csv\', iterator=True, chunksize=1000)\n']";"['import pandas.io.sql as psql\nsql = ""SELECT TOP 1000000 * FROM MyTable"" \ndata = psql.read_frame(sql, cnxn)\nsql = ""SELECT TOP 2000000 * FROM MyTable"" \ndata = psql.read_frame(sql, cnxn)\nread_csv(\'exp4326.csv\', iterator=True, chunksize=1000)\n']";False;"['import pandas as pd\nimport pandas.io.sql as psql\nsql = ""SELECT TOP 1000000 * FROM MyTable"" \ndata = psql.read_frame(sql, cnxn)\nsql = ""SELECT TOP 2000000 * FROM MyTable"" \ndata = psql.read_frame(sql, cnxn)\nread_csv(\'exp4326.csv\', iterator=True, chunksize=1000)\n']";False;1;2;['No objects to concatenate', 'Sucess'];['ValueError', 'Sucess'];1;2;['No objects to concatenate', 'Sucess'];['ValueError', 'Sucess'];1;2;['No objects to concatenate', 'Sucess'];['ValueError', 'Sucess']
444;444;444;444;5.0;0;18161926;;1;18;<python><pandas>;Pandas data frame from dictionary;26807.0;"[""sample={'user1': {'item1': 2.5, 'item2': 3.5, 'item3': 3.0, 'item4': 3.5, 'item5': 2.5, 'item6': 3.0}, \n'user2': {'item1': 2.5, 'item2': 3.0, 'item3': 3.5, 'item4': 4.0}, \n'user3': {'item2':4.5,'item5':1.0,'item6':4.0}}\n     col1   col2  col3\n0   user1  item1   2.5\n1   user1  item2   3.5\n2   user1  item3   3.0\n3   user1  item4   3.5\n4   user1  item5   2.5\n5   user1  item6   3.0\n6   user2  item1   2.5\n7   user2  item2   3.0\n8   user2  item3   3.5\n9   user2  item4   4.0\n10  user3  item2   4.5\n11  user3  item5   1.0\n12  user3  item6   4.0\n""]";"[""sample={'user1': {'item1': 2.5, 'item2': 3.5, 'item3': 3.0, 'item4': 3.5, 'item5': 2.5, 'item6': 3.0}, \n'user2': {'item1': 2.5, 'item2': 3.0, 'item3': 3.5, 'item4': 4.0}, \n'user3': {'item2':4.5,'item5':1.0,'item6':4.0}}\n"", '     col1   col2  col3\n0   user1  item1   2.5\n1   user1  item2   3.5\n2   user1  item3   3.0\n3   user1  item4   3.5\n4   user1  item5   2.5\n5   user1  item6   3.0\n6   user2  item1   2.5\n7   user2  item2   3.0\n8   user2  item3   3.5\n9   user2  item4   4.0\n10  user3  item2   4.5\n11  user3  item5   1.0\n12  user3  item6   4.0\n']";"[""sample={'user1': {'item1': 2.5, 'item2': 3.5, 'item3': 3.0, 'item4': 3.5, 'item5': 2.5, 'item6': 3.0}, \n'user2': {'item1': 2.5, 'item2': 3.0, 'item3': 3.5, 'item4': 4.0}, \n'user3': {'item2':4.5,'item5':1.0,'item6':4.0}}\n"", '     col1   col2  col3\n0   user1  item1   2.5\n1   user1  item2   3.5\n2   user1  item3   3.0\n3   user1  item4   3.5\n4   user1  item5   2.5\n5   user1  item6   3.0\n6   user2  item1   2.5\n7   user2  item2   3.0\n8   user2  item3   3.5\n9   user2  item4   4.0\n10  user3  item2   4.5\n11  user3  item5   1.0\n12  user3  item6   4.0\n']";[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'pd' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'sample' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'sample' is not defined""]";['Sucess', 'NameError']
445;445;445;445;1.0;0;18171739;;1;82;<python><csv><pandas><unicode>;UnicodeDecodeError when reading CSV file in Pandas with Python;46551.0;"['   File ""C:\\Importer\\src\\dfman\\importer.py"", line 26, in import_chr\n     data = pd.read_csv(filepath, names=fields)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 400, in parser_f\n     return _read(filepath_or_buffer, kwds)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 205, in _read\n     return parser.read()\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 608, in read\n     ret = self._engine.read(nrows)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 1028, in read\n     data = self._reader.read(nrows)\n   File ""parser.pyx"", line 706, in pandas.parser.TextReader.read (pandas\\parser.c:6745)\n   File ""parser.pyx"", line 728, in pandas.parser.TextReader._read_low_memory (pandas\\parser.c:6964)\n   File ""parser.pyx"", line 804, in pandas.parser.TextReader._read_rows (pandas\\parser.c:7780)\n   File ""parser.pyx"", line 890, in pandas.parser.TextReader._convert_column_data (pandas\\parser.c:8793)\n   File ""parser.pyx"", line 950, in pandas.parser.TextReader._convert_tokens (pandas\\parser.c:9484)\n   File ""parser.pyx"", line 1026, in pandas.parser.TextReader._convert_with_dtype (pandas\\parser.c:10642)\n   File ""parser.pyx"", line 1046, in pandas.parser.TextReader._string_convert (pandas\\parser.c:10853)\n   File ""parser.pyx"", line 1278, in pandas.parser._string_box_utf8 (pandas\\parser.c:15657)\n UnicodeDecodeError: \'utf-8\' codec can\'t decode byte 0xda in position 6: invalid    continuation byte\n']";"['   File ""C:\\Importer\\src\\dfman\\importer.py"", line 26, in import_chr\n     data = pd.read_csv(filepath, names=fields)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 400, in parser_f\n     return _read(filepath_or_buffer, kwds)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 205, in _read\n     return parser.read()\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 608, in read\n     ret = self._engine.read(nrows)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 1028, in read\n     data = self._reader.read(nrows)\n   File ""parser.pyx"", line 706, in pandas.parser.TextReader.read (pandas\\parser.c:6745)\n   File ""parser.pyx"", line 728, in pandas.parser.TextReader._read_low_memory (pandas\\parser.c:6964)\n   File ""parser.pyx"", line 804, in pandas.parser.TextReader._read_rows (pandas\\parser.c:7780)\n   File ""parser.pyx"", line 890, in pandas.parser.TextReader._convert_column_data (pandas\\parser.c:8793)\n   File ""parser.pyx"", line 950, in pandas.parser.TextReader._convert_tokens (pandas\\parser.c:9484)\n   File ""parser.pyx"", line 1026, in pandas.parser.TextReader._convert_with_dtype (pandas\\parser.c:10642)\n   File ""parser.pyx"", line 1046, in pandas.parser.TextReader._string_convert (pandas\\parser.c:10853)\n   File ""parser.pyx"", line 1278, in pandas.parser._string_box_utf8 (pandas\\parser.c:15657)\n UnicodeDecodeError: \'utf-8\' codec can\'t decode byte 0xda in position 6: invalid    continuation byte\n']";"['   File ""C:\\Importer\\src\\dfman\\importer.py"", line 26, in import_chr\n     data = pd.read_csv(filepath, names=fields)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 400, in parser_f\n     return _read(filepath_or_buffer, kwds)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 205, in _read\n     return parser.read()\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 608, in read\n     ret = self._engine.read(nrows)\n   File ""C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py"", line 1028, in read\n     data = self._reader.read(nrows)\n   File ""parser.pyx"", line 706, in pandas.parser.TextReader.read (pandas\\parser.c:6745)\n   File ""parser.pyx"", line 728, in pandas.parser.TextReader._read_low_memory (pandas\\parser.c:6964)\n   File ""parser.pyx"", line 804, in pandas.parser.TextReader._read_rows (pandas\\parser.c:7780)\n   File ""parser.pyx"", line 890, in pandas.parser.TextReader._convert_column_data (pandas\\parser.c:8793)\n   File ""parser.pyx"", line 950, in pandas.parser.TextReader._convert_tokens (pandas\\parser.c:9484)\n   File ""parser.pyx"", line 1026, in pandas.parser.TextReader._convert_with_dtype (pandas\\parser.c:10642)\n   File ""parser.pyx"", line 1046, in pandas.parser.TextReader._string_convert (pandas\\parser.c:10853)\n   File ""parser.pyx"", line 1278, in pandas.parser._string_box_utf8 (pandas\\parser.c:15657)\n UnicodeDecodeError: \'utf-8\' codec can\'t decode byte 0xda in position 6: invalid    continuation byte\n']";[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
446;446;446;446;4.0;1;18172851;;1;194;<python><pandas>;Deleting DataFrame row in Pandas based on column value;264894.0;['             daysago  line_race rating        rw    wrating\n line_date                                                 \n 2007-03-31       62         11     56  1.000000  56.000000\n 2007-03-10       83         11     67  1.000000  67.000000\n 2007-02-10      111          9     66  1.000000  66.000000\n 2007-01-13      139         10     83  0.880678  73.096278\n 2006-12-23      160         10     88  0.793033  69.786942\n 2006-11-09      204          9     52  0.636655  33.106077\n 2006-10-22      222          8     66  0.581946  38.408408\n 2006-09-29      245          9     70  0.518825  36.317752\n 2006-09-16      258         11     68  0.486226  33.063381\n 2006-08-30      275          8     72  0.446667  32.160051\n 2006-02-11      475          5     65  0.164591  10.698423\n 2006-01-13      504          0     70  0.142409   9.968634\n 2006-01-02      515          0     64  0.134800   8.627219\n 2005-12-06      542          0     70  0.117803   8.246238\n 2005-11-29      549          0     70  0.113758   7.963072\n 2005-11-22      556          0     -1  0.109852  -0.109852\n 2005-11-01      577          0     -1  0.098919  -0.098919\n 2005-10-20      589          0     -1  0.093168  -0.093168\n 2005-09-27      612          0     -1  0.083063  -0.083063\n 2005-09-07      632          0     -1  0.075171  -0.075171\n 2005-06-12      719          0     69  0.048690   3.359623\n 2005-05-29      733          0     -1  0.045404  -0.045404\n 2005-05-02      760          0     -1  0.039679  -0.039679\n 2005-04-02      790          0     -1  0.034160  -0.034160\n 2005-03-13      810          0     -1  0.030915  -0.030915\n 2004-11-09      934          0     -1  0.016647  -0.016647\n'];['             daysago  line_race rating        rw    wrating\n line_date                                                 \n 2007-03-31       62         11     56  1.000000  56.000000\n 2007-03-10       83         11     67  1.000000  67.000000\n 2007-02-10      111          9     66  1.000000  66.000000\n 2007-01-13      139         10     83  0.880678  73.096278\n 2006-12-23      160         10     88  0.793033  69.786942\n 2006-11-09      204          9     52  0.636655  33.106077\n 2006-10-22      222          8     66  0.581946  38.408408\n 2006-09-29      245          9     70  0.518825  36.317752\n 2006-09-16      258         11     68  0.486226  33.063381\n 2006-08-30      275          8     72  0.446667  32.160051\n 2006-02-11      475          5     65  0.164591  10.698423\n 2006-01-13      504          0     70  0.142409   9.968634\n 2006-01-02      515          0     64  0.134800   8.627219\n 2005-12-06      542          0     70  0.117803   8.246238\n 2005-11-29      549          0     70  0.113758   7.963072\n 2005-11-22      556          0     -1  0.109852  -0.109852\n 2005-11-01      577          0     -1  0.098919  -0.098919\n 2005-10-20      589          0     -1  0.093168  -0.093168\n 2005-09-27      612          0     -1  0.083063  -0.083063\n 2005-09-07      632          0     -1  0.075171  -0.075171\n 2005-06-12      719          0     69  0.048690   3.359623\n 2005-05-29      733          0     -1  0.045404  -0.045404\n 2005-05-02      760          0     -1  0.039679  -0.039679\n 2005-04-02      790          0     -1  0.034160  -0.034160\n 2005-03-13      810          0     -1  0.030915  -0.030915\n 2004-11-09      934          0     -1  0.016647  -0.016647\n'];['             daysago  line_race rating        rw    wrating\n line_date                                                 \n 2007-03-31       62         11     56  1.000000  56.000000\n 2007-03-10       83         11     67  1.000000  67.000000\n 2007-02-10      111          9     66  1.000000  66.000000\n 2007-01-13      139         10     83  0.880678  73.096278\n 2006-12-23      160         10     88  0.793033  69.786942\n 2006-11-09      204          9     52  0.636655  33.106077\n 2006-10-22      222          8     66  0.581946  38.408408\n 2006-09-29      245          9     70  0.518825  36.317752\n 2006-09-16      258         11     68  0.486226  33.063381\n 2006-08-30      275          8     72  0.446667  32.160051\n 2006-02-11      475          5     65  0.164591  10.698423\n 2006-01-13      504          0     70  0.142409   9.968634\n 2006-01-02      515          0     64  0.134800   8.627219\n 2005-12-06      542          0     70  0.117803   8.246238\n 2005-11-29      549          0     70  0.113758   7.963072\n 2005-11-22      556          0     -1  0.109852  -0.109852\n 2005-11-01      577          0     -1  0.098919  -0.098919\n 2005-10-20      589          0     -1  0.093168  -0.093168\n 2005-09-27      612          0     -1  0.083063  -0.083063\n 2005-09-07      632          0     -1  0.075171  -0.075171\n 2005-06-12      719          0     69  0.048690   3.359623\n 2005-05-29      733          0     -1  0.045404  -0.045404\n 2005-05-02      760          0     -1  0.039679  -0.039679\n 2005-04-02      790          0     -1  0.034160  -0.034160\n 2005-03-13      810          0     -1  0.030915  -0.030915\n 2004-11-09      934          0     -1  0.016647  -0.016647\n', 'line_race', '0'];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""'DataFrame' object has no attribute 'line_race'"", ""name 'df' is not defined""]";['NameError', 'AttributeError', 'NameError']
447;447;447;447;2.0;0;18176933;;1;13;<python><indexing><pandas>;Create an empty data frame with index from another data frame;13059.0;"['    TIME T1  T2 \n       1 10 100\n       2 20 200\n       3 30 300\n     df1=pd.read_csv(""1.txt"",index_col=""TIME"")\n\n     df2=df1.copy()[[]] #copy df1 and erase all columns\n     df2[""results1""],df2[""results2""]=df1[""T1""]*df[""T2""]*3,df1[""T2""]+100\n']";"['    TIME T1  T2 \n       1 10 100\n       2 20 200\n       3 30 300\n', '     df1=pd.read_csv(""1.txt"",index_col=""TIME"")\n\n     df2=df1.copy()[[]] #copy df1 and erase all columns\n', '     df2[""results1""],df2[""results2""]=df1[""T1""]*df[""T2""]*3,df1[""T2""]+100\n']";"['    TIME T1  T2 \n       1 10 100\n       2 20 200\n       3 30 300\n', '     df1=pd.read_csv(""1.txt"",index_col=""TIME"")\n\n     df2=df1.copy()[[]] #copy df1 and erase all columns\n', '     df2[""results1""],df2[""results2""]=df1[""T1""]*df[""T2""]*3,df1[""T2""]+100\n']";['\n'];['\n'];False;['import pandas as pd\n\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'df1' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
448;448;448;448;7.0;6;18180763;;1;18;<python><pandas>;set difference for pandas;13295.0;"[""In [5]: df1 = pd.DataFrame({'col1':[1,2,3], 'col2':[2,3,4]})\n\nIn [6]: df2 = pd.DataFrame({'col1':[4,2,5], 'col2':[6,3,5]})\n\nIn [7]: df1\nOut[7]: \n   col1  col2\n0     1     2\n1     2     3\n2     3     4\n\nIn [8]: df2\nOut[8]: \n   col1  col2\n0     4     6\n1     2     3\n2     5     5\n   col1  col2\n0     4     6\n2     5     5\n""]";"[""In [5]: df1 = pd.DataFrame({'col1':[1,2,3], 'col2':[2,3,4]})\n\nIn [6]: df2 = pd.DataFrame({'col1':[4,2,5], 'col2':[6,3,5]})\n\nIn [7]: df1\nOut[7]: \n   col1  col2\n0     1     2\n1     2     3\n2     3     4\n\nIn [8]: df2\nOut[8]: \n   col1  col2\n0     4     6\n1     2     3\n2     5     5\n"", '   col1  col2\n0     4     6\n2     5     5\n']";"['drop_duplicates()', ""In [5]: df1 = pd.DataFrame({'col1':[1,2,3], 'col2':[2,3,4]})\n\nIn [6]: df2 = pd.DataFrame({'col1':[4,2,5], 'col2':[6,3,5]})\n\nIn [7]: df1\nOut[7]: \n   col1  col2\n0     1     2\n1     2     3\n2     3     4\n\nIn [8]: df2\nOut[8]: \n   col1  col2\n0     4     6\n1     2     3\n2     5     5\n"", '   col1  col2\n0     4     6\n2     5     5\n']";"[""df1 = pd.DataFrame({'col1':[1,2,3], 'col2':[2,3,4]})\n\n\ndf2\n""]";"[""import pandas as pd\ndf1 = pd.DataFrame({'col1':[1,2,3], 'col2':[2,3,4]})\n\n\ndf2\n""]";True;"[""import pandas as pd\ndf2 = pd.DataFrame()\ndf = pd.DataFrame()\ndf1 = pd.DataFrame({'col1':[1,2,3], 'col2':[2,3,4]})\n\n\ndf2\n""]";True;1;2;"[""name 'df1' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df1' is not defined"", 'Sucess']";['NameError', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
449;449;449;449;1.0;0;18196203;;1;22;<python><pandas>;How to conditionally update DataFrame column in Pandas;20823.0;['    line_track  line_race  rating foreign\n 25        MTH         10     84    False\n 26        MTH          6     88    False\n 27        TAM          5     87    False\n 28         GP          2     86    False\n 29         GP          7     59    False\n 30        LCH          0    103     True\n 31        LEO          0    125     True\n 32        YOR          0    126     True\n 33        ASC          0    124     True\n'];['    line_track  line_race  rating foreign\n 25        MTH         10     84    False\n 26        MTH          6     88    False\n 27        TAM          5     87    False\n 28         GP          2     86    False\n 29         GP          7     59    False\n 30        LCH          0    103     True\n 31        LEO          0    125     True\n 32        YOR          0    126     True\n 33        ASC          0    124     True\n'];['rating', 'line_race', '    line_track  line_race  rating foreign\n 25        MTH         10     84    False\n 26        MTH          6     88    False\n 27        TAM          5     87    False\n 28         GP          2     86    False\n 29         GP          7     59    False\n 30        LCH          0    103     True\n 31        LEO          0    125     True\n 32        YOR          0    126     True\n 33        ASC          0    124     True\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'line_race'""]";['KeyError']
450;450;450;450;2.0;4;18196616;;1;17;<python><pandas>;Append rows to a pandas DataFrame without making a new copy;14082.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
451;451;451;451;2.0;0;18199288;;1;14;<python><numpy><pandas>;Getting the integer index of a Pandas DataFrame row fulfilling a condition?;33227.0;['   a  b  c\nb\n2  1  2  3\n5  4  5  6\n'];['   a  b  c\nb\n2  1  2  3\n5  4  5  6\n'];"['   a  b  c\nb\n2  1  2  3\n5  4  5  6\n', 'b', ""('b' == 5)"", '1', 'b', ""('c' == 6)""]";['b\n'];['b\n'];False;['import pandas as pd\nb\n'];False;0;2;"[""name 'df' is not defined"", ""name 'b' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'b' is not defined""]";['NameError', 'NameError'];0;2;"[""'c'"", ""name 'b' is not defined""]";['KeyError', 'NameError']
452;452;452;452;3.0;1;18215317;;1;36;<python><numpy><pandas>;extracting days from a numpy.timedelta64 value;32906.0;['0    385 days, 04:10:36\n1     57 days, 22:54:00\n2    642 days, 21:15:23\n3    615 days, 00:55:44\n4    160 days, 22:13:35\n5    196 days, 23:06:49\n6     23 days, 22:57:17\n7      2 days, 22:17:31\n8    622 days, 01:29:25\n9     79 days, 20:15:14\n10    23 days, 22:46:51\n11   268 days, 19:23:04\n12                  NaT\n13                  NaT\n14   583 days, 03:40:39\n'];['0    385 days, 04:10:36\n1     57 days, 22:54:00\n2    642 days, 21:15:23\n3    615 days, 00:55:44\n4    160 days, 22:13:35\n5    196 days, 23:06:49\n6     23 days, 22:57:17\n7      2 days, 22:17:31\n8    622 days, 01:29:25\n9     79 days, 20:15:14\n10    23 days, 22:46:51\n11   268 days, 19:23:04\n12                  NaT\n13                  NaT\n14   583 days, 03:40:39\n'];['0    385 days, 04:10:36\n1     57 days, 22:54:00\n2    642 days, 21:15:23\n3    615 days, 00:55:44\n4    160 days, 22:13:35\n5    196 days, 23:06:49\n6     23 days, 22:57:17\n7      2 days, 22:17:31\n8    622 days, 01:29:25\n9     79 days, 20:15:14\n10    23 days, 22:46:51\n11   268 days, 19:23:04\n12                  NaT\n13                  NaT\n14   583 days, 03:40:39\n'];[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'pd' is not defined""]";['Sucess', 'NameError'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
453;453;453;453;3.0;0;18233107;;1;11;<python><pandas>;pandas: convert datetime to end-of-month;9648.0;"['import pandas\nimport numpy\nimport datetime\nfrom pandas.tseries.offsets import Day, MonthEnd\n\ndef get_month_end(d):\n    month_end = d - Day() + MonthEnd() \n    if month_end.month == d.month:\n        return month_end # 31/March + MonthEnd() returns 30/April\n    else:\n        print ""Something went wrong while converting dates to EOM: "" + d + "" was converted to "" + month_end\n        raise\ndf = pandas.read_csv(inpath, na_values = nas, converters = {open_date: read_as_date})\ndf[open_date] = df[open_date].apply(get_month_end)\ndef read_as_date(x):\n    return datetime.datetime.strptime(x, fmt)\n']";"['import pandas\nimport numpy\nimport datetime\nfrom pandas.tseries.offsets import Day, MonthEnd\n\ndef get_month_end(d):\n    month_end = d - Day() + MonthEnd() \n    if month_end.month == d.month:\n        return month_end # 31/March + MonthEnd() returns 30/April\n    else:\n        print ""Something went wrong while converting dates to EOM: "" + d + "" was converted to "" + month_end\n        raise\n', 'df = pandas.read_csv(inpath, na_values = nas, converters = {open_date: read_as_date})\ndf[open_date] = df[open_date].apply(get_month_end)\n', 'def read_as_date(x):\n    return datetime.datetime.strptime(x, fmt)\n']";"['import pandas\nimport numpy\nimport datetime\nfrom pandas.tseries.offsets import Day, MonthEnd\n\ndef get_month_end(d):\n    month_end = d - Day() + MonthEnd() \n    if month_end.month == d.month:\n        return month_end # 31/March + MonthEnd() returns 30/April\n    else:\n        print ""Something went wrong while converting dates to EOM: "" + d + "" was converted to "" + month_end\n        raise\n', 'df = pandas.read_csv(inpath, na_values = nas, converters = {open_date: read_as_date})\ndf[open_date] = df[open_date].apply(get_month_end)\n', 'def read_as_date(x):\n    return datetime.datetime.strptime(x, fmt)\n']";['import pandas\nimport numpy\nimport datetime\nfrom pandas.tseries.offsets import Day, MonthEnd\n\ndf = pandas.read_csv(inpath, na_values = nas, converters = {open_date: read_as_date})\ndf[open_date] = df[open_date].apply(get_month_end)\n'];['import pandas\nimport numpy\nimport datetime\nfrom pandas.tseries.offsets import Day, MonthEnd\n\ndf = pandas.read_csv(inpath, na_values = nas, converters = {open_date: read_as_date})\ndf[open_date] = df[open_date].apply(get_month_end)\n'];False;['import pandas as pd\nimport pandas\nimport numpy\nimport datetime\nfrom pandas.tseries.offsets import Day, MonthEnd\n\ndf = pandas.read_csv(inpath, na_values = nas, converters = {open_date: read_as_date})\ndf[open_date] = df[open_date].apply(get_month_end)\n'];False;0;1;"[""name 'DataFrame' is not defined""]";['NameError'];0;1;"[""name 'Timestamp' is not defined""]";['NameError'];0;1;"[""name 'DataFrame' is not defined""]";['NameError']
454;454;454;454;2.0;0;18259067;;1;16;<numpy><pandas>;Unpivot Pandas Data;5020.0;['        Jan Feb Mar Apr ...\n2001    1   12  12  19  \n2002    9   ...\n2003    ...\nDate    Value\nJan 2001    1\nFeb 2001    1\nMar 2001    12\n...\nJan 2002    9\n'];['        Jan Feb Mar Apr ...\n2001    1   12  12  19  \n2002    9   ...\n2003    ...\n', 'Date    Value\nJan 2001    1\nFeb 2001    1\nMar 2001    12\n...\nJan 2002    9\n'];['DataFrame', '        Jan Feb Mar Apr ...\n2001    1   12  12  19  \n2002    9   ...\n2003    ...\n', 'Date    Value\nJan 2001    1\nFeb 2001    1\nMar 2001    12\n...\nJan 2002    9\n'];['...\n'];['...\n'];False;['import pandas as pd\n...\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
455;455;455;455;1.0;1;18282988;;1;11;<python><numpy><pandas>;Efficiently processing DataFrame rows with a Python function?;8669.0;[''];[];['process(row)', 'DataFrame.iterrows()', 'row', 'Series'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'you_function' is not defined""]";['NameError']
456;456;456;456;5.0;0;18290123;;1;11;<python><pandas>;disable index pandas data frame;20065.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
457;457;457;457;4.0;1;18316211;;1;33;<python><pandas>;Access index in pandas.Series.apply;16141.0;['>>> s\n     values\na b\n1 2  0.1 \n3 6  0.3\n4 4  0.7\ndef f(x):\n   # conditions or computations using the indexes\n   if x.index[0] and ...: \n   other = sum(x.index) + ...\n   return something\n'];['>>> s\n     values\na b\n1 2  0.1 \n3 6  0.3\n4 4  0.7\n', 'def f(x):\n   # conditions or computations using the indexes\n   if x.index[0] and ...: \n   other = sum(x.index) + ...\n   return something\n'];['s', '>>> s\n     values\na b\n1 2  0.1 \n3 6  0.3\n4 4  0.7\n', 'def f(x):\n   # conditions or computations using the indexes\n   if x.index[0] and ...: \n   other = sum(x.index) + ...\n   return something\n', 's.apply(f)'];['   # conditions or computations using the indexes\n'];['   # conditions or computations using the indexes\n'];False;['import pandas as pd\n   # conditions or computations using the indexes\n'];False;0;1;"[""name 's' is not defined""]";['NameError'];0;1;"[""name 's' is not defined""]";['NameError'];0;1;"[""name 's' is not defined""]";['NameError']
458;458;458;458;6.0;0;18327624;;1;43;<python><pandas>;Find element's index in pandas Series;81639.0;['import pandas as pd\nmyseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])\nprint myseries.find(7) # should output 3\ndef find(s, el):\n    for i in s.index:\n        if s[i] == el: \n            return i\n    return None\n\nprint find(myseries, 7)\n'];['import pandas as pd\nmyseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])\nprint myseries.find(7) # should output 3\n', 'def find(s, el):\n    for i in s.index:\n        if s[i] == el: \n            return i\n    return None\n\nprint find(myseries, 7)\n'];['import pandas as pd\nmyseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])\nprint myseries.find(7) # should output 3\n', 'def find(s, el):\n    for i in s.index:\n        if s[i] == el: \n            return i\n    return None\n\nprint find(myseries, 7)\n'];['import pandas as pd\nmyseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])\n\n'];['import pandas as pd\nmyseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])\n\n'];False;['import pandas as pd\nimport pandas as pd\nmyseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])\n\n'];False;1;2;"['Sucess', ""name 'pd' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'Index' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'Index' is not defined""]";['Sucess', 'NameError']
459;459;459;459;2.0;4;18358938;;1;25;<python><list><pandas><indexing>;Get index values of Pandas DataFrame as list?;35190.0;"["" list = list(df['column']) \n""]";"["" list = list(df['column']) \n""]";"["" list = list(df['column']) \n"", 'set_index']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
460;460;460;460;3.0;6;18366797;;1;14;<python><pandas>;pandas.read_csv: how to skip comment lines;8228.0;"[""# notes\na,b,c\n# more notes\n1,2,3\ndf = pandas.read_csv('j', comment='#')\nIn [15]: pandas.__version__\nOut[15]: '0.12.0rc1'\nIn [43]: df = pandas.read_csv('j', comment='#', header=None)\n""]";"['# notes\na,b,c\n# more notes\n1,2,3\n', ""df = pandas.read_csv('j', comment='#')\n"", ""In [15]: pandas.__version__\nOut[15]: '0.12.0rc1'\n"", ""In [43]: df = pandas.read_csv('j', comment='#', header=None)\n""]";"['# notes\na,b,c\n# more notes\n1,2,3\n', ""df = pandas.read_csv('j', comment='#')\n"", ""In [15]: pandas.__version__\nOut[15]: '0.12.0rc1'\n"", ""In [43]: df = pandas.read_csv('j', comment='#', header=None)\n""]";['pandas.__version__\n'];['pandas.__version__\n'];False;['import pandas as pd\npandas.__version__\n'];False;1;2;"[""name 'pd' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'StringIO' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'StringIO' is not defined"", 'Sucess']";['NameError', 'Sucess']
461;461;461;461;3.0;9;18388870;;1;18;<python><matplotlib><pandas><ipython><anaconda>;ipython pandas plot does not show;12938.0;"[""%pylab inline\nimport matplotlib.pyplot as plt\nimport pandas as pd \nts = pd.Series(randn(1000), index = pd.date_range('1/1/2000', periods=1000))\nts = ts.cumsum()\nts.plot()\n<matplotlib.axes.AxesSubplot at 0x109253410>\n""]";"['%pylab inline\n', ""import matplotlib.pyplot as plt\nimport pandas as pd \nts = pd.Series(randn(1000), index = pd.date_range('1/1/2000', periods=1000))\nts = ts.cumsum()\nts.plot()\n"", '<matplotlib.axes.AxesSubplot at 0x109253410>\n']";"['%pylab inline\n', ""import matplotlib.pyplot as plt\nimport pandas as pd \nts = pd.Series(randn(1000), index = pd.date_range('1/1/2000', periods=1000))\nts = ts.cumsum()\nts.plot()\n"", '<matplotlib.axes.AxesSubplot at 0x109253410>\n']";"[""import matplotlib.pyplot as plt\nimport pandas as pd \nts = pd.Series(randn(1000), index = pd.date_range('1/1/2000', periods=1000))\nts = ts.cumsum()\nts.plot()\n""]";"[""import matplotlib.pyplot as plt\nimport pandas as pd \nts = pd.Series(randn(1000), index = pd.date_range('1/1/2000', periods=1000))\nts = ts.cumsum()\nts.plot()\n""]";False;"[""import pandas as pd\nimport matplotlib.pyplot as plt\nimport pandas as pd \nts = pd.Series(randn(1000), index = pd.date_range('1/1/2000', periods=1000))\nts = ts.cumsum()\nts.plot()\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
462;462;462;462;1.0;0;18404946;;1;14;<python><format><pandas><dataframe>;Py Pandas .format(dataframe);12208.0;"[""print '{:20,.2f}'.format(123456789)\n123,456,789.00\nimport pandas as pd\nimport random\ndata = [[random.random()*10000 for i in range(1,4)] for j in range (1,8)]\ndf = pd.DataFrame (data)\nprint '{:20,.2f}'.format(df)\n Unknown format code 'f' for object of type 'str'\n""]";"[""print '{:20,.2f}'.format(123456789)\n"", '123,456,789.00\n', ""import pandas as pd\nimport random\ndata = [[random.random()*10000 for i in range(1,4)] for j in range (1,8)]\ndf = pd.DataFrame (data)\nprint '{:20,.2f}'.format(df)\n"", "" Unknown format code 'f' for object of type 'str'\n""]";"[""print '{:20,.2f}'.format(123456789)\n"", '123,456,789.00\n', ""import pandas as pd\nimport random\ndata = [[random.random()*10000 for i in range(1,4)] for j in range (1,8)]\ndf = pd.DataFrame (data)\nprint '{:20,.2f}'.format(df)\n"", "" Unknown format code 'f' for object of type 'str'\n"", ""'{:20,.2f}'.format(df)""]";['123,456,789.00\nimport pandas as pd\nimport random\ndata = [[random.random()*10000 for i in range(1,4)] for j in range (1,8)]\ndf = pd.DataFrame (data)\n'];['123,456,789.00\nimport pandas as pd\nimport random\ndata = [[random.random()*10000 for i in range(1,4)] for j in range (1,8)]\ndf = pd.DataFrame (data)\n'];False;['import pandas as pd\n123,456,789.00\nimport pandas as pd\nimport random\ndata = [[random.random()*10000 for i in range(1,4)] for j in range (1,8)]\ndf = pd.DataFrame (data)\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
463;463;463;463;6.0;4;18429491;;1;40;<pandas><grouping><nan>;groupby columns with NaN (missing) values;20547.0;"[""import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'a': ['1', '2', '3'], 'b': ['4', np.NaN, '6']})\n\nIn [4]: df.groupby('b').groups\nOut[4]: {'4': [0], '6': [2]}\n""]";"[""import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'a': ['1', '2', '3'], 'b': ['4', np.NaN, '6']})\n\nIn [4]: df.groupby('b').groups\nOut[4]: {'4': [0], '6': [2]}\n""]";"[""import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'a': ['1', '2', '3'], 'b': ['4', np.NaN, '6']})\n\nIn [4]: df.groupby('b').groups\nOut[4]: {'4': [0], '6': [2]}\n""]";"[""df.groupby('b').groups\n""]";"[""df.groupby('b').groups\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf.groupby('b').groups\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'b'""]";['KeyError']
464;464;464;464;3.0;0;18434208;;1;17;<python><pandas>;Pandas: Converting to numeric, creating NaNs when necessary;39278.0;"["">> df['foo']\n0       0.0\n1     103.8\n2     751.1\n3       0.0\n4       0.0\n5         -\n6         -\n7       0.0\n8         -\n9       0.0\nName: foo, Length: 9, dtype: object\n>> df['foo'].astype(np.float)\n>> df['foo'].apply(np.float)\n""]";"["">> df['foo']\n0       0.0\n1     103.8\n2     751.1\n3       0.0\n4       0.0\n5         -\n6         -\n7       0.0\n8         -\n9       0.0\nName: foo, Length: 9, dtype: object\n"", "">> df['foo'].astype(np.float)\n"", "">> df['foo'].apply(np.float)\n""]";"["">> df['foo']\n0       0.0\n1     103.8\n2     751.1\n3       0.0\n4       0.0\n5         -\n6         -\n7       0.0\n8         -\n9       0.0\nName: foo, Length: 9, dtype: object\n"", 'np.float', 'NaN', "">> df['foo'].astype(np.float)\n"", "">> df['foo'].apply(np.float)\n"", 'ValueError: could not convert string to float: -']";[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'df' is not defined"", ""name 's' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 's' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];1;3;"[""'foo'"", ""name 's' is not defined"", 'Sucess']";['KeyError', 'NameError', 'Sucess']
465;465;465;465;2.0;0;18504967;;1;21;<python><pandas><calculated-columns>;pandas dataframe create new columns and fill with calculated values from same df;37638.0;"[""ds = pd.DataFrame(np.abs(randn(3, 4)), index=[1,2,3], columns=['A','B','C','D'])\nds\n      A         B         C         D\n1  1.099679  0.042043  0.083903  0.410128\n2  0.268205  0.718933  1.459374  0.758887\n3  0.680566  0.538655  0.038236  1.169403\nds['sum']=ds.sum(axis=1)\nds\n      A         B         C         D       sum\n1  0.095389  0.556978  1.646888  1.959295  4.258550\n2  1.076190  2.668270  0.825116  1.477040  6.046616\n3  0.245034  1.066285  0.967124  0.791606  3.070049\n""]";"[""ds = pd.DataFrame(np.abs(randn(3, 4)), index=[1,2,3], columns=['A','B','C','D'])\nds\n      A         B         C         D\n1  1.099679  0.042043  0.083903  0.410128\n2  0.268205  0.718933  1.459374  0.758887\n3  0.680566  0.538655  0.038236  1.169403\n"", ""ds['sum']=ds.sum(axis=1)\nds\n      A         B         C         D       sum\n1  0.095389  0.556978  1.646888  1.959295  4.258550\n2  1.076190  2.668270  0.825116  1.477040  6.046616\n3  0.245034  1.066285  0.967124  0.791606  3.070049\n""]";"[""ds = pd.DataFrame(np.abs(randn(3, 4)), index=[1,2,3], columns=['A','B','C','D'])\nds\n      A         B         C         D\n1  1.099679  0.042043  0.083903  0.410128\n2  0.268205  0.718933  1.459374  0.758887\n3  0.680566  0.538655  0.038236  1.169403\n"", ""ds['sum']=ds.sum(axis=1)\nds\n      A         B         C         D       sum\n1  0.095389  0.556978  1.646888  1.959295  4.258550\n2  1.076190  2.668270  0.825116  1.477040  6.046616\n3  0.245034  1.066285  0.967124  0.791606  3.070049\n""]";"[""ds = pd.DataFrame(np.abs(randn(3, 4)), index=[1,2,3], columns=['A','B','C','D'])\nds\nds['sum']=ds.sum(axis=1)\nds\n""]";"[""import pandas as pd\nds = pd.DataFrame(np.abs(randn(3, 4)), index=[1,2,3], columns=['A','B','C','D'])\nds\nds['sum']=ds.sum(axis=1)\nds\n""]";True;"[""import pandas as pd\nds = pd.DataFrame(np.abs(randn(3, 4)), index=[1,2,3], columns=['A','B','C','D'])\nds\nds['sum']=ds.sum(axis=1)\nds\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'A'""]";['KeyError']
466;466;466;466;2.0;2;18517722;;1;13;<python><numpy><pandas><scipy>;Weighted moving average in python;11277.0;"[""import numpy as np\nimport matplotlib.pyplot as plt\n\n#first generate some datapoint for a randomly sampled noisy sinewave\nx = np.random.random(1000)*10\nnoise = np.random.normal(scale=0.3,size=len(x))\ny = np.sin(x) + noise\n\n#plot the data\nplt.plot(x,y,'ro',alpha=0.3,ms=4,label='data')\nplt.xlabel('Time')\nplt.ylabel('Intensity')\n\n#define a moving average function\ndef moving_average(x,y,step_size=.1,bin_size=1):\n    bin_centers  = np.arange(np.min(x),np.max(x)-0.5*step_size,step_size)+0.5*step_size\n    bin_avg = np.zeros(len(bin_centers))\n\n    for index in range(0,len(bin_centers)):\n        bin_center = bin_centers[index]\n        items_in_bin = y[(x>(bin_center-bin_size*0.5) ) & (x<(bin_center+bin_size*0.5))]\n        bin_avg[index] = np.mean(items_in_bin)\n\n    return bin_centers,bin_avg\n\n#plot the moving average\nbins, average = moving_average(x,y)\nplt.plot(bins, average,label='moving average')\n\nplt.show()\ndef weighted_moving_average(x,y,step_size=0.05,width=1):\n    bin_centers  = np.arange(np.min(x),np.max(x)-0.5*step_size,step_size)+0.5*step_size\n    bin_avg = np.zeros(len(bin_centers))\n\n    #We're going to weight with a Gaussian function\n    def gaussian(x,amp=1,mean=0,sigma=1):\n        return amp*np.exp(-(x-mean)**2/(2*sigma**2))\n\n    for index in range(0,len(bin_centers)):\n        bin_center = bin_centers[index]\n        weights = gaussian(x,mean=bin_center,sigma=width)\n        bin_avg[index] = np.average(y,weights=weights)\n\n    return (bin_centers,bin_avg)\n""]";"[""import numpy as np\nimport matplotlib.pyplot as plt\n\n#first generate some datapoint for a randomly sampled noisy sinewave\nx = np.random.random(1000)*10\nnoise = np.random.normal(scale=0.3,size=len(x))\ny = np.sin(x) + noise\n\n#plot the data\nplt.plot(x,y,'ro',alpha=0.3,ms=4,label='data')\nplt.xlabel('Time')\nplt.ylabel('Intensity')\n\n#define a moving average function\ndef moving_average(x,y,step_size=.1,bin_size=1):\n    bin_centers  = np.arange(np.min(x),np.max(x)-0.5*step_size,step_size)+0.5*step_size\n    bin_avg = np.zeros(len(bin_centers))\n\n    for index in range(0,len(bin_centers)):\n        bin_center = bin_centers[index]\n        items_in_bin = y[(x>(bin_center-bin_size*0.5) ) & (x<(bin_center+bin_size*0.5))]\n        bin_avg[index] = np.mean(items_in_bin)\n\n    return bin_centers,bin_avg\n\n#plot the moving average\nbins, average = moving_average(x,y)\nplt.plot(bins, average,label='moving average')\n\nplt.show()\n"", ""def weighted_moving_average(x,y,step_size=0.05,width=1):\n    bin_centers  = np.arange(np.min(x),np.max(x)-0.5*step_size,step_size)+0.5*step_size\n    bin_avg = np.zeros(len(bin_centers))\n\n    #We're going to weight with a Gaussian function\n    def gaussian(x,amp=1,mean=0,sigma=1):\n        return amp*np.exp(-(x-mean)**2/(2*sigma**2))\n\n    for index in range(0,len(bin_centers)):\n        bin_center = bin_centers[index]\n        weights = gaussian(x,mean=bin_center,sigma=width)\n        bin_avg[index] = np.average(y,weights=weights)\n\n    return (bin_centers,bin_avg)\n""]";"[""import numpy as np\nimport matplotlib.pyplot as plt\n\n#first generate some datapoint for a randomly sampled noisy sinewave\nx = np.random.random(1000)*10\nnoise = np.random.normal(scale=0.3,size=len(x))\ny = np.sin(x) + noise\n\n#plot the data\nplt.plot(x,y,'ro',alpha=0.3,ms=4,label='data')\nplt.xlabel('Time')\nplt.ylabel('Intensity')\n\n#define a moving average function\ndef moving_average(x,y,step_size=.1,bin_size=1):\n    bin_centers  = np.arange(np.min(x),np.max(x)-0.5*step_size,step_size)+0.5*step_size\n    bin_avg = np.zeros(len(bin_centers))\n\n    for index in range(0,len(bin_centers)):\n        bin_center = bin_centers[index]\n        items_in_bin = y[(x>(bin_center-bin_size*0.5) ) & (x<(bin_center+bin_size*0.5))]\n        bin_avg[index] = np.mean(items_in_bin)\n\n    return bin_centers,bin_avg\n\n#plot the moving average\nbins, average = moving_average(x,y)\nplt.plot(bins, average,label='moving average')\n\nplt.show()\n"", ""def weighted_moving_average(x,y,step_size=0.05,width=1):\n    bin_centers  = np.arange(np.min(x),np.max(x)-0.5*step_size,step_size)+0.5*step_size\n    bin_avg = np.zeros(len(bin_centers))\n\n    #We're going to weight with a Gaussian function\n    def gaussian(x,amp=1,mean=0,sigma=1):\n        return amp*np.exp(-(x-mean)**2/(2*sigma**2))\n\n    for index in range(0,len(bin_centers)):\n        bin_center = bin_centers[index]\n        weights = gaussian(x,mean=bin_center,sigma=width)\n        bin_avg[index] = np.average(y,weights=weights)\n\n    return (bin_centers,bin_avg)\n""]";"[""import numpy as np\nimport matplotlib.pyplot as plt\n\n#first generate some datapoint for a randomly sampled noisy sinewave\nx = np.random.random(1000)*10\nnoise = np.random.normal(scale=0.3,size=len(x))\ny = np.sin(x) + noise\n\n#plot the data\nplt.plot(x,y,'ro',alpha=0.3,ms=4,label='data')\nplt.xlabel('Time')\nplt.ylabel('Intensity')\n\n#define a moving average function\n\n\n\n#plot the moving average\nbins, average = moving_average(x,y)\nplt.plot(bins, average,label='moving average')\n\nplt.show()\n\n    #We're going to weight with a Gaussian function\n\n\n""]";"[""import numpy as np\nimport matplotlib.pyplot as plt\n\n#first generate some datapoint for a randomly sampled noisy sinewave\nx = np.random.random(1000)*10\nnoise = np.random.normal(scale=0.3,size=len(x))\ny = np.sin(x) + noise\n\n#plot the data\nplt.plot(x,y,'ro',alpha=0.3,ms=4,label='data')\nplt.xlabel('Time')\nplt.ylabel('Intensity')\n\n#define a moving average function\n\n\n\n#plot the moving average\nbins, average = moving_average(x,y)\nplt.plot(bins, average,label='moving average')\n\nplt.show()\n\n    #We're going to weight with a Gaussian function\n\n\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#first generate some datapoint for a randomly sampled noisy sinewave\nx = np.random.random(1000)*10\nnoise = np.random.normal(scale=0.3,size=len(x))\ny = np.sin(x) + noise\n\n#plot the data\nplt.plot(x,y,'ro',alpha=0.3,ms=4,label='data')\nplt.xlabel('Time')\nplt.ylabel('Intensity')\n\n#define a moving average function\n\n\n\n#plot the moving average\nbins, average = moving_average(x,y)\nplt.plot(bins, average,label='moving average')\n\nplt.show()\n\n    #We're going to weight with a Gaussian function\n\n\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
467;467;467;467;3.0;0;18528533;;1;19;<python><pandas><dataframe><printing>;Pretty Printing a pandas dataframe;21075.0;['+------------+---------+-------------+\n| column_one | col_two |   column_3  |\n+------------+---------+-------------+\n|          0 |  0.0001 | ABCD        |\n|          1 |  1e-005 | ABCD        |\n|          2 |  1e-006 | long string |\n|          3 |  1e-007 | ABCD        |\n+------------+---------+-------------+\n'];['+------------+---------+-------------+\n| column_one | col_two |   column_3  |\n+------------+---------+-------------+\n|          0 |  0.0001 | ABCD        |\n|          1 |  1e-005 | ABCD        |\n|          2 |  1e-006 | long string |\n|          3 |  1e-007 | ABCD        |\n+------------+---------+-------------+\n'];['+------------+---------+-------------+\n| column_one | col_two |   column_3  |\n+------------+---------+-------------+\n|          0 |  0.0001 | ABCD        |\n|          1 |  1e-005 | ABCD        |\n|          2 |  1e-006 | long string |\n|          3 |  1e-007 | ABCD        |\n+------------+---------+-------------+\n'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""No module named 'StringIO'"", ""No module named 'tabulate'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'StringIO'"", ""No module named 'tabulate'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'StringIO'"", ""No module named 'tabulate'""]";['ImportError', 'ImportError']
468;468;468;468;2.0;0;18554920;;1;39;<python><pandas>;Pandas aggregate count distinct;33368.0;"[""import numpy as np\nimport pandas as pd\ndf = pd.DataFrame({'date': ['2013-04-01','2013-04-01','2013-04-01','2013-04-02', '2013-04-02'],\n    'user_id': ['0001', '0001', '0002', '0002', '0002'],\n    'duration': [30, 15, 20, 15, 30]})\ngroup = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg\n            duration\ndate\n2013-04-01        65\n2013-04-02        45\nagg = group.aggregate({ 'duration': np.sum, 'user_id': count_distinct})\ngroup = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg['uv'] = df.groupby('date').user_id.nunique()\nagg\n            duration  uv\ndate\n2013-04-01        65   2\n2013-04-02        45   1\n""]";"[""import numpy as np\nimport pandas as pd\ndf = pd.DataFrame({'date': ['2013-04-01','2013-04-01','2013-04-01','2013-04-02', '2013-04-02'],\n    'user_id': ['0001', '0001', '0002', '0002', '0002'],\n    'duration': [30, 15, 20, 15, 30]})\n"", ""group = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg\n            duration\ndate\n2013-04-01        65\n2013-04-02        45\n"", ""agg = group.aggregate({ 'duration': np.sum, 'user_id': count_distinct})\n"", ""group = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg['uv'] = df.groupby('date').user_id.nunique()\nagg\n            duration  uv\ndate\n2013-04-01        65   2\n2013-04-02        45   1\n""]";"[""import numpy as np\nimport pandas as pd\ndf = pd.DataFrame({'date': ['2013-04-01','2013-04-01','2013-04-01','2013-04-02', '2013-04-02'],\n    'user_id': ['0001', '0001', '0002', '0002', '0002'],\n    'duration': [30, 15, 20, 15, 30]})\n"", ""group = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg\n            duration\ndate\n2013-04-01        65\n2013-04-02        45\n"", ""agg = group.aggregate({ 'duration': np.sum, 'user_id': count_distinct})\n"", ""group = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg['uv'] = df.groupby('date').user_id.nunique()\nagg\n            duration  uv\ndate\n2013-04-01        65   2\n2013-04-02        45   1\n""]";"[""import numpy as np\nimport pandas as pd\ngroup = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg\ndate\nagg = group.aggregate({ 'duration': np.sum, 'user_id': count_distinct})\ngroup = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg['uv'] = df.groupby('date').user_id.nunique()\nagg\ndate\n""]";"[""import numpy as np\nimport pandas as pd\ngroup = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg\ndate\nagg = group.aggregate({ 'duration': np.sum, 'user_id': count_distinct})\ngroup = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg['uv'] = df.groupby('date').user_id.nunique()\nagg\ndate\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport numpy as np\nimport pandas as pd\ngroup = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg\ndate\nagg = group.aggregate({ 'duration': np.sum, 'user_id': count_distinct})\ngroup = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg['uv'] = df.groupby('date').user_id.nunique()\nagg\ndate\n""]";True;0;1;"[""name 'date' is not defined""]";['NameError'];0;1;"[""name 'date' is not defined""]";['NameError'];0;1;"[""name 'date' is not defined""]";['NameError']
469;469;469;469;2.0;0;18580461;;1;12;<python><pandas><filtering><percentile>;Eliminating all data over a given percentile;9134.0;"[""limit = data.ms.describe(90)['95%']\nvalid_data = data[data['ms'] < limit]\n""]";"[""limit = data.ms.describe(90)['95%']\nvalid_data = data[data['ms'] < limit]\n""]";"['DataFrame', 'data', 'ms', 'data.ms', ""limit = data.ms.describe(90)['95%']\nvalid_data = data[data['ms'] < limit]\n""]";"[""limit = data.ms.describe(90)['95%']\nvalid_data = data[data['ms'] < limit]\n""]";"[""limit = data.ms.describe(90)['95%']\nvalid_data = data[data['ms'] < limit]\n""]";False;"[""import pandas as pd\nlimit = data.ms.describe(90)['95%']\nvalid_data = data[data['ms'] < limit]\n""]";False;0;2;"[""name 'df' is not defined"", ""name 'numpy' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'numpy' is not defined""]";['NameError', 'NameError'];0;2;"[""'DataFrame' object has no attribute 'a'"", ""name 'numpy' is not defined""]";['AttributeError', 'NameError']
470;470;470;470;2.0;0;18594469;;1;27;<python><pandas><normalization><dataframe>;Normalizing a pandas DataFrame by row;11082.0;['(df.T / df.T.sum()).T\n'];['(df.T / df.T.sum()).T\n'];['(df.T / df.T.sum()).T\n', 'df / df.sum(axis=1)'];['(df.T / df.T.sum()).T\n'];['(df.T / df.T.sum()).T\n'];False;['import pandas as pd\ndf = pd.DataFrame()\n(df.T / df.T.sum()).T\n'];True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
471;471;471;471;3.0;4;18603270;;1;20;<python><pandas><ipython>;Progress indicator during pandas operations (python);6648.0;"[""df_users.groupby(['userID', 'requestDate']).apply(feature_rollup)\n""]";"[""df_users.groupby(['userID', 'requestDate']).apply(feature_rollup)\n""]";"[""df_users.groupby(['userID', 'requestDate']).apply(feature_rollup)\n"", 'feature_rollup', 'apply']";"[""df_users.groupby(['userID', 'requestDate']).apply(feature_rollup)\n""]";"[""df_users.groupby(['userID', 'requestDate']).apply(feature_rollup)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf_users.groupby(['userID', 'requestDate']).apply(feature_rollup)\n""]";True;0;2;"[""name 'df_users' is not defined"", ""No module named 'tqdm'""]";['NameError', 'ImportError'];0;2;"[""name 'df_users' is not defined"", ""No module named 'tqdm'""]";['NameError', 'ImportError'];0;2;"[""name 'df_users' is not defined"", ""No module named 'tqdm'""]";['NameError', 'ImportError']
472;472;472;472;2.0;2;18624039;;1;14;<python><pandas>;Pandas reset index on series to remove multiindex;24921.0;"[""    H3=H2[['SOLD_PRICE']]\n    H5=H3.resample('Q',how='count')\n    H6=pd.rolling_mean(H5,4)\n1999-03-31  SOLD_PRICE     NaN\n1999-06-30  SOLD_PRICE     NaN\n1999-09-30  SOLD_PRICE     NaN\n1999-12-31  SOLD_PRICE    3.00\n2000-03-31  SOLD_PRICE    3.00\nMultiIndex\n[(1999-03-31 00:00:00, u'SOLD_PRICE'), (1999-06-30 00:00:00, u'SOLD_PRICE'), (1999-09-30 00:00:00, u'SOLD_PRICE'), (1999-12-31 00:00:00, u'SOLD_PRICE'),.....\n""]";"[""    H3=H2[['SOLD_PRICE']]\n    H5=H3.resample('Q',how='count')\n    H6=pd.rolling_mean(H5,4)\n"", '1999-03-31  SOLD_PRICE     NaN\n1999-06-30  SOLD_PRICE     NaN\n1999-09-30  SOLD_PRICE     NaN\n1999-12-31  SOLD_PRICE    3.00\n2000-03-31  SOLD_PRICE    3.00\n', ""MultiIndex\n[(1999-03-31 00:00:00, u'SOLD_PRICE'), (1999-06-30 00:00:00, u'SOLD_PRICE'), (1999-09-30 00:00:00, u'SOLD_PRICE'), (1999-12-31 00:00:00, u'SOLD_PRICE'),.....\n""]";"['Series', 'DataFrame', 'H2', 'DataFrame', ""    H3=H2[['SOLD_PRICE']]\n    H5=H3.resample('Q',how='count')\n    H6=pd.rolling_mean(H5,4)\n"", '1999-03-31  SOLD_PRICE     NaN\n1999-06-30  SOLD_PRICE     NaN\n1999-09-30  SOLD_PRICE     NaN\n1999-12-31  SOLD_PRICE    3.00\n2000-03-31  SOLD_PRICE    3.00\n', ""MultiIndex\n[(1999-03-31 00:00:00, u'SOLD_PRICE'), (1999-06-30 00:00:00, u'SOLD_PRICE'), (1999-09-30 00:00:00, u'SOLD_PRICE'), (1999-12-31 00:00:00, u'SOLD_PRICE'),.....\n"", 'DataFrame']";['MultiIndex\n'];['MultiIndex\n'];False;['import pandas as pd\nMultiIndex\n'];False;0;2;"[""name 's' is not defined"", ""name 'H2' is not defined""]";['NameError', 'NameError'];0;2;"[""name 's' is not defined"", ""name 'H2' is not defined""]";['NameError', 'NameError'];0;2;"[""name 's' is not defined"", ""name 'H2' is not defined""]";['NameError', 'NameError']
473;473;473;473;5.0;6;18645401;;1;14;<python><excel><utf-8><pandas>;Python pandas to_excel 'utf8' codec can't decode byte;28678.0;"['from pandas import ExcelWriter\ndata = pd.read_csv(input)\nwriter = ExcelWriter(output) #output is just the filename\nfundraisers.to_excel(writer, ""fundraisers"")\nlocations.to_excel(writer, ""locations"") #error\nlocations.to_csv(outputcsv) #works\nwriter.save()\nEvent ID    Constituent ID  Email Address   First Name  \\   Last Name\nf       1       A       A       1\nF       4       L       R       C\nM       1       1       A       D\nF       4       A       A       G\nM       2       0       R       G\nM       3       O       O       H\nM       2       T       E       H\nM       2       A       A       H\nM       2       M       M       K\nF       3       J       E       K\nLocation ID raised  raised con  raised email\na   0   0   0\na   8   0   0\no   0   0   0\no   0   0   0\no   0   0   0\nt   5   0   0\no   1   0   0\no   6   a   0\no   6   0   0\nd   0   0   0\n']";"['from pandas import ExcelWriter\ndata = pd.read_csv(input)\nwriter = ExcelWriter(output) #output is just the filename\nfundraisers.to_excel(writer, ""fundraisers"")\nlocations.to_excel(writer, ""locations"") #error\nlocations.to_csv(outputcsv) #works\nwriter.save()\n', 'Event ID    Constituent ID  Email Address   First Name  \\   Last Name\nf       1       A       A       1\nF       4       L       R       C\nM       1       1       A       D\nF       4       A       A       G\nM       2       0       R       G\nM       3       O       O       H\nM       2       T       E       H\nM       2       A       A       H\nM       2       M       M       K\nF       3       J       E       K\nLocation ID raised  raised con  raised email\na   0   0   0\na   8   0   0\no   0   0   0\no   0   0   0\no   0   0   0\nt   5   0   0\no   1   0   0\no   6   a   0\no   6   0   0\nd   0   0   0\n']";"['from pandas import ExcelWriter\ndata = pd.read_csv(input)\nwriter = ExcelWriter(output) #output is just the filename\nfundraisers.to_excel(writer, ""fundraisers"")\nlocations.to_excel(writer, ""locations"") #error\nlocations.to_csv(outputcsv) #works\nwriter.save()\n', 'Event ID    Constituent ID  Email Address   First Name  \\   Last Name\nf       1       A       A       1\nF       4       L       R       C\nM       1       1       A       D\nF       4       A       A       G\nM       2       0       R       G\nM       3       O       O       H\nM       2       T       E       H\nM       2       A       A       H\nM       2       M       M       K\nF       3       J       E       K\nLocation ID raised  raised con  raised email\na   0   0   0\na   8   0   0\no   0   0   0\no   0   0   0\no   0   0   0\nt   5   0   0\no   1   0   0\no   6   a   0\no   6   0   0\nd   0   0   0\n']";"['from pandas import ExcelWriter\ndata = pd.read_csv(input)\nwriter = ExcelWriter(output) #output is just the filename\nfundraisers.to_excel(writer, ""fundraisers"")\nlocations.to_excel(writer, ""locations"") #error\nlocations.to_csv(outputcsv) #works\nwriter.save()\n']";"['import pandas as pd\nfrom pandas import ExcelWriter\ndata = pd.read_csv(input)\nwriter = ExcelWriter(output) #output is just the filename\nfundraisers.to_excel(writer, ""fundraisers"")\nlocations.to_excel(writer, ""locations"") #error\nlocations.to_csv(outputcsv) #works\nwriter.save()\n']";True;"['import pandas as pd\nfrom pandas import ExcelWriter\ndata = pd.read_csv(input)\nwriter = ExcelWriter(output) #output is just the filename\nfundraisers.to_excel(writer, ""fundraisers"")\nlocations.to_excel(writer, ""locations"") #error\nlocations.to_csv(outputcsv) #works\nwriter.save()\n']";False;1;2;"['Sucess', ""name 'pd' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'fname' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'fname' is not defined""]";['Sucess', 'NameError']
474;474;474;474;2.0;5;18646076;;1;20;<python><numpy><pandas>;Add numpy array as column to Pandas data frame;32167.0;['[[1, 2, 3],\n[4, 5, 6],\n[7, 8, 9]]\n[[0, 1, 0],\n[0, 0, 1],\n[1, 0, 0]]\n[[1, 2, 3, [0, 1, 0]],\n[4, 5, 6, [0, 0, 1]],\n[7, 8, 9, [1, 0, 0]]]\n'];['[[1, 2, 3],\n[4, 5, 6],\n[7, 8, 9]]\n', '[[0, 1, 0],\n[0, 0, 1],\n[1, 0, 0]]\n', '[[1, 2, 3, [0, 1, 0]],\n[4, 5, 6, [0, 0, 1]],\n[7, 8, 9, [1, 0, 0]]]\n'];['[[1, 2, 3],\n[4, 5, 6],\n[7, 8, 9]]\n', '[[0, 1, 0],\n[0, 0, 1],\n[1, 0, 0]]\n', '[[1, 2, 3, [0, 1, 0]],\n[4, 5, 6, [0, 0, 1]],\n[7, 8, 9, [1, 0, 0]]]\n'];['[4, 5, 6],\n[0, 0, 1],\n[4, 5, 6, [0, 0, 1]],\n'];['[4, 5, 6],\n[0, 0, 1],\n[4, 5, 6, [0, 0, 1]],\n'];False;['import pandas as pd\n[4, 5, 6],\n[0, 0, 1],\n[4, 5, 6, [0, 0, 1]],\n'];False;0;2;"[""No module named 'scipy'"", ""name 'pd' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'scipy'"", ""name 'df' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'scipy'"", ""name 'csc' is not defined""]";['ImportError', 'NameError']
475;475;475;475;1.0;7;18665284;;1;12;<python><json><mongodb><twitter><pandas>;How do I access embedded json objects in a Pandas DataFrame?;4640.0;"['from twython import Twython\nfrom pymongo import MongoClient\n\ntw = Twython(...<auth>...)\n\n# Using mongo as object storage \nclient = MongoClient()\ndb = client.twitter\nuser_coll = db.users\n\nuser_batch = ... # collection of user ids\nuser_dict_batch = tw.lookup_user(user_id=user_batch)\n\nfor user_dict in user_dict_batch:\n    if(user_coll.find_one({""id"":user_dict[\'id\']}) == None):\n        user_coll.insert(user_dict)\n# Pull straight from mongo to pandas\ncursor = user_coll.find()\ndf = pandas.DataFrame(list(cursor))\n']";"['from twython import Twython\nfrom pymongo import MongoClient\n\ntw = Twython(...<auth>...)\n\n# Using mongo as object storage \nclient = MongoClient()\ndb = client.twitter\nuser_coll = db.users\n\nuser_batch = ... # collection of user ids\nuser_dict_batch = tw.lookup_user(user_id=user_batch)\n\nfor user_dict in user_dict_batch:\n    if(user_coll.find_one({""id"":user_dict[\'id\']}) == None):\n        user_coll.insert(user_dict)\n', '# Pull straight from mongo to pandas\ncursor = user_coll.find()\ndf = pandas.DataFrame(list(cursor))\n']";"['from twython import Twython\nfrom pymongo import MongoClient\n\ntw = Twython(...<auth>...)\n\n# Using mongo as object storage \nclient = MongoClient()\ndb = client.twitter\nuser_coll = db.users\n\nuser_batch = ... # collection of user ids\nuser_dict_batch = tw.lookup_user(user_id=user_batch)\n\nfor user_dict in user_dict_batch:\n    if(user_coll.find_one({""id"":user_dict[\'id\']}) == None):\n        user_coll.insert(user_dict)\n', '# Pull straight from mongo to pandas\ncursor = user_coll.find()\ndf = pandas.DataFrame(list(cursor))\n']";['from twython import Twython\nfrom pymongo import MongoClient\n\ntw = Twython(...<auth>...)\n\n# Using mongo as object storage \nclient = MongoClient()\ndb = client.twitter\nuser_coll = db.users\n\nuser_batch = ... # collection of user ids\nuser_dict_batch = tw.lookup_user(user_id=user_batch)\n\n# Pull straight from mongo to pandas\ncursor = user_coll.find()\ndf = pandas.DataFrame(list(cursor))\n'];['from twython import Twython\nfrom pymongo import MongoClient\n\ntw = Twython(...<auth>...)\n\n# Using mongo as object storage \nclient = MongoClient()\ndb = client.twitter\nuser_coll = db.users\n\nuser_batch = ... # collection of user ids\nuser_dict_batch = tw.lookup_user(user_id=user_batch)\n\n# Pull straight from mongo to pandas\ncursor = user_coll.find()\ndf = pandas.DataFrame(list(cursor))\n'];False;['import pandas as pd\nfrom twython import Twython\nfrom pymongo import MongoClient\n\ntw = Twython(...<auth>...)\n\n# Using mongo as object storage \nclient = MongoClient()\ndb = client.twitter\nuser_coll = db.users\n\nuser_batch = ... # collection of user ids\nuser_dict_batch = tw.lookup_user(user_id=user_batch)\n\n# Pull straight from mongo to pandas\ncursor = user_coll.find()\ndf = pandas.DataFrame(list(cursor))\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
476;476;476;476;2.0;0;18674064;;1;61;<python><indexing><pandas>;how do I insert a column at a specific column index in pandas?;47504.0;"[""import pandas as pd\ndf = pd.DataFrame({'l':['a','b','c','d'], 'v':[1,2,1,2]})\ndf['n'] = 0\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'l':['a','b','c','d'], 'v':[1,2,1,2]})\ndf['n'] = 0\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'l':['a','b','c','d'], 'v':[1,2,1,2]})\ndf['n'] = 0\n"", 'n', 'df', 'df', 'n']";"[""import pandas as pd\ndf = pd.DataFrame({'l':['a','b','c','d'], 'v':[1,2,1,2]})\ndf['n'] = 0\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'l':['a','b','c','d'], 'v':[1,2,1,2]})\ndf['n'] = 0\n""]";False;"[""import pandas as pd\nimport pandas as pd\ndf = pd.DataFrame({'l':['a','b','c','d'], 'v':[1,2,1,2]})\ndf['n'] = 0\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'idx' is not defined""]";['NameError']
477;477;477;477;1.0;0;18677271;;1;11;<python><pandas>;Grouping daily data by month in python/pandas and then normalizing;19066.0;['    q_string    q_visits    q_date\n0   nucleus         1790        2012-10-02 00:00:00\n1   neuron          364         2012-10-02 00:00:00\n2   current         280         2012-10-02 00:00:00\n3   molecular       259         2012-10-02 00:00:00\n4   stem            201         2012-10-02 00:00:00\n'];['    q_string    q_visits    q_date\n0   nucleus         1790        2012-10-02 00:00:00\n1   neuron          364         2012-10-02 00:00:00\n2   current         280         2012-10-02 00:00:00\n3   molecular       259         2012-10-02 00:00:00\n4   stem            201         2012-10-02 00:00:00\n'];['DataFrame', '    q_string    q_visits    q_date\n0   nucleus         1790        2012-10-02 00:00:00\n1   neuron          364         2012-10-02 00:00:00\n2   current         280         2012-10-02 00:00:00\n3   molecular       259         2012-10-02 00:00:00\n4   stem            201         2012-10-02 00:00:00\n', 'q_visits'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'Series' is not defined""]";['NameError'];0;1;"[""name 'Series' is not defined""]";['NameError'];0;1;"[""name 'Series' is not defined""]";['NameError']
478;478;478;478;1.0;0;18689474;;1;13;<python><pandas>;group multi-index pandas dataframe;6147.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
479;479;479;479;2.0;2;18689512;;1;55;<python><numpy><pandas>;Efficiently checking if arbitrary object is NaN in Python / numpy / pandas?;68431.0;"['>>> np.isnan(\'some_string\')\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nTypeError: Not implemented for this type\n']";"['>>> np.isnan(\'some_string\')\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nTypeError: Not implemented for this type\n']";"['np.nan', 'numpy.isnan(val)', 'val', 'numpy.isnan()', '>>> np.isnan(\'some_string\')\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\nTypeError: Not implemented for this type\n', 'False']";[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'Series' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'Series' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'Series' is not defined""]";['Sucess', 'NameError']
480;480;480;480;6.0;0;18689823;;1;29;<python><pandas><nan>;pandas DataFrame: replace nan values with average of columns;36285.0;[''];[];['nan', 'nan'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'sub2' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'sub2' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""name 'sub2' is not defined""]";['Sucess', 'NameError']
481;481;481;481;4.0;4;18695605;;1;42;<python><dictionary><pandas>;python pandas dataframe to dictionary;63301.0;['    id    value\n0    0     10.2\n1    1      5.7\n2    2      7.4\n'];['    id    value\n0    0     10.2\n1    1      5.7\n2    2      7.4\n'];['    id    value\n0    0     10.2\n1    1      5.7\n2    2      7.4\n'];[''];[''];False;['import pandas as pd\n'];False;2;4;"[""name 'df' is not defined"", 'Sucess', 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'Sucess', 'NameError'];2;4;"[""name 'df' is not defined"", 'Sucess', 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'Sucess', 'NameError'];2;4;"[""'id'"", 'Sucess', 'Sucess', ""'DataFrame' object has no attribute 'id'""]";['KeyError', 'Sucess', 'Sucess', 'AttributeError']
482;482;482;482;3.0;5;18713929;;1;11;<python><numpy><pandas><subsampling>;Subsample pandas dataframe;9619.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", 'Population must be a sequence or set.  For dicts, use list(d).']";['NameError', 'TypeError']
483;483;483;483;3.0;0;18792918;;1;28;<python><pandas><left-join><dataframe>;Pandas Combining 2 Data Frames (join on a common column);63764.0;"[""Data columns (total 13 columns):\nbusiness_id      4503  non-null values\ncategories       4503  non-null values\ncity             4503  non-null values\nfull_address     4503  non-null values\nlatitude         4503  non-null values\nlongitude        4503  non-null values\nname             4503  non-null values\nneighborhoods    4503  non-null values\nopen             4503  non-null values\nreview_count     4503  non-null values\nstars            4503  non-null values\nstate            4503  non-null values\ntype             4503  non-null values\ndtypes: bool(1), float64(3), int64(1), object(8)`\nInt64Index: 158430 entries, 0 to 229905\nData columns (total 8 columns):\nbusiness_id    158430  non-null values\ndate           158430  non-null values\nreview_id      158430  non-null values\nstars          158430  non-null values\ntext           158430  non-null values\ntype           158430  non-null values\nuser_id        158430  non-null values\nvotes          158430  non-null values\ndtypes: int64(1), object(7)\n#the following line of code creates a left join of restaurant_ids_frame and   restaurant_review_frame on the column 'business_id'\nrestaurant_review_frame.join(other=restaurant_ids_dataframe,on='business_id',how='left')\nException: columns overlap: Index([business_id, stars, type], dtype=object)\n""]";"['Data columns (total 13 columns):\nbusiness_id      4503  non-null values\ncategories       4503  non-null values\ncity             4503  non-null values\nfull_address     4503  non-null values\nlatitude         4503  non-null values\nlongitude        4503  non-null values\nname             4503  non-null values\nneighborhoods    4503  non-null values\nopen             4503  non-null values\nreview_count     4503  non-null values\nstars            4503  non-null values\nstate            4503  non-null values\ntype             4503  non-null values\ndtypes: bool(1), float64(3), int64(1), object(8)`\n', 'Int64Index: 158430 entries, 0 to 229905\nData columns (total 8 columns):\nbusiness_id    158430  non-null values\ndate           158430  non-null values\nreview_id      158430  non-null values\nstars          158430  non-null values\ntext           158430  non-null values\ntype           158430  non-null values\nuser_id        158430  non-null values\nvotes          158430  non-null values\ndtypes: int64(1), object(7)\n', ""#the following line of code creates a left join of restaurant_ids_frame and   restaurant_review_frame on the column 'business_id'\nrestaurant_review_frame.join(other=restaurant_ids_dataframe,on='business_id',how='left')\n"", 'Exception: columns overlap: Index([business_id, stars, type], dtype=object)\n']";"['Data columns (total 13 columns):\nbusiness_id      4503  non-null values\ncategories       4503  non-null values\ncity             4503  non-null values\nfull_address     4503  non-null values\nlatitude         4503  non-null values\nlongitude        4503  non-null values\nname             4503  non-null values\nneighborhoods    4503  non-null values\nopen             4503  non-null values\nreview_count     4503  non-null values\nstars            4503  non-null values\nstate            4503  non-null values\ntype             4503  non-null values\ndtypes: bool(1), float64(3), int64(1), object(8)`\n', 'Int64Index: 158430 entries, 0 to 229905\nData columns (total 8 columns):\nbusiness_id    158430  non-null values\ndate           158430  non-null values\nreview_id      158430  non-null values\nstars          158430  non-null values\ntext           158430  non-null values\ntype           158430  non-null values\nuser_id        158430  non-null values\nvotes          158430  non-null values\ndtypes: int64(1), object(7)\n', ""#the following line of code creates a left join of restaurant_ids_frame and   restaurant_review_frame on the column 'business_id'\nrestaurant_review_frame.join(other=restaurant_ids_dataframe,on='business_id',how='left')\n"", 'Exception: columns overlap: Index([business_id, stars, type], dtype=object)\n']";"[""#the following line of code creates a left join of restaurant_ids_frame and   restaurant_review_frame on the column 'business_id'\nrestaurant_review_frame.join(other=restaurant_ids_dataframe,on='business_id',how='left')\n""]";"[""#the following line of code creates a left join of restaurant_ids_frame and   restaurant_review_frame on the column 'business_id'\nrestaurant_review_frame.join(other=restaurant_ids_dataframe,on='business_id',how='left')\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\n#the following line of code creates a left join of restaurant_ids_frame and   restaurant_review_frame on the column 'business_id'\nrestaurant_review_frame.join(other=restaurant_ids_dataframe,on='business_id',how='left')\n""]";True;0;3;"[""name 'restaurant_review_frame' is not defined"", ""name 'restaurant_ids_dataframe' is not defined"", ""name 'T1' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'restaurant_review_frame' is not defined"", ""name 'restaurant_ids_dataframe' is not defined"", ""name 'T1' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'restaurant_review_frame' is not defined"", ""name 'restaurant_ids_dataframe' is not defined"", ""name 'T1' is not defined""]";['NameError', 'NameError', 'NameError']
484;484;484;484;2.0;2;18835077;;1;34;<python><pandas>;selecting from multi-index pandas;34779.0;['# has multi-index (A,B)\ndf\n#can i do this? I know this doesnt work because index is multi-index so I need to     specify a tuple\n\ndf.ix[df.A ==1]\n'];['# has multi-index (A,B)\ndf\n#can i do this? I know this doesnt work because index is multi-index so I need to     specify a tuple\n\ndf.ix[df.A ==1]\n'];['# has multi-index (A,B)\ndf\n#can i do this? I know this doesnt work because index is multi-index so I need to     specify a tuple\n\ndf.ix[df.A ==1]\n'];['# has multi-index (A,B)\ndf\n#can i do this? I know this doesnt work because index is multi-index so I need to     specify a tuple\n\ndf.ix[df.A ==1]\n'];['# has multi-index (A,B)\ndf\n#can i do this? I know this doesnt work because index is multi-index so I need to     specify a tuple\n\ndf.ix[df.A ==1]\n'];False;['import pandas as pd\ndf = pd.DataFrame()\n# has multi-index (A,B)\ndf\n#can i do this? I know this doesnt work because index is multi-index so I need to     specify a tuple\n\ndf.ix[df.A ==1]\n'];True;0;2;"[""name 'df' is not defined"", ""name 'DataFrame' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError'];0;2;"[""'Level A must be same as name (None)'"", ""name 'DataFrame' is not defined""]";['KeyError', 'NameError']
485;485;485;485;8.0;4;18837262;;1;78;<python><pandas><dataframe>;Convert Python dict into a dataframe;151737.0;"[""{u'2012-06-08': 388,\n u'2012-06-09': 388,\n u'2012-06-10': 388,\n u'2012-06-11': 389,\n u'2012-06-12': 389,\n u'2012-06-13': 389,\n u'2012-06-14': 389,\n u'2012-06-15': 389,\n u'2012-06-16': 389,\n u'2012-06-17': 389,\n u'2012-06-18': 390,\n u'2012-06-19': 390,\n u'2012-06-20': 390,\n u'2012-06-21': 390,\n u'2012-06-22': 390,\n u'2012-06-23': 390,\n u'2012-06-24': 390,\n u'2012-06-25': 391,\n u'2012-06-26': 391,\n u'2012-06-27': 391,\n u'2012-06-28': 391,\n u'2012-06-29': 391,\n u'2012-06-30': 391,\n u'2012-07-01': 391,\n u'2012-07-02': 392,\n u'2012-07-03': 392,\n u'2012-07-04': 392,\n u'2012-07-05': 392,\n u'2012-07-06': 392}\n     Date         DateValue\n0    2012-07-01    391\n1    2012-07-02    392\n2    2012-07-03    392\n.    2012-07-04    392\n.    ...           ...\n.    ...           ...\ns  = Series(my_dict,index=my_dict.keys())\n""]";"[""{u'2012-06-08': 388,\n u'2012-06-09': 388,\n u'2012-06-10': 388,\n u'2012-06-11': 389,\n u'2012-06-12': 389,\n u'2012-06-13': 389,\n u'2012-06-14': 389,\n u'2012-06-15': 389,\n u'2012-06-16': 389,\n u'2012-06-17': 389,\n u'2012-06-18': 390,\n u'2012-06-19': 390,\n u'2012-06-20': 390,\n u'2012-06-21': 390,\n u'2012-06-22': 390,\n u'2012-06-23': 390,\n u'2012-06-24': 390,\n u'2012-06-25': 391,\n u'2012-06-26': 391,\n u'2012-06-27': 391,\n u'2012-06-28': 391,\n u'2012-06-29': 391,\n u'2012-06-30': 391,\n u'2012-07-01': 391,\n u'2012-07-02': 392,\n u'2012-07-03': 392,\n u'2012-07-04': 392,\n u'2012-07-05': 392,\n u'2012-07-06': 392}\n"", '     Date         DateValue\n0    2012-07-01    391\n1    2012-07-02    392\n2    2012-07-03    392\n.    2012-07-04    392\n.    ...           ...\n.    ...           ...\n', 's  = Series(my_dict,index=my_dict.keys())\n']";"[""{u'2012-06-08': 388,\n u'2012-06-09': 388,\n u'2012-06-10': 388,\n u'2012-06-11': 389,\n u'2012-06-12': 389,\n u'2012-06-13': 389,\n u'2012-06-14': 389,\n u'2012-06-15': 389,\n u'2012-06-16': 389,\n u'2012-06-17': 389,\n u'2012-06-18': 390,\n u'2012-06-19': 390,\n u'2012-06-20': 390,\n u'2012-06-21': 390,\n u'2012-06-22': 390,\n u'2012-06-23': 390,\n u'2012-06-24': 390,\n u'2012-06-25': 391,\n u'2012-06-26': 391,\n u'2012-06-27': 391,\n u'2012-06-28': 391,\n u'2012-06-29': 391,\n u'2012-06-30': 391,\n u'2012-07-01': 391,\n u'2012-07-02': 392,\n u'2012-07-03': 392,\n u'2012-07-04': 392,\n u'2012-07-05': 392,\n u'2012-07-06': 392}\n"", '     Date         DateValue\n0    2012-07-01    391\n1    2012-07-02    392\n2    2012-07-03    392\n.    2012-07-04    392\n.    ...           ...\n.    ...           ...\n', 's  = Series(my_dict,index=my_dict.keys())\n']";['s  = Series(my_dict,index=my_dict.keys())\n'];['s  = Series(my_dict,index=my_dict.keys())\n'];False;['import pandas as pd\ns  = Series(my_dict,index=my_dict.keys())\n'];False;1;3;"[""name 'pd' is not defined"", ""name 'pd' is not defined"", 'Sucess']";['NameError', 'NameError', 'Sucess'];1;3;"[""name 'data' is not defined"", ""name 'd' is not defined"", 'Sucess']";['NameError', 'NameError', 'Sucess'];2;3;"['Sucess', ""name 'd' is not defined"", 'Sucess']";['Sucess', 'NameError', 'Sucess']
486;486;486;486;1.0;0;18837659;;1;18;<pandas><aggregation>;Pandas - possible to aggregate two columns using two different aggregations?;11390.0;"['data = pd.read_table(""file.csv"", sep="","", thousands=\',\')\ngrouped = data.groupby([""date"", ""textA"", ""textB""], as_index=False)\n']";"['data = pd.read_table(""file.csv"", sep="","", thousands=\',\')\ngrouped = data.groupby([""date"", ""textA"", ""textB""], as_index=False)\n']";"['data = pd.read_table(""file.csv"", sep="","", thousands=\',\')\ngrouped = data.groupby([""date"", ""textA"", ""textB""], as_index=False)\n', 'sum(numberA), min(numberB)']";"['data = pd.read_table(""file.csv"", sep="","", thousands=\',\')\ngrouped = data.groupby([""date"", ""textA"", ""textB""], as_index=False)\n']";"['import pandas as pd\ndata = pd.read_table(""file.csv"", sep="","", thousands=\',\')\ngrouped = data.groupby([""date"", ""textA"", ""textB""], as_index=False)\n']";True;"['import pandas as pd\ndata = pd.read_table(""file.csv"", sep="","", thousands=\',\')\ngrouped = data.groupby([""date"", ""textA"", ""textB""], as_index=False)\n']";False;0;1;"[""name 'grouped' is not defined""]";['NameError'];0;1;"[""name 'grouped' is not defined""]";['NameError'];0;1;"[""name 'grouped' is not defined""]";['NameError']
487;487;487;487;4.0;0;18851216;;1;11;<pandas><duplicates>;Pandas: Drop all records of duplicate indices;1815.0;[''];[];['appkey', 'appkey', 'drop_duplicates'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'pd' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
488;488;488;488;3.0;1;18876022;;1;25;<python><html><pandas><ipython>;How to format IPython html display of Pandas dataframe?;11860.0;"[""int_frmt:lambda x : '{:,}'.format(x)\nnp.set_printoptions(formatter={'int_kind':int_frmt})\npd.set_option('display.notebook_repr_html', True)\nfrom IPython.display import HTML\nint_frmt = lambda x: '{:,}'.format(x)\nfloat_frmt = lambda x: '{:,.0f}'.format(x) if x > 1e3 else '{:,.2f}'.format(x)\nfrmt_map = {np.dtype('int64'):int_frmt, np.dtype('float64'):float_frmt}\nfrmt = {col:frmt_map[df.dtypes[col]] for col in df.columns if df.dtypes[col] in frmt_map.keys()}\nHTML(df.to_html(formatters=frmt))\n""]";"[""int_frmt:lambda x : '{:,}'.format(x)\nnp.set_printoptions(formatter={'int_kind':int_frmt})\n"", ""pd.set_option('display.notebook_repr_html', True)\n"", ""from IPython.display import HTML\nint_frmt = lambda x: '{:,}'.format(x)\nfloat_frmt = lambda x: '{:,.0f}'.format(x) if x > 1e3 else '{:,.2f}'.format(x)\nfrmt_map = {np.dtype('int64'):int_frmt, np.dtype('float64'):float_frmt}\nfrmt = {col:frmt_map[df.dtypes[col]] for col in df.columns if df.dtypes[col] in frmt_map.keys()}\nHTML(df.to_html(formatters=frmt))\n""]";"['numpy', 'set_printoptions', ""int_frmt:lambda x : '{:,}'.format(x)\nnp.set_printoptions(formatter={'int_kind':int_frmt})\n"", ""pd.set_option('display.notebook_repr_html', True)\n"", ""from IPython.display import HTML\nint_frmt = lambda x: '{:,}'.format(x)\nfloat_frmt = lambda x: '{:,.0f}'.format(x) if x > 1e3 else '{:,.2f}'.format(x)\nfrmt_map = {np.dtype('int64'):int_frmt, np.dtype('float64'):float_frmt}\nfrmt = {col:frmt_map[df.dtypes[col]] for col in df.columns if df.dtypes[col] in frmt_map.keys()}\nHTML(df.to_html(formatters=frmt))\n""]";"[""np.set_printoptions(formatter={'int_kind':int_frmt})\npd.set_option('display.notebook_repr_html', True)\nfrom IPython.display import HTML\nint_frmt = lambda x: '{:,}'.format(x)\nfloat_frmt = lambda x: '{:,.0f}'.format(x) if x > 1e3 else '{:,.2f}'.format(x)\nfrmt_map = {np.dtype('int64'):int_frmt, np.dtype('float64'):float_frmt}\nfrmt = {col:frmt_map[df.dtypes[col]] for col in df.columns if df.dtypes[col] in frmt_map.keys()}\nHTML(df.to_html(formatters=frmt))\n""]";"[""import pandas as pd\nnp.set_printoptions(formatter={'int_kind':int_frmt})\npd.set_option('display.notebook_repr_html', True)\nfrom IPython.display import HTML\nint_frmt = lambda x: '{:,}'.format(x)\nfloat_frmt = lambda x: '{:,.0f}'.format(x) if x > 1e3 else '{:,.2f}'.format(x)\nfrmt_map = {np.dtype('int64'):int_frmt, np.dtype('float64'):float_frmt}\nfrmt = {col:frmt_map[df.dtypes[col]] for col in df.columns if df.dtypes[col] in frmt_map.keys()}\nHTML(df.to_html(formatters=frmt))\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\nnp.set_printoptions(formatter={'int_kind':int_frmt})\npd.set_option('display.notebook_repr_html', True)\nfrom IPython.display import HTML\nint_frmt = lambda x: '{:,}'.format(x)\nfloat_frmt = lambda x: '{:,.0f}'.format(x) if x > 1e3 else '{:,.2f}'.format(x)\nfrmt_map = {np.dtype('int64'):int_frmt, np.dtype('float64'):float_frmt}\nfrmt = {col:frmt_map[df.dtypes[col]] for col in df.columns if df.dtypes[col] in frmt_map.keys()}\nHTML(df.to_html(formatters=frmt))\n""]";True;0;1;"[""name 'HTML' is not defined""]";['NameError'];0;1;"[""name 'HTML' is not defined""]";['NameError'];0;1;"[""name 'HTML' is not defined""]";['NameError']
489;489;489;489;2.0;0;18878308;;1;15;<python><sorting><pandas><reindex>;Pandas: Reindex Unsorts Dataframe;11800.0;"[""dfm = dfm.sort(['delt'],ascending=False)\ndfm = dfm.reindex(index=range(1,len(dfm)))\n""]";"[""dfm = dfm.sort(['delt'],ascending=False)\ndfm = dfm.reindex(index=range(1,len(dfm)))\n""]";"[""dfm = dfm.sort(['delt'],ascending=False)\ndfm = dfm.reindex(index=range(1,len(dfm)))\n""]";"[""dfm = dfm.sort(['delt'],ascending=False)\ndfm = dfm.reindex(index=range(1,len(dfm)))\n""]";"[""dfm = dfm.sort(['delt'],ascending=False)\ndfm = dfm.reindex(index=range(1,len(dfm)))\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndfm = dfm.sort(['delt'],ascending=False)\ndfm = dfm.reindex(index=range(1,len(dfm)))\n""]";True;0;2;"[""name 'dfm' is not defined"", ""name 'DataFrame' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'dfm' is not defined"", ""name 'randn' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'dfm' is not defined"", ""name 'DataFrame' is not defined""]";['NameError', 'NameError']
490;490;490;490;3.0;0;18885175;;1;29;<python><zip><pandas>;Read a zipped file as a pandas DataFrame;18856.0;"[""import requests, zipfile, StringIO\nr = requests.get('http://data.octo.dc.gov/feeds/crime_incidents/archive/crime_incidents_2013_CSV.zip')\nz = zipfile.ZipFile(StringIO.StringIO(r.content))\ncrime2013 = pandas.read_csv(z.read('crime_incidents_2013_CSV.csv'))\n""]";"[""import requests, zipfile, StringIO\nr = requests.get('http://data.octo.dc.gov/feeds/crime_incidents/archive/crime_incidents_2013_CSV.zip')\nz = zipfile.ZipFile(StringIO.StringIO(r.content))\ncrime2013 = pandas.read_csv(z.read('crime_incidents_2013_CSV.csv'))\n""]";"[""import requests, zipfile, StringIO\nr = requests.get('http://data.octo.dc.gov/feeds/crime_incidents/archive/crime_incidents_2013_CSV.zip')\nz = zipfile.ZipFile(StringIO.StringIO(r.content))\ncrime2013 = pandas.read_csv(z.read('crime_incidents_2013_CSV.csv'))\n""]";"[""import requests, zipfile, StringIO\nr = requests.get('http://data.octo.dc.gov/feeds/crime_incidents/archive/crime_incidents_2013_CSV.zip')\nz = zipfile.ZipFile(StringIO.StringIO(r.content))\ncrime2013 = pandas.read_csv(z.read('crime_incidents_2013_CSV.csv'))\n""]";"[""import requests, zipfile, StringIO\nr = requests.get('http://data.octo.dc.gov/feeds/crime_incidents/archive/crime_incidents_2013_CSV.zip')\nz = zipfile.ZipFile(StringIO.StringIO(r.content))\ncrime2013 = pandas.read_csv(z.read('crime_incidents_2013_CSV.csv'))\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nimport requests, zipfile, StringIO\nr = requests.get('http://data.octo.dc.gov/feeds/crime_incidents/archive/crime_incidents_2013_CSV.zip')\nz = zipfile.ZipFile(StringIO.StringIO(r.content))\ncrime2013 = pandas.read_csv(z.read('crime_incidents_2013_CSV.csv'))\n""]";True;0;2;"[""name 'pd' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'z' is not defined"", ""name 'filename' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'z' is not defined"", ""name 'filename' is not defined""]";['NameError', 'NameError']
491;491;491;491;3.0;3;18889588;;1;21;<python><pandas><dummy-data><categorical-data>;Create dummies from column with multiple values in pandas;14589.0;[''];[];"['pandas.get_dummies()', ""['A', 'B']"", 'get_dummies()', ""['A', 'B', 'C', 'D', 'A*C', 'C*D']"", 'get_dummies()']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'label'""]";['KeyError']
492;492;492;492;4.0;4;18915941;;1;17;<python><pandas>;Create a pandas DataFrame from generator?;9974.0;['import pandas as pd\ndf = pd.DataFrame.from_records(tuple_generator, columns = tuple_fields_name_list)\n... \nC:\\Anaconda\\envs\\py33\\lib\\site-packages\\pandas\\core\\frame.py in from_records(cls, data, index, exclude, columns, coerce_float, nrows)\n   1046                 values.append(row)\n   1047                 i += 1\n-> 1048                 if i >= nrows:\n   1049                     break\n   1050 \n\nTypeError: unorderable types: int() >= NoneType()\ndf = pd.DataFrame.from_records(list(tuple_generator), columns = tuple_fields_name_list)\n'];['import pandas as pd\ndf = pd.DataFrame.from_records(tuple_generator, columns = tuple_fields_name_list)\n', '... \nC:\\Anaconda\\envs\\py33\\lib\\site-packages\\pandas\\core\\frame.py in from_records(cls, data, index, exclude, columns, coerce_float, nrows)\n   1046                 values.append(row)\n   1047                 i += 1\n-> 1048                 if i >= nrows:\n   1049                     break\n   1050 \n\nTypeError: unorderable types: int() >= NoneType()\n', 'df = pd.DataFrame.from_records(list(tuple_generator), columns = tuple_fields_name_list)\n'];['import pandas as pd\ndf = pd.DataFrame.from_records(tuple_generator, columns = tuple_fields_name_list)\n', '... \nC:\\Anaconda\\envs\\py33\\lib\\site-packages\\pandas\\core\\frame.py in from_records(cls, data, index, exclude, columns, coerce_float, nrows)\n   1046                 values.append(row)\n   1047                 i += 1\n-> 1048                 if i >= nrows:\n   1049                     break\n   1050 \n\nTypeError: unorderable types: int() >= NoneType()\n', 'df = pd.DataFrame.from_records(list(tuple_generator), columns = tuple_fields_name_list)\n'];['import pandas as pd\ndf = pd.DataFrame.from_records(tuple_generator, columns = tuple_fields_name_list)\n... \n\ndf = pd.DataFrame.from_records(list(tuple_generator), columns = tuple_fields_name_list)\n'];['import pandas as pd\ndf = pd.DataFrame.from_records(tuple_generator, columns = tuple_fields_name_list)\n... \n\ndf = pd.DataFrame.from_records(list(tuple_generator), columns = tuple_fields_name_list)\n'];False;['import pandas as pd\nimport pandas as pd\ndf = pd.DataFrame.from_records(tuple_generator, columns = tuple_fields_name_list)\n... \n\ndf = pd.DataFrame.from_records(list(tuple_generator), columns = tuple_fields_name_list)\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
493;493;493;493;6.0;0;18936957;;1;14;<python><text><pandas>;Count distinct words from a Pandas Data Frame;11485.0;"[""import pandas as pd\n\nr1=['My nickname is ft.jgt','Someone is going to my place']\n\ndf=pd.DataFrame(r1,columns=['text'])\n['my','nickname','is','ft.jgt','someone','going','to','place']\n""]";"[""import pandas as pd\n\nr1=['My nickname is ft.jgt','Someone is going to my place']\n\ndf=pd.DataFrame(r1,columns=['text'])\n"", ""['my','nickname','is','ft.jgt','someone','going','to','place']\n""]";"[""import pandas as pd\n\nr1=['My nickname is ft.jgt','Someone is going to my place']\n\ndf=pd.DataFrame(r1,columns=['text'])\n"", ""['my','nickname','is','ft.jgt','someone','going','to','place']\n""]";"[""import pandas as pd\n\nr1=['My nickname is ft.jgt','Someone is going to my place']\n\ndf=pd.DataFrame(r1,columns=['text'])\n['my','nickname','is','ft.jgt','someone','going','to','place']\n""]";"[""import pandas as pd\n\nr1=['My nickname is ft.jgt','Someone is going to my place']\n\ndf=pd.DataFrame(r1,columns=['text'])\n['my','nickname','is','ft.jgt','someone','going','to','place']\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\n\nr1=['My nickname is ft.jgt','Someone is going to my place']\n\ndf=pd.DataFrame(r1,columns=['text'])\n['my','nickname','is','ft.jgt','someone','going','to','place']\n""]";True;2;3;"['Sucess', ""name 'df' is not defined"", 'Sucess']";['Sucess', 'NameError', 'Sucess'];2;3;"['Sucess', ""name 'df' is not defined"", 'Sucess']";['Sucess', 'NameError', 'Sucess'];2;3;"['Sucess', ""'text'"", 'Sucess']";['Sucess', 'KeyError', 'Sucess']
494;494;494;494;1.0;1;18942506;;1;41;<python><pandas><dataframe>;Add new column in Pandas DataFrame Python;83369.0;['Col1 Col2\nA     1 \nB     2\nC     3\nCol1 Col2 Col3\nA    1    1\nB    2    0\nC    3    0\n'];['Col1 Col2\nA     1 \nB     2\nC     3\n', 'Col1 Col2 Col3\nA    1    1\nB    2    0\nC    3    0\n'];['Col1 Col2\nA     1 \nB     2\nC     3\n', 'Col1 Col2 Col3\nA    1    1\nB    2    0\nC    3    0\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'Col2'""]";['KeyError']
495;495;495;495;3.0;1;18973404;;1;34;<python><matplotlib><pandas><bar-chart>;Setting Different Bar color in matplotlib Python;46910.0;"["">>> f=plt.figure()\n>>> ax=f.add_subplot(1,1,1)\n>>> ax.bar([1,2,3,4], [1,2,3,4])\n<Container object of 4 artists>\n>>> ax.get_children()\n[<matplotlib.axis.XAxis object at 0x6529850>, <matplotlib.axis.YAxis object at 0x78460d0>,  <matplotlib.patches.Rectangle object at 0x733cc50>, <matplotlib.patches.Rectangle object at 0x733cdd0>, <matplotlib.patches.Rectangle object at 0x777f290>, <matplotlib.patches.Rectangle object at 0x777f710>, <matplotlib.text.Text object at 0x7836450>, <matplotlib.patches.Rectangle object at 0x7836390>, <matplotlib.spines.Spine object at 0x6529950>, <matplotlib.spines.Spine object at 0x69aef50>, <matplotlib.spines.Spine object at 0x69ae310>, <matplotlib.spines.Spine object at 0x69aea50>]\n>>> ax.get_children()[2].set_color('r') #You can also try to locate the first patches.Rectangle object instead of direct calling the index.\n""]";"["">>> f=plt.figure()\n>>> ax=f.add_subplot(1,1,1)\n>>> ax.bar([1,2,3,4], [1,2,3,4])\n<Container object of 4 artists>\n>>> ax.get_children()\n[<matplotlib.axis.XAxis object at 0x6529850>, <matplotlib.axis.YAxis object at 0x78460d0>,  <matplotlib.patches.Rectangle object at 0x733cc50>, <matplotlib.patches.Rectangle object at 0x733cdd0>, <matplotlib.patches.Rectangle object at 0x777f290>, <matplotlib.patches.Rectangle object at 0x777f710>, <matplotlib.text.Text object at 0x7836450>, <matplotlib.patches.Rectangle object at 0x7836390>, <matplotlib.spines.Spine object at 0x6529950>, <matplotlib.spines.Spine object at 0x69aef50>, <matplotlib.spines.Spine object at 0x69ae310>, <matplotlib.spines.Spine object at 0x69aea50>]\n>>> ax.get_children()[2].set_color('r') #You can also try to locate the first patches.Rectangle object instead of direct calling the index.\n""]";"["">>> f=plt.figure()\n>>> ax=f.add_subplot(1,1,1)\n>>> ax.bar([1,2,3,4], [1,2,3,4])\n<Container object of 4 artists>\n>>> ax.get_children()\n[<matplotlib.axis.XAxis object at 0x6529850>, <matplotlib.axis.YAxis object at 0x78460d0>,  <matplotlib.patches.Rectangle object at 0x733cc50>, <matplotlib.patches.Rectangle object at 0x733cdd0>, <matplotlib.patches.Rectangle object at 0x777f290>, <matplotlib.patches.Rectangle object at 0x777f710>, <matplotlib.text.Text object at 0x7836450>, <matplotlib.patches.Rectangle object at 0x7836390>, <matplotlib.spines.Spine object at 0x6529950>, <matplotlib.spines.Spine object at 0x69aef50>, <matplotlib.spines.Spine object at 0x69ae310>, <matplotlib.spines.Spine object at 0x69aea50>]\n>>> ax.get_children()[2].set_color('r') #You can also try to locate the first patches.Rectangle object instead of direct calling the index.\n""]";[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""No module named 'matplotlib'""]";['Sucess', 'ImportError'];1;2;"['Sucess', ""No module named 'matplotlib'""]";['Sucess', 'ImportError'];1;2;"['Sucess', ""No module named 'matplotlib'""]";['Sucess', 'ImportError']
496;496;496;496;1.0;0;18992086;;1;31;<python><pandas><histogram>;save a pandas.Series histogram plot to file;26802.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 's' is not defined""]";['NameError'];0;1;"[""name 's' is not defined""]";['NameError'];0;1;"[""name 's' is not defined""]";['NameError']
497;497;497;497;1.0;0;19062612;;1;14;<pandas>;Delete all but one column of pandas dataframe?;3170.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError']
498;498;498;498;4.0;0;19071199;;1;15;<python><pandas><dataframe>;Pandas dataframe: drop columns whose name contains a specific string;10447.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'DataFrame' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'randn' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'DataFrame' is not defined""]";['Sucess', 'NameError']
499;499;499;499;3.0;1;19078325;;1;22;<python><group-by><pandas><aggregate-functions>;Naming returned columns in Pandas aggregate function?;15533.0;"['data.groupby(""Country"").agg(\n        {""column1"": {""foo"": sum()}, ""column2"": {""mean"": np.mean, ""std"": np.std}})\n']";"['data.groupby(""Country"").agg(\n        {""column1"": {""foo"": sum()}, ""column2"": {""mean"": np.mean, ""std"": np.std}})\n']";"['data.groupby(""Country"").agg(\n        {""column1"": {""foo"": sum()}, ""column2"": {""mean"": np.mean, ""std"": np.std}})\n']";[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'data' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'data' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'data' is not defined"", ""'A'""]";['NameError', 'KeyError']
500;500;500;500;2.0;0;19103624;;1;14;<python><csv><numpy><pandas>;Load CSV to Pandas MultiIndex DataFrame;11781.0;"[""from, to, dep, freq, arr, code, mode   (header row)\nRGBOXFD,RGBPADTON,127,0,27,99999,2\nRGBOXFD,RGBPADTON,127,0,33,99999,2\nRGBOXFD,RGBRDLEY,127,0,1425,99999,2\nRGBOXFD,RGBCHOLSEY,127,0,52,99999,2\nRGBOXFD,RGBMDNHEAD,127,0,91,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,46,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,3,99999,2\nRGBDIDCOTP,RGBCHOLSEY,127,0,61,99999,2\nRGBDIDCOTP,RGBRDLEY,127,0,1430,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,115,99999,2\nand so on... \n r = pd.DataFrame.from_csv('test_data2.csv')\n                   dep, freq, arr, code, mode\nRGBOXFD RGBPADTON  127     0   27  99999    2\n        RGBRDLEY   127     0   33  99999    2\n        RGBCHOLSEY 127     0 1425  99999    2\n        RGBMDNHEAD 127     0 1525  99999    2\n""]";"['from, to, dep, freq, arr, code, mode   (header row)\nRGBOXFD,RGBPADTON,127,0,27,99999,2\nRGBOXFD,RGBPADTON,127,0,33,99999,2\nRGBOXFD,RGBRDLEY,127,0,1425,99999,2\nRGBOXFD,RGBCHOLSEY,127,0,52,99999,2\nRGBOXFD,RGBMDNHEAD,127,0,91,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,46,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,3,99999,2\nRGBDIDCOTP,RGBCHOLSEY,127,0,61,99999,2\nRGBDIDCOTP,RGBRDLEY,127,0,1430,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,115,99999,2\nand so on... \n', "" r = pd.DataFrame.from_csv('test_data2.csv')\n"", '                   dep, freq, arr, code, mode\nRGBOXFD RGBPADTON  127     0   27  99999    2\n        RGBRDLEY   127     0   33  99999    2\n        RGBCHOLSEY 127     0 1425  99999    2\n        RGBMDNHEAD 127     0 1525  99999    2\n']";"['from, to, dep, freq, arr, code, mode   (header row)\nRGBOXFD,RGBPADTON,127,0,27,99999,2\nRGBOXFD,RGBPADTON,127,0,33,99999,2\nRGBOXFD,RGBRDLEY,127,0,1425,99999,2\nRGBOXFD,RGBCHOLSEY,127,0,52,99999,2\nRGBOXFD,RGBMDNHEAD,127,0,91,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,46,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,3,99999,2\nRGBDIDCOTP,RGBCHOLSEY,127,0,61,99999,2\nRGBDIDCOTP,RGBRDLEY,127,0,1430,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,115,99999,2\nand so on... \n', "" r = pd.DataFrame.from_csv('test_data2.csv')\n"", '                   dep, freq, arr, code, mode\nRGBOXFD RGBPADTON  127     0   27  99999    2\n        RGBRDLEY   127     0   33  99999    2\n        RGBCHOLSEY 127     0 1425  99999    2\n        RGBMDNHEAD 127     0 1525  99999    2\n']";['RGBOXFD,RGBPADTON,127,0,27,99999,2\nRGBOXFD,RGBPADTON,127,0,33,99999,2\nRGBOXFD,RGBRDLEY,127,0,1425,99999,2\nRGBOXFD,RGBCHOLSEY,127,0,52,99999,2\nRGBOXFD,RGBMDNHEAD,127,0,91,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,46,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,3,99999,2\nRGBDIDCOTP,RGBCHOLSEY,127,0,61,99999,2\nRGBDIDCOTP,RGBRDLEY,127,0,1430,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,115,99999,2\n'];['RGBOXFD,RGBPADTON,127,0,27,99999,2\nRGBOXFD,RGBPADTON,127,0,33,99999,2\nRGBOXFD,RGBRDLEY,127,0,1425,99999,2\nRGBOXFD,RGBCHOLSEY,127,0,52,99999,2\nRGBOXFD,RGBMDNHEAD,127,0,91,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,46,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,3,99999,2\nRGBDIDCOTP,RGBCHOLSEY,127,0,61,99999,2\nRGBDIDCOTP,RGBRDLEY,127,0,1430,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,115,99999,2\n'];False;['import pandas as pd\nRGBOXFD,RGBPADTON,127,0,27,99999,2\nRGBOXFD,RGBPADTON,127,0,33,99999,2\nRGBOXFD,RGBRDLEY,127,0,1425,99999,2\nRGBOXFD,RGBCHOLSEY,127,0,52,99999,2\nRGBOXFD,RGBMDNHEAD,127,0,91,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,46,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,3,99999,2\nRGBDIDCOTP,RGBCHOLSEY,127,0,61,99999,2\nRGBDIDCOTP,RGBRDLEY,127,0,1430,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,115,99999,2\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
501;501;501;501;3.0;0;19105976;;1;15;<python><date><pandas>;Get MM-DD-YYYY from pandas Timestamp;25815.0;['Name: Created_Date, Length: 1162549, dtype: datetime64[ns]`\n'];['Name: Created_Date, Length: 1162549, dtype: datetime64[ns]`\n'];"['2013-09-29 02:34:44', '09-29-2013', 'Name: Created_Date, Length: 1162549, dtype: datetime64[ns]`\n', '.date()', 'df.Created_Date.date()', ""AttributeError: 'Series' object has no attribute 'date'""]";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 's' is not defined""]";['NameError'];0;1;"[""name 's' is not defined""]";['NameError'];0;1;"[""name 's' is not defined""]";['NameError']
502;502;502;502;2.0;0;19112398;;1;65;<python><pandas><datanitro>;Getting list of lists into pandas DataFrame;72639.0;"['table = Cell(""A1"").table\ntable = [[\'Heading1\', \'Heading2\'], [1 , 2], [3, 4]]\n\nheaders = table.pop(0) # gives the headers as list and leaves data\n']";"['table = Cell(""A1"").table\n', ""table = [['Heading1', 'Heading2'], [1 , 2], [3, 4]]\n\nheaders = table.pop(0) # gives the headers as list and leaves data\n""]";"['table = Cell(""A1"").table\n', ""table = [['Heading1', 'Heading2'], [1 , 2], [3, 4]]\n\nheaders = table.pop(0) # gives the headers as list and leaves data\n""]";"['table = Cell(""A1"").table\ntable = [[\'Heading1\', \'Heading2\'], [1 , 2], [3, 4]]\n\nheaders = table.pop(0) # gives the headers as list and leaves data\n']";"['table = Cell(""A1"").table\ntable = [[\'Heading1\', \'Heading2\'], [1 , 2], [3, 4]]\n\nheaders = table.pop(0) # gives the headers as list and leaves data\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\ntable = Cell(""A1"").table\ntable = [[\'Heading1\', \'Heading2\'], [1 , 2], [3, 4]]\n\nheaders = table.pop(0) # gives the headers as list and leaves data\n']";True;0;2;"[""name 'DataFrame' is not defined"", ""name 'DataFrame' is not defined""]";['NameError', 'NameError'];1;2;"[""name 'table' is not defined"", 'Sucess']";['NameError', 'Sucess'];0;2;"[""name 'DataFrame' is not defined"", ""name 'DataFrame' is not defined""]";['NameError', 'NameError']
503;503;503;503;3.0;0;19119039;;1;13;<python><pandas>;pandas - Extend Index of a DataFrame setting all columns for new rows to NaN?;6379.0;"[""df2 = pd.DataFrame({ 'day': pd.Series([date(2012, 1, 1), date(2012, 1, 3)]), 'b' : pd.Series([0.22, 0.3]) })\ndf2 = df2.set_index('day')\ndf2\n               b\n day             \n2012-01-01  0.22\n2012-01-03  0.30\n               b\n day             \n2012-01-01  0.22\n2012-01-02   NaN\n2012-01-03  0.30\n2012-01-04   NaN\n...\n2012-01-31   NaN\n""]";"[""df2 = pd.DataFrame({ 'day': pd.Series([date(2012, 1, 1), date(2012, 1, 3)]), 'b' : pd.Series([0.22, 0.3]) })\ndf2 = df2.set_index('day')\ndf2\n               b\n day             \n2012-01-01  0.22\n2012-01-03  0.30\n"", '               b\n day             \n2012-01-01  0.22\n2012-01-02   NaN\n2012-01-03  0.30\n2012-01-04   NaN\n...\n2012-01-31   NaN\n']";"[""df2 = pd.DataFrame({ 'day': pd.Series([date(2012, 1, 1), date(2012, 1, 3)]), 'b' : pd.Series([0.22, 0.3]) })\ndf2 = df2.set_index('day')\ndf2\n               b\n day             \n2012-01-01  0.22\n2012-01-03  0.30\n"", 'NaN', 'b', '               b\n day             \n2012-01-01  0.22\n2012-01-02   NaN\n2012-01-03  0.30\n2012-01-04   NaN\n...\n2012-01-31   NaN\n']";"[""df2 = pd.DataFrame({ 'day': pd.Series([date(2012, 1, 1), date(2012, 1, 3)]), 'b' : pd.Series([0.22, 0.3]) })\ndf2 = df2.set_index('day')\ndf2\n...\n""]";"[""import pandas as pd\ndf2 = pd.DataFrame({ 'day': pd.Series([date(2012, 1, 1), date(2012, 1, 3)]), 'b' : pd.Series([0.22, 0.3]) })\ndf2 = df2.set_index('day')\ndf2\n...\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\ndf2 = pd.DataFrame({ 'day': pd.Series([date(2012, 1, 1), date(2012, 1, 3)]), 'b' : pd.Series([0.22, 0.3]) })\ndf2 = df2.set_index('day')\ndf2\n...\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'date' is not defined""]";['NameError'];0;1;"[""name 'date' is not defined""]";['NameError']
504;504;504;504;4.0;4;19124601;;1;138;<python><pandas><dataframe>;Is there a way to (pretty) print the entire Pandas Series / DataFrame?;94738.0;[''];[];['__repr__'];[''];[''];False;['import pandas as pd\n'];False;2;4;"['Sucess', 'Sucess', ""name 'pd' is not defined"", ""name 'df' is not defined""]";['Sucess', 'Sucess', 'NameError', 'NameError'];3;4;"['Sucess', 'Sucess', 'Sucess', ""name 'df' is not defined""]";['Sucess', 'Sucess', 'Sucess', 'NameError'];4;4;['Sucess', 'Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess', 'Sucess']
505;505;505;505;2.0;4;19125091;;1;22;<python><pandas>;Pandas Merge - How to avoid duplicating columns;21524.0;"[""df:                 currency  adj_date   data_col1 ...\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45\n...\n\ndf2:                currency  adj_date   data_col2 ...\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45\n...\ndfNew = merge(df, df2, left_index=True, right_index=True, how='outer')\ndfNew:              currency_x  adj_date_x   data_col2 ... currency_y adj_date_y\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45             USD         2012-01-03\n""]";"['df:                 currency  adj_date   data_col1 ...\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45\n...\n\ndf2:                currency  adj_date   data_col2 ...\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45\n...\n', ""dfNew = merge(df, df2, left_index=True, right_index=True, how='outer')\n"", 'dfNew:              currency_x  adj_date_x   data_col2 ... currency_y adj_date_y\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45             USD         2012-01-03\n']";"['df:                 currency  adj_date   data_col1 ...\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45\n...\n\ndf2:                currency  adj_date   data_col2 ...\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45\n...\n', ""dfNew = merge(df, df2, left_index=True, right_index=True, how='outer')\n"", 'dfNew:              currency_x  adj_date_x   data_col2 ... currency_y adj_date_y\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45             USD         2012-01-03\n']";"[""...\n\n...\ndfNew = merge(df, df2, left_index=True, right_index=True, how='outer')\n""]";"[""...\n\n...\ndfNew = merge(df, df2, left_index=True, right_index=True, how='outer')\n""]";False;"[""import pandas as pd\ndf2 = pd.DataFrame()\ndf = pd.DataFrame()\n...\n\n...\ndfNew = merge(df, df2, left_index=True, right_index=True, how='outer')\n""]";True;1;2;"[""name 'df2' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df2' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""cannot perform __sub__ with this index type: <class 'pandas.core.indexes.base.Index'>"", 'Sucess']";['TypeError', 'Sucess']
506;506;506;506;2.0;0;19155718;;1;23;<python><pandas>;Select Pandas rows based on list index;38383.0;['   20060930  10.103       NaN     10.103   7.981\n   20061231  15.915       NaN     15.915  12.686\n   20070331   3.196       NaN      3.196   2.710\n   20070630   7.907       NaN      7.907   6.459\n   20061231  15.915       NaN     15.915  12.686\n   20070630   7.907       NaN      7.907   6.459\n'];['   20060930  10.103       NaN     10.103   7.981\n   20061231  15.915       NaN     15.915  12.686\n   20070331   3.196       NaN      3.196   2.710\n   20070630   7.907       NaN      7.907   6.459\n', '   20061231  15.915       NaN     15.915  12.686\n   20070630   7.907       NaN      7.907   6.459\n'];['   20060930  10.103       NaN     10.103   7.981\n   20061231  15.915       NaN     15.915  12.686\n   20070331   3.196       NaN      3.196   2.710\n   20070630   7.907       NaN      7.907   6.459\n', '   20061231  15.915       NaN     15.915  12.686\n   20070630   7.907       NaN      7.907   6.459\n'];[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;['Sucess', 'positional indexers are out-of-bounds'];['Sucess', 'IndexError']
507;507;507;507;1.0;0;19169649;;1;13;<python><pandas><contains>;Using str.contains() in pandas with dataframes;23580.0;"['    df[df[\'Behavior\'].str.contains(""nt"", na=False)]\n    df[df[\'Behavior\'].str.contains(""nv"", na=False)]\n    ---------------------------------------------------------------------------\n    TypeError                                 Traceback (most recent call last)\n    <ipython-input-113-1d11e906812c> in <module>()\n    3 \n    4 \n    ----> 5 soctol = f_recs[f_recs[\'Behavior\'].str.contains(""nt""|""nv"", na=False)]\n    6 soctol\n\n    TypeError: unsupported operand type(s) for |: \'str\' and \'str\'\n']";"['    df[df[\'Behavior\'].str.contains(""nt"", na=False)]\n    df[df[\'Behavior\'].str.contains(""nv"", na=False)]\n', '    ---------------------------------------------------------------------------\n    TypeError                                 Traceback (most recent call last)\n    <ipython-input-113-1d11e906812c> in <module>()\n    3 \n    4 \n    ----> 5 soctol = f_recs[f_recs[\'Behavior\'].str.contains(""nt""|""nv"", na=False)]\n    6 soctol\n\n    TypeError: unsupported operand type(s) for |: \'str\' and \'str\'\n']";"['    df[df[\'Behavior\'].str.contains(""nt"", na=False)]\n    df[df[\'Behavior\'].str.contains(""nv"", na=False)]\n', '    ---------------------------------------------------------------------------\n    TypeError                                 Traceback (most recent call last)\n    <ipython-input-113-1d11e906812c> in <module>()\n    3 \n    4 \n    ----> 5 soctol = f_recs[f_recs[\'Behavior\'].str.contains(""nt""|""nv"", na=False)]\n    6 soctol\n\n    TypeError: unsupported operand type(s) for |: \'str\' and \'str\'\n']";['\n'];['\n'];False;['import pandas as pd\n\n'];False;0;1;"[""name 'f_recs' is not defined""]";['NameError'];0;1;"[""name 'f_recs' is not defined""]";['NameError'];0;1;"[""name 'f_recs' is not defined""]";['NameError']
508;508;508;508;2.0;0;19213789;;1;44;<python><matplotlib><plot><pandas>;How do you plot a vertical line on a time series plot in Pandas?;39922.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'plt' is not defined"", ""name 'ax' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'plt' is not defined"", ""name 'ax' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'plt' is not defined"", ""name 'ax' is not defined""]";['NameError', 'NameError']
509;509;509;509;1.0;0;19214588;;1;11;<javascript><python><json><d3.js><pandas>;How can I efficiently move from a Pandas dataframe to JSON;11979.0;"['queryset = [{\'created\':""05-16-13"", \'counter\':1, \'id\':13}, {\'created\':""05-16-13"", \'counter\':1, \'id\':34}, {\'created\':""05-17-13"", \'counter\':1, \'id\':12}, {\'created\':""05-16-13"", \'counter\':1, \'id\':7}, {\'created\':""05-18-13"", \'counter\':1, \'id\':6}]\nimport pandas as pd\nqueryset_df = pd.DataFrame.from_records(queryset).set_index(\'id\')\naggregated_df = queryset_df.groupby(\'created\').sum()\n          counter\ncreated          \n05-16-13        3\n05-17-13        1\n05-18-13        1\naggregated_df.to_json()\n{""counter"":{""05-16-13"":3,""05-17-13"":1,""05-18-13"":1}}\ndata = {""c1"":{""date"":""05-16-13"", ""counter"":3},""c2"":{""date"":""05-17-13"", ""counter"":1}, ""c3"":{""date"":""05-18-13"", ""counter"":1}}\n  x.domain(d3.extent(data, function(d) { return d.date; }));\n  y.domain(d3.extent(data, function(d) { return d.counter; }));\n']";"['queryset = [{\'created\':""05-16-13"", \'counter\':1, \'id\':13}, {\'created\':""05-16-13"", \'counter\':1, \'id\':34}, {\'created\':""05-17-13"", \'counter\':1, \'id\':12}, {\'created\':""05-16-13"", \'counter\':1, \'id\':7}, {\'created\':""05-18-13"", \'counter\':1, \'id\':6}]\n', ""import pandas as pd\nqueryset_df = pd.DataFrame.from_records(queryset).set_index('id')\naggregated_df = queryset_df.groupby('created').sum()\n"", '          counter\ncreated          \n05-16-13        3\n05-17-13        1\n05-18-13        1\n', 'aggregated_df.to_json()\n', '{""counter"":{""05-16-13"":3,""05-17-13"":1,""05-18-13"":1}}\n', 'data = {""c1"":{""date"":""05-16-13"", ""counter"":3},""c2"":{""date"":""05-17-13"", ""counter"":1}, ""c3"":{""date"":""05-18-13"", ""counter"":1}}\n', '  x.domain(d3.extent(data, function(d) { return d.date; }));\n  y.domain(d3.extent(data, function(d) { return d.counter; }));\n']";"['pandas', 'D3', 'Django', 'queryset = [{\'created\':""05-16-13"", \'counter\':1, \'id\':13}, {\'created\':""05-16-13"", \'counter\':1, \'id\':34}, {\'created\':""05-17-13"", \'counter\':1, \'id\':12}, {\'created\':""05-16-13"", \'counter\':1, \'id\':7}, {\'created\':""05-18-13"", \'counter\':1, \'id\':6}]\n', 'pandas', ""import pandas as pd\nqueryset_df = pd.DataFrame.from_records(queryset).set_index('id')\naggregated_df = queryset_df.groupby('created').sum()\n"", '          counter\ncreated          \n05-16-13        3\n05-17-13        1\n05-18-13        1\n', 'D3', 'JSON', 'Pandas', 'to_json()', 'aggregated_df.to_json()\n', 'JSON', '{""counter"":{""05-16-13"":3,""05-17-13"":1,""05-18-13"":1}}\n', 'data = {""c1"":{""date"":""05-16-13"", ""counter"":3},""c2"":{""date"":""05-17-13"", ""counter"":1}, ""c3"":{""date"":""05-18-13"", ""counter"":1}}\n', 'Python', 'JS', '  x.domain(d3.extent(data, function(d) { return d.date; }));\n  y.domain(d3.extent(data, function(d) { return d.counter; }));\n', 'D3', 'pandas', 'python', 'D3']";"['queryset = [{\'created\':""05-16-13"", \'counter\':1, \'id\':13}, {\'created\':""05-16-13"", \'counter\':1, \'id\':34}, {\'created\':""05-17-13"", \'counter\':1, \'id\':12}, {\'created\':""05-16-13"", \'counter\':1, \'id\':7}, {\'created\':""05-18-13"", \'counter\':1, \'id\':6}]\nimport pandas as pd\nqueryset_df = pd.DataFrame.from_records(queryset).set_index(\'id\')\naggregated_df = queryset_df.groupby(\'created\').sum()\ncreated          \naggregated_df.to_json()\n{""counter"":{""05-16-13"":3,""05-17-13"":1,""05-18-13"":1}}\ndata = {""c1"":{""date"":""05-16-13"", ""counter"":3},""c2"":{""date"":""05-17-13"", ""counter"":1}, ""c3"":{""date"":""05-18-13"", ""counter"":1}}\n']";"['queryset = [{\'created\':""05-16-13"", \'counter\':1, \'id\':13}, {\'created\':""05-16-13"", \'counter\':1, \'id\':34}, {\'created\':""05-17-13"", \'counter\':1, \'id\':12}, {\'created\':""05-16-13"", \'counter\':1, \'id\':7}, {\'created\':""05-18-13"", \'counter\':1, \'id\':6}]\nimport pandas as pd\nqueryset_df = pd.DataFrame.from_records(queryset).set_index(\'id\')\naggregated_df = queryset_df.groupby(\'created\').sum()\ncreated          \naggregated_df.to_json()\n{""counter"":{""05-16-13"":3,""05-17-13"":1,""05-18-13"":1}}\ndata = {""c1"":{""date"":""05-16-13"", ""counter"":3},""c2"":{""date"":""05-17-13"", ""counter"":1}, ""c3"":{""date"":""05-18-13"", ""counter"":1}}\n']";False;"['import pandas as pd\nqueryset = [{\'created\':""05-16-13"", \'counter\':1, \'id\':13}, {\'created\':""05-16-13"", \'counter\':1, \'id\':34}, {\'created\':""05-17-13"", \'counter\':1, \'id\':12}, {\'created\':""05-16-13"", \'counter\':1, \'id\':7}, {\'created\':""05-18-13"", \'counter\':1, \'id\':6}]\nimport pandas as pd\nqueryset_df = pd.DataFrame.from_records(queryset).set_index(\'id\')\naggregated_df = queryset_df.groupby(\'created\').sum()\ncreated          \naggregated_df.to_json()\n{""counter"":{""05-16-13"":3,""05-17-13"":1,""05-18-13"":1}}\ndata = {""c1"":{""date"":""05-16-13"", ""counter"":3},""c2"":{""date"":""05-17-13"", ""counter"":1}, ""c3"":{""date"":""05-18-13"", ""counter"":1}}\n']";False;0;1;"[""name 'aggregated_df' is not defined""]";['NameError'];0;1;"[""name 'aggregated_df' is not defined""]";['NameError'];0;1;"[""name 'aggregated_df' is not defined""]";['NameError']
510;510;510;510;3.0;0;19226488;;1;15;<python><pandas>;Python PANDAS, change one value based on another value;25399.0;"['replace FirstName = ""Matt"" if ID==103\nreplace LastName =  ""Jones"" if ID==103\ndf = read_csv(""test.csv"")\nfor i in df[\'ID\']:\n    if i ==103:\n          ...\n']";"['replace FirstName = ""Matt"" if ID==103\nreplace LastName =  ""Jones"" if ID==103\n', 'df = read_csv(""test.csv"")\nfor i in df[\'ID\']:\n    if i ==103:\n          ...\n']";"['replace FirstName = ""Matt"" if ID==103\nreplace LastName =  ""Jones"" if ID==103\n', 'df = read_csv(""test.csv"")\nfor i in df[\'ID\']:\n    if i ==103:\n          ...\n']";"['df = read_csv(""test.csv"")\n']";"['df = read_csv(""test.csv"")\n']";False;"['import pandas as pd\ndf = read_csv(""test.csv"")\n']";False;0;2;"[""File b'test.csv' does not exist"", ""name 'df' is not defined""]";['FileNotFoundError', 'NameError'];0;2;"[""File b'test.csv' does not exist"", ""name 'df' is not defined""]";['FileNotFoundError', 'NameError'];0;2;"[""File b'test.csv' does not exist"", ""'ID'""]";['FileNotFoundError', 'KeyError']
511;511;511;511;2.0;0;19231871;;1;30;<python><pandas><unix-timestamp><dataframe>;Convert unix time to readable date in pandas DataFrame;24317.0;"['import json\nimport urllib2\nfrom datetime import datetime\nresponse = urllib2.urlopen(\'http://blockchain.info/charts/market-price?&format=json\')\ndata = json.load(response)   \ndf = DataFrame(data[\'values\'])\ndf.columns = [""date"",""price""]\n#convert dates \ndf.date = df.date.apply(lambda d: datetime.strptime(d, ""%Y-%m-%d""))\ndf.index = df.date   \ndf\n']";"['import json\nimport urllib2\nfrom datetime import datetime\nresponse = urllib2.urlopen(\'http://blockchain.info/charts/market-price?&format=json\')\ndata = json.load(response)   \ndf = DataFrame(data[\'values\'])\ndf.columns = [""date"",""price""]\n#convert dates \ndf.date = df.date.apply(lambda d: datetime.strptime(d, ""%Y-%m-%d""))\ndf.index = df.date   \ndf\n']";"['import json\nimport urllib2\nfrom datetime import datetime\nresponse = urllib2.urlopen(\'http://blockchain.info/charts/market-price?&format=json\')\ndata = json.load(response)   \ndf = DataFrame(data[\'values\'])\ndf.columns = [""date"",""price""]\n#convert dates \ndf.date = df.date.apply(lambda d: datetime.strptime(d, ""%Y-%m-%d""))\ndf.index = df.date   \ndf\n', 'df.date = df.date.apply(lambda d: datetime.strptime(d, ""%Y-%m-%d""))', 'datetime.date.fromtimestamp']";"['import json\nimport urllib2\nfrom datetime import datetime\nresponse = urllib2.urlopen(\'http://blockchain.info/charts/market-price?&format=json\')\ndata = json.load(response)   \ndf = DataFrame(data[\'values\'])\ndf.columns = [""date"",""price""]\n#convert dates \ndf.date = df.date.apply(lambda d: datetime.strptime(d, ""%Y-%m-%d""))\ndf.index = df.date   \ndf\n']";"['from pandas import DataFrame\nimport json\nimport urllib2\nfrom datetime import datetime\nresponse = urllib2.urlopen(\'http://blockchain.info/charts/market-price?&format=json\')\ndata = json.load(response)   \ndf = DataFrame(data[\'values\'])\ndf.columns = [""date"",""price""]\n#convert dates \ndf.date = df.date.apply(lambda d: datetime.strptime(d, ""%Y-%m-%d""))\ndf.index = df.date   \ndf\n']";True;"['import pandas as pd\nimport json\nimport urllib2\nfrom datetime import datetime\nresponse = urllib2.urlopen(\'http://blockchain.info/charts/market-price?&format=json\')\ndata = json.load(response)   \ndf = DataFrame(data[\'values\'])\ndf.columns = [""date"",""price""]\n#convert dates \ndf.date = df.date.apply(lambda d: datetime.strptime(d, ""%Y-%m-%d""))\ndf.index = df.date   \ndf\n']";False;0;1;"[""name 'DataFrame' is not defined""]";['NameError'];0;1;"[""name 'data' is not defined""]";['NameError'];0;1;"[""name 'DataFrame' is not defined""]";['NameError']
512;512;512;512;3.0;0;19237878;;1;30;<pandas><subset>;subsetting a Python DataFrame;71327.0;"['k1 <- subset(data, Product = p.id & Month < mn & Year == yr, select = c(Time, Product))\nimport pandas as pd\ndata = pd.read_csv(""../data/monthly_prod_sales.csv"")\n\n\n#first, index the dataset by Product. And, get all that matches a given \'p.id\' and time.\n data.set_index(\'Product\')\n k = data.ix[[p.id, \'Time\']]\n\n# then, index this subset with Time and do more subsetting..\nk1 <- subset(data, Product = p.id & Time >= start_time & Time < end_time, select = c(Time, Product))\n']";"['k1 <- subset(data, Product = p.id & Month < mn & Year == yr, select = c(Time, Product))\n', 'import pandas as pd\ndata = pd.read_csv(""../data/monthly_prod_sales.csv"")\n\n\n#first, index the dataset by Product. And, get all that matches a given \'p.id\' and time.\n data.set_index(\'Product\')\n k = data.ix[[p.id, \'Time\']]\n\n# then, index this subset with Time and do more subsetting..\n', 'k1 <- subset(data, Product = p.id & Time >= start_time & Time < end_time, select = c(Time, Product))\n']";"['k1 <- subset(data, Product = p.id & Month < mn & Year == yr, select = c(Time, Product))\n', 'import pandas as pd\ndata = pd.read_csv(""../data/monthly_prod_sales.csv"")\n\n\n#first, index the dataset by Product. And, get all that matches a given \'p.id\' and time.\n data.set_index(\'Product\')\n k = data.ix[[p.id, \'Time\']]\n\n# then, index this subset with Time and do more subsetting..\n', 'k1 <- subset(data, Product = p.id & Time >= start_time & Time < end_time, select = c(Time, Product))\n']";"['k1 <- subset(data, Product = p.id & Month < mn & Year == yr, select = c(Time, Product))\nimport pandas as pd\ndata = pd.read_csv(""../data/monthly_prod_sales.csv"")\n\n\n#first, index the dataset by Product. And, get all that matches a given \'p.id\' and time.\n\n# then, index this subset with Time and do more subsetting..\nk1 <- subset(data, Product = p.id & Time >= start_time & Time < end_time, select = c(Time, Product))\n']";"['k1 <- subset(data, Product = p.id & Month < mn & Year == yr, select = c(Time, Product))\nimport pandas as pd\ndata = pd.read_csv(""../data/monthly_prod_sales.csv"")\n\n\n#first, index the dataset by Product. And, get all that matches a given \'p.id\' and time.\n\n# then, index this subset with Time and do more subsetting..\nk1 <- subset(data, Product = p.id & Time >= start_time & Time < end_time, select = c(Time, Product))\n']";False;"['import pandas as pd\nk1 <- subset(data, Product = p.id & Month < mn & Year == yr, select = c(Time, Product))\nimport pandas as pd\ndata = pd.read_csv(""../data/monthly_prod_sales.csv"")\n\n\n#first, index the dataset by Product. And, get all that matches a given \'p.id\' and time.\n\n# then, index this subset with Time and do more subsetting..\nk1 <- subset(data, Product = p.id & Time >= start_time & Time < end_time, select = c(Time, Product))\n']";False;0;2;"[""name 'DataFrame' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'DataFrame' is not defined"", ""'DataFrame' object has no attribute 'Product'""]";['NameError', 'AttributeError']
513;513;513;513;1.0;1;19267029;;1;11;<python><pandas>;Why Pandas Transform fails if you only have a single column;7646.0;"['import pandas as pd\n\ndf = pd.DataFrame({\'a\':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})\ndf[\'num_totals\'] = df.groupby(\'a\').transform(\'count\')\n\ngives ValueError:\n\nValueError                                Traceback (most recent call last)\n<ipython-input-38-157c6339ad93> in <module>()\n      3 #df = pd.DataFrame({\'a\':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4], \'b\':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})\n      4 df = pd.DataFrame({\'a\':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})\n----> 5 df[\'num_totals\'] = df.groupby(\'a\').transform(\'count\')\n      6 \n      7 #df[\'num_totals\']=df.groupby(\'a\')[[\'a\']].transform(\'count\')\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\frame.pyc in __setitem__(self, key, value)\n   2117         else:\n   2118             # set column\n-> 2119             self._set_item(key, value)\n   2120 \n   2121     def _setitem_slice(self, key, value):\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\frame.pyc in _set_item(self, key, value)\n   2164         """"""\n   2165         value = self._sanitize_column(key, value)\n-> 2166         NDFrame._set_item(self, key, value)\n   2167 \n   2168     def insert(self, loc, column, value, allow_duplicates=False):\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\generic.pyc in _set_item(self, key, value)\n    677 \n    678     def _set_item(self, key, value):\n--> 679         self._data.set(key, value)\n    680         self._clear_item_cache()\n    681 \n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\internals.pyc in set(self, item, value)\n   1779         except KeyError:\n   1780             # insert at end\n-> 1781             self.insert(len(self.items), item, value)\n   1782 \n   1783         self._known_consolidated = False\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\internals.pyc in insert(self, loc, item, value, allow_duplicates)\n   1793 \n   1794             # new block\n-> 1795             self._add_new_block(item, value, loc=loc)\n   1796 \n   1797         except:\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\internals.pyc in _add_new_block(self, item, value, loc)\n   1909             loc = self.items.get_loc(item)\n   1910         new_block = make_block(value, self.items[loc:loc + 1].copy(),\n-> 1911                                self.items, fastpath=True)\n   1912         self.blocks.append(new_block)\n   1913 \n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\internals.pyc in make_block(values, items, ref_items, klass, fastpath, placement)\n    964             klass = ObjectBlock\n    965 \n--> 966     return klass(values, items, ref_items, ndim=values.ndim, fastpath=fastpath, placement=placement)\n    967 \n    968 # TODO: flexible with index=None and/or items=None\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\internals.pyc in __init__(self, values, items, ref_items, ndim, fastpath, placement)\n     42         if len(items) != len(values):\n     43             raise ValueError(\'Wrong number of items passed %d, indices imply %d\'\n---> 44                              % (len(items), len(values)))\n     45 \n     46         self.set_ref_locs(placement)\n\nValueError: Wrong number of items passed 1, indices imply 0\ndf = pd.DataFrame({\'a\':1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4],\'b\':1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})\ndf[\'num_totals\'] = df.groupby(\'a\').transform(\'count\')\ndf\n\n\n\nOut[40]:\n    a  b  num_totals\n0   1  1           4\n1   1  1           4\n2   1  1           4\n3   1  1           4\n4   2  2           2\n5   2  2           2\n6   3  3           3\n7   3  3           3\n8   3  3           3\n9   4  4           7\n10  4  4           7\n11  4  4           7\n12  4  4           7\n13  4  4           7\n14  4  4           7\n15  4  4           7\ndf[\'num_totals\']=df.groupby(\'a\')[[\'a\']].transform(\'count\')\n']";"['import pandas as pd\n\ndf = pd.DataFrame({\'a\':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})\ndf[\'num_totals\'] = df.groupby(\'a\').transform(\'count\')\n\ngives ValueError:\n\nValueError                                Traceback (most recent call last)\n<ipython-input-38-157c6339ad93> in <module>()\n      3 #df = pd.DataFrame({\'a\':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4], \'b\':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})\n      4 df = pd.DataFrame({\'a\':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})\n----> 5 df[\'num_totals\'] = df.groupby(\'a\').transform(\'count\')\n      6 \n      7 #df[\'num_totals\']=df.groupby(\'a\')[[\'a\']].transform(\'count\')\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\frame.pyc in __setitem__(self, key, value)\n   2117         else:\n   2118             # set column\n-> 2119             self._set_item(key, value)\n   2120 \n   2121     def _setitem_slice(self, key, value):\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\frame.pyc in _set_item(self, key, value)\n   2164         """"""\n   2165         value = self._sanitize_column(key, value)\n-> 2166         NDFrame._set_item(self, key, value)\n   2167 \n   2168     def insert(self, loc, column, value, allow_duplicates=False):\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\generic.pyc in _set_item(self, key, value)\n    677 \n    678     def _set_item(self, key, value):\n--> 679         self._data.set(key, value)\n    680         self._clear_item_cache()\n    681 \n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\internals.pyc in set(self, item, value)\n   1779         except KeyError:\n   1780             # insert at end\n-> 1781             self.insert(len(self.items), item, value)\n   1782 \n   1783         self._known_consolidated = False\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\internals.pyc in insert(self, loc, item, value, allow_duplicates)\n   1793 \n   1794             # new block\n-> 1795             self._add_new_block(item, value, loc=loc)\n   1796 \n   1797         except:\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\internals.pyc in _add_new_block(self, item, value, loc)\n   1909             loc = self.items.get_loc(item)\n   1910         new_block = make_block(value, self.items[loc:loc + 1].copy(),\n-> 1911                                self.items, fastpath=True)\n   1912         self.blocks.append(new_block)\n   1913 \n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\internals.pyc in make_block(values, items, ref_items, klass, fastpath, placement)\n    964             klass = ObjectBlock\n    965 \n--> 966     return klass(values, items, ref_items, ndim=values.ndim, fastpath=fastpath, placement=placement)\n    967 \n    968 # TODO: flexible with index=None and/or items=None\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\internals.pyc in __init__(self, values, items, ref_items, ndim, fastpath, placement)\n     42         if len(items) != len(values):\n     43             raise ValueError(\'Wrong number of items passed %d, indices imply %d\'\n---> 44                              % (len(items), len(values)))\n     45 \n     46         self.set_ref_locs(placement)\n\nValueError: Wrong number of items passed 1, indices imply 0\n', ""df = pd.DataFrame({'a':1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4],'b':1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})\ndf['num_totals'] = df.groupby('a').transform('count')\ndf\n\n\n\nOut[40]:\n    a  b  num_totals\n0   1  1           4\n1   1  1           4\n2   1  1           4\n3   1  1           4\n4   2  2           2\n5   2  2           2\n6   3  3           3\n7   3  3           3\n8   3  3           3\n9   4  4           7\n10  4  4           7\n11  4  4           7\n12  4  4           7\n13  4  4           7\n14  4  4           7\n15  4  4           7\n"", ""df['num_totals']=df.groupby('a')[['a']].transform('count')\n""]";"['import pandas as pd\n\ndf = pd.DataFrame({\'a\':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})\ndf[\'num_totals\'] = df.groupby(\'a\').transform(\'count\')\n\ngives ValueError:\n\nValueError                                Traceback (most recent call last)\n<ipython-input-38-157c6339ad93> in <module>()\n      3 #df = pd.DataFrame({\'a\':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4], \'b\':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})\n      4 df = pd.DataFrame({\'a\':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})\n----> 5 df[\'num_totals\'] = df.groupby(\'a\').transform(\'count\')\n      6 \n      7 #df[\'num_totals\']=df.groupby(\'a\')[[\'a\']].transform(\'count\')\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\frame.pyc in __setitem__(self, key, value)\n   2117         else:\n   2118             # set column\n-> 2119             self._set_item(key, value)\n   2120 \n   2121     def _setitem_slice(self, key, value):\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\frame.pyc in _set_item(self, key, value)\n   2164         """"""\n   2165         value = self._sanitize_column(key, value)\n-> 2166         NDFrame._set_item(self, key, value)\n   2167 \n   2168     def insert(self, loc, column, value, allow_duplicates=False):\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\generic.pyc in _set_item(self, key, value)\n    677 \n    678     def _set_item(self, key, value):\n--> 679         self._data.set(key, value)\n    680         self._clear_item_cache()\n    681 \n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\internals.pyc in set(self, item, value)\n   1779         except KeyError:\n   1780             # insert at end\n-> 1781             self.insert(len(self.items), item, value)\n   1782 \n   1783         self._known_consolidated = False\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\internals.pyc in insert(self, loc, item, value, allow_duplicates)\n   1793 \n   1794             # new block\n-> 1795             self._add_new_block(item, value, loc=loc)\n   1796 \n   1797         except:\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\internals.pyc in _add_new_block(self, item, value, loc)\n   1909             loc = self.items.get_loc(item)\n   1910         new_block = make_block(value, self.items[loc:loc + 1].copy(),\n-> 1911                                self.items, fastpath=True)\n   1912         self.blocks.append(new_block)\n   1913 \n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\internals.pyc in make_block(values, items, ref_items, klass, fastpath, placement)\n    964             klass = ObjectBlock\n    965 \n--> 966     return klass(values, items, ref_items, ndim=values.ndim, fastpath=fastpath, placement=placement)\n    967 \n    968 # TODO: flexible with index=None and/or items=None\n\nC:\\WinPython-64bit-2.7.5.3\\python-2.7.5.amd64\\lib\\site-packages\\pandas\\core\\internals.pyc in __init__(self, values, items, ref_items, ndim, fastpath, placement)\n     42         if len(items) != len(values):\n     43             raise ValueError(\'Wrong number of items passed %d, indices imply %d\'\n---> 44                              % (len(items), len(values)))\n     45 \n     46         self.set_ref_locs(placement)\n\nValueError: Wrong number of items passed 1, indices imply 0\n', ""df = pd.DataFrame({'a':1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4],'b':1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})\ndf['num_totals'] = df.groupby('a').transform('count')\ndf\n\n\n\nOut[40]:\n    a  b  num_totals\n0   1  1           4\n1   1  1           4\n2   1  1           4\n3   1  1           4\n4   2  2           2\n5   2  2           2\n6   3  3           3\n7   3  3           3\n8   3  3           3\n9   4  4           7\n10  4  4           7\n11  4  4           7\n12  4  4           7\n13  4  4           7\n14  4  4           7\n15  4  4           7\n"", ""df['num_totals']=df.groupby('a')[['a']].transform('count')\n""]";"[""import pandas as pd\n\ndf = pd.DataFrame({'a':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})\ndf['num_totals'] = df.groupby('a').transform('count')\n\n\n\n\n\n\n\n\n\n\n\ndf['num_totals'] = df.groupby('a').transform('count')\ndf\n\n\n\ndf['num_totals']=df.groupby('a')[['a']].transform('count')\n""]";"[""import pandas as pd\n\ndf = pd.DataFrame({'a':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})\ndf['num_totals'] = df.groupby('a').transform('count')\n\n\n\n\n\n\n\n\n\n\n\ndf['num_totals'] = df.groupby('a').transform('count')\ndf\n\n\n\ndf['num_totals']=df.groupby('a')[['a']].transform('count')\n""]";False;"[""import pandas as pd\nimport pandas as pd\n\ndf = pd.DataFrame({'a':[1,1,1,1,2,2,3,3,3,4,4,4,4,4,4,4]})\ndf['num_totals'] = df.groupby('a').transform('count')\n\n\n\n\n\n\n\n\n\n\n\ndf['num_totals'] = df.groupby('a').transform('count')\ndf\n\n\n\ndf['num_totals']=df.groupby('a')[['a']].transform('count')\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'a'""]";['KeyError']
514;514;514;514;5.0;0;19324453;;1;32;<python><date><plot><pandas><dataframe>;Add missing dates to pandas dataframe;17676.0;"[""idx = pd.date_range(df['simpleDate'].min(), df['simpleDate'].max())\ns = df.groupby(['simpleDate']).size()\n09-02-2013     2\n09-03-2013    10\n09-06-2013     5\n09-07-2013     1\n""]";"[""idx = pd.date_range(df['simpleDate'].min(), df['simpleDate'].max())\ns = df.groupby(['simpleDate']).size()\n"", '09-02-2013     2\n09-03-2013    10\n09-06-2013     5\n09-07-2013     1\n']";"[""idx = pd.date_range(df['simpleDate'].min(), df['simpleDate'].max())\ns = df.groupby(['simpleDate']).size()\n"", 'reindex', ""df.groupby(['simpleDate']).size()"", '09-02-2013     2\n09-03-2013    10\n09-06-2013     5\n09-07-2013     1\n']";"[""idx = pd.date_range(df['simpleDate'].min(), df['simpleDate'].max())\ns = df.groupby(['simpleDate']).size()\n""]";"[""import pandas as pd\nidx = pd.date_range(df['simpleDate'].min(), df['simpleDate'].max())\ns = df.groupby(['simpleDate']).size()\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\nidx = pd.date_range(df['simpleDate'].min(), df['simpleDate'].max())\ns = df.groupby(['simpleDate']).size()\n""]";True;0;1;"[""name 's' is not defined""]";['NameError'];0;1;"[""name 's' is not defined""]";['NameError'];0;1;"[""name 's' is not defined""]";['NameError']
515;515;515;515;3.0;3;19332171;;1;16;<python><pandas>;Difference between sort_values and sort_index;6654.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
516;516;516;516;7.0;1;19350806;;1;19;<datetime><python-2.7><pandas>;How to convert columns into one datetime column in pandas?;14607.0;['M    D    Y    Apples   Oranges\n5    6  1990      12        3\n5    7  1990      14        4\n5    8  1990      15       34\n5    9  1990      23       21\nDatetimes    Apples   Oranges\n1990-6-5        12        3\n1990-7-5        14        4\n1990-8-5        15       34\n1990-9-5        23       21\n'];['M    D    Y    Apples   Oranges\n5    6  1990      12        3\n5    7  1990      14        4\n5    8  1990      15       34\n5    9  1990      23       21\n', 'Datetimes    Apples   Oranges\n1990-6-5        12        3\n1990-7-5        14        4\n1990-8-5        15       34\n1990-9-5        23       21\n'];['M    D    Y    Apples   Oranges\n5    6  1990      12        3\n5    7  1990      14        4\n5    8  1990      15       34\n5    9  1990      23       21\n', 'Datetimes    Apples   Oranges\n1990-6-5        12        3\n1990-7-5        14        4\n1990-8-5        15       34\n1990-9-5        23       21\n'];[''];[''];False;['import pandas as pd\n'];False;1;3;"[""name 'df' is not defined"", ""name 'pd' is not defined"", 'Sucess']";['NameError', 'NameError', 'Sucess'];1;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess']";['NameError', 'NameError', 'Sucess'];1;3;"[""'DataFrame' object has no attribute 'Y'"", ""name 'df' is not defined"", 'Sucess']";['AttributeError', 'NameError', 'Sucess']
517;517;517;517;3.0;0;19365513;;1;22;<python><pandas>;How to add an extra row to a pandas dataframe;80190.0;"[""columns = ['Date', 'Name', 'Action','ID']\ndf = pd.DataFrame(columns=columns) \n""]";"[""columns = ['Date', 'Name', 'Action','ID']\ndf = pd.DataFrame(columns=columns) \n""]";"[""columns = ['Date', 'Name', 'Action','ID']\ndf = pd.DataFrame(columns=columns) \n""]";"[""columns = ['Date', 'Name', 'Action','ID']\ndf = pd.DataFrame(columns=columns) \n""]";"[""import pandas as pd\ncolumns = ['Date', 'Name', 'Action','ID']\ndf = pd.DataFrame(columns=columns) \n""]";True;"[""import pandas as pd\ncolumns = ['Date', 'Name', 'Action','ID']\ndf = pd.DataFrame(columns=columns) \n""]";False;1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;['Sucess', 'cannot set a frame with no defined columns'];['Sucess', 'ValueError']
518;518;518;518;8.0;1;19377969;;1;103;<python><pandas><dataframe>;Combine two columns of text in dataframe in pandas/python;112188.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;4;7;"[""name 'dataframe' is not defined"", ""name 'pd' is not defined"", 'Sucess', 'Sucess', 'Sucess', ""name 'df' is not defined"", 'Sucess']";['NameError', 'NameError', 'Sucess', 'Sucess', 'Sucess', 'NameError', 'Sucess'];5;7;"[""name 'dataframe' is not defined"", 'Sucess', 'Sucess', 'Sucess', 'Sucess', ""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'NameError', 'Sucess'];5;7;"[""name 'dataframe' is not defined"", 'Sucess', 'Sucess', 'Sucess', 'Sucess', ""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess', 'Sucess', 'Sucess', 'Sucess', 'NameError', 'Sucess']
519;519;519;519;2.0;0;19384532;;1;93;<python><group-by><pandas><distinct>;How to count number of rows in a group in pandas group by object?;144711.0;"[""df['col1','col2','col3','col4'].groupby(['col1','col2']).mean()\n""]";"[""df['col1','col2','col3','col4'].groupby(['col1','col2']).mean()\n""]";"['df', 'groupby', ""df['col1','col2','col3','col4'].groupby(['col1','col2']).mean()\n""]";"[""df['col1','col2','col3','col4'].groupby(['col1','col2']).mean()\n""]";"[""df['col1','col2','col3','col4'].groupby(['col1','col2']).mean()\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf['col1','col2','col3','col4'].groupby(['col1','col2']).mean()\n""]";True;0;2;"[""name 'df' is not defined"", 'invalid syntax (<unknown>, line 11)']";['NameError', 'SyntaxError'];0;2;"[""name 'df' is not defined"", 'invalid syntax (<unknown>, line 11)']";['NameError', 'SyntaxError'];0;2;"['""[\'col1\' \'col2\' \'col3\' \'col4\'] not in index""', 'invalid syntax (<unknown>, line 12)']";['KeyError', 'SyntaxError']
520;520;520;520;6.0;1;19387868;;1;17;<python><pandas><finance><bloomberg><blpapi>;How do I store data from the Bloomberg API into a Pandas dataframe?;23455.0;"['# SimpleHistoryExample.py\n\nimport blpapi\nfrom optparse import OptionParser\n\n\ndef parseCmdLine():\n    parser = OptionParser(description=""Retrieve reference data."")\n    parser.add_option(""-a"",\n                      ""--ip"",\n                      dest=""host"",\n                      help=""server name or IP (default: %default)"",\n                      metavar=""ipAddress"",\n                      default=""localhost"")\n    parser.add_option(""-p"",\n                      dest=""port"",\n                      type=""int"",\n                      help=""server port (default: %default)"",\n                      metavar=""tcpPort"",\n                      default=8194)\n\n    (options, args) = parser.parse_args()\n\n    return options\n\n\ndef main():\n    options = parseCmdLine()\n\n    # Fill SessionOptions\n    sessionOptions = blpapi.SessionOptions()\n    sessionOptions.setServerHost(options.host)\n    sessionOptions.setServerPort(options.port)\n\n    print ""Connecting to %s:%s"" % (options.host, options.port)\n    # Create a Session\n    session = blpapi.Session(sessionOptions)\n\n    # Start a Session\n    if not session.start():\n        print ""Failed to start session.""\n        return\n\n    try:\n        # Open service to get historical data from\n        if not session.openService(""//blp/refdata""):\n            print ""Failed to open //blp/refdata""\n            return\n\n        # Obtain previously opened service\n        refDataService = session.getService(""//blp/refdata"")\n\n        # Create and fill the request for the historical data\n        request = refDataService.createRequest(""HistoricalDataRequest"")\n        request.getElement(""securities"").appendValue(""IBM US Equity"")\n        request.getElement(""securities"").appendValue(""MSFT US Equity"")\n        request.getElement(""fields"").appendValue(""PX_LAST"")\n        request.getElement(""fields"").appendValue(""OPEN"")\n        request.set(""periodicityAdjustment"", ""ACTUAL"")\n        request.set(""periodicitySelection"", ""DAILY"")\n        request.set(""startDate"", ""20061227"")\n        request.set(""endDate"", ""20061231"")\n        request.set(""maxDataPoints"", 100)\n\n        print ""Sending Request:"", request\n        # Send the request\n        session.sendRequest(request)\n\n        # Process received events\n        while(True):\n            # We provide timeout to give the chance for Ctrl+C handling:\n            ev = session.nextEvent(500)\n            for msg in ev:\n                print msg\n\n            if ev.eventType() == blpapi.Event.RESPONSE:\n                # Response completly received, so we could exit\n                break\n    finally:\n        # Stop the session\n        session.stop()\n\nif __name__ == ""__main__"":\n    print ""SimpleHistoryExample""\n    try:\n        main()\n    except KeyboardInterrupt:\n        print ""Ctrl+C pressed. Stopping...""\n']";"['# SimpleHistoryExample.py\n\nimport blpapi\nfrom optparse import OptionParser\n\n\ndef parseCmdLine():\n    parser = OptionParser(description=""Retrieve reference data."")\n    parser.add_option(""-a"",\n                      ""--ip"",\n                      dest=""host"",\n                      help=""server name or IP (default: %default)"",\n                      metavar=""ipAddress"",\n                      default=""localhost"")\n    parser.add_option(""-p"",\n                      dest=""port"",\n                      type=""int"",\n                      help=""server port (default: %default)"",\n                      metavar=""tcpPort"",\n                      default=8194)\n\n    (options, args) = parser.parse_args()\n\n    return options\n\n\ndef main():\n    options = parseCmdLine()\n\n    # Fill SessionOptions\n    sessionOptions = blpapi.SessionOptions()\n    sessionOptions.setServerHost(options.host)\n    sessionOptions.setServerPort(options.port)\n\n    print ""Connecting to %s:%s"" % (options.host, options.port)\n    # Create a Session\n    session = blpapi.Session(sessionOptions)\n\n    # Start a Session\n    if not session.start():\n        print ""Failed to start session.""\n        return\n\n    try:\n        # Open service to get historical data from\n        if not session.openService(""//blp/refdata""):\n            print ""Failed to open //blp/refdata""\n            return\n\n        # Obtain previously opened service\n        refDataService = session.getService(""//blp/refdata"")\n\n        # Create and fill the request for the historical data\n        request = refDataService.createRequest(""HistoricalDataRequest"")\n        request.getElement(""securities"").appendValue(""IBM US Equity"")\n        request.getElement(""securities"").appendValue(""MSFT US Equity"")\n        request.getElement(""fields"").appendValue(""PX_LAST"")\n        request.getElement(""fields"").appendValue(""OPEN"")\n        request.set(""periodicityAdjustment"", ""ACTUAL"")\n        request.set(""periodicitySelection"", ""DAILY"")\n        request.set(""startDate"", ""20061227"")\n        request.set(""endDate"", ""20061231"")\n        request.set(""maxDataPoints"", 100)\n\n        print ""Sending Request:"", request\n        # Send the request\n        session.sendRequest(request)\n\n        # Process received events\n        while(True):\n            # We provide timeout to give the chance for Ctrl+C handling:\n            ev = session.nextEvent(500)\n            for msg in ev:\n                print msg\n\n            if ev.eventType() == blpapi.Event.RESPONSE:\n                # Response completly received, so we could exit\n                break\n    finally:\n        # Stop the session\n        session.stop()\n\nif __name__ == ""__main__"":\n    print ""SimpleHistoryExample""\n    try:\n        main()\n    except KeyboardInterrupt:\n        print ""Ctrl+C pressed. Stopping...""\n']";"['# SimpleHistoryExample.py\n\nimport blpapi\nfrom optparse import OptionParser\n\n\ndef parseCmdLine():\n    parser = OptionParser(description=""Retrieve reference data."")\n    parser.add_option(""-a"",\n                      ""--ip"",\n                      dest=""host"",\n                      help=""server name or IP (default: %default)"",\n                      metavar=""ipAddress"",\n                      default=""localhost"")\n    parser.add_option(""-p"",\n                      dest=""port"",\n                      type=""int"",\n                      help=""server port (default: %default)"",\n                      metavar=""tcpPort"",\n                      default=8194)\n\n    (options, args) = parser.parse_args()\n\n    return options\n\n\ndef main():\n    options = parseCmdLine()\n\n    # Fill SessionOptions\n    sessionOptions = blpapi.SessionOptions()\n    sessionOptions.setServerHost(options.host)\n    sessionOptions.setServerPort(options.port)\n\n    print ""Connecting to %s:%s"" % (options.host, options.port)\n    # Create a Session\n    session = blpapi.Session(sessionOptions)\n\n    # Start a Session\n    if not session.start():\n        print ""Failed to start session.""\n        return\n\n    try:\n        # Open service to get historical data from\n        if not session.openService(""//blp/refdata""):\n            print ""Failed to open //blp/refdata""\n            return\n\n        # Obtain previously opened service\n        refDataService = session.getService(""//blp/refdata"")\n\n        # Create and fill the request for the historical data\n        request = refDataService.createRequest(""HistoricalDataRequest"")\n        request.getElement(""securities"").appendValue(""IBM US Equity"")\n        request.getElement(""securities"").appendValue(""MSFT US Equity"")\n        request.getElement(""fields"").appendValue(""PX_LAST"")\n        request.getElement(""fields"").appendValue(""OPEN"")\n        request.set(""periodicityAdjustment"", ""ACTUAL"")\n        request.set(""periodicitySelection"", ""DAILY"")\n        request.set(""startDate"", ""20061227"")\n        request.set(""endDate"", ""20061231"")\n        request.set(""maxDataPoints"", 100)\n\n        print ""Sending Request:"", request\n        # Send the request\n        session.sendRequest(request)\n\n        # Process received events\n        while(True):\n            # We provide timeout to give the chance for Ctrl+C handling:\n            ev = session.nextEvent(500)\n            for msg in ev:\n                print msg\n\n            if ev.eventType() == blpapi.Event.RESPONSE:\n                # Response completly received, so we could exit\n                break\n    finally:\n        # Stop the session\n        session.stop()\n\nif __name__ == ""__main__"":\n    print ""SimpleHistoryExample""\n    try:\n        main()\n    except KeyboardInterrupt:\n        print ""Ctrl+C pressed. Stopping...""\n']";['# SimpleHistoryExample.py\n\nimport blpapi\nfrom optparse import OptionParser\n\n\n\n\n\n\n\n    # Fill SessionOptions\n\n    # Create a Session\n\n    # Start a Session\n\n        # Open service to get historical data from\n\n        # Obtain previously opened service\n\n        # Create and fill the request for the historical data\n\n        # Send the request\n\n        # Process received events\n            # We provide timeout to give the chance for Ctrl+C handling:\n\n                # Response completly received, so we could exit\n        # Stop the session\n\n'];['# SimpleHistoryExample.py\n\nimport blpapi\nfrom optparse import OptionParser\n\n\n\n\n\n\n\n    # Fill SessionOptions\n\n    # Create a Session\n\n    # Start a Session\n\n        # Open service to get historical data from\n\n        # Obtain previously opened service\n\n        # Create and fill the request for the historical data\n\n        # Send the request\n\n        # Process received events\n            # We provide timeout to give the chance for Ctrl+C handling:\n\n                # Response completly received, so we could exit\n        # Stop the session\n\n'];False;['import pandas as pd\ndata = pd.DataFrame()\n# SimpleHistoryExample.py\n\nimport blpapi\nfrom optparse import OptionParser\n\n\n\n\n\n\n\n    # Fill SessionOptions\n\n    # Create a Session\n\n    # Start a Session\n\n        # Open service to get historical data from\n\n        # Obtain previously opened service\n\n        # Create and fill the request for the historical data\n\n        # Send the request\n\n        # Process received events\n            # We provide timeout to give the chance for Ctrl+C handling:\n\n                # Response completly received, so we could exit\n        # Stop the session\n\n'];True;0;1;"[""No module named 'tia'""]";['ImportError'];0;1;"[""No module named 'tia'""]";['ImportError'];0;1;"[""No module named 'tia'""]";['ImportError']
521;521;521;521;1.0;0;19403133;;1;11;<python><group-by><pandas>;Pandas groupby and qcut;3879.0;['     A    B  C\n0  foo  0.1  1\n1  foo  0.5  2\n2  foo  1.0  3\n3  bar  0.1  1\n4  bar  0.5  2\n5  bar  1.0  3\n'];['     A    B  C\n0  foo  0.1  1\n1  foo  0.5  2\n2  foo  1.0  3\n3  bar  0.1  1\n4  bar  0.5  2\n5  bar  1.0  3\n'];['     A    B  C\n0  foo  0.1  1\n1  foo  0.5  2\n2  foo  1.0  3\n3  bar  0.1  1\n4  bar  0.5  2\n5  bar  1.0  3\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
522;522;522;522;4.0;3;19455100;;1;11;<python><numpy><pandas><jython>;Can I run numpy and pandas with Jython;6121.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
523;523;523;523;2.0;0;19463985;;1;17;<python><pandas>;Pandas: Drop consecutive duplicates;3912.0;['In [3]: a = pandas.Series([1,2,2,3,2], index=[1,2,3,4,5])\n\nIn [4]: a.drop_duplicates()\nOut[4]: \n1    1\n2    2\n4    3\ndtype: int64\nIn [4]: a.something()\nOut[4]: \n1    1\n2    2\n4    3\n5    2\ndtype: int64\n'];['In [3]: a = pandas.Series([1,2,2,3,2], index=[1,2,3,4,5])\n\nIn [4]: a.drop_duplicates()\nOut[4]: \n1    1\n2    2\n4    3\ndtype: int64\n', 'In [4]: a.something()\nOut[4]: \n1    1\n2    2\n4    3\n5    2\ndtype: int64\n'];['In [3]: a = pandas.Series([1,2,2,3,2], index=[1,2,3,4,5])\n\nIn [4]: a.drop_duplicates()\nOut[4]: \n1    1\n2    2\n4    3\ndtype: int64\n', 'In [4]: a.something()\nOut[4]: \n1    1\n2    2\n4    3\n5    2\ndtype: int64\n'];['a = pandas.Series([1,2,2,3,2], index=[1,2,3,4,5])\n\na.something()\n'];['a = pandas.Series([1,2,2,3,2], index=[1,2,3,4,5])\n\na.something()\n'];False;['import pandas as pd\na = pandas.Series([1,2,2,3,2], index=[1,2,3,4,5])\n\na.something()\n'];False;0;1;"[""name 'a' is not defined""]";['NameError'];0;1;"[""name 'a' is not defined""]";['NameError'];0;1;"[""name 'a' is not defined""]";['NameError']
524;524;524;524;2.0;0;19472566;;1;13;<python><parsing><pandas>;python read_fwf error: 'dtype is not supported with python-fwf parser';2114.0;[''];[];['data= pd.io.parsers.read_fwf(file, colspecs = ([79,81], [87,90]), header = None, dtype = {0: np.str, 1: np.str})', 'ValueError: dtype is not supported with python-fwf parser'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'data' is not defined""]";['NameError'];0;1;"[""name 'data' is not defined""]";['NameError'];0;1;"[""Invalid file path or buffer object type: <class 'pandas.core.frame.DataFrame'>""]";['ValueError']
525;525;525;525;12.0;1;19482970;;1;304;<python><pandas><dataframe>;Get list from pandas DataFrame column headers;438665.0;['>>> my_dataframe\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n5   4    8    3\n6   8    2    8\n7   9    9   10\n8   6    6    4\n9  10   10    7\n>>> header_list\n[y, gdp, cap]\n'];['>>> my_dataframe\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n5   4    8    3\n6   8    2    8\n7   9    9   10\n8   6    6    4\n9  10   10    7\n', '>>> header_list\n[y, gdp, cap]\n'];['>>> my_dataframe\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n5   4    8    3\n6   8    2    8\n7   9    9   10\n8   6    6    4\n9  10   10    7\n', '>>> header_list\n[y, gdp, cap]\n'];['[y, gdp, cap]\n'];['[y, gdp, cap]\n'];False;['import pandas as pd\n[y, gdp, cap]\n'];False;4;8;"['Sucess', ""name 'my_dataframe' is not defined"", ""name 'my_dataframe' is not defined"", ""name 'my_dataframe' is not defined"", 'Sucess', ""name 'df' is not defined"", 'Sucess', 'Sucess']";['Sucess', 'NameError', 'NameError', 'NameError', 'Sucess', 'NameError', 'Sucess', 'Sucess'];4;8;"['Sucess', ""name 'my_dataframe' is not defined"", ""name 'my_dataframe' is not defined"", ""name 'my_dataframe' is not defined"", 'Sucess', ""name 'df' is not defined"", 'Sucess', 'Sucess']";['Sucess', 'NameError', 'NameError', 'NameError', 'Sucess', 'NameError', 'Sucess', 'Sucess'];5;8;"['Sucess', ""name 'my_dataframe' is not defined"", ""name 'my_dataframe' is not defined"", ""name 'my_dataframe' is not defined"", 'Sucess', 'Sucess', 'Sucess', 'Sucess']";['Sucess', 'NameError', 'NameError', 'NameError', 'Sucess', 'Sucess', 'Sucess', 'Sucess']
526;526;526;526;2.0;0;19523277;;1;18;<python><group-by><pandas><rename>;Renaming Column Names in Pandas Groupby function;24019.0;"["">>> df\n    ID     Region  count\n0  100       Asia      2\n1  101     Europe      3\n2  102         US      1\n3  103     Africa      5\n4  100     Russia      5\n5  101  Australia      7\n6  102         US      8\n7  104       Asia     10\n8  105     Europe     11\n9  110     Africa     23\n>>> print(df.groupby(['ID','Region'],as_index=False).count.sum())\n\n    ID     Region  count\n0  100       Asia      2\n1  100     Russia      5\n2  101  Australia      7\n3  101     Europe      3\n4  102         US      9\n5  103     Africa      5\n6  104       Asia     10\n7  105     Europe     11\n8  110     Africa     23\nselect ID, Region, sum(count) as Total_Numbers\nfrom df\ngroup by ID,Region\norder by ID, Region\n""]";"['>>> df\n    ID     Region  count\n0  100       Asia      2\n1  101     Europe      3\n2  102         US      1\n3  103     Africa      5\n4  100     Russia      5\n5  101  Australia      7\n6  102         US      8\n7  104       Asia     10\n8  105     Europe     11\n9  110     Africa     23\n', "">>> print(df.groupby(['ID','Region'],as_index=False).count.sum())\n\n    ID     Region  count\n0  100       Asia      2\n1  100     Russia      5\n2  101  Australia      7\n3  101     Europe      3\n4  102         US      9\n5  103     Africa      5\n6  104       Asia     10\n7  105     Europe     11\n8  110     Africa     23\n"", 'select ID, Region, sum(count) as Total_Numbers\nfrom df\ngroup by ID,Region\norder by ID, Region\n']";"['>>> df\n    ID     Region  count\n0  100       Asia      2\n1  101     Europe      3\n2  102         US      1\n3  103     Africa      5\n4  100     Russia      5\n5  101  Australia      7\n6  102         US      8\n7  104       Asia     10\n8  105     Europe     11\n9  110     Africa     23\n', "">>> print(df.groupby(['ID','Region'],as_index=False).count.sum())\n\n    ID     Region  count\n0  100       Asia      2\n1  100     Russia      5\n2  101  Australia      7\n3  101     Europe      3\n4  102         US      9\n5  103     Africa      5\n6  104       Asia     10\n7  105     Europe     11\n8  110     Africa     23\n"", 'select ID, Region, sum(count) as Total_Numbers\nfrom df\ngroup by ID,Region\norder by ID, Region\n']";['\n'];['\n'];False;['import pandas as pd\n\n'];False;1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""'A'""]";['Sucess', 'KeyError']
527;527;527;527;5.0;0;19530568;;1;25;<python><pandas>;Can pandas groupby aggregate into a list, rather than sum, mean, etc?;16908.0;['    A    B    C  \n    1    10   22\n    1    12   20\n    1    11   8\n    1    10   10\n    2    11   13\n    2    12   10 \n    3    14   0\n     A    B    C  New1  New2  New3  New4  New5  New6\n    1    10   22  12    20    11    8     10    10\n    2    11   13  12    10 \n    3    14   0\n'];['    A    B    C  \n    1    10   22\n    1    12   20\n    1    11   8\n    1    10   10\n    2    11   13\n    2    12   10 \n    3    14   0\n', '     A    B    C  New1  New2  New3  New4  New5  New6\n    1    10   22  12    20    11    8     10    10\n    2    11   13  12    10 \n    3    14   0\n'];['    A    B    C  \n    1    10   22\n    1    12   20\n    1    11   8\n    1    10   10\n    2    11   13\n    2    12   10 \n    3    14   0\n', '     A    B    C  New1  New2  New3  New4  New5  New6\n    1    10   22  12    20    11    8     10    10\n    2    11   13  12    10 \n    3    14   0\n'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'A' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'A' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'A' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError']
528;528;528;528;3.0;2;19555525;;1;28;<python><matplotlib><pandas>;Saving plots (AxesSubPlot) generated from python pandas with matplotlib's savefig;25060.0;"['dtf = pd.DataFrame.from_records(d,columns=h)\nfig = plt.figure()\nax = dtf2.plot()\nax = fig.add_subplot(ax)\nfig.savefig(\'~/Documents/output.png\')\nTraceback (most recent call last):\n  File ""./testgraph.py"", line 76, in <module>\n    ax = fig.add_subplot(ax)\n  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/figure.py"", line 890, in add_subplot\n    assert(a.get_figure() is self)\nAssertionError\ndtf2.plot().savefig(\'~/Documents/output.png\')\n\n\n  File ""./testgraph.py"", line 79, in <module>\n    dtf2.plot().savefig(\'~/Documents/output.png\')\nAttributeError: \'AxesSubplot\' object has no attribute \'savefig\'\nfig = plt.figure()\ndtf2.plot()\nfig.savefig(\'output.png\')\n']";"[""dtf = pd.DataFrame.from_records(d,columns=h)\nfig = plt.figure()\nax = dtf2.plot()\nax = fig.add_subplot(ax)\nfig.savefig('~/Documents/output.png')\n"", 'Traceback (most recent call last):\n  File ""./testgraph.py"", line 76, in <module>\n    ax = fig.add_subplot(ax)\n  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/figure.py"", line 890, in add_subplot\n    assert(a.get_figure() is self)\nAssertionError\n', 'dtf2.plot().savefig(\'~/Documents/output.png\')\n\n\n  File ""./testgraph.py"", line 79, in <module>\n    dtf2.plot().savefig(\'~/Documents/output.png\')\nAttributeError: \'AxesSubplot\' object has no attribute \'savefig\'\n', ""fig = plt.figure()\ndtf2.plot()\nfig.savefig('output.png')\n""]";"[""dtf = pd.DataFrame.from_records(d,columns=h)\nfig = plt.figure()\nax = dtf2.plot()\nax = fig.add_subplot(ax)\nfig.savefig('~/Documents/output.png')\n"", 'Traceback (most recent call last):\n  File ""./testgraph.py"", line 76, in <module>\n    ax = fig.add_subplot(ax)\n  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/figure.py"", line 890, in add_subplot\n    assert(a.get_figure() is self)\nAssertionError\n', 'dtf2.plot().savefig(\'~/Documents/output.png\')\n\n\n  File ""./testgraph.py"", line 79, in <module>\n    dtf2.plot().savefig(\'~/Documents/output.png\')\nAttributeError: \'AxesSubplot\' object has no attribute \'savefig\'\n', ""fig = plt.figure()\ndtf2.plot()\nfig.savefig('output.png')\n""]";"[""dtf = pd.DataFrame.from_records(d,columns=h)\nfig = plt.figure()\nax = dtf2.plot()\nax = fig.add_subplot(ax)\nfig.savefig('~/Documents/output.png')\nAssertionError\ndtf2.plot().savefig('~/Documents/output.png')\n\n\nfig = plt.figure()\ndtf2.plot()\nfig.savefig('output.png')\n""]";"[""import pandas as pd\ndtf = pd.DataFrame.from_records(d,columns=h)\nfig = plt.figure()\nax = dtf2.plot()\nax = fig.add_subplot(ax)\nfig.savefig('~/Documents/output.png')\nAssertionError\ndtf2.plot().savefig('~/Documents/output.png')\n\n\nfig = plt.figure()\ndtf2.plot()\nfig.savefig('output.png')\n""]";True;"[""import pandas as pd\ndtf = pd.DataFrame.from_records(d,columns=h)\nfig = plt.figure()\nax = dtf2.plot()\nax = fig.add_subplot(ax)\nfig.savefig('~/Documents/output.png')\nAssertionError\ndtf2.plot().savefig('~/Documents/output.png')\n\n\nfig = plt.figure()\ndtf2.plot()\nfig.savefig('output.png')\n""]";False;0;2;"[""name 'pd' is not defined"", ""name 'dtf' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'd' is not defined"", ""name 'dtf' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'd' is not defined"", ""name 'dtf' is not defined""]";['NameError', 'NameError']
529;529;529;529;3.0;0;19584029;;1;27;<python><pandas><histogram>;Plotting histograms from grouped data in a pandas DataFrame;47761.0;"[""from pandas import DataFrame\nimport numpy as np\nx = ['A']*300 + ['B']*400 + ['C']*300\ny = np.random.randn(1000)\ndf = DataFrame({'Letter':x, 'N':y})\ngrouped = df.groupby('Letter')\ndf.groupby('Letter').hist()\n""]";"[""from pandas import DataFrame\nimport numpy as np\nx = ['A']*300 + ['B']*400 + ['C']*300\ny = np.random.randn(1000)\ndf = DataFrame({'Letter':x, 'N':y})\ngrouped = df.groupby('Letter')\n"", ""df.groupby('Letter').hist()\n""]";"[""from pandas import DataFrame\nimport numpy as np\nx = ['A']*300 + ['B']*400 + ['C']*300\ny = np.random.randn(1000)\ndf = DataFrame({'Letter':x, 'N':y})\ngrouped = df.groupby('Letter')\n"", ""df.groupby('Letter').hist()\n""]";"[""from pandas import DataFrame\nimport numpy as np\nx = ['A']*300 + ['B']*400 + ['C']*300\ny = np.random.randn(1000)\ndf = DataFrame({'Letter':x, 'N':y})\ngrouped = df.groupby('Letter')\ndf.groupby('Letter').hist()\n""]";"[""from pandas import DataFrame\nimport numpy as np\nx = ['A']*300 + ['B']*400 + ['C']*300\ny = np.random.randn(1000)\ndf = DataFrame({'Letter':x, 'N':y})\ngrouped = df.groupby('Letter')\ndf.groupby('Letter').hist()\n""]";False;"[""import pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\nx = ['A']*300 + ['B']*400 + ['C']*300\ny = np.random.randn(1000)\ndf = DataFrame({'Letter':x, 'N':y})\ngrouped = df.groupby('Letter')\ndf.groupby('Letter').hist()\n""]";False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'N'"", ""'N'""]";['KeyError', 'KeyError']
530;530;530;530;3.0;0;19585280;;1;21;<python><pandas>;Convert a row in pandas into list;21327.0;['admit   gpa  gre  rank   \n0  3.61  380     3  \n1  3.67  660     3  \n1  3.19  640     4  \n0  2.93  520     4\n[[0,3.61,380,3], [1,3.67,660,3], [1,3.19,640,4], [0,2.93,520,4]]   \n'];['admit   gpa  gre  rank   \n0  3.61  380     3  \n1  3.67  660     3  \n1  3.19  640     4  \n0  2.93  520     4\n', '[[0,3.61,380,3], [1,3.67,660,3], [1,3.19,640,4], [0,2.93,520,4]]   \n'];['admit   gpa  gre  rank   \n0  3.61  380     3  \n1  3.67  660     3  \n1  3.19  640     4  \n0  2.93  520     4\n', '[[0,3.61,380,3], [1,3.67,660,3], [1,3.19,640,4], [0,2.93,520,4]]   \n'];['[[0,3.61,380,3], [1,3.67,660,3], [1,3.19,640,4], [0,2.93,520,4]]   \n'];['[[0,3.61,380,3], [1,3.67,660,3], [1,3.19,640,4], [0,2.93,520,4]]   \n'];False;['import pandas as pd\n[[0,3.61,380,3], [1,3.67,660,3], [1,3.19,640,4], [0,2.93,520,4]]   \n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
531;531;531;531;2.0;0;19599578;;1;18;<python><pandas>;Get particular row as series from pandas dataframe;9615.0;"['>>> df = pd.DataFrame({\'date\': [20130101, 20130101, 20130102], \'location\': [\'a\', \'a\', \'c\']})\n>>> df\n       date location\n0  20130101        a\n1  20130101        a\n2  20130102        c\nrow = df[df[""location""] == ""c""].head(1)  # gives a dataframe\nrow = df.ix[df[""location""] == ""c""]       # also gives a dataframe with single row\n']";"["">>> df = pd.DataFrame({'date': [20130101, 20130101, 20130102], 'location': ['a', 'a', 'c']})\n>>> df\n       date location\n0  20130101        a\n1  20130101        a\n2  20130102        c\n"", 'row = df[df[""location""] == ""c""].head(1)  # gives a dataframe\nrow = df.ix[df[""location""] == ""c""]       # also gives a dataframe with single row\n']";"["">>> df = pd.DataFrame({'date': [20130101, 20130101, 20130102], 'location': ['a', 'a', 'c']})\n>>> df\n       date location\n0  20130101        a\n1  20130101        a\n2  20130102        c\n"", 'location', 'c', 'row = df[df[""location""] == ""c""].head(1)  # gives a dataframe\nrow = df.ix[df[""location""] == ""c""]       # also gives a dataframe with single row\n']";"['row = df[df[""location""] == ""c""].head(1)  # gives a dataframe\nrow = df.ix[df[""location""] == ""c""]       # also gives a dataframe with single row\n']";"['row = df[df[""location""] == ""c""].head(1)  # gives a dataframe\nrow = df.ix[df[""location""] == ""c""]       # also gives a dataframe with single row\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\nrow = df[df[""location""] == ""c""].head(1)  # gives a dataframe\nrow = df.ix[df[""location""] == ""c""]       # also gives a dataframe with single row\n']";True;1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""'location'"", 'Sucess']";['KeyError', 'Sucess']
532;532;532;532;3.0;1;19609631;;1;18;<python><pandas>;python: changing row index of pandas data frame;34376.0;[' followers_df\n\n             0\n0         oasikhia \n0     LEANEnergyUS\n0  _johannesngwako\n0     jamesbreenre\n0   CaitlinFecteau\n0  mantequillaFACE\n0         apowersb\n0       ecoprinter\n0        tsdesigns\n0      GreenBizDoc\n0        JimHarris\n0    Jmarti11Julia\n0         JAslat63\n0            prAna\n0    GrantLundberg \n0        Jitasa_Is\n0     ChoosePAWind\n0  cleanpowerperks\n0          WoWEorg\n0      Laura_Chuck\n followers_df\n\n             0\n0          oasikhia \n1      LEANEnergyUS\n2   _johannesngwako\n3      jamesbreenre\n4    CaitlinFecteau\n5   mantequillaFACE\n6          apowersb\n7        ecoprinter\n8         tsdesigns\n9       GreenBizDoc\n10        JimHarris\n11    Jmarti11Julia\n12         JAslat63\n13            prAna\n14    GrantLundberg \n15        Jitasa_Is\n16     ChoosePAWind\n17  cleanpowerperks\n18          WoWEorg\n19      Laura_Chuck\n     index = pandas.Index(range(20))\n     followers_df = pandas.DataFrame(followers_df, index=index)\n  ValueError: Shape of passed values is (1, 39), indices imply (1, 20)\n'];[' followers_df\n\n             0\n0         oasikhia \n0     LEANEnergyUS\n0  _johannesngwako\n0     jamesbreenre\n0   CaitlinFecteau\n0  mantequillaFACE\n0         apowersb\n0       ecoprinter\n0        tsdesigns\n0      GreenBizDoc\n0        JimHarris\n0    Jmarti11Julia\n0         JAslat63\n0            prAna\n0    GrantLundberg \n0        Jitasa_Is\n0     ChoosePAWind\n0  cleanpowerperks\n0          WoWEorg\n0      Laura_Chuck\n', ' followers_df\n\n             0\n0          oasikhia \n1      LEANEnergyUS\n2   _johannesngwako\n3      jamesbreenre\n4    CaitlinFecteau\n5   mantequillaFACE\n6          apowersb\n7        ecoprinter\n8         tsdesigns\n9       GreenBizDoc\n10        JimHarris\n11    Jmarti11Julia\n12         JAslat63\n13            prAna\n14    GrantLundberg \n15        Jitasa_Is\n16     ChoosePAWind\n17  cleanpowerperks\n18          WoWEorg\n19      Laura_Chuck\n', '     index = pandas.Index(range(20))\n     followers_df = pandas.DataFrame(followers_df, index=index)\n', '  ValueError: Shape of passed values is (1, 39), indices imply (1, 20)\n'];['followers_df', ' followers_df\n\n             0\n0         oasikhia \n0     LEANEnergyUS\n0  _johannesngwako\n0     jamesbreenre\n0   CaitlinFecteau\n0  mantequillaFACE\n0         apowersb\n0       ecoprinter\n0        tsdesigns\n0      GreenBizDoc\n0        JimHarris\n0    Jmarti11Julia\n0         JAslat63\n0            prAna\n0    GrantLundberg \n0        Jitasa_Is\n0     ChoosePAWind\n0  cleanpowerperks\n0          WoWEorg\n0      Laura_Chuck\n', ' followers_df\n\n             0\n0          oasikhia \n1      LEANEnergyUS\n2   _johannesngwako\n3      jamesbreenre\n4    CaitlinFecteau\n5   mantequillaFACE\n6          apowersb\n7        ecoprinter\n8         tsdesigns\n9       GreenBizDoc\n10        JimHarris\n11    Jmarti11Julia\n12         JAslat63\n13            prAna\n14    GrantLundberg \n15        Jitasa_Is\n16     ChoosePAWind\n17  cleanpowerperks\n18          WoWEorg\n19      Laura_Chuck\n', '     index = pandas.Index(range(20))\n     followers_df = pandas.DataFrame(followers_df, index=index)\n', '  ValueError: Shape of passed values is (1, 39), indices imply (1, 20)\n'];['\n\n'];['\n\n'];False;['import pandas as pd\n\n\n'];False;0;2;"[""name 'followers_df' is not defined"", ""name 'followers_df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'followers_df' is not defined"", ""name 'followers_df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'followers_df' is not defined"", ""name 'followers_df' is not defined""]";['NameError', 'NameError']
533;533;533;533;2.0;0;19611729;;1;23;<python><pandas><google-spreadsheet><google-apps>;Getting Google Spreadsheet CSV into A Pandas Dataframe;7482.0;"[""import requests\nr = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&output=csv')\ndata = r.content\n',City,region,Res_Comm,mkt_type,Quradate,National_exp,Alabama_exp,Sales_exp,Inventory_exp,Price_exp,Credit_exp\\n0,Dothan,South_Central-Montgomery-Auburn-Wiregrass-Dothan,Residential,Rural,1/15/2010,2,2,3,2,3,3\\n10,Foley,South_Mobile-Baldwin,Residential,Suburban_Urban,1/15/2010,4,4,4,4,4,3\\n12,Birmingham,North_Central-Birmingham-Tuscaloosa-Anniston,Commercial,Suburban_Urban,1/15/2010,2,2,3,2,2,3\\n\ndf = pd.io.parsers.read_csv('/home/tom/Dropbox/Projects/annonallanswerswithmaster1012013.csv',index_col=0,parse_dates=['Quradate'])\nhttps://docs.google.com/spreadsheets/d/177_dFZ0i-duGxLiyg6tnwNDKruAYE-_Dd8vAQziipJQ/export?format=csv&id\n""]";"[""import requests\nr = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&output=csv')\ndata = r.content\n"", ""',City,region,Res_Comm,mkt_type,Quradate,National_exp,Alabama_exp,Sales_exp,Inventory_exp,Price_exp,Credit_exp\\n0,Dothan,South_Central-Montgomery-Auburn-Wiregrass-Dothan,Residential,Rural,1/15/2010,2,2,3,2,3,3\\n10,Foley,South_Mobile-Baldwin,Residential,Suburban_Urban,1/15/2010,4,4,4,4,4,3\\n12,Birmingham,North_Central-Birmingham-Tuscaloosa-Anniston,Commercial,Suburban_Urban,1/15/2010,2,2,3,2,2,3\\n\n"", ""df = pd.io.parsers.read_csv('/home/tom/Dropbox/Projects/annonallanswerswithmaster1012013.csv',index_col=0,parse_dates=['Quradate'])\n"", 'https://docs.google.com/spreadsheets/d/177_dFZ0i-duGxLiyg6tnwNDKruAYE-_Dd8vAQziipJQ/export?format=csv&id\n']";"[""import requests\nr = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&output=csv')\ndata = r.content\n"", ""',City,region,Res_Comm,mkt_type,Quradate,National_exp,Alabama_exp,Sales_exp,Inventory_exp,Price_exp,Credit_exp\\n0,Dothan,South_Central-Montgomery-Auburn-Wiregrass-Dothan,Residential,Rural,1/15/2010,2,2,3,2,3,3\\n10,Foley,South_Mobile-Baldwin,Residential,Suburban_Urban,1/15/2010,4,4,4,4,4,3\\n12,Birmingham,North_Central-Birmingham-Tuscaloosa-Anniston,Commercial,Suburban_Urban,1/15/2010,2,2,3,2,2,3\\n\n"", ""df = pd.io.parsers.read_csv('/home/tom/Dropbox/Projects/annonallanswerswithmaster1012013.csv',index_col=0,parse_dates=['Quradate'])\n"", 'https://docs.google.com/spreadsheets/d/177_dFZ0i-duGxLiyg6tnwNDKruAYE-_Dd8vAQziipJQ/export?format=csv&id\n']";"[""import requests\nr = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&output=csv')\ndata = r.content\ndf = pd.io.parsers.read_csv('/home/tom/Dropbox/Projects/annonallanswerswithmaster1012013.csv',index_col=0,parse_dates=['Quradate'])\n""]";"[""import pandas as pd\nimport requests\nr = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&output=csv')\ndata = r.content\ndf = pd.io.parsers.read_csv('/home/tom/Dropbox/Projects/annonallanswerswithmaster1012013.csv',index_col=0,parse_dates=['Quradate'])\n""]";True;"[""import pandas as pd\nimport requests\nr = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&output=csv')\ndata = r.content\ndf = pd.io.parsers.read_csv('/home/tom/Dropbox/Projects/annonallanswerswithmaster1012013.csv',index_col=0,parse_dates=['Quradate'])\n""]";False;0;2;"[""name 'pd' is not defined"", ""name 'test' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'StringIO' is not defined"", ""name 'test' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'StringIO' is not defined"", ""name 'test' is not defined""]";['NameError', 'NameError']
534;534;534;534;3.0;0;19618912;;1;21;<python><python-2.7><pandas><dataframe><intersect>;Finding common rows (intersection) in two Pandas dataframes;23593.0;['+------------------------+------------------------+--------+\n|        user_id         |      business_id       | rating |\n+------------------------+------------------------+--------+\n| rLtl8ZkDX5vH5nAx9C3q5Q | eIxSLxzIlfExI6vgAbn2JA |      4 |\n| C6IOtaaYdLIT5fWd7ZYIuA | eIxSLxzIlfExI6vgAbn2JA |      5 |\n| mlBC3pN9GXlUUfQi1qBBZA | KoIRdcIfh3XWxiCeV1BDmA |      3 |\n+------------------------+------------------------+--------+\n'];['+------------------------+------------------------+--------+\n|        user_id         |      business_id       | rating |\n+------------------------+------------------------+--------+\n| rLtl8ZkDX5vH5nAx9C3q5Q | eIxSLxzIlfExI6vgAbn2JA |      4 |\n| C6IOtaaYdLIT5fWd7ZYIuA | eIxSLxzIlfExI6vgAbn2JA |      5 |\n| mlBC3pN9GXlUUfQi1qBBZA | KoIRdcIfh3XWxiCeV1BDmA |      3 |\n+------------------------+------------------------+--------+\n'];['df1', 'df2', '+------------------------+------------------------+--------+\n|        user_id         |      business_id       | rating |\n+------------------------+------------------------+--------+\n| rLtl8ZkDX5vH5nAx9C3q5Q | eIxSLxzIlfExI6vgAbn2JA |      4 |\n| C6IOtaaYdLIT5fWd7ZYIuA | eIxSLxzIlfExI6vgAbn2JA |      5 |\n| mlBC3pN9GXlUUfQi1qBBZA | KoIRdcIfh3XWxiCeV1BDmA |      3 |\n+------------------------+------------------------+--------+\n', 'user_id', 'df1', 'df2', 'user_id', 'df1', 'df2', 'user_id', 'merge'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df1' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df1' is not defined"", ""name 'df1' is not defined""]";['NameError', 'NameError'];0;2;"[""'DataFrame' object has no attribute 'user_id'"", ""'user_id'""]";['AttributeError', 'KeyError']
535;535;535;535;2.0;1;19632075;;1;25;<python><pandas>;how to read file with space separated values;17660.0;"[""pd.read_csv('file.csv', delimiter=' ')\n""]";"[""pd.read_csv('file.csv', delimiter=' ')\n""]";"[""pd.read_csv('file.csv', delimiter=' ')\n""]";"[""pd.read_csv('file.csv', delimiter=' ')\n""]";"[""import pandas as pd\npd.read_csv('file.csv', delimiter=' ')\n""]";True;"[""import pandas as pd\npd.read_csv('file.csv', delimiter=' ')\n""]";False;1;2;"[""name 'pd' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""File b'whitespace.csv' does not exist"", 'Sucess']";['FileNotFoundError', 'Sucess'];1;2;"[""File b'whitespace.csv' does not exist"", 'Sucess']";['FileNotFoundError', 'Sucess']
536;536;536;536;1.0;7;19666904;;1;14;<python-2.7><pandas><ipython><dataframe>;Pandas Dataframe ValueError: Shape of passed values is (X, ), indices imply (X, Y);22595.0;"[""def random(row):\n   return [1,2,3,4]\n\ndf = pandas.DataFrame(np.random.randn(5, 4), columns=list('ABCD'))\n\ndf.apply(func = random, axis = 1)\n[1,2,3,4]\n[1,2,3,4]\n[1,2,3,4]\n[1,2,3,4]\ndef random(row):\n   return [1,2,3,4]\n\ndf = pandas.DataFrame(np.random.randn(5, 4), columns=list('ABCD'))\ndf['E'] = 1\n\ndf.apply(func = random, axis = 1)\nValueError: Shape of passed values is (5,), indices imply (5, 5)\ndef random(row):\n   return [1,2,3,4]\ndef random(row):\n   print [1,2,3,4]\n""]";"[""def random(row):\n   return [1,2,3,4]\n\ndf = pandas.DataFrame(np.random.randn(5, 4), columns=list('ABCD'))\n\ndf.apply(func = random, axis = 1)\n"", '[1,2,3,4]\n[1,2,3,4]\n[1,2,3,4]\n[1,2,3,4]\n', ""def random(row):\n   return [1,2,3,4]\n\ndf = pandas.DataFrame(np.random.randn(5, 4), columns=list('ABCD'))\ndf['E'] = 1\n\ndf.apply(func = random, axis = 1)\n"", 'ValueError: Shape of passed values is (5,), indices imply (5, 5)\n', 'def random(row):\n   return [1,2,3,4]\n', 'def random(row):\n   print [1,2,3,4]\n']";"[""def random(row):\n   return [1,2,3,4]\n\ndf = pandas.DataFrame(np.random.randn(5, 4), columns=list('ABCD'))\n\ndf.apply(func = random, axis = 1)\n"", '[1,2,3,4]\n[1,2,3,4]\n[1,2,3,4]\n[1,2,3,4]\n', ""def random(row):\n   return [1,2,3,4]\n\ndf = pandas.DataFrame(np.random.randn(5, 4), columns=list('ABCD'))\ndf['E'] = 1\n\ndf.apply(func = random, axis = 1)\n"", 'ValueError: Shape of passed values is (5,), indices imply (5, 5)\n', 'def random(row):\n   return [1,2,3,4]\n', 'def random(row):\n   print [1,2,3,4]\n']";"[""\ndf = pandas.DataFrame(np.random.randn(5, 4), columns=list('ABCD'))\n\ndf.apply(func = random, axis = 1)\n[1,2,3,4]\n[1,2,3,4]\n[1,2,3,4]\n[1,2,3,4]\n\ndf = pandas.DataFrame(np.random.randn(5, 4), columns=list('ABCD'))\ndf['E'] = 1\n\ndf.apply(func = random, axis = 1)\n""]";"[""\ndf = pandas.DataFrame(np.random.randn(5, 4), columns=list('ABCD'))\n\ndf.apply(func = random, axis = 1)\n[1,2,3,4]\n[1,2,3,4]\n[1,2,3,4]\n[1,2,3,4]\n\ndf = pandas.DataFrame(np.random.randn(5, 4), columns=list('ABCD'))\ndf['E'] = 1\n\ndf.apply(func = random, axis = 1)\n""]";False;"[""import pandas as pd\n\ndf = pandas.DataFrame(np.random.randn(5, 4), columns=list('ABCD'))\n\ndf.apply(func = random, axis = 1)\n[1,2,3,4]\n[1,2,3,4]\n[1,2,3,4]\n[1,2,3,4]\n\ndf = pandas.DataFrame(np.random.randn(5, 4), columns=list('ABCD'))\ndf['E'] = 1\n\ndf.apply(func = random, axis = 1)\n""]";False;0;0;[];[];0;0;[];[];0;0;[];[]
537;537;537;537;3.0;0;19726029;;1;20;<python><pandas><dataframe>;python: make pandas dataframe column headers all lowercase;12664.0;['data =\n\n  country country isocode  year     XRAT          tcgdp\n0  Canada             CAN  2001  1.54876   924909.44207\n1  Canada             CAN  2002  1.56932   957299.91586\n2  Canada             CAN  2003  1.40105  1016902.00180\n....\ndata.headers.lowercase()\n  country country isocode  year     xrat          tcgdp\n0  Canada             CAN  2001  1.54876   924909.44207\n1  Canada             CAN  2002  1.56932   957299.91586\n2  Canada             CAN  2003  1.40105  1016902.00180\n3  Canada             CAN  2004  1.30102  1096000.35500\n....\n'];['data =\n\n  country country isocode  year     XRAT          tcgdp\n0  Canada             CAN  2001  1.54876   924909.44207\n1  Canada             CAN  2002  1.56932   957299.91586\n2  Canada             CAN  2003  1.40105  1016902.00180\n....\n', 'data.headers.lowercase()\n', '  country country isocode  year     xrat          tcgdp\n0  Canada             CAN  2001  1.54876   924909.44207\n1  Canada             CAN  2002  1.56932   957299.91586\n2  Canada             CAN  2003  1.40105  1016902.00180\n3  Canada             CAN  2004  1.30102  1096000.35500\n....\n'];['data =\n\n  country country isocode  year     XRAT          tcgdp\n0  Canada             CAN  2001  1.54876   924909.44207\n1  Canada             CAN  2002  1.56932   957299.91586\n2  Canada             CAN  2003  1.40105  1016902.00180\n....\n', 'data.headers.lowercase()\n', '  country country isocode  year     xrat          tcgdp\n0  Canada             CAN  2001  1.54876   924909.44207\n1  Canada             CAN  2002  1.56932   957299.91586\n2  Canada             CAN  2003  1.40105  1016902.00180\n3  Canada             CAN  2004  1.30102  1096000.35500\n....\n'];['\ndata.headers.lowercase()\n'];['\ndata.headers.lowercase()\n'];False;['import pandas as pd\ndata = pd.DataFrame()\n\ndata.headers.lowercase()\n'];True;0;2;"[""name 'data' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'data' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""Can only use .str accessor with string values (i.e. inferred_type is 'string', 'unicode' or 'mixed')""]";['Sucess', 'AttributeError']
538;538;538;538;4.0;4;19726663;;1;27;<python><matplotlib><pandas>;How to save the Pandas dataframe/series data as a figure?;22214.0;['>>> df\n                   sales  net_pft     ROE    ROIC\nSTK_ID RPT_Date                                  \n600809 20120331  22.1401   4.9253  0.1651  0.6656\n       20120630  38.1565   7.8684  0.2567  1.0385\n       20120930  52.5098  12.4338  0.3587  1.2867\n       20121231  64.7876  13.2731  0.3736  1.2205\n       20130331  27.9517   7.5182  0.1745  0.3723\n       20130630  40.6460   9.8572  0.2560  0.4290\n       20130930  53.0501  11.8605  0.2927  0.4369 \n'];['>>> df\n                   sales  net_pft     ROE    ROIC\nSTK_ID RPT_Date                                  \n600809 20120331  22.1401   4.9253  0.1651  0.6656\n       20120630  38.1565   7.8684  0.2567  1.0385\n       20120930  52.5098  12.4338  0.3587  1.2867\n       20121231  64.7876  13.2731  0.3736  1.2205\n       20130331  27.9517   7.5182  0.1745  0.3723\n       20130630  40.6460   9.8572  0.2560  0.4290\n       20130930  53.0501  11.8605  0.2927  0.4369 \n'];"['>>> df\n                   sales  net_pft     ROE    ROIC\nSTK_ID RPT_Date                                  \n600809 20120331  22.1401   4.9253  0.1651  0.6656\n       20120630  38.1565   7.8684  0.2567  1.0385\n       20120930  52.5098  12.4338  0.3587  1.2867\n       20121231  64.7876  13.2731  0.3736  1.2205\n       20130331  27.9517   7.5182  0.1745  0.3723\n       20130630  40.6460   9.8572  0.2560  0.4290\n       20130930  53.0501  11.8605  0.2927  0.4369 \n', ""df.output_as_png(filename='df_data.png')""]";[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""No module named 'matplotlib'""]";['NameError', 'ImportError'];0;2;"[""name 'df' is not defined"", ""No module named 'matplotlib'""]";['NameError', 'ImportError'];0;2;"[""name '_converter' is not defined"", ""No module named 'matplotlib'""]";['NameError', 'ImportError']
539;539;539;539;3.0;0;19736080;;1;23;<python><pandas>;Creating dataframe from a dictionary where entries have different lengths;17482.0;['pd.DataFrame(my_dict)\nValueError: arrays must all be the same length\n'];['pd.DataFrame(my_dict)\n', 'ValueError: arrays must all be the same length\n'];['pd.DataFrame(my_dict)\n', 'ValueError: arrays must all be the same length\n', 'NaN'];['pd.DataFrame(my_dict)\n'];['import pandas as pd\npd.DataFrame(my_dict)\n'];True;['import pandas as pd\npd.DataFrame(my_dict)\n'];False;1;2;"[""name 'np' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'np' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'np' is not defined"", 'Sucess']";['NameError', 'Sucess']
540;540;540;540;2.0;2;19758364;;1;80;<python><pandas><dataframe>;python: rename single column header in pandas dataframe;50105.0;['data =\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n5   4    8    3\n6   8    2    8\n7   9    9   10\n8   6    6    4\n9  10   10    7\n'];['data =\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n5   4    8    3\n6   8    2    8\n7   9    9   10\n8   6    6    4\n9  10   10    7\n'];['data', 'gdp', 'log(gdp)', 'data =\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n5   4    8    3\n6   8    2    8\n7   9    9   10\n8   6    6    4\n9  10   10    7\n'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'data' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'data' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
541;541;541;541;3.0;0;19781609;;1;13;<python><pandas>;How do you remove the column name row from a pandas DataFrame?;19161.0;['Val1 Val2 Val3\n1     2    3 \n5     6    7 \n9     1    2\n'];['Val1 Val2 Val3\n1     2    3 \n5     6    7 \n9     1    2\n'];['Val1 Val2 Val3\n1     2    3 \n5     6    7 \n9     1    2\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
542;542;542;542;6.0;3;19790790;;1;33;<python><split><pandas><dataframe>;Splitting dataframe into multiple dataframes;55592.0;"[""import pandas as pd\n\ndef splitframe(data, name='name'):\n\n    n = data[name][0]\n\n    df = pd.DataFrame(columns=data.columns)\n\n    datalist = []\n\n    for i in range(len(data)):\n        if data[name][i] == n:\n            df = df.append(data.iloc[i])\n        else:\n            datalist.append(df)\n            df = pd.DataFrame(columns=data.columns)\n            n = data[name][i]\n            df = df.append(data.iloc[i])\n\n    return datalist\n""]";"[""import pandas as pd\n\ndef splitframe(data, name='name'):\n\n    n = data[name][0]\n\n    df = pd.DataFrame(columns=data.columns)\n\n    datalist = []\n\n    for i in range(len(data)):\n        if data[name][i] == n:\n            df = df.append(data.iloc[i])\n        else:\n            datalist.append(df)\n            df = pd.DataFrame(columns=data.columns)\n            n = data[name][i]\n            df = df.append(data.iloc[i])\n\n    return datalist\n""]";"[""import pandas as pd\n\ndef splitframe(data, name='name'):\n\n    n = data[name][0]\n\n    df = pd.DataFrame(columns=data.columns)\n\n    datalist = []\n\n    for i in range(len(data)):\n        if data[name][i] == n:\n            df = df.append(data.iloc[i])\n        else:\n            datalist.append(df)\n            df = pd.DataFrame(columns=data.columns)\n            n = data[name][i]\n            df = df.append(data.iloc[i])\n\n    return datalist\n""]";['import pandas as pd\n\n\n\n\n\n\n'];['import pandas as pd\n\n\n\n\n\n\n'];False;['import pandas as pd\nimport pandas as pd\n\n\n\n\n\n\n'];False;0;2;"[""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""'DataFrame' object has no attribute 'sort'""]";['NameError', 'AttributeError']
543;543;543;543;3.0;1;19798112;;1;15;<python><pandas>;Convert pandas DataFrame to a nested dict;5387.0;"[""    name    v1  v2  v3\n0   A       A1  A11 1\n1   A       A2  A12 2\n2   B       B1  B12 3\n3   C       C1  C11 4\n4   B       B2  B21 5\n5   A       A2  A21 6\n{\n'A' : { \n    'A1' : { 'A11' : 1 }\n    'A2' : { 'A12' : 2 , 'A21' : 6 }} , \n'B' : { \n    'B1' : { 'B12' : 3 } } , \n'C' : { \n    'C1' : { 'C11' : 4}}\n}\n""]";"['    name    v1  v2  v3\n0   A       A1  A11 1\n1   A       A2  A12 2\n2   B       B1  B12 3\n3   C       C1  C11 4\n4   B       B2  B21 5\n5   A       A2  A21 6\n', ""{\n'A' : { \n    'A1' : { 'A11' : 1 }\n    'A2' : { 'A12' : 2 , 'A21' : 6 }} , \n'B' : { \n    'B1' : { 'B12' : 3 } } , \n'C' : { \n    'C1' : { 'C11' : 4}}\n}\n""]";"['    name    v1  v2  v3\n0   A       A1  A11 1\n1   A       A2  A12 2\n2   B       B1  B12 3\n3   C       C1  C11 4\n4   B       B2  B21 5\n5   A       A2  A21 6\n', ""{\n'A' : { \n    'A1' : { 'A11' : 1 }\n    'A2' : { 'A12' : 2 , 'A21' : 6 }} , \n'B' : { \n    'B1' : { 'B12' : 3 } } , \n'C' : { \n    'C1' : { 'C11' : 4}}\n}\n"", 'zip']";[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
544;544;544;544;6.0;0;19798153;;1;189;<python><numpy><pandas><vectorization>;Difference between map, applymap and apply methods in Pandas;98032.0;[''];[];['map', 'Series', 'DataFrame', 'apply', 'applymap'];[''];[''];False;['import pandas as pd\n'];False;2;4;"[""name 'DataFrame' is not defined"", 'Sucess', ""name 'pd' is not defined"", 'Sucess']";['NameError', 'Sucess', 'NameError', 'Sucess'];3;4;"[""name 'np' is not defined"", 'Sucess', 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess', 'Sucess'];3;4;"[""name 'DataFrame' is not defined"", 'Sucess', 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess', 'Sucess']
545;545;545;545;3.0;1;19818756;;1;19;<python><pandas>;Extract row with maximum value in a group pandas dataframe;20452.0;"["">>> df = DataFrame({'Sp':['a','b','c','d','e','f'], 'Mt':['s1', 's1', 's2','s2','s2','s3'], 'Value':[1,2,3,4,5,6], 'count':[3,2,5,10,10,6]})\n>>> df\n   Mt Sp  Value  count\n0  s1  a      1      3\n1  s1  b      2      2\n2  s2  c      3      5\n3  s2  d      4     10\n4  s2  e      5     10\n5  s3  f      6      6\n>>> idx = df.groupby(['Mt'])['count'].transform(max) == df['count']\n>>> df[idx]\n   Mt Sp  Value  count\n0  s1  a      1      3\n3  s2  d      4     10\n4  s2  e      5     10\n5  s3  f      6      6\n>>> \n""]";"["">>> df = DataFrame({'Sp':['a','b','c','d','e','f'], 'Mt':['s1', 's1', 's2','s2','s2','s3'], 'Value':[1,2,3,4,5,6], 'count':[3,2,5,10,10,6]})\n>>> df\n   Mt Sp  Value  count\n0  s1  a      1      3\n1  s1  b      2      2\n2  s2  c      3      5\n3  s2  d      4     10\n4  s2  e      5     10\n5  s3  f      6      6\n>>> idx = df.groupby(['Mt'])['count'].transform(max) == df['count']\n>>> df[idx]\n   Mt Sp  Value  count\n0  s1  a      1      3\n3  s2  d      4     10\n4  s2  e      5     10\n5  s3  f      6      6\n>>> \n""]";"["">>> df = DataFrame({'Sp':['a','b','c','d','e','f'], 'Mt':['s1', 's1', 's2','s2','s2','s3'], 'Value':[1,2,3,4,5,6], 'count':[3,2,5,10,10,6]})\n>>> df\n   Mt Sp  Value  count\n0  s1  a      1      3\n1  s1  b      2      2\n2  s2  c      3      5\n3  s2  d      4     10\n4  s2  e      5     10\n5  s3  f      6      6\n>>> idx = df.groupby(['Mt'])['count'].transform(max) == df['count']\n>>> df[idx]\n   Mt Sp  Value  count\n0  s1  a      1      3\n3  s2  d      4     10\n4  s2  e      5     10\n5  s3  f      6      6\n>>> \n""]";[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""'Mt'"", 'Sucess']";['KeyError', 'Sucess']
546;546;546;546;3.0;1;19828822;;1;111;<python><pandas>;How to check whether a pandas DataFrame is empty?;70044.0;[''];[];['DataFrame', 'DataFrame'];[''];[''];False;['import pandas as pd\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
547;547;547;547;4.0;0;19851005;;1;43;<python><pandas><dataframe>;Rename Pandas DataFrame Index;76528.0;"[""In [2]: df = pd.read_csv(r'D:\\Data\\DataTimeSeries_csv//seriesSM.csv', header=None, parse_dates=[[0]], index_col=[0] )\n\nIn [3]: df.head()\nOut[3]: \n                   1\n0                   \n2002-06-18  0.112000\n2002-06-22  0.190333\n2002-06-26  0.134000\n2002-06-30  0.093000\n2002-07-04  0.098667\n\nIn [4]: df.rename(index={0:'Date'}, columns={1:'SM'}, inplace=True)\n\nIn [5]: df.head()\nOut[5]: \n                  SM\n0                   \n2002-06-18  0.112000\n2002-06-22  0.190333\n2002-06-26  0.134000\n2002-06-30  0.093000\n2002-07-04  0.098667\n""]";"[""In [2]: df = pd.read_csv(r'D:\\Data\\DataTimeSeries_csv//seriesSM.csv', header=None, parse_dates=[[0]], index_col=[0] )\n\nIn [3]: df.head()\nOut[3]: \n                   1\n0                   \n2002-06-18  0.112000\n2002-06-22  0.190333\n2002-06-26  0.134000\n2002-06-30  0.093000\n2002-07-04  0.098667\n\nIn [4]: df.rename(index={0:'Date'}, columns={1:'SM'}, inplace=True)\n\nIn [5]: df.head()\nOut[5]: \n                  SM\n0                   \n2002-06-18  0.112000\n2002-06-22  0.190333\n2002-06-26  0.134000\n2002-06-30  0.093000\n2002-07-04  0.098667\n""]";"[""In [2]: df = pd.read_csv(r'D:\\Data\\DataTimeSeries_csv//seriesSM.csv', header=None, parse_dates=[[0]], index_col=[0] )\n\nIn [3]: df.head()\nOut[3]: \n                   1\n0                   \n2002-06-18  0.112000\n2002-06-22  0.190333\n2002-06-26  0.134000\n2002-06-30  0.093000\n2002-07-04  0.098667\n\nIn [4]: df.rename(index={0:'Date'}, columns={1:'SM'}, inplace=True)\n\nIn [5]: df.head()\nOut[5]: \n                  SM\n0                   \n2002-06-18  0.112000\n2002-06-22  0.190333\n2002-06-26  0.134000\n2002-06-30  0.093000\n2002-07-04  0.098667\n""]";"[""df = pd.read_csv(r'D:\\Data\\DataTimeSeries_csv//seriesSM.csv', header=None, parse_dates=[[0]], index_col=[0] )\n\ndf.rename(index={0:'Date'}, columns={1:'SM'}, inplace=True)\n\n""]";"[""import pandas as pd\ndf = pd.read_csv(r'D:\\Data\\DataTimeSeries_csv//seriesSM.csv', header=None, parse_dates=[[0]], index_col=[0] )\n\ndf.rename(index={0:'Date'}, columns={1:'SM'}, inplace=True)\n\n""]";True;"[""import pandas as pd\ndf = pd.read_csv(r'D:\\Data\\DataTimeSeries_csv//seriesSM.csv', header=None, parse_dates=[[0]], index_col=[0] )\n\ndf.rename(index={0:'Date'}, columns={1:'SM'}, inplace=True)\n\n""]";False;0;2;"[""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError']
548;548;548;548;2.0;0;19860389;;1;14;<python><pandas><dataframe>;python: remove all rows in pandas dataframe that contain a string;6964.0;"[""data =\n\n    y  gdp  cap\n0   1    2    5\n1   2    3    ab\n2   8    7    2\n3   3    bc   7\n4   6    7    7\n5   4    8    3\n...\nexp_list = ['gdp', 'cap']\n\nfor var_name in exp_list:\n    data = data[data.var_name != 'ab']\n""]";"['data =\n\n    y  gdp  cap\n0   1    2    5\n1   2    3    ab\n2   8    7    2\n3   3    bc   7\n4   6    7    7\n5   4    8    3\n...\n', ""exp_list = ['gdp', 'cap']\n\nfor var_name in exp_list:\n    data = data[data.var_name != 'ab']\n""]";"['data =\n\n    y  gdp  cap\n0   1    2    5\n1   2    3    ab\n2   8    7    2\n3   3    bc   7\n4   6    7    7\n5   4    8    3\n...\n', ""exp_list = ['gdp', 'cap']\n\nfor var_name in exp_list:\n    data = data[data.var_name != 'ab']\n""]";"[""\n...\nexp_list = ['gdp', 'cap']\n\n""]";"[""\n...\nexp_list = ['gdp', 'cap']\n\n""]";False;"[""import pandas as pd\n\n...\nexp_list = ['gdp', 'cap']\n\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
549;549;549;549;2.0;1;19867734;;1;20;<python><pandas>;Changing certain values in multiple columns of a pandas DataFrame at once;18591.0;"[""In [1]: df\nOut[1]:\n  apple banana cherry\n0     0      3   good\n1     1      4    bad\n2     2      5   good\nIn [2]: df['apple'][df.cherry == 'bad'] = np.nan\nIn [3]: df\nOut[3]:\n  apple banana cherry\n0     0      3   good\n1   NaN      4    bad\n2     2      5   good\nIn [2]: df[['apple', 'banana']][df.cherry == 'bad'] = np.nan\nIn [3]: df\nOut[3]:\n  apple banana cherry\n0     0      3   good\n1     1      4    bad\n2     2      5   good\nIn [2]: df['apple'][df.cherry == 'bad'] = np.nan\nIn [3]: df['banana'][df.cherry == 'bad'] = np.nan\n""]";"['In [1]: df\nOut[1]:\n  apple banana cherry\n0     0      3   good\n1     1      4    bad\n2     2      5   good\n', ""In [2]: df['apple'][df.cherry == 'bad'] = np.nan\nIn [3]: df\nOut[3]:\n  apple banana cherry\n0     0      3   good\n1   NaN      4    bad\n2     2      5   good\n"", ""In [2]: df[['apple', 'banana']][df.cherry == 'bad'] = np.nan\nIn [3]: df\nOut[3]:\n  apple banana cherry\n0     0      3   good\n1     1      4    bad\n2     2      5   good\n"", ""In [2]: df['apple'][df.cherry == 'bad'] = np.nan\nIn [3]: df['banana'][df.cherry == 'bad'] = np.nan\n""]";"['In [1]: df\nOut[1]:\n  apple banana cherry\n0     0      3   good\n1     1      4    bad\n2     2      5   good\n', ""In [2]: df['apple'][df.cherry == 'bad'] = np.nan\nIn [3]: df\nOut[3]:\n  apple banana cherry\n0     0      3   good\n1   NaN      4    bad\n2     2      5   good\n"", ""In [2]: df[['apple', 'banana']][df.cherry == 'bad'] = np.nan\nIn [3]: df\nOut[3]:\n  apple banana cherry\n0     0      3   good\n1     1      4    bad\n2     2      5   good\n"", ""In [2]: df['apple'][df.cherry == 'bad'] = np.nan\nIn [3]: df['banana'][df.cherry == 'bad'] = np.nan\n""]";"[""df\ndf['apple'][df.cherry == 'bad'] = np.nan\ndf[['apple', 'banana']][df.cherry == 'bad'] = np.nan\n""]";"[""df\ndf['apple'][df.cherry == 'bad'] = np.nan\ndf[['apple', 'banana']][df.cherry == 'bad'] = np.nan\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf\ndf['apple'][df.cherry == 'bad'] = np.nan\ndf[['apple', 'banana']][df.cherry == 'bad'] = np.nan\n""]";True;0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
550;550;550;550;1.0;0;19894939;;1;11;<pandas>;Calculate Arbitrary Percentile on Pandas GroupBy;7394.0;[''];[];['median', 'GroupBy', 'percentile', 'q=50'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'C'""]";['KeyError']
551;551;551;551;5.0;2;19900202;;1;12;<python><pandas><numpy>;How to determine whether a column/variable is numeric or not in Pandas/Numpy?;7605.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
552;552;552;552;5.0;1;19913659;;1;85;<python><pandas>;Pandas conditional creation of a series/dataframe column;70108.0;['    Type       Set\n1    A          Z\n2    B          Z           \n3    B          X\n4    C          Y\n'];['    Type       Set\n1    A          Z\n2    B          Z           \n3    B          X\n4    C          Y\n'];['    Type       Set\n1    A          Z\n2    B          Z           \n3    B          X\n4    C          Y\n'];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'np' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'np' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'np' is not defined"", ""'DataFrame' object has no attribute 'Set'"", ""name 'df' is not defined""]";['NameError', 'AttributeError', 'NameError']
553;553;553;553;4.0;0;19914937;;1;52;<python><pandas>;Applying function with multiple arguments to create a new pandas column;45281.0;"['import pandas as pd\ndf = pd.DataFrame({""A"": [10,20,30], ""B"": [20, 30, 10]})\n\ndef fx(x):\n    return x * x\n\nprint(df)\ndf[\'newcolumn\'] = df.A.apply(fx)\nprint(df)\ndef fxy(x, y):\n    return x * y\n']";"['import pandas as pd\ndf = pd.DataFrame({""A"": [10,20,30], ""B"": [20, 30, 10]})\n\ndef fx(x):\n    return x * x\n\nprint(df)\ndf[\'newcolumn\'] = df.A.apply(fx)\nprint(df)\n', 'def fxy(x, y):\n    return x * y\n']";"['pandas', 'import pandas as pd\ndf = pd.DataFrame({""A"": [10,20,30], ""B"": [20, 30, 10]})\n\ndef fx(x):\n    return x * x\n\nprint(df)\ndf[\'newcolumn\'] = df.A.apply(fx)\nprint(df)\n', 'def fxy(x, y):\n    return x * y\n']";"['import pandas as pd\ndf = pd.DataFrame({""A"": [10,20,30], ""B"": [20, 30, 10]})\n\n\nprint(df)\ndf[\'newcolumn\'] = df.A.apply(fx)\nprint(df)\n']";"['import pandas as pd\ndf = pd.DataFrame({""A"": [10,20,30], ""B"": [20, 30, 10]})\n\n\nprint(df)\ndf[\'newcolumn\'] = df.A.apply(fx)\nprint(df)\n']";False;"['import pandas as pd\nimport pandas as pd\ndf = pd.DataFrame({""A"": [10,20,30], ""B"": [20, 30, 10]})\n\n\nprint(df)\ndf[\'newcolumn\'] = df.A.apply(fx)\nprint(df)\n']";False;2;3;"[""name 'df' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess'];2;3;"[""name 'df' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess'];2;3;"[""'DataFrame' object has no attribute 'A'"", 'Sucess', 'Sucess']";['AttributeError', 'Sucess', 'Sucess']
554;554;554;554;5.0;2;19917545;;1;14;<python><python-2.7><pandas>;Comparing two pandas dataframes for differences;29225.0;"[""csvdata = pandas.read_csv('csvfile.csv')\ncsvdata_old = csvdata\n\n# ... do stuff with csvdata dataframe\n\nif csvdata_old != csvdata:\n    csvdata.to_csv('csvfile.csv', index=False)\n""]";"[""csvdata = pandas.read_csv('csvfile.csv')\ncsvdata_old = csvdata\n\n# ... do stuff with csvdata dataframe\n\nif csvdata_old != csvdata:\n    csvdata.to_csv('csvfile.csv', index=False)\n""]";"[""csvdata = pandas.read_csv('csvfile.csv')\ncsvdata_old = csvdata\n\n# ... do stuff with csvdata dataframe\n\nif csvdata_old != csvdata:\n    csvdata.to_csv('csvfile.csv', index=False)\n""]";"[""csvdata = pandas.read_csv('csvfile.csv')\ncsvdata_old = csvdata\n\n# ... do stuff with csvdata dataframe\n\n""]";"[""csvdata = pandas.read_csv('csvfile.csv')\ncsvdata_old = csvdata\n\n# ... do stuff with csvdata dataframe\n\n""]";False;"[""import pandas as pd\ncsvdata = pandas.read_csv('csvfile.csv')\ncsvdata_old = csvdata\n\n# ... do stuff with csvdata dataframe\n\n""]";False;0;1;"[""name 'csvdata' is not defined""]";['NameError'];0;1;"[""name 'csvdata' is not defined""]";['NameError'];0;1;"[""name 'csvdata' is not defined""]";['NameError']
555;555;555;555;4.0;0;19928284;;1;23;<pandas>;Pandas dataframe values equality test;7575.0;"[""dates = pd.date_range('20130101', periods=6)\n\ndf1 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\ndf2 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n\nprint df1\nprint df2\nself.assertItemsEqual(df1, df2)\n""]";"[""dates = pd.date_range('20130101', periods=6)\n\ndf1 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\ndf2 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n\nprint df1\nprint df2\nself.assertItemsEqual(df1, df2)\n""]";"[""dates = pd.date_range('20130101', periods=6)\n\ndf1 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\ndf2 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n\nprint df1\nprint df2\nself.assertItemsEqual(df1, df2)\n""]";"[""dates = pd.date_range('20130101', periods=6)\n\ndf1 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\ndf2 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n\nself.assertItemsEqual(df1, df2)\n""]";"[""import pandas as pd\ndates = pd.date_range('20130101', periods=6)\n\ndf1 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\ndf2 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n\nself.assertItemsEqual(df1, df2)\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\ndates = pd.date_range('20130101', periods=6)\n\ndf1 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\ndf2 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n\nself.assertItemsEqual(df1, df2)\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
556;556;556;556;1.0;0;19952290;;1;11;<python><matplotlib><pandas>;How to align the bar and line in matplotlib two y-axes chart?;7826.0;"["">>> df\n                   sales  net_pft  sales_gr  net_pft_gr\nSTK_ID RPT_Date                                        \n600809 20120331  22.1401   4.9253    0.1824     -0.0268\n       20120630  38.1565   7.8684    0.3181      0.1947\n       20120930  52.5098  12.4338    0.4735      0.7573\n       20121231  64.7876  13.2731    0.4435      0.7005\n       20130331  27.9517   7.5182    0.2625      0.5264\n       20130630  40.6460   9.8572    0.0652      0.2528\n       20130930  53.0501  11.8605    0.0103     -0.0461\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax = df[['sales','net_pft']].unstack('STK_ID').plot(kind='bar', use_index=True)\nax2 = ax.twinx()\nax2.plot(df[['sales_gr','net_pft_gr']].values, linestyle='-', marker='o', linewidth=2.0)\n""]";"['>>> df\n                   sales  net_pft  sales_gr  net_pft_gr\nSTK_ID RPT_Date                                        \n600809 20120331  22.1401   4.9253    0.1824     -0.0268\n       20120630  38.1565   7.8684    0.3181      0.1947\n       20120930  52.5098  12.4338    0.4735      0.7573\n       20121231  64.7876  13.2731    0.4435      0.7005\n       20130331  27.9517   7.5182    0.2625      0.5264\n       20130630  40.6460   9.8572    0.0652      0.2528\n       20130930  53.0501  11.8605    0.0103     -0.0461\n', ""import matplotlib.pyplot as plt\nfig = plt.figure()\nax = df[['sales','net_pft']].unstack('STK_ID').plot(kind='bar', use_index=True)\nax2 = ax.twinx()\nax2.plot(df[['sales_gr','net_pft_gr']].values, linestyle='-', marker='o', linewidth=2.0)\n""]";"['>>> df\n                   sales  net_pft  sales_gr  net_pft_gr\nSTK_ID RPT_Date                                        \n600809 20120331  22.1401   4.9253    0.1824     -0.0268\n       20120630  38.1565   7.8684    0.3181      0.1947\n       20120930  52.5098  12.4338    0.4735      0.7573\n       20121231  64.7876  13.2731    0.4435      0.7005\n       20130331  27.9517   7.5182    0.2625      0.5264\n       20130630  40.6460   9.8572    0.0652      0.2528\n       20130930  53.0501  11.8605    0.0103     -0.0461\n', ""df[['sales','net_pft']].unstack('STK_ID').plot(kind='bar', use_index=True)"", ""df[['sales_gr','net_pft_gr']].plot(kind='line', use_index=True)"", ""import matplotlib.pyplot as plt\nfig = plt.figure()\nax = df[['sales','net_pft']].unstack('STK_ID').plot(kind='bar', use_index=True)\nax2 = ax.twinx()\nax2.plot(df[['sales_gr','net_pft_gr']].values, linestyle='-', marker='o', linewidth=2.0)\n""]";"[""import matplotlib.pyplot as plt\nfig = plt.figure()\nax = df[['sales','net_pft']].unstack('STK_ID').plot(kind='bar', use_index=True)\nax2 = ax.twinx()\nax2.plot(df[['sales_gr','net_pft_gr']].values, linestyle='-', marker='o', linewidth=2.0)\n""]";"[""import matplotlib.pyplot as plt\nfig = plt.figure()\nax = df[['sales','net_pft']].unstack('STK_ID').plot(kind='bar', use_index=True)\nax2 = ax.twinx()\nax2.plot(df[['sales_gr','net_pft_gr']].values, linestyle='-', marker='o', linewidth=2.0)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax = df[['sales','net_pft']].unstack('STK_ID').plot(kind='bar', use_index=True)\nax2 = ax.twinx()\nax2.plot(df[['sales_gr','net_pft_gr']].values, linestyle='-', marker='o', linewidth=2.0)\n""]";True;0;1;"[""name 'ax2' is not defined""]";['NameError'];0;1;"[""name 'ax2' is not defined""]";['NameError'];0;1;"[""name 'ax2' is not defined""]";['NameError']
557;557;557;557;4.0;2;19960077;;1;84;<python><pandas><dataframe><sql-function>;How to implement 'in' and 'not in' for Pandas dataframe;72686.0;"[""df = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = ['UK','China']\n\n# pseudo-code:\ndf[df['countries'] not in countries]\ndf = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = pd.DataFrame({'countries':['UK','China'], 'matched':True})\n\n# IN\ndf.merge(countries,how='inner',on='countries')\n\n# NOT IN\nnot_in = df.merge(countries,how='left',on='countries')\nnot_in = not_in[pd.isnull(not_in['matched'])]\n""]";"[""df = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = ['UK','China']\n\n# pseudo-code:\ndf[df['countries'] not in countries]\n"", ""df = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = pd.DataFrame({'countries':['UK','China'], 'matched':True})\n\n# IN\ndf.merge(countries,how='inner',on='countries')\n\n# NOT IN\nnot_in = df.merge(countries,how='left',on='countries')\nnot_in = not_in[pd.isnull(not_in['matched'])]\n""]";"['IN', 'NOT IN', ""df = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = ['UK','China']\n\n# pseudo-code:\ndf[df['countries'] not in countries]\n"", ""df = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = pd.DataFrame({'countries':['UK','China'], 'matched':True})\n\n# IN\ndf.merge(countries,how='inner',on='countries')\n\n# NOT IN\nnot_in = df.merge(countries,how='left',on='countries')\nnot_in = not_in[pd.isnull(not_in['matched'])]\n""]";"[""df = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = ['UK','China']\n\n# pseudo-code:\ndf[df['countries'] not in countries]\ndf = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = pd.DataFrame({'countries':['UK','China'], 'matched':True})\n\n# IN\ndf.merge(countries,how='inner',on='countries')\n\n# NOT IN\nnot_in = df.merge(countries,how='left',on='countries')\nnot_in = not_in[pd.isnull(not_in['matched'])]\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = ['UK','China']\n\n# pseudo-code:\ndf[df['countries'] not in countries]\ndf = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = pd.DataFrame({'countries':['UK','China'], 'matched':True})\n\n# IN\ndf.merge(countries,how='inner',on='countries')\n\n# NOT IN\nnot_in = df.merge(countries,how='left',on='countries')\nnot_in = not_in[pd.isnull(not_in['matched'])]\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = ['UK','China']\n\n# pseudo-code:\ndf[df['countries'] not in countries]\ndf = pd.DataFrame({'countries':['US','UK','Germany','China']})\ncountries = pd.DataFrame({'countries':['UK','China'], 'matched':True})\n\n# IN\ndf.merge(countries,how='inner',on='countries')\n\n# NOT IN\nnot_in = df.merge(countries,how='left',on='countries')\nnot_in = not_in[pd.isnull(not_in['matched'])]\n""]";False;1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
558;558;558;558;2.0;3;19961490;;1;43;<python><python-2.7><pandas>;Construct pandas DataFrame from list of tuples;96105.0;"[""data = [\n('r1', 'c1', avg11, stdev11),\n('r1', 'c2', avg12, stdev12),\n('r2', 'c1', avg21, stdev21),\n('r2', 'c2', avg22, stdev22)\n]\n""]";"[""data = [\n('r1', 'c1', avg11, stdev11),\n('r1', 'c2', avg12, stdev12),\n('r2', 'c1', avg21, stdev21),\n('r2', 'c2', avg22, stdev22)\n]\n""]";"[""data = [\n('r1', 'c1', avg11, stdev11),\n('r1', 'c2', avg12, stdev12),\n('r2', 'c1', avg21, stdev21),\n('r2', 'c2', avg22, stdev22)\n]\n"", 'pandas.DataFrame([x[1:] for x in data], index = [x[0] for x in data])']";"[""('r1', 'c1', avg11, stdev11),\n('r1', 'c2', avg12, stdev12),\n('r2', 'c1', avg21, stdev21),\n('r2', 'c2', avg22, stdev22)\n""]";"[""('r1', 'c1', avg11, stdev11),\n('r1', 'c2', avg12, stdev12),\n('r2', 'c1', avg21, stdev21),\n('r2', 'c2', avg22, stdev22)\n""]";False;"[""import pandas as pd\n('r1', 'c1', avg11, stdev11),\n('r1', 'c2', avg12, stdev12),\n('r2', 'c1', avg21, stdev21),\n('r2', 'c2', avg22, stdev22)\n""]";False;1;2;"['Sucess', ""name 'pandas' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'pandas' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'pandas' is not defined""]";['Sucess', 'NameError']
559;559;559;559;5.0;0;19966018;;1;18;<python><pandas>;Pandas: filling missing values by mean in each group;6968.0;"[""df = pd.DataFrame({'value': [1, np.nan, np.nan, 2, 3, 1, 3, np.nan, 3], 'name': ['A','A', 'B','B','B','B', 'C','C','C']})\n\n  name  value\n0    A      1\n1    A    NaN\n2    B    NaN\n3    B      2\n4    B      3\n5    B      1\n6    C      3\n7    C    NaN\n8    C      3\n      name  value\n0    A      1\n1    A      1\n2    B      2\n3    B      2\n4    B      3\n5    B      1\n6    C      3\n7    C      3\n8    C      3\ngrouped = df.groupby('name').mean()\n""]";"[""df = pd.DataFrame({'value': [1, np.nan, np.nan, 2, 3, 1, 3, np.nan, 3], 'name': ['A','A', 'B','B','B','B', 'C','C','C']})\n\n  name  value\n0    A      1\n1    A    NaN\n2    B    NaN\n3    B      2\n4    B      3\n5    B      1\n6    C      3\n7    C    NaN\n8    C      3\n"", '      name  value\n0    A      1\n1    A      1\n2    B      2\n3    B      2\n4    B      3\n5    B      1\n6    C      3\n7    C      3\n8    C      3\n', ""grouped = df.groupby('name').mean()\n""]";"[""df = pd.DataFrame({'value': [1, np.nan, np.nan, 2, 3, 1, 3, np.nan, 3], 'name': ['A','A', 'B','B','B','B', 'C','C','C']})\n\n  name  value\n0    A      1\n1    A    NaN\n2    B    NaN\n3    B      2\n4    B      3\n5    B      1\n6    C      3\n7    C    NaN\n8    C      3\n"", '      name  value\n0    A      1\n1    A      1\n2    B      2\n3    B      2\n4    B      3\n5    B      1\n6    C      3\n7    C      3\n8    C      3\n', ""grouped = df.groupby('name').mean()\n""]";"[""df = pd.DataFrame({'value': [1, np.nan, np.nan, 2, 3, 1, 3, np.nan, 3], 'name': ['A','A', 'B','B','B','B', 'C','C','C']})\n\ngrouped = df.groupby('name').mean()\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'value': [1, np.nan, np.nan, 2, 3, 1, 3, np.nan, 3], 'name': ['A','A', 'B','B','B','B', 'C','C','C']})\n\ngrouped = df.groupby('name').mean()\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame({'value': [1, np.nan, np.nan, 2, 3, 1, 3, np.nan, 3], 'name': ['A','A', 'B','B','B','B', 'C','C','C']})\n\ngrouped = df.groupby('name').mean()\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
560;560;560;560;2.0;0;19981518;;1;15;<python><pandas>;Sorting Multi-Index to full depth (Pandas);6570.0;"[""df.set_index(['fileName','phrase'])\ndf.ix['somePath','somePhrase']\n""]";"[""df.set_index(['fileName','phrase'])\ndf.ix['somePath','somePhrase']\n""]";"[""df.set_index(['fileName','phrase'])\ndf.ix['somePath','somePhrase']\n"", 'df.index.lexsort_depth', 'MultiIndex lexsort depth 1, key was length 2']";"[""df.set_index(['fileName','phrase'])\ndf.ix['somePath','somePhrase']\n""]";"[""df.set_index(['fileName','phrase'])\ndf.ix['somePath','somePhrase']\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf.set_index(['fileName','phrase'])\ndf.ix['somePath','somePhrase']\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'Index' object has no attribute 'lexsort_depth'""]";['AttributeError']
561;561;561;561;4.0;0;19991445;;1;60;<python><pandas><scikit-learn><regression><statsmodels>;Run an OLS regression with Pandas Data Frame;82103.0;"['import pandas as pd\ndf = pd.DataFrame({""A"": [10,20,30,40,50], \n                   ""B"": [20, 30, 10, 40, 50], \n                   ""C"": [32, 234, 23, 23, 42523]})\n']";"['import pandas as pd\ndf = pd.DataFrame({""A"": [10,20,30,40,50], \n                   ""B"": [20, 30, 10, 40, 50], \n                   ""C"": [32, 234, 23, 23, 42523]})\n']";"['pandas', 'import pandas as pd\ndf = pd.DataFrame({""A"": [10,20,30,40,50], \n                   ""B"": [20, 30, 10, 40, 50], \n                   ""C"": [32, 234, 23, 23, 42523]})\n', 'ols(A ~ B + C, data = df)', 'scikit-learn']";['import pandas as pd\n'];['import pandas as pd\n'];False;['import pandas as pd\nimport pandas as pd\n'];False;2;4;"['Sucess', 'Sucess', ""name 'LinearRegression' is not defined"", ""No module named 'sklearn'""]";['Sucess', 'Sucess', 'NameError', 'ImportError'];2;4;"['Sucess', 'Sucess', ""name 'LinearRegression' is not defined"", ""No module named 'sklearn'""]";['Sucess', 'Sucess', 'NameError', 'ImportError'];2;4;"['Sucess', 'Sucess', ""name 'LinearRegression' is not defined"", ""No module named 'sklearn'""]";['Sucess', 'Sucess', 'NameError', 'ImportError']
562;562;562;562;5.0;2;20003290;;1;22;<python><csv><numpy><floating-point><pandas>;Print different precision by column with pandas.DataFrame.to_csv()?;16557.0;"[""In [53]: df_data[:5]\nOut[53]: \n    year  month  day       lats       lons  vals\n0   2012      6   16  81.862745 -29.834254   0.0\n1   2012      6   16  81.862745 -29.502762   0.1\n2   2012      6   16  81.862745 -29.171271   0.0\n3   2012      6   16  81.862745 -28.839779   0.2\n4   2012      6   16  81.862745 -28.508287   0.0\ndf_data.to_csv(outfile, index=False,\n                   header=False, float_format='%11.6f')\n2012,6,16,  81.862745, -29.834254,   0.000000\n2012,6,16,  81.862745, -29.502762,   0.100000\n2012,6,16,  81.862745, -29.171270,   0.000000\n2012,6,16,  81.862745, -28.839779,   0.200000\n2012,6,16,  81.862745, -28.508287,   0.000000\n""]";"['In [53]: df_data[:5]\nOut[53]: \n    year  month  day       lats       lons  vals\n0   2012      6   16  81.862745 -29.834254   0.0\n1   2012      6   16  81.862745 -29.502762   0.1\n2   2012      6   16  81.862745 -29.171271   0.0\n3   2012      6   16  81.862745 -28.839779   0.2\n4   2012      6   16  81.862745 -28.508287   0.0\n', ""df_data.to_csv(outfile, index=False,\n                   header=False, float_format='%11.6f')\n"", '2012,6,16,  81.862745, -29.834254,   0.000000\n2012,6,16,  81.862745, -29.502762,   0.100000\n2012,6,16,  81.862745, -29.171270,   0.000000\n2012,6,16,  81.862745, -28.839779,   0.200000\n2012,6,16,  81.862745, -28.508287,   0.000000\n']";"['pandas', 'pandas', 'In [53]: df_data[:5]\nOut[53]: \n    year  month  day       lats       lons  vals\n0   2012      6   16  81.862745 -29.834254   0.0\n1   2012      6   16  81.862745 -29.502762   0.1\n2   2012      6   16  81.862745 -29.171271   0.0\n3   2012      6   16  81.862745 -28.839779   0.2\n4   2012      6   16  81.862745 -28.508287   0.0\n', 'float_format', ""df_data.to_csv(outfile, index=False,\n                   header=False, float_format='%11.6f')\n"", 'vals', '2012,6,16,  81.862745, -29.834254,   0.000000\n2012,6,16,  81.862745, -29.502762,   0.100000\n2012,6,16,  81.862745, -29.171270,   0.000000\n2012,6,16,  81.862745, -28.839779,   0.200000\n2012,6,16,  81.862745, -28.508287,   0.000000\n']";['df_data[:5]\n'];['df_data[:5]\n'];False;['import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\ndf_data[:5]\n'];True;0;1;"[""name 'df_data' is not defined""]";['NameError'];0;1;"[""name 'df_data' is not defined""]";['NameError'];0;1;"[""name 'df_data' is not defined""]";['NameError']
563;563;563;563;2.0;0;20012507;;1;14;<python-2.7><pandas><dataframe>;Pandas: A clean way to initialize data frame with a list of namedtuple;5253.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'namedtuple' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'namedtuple' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'namedtuple' is not defined""]";['Sucess', 'NameError']
564;564;564;564;4.0;0;20024584;;1;12;<python><pandas><scikit-learn>;Vectorizing a Pandas dataframe for Scikit-Learn;11256.0;['> my_dataframe\n\ncol1   col2\nA      foo\nB      bar\nC      something\nA      foo\nA      bar\nB      foo\n'];['> my_dataframe\n\ncol1   col2\nA      foo\nB      bar\nC      something\nA      foo\nA      bar\nB      foo\n'];['> my_dataframe\n\ncol1   col2\nA      foo\nB      bar\nC      something\nA      foo\nA      bar\nB      foo\n', 'my_dataframe', 'DictVectorizer'];['\n'];['\n'];False;['import pandas as pd\n\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
565;565;565;565;3.0;5;20025325;;1;29;<python><pandas><indexing><dataframe>;Apply Function on DataFrame Index;15303.0;"['pd.DataFrame({""Month"": df.reset_index().Date.apply(foo)})\n']";"['pd.DataFrame({""Month"": df.reset_index().Date.apply(foo)})\n']";"['DataFrame', 'pd.DataFrame({""Month"": df.reset_index().Date.apply(foo)})\n', 'Date', 'foo']";"['pd.DataFrame({""Month"": df.reset_index().Date.apply(foo)})\n']";"['import pandas as pd\npd.DataFrame({""Month"": df.reset_index().Date.apply(foo)})\n']";True;"['import pandas as pd\ndf = pd.DataFrame()\npd.DataFrame({""Month"": df.reset_index().Date.apply(foo)})\n']";True;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
566;566;566;566;1.0;6;20025882;;1;20;<python><pandas><dataframe>;Append string to the start of each value in a said column of a pandas dataframe (elegantly);18507.0;"[""df.ix[(df['col'] != False), 'col'] = 'str'+df[(df['col'] != False), 'col']\n    col \n1     a\n2     0\n       col \n1     stra\n2     str0\n""]";"[""df.ix[(df['col'] != False), 'col'] = 'str'+df[(df['col'] != False), 'col']\n"", '    col \n1     a\n2     0\n', '       col \n1     stra\n2     str0\n']";"[""df.ix[(df['col'] != False), 'col'] = 'str'+df[(df['col'] != False), 'col']\n"", '    col \n1     a\n2     0\n', '       col \n1     stra\n2     str0\n']";"[""df.ix[(df['col'] != False), 'col'] = 'str'+df[(df['col'] != False), 'col']\n""]";"[""df.ix[(df['col'] != False), 'col'] = 'str'+df[(df['col'] != False), 'col']\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf.ix[(df['col'] != False), 'col'] = 'str'+df[(df['col'] != False), 'col']\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'col'""]";['KeyError']
567;567;567;567;2.0;0;20033111;;1;37;<python><python-2.7><pandas><max>;Python Pandas max value of selected columns;43953.0;"[""data = {'name' : ['bill', 'joe', 'steve'],\n    'test1' : [85, 75, 85],\n    'test2' : [35, 45, 83],\n     'test3' : [51, 61, 45]}\nframe = pd.DataFrame(data)\n name test1 test2 test3 HighScore\n bill  75    75    85    85\n joe   35    45    83    83 \n steve  51   61    45    61 \nframe['HighScore'] = max(data['test1'], data['test2'], data['test3'])\n""]";"[""data = {'name' : ['bill', 'joe', 'steve'],\n    'test1' : [85, 75, 85],\n    'test2' : [35, 45, 83],\n     'test3' : [51, 61, 45]}\nframe = pd.DataFrame(data)\n"", ' name test1 test2 test3 HighScore\n bill  75    75    85    85\n joe   35    45    83    83 \n steve  51   61    45    61 \n', ""frame['HighScore'] = max(data['test1'], data['test2'], data['test3'])\n""]";"[""data = {'name' : ['bill', 'joe', 'steve'],\n    'test1' : [85, 75, 85],\n    'test2' : [35, 45, 83],\n     'test3' : [51, 61, 45]}\nframe = pd.DataFrame(data)\n"", ' name test1 test2 test3 HighScore\n bill  75    75    85    85\n joe   35    45    83    83 \n steve  51   61    45    61 \n', ""frame['HighScore'] = max(data['test1'], data['test2'], data['test3'])\n""]";"[""frame = pd.DataFrame(data)\nframe['HighScore'] = max(data['test1'], data['test2'], data['test3'])\n""]";"[""import pandas as pd\nframe = pd.DataFrame(data)\nframe['HighScore'] = max(data['test1'], data['test2'], data['test3'])\n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\nframe = pd.DataFrame(data)\nframe['HighScore'] = max(data['test1'], data['test2'], data['test3'])\n""]";True;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
568;568;568;568;1.0;0;20035518;;1;18;<python><pandas><ipython-notebook>;Insert a link inside a pandas table;5078.0;"['In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame(range(5), columns=[\'a\'])\n\nIn [3]: df[\'b\'] = df[\'a\'].apply(lambda x: \'http://example.com/{0}\'.format(x))\n\nIn [4]: df\nOut[4]:\n   a                     b\n0  0  http://example.com/0\n1  1  http://example.com/1\n2  2  http://example.com/2\n3  3  http://example.com/3\n4  4  http://example.com/4\nIn [5]: from IPython.display import HTML\n\nIn [6]: df[\'b\'] = df[\'a\'].apply(lambda x:HTML(\'http://example.com/{0}\'.format(x)))\n\nIn [7]: df\nOut[7]:\n   a                                                 b\n0  0  <IPython.core.display.HTML object at 0x0481E530>\n1  1  <IPython.core.display.HTML object at 0x0481E770>\n2  2  <IPython.core.display.HTML object at 0x0481E7B0>\n3  3  <IPython.core.display.HTML object at 0x0481E810>\n4  4  <IPython.core.display.HTML object at 0x0481EA70>\n<a href=""aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0"">xxx</a>\n<a href=""aaaaaaaaaaaaaaaaaaaaaa...\npd.set_printoptions(max_colwidth=-1)\n']";"[""In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame(range(5), columns=['a'])\n\nIn [3]: df['b'] = df['a'].apply(lambda x: 'http://example.com/{0}'.format(x))\n\nIn [4]: df\nOut[4]:\n   a                     b\n0  0  http://example.com/0\n1  1  http://example.com/1\n2  2  http://example.com/2\n3  3  http://example.com/3\n4  4  http://example.com/4\n"", ""In [5]: from IPython.display import HTML\n\nIn [6]: df['b'] = df['a'].apply(lambda x:HTML('http://example.com/{0}'.format(x)))\n\nIn [7]: df\nOut[7]:\n   a                                                 b\n0  0  <IPython.core.display.HTML object at 0x0481E530>\n1  1  <IPython.core.display.HTML object at 0x0481E770>\n2  2  <IPython.core.display.HTML object at 0x0481E7B0>\n3  3  <IPython.core.display.HTML object at 0x0481E810>\n4  4  <IPython.core.display.HTML object at 0x0481EA70>\n"", '<a href=""aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0"">xxx</a>\n', '<a href=""aaaaaaaaaaaaaaaaaaaaaa...\n', 'pd.set_printoptions(max_colwidth=-1)\n']";"[""In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame(range(5), columns=['a'])\n\nIn [3]: df['b'] = df['a'].apply(lambda x: 'http://example.com/{0}'.format(x))\n\nIn [4]: df\nOut[4]:\n   a                     b\n0  0  http://example.com/0\n1  1  http://example.com/1\n2  2  http://example.com/2\n3  3  http://example.com/3\n4  4  http://example.com/4\n"", ""In [5]: from IPython.display import HTML\n\nIn [6]: df['b'] = df['a'].apply(lambda x:HTML('http://example.com/{0}'.format(x)))\n\nIn [7]: df\nOut[7]:\n   a                                                 b\n0  0  <IPython.core.display.HTML object at 0x0481E530>\n1  1  <IPython.core.display.HTML object at 0x0481E770>\n2  2  <IPython.core.display.HTML object at 0x0481E7B0>\n3  3  <IPython.core.display.HTML object at 0x0481E810>\n4  4  <IPython.core.display.HTML object at 0x0481EA70>\n"", '<a href=""aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0"">xxx</a>\n', '<a href=""aaaaaaaaaaaaaaaaaaaaaa...\n', 'pd.set_printoptions(max_colwidth=-1)\n']";['import pandas as pd\n\n\n\nfrom IPython.display import HTML\n\n\n'];['import pandas as pd\n\n\n\nfrom IPython.display import HTML\n\n\n'];False;['import pandas as pd\nimport pandas as pd\n\n\n\nfrom IPython.display import HTML\n\n\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
569;569;569;569;4.0;0;20037430;;1;12;<python><json><pandas>;pandas - reading multiple JSON records into dataframe;12000.0;"['import json\nimport pandas as pd\n\ntest=\'\'\'{""a"":1,""b"":2}\n{""a"":3,""b"":4}\'\'\'\n#df=pd.read_json(test,orient=\'records\') doesn\'t work, expects []\n\nl=[ json.loads(l) for l in test.splitlines()]\ndf=pd.DataFrame(l)\n']";"['import json\nimport pandas as pd\n\ntest=\'\'\'{""a"":1,""b"":2}\n{""a"":3,""b"":4}\'\'\'\n#df=pd.read_json(test,orient=\'records\') doesn\'t work, expects []\n\nl=[ json.loads(l) for l in test.splitlines()]\ndf=pd.DataFrame(l)\n']";"['import json\nimport pandas as pd\n\ntest=\'\'\'{""a"":1,""b"":2}\n{""a"":3,""b"":4}\'\'\'\n#df=pd.read_json(test,orient=\'records\') doesn\'t work, expects []\n\nl=[ json.loads(l) for l in test.splitlines()]\ndf=pd.DataFrame(l)\n']";"[""import json\nimport pandas as pd\n\n#df=pd.read_json(test,orient='records') doesn't work, expects []\n\nl=[ json.loads(l) for l in test.splitlines()]\ndf=pd.DataFrame(l)\n""]";"[""import json\nimport pandas as pd\n\n#df=pd.read_json(test,orient='records') doesn't work, expects []\n\nl=[ json.loads(l) for l in test.splitlines()]\ndf=pd.DataFrame(l)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport json\nimport pandas as pd\n\n#df=pd.read_json(test,orient='records') doesn't work, expects []\n\nl=[ json.loads(l) for l in test.splitlines()]\ndf=pd.DataFrame(l)\n""]";True;1;3;"[""name 'test' is not defined"", 'Sucess', ""name 'pd' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""name 'test' is not defined"", 'Sucess', ""name 'jsonfile' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""name 'test' is not defined"", 'Sucess', ""name 'jsonfile' is not defined""]";['NameError', 'Sucess', 'NameError']
570;570;570;570;2.0;6;20055257;;1;11;<python><windows-8><pandas><64bit><pyodbc>;PYODBC to Pandas - DataFrame not working - Shape of passed values is (x,y), indices imply (w,z);10039.0;"[""columns = [column[0] for column in cursor.description]\ntemp = cursor.fetchall()\ndata = pandas.DataFrame(temp,columns=columns)\nsql = 'Select * form TABLE'\ncursor.execute(sql)\ncolumns = [column[0] for column in cursor.description]\ndata    = cursor.fetchall()\n        con.close()\n            results = DataFrame(data, columns=columns)\nipdb> type(data)\n<type 'list'>\nipdb> np.shape(data)\n(1540, 51)\nipdb> type(data[0])\n<type 'pyodbc.Row'>\nipdb> DataFrame([1,2,3],columns=['a','b','c'])\nipdb> DataFrame([[1,2,3]],columns=['a','b','c'])\nipdb> DataFrame([data[0]], columns=columns)\n*** ValueError: Shape of passed values is (1, 1), indices imply (51, 1)\nipdb> DataFrame(data[0], columns=columns)\n*** PandasError: DataFrame constructor not properly called!\n""]";"['columns = [column[0] for column in cursor.description]\ntemp = cursor.fetchall()\ndata = pandas.DataFrame(temp,columns=columns)\n', ""sql = 'Select * form TABLE'\ncursor.execute(sql)\ncolumns = [column[0] for column in cursor.description]\ndata    = cursor.fetchall()\n        con.close()\n            results = DataFrame(data, columns=columns)\n"", ""ipdb> type(data)\n<type 'list'>\nipdb> np.shape(data)\n(1540, 51)\nipdb> type(data[0])\n<type 'pyodbc.Row'>\n"", ""ipdb> DataFrame([1,2,3],columns=['a','b','c'])\n"", ""ipdb> DataFrame([[1,2,3]],columns=['a','b','c'])\n"", 'ipdb> DataFrame([data[0]], columns=columns)\n*** ValueError: Shape of passed values is (1, 1), indices imply (51, 1)\n', 'ipdb> DataFrame(data[0], columns=columns)\n*** PandasError: DataFrame constructor not properly called!\n']";"['columns = [column[0] for column in cursor.description]\ntemp = cursor.fetchall()\ndata = pandas.DataFrame(temp,columns=columns)\n', ""sql = 'Select * form TABLE'\ncursor.execute(sql)\ncolumns = [column[0] for column in cursor.description]\ndata    = cursor.fetchall()\n        con.close()\n            results = DataFrame(data, columns=columns)\n"", ""ipdb> type(data)\n<type 'list'>\nipdb> np.shape(data)\n(1540, 51)\nipdb> type(data[0])\n<type 'pyodbc.Row'>\n"", ""ipdb> DataFrame([1,2,3],columns=['a','b','c'])\n"", ""ipdb> DataFrame([[1,2,3]],columns=['a','b','c'])\n"", 'ipdb> DataFrame([data[0]], columns=columns)\n*** ValueError: Shape of passed values is (1, 1), indices imply (51, 1)\n', 'ipdb> DataFrame(data[0], columns=columns)\n*** PandasError: DataFrame constructor not properly called!\n']";"[""columns = [column[0] for column in cursor.description]\ntemp = cursor.fetchall()\ndata = pandas.DataFrame(temp,columns=columns)\nsql = 'Select * form TABLE'\ncursor.execute(sql)\ncolumns = [column[0] for column in cursor.description]\ndata    = cursor.fetchall()\nipdb> type(data)\nipdb> np.shape(data)\n(1540, 51)\nipdb> type(data[0])\nipdb> DataFrame([1,2,3],columns=['a','b','c'])\nipdb> DataFrame([[1,2,3]],columns=['a','b','c'])\nipdb> DataFrame([data[0]], columns=columns)\nipdb> DataFrame(data[0], columns=columns)\n""]";"[""from pandas import DataFrame\ncolumns = [column[0] for column in cursor.description]\ntemp = cursor.fetchall()\ndata = pandas.DataFrame(temp,columns=columns)\nsql = 'Select * form TABLE'\ncursor.execute(sql)\ncolumns = [column[0] for column in cursor.description]\ndata    = cursor.fetchall()\nipdb> type(data)\nipdb> np.shape(data)\n(1540, 51)\nipdb> type(data[0])\nipdb> DataFrame([1,2,3],columns=['a','b','c'])\nipdb> DataFrame([[1,2,3]],columns=['a','b','c'])\nipdb> DataFrame([data[0]], columns=columns)\nipdb> DataFrame(data[0], columns=columns)\n""]";True;"[""import pandas as pd\ncolumns = [column[0] for column in cursor.description]\ntemp = cursor.fetchall()\ndata = pandas.DataFrame(temp,columns=columns)\nsql = 'Select * form TABLE'\ncursor.execute(sql)\ncolumns = [column[0] for column in cursor.description]\ndata    = cursor.fetchall()\nipdb> type(data)\nipdb> np.shape(data)\n(1540, 51)\nipdb> type(data[0])\nipdb> DataFrame([1,2,3],columns=['a','b','c'])\nipdb> DataFrame([[1,2,3]],columns=['a','b','c'])\nipdb> DataFrame([data[0]], columns=columns)\nipdb> DataFrame(data[0], columns=columns)\n""]";False;0;1;"[""No module named 'pyodbc'""]";['ImportError'];0;1;"[""No module named 'pyodbc'""]";['ImportError'];0;1;"[""No module named 'pyodbc'""]";['ImportError']
571;571;571;571;3.0;0;20067636;;1;38;<python><pandas><dataframe>;Pandas dataframe get first row of each group;35554.0;"['df = pd.DataFrame({\'id\' : [1,1,1,2,2,3,3,3,3,4,4,5,6,6,6,7,7],\n                \'value\'  : [""first"",""second"",""second"",""first"",\n                            ""second"",""first"",""third"",""fourth"",\n                            ""fifth"",""second"",""fifth"",""first"",\n                            ""first"",""second"",""third"",""fourth"",""fifth""]})\n        id   value\n0        1   first\n1        1  second\n2        1  second\n3        2   first\n4        2  second\n5        3   first\n6        3   third\n7        3  fourth\n8        3   fifth\n9        4  second\n10       4   fifth\n11       5   first\n12       6   first\n13       6  second\n14       6   third\n15       7  fourth\n16       7   fifth\n    id   value\n     1   first\n     2   first\n     3   first\n     4  second\n     5  first\n     6  first\n     7  fourth\nIn [25]: for index, row in df.iterrows():\n   ....:     df2 = pd.DataFrame(df.groupby([\'id\',\'value\']).reset_index().ix[0])\n']";"['df = pd.DataFrame({\'id\' : [1,1,1,2,2,3,3,3,3,4,4,5,6,6,6,7,7],\n                \'value\'  : [""first"",""second"",""second"",""first"",\n                            ""second"",""first"",""third"",""fourth"",\n                            ""fifth"",""second"",""fifth"",""first"",\n                            ""first"",""second"",""third"",""fourth"",""fifth""]})\n', '        id   value\n0        1   first\n1        1  second\n2        1  second\n3        2   first\n4        2  second\n5        3   first\n6        3   third\n7        3  fourth\n8        3   fifth\n9        4  second\n10       4   fifth\n11       5   first\n12       6   first\n13       6  second\n14       6   third\n15       7  fourth\n16       7   fifth\n', '    id   value\n     1   first\n     2   first\n     3   first\n     4  second\n     5  first\n     6  first\n     7  fourth\n', ""In [25]: for index, row in df.iterrows():\n   ....:     df2 = pd.DataFrame(df.groupby(['id','value']).reset_index().ix[0])\n""]";"['DataFrame', 'df = pd.DataFrame({\'id\' : [1,1,1,2,2,3,3,3,3,4,4,5,6,6,6,7,7],\n                \'value\'  : [""first"",""second"",""second"",""first"",\n                            ""second"",""first"",""third"",""fourth"",\n                            ""fifth"",""second"",""fifth"",""first"",\n                            ""first"",""second"",""third"",""fourth"",""fifth""]})\n', '        id   value\n0        1   first\n1        1  second\n2        1  second\n3        2   first\n4        2  second\n5        3   first\n6        3   third\n7        3  fourth\n8        3   fifth\n9        4  second\n10       4   fifth\n11       5   first\n12       6   first\n13       6  second\n14       6   third\n15       7  fourth\n16       7   fifth\n', '    id   value\n     1   first\n     2   first\n     3   first\n     4  second\n     5  first\n     6  first\n     7  fourth\n', 'DataFrame', ""In [25]: for index, row in df.iterrows():\n   ....:     df2 = pd.DataFrame(df.groupby(['id','value']).reset_index().ix[0])\n""]";[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""'id'""]";['Sucess', 'KeyError']
572;572;572;572;2.0;1;20069009;;1;54;<python><pandas><greatest-n-per-group><window-functions><top-n>;Pandas good approach to get top-n records within each group;41787.0;"["">>> df = pd.DataFrame({'id':[1,1,1,2,2,2,2,3,4],'value':[1,2,3,1,2,3,4,1,1]})\n>>> df\n   id  value\n0   1      1\n1   1      2\n2   1      3\n3   2      1\n4   2      2\n5   2      3\n6   2      4\n7   3      1\n8   4      1\n   id  value\n0   1      1\n1   1      2\n3   2      1\n4   2      2\n7   3      1\n8   4      1\n>>> dfN = df.groupby('id').apply(lambda x:x['value'].reset_index()).reset_index()\n>>> dfN\n   id  level_1  index  value\n0   1        0      0      1\n1   1        1      1      2\n2   1        2      2      3\n3   2        0      3      1\n4   2        1      4      2\n5   2        2      5      3\n6   2        3      6      4\n7   3        0      7      1\n8   4        0      8      1\n>>> dfN[dfN['level_1'] <= 1][['id', 'value']]\n   id  value\n0   1      1\n1   1      2\n3   2      1\n4   2      2\n7   3      1\n8   4      1\n""]";"["">>> df = pd.DataFrame({'id':[1,1,1,2,2,2,2,3,4],'value':[1,2,3,1,2,3,4,1,1]})\n>>> df\n   id  value\n0   1      1\n1   1      2\n2   1      3\n3   2      1\n4   2      2\n5   2      3\n6   2      4\n7   3      1\n8   4      1\n"", '   id  value\n0   1      1\n1   1      2\n3   2      1\n4   2      2\n7   3      1\n8   4      1\n', "">>> dfN = df.groupby('id').apply(lambda x:x['value'].reset_index()).reset_index()\n>>> dfN\n   id  level_1  index  value\n0   1        0      0      1\n1   1        1      1      2\n2   1        2      2      3\n3   2        0      3      1\n4   2        1      4      2\n5   2        2      5      3\n6   2        3      6      4\n7   3        0      7      1\n8   4        0      8      1\n>>> dfN[dfN['level_1'] <= 1][['id', 'value']]\n   id  value\n0   1      1\n1   1      2\n3   2      1\n4   2      2\n7   3      1\n8   4      1\n""]";"["">>> df = pd.DataFrame({'id':[1,1,1,2,2,2,2,3,4],'value':[1,2,3,1,2,3,4,1,1]})\n>>> df\n   id  value\n0   1      1\n1   1      2\n2   1      3\n3   2      1\n4   2      2\n5   2      3\n6   2      4\n7   3      1\n8   4      1\n"", '   id  value\n0   1      1\n1   1      2\n3   2      1\n4   2      2\n7   3      1\n8   4      1\n', "">>> dfN = df.groupby('id').apply(lambda x:x['value'].reset_index()).reset_index()\n>>> dfN\n   id  level_1  index  value\n0   1        0      0      1\n1   1        1      1      2\n2   1        2      2      3\n3   2        0      3      1\n4   2        1      4      2\n5   2        2      5      3\n6   2        3      6      4\n7   3        0      7      1\n8   4        0      8      1\n>>> dfN[dfN['level_1'] <= 1][['id', 'value']]\n   id  value\n0   1      1\n1   1      2\n3   2      1\n4   2      2\n7   3      1\n8   4      1\n""]";[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""'id'""]";['Sucess', 'KeyError']
573;573;573;573;2.0;0;20076195;;1;27;<pandas>;what is the most efficient way of counting occurrences in pandas?;34623.0;"[""df.columns = ['word','documents','frequency']\nword_grouping = df[['word','frequency']].groupby('word')\nMaxFrequency_perWord = word_grouping[['frequency']].max().reset_index()\nMaxFrequency_perWord.columns = ['word','MaxFrequency']\nOccurrences_of_Words = word_grouping[['word']].count().reset_index()\ndf.word.describe()\n""]";"[""df.columns = ['word','documents','frequency']\n"", ""word_grouping = df[['word','frequency']].groupby('word')\nMaxFrequency_perWord = word_grouping[['frequency']].max().reset_index()\nMaxFrequency_perWord.columns = ['word','MaxFrequency']\n"", ""Occurrences_of_Words = word_grouping[['word']].count().reset_index()\n"", 'df.word.describe()\n']";"[""df.columns = ['word','documents','frequency']\n"", ""word_grouping = df[['word','frequency']].groupby('word')\nMaxFrequency_perWord = word_grouping[['frequency']].max().reset_index()\nMaxFrequency_perWord.columns = ['word','MaxFrequency']\n"", ""Occurrences_of_Words = word_grouping[['word']].count().reset_index()\n"", 'df.word.describe()\n']";"[""df.columns = ['word','documents','frequency']\nword_grouping = df[['word','frequency']].groupby('word')\nMaxFrequency_perWord = word_grouping[['frequency']].max().reset_index()\nMaxFrequency_perWord.columns = ['word','MaxFrequency']\nOccurrences_of_Words = word_grouping[['word']].count().reset_index()\ndf.word.describe()\n""]";"[""df.columns = ['word','documents','frequency']\nword_grouping = df[['word','frequency']].groupby('word')\nMaxFrequency_perWord = word_grouping[['frequency']].max().reset_index()\nMaxFrequency_perWord.columns = ['word','MaxFrequency']\nOccurrences_of_Words = word_grouping[['word']].count().reset_index()\ndf.word.describe()\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf.columns = ['word','documents','frequency']\nword_grouping = df[['word','frequency']].groupby('word')\nMaxFrequency_perWord = word_grouping[['frequency']].max().reset_index()\nMaxFrequency_perWord.columns = ['word','MaxFrequency']\nOccurrences_of_Words = word_grouping[['word']].count().reset_index()\ndf.word.describe()\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
574;574;574;574;2.0;1;20083098;;1;29;<python><performance><pandas><hdf5><pytables>;Improve pandas (PyTables?) HDF5 table write performance;10552.0;"['with Timer() as t:\n    store = pd.HDFStore(\'test_storer.h5\', \'w\')\n    store.put(\'events\', events_dataset, table=False, append=False)\nprint(\'Fixed format write took \' + str(t.interval))\nwith Timer() as t:\n    store = pd.HDFStore(\'test_table.h5\', \'w\')\n    store.put(\'events\', events_dataset, table=True, append=False)\nprint(\'Table format write took \' + str(t.interval))\nFixed format write took 7.1\nTable format write took 178.7\nnode_id           int64\nthread_id         int64\nhandle_id         int64\ntype              int64\nbegin             int64\nend               int64\nduration          int64\nflags             int64\nunique_id         int64\nid                int64\nDSTL_LS_FULL    float64\nL2_DMISS        float64\nL3_MISS         float64\nkernel_type     float64\ndtype: object\n%prun -l 20 profile.events.to_hdf(\'test.h5\', \'events\', table=False, append=False)\n\n3223 function calls (3222 primitive calls) in 7.385 seconds\n\nOrdered by: internal time\nList reduced from 208 to 20 due to restriction <20>\n\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n    6    7.127    1.188    7.128    1.188 {method \'_createArray\' of \'tables.hdf5Extension.Array\' objects}\n    1    0.242    0.242    0.242    0.242 {method \'_closeFile\' of \'tables.hdf5Extension.File\' objects}\n    1    0.003    0.003    0.003    0.003 {method \'_g_new\' of \'tables.hdf5Extension.File\' objects}\n   46    0.001    0.000    0.001    0.000 {method \'reduce\' of \'numpy.ufunc\' objects}\n   %prun -l 40 profile.events.to_hdf(\'test.h5\', \'events\', table=True, append=False, chunksize=1000000)\n\n   499082 function calls (499040 primitive calls) in 188.981 seconds\n\n   Ordered by: internal time\n   List reduced from 526 to 40 due to restriction <40>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n       29   92.018    3.173   92.018    3.173 {pandas.lib.create_hdf_rows_2d}\n      640   20.987    0.033   20.987    0.033 {method \'_append\' of \'tables.hdf5Extension.Array\' objects}\n       29   19.256    0.664   19.256    0.664 {method \'_append_records\' of \'tables.tableExtension.Table\' objects}\n      406   19.182    0.047   19.182    0.047 {method \'_g_writeSlice\' of \'tables.hdf5Extension.Array\' objects}\n    14244   10.646    0.001   10.646    0.001 {method \'_g_readSlice\' of \'tables.hdf5Extension.Array\' objects}\n      472   10.359    0.022   10.359    0.022 {method \'copy\' of \'numpy.ndarray\' objects}\n       80    3.409    0.043    3.409    0.043 {tables.indexesExtension.keysort}\n        2    3.023    1.512    3.023    1.512 common.py:134(_isnull_ndarraylike)\n       41    2.489    0.061    2.533    0.062 {method \'_fillCol\' of \'tables.tableExtension.Row\' objects}\n       87    2.401    0.028    2.401    0.028 {method \'astype\' of \'numpy.ndarray\' objects}\n       30    1.880    0.063    1.880    0.063 {method \'_g_flush\' of \'tables.hdf5Extension.Leaf\' objects}\n      282    0.824    0.003    0.824    0.003 {method \'reduce\' of \'numpy.ufunc\' objects}\n       41    0.537    0.013    0.668    0.016 index.py:607(final_idx32)\n    14490    0.385    0.000    0.712    0.000 array.py:342(_interpret_indexing)\n       39    0.279    0.007   19.635    0.503 index.py:1219(reorder_slice)\n        2    0.256    0.128   10.063    5.031 index.py:1099(get_neworder)\n        1    0.090    0.090  119.392  119.392 pytables.py:3016(write_data)\n    57842    0.087    0.000    0.087    0.000 {numpy.core.multiarray.empty}\n    28570    0.062    0.000    0.107    0.000 utils.py:42(is_idx)\n    14164    0.062    0.000    7.181    0.001 array.py:711(_readSlice)\n%prun -l 40 profile.events.to_hdf(\'test.h5\', \'events\', table=True, append=False, chunksize=1000000)\n\n         499748 function calls (499720 primitive calls) in 117.187 seconds\n\n   Ordered by: internal time\n   List reduced from 539 to 20 due to restriction <20>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      640   22.010    0.034   22.010    0.034 {method \'_append\' of \'tables.hdf5Extension.Array\' objects}\n       29   20.782    0.717   20.782    0.717 {method \'_append_records\' of \'tables.tableExtension.Table\' objects}\n      406   19.248    0.047   19.248    0.047 {method \'_g_writeSlice\' of \'tables.hdf5Extension.Array\' objects}\n    14244   10.685    0.001   10.685    0.001 {method \'_g_readSlice\' of \'tables.hdf5Extension.Array\' objects}\n      472   10.439    0.022   10.439    0.022 {method \'copy\' of \'numpy.ndarray\' objects}\n       30    7.356    0.245    7.356    0.245 {method \'_g_flush\' of \'tables.hdf5Extension.Leaf\' objects}\n       29    7.161    0.247   37.609    1.297 pytables.py:3498(write_data_chunk)\n        2    3.888    1.944    3.888    1.944 common.py:197(_isnull_ndarraylike)\n       80    3.581    0.045    3.581    0.045 {tables.indexesExtension.keysort}\n       41    3.248    0.079    3.294    0.080 {method \'_fillCol\' of \'tables.tableExtension.Row\' objects}\n       34    2.744    0.081    2.744    0.081 {method \'ravel\' of \'numpy.ndarray\' objects}\n      115    2.591    0.023    2.591    0.023 {method \'astype\' of \'numpy.ndarray\' objects}\n      270    0.875    0.003    0.875    0.003 {method \'reduce\' of \'numpy.ufunc\' objects}\n       41    0.560    0.014    0.732    0.018 index.py:607(final_idx32)\n    14490    0.387    0.000    0.712    0.000 array.py:342(_interpret_indexing)\n       39    0.303    0.008   19.617    0.503 index.py:1219(reorder_slice)\n        2    0.288    0.144   10.299    5.149 index.py:1099(get_neworder)\n    57871    0.087    0.000    0.087    0.000 {numpy.core.multiarray.empty}\n        1    0.084    0.084   45.266   45.266 pytables.py:3424(write_data)\n        1    0.080    0.080   55.542   55.542 pytables.py:3385(write)\nIn [7]: %timeit f(df)\n1 loops, best of 3: 3.7 s per loop\n\nIn [8]: %timeit f2(df) # where chunksize= 2 000 000\n1 loops, best of 3: 13.8 s per loop\n\nIn [9]: %timeit f3(df) # where chunksize= 2 000 000\n1 loops, best of 3: 43.4 s per loop\nIn [28]: %time f(profile.events)\nCPU times: user 0 ns, sys: 7.16 s, total: 7.16 s\nWall time: 7.51 s\n\nIn [29]: %time f2(profile.events)\nCPU times: user 18.7 s, sys: 14 s, total: 32.7 s\nWall time: 47.2 s\n\nIn [31]: %time f3(profile.events)\nCPU times: user 1min 18s, sys: 14.4 s, total: 1min 32s\nWall time: 2min 5s\nptdump -av test.h5\n/ (RootGroup) \'\'\n  /._v_attrs (AttributeSet), 4 attributes:\n   [CLASS := \'GROUP\',\n    PYTABLES_FORMAT_VERSION := \'2.1\',\n    TITLE := \'\',\n    VERSION := \'1.0\']\n/df (Group) \'\'\n  /df._v_attrs (AttributeSet), 14 attributes:\n   [CLASS := \'GROUP\',\n    TITLE := \'\',\n    VERSION := \'1.0\',\n    data_columns := [],\n    encoding := None,\n    index_cols := [(0, \'index\')],\n    info := {1: {\'type\': \'Index\', \'names\': [None]}, \'index\': {}},\n    levels := 1,\n    nan_rep := \'nan\',\n    non_index_axes := \n    [(1, [\'node_id\', \'thread_id\', \'handle_id\', \'type\', \'begin\', \'end\', \'duration\', \'flags\', \'unique_id\', \'id\', \'DSTL_LS_FULL\', \'L2_DMISS\', \'L3_MISS\', \'kernel_type\'])],\n    pandas_type := \'frame_table\',\n    pandas_version := \'0.10.1\',\n    table_type := \'appendable_frame\',\n    values_cols := [\'values_block_0\', \'values_block_1\']]\n/df/table (Table(28880943,)) \'\'\n  description := {\n  ""index"": Int64Col(shape=(), dflt=0, pos=0),\n  ""values_block_0"": Int64Col(shape=(10,), dflt=0, pos=1),\n  ""values_block_1"": Float64Col(shape=(4,), dflt=0.0, pos=2)}\n  byteorder := \'little\'\n  chunkshape := (4369,)\n  autoindex := True\n  colindexes := {\n    ""index"": Index(6, medium, shuffle, zlib(1)).is_csi=False}\n  /df/table._v_attrs (AttributeSet), 15 attributes:\n   [CLASS := \'TABLE\',\n    FIELD_0_FILL := 0,\n    FIELD_0_NAME := \'index\',\n    FIELD_1_FILL := 0,\n    FIELD_1_NAME := \'values_block_0\',\n    FIELD_2_FILL := 0.0,\n    FIELD_2_NAME := \'values_block_1\',\n    NROWS := 28880943,\n    TITLE := \'\',\n    VERSION := \'2.7\',\n    index_kind := \'integer\',\n    values_block_0_dtype := \'int64\',\n    values_block_0_kind := [\'node_id\', \'thread_id\', \'handle_id\', \'type\', \'begin\', \'end\', \'duration\', \'flags\', \'unique_id\', \'id\'],\n    values_block_1_dtype := \'float64\',\n    values_block_1_kind := [\'DSTL_LS_FULL\', \'L2_DMISS\', \'L3_MISS\', \'kernel_type\']]\n%prun -l 25  %time f3(profile.events)\nCPU times: user 1min 14s, sys: 16.2 s, total: 1min 30s\nWall time: 1min 48s\n\n        542678 function calls (542650 primitive calls) in 108.678 seconds\n\n   Ordered by: internal time\n   List reduced from 629 to 25 due to restriction <25>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      640   23.633    0.037   23.633    0.037 {method \'_append\' of \'tables.hdf5extension.Array\' objects}\n       15   20.852    1.390   20.852    1.390 {method \'_append_records\' of \'tables.tableextension.Table\' objects}\n      406   19.584    0.048   19.584    0.048 {method \'_g_write_slice\' of \'tables.hdf5extension.Array\' objects}\n    14244   10.591    0.001   10.591    0.001 {method \'_g_read_slice\' of \'tables.hdf5extension.Array\' objects}\n      458    9.693    0.021    9.693    0.021 {method \'copy\' of \'numpy.ndarray\' objects}\n       15    6.350    0.423   30.989    2.066 pytables.py:3498(write_data_chunk)\n       80    3.496    0.044    3.496    0.044 {tables.indexesextension.keysort}\n       41    3.335    0.081    3.376    0.082 {method \'_fill_col\' of \'tables.tableextension.Row\' objects}\n       20    2.551    0.128    2.551    0.128 {method \'ravel\' of \'numpy.ndarray\' objects}\n      101    2.449    0.024    2.449    0.024 {method \'astype\' of \'numpy.ndarray\' objects}\n       16    1.789    0.112    1.789    0.112 {method \'_g_flush\' of \'tables.hdf5extension.Leaf\' objects}\n        2    1.728    0.864    1.728    0.864 common.py:197(_isnull_ndarraylike)\n       41    0.586    0.014    0.842    0.021 index.py:637(final_idx32)\n    14490    0.292    0.000    0.616    0.000 array.py:368(_interpret_indexing)\n        2    0.283    0.142   10.267    5.134 index.py:1158(get_neworder)\n      274    0.251    0.001    0.251    0.001 {method \'reduce\' of \'numpy.ufunc\' objects}\n       39    0.174    0.004   19.373    0.497 index.py:1280(reorder_slice)\n    57857    0.085    0.000    0.085    0.000 {numpy.core.multiarray.empty}\n        1    0.083    0.083   35.657   35.657 pytables.py:3424(write_data)\n        1    0.065    0.065   45.338   45.338 pytables.py:3385(write)\n    14164    0.065    0.000    7.831    0.001 array.py:615(__getitem__)\n    28570    0.062    0.000    0.108    0.000 utils.py:47(is_idx)\n       47    0.055    0.001    0.055    0.001 {numpy.core.multiarray.arange}\n    28570    0.050    0.000    0.090    0.000 leaf.py:397(_process_range)\n    87797    0.048    0.000    0.048    0.000 {isinstance}\n']";"[""with Timer() as t:\n    store = pd.HDFStore('test_storer.h5', 'w')\n    store.put('events', events_dataset, table=False, append=False)\nprint('Fixed format write took ' + str(t.interval))\nwith Timer() as t:\n    store = pd.HDFStore('test_table.h5', 'w')\n    store.put('events', events_dataset, table=True, append=False)\nprint('Table format write took ' + str(t.interval))\n"", 'Fixed format write took 7.1\nTable format write took 178.7\n', 'node_id           int64\nthread_id         int64\nhandle_id         int64\ntype              int64\nbegin             int64\nend               int64\nduration          int64\nflags             int64\nunique_id         int64\nid                int64\nDSTL_LS_FULL    float64\nL2_DMISS        float64\nL3_MISS         float64\nkernel_type     float64\ndtype: object\n', ""%prun -l 20 profile.events.to_hdf('test.h5', 'events', table=False, append=False)\n\n3223 function calls (3222 primitive calls) in 7.385 seconds\n\nOrdered by: internal time\nList reduced from 208 to 20 due to restriction <20>\n\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n    6    7.127    1.188    7.128    1.188 {method '_createArray' of 'tables.hdf5Extension.Array' objects}\n    1    0.242    0.242    0.242    0.242 {method '_closeFile' of 'tables.hdf5Extension.File' objects}\n    1    0.003    0.003    0.003    0.003 {method '_g_new' of 'tables.hdf5Extension.File' objects}\n   46    0.001    0.000    0.001    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n"", ""   %prun -l 40 profile.events.to_hdf('test.h5', 'events', table=True, append=False, chunksize=1000000)\n\n   499082 function calls (499040 primitive calls) in 188.981 seconds\n\n   Ordered by: internal time\n   List reduced from 526 to 40 due to restriction <40>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n       29   92.018    3.173   92.018    3.173 {pandas.lib.create_hdf_rows_2d}\n      640   20.987    0.033   20.987    0.033 {method '_append' of 'tables.hdf5Extension.Array' objects}\n       29   19.256    0.664   19.256    0.664 {method '_append_records' of 'tables.tableExtension.Table' objects}\n      406   19.182    0.047   19.182    0.047 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n    14244   10.646    0.001   10.646    0.001 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n      472   10.359    0.022   10.359    0.022 {method 'copy' of 'numpy.ndarray' objects}\n       80    3.409    0.043    3.409    0.043 {tables.indexesExtension.keysort}\n        2    3.023    1.512    3.023    1.512 common.py:134(_isnull_ndarraylike)\n       41    2.489    0.061    2.533    0.062 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       87    2.401    0.028    2.401    0.028 {method 'astype' of 'numpy.ndarray' objects}\n       30    1.880    0.063    1.880    0.063 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n      282    0.824    0.003    0.824    0.003 {method 'reduce' of 'numpy.ufunc' objects}\n       41    0.537    0.013    0.668    0.016 index.py:607(final_idx32)\n    14490    0.385    0.000    0.712    0.000 array.py:342(_interpret_indexing)\n       39    0.279    0.007   19.635    0.503 index.py:1219(reorder_slice)\n        2    0.256    0.128   10.063    5.031 index.py:1099(get_neworder)\n        1    0.090    0.090  119.392  119.392 pytables.py:3016(write_data)\n    57842    0.087    0.000    0.087    0.000 {numpy.core.multiarray.empty}\n    28570    0.062    0.000    0.107    0.000 utils.py:42(is_idx)\n    14164    0.062    0.000    7.181    0.001 array.py:711(_readSlice)\n"", ""%prun -l 40 profile.events.to_hdf('test.h5', 'events', table=True, append=False, chunksize=1000000)\n\n         499748 function calls (499720 primitive calls) in 117.187 seconds\n\n   Ordered by: internal time\n   List reduced from 539 to 20 due to restriction <20>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      640   22.010    0.034   22.010    0.034 {method '_append' of 'tables.hdf5Extension.Array' objects}\n       29   20.782    0.717   20.782    0.717 {method '_append_records' of 'tables.tableExtension.Table' objects}\n      406   19.248    0.047   19.248    0.047 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n    14244   10.685    0.001   10.685    0.001 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n      472   10.439    0.022   10.439    0.022 {method 'copy' of 'numpy.ndarray' objects}\n       30    7.356    0.245    7.356    0.245 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n       29    7.161    0.247   37.609    1.297 pytables.py:3498(write_data_chunk)\n        2    3.888    1.944    3.888    1.944 common.py:197(_isnull_ndarraylike)\n       80    3.581    0.045    3.581    0.045 {tables.indexesExtension.keysort}\n       41    3.248    0.079    3.294    0.080 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       34    2.744    0.081    2.744    0.081 {method 'ravel' of 'numpy.ndarray' objects}\n      115    2.591    0.023    2.591    0.023 {method 'astype' of 'numpy.ndarray' objects}\n      270    0.875    0.003    0.875    0.003 {method 'reduce' of 'numpy.ufunc' objects}\n       41    0.560    0.014    0.732    0.018 index.py:607(final_idx32)\n    14490    0.387    0.000    0.712    0.000 array.py:342(_interpret_indexing)\n       39    0.303    0.008   19.617    0.503 index.py:1219(reorder_slice)\n        2    0.288    0.144   10.299    5.149 index.py:1099(get_neworder)\n    57871    0.087    0.000    0.087    0.000 {numpy.core.multiarray.empty}\n        1    0.084    0.084   45.266   45.266 pytables.py:3424(write_data)\n        1    0.080    0.080   55.542   55.542 pytables.py:3385(write)\n"", 'In [7]: %timeit f(df)\n1 loops, best of 3: 3.7 s per loop\n\nIn [8]: %timeit f2(df) # where chunksize= 2 000 000\n1 loops, best of 3: 13.8 s per loop\n\nIn [9]: %timeit f3(df) # where chunksize= 2 000 000\n1 loops, best of 3: 43.4 s per loop\n', 'In [28]: %time f(profile.events)\nCPU times: user 0 ns, sys: 7.16 s, total: 7.16 s\nWall time: 7.51 s\n\nIn [29]: %time f2(profile.events)\nCPU times: user 18.7 s, sys: 14 s, total: 32.7 s\nWall time: 47.2 s\n\nIn [31]: %time f3(profile.events)\nCPU times: user 1min 18s, sys: 14.4 s, total: 1min 32s\nWall time: 2min 5s\n', 'ptdump -av test.h5\n/ (RootGroup) \'\'\n  /._v_attrs (AttributeSet), 4 attributes:\n   [CLASS := \'GROUP\',\n    PYTABLES_FORMAT_VERSION := \'2.1\',\n    TITLE := \'\',\n    VERSION := \'1.0\']\n/df (Group) \'\'\n  /df._v_attrs (AttributeSet), 14 attributes:\n   [CLASS := \'GROUP\',\n    TITLE := \'\',\n    VERSION := \'1.0\',\n    data_columns := [],\n    encoding := None,\n    index_cols := [(0, \'index\')],\n    info := {1: {\'type\': \'Index\', \'names\': [None]}, \'index\': {}},\n    levels := 1,\n    nan_rep := \'nan\',\n    non_index_axes := \n    [(1, [\'node_id\', \'thread_id\', \'handle_id\', \'type\', \'begin\', \'end\', \'duration\', \'flags\', \'unique_id\', \'id\', \'DSTL_LS_FULL\', \'L2_DMISS\', \'L3_MISS\', \'kernel_type\'])],\n    pandas_type := \'frame_table\',\n    pandas_version := \'0.10.1\',\n    table_type := \'appendable_frame\',\n    values_cols := [\'values_block_0\', \'values_block_1\']]\n/df/table (Table(28880943,)) \'\'\n  description := {\n  ""index"": Int64Col(shape=(), dflt=0, pos=0),\n  ""values_block_0"": Int64Col(shape=(10,), dflt=0, pos=1),\n  ""values_block_1"": Float64Col(shape=(4,), dflt=0.0, pos=2)}\n  byteorder := \'little\'\n  chunkshape := (4369,)\n  autoindex := True\n  colindexes := {\n    ""index"": Index(6, medium, shuffle, zlib(1)).is_csi=False}\n  /df/table._v_attrs (AttributeSet), 15 attributes:\n   [CLASS := \'TABLE\',\n    FIELD_0_FILL := 0,\n    FIELD_0_NAME := \'index\',\n    FIELD_1_FILL := 0,\n    FIELD_1_NAME := \'values_block_0\',\n    FIELD_2_FILL := 0.0,\n    FIELD_2_NAME := \'values_block_1\',\n    NROWS := 28880943,\n    TITLE := \'\',\n    VERSION := \'2.7\',\n    index_kind := \'integer\',\n    values_block_0_dtype := \'int64\',\n    values_block_0_kind := [\'node_id\', \'thread_id\', \'handle_id\', \'type\', \'begin\', \'end\', \'duration\', \'flags\', \'unique_id\', \'id\'],\n    values_block_1_dtype := \'float64\',\n    values_block_1_kind := [\'DSTL_LS_FULL\', \'L2_DMISS\', \'L3_MISS\', \'kernel_type\']]\n', ""%prun -l 25  %time f3(profile.events)\nCPU times: user 1min 14s, sys: 16.2 s, total: 1min 30s\nWall time: 1min 48s\n\n        542678 function calls (542650 primitive calls) in 108.678 seconds\n\n   Ordered by: internal time\n   List reduced from 629 to 25 due to restriction <25>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      640   23.633    0.037   23.633    0.037 {method '_append' of 'tables.hdf5extension.Array' objects}\n       15   20.852    1.390   20.852    1.390 {method '_append_records' of 'tables.tableextension.Table' objects}\n      406   19.584    0.048   19.584    0.048 {method '_g_write_slice' of 'tables.hdf5extension.Array' objects}\n    14244   10.591    0.001   10.591    0.001 {method '_g_read_slice' of 'tables.hdf5extension.Array' objects}\n      458    9.693    0.021    9.693    0.021 {method 'copy' of 'numpy.ndarray' objects}\n       15    6.350    0.423   30.989    2.066 pytables.py:3498(write_data_chunk)\n       80    3.496    0.044    3.496    0.044 {tables.indexesextension.keysort}\n       41    3.335    0.081    3.376    0.082 {method '_fill_col' of 'tables.tableextension.Row' objects}\n       20    2.551    0.128    2.551    0.128 {method 'ravel' of 'numpy.ndarray' objects}\n      101    2.449    0.024    2.449    0.024 {method 'astype' of 'numpy.ndarray' objects}\n       16    1.789    0.112    1.789    0.112 {method '_g_flush' of 'tables.hdf5extension.Leaf' objects}\n        2    1.728    0.864    1.728    0.864 common.py:197(_isnull_ndarraylike)\n       41    0.586    0.014    0.842    0.021 index.py:637(final_idx32)\n    14490    0.292    0.000    0.616    0.000 array.py:368(_interpret_indexing)\n        2    0.283    0.142   10.267    5.134 index.py:1158(get_neworder)\n      274    0.251    0.001    0.251    0.001 {method 'reduce' of 'numpy.ufunc' objects}\n       39    0.174    0.004   19.373    0.497 index.py:1280(reorder_slice)\n    57857    0.085    0.000    0.085    0.000 {numpy.core.multiarray.empty}\n        1    0.083    0.083   35.657   35.657 pytables.py:3424(write_data)\n        1    0.065    0.065   45.338   45.338 pytables.py:3385(write)\n    14164    0.065    0.000    7.831    0.001 array.py:615(__getitem__)\n    28570    0.062    0.000    0.108    0.000 utils.py:47(is_idx)\n       47    0.055    0.001    0.055    0.001 {numpy.core.multiarray.arange}\n    28570    0.050    0.000    0.090    0.000 leaf.py:397(_process_range)\n    87797    0.048    0.000    0.048    0.000 {isinstance}\n""]";"[""with Timer() as t:\n    store = pd.HDFStore('test_storer.h5', 'w')\n    store.put('events', events_dataset, table=False, append=False)\nprint('Fixed format write took ' + str(t.interval))\nwith Timer() as t:\n    store = pd.HDFStore('test_table.h5', 'w')\n    store.put('events', events_dataset, table=True, append=False)\nprint('Table format write took ' + str(t.interval))\n"", 'Fixed format write took 7.1\nTable format write took 178.7\n', 'node_id           int64\nthread_id         int64\nhandle_id         int64\ntype              int64\nbegin             int64\nend               int64\nduration          int64\nflags             int64\nunique_id         int64\nid                int64\nDSTL_LS_FULL    float64\nL2_DMISS        float64\nL3_MISS         float64\nkernel_type     float64\ndtype: object\n', ""%prun -l 20 profile.events.to_hdf('test.h5', 'events', table=False, append=False)\n\n3223 function calls (3222 primitive calls) in 7.385 seconds\n\nOrdered by: internal time\nList reduced from 208 to 20 due to restriction <20>\n\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n    6    7.127    1.188    7.128    1.188 {method '_createArray' of 'tables.hdf5Extension.Array' objects}\n    1    0.242    0.242    0.242    0.242 {method '_closeFile' of 'tables.hdf5Extension.File' objects}\n    1    0.003    0.003    0.003    0.003 {method '_g_new' of 'tables.hdf5Extension.File' objects}\n   46    0.001    0.000    0.001    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n"", ""   %prun -l 40 profile.events.to_hdf('test.h5', 'events', table=True, append=False, chunksize=1000000)\n\n   499082 function calls (499040 primitive calls) in 188.981 seconds\n\n   Ordered by: internal time\n   List reduced from 526 to 40 due to restriction <40>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n       29   92.018    3.173   92.018    3.173 {pandas.lib.create_hdf_rows_2d}\n      640   20.987    0.033   20.987    0.033 {method '_append' of 'tables.hdf5Extension.Array' objects}\n       29   19.256    0.664   19.256    0.664 {method '_append_records' of 'tables.tableExtension.Table' objects}\n      406   19.182    0.047   19.182    0.047 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n    14244   10.646    0.001   10.646    0.001 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n      472   10.359    0.022   10.359    0.022 {method 'copy' of 'numpy.ndarray' objects}\n       80    3.409    0.043    3.409    0.043 {tables.indexesExtension.keysort}\n        2    3.023    1.512    3.023    1.512 common.py:134(_isnull_ndarraylike)\n       41    2.489    0.061    2.533    0.062 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       87    2.401    0.028    2.401    0.028 {method 'astype' of 'numpy.ndarray' objects}\n       30    1.880    0.063    1.880    0.063 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n      282    0.824    0.003    0.824    0.003 {method 'reduce' of 'numpy.ufunc' objects}\n       41    0.537    0.013    0.668    0.016 index.py:607(final_idx32)\n    14490    0.385    0.000    0.712    0.000 array.py:342(_interpret_indexing)\n       39    0.279    0.007   19.635    0.503 index.py:1219(reorder_slice)\n        2    0.256    0.128   10.063    5.031 index.py:1099(get_neworder)\n        1    0.090    0.090  119.392  119.392 pytables.py:3016(write_data)\n    57842    0.087    0.000    0.087    0.000 {numpy.core.multiarray.empty}\n    28570    0.062    0.000    0.107    0.000 utils.py:42(is_idx)\n    14164    0.062    0.000    7.181    0.001 array.py:711(_readSlice)\n"", ""%prun -l 40 profile.events.to_hdf('test.h5', 'events', table=True, append=False, chunksize=1000000)\n\n         499748 function calls (499720 primitive calls) in 117.187 seconds\n\n   Ordered by: internal time\n   List reduced from 539 to 20 due to restriction <20>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      640   22.010    0.034   22.010    0.034 {method '_append' of 'tables.hdf5Extension.Array' objects}\n       29   20.782    0.717   20.782    0.717 {method '_append_records' of 'tables.tableExtension.Table' objects}\n      406   19.248    0.047   19.248    0.047 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n    14244   10.685    0.001   10.685    0.001 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n      472   10.439    0.022   10.439    0.022 {method 'copy' of 'numpy.ndarray' objects}\n       30    7.356    0.245    7.356    0.245 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n       29    7.161    0.247   37.609    1.297 pytables.py:3498(write_data_chunk)\n        2    3.888    1.944    3.888    1.944 common.py:197(_isnull_ndarraylike)\n       80    3.581    0.045    3.581    0.045 {tables.indexesExtension.keysort}\n       41    3.248    0.079    3.294    0.080 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       34    2.744    0.081    2.744    0.081 {method 'ravel' of 'numpy.ndarray' objects}\n      115    2.591    0.023    2.591    0.023 {method 'astype' of 'numpy.ndarray' objects}\n      270    0.875    0.003    0.875    0.003 {method 'reduce' of 'numpy.ufunc' objects}\n       41    0.560    0.014    0.732    0.018 index.py:607(final_idx32)\n    14490    0.387    0.000    0.712    0.000 array.py:342(_interpret_indexing)\n       39    0.303    0.008   19.617    0.503 index.py:1219(reorder_slice)\n        2    0.288    0.144   10.299    5.149 index.py:1099(get_neworder)\n    57871    0.087    0.000    0.087    0.000 {numpy.core.multiarray.empty}\n        1    0.084    0.084   45.266   45.266 pytables.py:3424(write_data)\n        1    0.080    0.080   55.542   55.542 pytables.py:3385(write)\n"", 'In [7]: %timeit f(df)\n1 loops, best of 3: 3.7 s per loop\n\nIn [8]: %timeit f2(df) # where chunksize= 2 000 000\n1 loops, best of 3: 13.8 s per loop\n\nIn [9]: %timeit f3(df) # where chunksize= 2 000 000\n1 loops, best of 3: 43.4 s per loop\n', 'top', 'ls', 'In [28]: %time f(profile.events)\nCPU times: user 0 ns, sys: 7.16 s, total: 7.16 s\nWall time: 7.51 s\n\nIn [29]: %time f2(profile.events)\nCPU times: user 18.7 s, sys: 14 s, total: 32.7 s\nWall time: 47.2 s\n\nIn [31]: %time f3(profile.events)\nCPU times: user 1min 18s, sys: 14.4 s, total: 1min 32s\nWall time: 2min 5s\n', 'ptdump -av test.h5\n/ (RootGroup) \'\'\n  /._v_attrs (AttributeSet), 4 attributes:\n   [CLASS := \'GROUP\',\n    PYTABLES_FORMAT_VERSION := \'2.1\',\n    TITLE := \'\',\n    VERSION := \'1.0\']\n/df (Group) \'\'\n  /df._v_attrs (AttributeSet), 14 attributes:\n   [CLASS := \'GROUP\',\n    TITLE := \'\',\n    VERSION := \'1.0\',\n    data_columns := [],\n    encoding := None,\n    index_cols := [(0, \'index\')],\n    info := {1: {\'type\': \'Index\', \'names\': [None]}, \'index\': {}},\n    levels := 1,\n    nan_rep := \'nan\',\n    non_index_axes := \n    [(1, [\'node_id\', \'thread_id\', \'handle_id\', \'type\', \'begin\', \'end\', \'duration\', \'flags\', \'unique_id\', \'id\', \'DSTL_LS_FULL\', \'L2_DMISS\', \'L3_MISS\', \'kernel_type\'])],\n    pandas_type := \'frame_table\',\n    pandas_version := \'0.10.1\',\n    table_type := \'appendable_frame\',\n    values_cols := [\'values_block_0\', \'values_block_1\']]\n/df/table (Table(28880943,)) \'\'\n  description := {\n  ""index"": Int64Col(shape=(), dflt=0, pos=0),\n  ""values_block_0"": Int64Col(shape=(10,), dflt=0, pos=1),\n  ""values_block_1"": Float64Col(shape=(4,), dflt=0.0, pos=2)}\n  byteorder := \'little\'\n  chunkshape := (4369,)\n  autoindex := True\n  colindexes := {\n    ""index"": Index(6, medium, shuffle, zlib(1)).is_csi=False}\n  /df/table._v_attrs (AttributeSet), 15 attributes:\n   [CLASS := \'TABLE\',\n    FIELD_0_FILL := 0,\n    FIELD_0_NAME := \'index\',\n    FIELD_1_FILL := 0,\n    FIELD_1_NAME := \'values_block_0\',\n    FIELD_2_FILL := 0.0,\n    FIELD_2_NAME := \'values_block_1\',\n    NROWS := 28880943,\n    TITLE := \'\',\n    VERSION := \'2.7\',\n    index_kind := \'integer\',\n    values_block_0_dtype := \'int64\',\n    values_block_0_kind := [\'node_id\', \'thread_id\', \'handle_id\', \'type\', \'begin\', \'end\', \'duration\', \'flags\', \'unique_id\', \'id\'],\n    values_block_1_dtype := \'float64\',\n    values_block_1_kind := [\'DSTL_LS_FULL\', \'L2_DMISS\', \'L3_MISS\', \'kernel_type\']]\n', ""%prun -l 25  %time f3(profile.events)\nCPU times: user 1min 14s, sys: 16.2 s, total: 1min 30s\nWall time: 1min 48s\n\n        542678 function calls (542650 primitive calls) in 108.678 seconds\n\n   Ordered by: internal time\n   List reduced from 629 to 25 due to restriction <25>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      640   23.633    0.037   23.633    0.037 {method '_append' of 'tables.hdf5extension.Array' objects}\n       15   20.852    1.390   20.852    1.390 {method '_append_records' of 'tables.tableextension.Table' objects}\n      406   19.584    0.048   19.584    0.048 {method '_g_write_slice' of 'tables.hdf5extension.Array' objects}\n    14244   10.591    0.001   10.591    0.001 {method '_g_read_slice' of 'tables.hdf5extension.Array' objects}\n      458    9.693    0.021    9.693    0.021 {method 'copy' of 'numpy.ndarray' objects}\n       15    6.350    0.423   30.989    2.066 pytables.py:3498(write_data_chunk)\n       80    3.496    0.044    3.496    0.044 {tables.indexesextension.keysort}\n       41    3.335    0.081    3.376    0.082 {method '_fill_col' of 'tables.tableextension.Row' objects}\n       20    2.551    0.128    2.551    0.128 {method 'ravel' of 'numpy.ndarray' objects}\n      101    2.449    0.024    2.449    0.024 {method 'astype' of 'numpy.ndarray' objects}\n       16    1.789    0.112    1.789    0.112 {method '_g_flush' of 'tables.hdf5extension.Leaf' objects}\n        2    1.728    0.864    1.728    0.864 common.py:197(_isnull_ndarraylike)\n       41    0.586    0.014    0.842    0.021 index.py:637(final_idx32)\n    14490    0.292    0.000    0.616    0.000 array.py:368(_interpret_indexing)\n        2    0.283    0.142   10.267    5.134 index.py:1158(get_neworder)\n      274    0.251    0.001    0.251    0.001 {method 'reduce' of 'numpy.ufunc' objects}\n       39    0.174    0.004   19.373    0.497 index.py:1280(reorder_slice)\n    57857    0.085    0.000    0.085    0.000 {numpy.core.multiarray.empty}\n        1    0.083    0.083   35.657   35.657 pytables.py:3424(write_data)\n        1    0.065    0.065   45.338   45.338 pytables.py:3385(write)\n    14164    0.065    0.000    7.831    0.001 array.py:615(__getitem__)\n    28570    0.062    0.000    0.108    0.000 utils.py:47(is_idx)\n       47    0.055    0.001    0.055    0.001 {numpy.core.multiarray.arange}\n    28570    0.050    0.000    0.090    0.000 leaf.py:397(_process_range)\n    87797    0.048    0.000    0.048    0.000 {isinstance}\n""]";"[""print('Fixed format write took ' + str(t.interval))\nprint('Table format write took ' + str(t.interval))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n""]";"[""print('Fixed format write took ' + str(t.interval))\nprint('Table format write took ' + str(t.interval))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n""]";False;"[""import pandas as pd\nprint('Fixed format write took ' + str(t.interval))\nprint('Table format write took ' + str(t.interval))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n""]";False;0;1;"[""name 'concat' is not defined""]";['NameError'];0;1;"[""name 'concat' is not defined""]";['NameError'];0;1;"[""name 'concat' is not defined""]";['NameError']
575;575;575;575;2.0;0;20084382;;1;48;<python><pandas><dataframe>;Find unique values in a Pandas dataframe, irrespective of row or column location;81292.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'DataFrame' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'DataFrame' is not defined""]";['NameError']
576;576;576;576;3.0;0;20095673;;1;26;<python><pandas><dataframe>;python: shift column in pandas dataframe up by one;24501.0;['df =\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n\ndf_lag =\n    y  gdp  cap\n0   1    3    5\n1   2    7    9\n2   8    4    2\n3   3    7    7\n'];['df =\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n\ndf_lag =\n    y  gdp  cap\n0   1    3    5\n1   2    7    9\n2   8    4    2\n3   3    7    7\n'];['df =\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n\ndf_lag =\n    y  gdp  cap\n0   1    3    5\n1   2    7    9\n2   8    4    2\n3   3    7    7\n'];['\n'];['\n'];False;['import pandas as pd\n\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'DataFrame' object has no attribute 'gdp'""]";['AttributeError']
577;577;577;577;1.0;1;20095983;;1;13;<python><csv><pandas><data-type-conversion>;Specify correct dtypes using pandas.read_csv;17163.0;"[""import pandas as pd\nimport numpy as np\ndf = pd.read_csv(<file-name>, dtype={'A': np.int64, 'B': np.float64})\n""]";"[""import pandas as pd\nimport numpy as np\ndf = pd.read_csv(<file-name>, dtype={'A': np.int64, 'B': np.float64})\n""]";"['dtype', ""import pandas as pd\nimport numpy as np\ndf = pd.read_csv(<file-name>, dtype={'A': np.int64, 'B': np.float64})\n"", 'np.bool_', 'pd.tslib.Timestamp']";['import pandas as pd\nimport numpy as np\n'];['import pandas as pd\nimport numpy as np\n'];False;['import pandas as pd\nimport pandas as pd\nimport numpy as np\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
578;578;578;578;5.0;0;20107570;;1;27;<python><pandas>;Removing index column in pandas;70802.0;"[""df = pd.DataFrame.from_csv('Efficiency_Data.csv', header=0, parse_dates=False)\nenergy = df.index\nefficiency = df.Efficiency\nprint efficiency\ndel df['index']\nenergy = df.index\n""]";"[""df = pd.DataFrame.from_csv('Efficiency_Data.csv', header=0, parse_dates=False)\nenergy = df.index\nefficiency = df.Efficiency\nprint efficiency\n"", ""del df['index']\n"", 'energy = df.index\n']";"[""df = pd.DataFrame.from_csv('Efficiency_Data.csv', header=0, parse_dates=False)\nenergy = df.index\nefficiency = df.Efficiency\nprint efficiency\n"", ""del df['index']\n"", 'energy = df.index\n']";"[""df = pd.DataFrame.from_csv('Efficiency_Data.csv', header=0, parse_dates=False)\nenergy = df.index\nefficiency = df.Efficiency\ndel df['index']\nenergy = df.index\n""]";"[""import pandas as pd\ndf = pd.DataFrame.from_csv('Efficiency_Data.csv', header=0, parse_dates=False)\nenergy = df.index\nefficiency = df.Efficiency\ndel df['index']\nenergy = df.index\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame.from_csv('Efficiency_Data.csv', header=0, parse_dates=False)\nenergy = df.index\nefficiency = df.Efficiency\ndel df['index']\nenergy = df.index\n""]";False;1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""'DataFrame' object has no attribute 'read_csv'""]";['Sucess', 'AttributeError']
579;579;579;579;5.0;3;20109391;;1;105;<python><pandas>;How to make good reproducible pandas examples;3594.0;"[""import pandas as pd\ndf = pd.DataFrame({'user': ['Bob', 'Jane', 'Alice'], \n                   'income': [40000, 50000, 42000]})\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'user': ['Bob', 'Jane', 'Alice'], \n                   'income': [40000, 50000, 42000]})\n""]";"['pandas', 'pandas', ""import pandas as pd\ndf = pd.DataFrame({'user': ['Bob', 'Jane', 'Alice'], \n                   'income': [40000, 50000, 42000]})\n"", 'datetime', 'expand.grid()', 'dput()']";['import pandas as pd\n'];['import pandas as pd\n'];False;['import pandas as pd\nimport pandas as pd\n'];False;2;4;"[""name 'pd' is not defined"", 'Sucess', ""name 'stocks' is not defined"", 'Sucess']";['NameError', 'Sucess', 'NameError', 'Sucess'];2;4;"[""name 'iwantthis' is not defined"", 'Sucess', ""name 'stocks' is not defined"", 'Sucess']";['NameError', 'Sucess', 'NameError', 'Sucess'];2;4;"[""name 'iwantthis' is not defined"", 'Sucess', ""name 'stocks' is not defined"", 'Sucess']";['NameError', 'Sucess', 'NameError', 'Sucess']
580;580;580;580;1.0;3;20110170;;1;63;<python><pandas><dataframe><flatten><multi-index>;Turn Pandas Multi-Index into column;31159.0;['                         value\nTrial    measurement\n    1              0        13\n                   1         3\n                   2         4\n    2              0       NaN\n                   1        12\n    3              0        34 \nTrial    measurement       value\n\n    1              0        13\n    1              1         3\n    1              2         4\n    2              0       NaN\n    2              1        12\n    3              0        34 \n'];['                         value\nTrial    measurement\n    1              0        13\n                   1         3\n                   2         4\n    2              0       NaN\n                   1        12\n    3              0        34 \n', 'Trial    measurement       value\n\n    1              0        13\n    1              1         3\n    1              2         4\n    2              0       NaN\n    2              1        12\n    3              0        34 \n'];['                         value\nTrial    measurement\n    1              0        13\n                   1         3\n                   2         4\n    2              0       NaN\n                   1        12\n    3              0        34 \n', 'Trial    measurement       value\n\n    1              0        13\n    1              1         3\n    1              2         4\n    2              0       NaN\n    2              1        12\n    3              0        34 \n'];['\n'];['\n'];False;['import pandas as pd\n\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
581;581;581;581;2.0;0;20119414;;1;11;<python><python-2.7><pandas>;define aggfunc for each values column in pandas pivot table;9943.0;"[""df = pd.DataFrame({\n          'A' : ['one', 'one', 'two', 'three'] * 6,\n          'B' : ['A', 'B', 'C'] * 8,\n          'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n          'D' : np.random.randn(24),\n          'E' : np.random.randn(24)\n})\npd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\npd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\n""]";"[""df = pd.DataFrame({\n          'A' : ['one', 'one', 'two', 'three'] * 6,\n          'B' : ['A', 'B', 'C'] * 8,\n          'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n          'D' : np.random.randn(24),\n          'E' : np.random.randn(24)\n})\n"", ""pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\n"", ""pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\n""]";"[""df = pd.DataFrame({\n          'A' : ['one', 'one', 'two', 'three'] * 6,\n          'B' : ['A', 'B', 'C'] * 8,\n          'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n          'D' : np.random.randn(24),\n          'E' : np.random.randn(24)\n})\n"", ""pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\n"", ""pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\n"", 'D', 'E']";"[""pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\npd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\n""]";"[""import pandas as pd\npd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\npd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\npd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.sum)\npd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=np.mean)\n""]";True;0;2;"[""name 'B' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'B' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'B' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError']
582;582;582;582;4.0;3;20154303;;1;12;<python><csv><pandas><ragged>;Pandas read_csv expects wrong number of columns, with ragged csv file;8426.0;"['In [3]:\n\ninfile =open(easygui.fileopenbox(),""r"")\npledge = read_csv(infile,parse_dates=\'true\')\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-3-b35e7a16b389> in <module>()\n      1 infile =open(easygui.fileopenbox(),""r"")\n      2 \n----> 3 pledge = read_csv(infile,parse_dates=\'true\')\n\n\nC:\\Python27\\lib\\site-packages\\pandas-0.8.1-py2.7-win32.egg\\pandas\\io\\parsers.pyc in read_csv(filepath_or_buffer, sep, dialect, header, index_col, names, skiprows, na_values, thousands, comment, parse_dates, keep_date_col, dayfirst, date_parser, nrows, iterator, chunksize, skip_footer, converters, verbose, delimiter, encoding, squeeze)\n    234         kwds[\'delimiter\'] = sep\n    235 \n--> 236     return _read(TextParser, filepath_or_buffer, kwds)\n    237 \n    238 @Appender(_read_table_doc)\n\nC:\\Python27\\lib\\site-packages\\pandas-0.8.1-py2.7-win32.egg\\pandas\\io\\parsers.pyc in _read(cls, filepath_or_buffer, kwds)\n    189         return parser\n    190 \n--> 191     return parser.get_chunk()\n    192 \n    193 @Appender(_read_csv_doc)\n\nC:\\Python27\\lib\\site-packages\\pandas-0.8.1-py2.7-win32.egg\\pandas\\io\\parsers.pyc in get_chunk(self, rows)\n    779             msg = (\'Expecting %d columns, got %d in row %d\' %\n    780                    (col_len, zip_len, row_num))\n--> 781             raise ValueError(msg)\n    782 \n    783         data = dict((k, v) for k, v in izip(self.columns, zipped_content))\n\nValueError: Expecting 23 columns, got 26 in row 64\n']";"['In [3]:\n\ninfile =open(easygui.fileopenbox(),""r"")\npledge = read_csv(infile,parse_dates=\'true\')\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-3-b35e7a16b389> in <module>()\n      1 infile =open(easygui.fileopenbox(),""r"")\n      2 \n----> 3 pledge = read_csv(infile,parse_dates=\'true\')\n\n\nC:\\Python27\\lib\\site-packages\\pandas-0.8.1-py2.7-win32.egg\\pandas\\io\\parsers.pyc in read_csv(filepath_or_buffer, sep, dialect, header, index_col, names, skiprows, na_values, thousands, comment, parse_dates, keep_date_col, dayfirst, date_parser, nrows, iterator, chunksize, skip_footer, converters, verbose, delimiter, encoding, squeeze)\n    234         kwds[\'delimiter\'] = sep\n    235 \n--> 236     return _read(TextParser, filepath_or_buffer, kwds)\n    237 \n    238 @Appender(_read_table_doc)\n\nC:\\Python27\\lib\\site-packages\\pandas-0.8.1-py2.7-win32.egg\\pandas\\io\\parsers.pyc in _read(cls, filepath_or_buffer, kwds)\n    189         return parser\n    190 \n--> 191     return parser.get_chunk()\n    192 \n    193 @Appender(_read_csv_doc)\n\nC:\\Python27\\lib\\site-packages\\pandas-0.8.1-py2.7-win32.egg\\pandas\\io\\parsers.pyc in get_chunk(self, rows)\n    779             msg = (\'Expecting %d columns, got %d in row %d\' %\n    780                    (col_len, zip_len, row_num))\n--> 781             raise ValueError(msg)\n    782 \n    783         data = dict((k, v) for k, v in izip(self.columns, zipped_content))\n\nValueError: Expecting 23 columns, got 26 in row 64\n']";"['In [3]:\n\ninfile =open(easygui.fileopenbox(),""r"")\npledge = read_csv(infile,parse_dates=\'true\')\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-3-b35e7a16b389> in <module>()\n      1 infile =open(easygui.fileopenbox(),""r"")\n      2 \n----> 3 pledge = read_csv(infile,parse_dates=\'true\')\n\n\nC:\\Python27\\lib\\site-packages\\pandas-0.8.1-py2.7-win32.egg\\pandas\\io\\parsers.pyc in read_csv(filepath_or_buffer, sep, dialect, header, index_col, names, skiprows, na_values, thousands, comment, parse_dates, keep_date_col, dayfirst, date_parser, nrows, iterator, chunksize, skip_footer, converters, verbose, delimiter, encoding, squeeze)\n    234         kwds[\'delimiter\'] = sep\n    235 \n--> 236     return _read(TextParser, filepath_or_buffer, kwds)\n    237 \n    238 @Appender(_read_table_doc)\n\nC:\\Python27\\lib\\site-packages\\pandas-0.8.1-py2.7-win32.egg\\pandas\\io\\parsers.pyc in _read(cls, filepath_or_buffer, kwds)\n    189         return parser\n    190 \n--> 191     return parser.get_chunk()\n    192 \n    193 @Appender(_read_csv_doc)\n\nC:\\Python27\\lib\\site-packages\\pandas-0.8.1-py2.7-win32.egg\\pandas\\io\\parsers.pyc in get_chunk(self, rows)\n    779             msg = (\'Expecting %d columns, got %d in row %d\' %\n    780                    (col_len, zip_len, row_num))\n--> 781             raise ValueError(msg)\n    782 \n    783         data = dict((k, v) for k, v in izip(self.columns, zipped_content))\n\nValueError: Expecting 23 columns, got 26 in row 64\n']";"['\ninfile =open(easygui.fileopenbox(),""r"")\npledge = read_csv(infile,parse_dates=\'true\')\n\n\n\n\n\n\n\n']";"['\ninfile =open(easygui.fileopenbox(),""r"")\npledge = read_csv(infile,parse_dates=\'true\')\n\n\n\n\n\n\n\n']";False;"['import pandas as pd\n\ninfile =open(easygui.fileopenbox(),""r"")\npledge = read_csv(infile,parse_dates=\'true\')\n\n\n\n\n\n\n\n']";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
583;583;583;583;4.0;5;20158597;;1;29;<python><pandas>;How to qcut with non unique bin edges?;8987.0;['def fractile_cut(ser, num_fractiles):\n    num_valid = ser.valid().shape[0]\n    remain_fractiles = num_fractiles\n    vcounts = ser.value_counts()\n    high_freq = []\n    i = 0\n    while vcounts.iloc[i] > num_valid/ float(remain_fractiles):\n        curr_val = vcounts.index[i]\n        high_freq.append(curr_val)\n        remain_fractiles -= 1\n        num_valid = num_valid - vcounts[i]\n        i += 1\n    curr_ser = ser.copy()\n    curr_ser = curr_ser[~curr_ser.isin(high_freq)]\n    qcut = pd.qcut(curr_ser, remain_fractiles, retbins=True)\n    qcut_bins = qcut[1]\n    all_bins = list(qcut_bins)\n    for val in high_freq:\n        bisect.insort(all_bins, val)\n    cut = pd.cut(ser, bins=all_bins)\n    ser_fractiles = pd.Series(cut.labels + 1, index=ser.index)\n    return ser_fractiles\n'];['def fractile_cut(ser, num_fractiles):\n    num_valid = ser.valid().shape[0]\n    remain_fractiles = num_fractiles\n    vcounts = ser.value_counts()\n    high_freq = []\n    i = 0\n    while vcounts.iloc[i] > num_valid/ float(remain_fractiles):\n        curr_val = vcounts.index[i]\n        high_freq.append(curr_val)\n        remain_fractiles -= 1\n        num_valid = num_valid - vcounts[i]\n        i += 1\n    curr_ser = ser.copy()\n    curr_ser = curr_ser[~curr_ser.isin(high_freq)]\n    qcut = pd.qcut(curr_ser, remain_fractiles, retbins=True)\n    qcut_bins = qcut[1]\n    all_bins = list(qcut_bins)\n    for val in high_freq:\n        bisect.insort(all_bins, val)\n    cut = pd.cut(ser, bins=all_bins)\n    ser_fractiles = pd.Series(cut.labels + 1, index=ser.index)\n    return ser_fractiles\n'];['def fractile_cut(ser, num_fractiles):\n    num_valid = ser.valid().shape[0]\n    remain_fractiles = num_fractiles\n    vcounts = ser.value_counts()\n    high_freq = []\n    i = 0\n    while vcounts.iloc[i] > num_valid/ float(remain_fractiles):\n        curr_val = vcounts.index[i]\n        high_freq.append(curr_val)\n        remain_fractiles -= 1\n        num_valid = num_valid - vcounts[i]\n        i += 1\n    curr_ser = ser.copy()\n    curr_ser = curr_ser[~curr_ser.isin(high_freq)]\n    qcut = pd.qcut(curr_ser, remain_fractiles, retbins=True)\n    qcut_bins = qcut[1]\n    all_bins = list(qcut_bins)\n    for val in high_freq:\n        bisect.insort(all_bins, val)\n    cut = pd.cut(ser, bins=all_bins)\n    ser_fractiles = pd.Series(cut.labels + 1, index=ser.index)\n    return ser_fractiles\n'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""module 'pandas.tools' has no attribute 'tile'"", ""name 'pd' is not defined""]";['AttributeError', 'NameError'];0;2;"[""module 'pandas.tools' has no attribute 'tile'"", ""unsupported operand type(s) for +: 'range' and 'list'""]";['AttributeError', 'TypeError'];0;2;"[""module 'pandas.tools' has no attribute 'tile'"", ""unsupported operand type(s) for +: 'range' and 'list'""]";['AttributeError', 'TypeError']
584;584;584;584;5.0;2;20167194;;1;11;<python><mongodb><python-2.7><pandas><pymongo>;Insert a Pandas Dataframe into mongodb using PyMongo;13066.0;"[""db.myCollection.insert(df.to_dict())\ndb.myCollection.insert(df.to_json())\ndb.myCollection.insert({id: df.to_json()})\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 150 entries, 2013-11-23 13:31:26 to 2013-11-23 13:24:07\nData columns (total 3 columns):\namount    150  non-null values\nprice     150  non-null values\ntid       150  non-null values\ndtypes: float64(2), int64(1)\n""]";"['db.myCollection.insert(df.to_dict())\n', 'db.myCollection.insert(df.to_json())\n', 'db.myCollection.insert({id: df.to_json()})\n', ""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 150 entries, 2013-11-23 13:31:26 to 2013-11-23 13:24:07\nData columns (total 3 columns):\namount    150  non-null values\nprice     150  non-null values\ntid       150  non-null values\ndtypes: float64(2), int64(1)\n""]";"['PyMongo', 'db.myCollection.insert(df.to_dict())\n', ""InvalidDocument: documents must have only string keys, key was Timestamp('2013-11-23 13:31:00', tz=None)"", 'db.myCollection.insert(df.to_json())\n', ""TypeError: 'str' object does not support item assignment"", 'db.myCollection.insert({id: df.to_json()})\n', 'InvalidDocument: documents must have only string keys, key was <built-in function id>', ""<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 150 entries, 2013-11-23 13:31:26 to 2013-11-23 13:24:07\nData columns (total 3 columns):\namount    150  non-null values\nprice     150  non-null values\ntid       150  non-null values\ndtypes: float64(2), int64(1)\n""]";['db.myCollection.insert(df.to_dict())\ndb.myCollection.insert(df.to_json())\ndb.myCollection.insert({id: df.to_json()})\n'];['db.myCollection.insert(df.to_dict())\ndb.myCollection.insert(df.to_json())\ndb.myCollection.insert({id: df.to_json()})\n'];False;['import pandas as pd\ndf = pd.DataFrame()\ndb.myCollection.insert(df.to_dict())\ndb.myCollection.insert(df.to_json())\ndb.myCollection.insert({id: df.to_json()})\n'];True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
585;585;585;585;4.0;0;20167930;;1;14;<python><csv><pandas>;Start index at 1 when writing Pandas DataFrame to CSV;8795.0;"[""In [1]: import pandas as pd\n\nIn [2]: result = pd.DataFrame({'Count': [83, 19, 20]})\n\nIn [3]: result.to_csv('result.csv', index_label='Event_id')                               \nIn [4]: !cat result.csv\nEvent_id,Count\n0,83\n1,19\n2,20\nIn [5]: !cat result2.csv\nEvent_id,Count\n1,83\n2,19\n3,20\n""]";"[""In [1]: import pandas as pd\n\nIn [2]: result = pd.DataFrame({'Count': [83, 19, 20]})\n\nIn [3]: result.to_csv('result.csv', index_label='Event_id')                               \n"", 'In [4]: !cat result.csv\nEvent_id,Count\n0,83\n1,19\n2,20\n', 'In [5]: !cat result2.csv\nEvent_id,Count\n1,83\n2,19\n3,20\n']";"[""In [1]: import pandas as pd\n\nIn [2]: result = pd.DataFrame({'Count': [83, 19, 20]})\n\nIn [3]: result.to_csv('result.csv', index_label='Event_id')                               \n"", 'In [4]: !cat result.csv\nEvent_id,Count\n0,83\n1,19\n2,20\n', 'In [5]: !cat result2.csv\nEvent_id,Count\n1,83\n2,19\n3,20\n']";['\n\nEvent_id,Count\n0,83\n1,19\n2,20\nEvent_id,Count\n1,83\n2,19\n3,20\n'];['\n\nEvent_id,Count\n0,83\n1,19\n2,20\nEvent_id,Count\n1,83\n2,19\n3,20\n'];False;['import pandas as pd\n\n\nEvent_id,Count\n0,83\n1,19\n2,20\nEvent_id,Count\n1,83\n2,19\n3,20\n'];False;0;1;"[""name 'Int64Index' is not defined""]";['NameError'];0;1;"[""name 'Int64Index' is not defined""]";['NameError'];0;1;"[""name 'Int64Index' is not defined""]";['NameError']
586;586;586;586;1.0;0;20181456;;1;14;<python><python-2.7><pandas>;Sum up column values in Pandas DataFrame;28899.0;"['data = {""score"":{""0"":9.397,""1"":9.397,""2"":9.397995,""3"":9.397996,""4"":9.3999},""type"":{""0"":""advanced"",""1"":""advanced"",""2"":""advanced"",""3"":""newbie"",""4"":""expert""},""count"":{""0"":394.18930604,""1"":143.14226729,""2"":9.64172783,""3"":0.1,""4"":19.65413734}}\ndf = pd.DataFrame(data)\ndf\n     count       score       type\n0    394.189306  9.397000    advanced\n1    143.142267  9.397000    advanced\n2    9.641728    9.397995    advanced\n3    0.100000    9.397996    newbie\n4    19.654137   9.399900    expert\n     count       score       type\n0    537.331573  9.397000    advanced\n1    9.641728    9.397995    advanced\n2    0.100000    9.397996    newbie\n3    19.654137   9.399900    expert\n']";"['data = {""score"":{""0"":9.397,""1"":9.397,""2"":9.397995,""3"":9.397996,""4"":9.3999},""type"":{""0"":""advanced"",""1"":""advanced"",""2"":""advanced"",""3"":""newbie"",""4"":""expert""},""count"":{""0"":394.18930604,""1"":143.14226729,""2"":9.64172783,""3"":0.1,""4"":19.65413734}}\ndf = pd.DataFrame(data)\ndf\n', '     count       score       type\n0    394.189306  9.397000    advanced\n1    143.142267  9.397000    advanced\n2    9.641728    9.397995    advanced\n3    0.100000    9.397996    newbie\n4    19.654137   9.399900    expert\n', '     count       score       type\n0    537.331573  9.397000    advanced\n1    9.641728    9.397995    advanced\n2    0.100000    9.397996    newbie\n3    19.654137   9.399900    expert\n']";"['data = {""score"":{""0"":9.397,""1"":9.397,""2"":9.397995,""3"":9.397996,""4"":9.3999},""type"":{""0"":""advanced"",""1"":""advanced"",""2"":""advanced"",""3"":""newbie"",""4"":""expert""},""count"":{""0"":394.18930604,""1"":143.14226729,""2"":9.64172783,""3"":0.1,""4"":19.65413734}}\ndf = pd.DataFrame(data)\ndf\n', '     count       score       type\n0    394.189306  9.397000    advanced\n1    143.142267  9.397000    advanced\n2    9.641728    9.397995    advanced\n3    0.100000    9.397996    newbie\n4    19.654137   9.399900    expert\n', 'score', 'type', '     count       score       type\n0    537.331573  9.397000    advanced\n1    9.641728    9.397995    advanced\n2    0.100000    9.397996    newbie\n3    19.654137   9.399900    expert\n']";"['data = {""score"":{""0"":9.397,""1"":9.397,""2"":9.397995,""3"":9.397996,""4"":9.3999},""type"":{""0"":""advanced"",""1"":""advanced"",""2"":""advanced"",""3"":""newbie"",""4"":""expert""},""count"":{""0"":394.18930604,""1"":143.14226729,""2"":9.64172783,""3"":0.1,""4"":19.65413734}}\ndf = pd.DataFrame(data)\ndf\n']";"['import pandas as pd\ndata = {""score"":{""0"":9.397,""1"":9.397,""2"":9.397995,""3"":9.397996,""4"":9.3999},""type"":{""0"":""advanced"",""1"":""advanced"",""2"":""advanced"",""3"":""newbie"",""4"":""expert""},""count"":{""0"":394.18930604,""1"":143.14226729,""2"":9.64172783,""3"":0.1,""4"":19.65413734}}\ndf = pd.DataFrame(data)\ndf\n']";True;"['import pandas as pd\ndata = {""score"":{""0"":9.397,""1"":9.397,""2"":9.397995,""3"":9.397996,""4"":9.3999},""type"":{""0"":""advanced"",""1"":""advanced"",""2"":""advanced"",""3"":""newbie"",""4"":""expert""},""count"":{""0"":394.18930604,""1"":143.14226729,""2"":9.64172783,""3"":0.1,""4"":19.65413734}}\ndf = pd.DataFrame(data)\ndf\n']";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
587;587;587;587;5.0;1;20199129;;1;15;<python><indexing><pandas>;Pandas: Get duplicated indexes;9123.0;"[""import pandas as pd\nwget https://www.dropbox.com/s/vmimze2g4lt4ud3/alt_exon_repeatmasker_intersect.bed\nalt_exon_repeatmasker = pd.read_table('alt_exon_repeatmasker_intersect.bed', header=None, index_col=3)\n\nIn [74]: alt_exon_repeatmasker.index.is_unique\nOut[74]: False\ngenome_location1    MIR3\ngenome_location1    AluJb\ngenome_location2    Tigger1\ngenome_location3    AT_rich\ngenome_location1    MIR3\ngenome_location1    AluJb\n""]";"[""import pandas as pd\nwget https://www.dropbox.com/s/vmimze2g4lt4ud3/alt_exon_repeatmasker_intersect.bed\nalt_exon_repeatmasker = pd.read_table('alt_exon_repeatmasker_intersect.bed', header=None, index_col=3)\n\nIn [74]: alt_exon_repeatmasker.index.is_unique\nOut[74]: False\n"", 'genome_location1    MIR3\ngenome_location1    AluJb\ngenome_location2    Tigger1\ngenome_location3    AT_rich\n', 'genome_location1    MIR3\ngenome_location1    AluJb\n']";"[""import pandas as pd\nwget https://www.dropbox.com/s/vmimze2g4lt4ud3/alt_exon_repeatmasker_intersect.bed\nalt_exon_repeatmasker = pd.read_table('alt_exon_repeatmasker_intersect.bed', header=None, index_col=3)\n\nIn [74]: alt_exon_repeatmasker.index.is_unique\nOut[74]: False\n"", 'groupby', 'groupby', 'genome_location1    MIR3\ngenome_location1    AluJb\ngenome_location2    Tigger1\ngenome_location3    AT_rich\n', 'genome_location1    MIR3\ngenome_location1    AluJb\n']";['alt_exon_repeatmasker.index.is_unique\n'];['alt_exon_repeatmasker.index.is_unique\n'];False;['import pandas as pd\nalt_exon_repeatmasker.index.is_unique\n'];False;2;3;"['Sucess', ""name 'df' is not defined"", 'Sucess']";['Sucess', 'NameError', 'Sucess'];2;3;"['Sucess', ""name 'df' is not defined"", 'Sucess']";['Sucess', 'NameError', 'Sucess'];2;3;"['Sucess', ""'type'"", 'Sucess']";['Sucess', 'KeyError', 'Sucess']
588;588;588;588;3.0;1;20206615;;1;17;<python><pandas>;How can a pandas merge preserve order?;6052.0;"[""import pandas\nloans = [  'a',  'b', 'c' ]\nstates = [  'OR',  'CA', 'OR' ]\nx = pandas.DataFrame({ 'loan' : loans, 'state' : states })\ny = pandas.DataFrame({ 'state' : [ 'CA', 'OR' ], 'value' : [ 1, 2]})\nz = x.merge(y, how='left', on='state')\n""]";"[""import pandas\nloans = [  'a',  'b', 'c' ]\nstates = [  'OR',  'CA', 'OR' ]\nx = pandas.DataFrame({ 'loan' : loans, 'state' : states })\ny = pandas.DataFrame({ 'state' : [ 'CA', 'OR' ], 'value' : [ 1, 2]})\nz = x.merge(y, how='left', on='state')\n""]";"[""import pandas\nloans = [  'a',  'b', 'c' ]\nstates = [  'OR',  'CA', 'OR' ]\nx = pandas.DataFrame({ 'loan' : loans, 'state' : states })\ny = pandas.DataFrame({ 'state' : [ 'CA', 'OR' ], 'value' : [ 1, 2]})\nz = x.merge(y, how='left', on='state')\n""]";"[""import pandas\nloans = [  'a',  'b', 'c' ]\nstates = [  'OR',  'CA', 'OR' ]\nx = pandas.DataFrame({ 'loan' : loans, 'state' : states })\ny = pandas.DataFrame({ 'state' : [ 'CA', 'OR' ], 'value' : [ 1, 2]})\nz = x.merge(y, how='left', on='state')\n""]";"[""import pandas\nloans = [  'a',  'b', 'c' ]\nstates = [  'OR',  'CA', 'OR' ]\nx = pandas.DataFrame({ 'loan' : loans, 'state' : states })\ny = pandas.DataFrame({ 'state' : [ 'CA', 'OR' ], 'value' : [ 1, 2]})\nz = x.merge(y, how='left', on='state')\n""]";False;"[""import pandas as pd\nimport pandas\nloans = [  'a',  'b', 'c' ]\nstates = [  'OR',  'CA', 'OR' ]\nx = pandas.DataFrame({ 'loan' : loans, 'state' : states })\ny = pandas.DataFrame({ 'state' : [ 'CA', 'OR' ], 'value' : [ 1, 2]})\nz = x.merge(y, how='left', on='state')\n""]";False;0;1;"[""name 'x' is not defined""]";['NameError'];0;1;"[""name 'x' is not defined""]";['NameError'];0;1;"[""name 'x' is not defined""]";['NameError']
589;589;589;589;5.0;5;20219254;;1;46;<python><excel><python-2.7><pandas>;How to write to an existing excel file without overwriting data (using pandas)?;29344.0;"['import pandas\n\nwriter = pandas.ExcelWriter(\'Masterfile.xlsx\') \n\ndata_filtered.to_excel(writer, ""Main"", cols=[\'Diff1\', \'Diff2\'])\n\nwriter.save()\n']";"['import pandas\n\nwriter = pandas.ExcelWriter(\'Masterfile.xlsx\') \n\ndata_filtered.to_excel(writer, ""Main"", cols=[\'Diff1\', \'Diff2\'])\n\nwriter.save()\n']";"['import pandas\n\nwriter = pandas.ExcelWriter(\'Masterfile.xlsx\') \n\ndata_filtered.to_excel(writer, ""Main"", cols=[\'Diff1\', \'Diff2\'])\n\nwriter.save()\n']";"['import pandas\n\nwriter = pandas.ExcelWriter(\'Masterfile.xlsx\') \n\ndata_filtered.to_excel(writer, ""Main"", cols=[\'Diff1\', \'Diff2\'])\n\nwriter.save()\n']";"['import pandas\n\nwriter = pandas.ExcelWriter(\'Masterfile.xlsx\') \n\ndata_filtered.to_excel(writer, ""Main"", cols=[\'Diff1\', \'Diff2\'])\n\nwriter.save()\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\nimport pandas\n\nwriter = pandas.ExcelWriter(\'Masterfile.xlsx\') \n\ndata_filtered.to_excel(writer, ""Main"", cols=[\'Diff1\', \'Diff2\'])\n\nwriter.save()\n']";True;0;2;"[""No module named 'openpyxl'"", ""name 'pd' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'openpyxl'"", ""name 'excel_file' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'openpyxl'"", ""name 'excel_file' is not defined""]";['ImportError', 'NameError']
590;590;590;590;5.0;3;20225110;;1;18;<python><pandas><dataframe>;Comparing two dataframes and getting the differences;29288.0;['df1:\nDate       Fruit  Num  Color \n2013-11-24 Banana 22.1 Yellow\n2013-11-24 Orange  8.6 Orange\n2013-11-24 Apple   7.6 Green\n2013-11-24 Celery 10.2 Green\n\ndf2:\nDate       Fruit  Num  Color \n2013-11-24 Banana 22.1 Yellow\n2013-11-24 Orange  8.6 Orange\n2013-11-24 Apple   7.6 Green\n2013-11-24 Celery 10.2 Green\n2013-11-25 Apple  22.1 Red\n2013-11-25 Orange  8.6 Orange\n'];['df1:\nDate       Fruit  Num  Color \n2013-11-24 Banana 22.1 Yellow\n2013-11-24 Orange  8.6 Orange\n2013-11-24 Apple   7.6 Green\n2013-11-24 Celery 10.2 Green\n\ndf2:\nDate       Fruit  Num  Color \n2013-11-24 Banana 22.1 Yellow\n2013-11-24 Orange  8.6 Orange\n2013-11-24 Apple   7.6 Green\n2013-11-24 Celery 10.2 Green\n2013-11-25 Apple  22.1 Red\n2013-11-25 Orange  8.6 Orange\n'];['df1:\nDate       Fruit  Num  Color \n2013-11-24 Banana 22.1 Yellow\n2013-11-24 Orange  8.6 Orange\n2013-11-24 Apple   7.6 Green\n2013-11-24 Celery 10.2 Green\n\ndf2:\nDate       Fruit  Num  Color \n2013-11-24 Banana 22.1 Yellow\n2013-11-24 Orange  8.6 Orange\n2013-11-24 Apple   7.6 Green\n2013-11-24 Celery 10.2 Green\n2013-11-25 Apple  22.1 Red\n2013-11-25 Orange  8.6 Orange\n'];['\n'];['\n'];False;['import pandas as pd\n\n'];False;1;2;"['Sucess', ""name 'DF1' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'DF1' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'DF1' is not defined""]";['Sucess', 'NameError']
591;591;591;591;4.0;0;20230326;;1;29;<python><pandas><dataframe>;Retrieve DataFrame of all but one specified column;17402.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'column_name' is not defined"", '""[\'col1\' \'col2\' \'col3\'] not in index""', '\n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated']";['NameError', 'KeyError', 'DeprecationWarning']
592;592;592;592;1.0;4;20233071;;1;21;<python><pandas>;Filter Pandas DataFrame by time index;15944.0;"[""df = df[df.index < '2013-10-16 08:00:00']\n""]";"[""df = df[df.index < '2013-10-16 08:00:00']\n""]";"[""df = df[df.index < '2013-10-16 08:00:00']\n""]";"[""df = df[df.index < '2013-10-16 08:00:00']\n""]";"[""df = df[df.index < '2013-10-16 08:00:00']\n""]";False;"[""import pandas as pd\ndf = df[df.index < '2013-10-16 08:00:00']\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
593;593;593;593;1.0;0;20235401;;1;34;<python><pandas><series>;Remove NaN from pandas series;43902.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
594;594;594;594;3.0;0;20250771;;1;69;<python><dictionary><pandas><remap>;Remap values in pandas column with a dict;48507.0;['     col1   col2\n0       w      a\n1       1      2\n2       2    NaN\n     col1   col2\n0       w      a\n1       A      2\n2       B    NaN\n'];['     col1   col2\n0       w      a\n1       1      2\n2       2    NaN\n', '     col1   col2\n0       w      a\n1       A      2\n2       B    NaN\n'];"['di = {1: ""A"", 2: ""B""}', '     col1   col2\n0       w      a\n1       1      2\n2       2    NaN\n', '     col1   col2\n0       w      a\n1       A      2\n2       B    NaN\n']";[''];[''];False;['import pandas as pd\n'];False;1;3;"[""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""'col1'"", 'Sucess', ""name 'df' is not defined""]";['KeyError', 'Sucess', 'NameError']
595;595;595;595;3.0;1;20297317;;1;37;<python><pandas><dataframe>;python dataframe pandas drop column using int;28907.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", 'index 1 is out of bounds for axis 1 with size 0']";['NameError', 'IndexError']
596;596;596;596;3.0;1;20297332;;1;52;<python><pandas><dataframe>;Python pandas dataframe: retrieve number of columns;66459.0;['df.num_columns\n'];['df.num_columns\n'];['df.num_columns\n'];['df.num_columns\n'];['df.num_columns\n'];False;['import pandas as pd\ndf = pd.DataFrame()\ndf.num_columns\n'];True;1;3;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess']
597;597;597;597;1.0;0;20333435;;1;11;<python><pandas><typeerror><dataframe>;pandas comparison raises TypeError: cannot compare a dtyped [float64] array with a scalar of type [bool];7916.0;"[""Index: 1008 entries, Trial1.0 to Trial3.84\nData columns (total 5 columns):\nCHUNK_NAME                    1008  non-null values\nLAMBDA                        1008  non-null values\nBETA                          1008  non-null values\nHIT_RATE                      1008  non-null values\nAVERAGE_RECIPROCAL_HITRATE    1008  non-null values\n\nchunks=['300_321','322_343','344_365','366_387','388_408','366_408','344_408','322_408','300_408']\nlam_beta=[(lambda1,beta1),(lambda1,beta2),(lambda1,beta3),...(lambda1,beta_n),(lambda2,beta1),(lambda2,beta2)...(lambda2,beta_n),........]\n\nmy_df.ix[my_df.CHUNK_NAME==chunks[0]&my_df.LAMBDA==lam_beta[0][0]]\nTypeError: cannot compare a dtyped [float64] array with a scalar of type [bool]\n""]";"[""Index: 1008 entries, Trial1.0 to Trial3.84\nData columns (total 5 columns):\nCHUNK_NAME                    1008  non-null values\nLAMBDA                        1008  non-null values\nBETA                          1008  non-null values\nHIT_RATE                      1008  non-null values\nAVERAGE_RECIPROCAL_HITRATE    1008  non-null values\n\nchunks=['300_321','322_343','344_365','366_387','388_408','366_408','344_408','322_408','300_408']\nlam_beta=[(lambda1,beta1),(lambda1,beta2),(lambda1,beta3),...(lambda1,beta_n),(lambda2,beta1),(lambda2,beta2)...(lambda2,beta_n),........]\n\nmy_df.ix[my_df.CHUNK_NAME==chunks[0]&my_df.LAMBDA==lam_beta[0][0]]\n"", 'TypeError: cannot compare a dtyped [float64] array with a scalar of type [bool]\n']";"[""Index: 1008 entries, Trial1.0 to Trial3.84\nData columns (total 5 columns):\nCHUNK_NAME                    1008  non-null values\nLAMBDA                        1008  non-null values\nBETA                          1008  non-null values\nHIT_RATE                      1008  non-null values\nAVERAGE_RECIPROCAL_HITRATE    1008  non-null values\n\nchunks=['300_321','322_343','344_365','366_387','388_408','366_408','344_408','322_408','300_408']\nlam_beta=[(lambda1,beta1),(lambda1,beta2),(lambda1,beta3),...(lambda1,beta_n),(lambda2,beta1),(lambda2,beta2)...(lambda2,beta_n),........]\n\nmy_df.ix[my_df.CHUNK_NAME==chunks[0]&my_df.LAMBDA==lam_beta[0][0]]\n"", 'TypeError: cannot compare a dtyped [float64] array with a scalar of type [bool]\n']";"[""\nchunks=['300_321','322_343','344_365','366_387','388_408','366_408','344_408','322_408','300_408']\n\nmy_df.ix[my_df.CHUNK_NAME==chunks[0]&my_df.LAMBDA==lam_beta[0][0]]\n""]";"[""\nchunks=['300_321','322_343','344_365','366_387','388_408','366_408','344_408','322_408','300_408']\n\nmy_df.ix[my_df.CHUNK_NAME==chunks[0]&my_df.LAMBDA==lam_beta[0][0]]\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\n\nchunks=['300_321','322_343','344_365','366_387','388_408','366_408','344_408','322_408','300_408']\n\nmy_df.ix[my_df.CHUNK_NAME==chunks[0]&my_df.LAMBDA==lam_beta[0][0]]\n""]";True;0;1;"[""name 'my_df' is not defined""]";['NameError'];0;1;"[""name 'my_df' is not defined""]";['NameError'];0;1;"[""name 'my_df' is not defined""]";['NameError']
598;598;598;598;2.0;0;20340844;;1;13;<python><pandas><typeerror><dataframe>;pandas create named columns in dataframe from dict;15536.0;"[""my_dict = {id1: val1, id2: val2, id3: val3, ...}\nbusiness_df = DataFrame.from_dict(my_dict,orient='index',columns=['business_id','business_code'])\nTypeError: from_dict() got an unexpected keyword argument 'columns' \n""]";"['my_dict = {id1: val1, id2: val2, id3: val3, ...}\n', ""business_df = DataFrame.from_dict(my_dict,orient='index',columns=['business_id','business_code'])\n"", ""TypeError: from_dict() got an unexpected keyword argument 'columns' \n""]";"['my_dict = {id1: val1, id2: val2, id3: val3, ...}\n', ""business_df = DataFrame.from_dict(my_dict,orient='index',columns=['business_id','business_code'])\n"", 'from_dict', ""TypeError: from_dict() got an unexpected keyword argument 'columns' \n""]";"[""business_df = DataFrame.from_dict(my_dict,orient='index',columns=['business_id','business_code'])\n""]";"[""from pandas import DataFrame\nbusiness_df = DataFrame.from_dict(my_dict,orient='index',columns=['business_id','business_code'])\n""]";True;"[""import pandas as pd\nbusiness_df = DataFrame.from_dict(my_dict,orient='index',columns=['business_id','business_code'])\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
599;599;599;599;2.0;0;20375561;;1;28;<python><pandas><dataframe>;Joining pandas dataframes by column names;16792.0;['frame_1:\nevent_id, date, time, county_ID\n\nframe_2:\ncountyid, state\njoined_dataframe\nevent_id, date, time, county, state\n'];['frame_1:\nevent_id, date, time, county_ID\n\nframe_2:\ncountyid, state\n', 'joined_dataframe\nevent_id, date, time, county, state\n'];['frame_1:\nevent_id, date, time, county_ID\n\nframe_2:\ncountyid, state\n', 'county_ID = countyid', 'joined_dataframe\nevent_id, date, time, county, state\n'];['event_id, date, time, county_ID\n\ncountyid, state\njoined_dataframe\nevent_id, date, time, county, state\n'];['event_id, date, time, county_ID\n\ncountyid, state\njoined_dataframe\nevent_id, date, time, county, state\n'];False;['import pandas as pd\ndata = pd.DataFrame()\nevent_id, date, time, county_ID\n\ncountyid, state\njoined_dataframe\nevent_id, date, time, county, state\n'];True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'frame_1' is not defined""]";['NameError'];0;1;"[""name 'frame_1' is not defined""]";['NameError']
600;600;600;600;3.0;0;20383647;;1;26;<python><pandas>;Pandas selecting by label sometimes return series, sometimes returns dataframe;7456.0;['In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame(data=range(5), index=[1, 2, 3, 3, 3])\n\nIn [3]: type(df.loc[3])\nOut[3]: pandas.core.frame.DataFrame\n\nIn [4]: type(df.loc[1])\nOut[4]: pandas.core.series.Series\n'];['In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame(data=range(5), index=[1, 2, 3, 3, 3])\n\nIn [3]: type(df.loc[3])\nOut[3]: pandas.core.frame.DataFrame\n\nIn [4]: type(df.loc[1])\nOut[4]: pandas.core.series.Series\n'];['In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame(data=range(5), index=[1, 2, 3, 3, 3])\n\nIn [3]: type(df.loc[3])\nOut[3]: pandas.core.frame.DataFrame\n\nIn [4]: type(df.loc[1])\nOut[4]: pandas.core.series.Series\n'];['import pandas as pd\n\n\ntype(df.loc[1])\n'];['import pandas as pd\n\n\ntype(df.loc[1])\n'];False;['import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\n\n\ntype(df.loc[1])\n'];True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'None of [[3]] are in the [index]'""]";['KeyError']
601;601;601;601;6.0;2;20410312;;1;18;<pandas>;How to create a lagged data structure using pandas dataframe;12939.0;"[""s=pd.Series([5,4,3,2,1], index=[1,2,3,4,5])\nprint s \n1    5\n2    4\n3    3\n4    2\n5    1\n3    [3, 4, 5]\n4    [2, 3, 4]\n5    [1, 2, 3]\ndef buildLaggedFeatures(s,lag=2,dropna=True):\n'''\nBuilds a new DataFrame to facilitate regressing over all possible lagged features\n'''\nif type(s) is pd.DataFrame:\n    new_dict={}\n    for col_name in s:\n        new_dict[col_name]=s[col_name]\n        # create lagged Series\n        for l in range(1,lag+1):\n            new_dict['%s_lag%d' %(col_name,l)]=s[col_name].shift(l)\n    res=pd.DataFrame(new_dict,index=s.index)\n\nelif type(s) is pd.Series:\n    the_range=range(lag+1)\n    res=pd.concat([s.shift(i) for i in the_range],axis=1)\n    res.columns=['lag_%d' %i for i in the_range]\nelse:\n    print 'Only works for DataFrame or Series'\n    return None\nif dropna:\n    return res.dropna()\nelse:\n    return res \ns=pd.Series([5,4,3,2,1], index=[1,2,3,4,5])\nres=buildLaggedFeatures(s,lag=2,dropna=False)\n   lag_0  lag_1  lag_2\n1      5    NaN    NaN\n2      4      5    NaN\n3      3      4      5\n4      2      3      4\n5      1      2      3\ns2=s=pd.DataFrame({'a':[5,4,3,2,1], 'b':[50,40,30,20,10]},index=[1,2,3,4,5])\nres2=buildLaggedFeatures(s2,lag=2,dropna=True)\n\n   a  a_lag1  a_lag2   b  b_lag1  b_lag2\n3  3       4       5  30      40      50\n4  2       3       4  20      30      40\n5  1       2       3  10      20      30\n""]";"['s=pd.Series([5,4,3,2,1], index=[1,2,3,4,5])\nprint s \n1    5\n2    4\n3    3\n4    2\n5    1\n', '3    [3, 4, 5]\n4    [2, 3, 4]\n5    [1, 2, 3]\n', ""def buildLaggedFeatures(s,lag=2,dropna=True):\n'''\nBuilds a new DataFrame to facilitate regressing over all possible lagged features\n'''\nif type(s) is pd.DataFrame:\n    new_dict={}\n    for col_name in s:\n        new_dict[col_name]=s[col_name]\n        # create lagged Series\n        for l in range(1,lag+1):\n            new_dict['%s_lag%d' %(col_name,l)]=s[col_name].shift(l)\n    res=pd.DataFrame(new_dict,index=s.index)\n\nelif type(s) is pd.Series:\n    the_range=range(lag+1)\n    res=pd.concat([s.shift(i) for i in the_range],axis=1)\n    res.columns=['lag_%d' %i for i in the_range]\nelse:\n    print 'Only works for DataFrame or Series'\n    return None\nif dropna:\n    return res.dropna()\nelse:\n    return res \n"", 's=pd.Series([5,4,3,2,1], index=[1,2,3,4,5])\nres=buildLaggedFeatures(s,lag=2,dropna=False)\n   lag_0  lag_1  lag_2\n1      5    NaN    NaN\n2      4      5    NaN\n3      3      4      5\n4      2      3      4\n5      1      2      3\n', ""s2=s=pd.DataFrame({'a':[5,4,3,2,1], 'b':[50,40,30,20,10]},index=[1,2,3,4,5])\nres2=buildLaggedFeatures(s2,lag=2,dropna=True)\n\n   a  a_lag1  a_lag2   b  b_lag1  b_lag2\n3  3       4       5  30      40      50\n4  2       3       4  20      30      40\n5  1       2       3  10      20      30\n""]";"['s=pd.Series([5,4,3,2,1], index=[1,2,3,4,5])\nprint s \n1    5\n2    4\n3    3\n4    2\n5    1\n', '3    [3, 4, 5]\n4    [2, 3, 4]\n5    [1, 2, 3]\n', ""def buildLaggedFeatures(s,lag=2,dropna=True):\n'''\nBuilds a new DataFrame to facilitate regressing over all possible lagged features\n'''\nif type(s) is pd.DataFrame:\n    new_dict={}\n    for col_name in s:\n        new_dict[col_name]=s[col_name]\n        # create lagged Series\n        for l in range(1,lag+1):\n            new_dict['%s_lag%d' %(col_name,l)]=s[col_name].shift(l)\n    res=pd.DataFrame(new_dict,index=s.index)\n\nelif type(s) is pd.Series:\n    the_range=range(lag+1)\n    res=pd.concat([s.shift(i) for i in the_range],axis=1)\n    res.columns=['lag_%d' %i for i in the_range]\nelse:\n    print 'Only works for DataFrame or Series'\n    return None\nif dropna:\n    return res.dropna()\nelse:\n    return res \n"", 's=pd.Series([5,4,3,2,1], index=[1,2,3,4,5])\nres=buildLaggedFeatures(s,lag=2,dropna=False)\n   lag_0  lag_1  lag_2\n1      5    NaN    NaN\n2      4      5    NaN\n3      3      4      5\n4      2      3      4\n5      1      2      3\n', ""s2=s=pd.DataFrame({'a':[5,4,3,2,1], 'b':[50,40,30,20,10]},index=[1,2,3,4,5])\nres2=buildLaggedFeatures(s2,lag=2,dropna=True)\n\n   a  a_lag1  a_lag2   b  b_lag1  b_lag2\n3  3       4       5  30      40      50\n4  2       3       4  20      30      40\n5  1       2       3  10      20      30\n""]";"[""s=pd.Series([5,4,3,2,1], index=[1,2,3,4,5])\n3    [3, 4, 5]\n4    [2, 3, 4]\n5    [1, 2, 3]\n        # create lagged Series\n\ns=pd.Series([5,4,3,2,1], index=[1,2,3,4,5])\nres=buildLaggedFeatures(s,lag=2,dropna=False)\ns2=s=pd.DataFrame({'a':[5,4,3,2,1], 'b':[50,40,30,20,10]},index=[1,2,3,4,5])\nres2=buildLaggedFeatures(s2,lag=2,dropna=True)\n\n""]";"[""import pandas as pd\ns=pd.Series([5,4,3,2,1], index=[1,2,3,4,5])\n3    [3, 4, 5]\n4    [2, 3, 4]\n5    [1, 2, 3]\n        # create lagged Series\n\ns=pd.Series([5,4,3,2,1], index=[1,2,3,4,5])\nres=buildLaggedFeatures(s,lag=2,dropna=False)\ns2=s=pd.DataFrame({'a':[5,4,3,2,1], 'b':[50,40,30,20,10]},index=[1,2,3,4,5])\nres2=buildLaggedFeatures(s2,lag=2,dropna=True)\n\n""]";True;"[""import pandas as pd\ns=pd.Series([5,4,3,2,1], index=[1,2,3,4,5])\n3    [3, 4, 5]\n4    [2, 3, 4]\n5    [1, 2, 3]\n        # create lagged Series\n\ns=pd.Series([5,4,3,2,1], index=[1,2,3,4,5])\nres=buildLaggedFeatures(s,lag=2,dropna=False)\ns2=s=pd.DataFrame({'a':[5,4,3,2,1], 'b':[50,40,30,20,10]},index=[1,2,3,4,5])\nres2=buildLaggedFeatures(s2,lag=2,dropna=True)\n\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 's' is not defined""]";['NameError'];0;1;"[""name 's' is not defined""]";['NameError']
602;602;602;602;1.0;5;20444087;;1;28;<python><pandas><reverse>;Right way to reverse pandas.DataFrame?;31880.0;"['import pandas as pd\n\ndata = pd.DataFrame({\'Odd\':[1,3,5,6,7,9], \'Even\':[0,2,4,6,8,10]})\n\nfor i in reversed(data):\n    print(data[\'Odd\'], data[\'Even\'])\nTraceback (most recent call last):\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\generic.py"", line 665, in _get_item_cache\n    return cache[item]\nKeyError: 5\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""C:\\Users\\*****\\Documents\\******\\********\\****.py"", line 5, in <module>\n    for i in reversed(data):\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\frame.py"", line 2003, in __getitem__\n    return self._get_item_cache(key)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\generic.py"", line 667, in _get_item_cache\n    values = self._data.get(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1656, in get\n    _, block = self._find_block(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1936, in _find_block\n    self._check_have(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1943, in _check_have\n    raise KeyError(\'no item named %s\' % com.pprint_thing(item))\nKeyError: \'no item named 5\'\n']";"[""import pandas as pd\n\ndata = pd.DataFrame({'Odd':[1,3,5,6,7,9], 'Even':[0,2,4,6,8,10]})\n\nfor i in reversed(data):\n    print(data['Odd'], data['Even'])\n"", 'Traceback (most recent call last):\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\generic.py"", line 665, in _get_item_cache\n    return cache[item]\nKeyError: 5\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""C:\\Users\\*****\\Documents\\******\\********\\****.py"", line 5, in <module>\n    for i in reversed(data):\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\frame.py"", line 2003, in __getitem__\n    return self._get_item_cache(key)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\generic.py"", line 667, in _get_item_cache\n    values = self._data.get(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1656, in get\n    _, block = self._find_block(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1936, in _find_block\n    self._check_have(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1943, in _check_have\n    raise KeyError(\'no item named %s\' % com.pprint_thing(item))\nKeyError: \'no item named 5\'\n']";"[""import pandas as pd\n\ndata = pd.DataFrame({'Odd':[1,3,5,6,7,9], 'Even':[0,2,4,6,8,10]})\n\nfor i in reversed(data):\n    print(data['Odd'], data['Even'])\n"", 'Traceback (most recent call last):\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\generic.py"", line 665, in _get_item_cache\n    return cache[item]\nKeyError: 5\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""C:\\Users\\*****\\Documents\\******\\********\\****.py"", line 5, in <module>\n    for i in reversed(data):\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\frame.py"", line 2003, in __getitem__\n    return self._get_item_cache(key)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\generic.py"", line 667, in _get_item_cache\n    values = self._data.get(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1656, in get\n    _, block = self._find_block(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1936, in _find_block\n    self._check_have(item)\n  File ""C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py"", line 1943, in _check_have\n    raise KeyError(\'no item named %s\' % com.pprint_thing(item))\nKeyError: \'no item named 5\'\n', 'pandas.DataFrame']";"[""import pandas as pd\n\ndata = pd.DataFrame({'Odd':[1,3,5,6,7,9], 'Even':[0,2,4,6,8,10]})\n\n\n\n""]";"[""import pandas as pd\n\ndata = pd.DataFrame({'Odd':[1,3,5,6,7,9], 'Even':[0,2,4,6,8,10]})\n\n\n\n""]";False;"[""import pandas as pd\nimport pandas as pd\n\ndata = pd.DataFrame({'Odd':[1,3,5,6,7,9], 'Even':[0,2,4,6,8,10]})\n\n\n\n""]";False;0;1;"[""name 'data' is not defined""]";['NameError'];0;1;"[""name 'data' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
603;603;603;603;1.0;9;20444593;;1;12;<python><pandas><pickle>;Pandas compiled from source: default pickle behavior changed;2728.0;"[""In [1]: import pickle\n\nIn [2]: pickle.load(open('pickle_L1cor_s1.pic','rb'))\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-2-88719f8f9506> in <module>()\n----> 1 pickle.load(open('pickle_L1cor_s1.pic','rb'))\n\n/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in load(file)\n   1376\n   1377 def load(file):\n-> 1378     return Unpickler(file).load()\n   1379\n   1380 def loads(str):\n\n/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in load(self)\n    856             while 1:\n    857                 key = read(1)\n--> 858                 dispatch[key](self)\n    859         except _Stop, stopinst:\n    860             return stopinst.value\n\n/home/acorbe/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas-0.12.0_1090_g46008ec-py2.7-linux-x86_64.egg/pandas/compat/pickle_compat.pyc in load_reduce(self)\n     28\n     29         # try to reencode the arguments\n---> 30         if self.encoding is not None:\n     31             args = tuple([ arg.encode(self.encoding) if isinstance(arg, string_types)     else arg for arg in args ])\n     32             try:\n\nAttributeError: Unpickler instance has no attribute 'encoding'\nIn [4]: pickle.load(open('pickle_L1cor_s1.pic','rb'))\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-4-88719f8f9506> in <module>()\n----> 1 pickle.load(open('pickle_L1cor_s1.pic','rb'))\n\n/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in load(file)\n   1376\n   1377 def load(file):\n-> 1378     return Unpickler(file).load()\n   1379\n   1380 def loads(str):\n\n/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in load(self)\n    856             while 1:\n    857                 key = read(1)\n--> 858                 dispatch[key](self)\n    859         except _Stop, stopinst:\n    860             return stopinst.value\n\n/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in             load_reduce(self)\n   1131         args = stack.pop()\n   1132         func = stack[-1]\n-> 1133         value = func(*args)\n   1134         stack[-1] = value\n   1135     dispatch[REDUCE] = load_reduce\n\nTypeError: _reconstruct: First argument must be a sub-type of ndarray\nSize: 7.32 MB\nVersion: 0.12.0\nBuild: 2\nDependencies:\n numpy 1.7.1\n python_dateutil\n pytz 2011n\n\n  md5: 7dd4385bed058e6ac15b0841b312ae35\n""]";"[""In [1]: import pickle\n\nIn [2]: pickle.load(open('pickle_L1cor_s1.pic','rb'))\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-2-88719f8f9506> in <module>()\n----> 1 pickle.load(open('pickle_L1cor_s1.pic','rb'))\n\n/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in load(file)\n   1376\n   1377 def load(file):\n-> 1378     return Unpickler(file).load()\n   1379\n   1380 def loads(str):\n\n/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in load(self)\n    856             while 1:\n    857                 key = read(1)\n--> 858                 dispatch[key](self)\n    859         except _Stop, stopinst:\n    860             return stopinst.value\n\n/home/acorbe/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas-0.12.0_1090_g46008ec-py2.7-linux-x86_64.egg/pandas/compat/pickle_compat.pyc in load_reduce(self)\n     28\n     29         # try to reencode the arguments\n---> 30         if self.encoding is not None:\n     31             args = tuple([ arg.encode(self.encoding) if isinstance(arg, string_types)     else arg for arg in args ])\n     32             try:\n\nAttributeError: Unpickler instance has no attribute 'encoding'\n"", ""In [4]: pickle.load(open('pickle_L1cor_s1.pic','rb'))\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-4-88719f8f9506> in <module>()\n----> 1 pickle.load(open('pickle_L1cor_s1.pic','rb'))\n\n/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in load(file)\n   1376\n   1377 def load(file):\n-> 1378     return Unpickler(file).load()\n   1379\n   1380 def loads(str):\n\n/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in load(self)\n    856             while 1:\n    857                 key = read(1)\n--> 858                 dispatch[key](self)\n    859         except _Stop, stopinst:\n    860             return stopinst.value\n\n/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in             load_reduce(self)\n   1131         args = stack.pop()\n   1132         func = stack[-1]\n-> 1133         value = func(*args)\n   1134         stack[-1] = value\n   1135     dispatch[REDUCE] = load_reduce\n\nTypeError: _reconstruct: First argument must be a sub-type of ndarray\n"", 'Size: 7.32 MB\nVersion: 0.12.0\nBuild: 2\nDependencies:\n numpy 1.7.1\n python_dateutil\n pytz 2011n\n\n  md5: 7dd4385bed058e6ac15b0841b312ae35\n']";"['>>> setup.py install', 'pickle', 'pickle', ""In [1]: import pickle\n\nIn [2]: pickle.load(open('pickle_L1cor_s1.pic','rb'))\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-2-88719f8f9506> in <module>()\n----> 1 pickle.load(open('pickle_L1cor_s1.pic','rb'))\n\n/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in load(file)\n   1376\n   1377 def load(file):\n-> 1378     return Unpickler(file).load()\n   1379\n   1380 def loads(str):\n\n/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in load(self)\n    856             while 1:\n    857                 key = read(1)\n--> 858                 dispatch[key](self)\n    859         except _Stop, stopinst:\n    860             return stopinst.value\n\n/home/acorbe/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas-0.12.0_1090_g46008ec-py2.7-linux-x86_64.egg/pandas/compat/pickle_compat.pyc in load_reduce(self)\n     28\n     29         # try to reencode the arguments\n---> 30         if self.encoding is not None:\n     31             args = tuple([ arg.encode(self.encoding) if isinstance(arg, string_types)     else arg for arg in args ])\n     32             try:\n\nAttributeError: Unpickler instance has no attribute 'encoding'\n"", 'DataFrames', ""In [4]: pickle.load(open('pickle_L1cor_s1.pic','rb'))\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-4-88719f8f9506> in <module>()\n----> 1 pickle.load(open('pickle_L1cor_s1.pic','rb'))\n\n/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in load(file)\n   1376\n   1377 def load(file):\n-> 1378     return Unpickler(file).load()\n   1379\n   1380 def loads(str):\n\n/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in load(self)\n    856             while 1:\n    857                 key = read(1)\n--> 858                 dispatch[key](self)\n    859         except _Stop, stopinst:\n    860             return stopinst.value\n\n/home/acorbe/Canopy/appdata/canopy-1.1.0.1371.rh5-x86_64/lib/python2.7/pickle.pyc in             load_reduce(self)\n   1131         args = stack.pop()\n   1132         func = stack[-1]\n-> 1133         value = func(*args)\n   1134         stack[-1] = value\n   1135     dispatch[REDUCE] = load_reduce\n\nTypeError: _reconstruct: First argument must be a sub-type of ndarray\n"", 'Size: 7.32 MB\nVersion: 0.12.0\nBuild: 2\nDependencies:\n numpy 1.7.1\n python_dateutil\n pytz 2011n\n\n  md5: 7dd4385bed058e6ac15b0841b312ae35\n']";['\n\n\n\n\n\n\n\n\n\n'];['\n\n\n\n\n\n\n\n\n\n'];False;['import pandas as pd\n\n\n\n\n\n\n\n\n\n\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
604;604;604;604;1.0;0;20459536;;1;14;<python><numpy><pandas><scipy>;Convert Pandas dataframe to Sparse Numpy Matrix directly;11512.0;['dense_matrix = np.array(df.as_matrix(columns = None), dtype=bool).astype(np.int)\nsparse_matrix = scipy.sparse.csr_matrix(dense_matrix)\n'];['dense_matrix = np.array(df.as_matrix(columns = None), dtype=bool).astype(np.int)\n', 'sparse_matrix = scipy.sparse.csr_matrix(dense_matrix)\n'];['dense_matrix = np.array(df.as_matrix(columns = None), dtype=bool).astype(np.int)\n', 'sparse_matrix = scipy.sparse.csr_matrix(dense_matrix)\n'];['dense_matrix = np.array(df.as_matrix(columns = None), dtype=bool).astype(np.int)\nsparse_matrix = scipy.sparse.csr_matrix(dense_matrix)\n'];['dense_matrix = np.array(df.as_matrix(columns = None), dtype=bool).astype(np.int)\nsparse_matrix = scipy.sparse.csr_matrix(dense_matrix)\n'];False;['import pandas as pd\ndf = pd.DataFrame()\ndense_matrix = np.array(df.as_matrix(columns = None), dtype=bool).astype(np.int)\nsparse_matrix = scipy.sparse.csr_matrix(dense_matrix)\n'];True;0;1;"[""name 'scipy' is not defined""]";['NameError'];0;1;"[""name 'scipy' is not defined""]";['NameError'];0;1;"[""name 'scipy' is not defined""]";['NameError']
605;605;605;605;2.0;0;20461165;;1;132;<python><pandas>;How to convert pandas index in a dataframe to a column?;106030.0;['df=\n           gi  ptt_loc\n 0  384444683      593  \n 1  384444684      594 \n 2  384444686      596  \ndf=\n    index1       gi    ptt_loc\n 0  0     384444683      593  \n 1  1     384444684      594 \n 2  2     384444686      596  \n'];['df=\n           gi  ptt_loc\n 0  384444683      593  \n 1  384444684      594 \n 2  384444686      596  \n', 'df=\n    index1       gi    ptt_loc\n 0  0     384444683      593  \n 1  1     384444684      594 \n 2  2     384444686      596  \n'];['df=\n           gi  ptt_loc\n 0  384444683      593  \n 1  384444684      594 \n 2  384444686      596  \n', 'df=\n    index1       gi    ptt_loc\n 0  0     384444683      593  \n 1  1     384444684      594 \n 2  2     384444686      596  \n'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'R' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'R' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'tag' is not defined"", ""name 'R' is not defined""]";['NameError', 'NameError']
606;606;606;606;4.0;0;20480897;;1;13;<python><pandas>;Pandas add one day to column;11860.0;"[""montdist['date'] + pd.DateOffset(1)\nTypeError: cannot use a non-absolute DateOffset in datetime/timedelta operations [<DateOffset>]\n    Units   mondist                date\n1    6491  0.057785 2013-12-31 00:00:00\n2    7377  0.065672 2014-01-31 00:00:00\n3    9990  0.088934 2014-02-28 00:00:00\n4   10362  0.092245 2014-03-31 00:00:00\n5   11271  0.100337 2014-04-30 00:00:00\n6   11637  0.103596 2014-05-31 00:00:00\n7   10199  0.090794 2014-06-30 00:00:00\n8   10486  0.093349 2014-07-31 00:00:00\n9    9282  0.082631 2014-08-31 00:00:00\n10   8632  0.076844 2014-09-30 00:00:00\n11   8204  0.073034 2013-10-31 00:00:00\n12   8400  0.074779 2013-11-30 00:00:00\n""]";"[""montdist['date'] + pd.DateOffset(1)\n"", 'TypeError: cannot use a non-absolute DateOffset in datetime/timedelta operations [<DateOffset>]\n', '    Units   mondist                date\n1    6491  0.057785 2013-12-31 00:00:00\n2    7377  0.065672 2014-01-31 00:00:00\n3    9990  0.088934 2014-02-28 00:00:00\n4   10362  0.092245 2014-03-31 00:00:00\n5   11271  0.100337 2014-04-30 00:00:00\n6   11637  0.103596 2014-05-31 00:00:00\n7   10199  0.090794 2014-06-30 00:00:00\n8   10486  0.093349 2014-07-31 00:00:00\n9    9282  0.082631 2014-08-31 00:00:00\n10   8632  0.076844 2014-09-30 00:00:00\n11   8204  0.073034 2013-10-31 00:00:00\n12   8400  0.074779 2013-11-30 00:00:00\n']";"[""montdist['date'] + pd.DateOffset(1)\n"", 'TypeError: cannot use a non-absolute DateOffset in datetime/timedelta operations [<DateOffset>]\n', '    Units   mondist                date\n1    6491  0.057785 2013-12-31 00:00:00\n2    7377  0.065672 2014-01-31 00:00:00\n3    9990  0.088934 2014-02-28 00:00:00\n4   10362  0.092245 2014-03-31 00:00:00\n5   11271  0.100337 2014-04-30 00:00:00\n6   11637  0.103596 2014-05-31 00:00:00\n7   10199  0.090794 2014-06-30 00:00:00\n8   10486  0.093349 2014-07-31 00:00:00\n9    9282  0.082631 2014-08-31 00:00:00\n10   8632  0.076844 2014-09-30 00:00:00\n11   8204  0.073034 2013-10-31 00:00:00\n12   8400  0.074779 2013-11-30 00:00:00\n']";"[""montdist['date'] + pd.DateOffset(1)\n""]";"[""import pandas as pd\nmontdist['date'] + pd.DateOffset(1)\n""]";True;"[""import pandas as pd\nmontdist['date'] + pd.DateOffset(1)\n""]";False;0;2;"[""name 'pd' is not defined"", ""name 'mondist' is not defined""]";['NameError', 'NameError'];0;2;"[""'DataFrame' object has no attribute 'date'"", ""name 'mondist' is not defined""]";['AttributeError', 'NameError'];0;2;"[""'DataFrame' object has no attribute 'date'"", ""name 'mondist' is not defined""]";['AttributeError', 'NameError']
607;607;607;607;2.0;0;20490274;;1;113;<python><indexing><pandas><dataframe>;How to reset index in a pandas data frame?;101981.0;"[""df = df.reset_index()\ndel df['index']\ndf = df.reindex()\n""]";"[""df = df.reset_index()\ndel df['index']\n"", 'df = df.reindex()\n']";"['[1,5,6,10,11]', '[0,1,2,3,4]', ""df = df.reset_index()\ndel df['index']\n"", 'df = df.reindex()\n']";"[""df = df.reset_index()\ndel df['index']\ndf = df.reindex()\n""]";"[""df = df.reset_index()\ndel df['index']\ndf = df.reindex()\n""]";False;"[""import pandas as pd\ndf = df.reset_index()\ndel df['index']\ndf = df.reindex()\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError']
608;608;608;608;1.0;3;20520246;;1;11;<python><datetime><numpy><matplotlib><pandas>;Create heatmap using pandas TimeSeries;5264.0;"[""x = pd.date_range(df_all.ts.min(),df_all.ts.max(),freq='H')\nxt = mdates.drange(df_all.ts.min(), df_all.ts.max(), dt.timedelta(hours=1))\ny = arange(ylen)\nX,Y = np.meshgrid(xt, y)\n""]";"[""x = pd.date_range(df_all.ts.min(),df_all.ts.max(),freq='H')\nxt = mdates.drange(df_all.ts.min(), df_all.ts.max(), dt.timedelta(hours=1))\ny = arange(ylen)\nX,Y = np.meshgrid(xt, y)\n""]";"[""x = pd.date_range(df_all.ts.min(),df_all.ts.max(),freq='H')\nxt = mdates.drange(df_all.ts.min(), df_all.ts.max(), dt.timedelta(hours=1))\ny = arange(ylen)\nX,Y = np.meshgrid(xt, y)\n""]";"[""x = pd.date_range(df_all.ts.min(),df_all.ts.max(),freq='H')\nxt = mdates.drange(df_all.ts.min(), df_all.ts.max(), dt.timedelta(hours=1))\ny = arange(ylen)\nX,Y = np.meshgrid(xt, y)\n""]";"[""import pandas as pd\nx = pd.date_range(df_all.ts.min(),df_all.ts.max(),freq='H')\nxt = mdates.drange(df_all.ts.min(), df_all.ts.max(), dt.timedelta(hours=1))\ny = arange(ylen)\nX,Y = np.meshgrid(xt, y)\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\nx = pd.date_range(df_all.ts.min(),df_all.ts.max(),freq='H')\nxt = mdates.drange(df_all.ts.min(), df_all.ts.max(), dt.timedelta(hours=1))\ny = arange(ylen)\nX,Y = np.meshgrid(xt, y)\n""]";True;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
609;609;609;609;1.0;5;20555761;;1;17;<python><ubuntu><pip><python-2.x><digital-ocean>;`pip install pandas` gives UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 41: ordinal not in range(128);8619.0;[''];[];"['pip install pandas', ""UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 41: ordinal not in range(128)\n""]";[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
610;610;610;610;3.0;0;20574257;;1;24;<python><pandas><statistics>;Constructing a co-occurrence matrix in python pandas;8487.0;"[""import pandas as pd\n\ndf = pd.DataFrame({'TFD' : ['AA', 'SL', 'BB', 'D0', 'Dk', 'FF'],\n                    'Snack' : ['1', '0', '1', '1', '0', '0'],\n                    'Trans' : ['1', '1', '1', '0', '0', '1'],\n                    'Dop' : ['1', '0', '1', '0', '1', '1']}).set_index('TFD')\n\nprint df\n\n>>> \n    Dop Snack Trans\nTFD                \nAA    1     1     1\nSL    0     0     1\nBB    1     1     1\nD0    0     1     0\nDk    1     0     0\nFF    1     0     1\n\n[6 rows x 3 columns]\n    Dop Snack Trans\n\nDop   0     2     3\nSnack 2     0     2\nTrans 3     2     0\n""]";"[""import pandas as pd\n\ndf = pd.DataFrame({'TFD' : ['AA', 'SL', 'BB', 'D0', 'Dk', 'FF'],\n                    'Snack' : ['1', '0', '1', '1', '0', '0'],\n                    'Trans' : ['1', '1', '1', '0', '0', '1'],\n                    'Dop' : ['1', '0', '1', '0', '1', '1']}).set_index('TFD')\n\nprint df\n\n>>> \n    Dop Snack Trans\nTFD                \nAA    1     1     1\nSL    0     0     1\nBB    1     1     1\nD0    0     1     0\nDk    1     0     0\nFF    1     0     1\n\n[6 rows x 3 columns]\n"", '    Dop Snack Trans\n\nDop   0     2     3\nSnack 2     0     2\nTrans 3     2     0\n']";"[""import pandas as pd\n\ndf = pd.DataFrame({'TFD' : ['AA', 'SL', 'BB', 'D0', 'Dk', 'FF'],\n                    'Snack' : ['1', '0', '1', '1', '0', '0'],\n                    'Trans' : ['1', '1', '1', '0', '0', '1'],\n                    'Dop' : ['1', '0', '1', '0', '1', '1']}).set_index('TFD')\n\nprint df\n\n>>> \n    Dop Snack Trans\nTFD                \nAA    1     1     1\nSL    0     0     1\nBB    1     1     1\nD0    0     1     0\nDk    1     0     0\nFF    1     0     1\n\n[6 rows x 3 columns]\n"", '    Dop Snack Trans\n\nDop   0     2     3\nSnack 2     0     2\nTrans 3     2     0\n']";['import pandas as pd\n\n\n\nTFD                \n\n\n'];['import pandas as pd\n\n\n\nTFD                \n\n\n'];False;['import pandas as pd\nimport pandas as pd\n\n\n\nTFD                \n\n\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
611;611;611;611;3.0;4;20602947;;1;11;<python><pandas>;Append column to pandas dataframe;34026.0;['index dat1\n0     9\n1     5\nindex dat2\n0     7\n1     6\nindex dat1  dat2\n0     9     7\n1     5     6\n'];['index dat1\n0     9\n1     5\n', 'index dat2\n0     7\n1     6\n', 'index dat1  dat2\n0     9     7\n1     5     6\n'];['index dat1\n0     9\n1     5\n', 'index dat2\n0     7\n1     6\n', 'index dat1  dat2\n0     9     7\n1     5     6\n', 'append'];[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'pd' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'dat1' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'dat1' is not defined""]";['Sucess', 'NameError']
612;612;612;612;3.0;0;20612645;;1;103;<python><pandas>;How to find the installed pandas version;58878.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
613;613;613;613;2.0;4;20618523;;1;11;<python><pandas><hdf5><pytables>;Reading a large table with millions of rows from Oracle and writing to HDF5;2339.0;[''];[];"['pandas.HDFStore', 'pandas.HDFStore', 'cursor.fecthmany()', ""DataFrame(cursor.fetchmany(), columns = ['a','b','c'], dtype=my_dtype)"", 'HDFDatastore']";[''];[''];False;['import pandas as pd\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
614;614;614;614;3.0;7;20619851;;1;14;<python><pandas><stata>;pandas equivalent of Stata's encode;869.0;"[""x = pd.DataFrame({'cat':['A','A','B'], 'val':[10,20,30]})\nx = x.set_index('cat')\n     val\ncat     \nA     10\nA     20\nB     30\n     val\ncat     \n1     10\n1     20\n2     30\n  cat  val\n0   1   10\n1   1   20\n2   2   30\n""]";"[""x = pd.DataFrame({'cat':['A','A','B'], 'val':[10,20,30]})\nx = x.set_index('cat')\n"", '     val\ncat     \nA     10\nA     20\nB     30\n', '     val\ncat     \n1     10\n1     20\n2     30\n', '  cat  val\n0   1   10\n1   1   20\n2   2   30\n']";"[""x = pd.DataFrame({'cat':['A','A','B'], 'val':[10,20,30]})\nx = x.set_index('cat')\n"", '     val\ncat     \nA     10\nA     20\nB     30\n', '     val\ncat     \n1     10\n1     20\n2     30\n', '  cat  val\n0   1   10\n1   1   20\n2   2   30\n']";"[""x = pd.DataFrame({'cat':['A','A','B'], 'val':[10,20,30]})\nx = x.set_index('cat')\ncat     \ncat     \n""]";"[""import pandas as pd\nx = pd.DataFrame({'cat':['A','A','B'], 'val':[10,20,30]})\nx = x.set_index('cat')\ncat     \ncat     \n""]";True;"[""import pandas as pd\nx = pd.DataFrame({'cat':['A','A','B'], 'val':[10,20,30]})\nx = x.set_index('cat')\ncat     \ncat     \n""]";False;0;2;"[""name 'cat' is not defined"", ""name 'x' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'cat' is not defined"", ""name 'x' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'cat' is not defined"", ""name 'x' is not defined""]";['NameError', 'NameError']
615;615;615;615;5.0;1;20625582;;1;170;<python><parsing><pandas><dataframe>;How to deal with SettingWithCopyWarning in Pandas?;145172.0;"['E:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df[\'TVol\']   = quote_df[\'TVol\']/TVOL_SCALE\ndef _decode_stock_quote(list_of_150_stk_str):\n    """"""decode the webpage and return dataframe""""""\n\n    from cStringIO import StringIO\n\n    str_of_all = """".join(list_of_150_stk_str)\n\n    quote_df = pd.read_csv(StringIO(str_of_all), sep=\',\', names=list(\'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg\')) #dtype={\'A\': object, \'B\': object, \'C\': np.float64}\n    quote_df.rename(columns={\'A\':\'STK\', \'B\':\'TOpen\', \'C\':\'TPCLOSE\', \'D\':\'TPrice\', \'E\':\'THigh\', \'F\':\'TLow\', \'I\':\'TVol\', \'J\':\'TAmt\', \'e\':\'TDate\', \'f\':\'TTime\'}, inplace=True)\n    quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\n    quote_df[\'TClose\'] = quote_df[\'TPrice\']\n    quote_df[\'RT\']     = 100 * (quote_df[\'TPrice\']/quote_df[\'TPCLOSE\'] - 1)\n    quote_df[\'TVol\']   = quote_df[\'TVol\']/TVOL_SCALE\n    quote_df[\'TAmt\']   = quote_df[\'TAmt\']/TAMT_SCALE\n    quote_df[\'STK_ID\'] = quote_df[\'STK\'].str.slice(13,19)\n    quote_df[\'STK_Name\'] = quote_df[\'STK\'].str.slice(21,30)#.decode(\'gb2312\')\n    quote_df[\'TDate\']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])\n\n    return quote_df\nE:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df[\'TVol\']   = quote_df[\'TVol\']/TVOL_SCALE\nE:\\FinReporter\\FM_EXT.py:450: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df[\'TAmt\']   = quote_df[\'TAmt\']/TAMT_SCALE\nE:\\FinReporter\\FM_EXT.py:453: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df[\'TDate\']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])\n']";"[""E:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE\n"", 'def _decode_stock_quote(list_of_150_stk_str):\n    """"""decode the webpage and return dataframe""""""\n\n    from cStringIO import StringIO\n\n    str_of_all = """".join(list_of_150_stk_str)\n\n    quote_df = pd.read_csv(StringIO(str_of_all), sep=\',\', names=list(\'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg\')) #dtype={\'A\': object, \'B\': object, \'C\': np.float64}\n    quote_df.rename(columns={\'A\':\'STK\', \'B\':\'TOpen\', \'C\':\'TPCLOSE\', \'D\':\'TPrice\', \'E\':\'THigh\', \'F\':\'TLow\', \'I\':\'TVol\', \'J\':\'TAmt\', \'e\':\'TDate\', \'f\':\'TTime\'}, inplace=True)\n    quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\n    quote_df[\'TClose\'] = quote_df[\'TPrice\']\n    quote_df[\'RT\']     = 100 * (quote_df[\'TPrice\']/quote_df[\'TPCLOSE\'] - 1)\n    quote_df[\'TVol\']   = quote_df[\'TVol\']/TVOL_SCALE\n    quote_df[\'TAmt\']   = quote_df[\'TAmt\']/TAMT_SCALE\n    quote_df[\'STK_ID\'] = quote_df[\'STK\'].str.slice(13,19)\n    quote_df[\'STK_Name\'] = quote_df[\'STK\'].str.slice(21,30)#.decode(\'gb2312\')\n    quote_df[\'TDate\']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])\n\n    return quote_df\n', ""E:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE\nE:\\FinReporter\\FM_EXT.py:450: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TAmt']   = quote_df['TAmt']/TAMT_SCALE\nE:\\FinReporter\\FM_EXT.py:453: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TDate']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])\n""]";"[""E:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE\n"", ""quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE"", 'def _decode_stock_quote(list_of_150_stk_str):\n    """"""decode the webpage and return dataframe""""""\n\n    from cStringIO import StringIO\n\n    str_of_all = """".join(list_of_150_stk_str)\n\n    quote_df = pd.read_csv(StringIO(str_of_all), sep=\',\', names=list(\'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg\')) #dtype={\'A\': object, \'B\': object, \'C\': np.float64}\n    quote_df.rename(columns={\'A\':\'STK\', \'B\':\'TOpen\', \'C\':\'TPCLOSE\', \'D\':\'TPrice\', \'E\':\'THigh\', \'F\':\'TLow\', \'I\':\'TVol\', \'J\':\'TAmt\', \'e\':\'TDate\', \'f\':\'TTime\'}, inplace=True)\n    quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\n    quote_df[\'TClose\'] = quote_df[\'TPrice\']\n    quote_df[\'RT\']     = 100 * (quote_df[\'TPrice\']/quote_df[\'TPCLOSE\'] - 1)\n    quote_df[\'TVol\']   = quote_df[\'TVol\']/TVOL_SCALE\n    quote_df[\'TAmt\']   = quote_df[\'TAmt\']/TAMT_SCALE\n    quote_df[\'STK_ID\'] = quote_df[\'STK\'].str.slice(13,19)\n    quote_df[\'STK_Name\'] = quote_df[\'STK\'].str.slice(21,30)#.decode(\'gb2312\')\n    quote_df[\'TDate\']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])\n\n    return quote_df\n', ""E:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE\nE:\\FinReporter\\FM_EXT.py:450: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TAmt']   = quote_df['TAmt']/TAMT_SCALE\nE:\\FinReporter\\FM_EXT.py:453: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TDate']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])\n""]";['\n\n\n\n'];['\n\n\n\n'];False;['import pandas as pd\n\n\n\n\n'];False;0;3;"[""name 'new_val' is not defined"", ""name 'DataFrame' is not defined"", ""name 'quote_df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'new_val' is not defined"", ""name 'np' is not defined"", ""name 'quote_df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'new_val' is not defined"", ""name 'DataFrame' is not defined"", ""name 'quote_df' is not defined""]";['NameError', 'NameError', 'NameError']
616;616;616;616;1.0;7;20625982;;1;12;<python><pandas>;split-apply-combine on pandas timedelta column;1173.0;"['import pandas as pd\n\nimport numpy as np\n\npd.__version__\nOut[3]: \'0.13.0rc1\'\n\nnp.__version__\nOut[4]: \'1.8.0\'\n\ndata = pd.DataFrame(np.random.rand(10, 3), columns=[\'f1\', \'f2\', \'td\'])\n\ndata[\'td\'] *= 10000000\n\ndata[\'td\'] = pd.Series(data[\'td\'], dtype=\'<m8[ns]\')\n\ndata\nOut[8]: \n         f1        f2              td\n0  0.990140  0.948313 00:00:00.003066\n1  0.277125  0.993549 00:00:00.001443\n2  0.016427  0.581129 00:00:00.009257\n3  0.048662  0.512215 00:00:00.000702\n4  0.846301  0.179160 00:00:00.000396\n5  0.568323  0.419887 00:00:00.000266\n6  0.328182  0.919897 00:00:00.006138\n7  0.292882  0.213219 00:00:00.008876\n8  0.623332  0.003409 00:00:00.000322\n9  0.650436  0.844180 00:00:00.006873\n\n[10 rows x 3 columns]\n\ndata.groupby(data.index < 5).mean()\nOut[9]: \n             f1        f2\nFalse  0.492631  0.480118\nTrue   0.435731  0.642873\n\n[2 rows x 2 columns]\ndata.groupby(data.index < 5)[\'td\'].mean()\n---------------------------------------------------------------------------\nDataError                                 Traceback (most recent call last)\n<ipython-input-12-88cc94e534b7> in <module>()\n----> 1 data.groupby(data.index < 5)[\'td\'].mean()\n\n/path/to/lib/python3.3/site-packages/pandas-0.13.0rc1-py3.3-linux-x86_64.egg/pandas/core/groupby.py in mean(self)\n    417         """"""\n    418         try:\n--> 419             return self._cython_agg_general(\'mean\')\n    420         except GroupByError:\n    421             raise\n\n/path/to/lib/python3.3/site-packages/pandas-0.13.0rc1-py3.3-linux-x86_64.egg/pandas/core/groupby.py in _cython_agg_general(self, how, numeric_only)\n    669 \n    670         if len(output) == 0:\n--> 671             raise DataError(\'No numeric types to aggregate\')\n    672 \n    673         return self._wrap_aggregated_output(output, names)\n\nDataError: No numeric types to aggregate\ndata[\'td\'].mean()\nOut[11]: \n0   00:00:00.003734\ndtype: timedelta64[ns]\n']";"[""import pandas as pd\n\nimport numpy as np\n\npd.__version__\nOut[3]: '0.13.0rc1'\n\nnp.__version__\nOut[4]: '1.8.0'\n\ndata = pd.DataFrame(np.random.rand(10, 3), columns=['f1', 'f2', 'td'])\n\ndata['td'] *= 10000000\n\ndata['td'] = pd.Series(data['td'], dtype='<m8[ns]')\n\ndata\nOut[8]: \n         f1        f2              td\n0  0.990140  0.948313 00:00:00.003066\n1  0.277125  0.993549 00:00:00.001443\n2  0.016427  0.581129 00:00:00.009257\n3  0.048662  0.512215 00:00:00.000702\n4  0.846301  0.179160 00:00:00.000396\n5  0.568323  0.419887 00:00:00.000266\n6  0.328182  0.919897 00:00:00.006138\n7  0.292882  0.213219 00:00:00.008876\n8  0.623332  0.003409 00:00:00.000322\n9  0.650436  0.844180 00:00:00.006873\n\n[10 rows x 3 columns]\n\ndata.groupby(data.index < 5).mean()\nOut[9]: \n             f1        f2\nFalse  0.492631  0.480118\nTrue   0.435731  0.642873\n\n[2 rows x 2 columns]\n"", 'data.groupby(data.index < 5)[\'td\'].mean()\n---------------------------------------------------------------------------\nDataError                                 Traceback (most recent call last)\n<ipython-input-12-88cc94e534b7> in <module>()\n----> 1 data.groupby(data.index < 5)[\'td\'].mean()\n\n/path/to/lib/python3.3/site-packages/pandas-0.13.0rc1-py3.3-linux-x86_64.egg/pandas/core/groupby.py in mean(self)\n    417         """"""\n    418         try:\n--> 419             return self._cython_agg_general(\'mean\')\n    420         except GroupByError:\n    421             raise\n\n/path/to/lib/python3.3/site-packages/pandas-0.13.0rc1-py3.3-linux-x86_64.egg/pandas/core/groupby.py in _cython_agg_general(self, how, numeric_only)\n    669 \n    670         if len(output) == 0:\n--> 671             raise DataError(\'No numeric types to aggregate\')\n    672 \n    673         return self._wrap_aggregated_output(output, names)\n\nDataError: No numeric types to aggregate\n', ""data['td'].mean()\nOut[11]: \n0   00:00:00.003734\ndtype: timedelta64[ns]\n""]";"['timedelta64[ns]', '<m8[ns]', ""import pandas as pd\n\nimport numpy as np\n\npd.__version__\nOut[3]: '0.13.0rc1'\n\nnp.__version__\nOut[4]: '1.8.0'\n\ndata = pd.DataFrame(np.random.rand(10, 3), columns=['f1', 'f2', 'td'])\n\ndata['td'] *= 10000000\n\ndata['td'] = pd.Series(data['td'], dtype='<m8[ns]')\n\ndata\nOut[8]: \n         f1        f2              td\n0  0.990140  0.948313 00:00:00.003066\n1  0.277125  0.993549 00:00:00.001443\n2  0.016427  0.581129 00:00:00.009257\n3  0.048662  0.512215 00:00:00.000702\n4  0.846301  0.179160 00:00:00.000396\n5  0.568323  0.419887 00:00:00.000266\n6  0.328182  0.919897 00:00:00.006138\n7  0.292882  0.213219 00:00:00.008876\n8  0.623332  0.003409 00:00:00.000322\n9  0.650436  0.844180 00:00:00.006873\n\n[10 rows x 3 columns]\n\ndata.groupby(data.index < 5).mean()\nOut[9]: \n             f1        f2\nFalse  0.492631  0.480118\nTrue   0.435731  0.642873\n\n[2 rows x 2 columns]\n"", ""'td'"", 'data.groupby(data.index < 5)[\'td\'].mean()\n---------------------------------------------------------------------------\nDataError                                 Traceback (most recent call last)\n<ipython-input-12-88cc94e534b7> in <module>()\n----> 1 data.groupby(data.index < 5)[\'td\'].mean()\n\n/path/to/lib/python3.3/site-packages/pandas-0.13.0rc1-py3.3-linux-x86_64.egg/pandas/core/groupby.py in mean(self)\n    417         """"""\n    418         try:\n--> 419             return self._cython_agg_general(\'mean\')\n    420         except GroupByError:\n    421             raise\n\n/path/to/lib/python3.3/site-packages/pandas-0.13.0rc1-py3.3-linux-x86_64.egg/pandas/core/groupby.py in _cython_agg_general(self, how, numeric_only)\n    669 \n    670         if len(output) == 0:\n--> 671             raise DataError(\'No numeric types to aggregate\')\n    672 \n    673         return self._wrap_aggregated_output(output, names)\n\nDataError: No numeric types to aggregate\n', ""data['td'].mean()\nOut[11]: \n0   00:00:00.003734\ndtype: timedelta64[ns]\n""]";"[""import pandas as pd\n\nimport numpy as np\n\npd.__version__\n\nnp.__version__\n\ndata = pd.DataFrame(np.random.rand(10, 3), columns=['f1', 'f2', 'td'])\n\ndata['td'] *= 10000000\n\ndata['td'] = pd.Series(data['td'], dtype='<m8[ns]')\n\ndata\n\n\ndata.groupby(data.index < 5).mean()\n\ndata.groupby(data.index < 5)['td'].mean()\n\n\n\ndata['td'].mean()\n""]";"[""import pandas as pd\n\nimport numpy as np\n\npd.__version__\n\nnp.__version__\n\ndata = pd.DataFrame(np.random.rand(10, 3), columns=['f1', 'f2', 'td'])\n\ndata['td'] *= 10000000\n\ndata['td'] = pd.Series(data['td'], dtype='<m8[ns]')\n\ndata\n\n\ndata.groupby(data.index < 5).mean()\n\ndata.groupby(data.index < 5)['td'].mean()\n\n\n\ndata['td'].mean()\n""]";False;"[""import pandas as pd\nimport pandas as pd\n\nimport numpy as np\n\npd.__version__\n\nnp.__version__\n\ndata = pd.DataFrame(np.random.rand(10, 3), columns=['f1', 'f2', 'td'])\n\ndata['td'] *= 10000000\n\ndata['td'] = pd.Series(data['td'], dtype='<m8[ns]')\n\ndata\n\n\ndata.groupby(data.index < 5).mean()\n\ndata.groupby(data.index < 5)['td'].mean()\n\n\n\ndata['td'].mean()\n""]";False;0;0;[];[];0;0;[];[];0;0;[];[]
617;617;617;617;3.0;1;20630121;;1;17;<python><r><pandas>;Pandas - how to convert r dataframe back to pandas?;4507.0;['import pandas as pd\nimport pandas.rpy.common as com\nimport rpy2.robjects as ro\nfrom rpy2.robjects.packages import importr\nrdf = com.convert_to_r_dataframe(df)\ndf = f(rdf) ?\n'];['import pandas as pd\nimport pandas.rpy.common as com\nimport rpy2.robjects as ro\nfrom rpy2.robjects.packages import importr\nrdf = com.convert_to_r_dataframe(df)\n', 'df = f(rdf) ?\n'];['import pandas as pd\nimport pandas.rpy.common as com\nimport rpy2.robjects as ro\nfrom rpy2.robjects.packages import importr\nrdf = com.convert_to_r_dataframe(df)\n', 'df = f(rdf) ?\n'];['import pandas as pd\nimport pandas.rpy.common as com\nimport rpy2.robjects as ro\nfrom rpy2.robjects.packages import importr\nrdf = com.convert_to_r_dataframe(df)\n'];['import pandas as pd\nimport pandas.rpy.common as com\nimport rpy2.robjects as ro\nfrom rpy2.robjects.packages import importr\nrdf = com.convert_to_r_dataframe(df)\n'];False;['import pandas as pd\ndata = pd.DataFrame()\nimport pandas as pd\nimport pandas.rpy.common as com\nimport rpy2.robjects as ro\nfrom rpy2.robjects.packages import importr\nrdf = com.convert_to_r_dataframe(df)\n'];True;0;3;"[""name 'dfrm' is not defined"", ""No module named 'rpy2'"", ""No module named 'rpy2'""]";['NameError', 'ImportError', 'ImportError'];0;3;"[""name 'dfrm' is not defined"", ""No module named 'rpy2'"", ""No module named 'rpy2'""]";['NameError', 'ImportError', 'ImportError'];0;3;"[""name 'dfrm' is not defined"", ""No module named 'rpy2'"", ""No module named 'rpy2'""]";['NameError', 'ImportError', 'ImportError']
618;618;618;618;3.0;2;20637439;;1;29;<python><csv><pandas>;Skip rows during csv import pandas;35315.0;[''];[];['pandas.read_csv()', 'skiprows=1'];[''];[''];False;['import pandas as pd\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
619;619;619;619;3.0;0;20638006;;1;221;<python><dictionary><pandas><dataframe>;Convert list of dictionaries to Dataframe;57095.0;"['[{\'points\': 50, \'time\': \'5:00\', \'year\': 2010}, \n{\'points\': 25, \'time\': \'6:00\', \'month\': ""february""}, \n{\'points\':90, \'time\': \'9:00\', \'month\': \'january\'}, \n{\'points_h1\':20, \'month\': \'june\'}]\n      month  points  points_h1  time  year\n0       NaN      50        NaN  5:00  2010\n1  february      25        NaN  6:00   NaN\n2   january      90        NaN  9:00   NaN\n3      june     NaN         20   NaN   NaN\n']";"['[{\'points\': 50, \'time\': \'5:00\', \'year\': 2010}, \n{\'points\': 25, \'time\': \'6:00\', \'month\': ""february""}, \n{\'points\':90, \'time\': \'9:00\', \'month\': \'january\'}, \n{\'points_h1\':20, \'month\': \'june\'}]\n', '      month  points  points_h1  time  year\n0       NaN      50        NaN  5:00  2010\n1  february      25        NaN  6:00   NaN\n2   january      90        NaN  9:00   NaN\n3      june     NaN         20   NaN   NaN\n']";"['[{\'points\': 50, \'time\': \'5:00\', \'year\': 2010}, \n{\'points\': 25, \'time\': \'6:00\', \'month\': ""february""}, \n{\'points\':90, \'time\': \'9:00\', \'month\': \'january\'}, \n{\'points_h1\':20, \'month\': \'june\'}]\n', 'DataFrame', '      month  points  points_h1  time  year\n0       NaN      50        NaN  5:00  2010\n1  february      25        NaN  6:00   NaN\n2   january      90        NaN  9:00   NaN\n3      june     NaN         20   NaN   NaN\n']";"['{\'points\': 25, \'time\': \'6:00\', \'month\': ""february""}, \n{\'points\':90, \'time\': \'9:00\', \'month\': \'january\'}, \n']";"['{\'points\': 25, \'time\': \'6:00\', \'month\': ""february""}, \n{\'points\':90, \'time\': \'9:00\', \'month\': \'january\'}, \n']";False;"['import pandas as pd\n{\'points\': 25, \'time\': \'6:00\', \'month\': ""february""}, \n{\'points\':90, \'time\': \'9:00\', \'month\': \'january\'}, \n']";False;1;2;"[""name 'pd' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'd' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'd' is not defined"", 'Sucess']";['NameError', 'Sucess']
620;620;620;620;2.0;0;20644536;;1;13;<python><pandas>;Square of each element of a column in pandas;8956.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
621;621;621;621;4.0;0;20648346;;1;17;<python><pandas>;Computing diffs within groups of a dataframe;9122.0;"[""df['diffs'] = df['value'].diff()\ndf = df.sort(['ticker', 'date'])\ndf['diffs'] = df['value'].diff()\nresult = df.set_index(['ticker', 'date'])\\\n    .groupby(level='ticker')\\\n    .transform(lambda x: x.sort_index().diff())\\\n    .reset_index()\n""]";"[""df['diffs'] = df['value'].diff()\n"", ""df = df.sort(['ticker', 'date'])\ndf['diffs'] = df['value'].diff()\n"", ""result = df.set_index(['ticker', 'date'])\\\n    .groupby(level='ticker')\\\n    .transform(lambda x: x.sort_index().diff())\\\n    .reset_index()\n""]";"['(ticker, date)', ""df['diffs'] = df['value'].diff()\n"", ""df = df.sort(['ticker', 'date'])\ndf['diffs'] = df['value'].diff()\n"", 'NaN', 'groupby', 'ticker', 'date', 'value', 'diffs', ""result = df.set_index(['ticker', 'date'])\\\n    .groupby(level='ticker')\\\n    .transform(lambda x: x.sort_index().diff())\\\n    .reset_index()\n"", 'ticker', 'date', ""result['current']"", 'df']";"[""df['diffs'] = df['value'].diff()\ndf = df.sort(['ticker', 'date'])\ndf['diffs'] = df['value'].diff()\nresult = df.set_index(['ticker', 'date'])\\\n""]";"[""df['diffs'] = df['value'].diff()\ndf = df.sort(['ticker', 'date'])\ndf['diffs'] = df['value'].diff()\nresult = df.set_index(['ticker', 'date'])\\\n""]";False;"[""import pandas as pd\ndf['diffs'] = df['value'].diff()\ndf = df.sort(['ticker', 'date'])\ndf['diffs'] = df['value'].diff()\nresult = df.set_index(['ticker', 'date'])\\\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'DataFrame' object has no attribute 'sort'""]";['AttributeError']
622;622;622;622;1.0;3;20656663;;1;36;<python><matplotlib><pandas><histogram>;Matplotlib/Pandas error using histogram;24955.0;"[""type(dfj2_MARKET1['VSPD2_perc'])\nfig, axes = plt.subplots(1, 7, figsize=(30,4))\naxes[0].hist(dfj2_MARKET1['VSPD1_perc'],alpha=0.9, color='blue')\naxes[0].grid(True)\naxes[0].set_title(MARKET1 + '  5-40 km / h')\n    AttributeError                            Traceback (most recent call last)\n    <ipython-input-75-3810c361db30> in <module>()\n      1 fig, axes = plt.subplots(1, 7, figsize=(30,4))\n      2 \n    ----> 3 axes[1].hist(dfj2_MARKET1['VSPD2_perc'],alpha=0.9, color='blue')\n      4 axes[1].grid(True)\n      5 axes[1].set_xlabel('Time spent [%]')\n\n    C:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in hist(self, x, bins, range, normed,          weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label,    stacked, **kwargs)\n   8322             # this will automatically overwrite bins,\n   8323             # so that each histogram uses the same bins\n-> 8324             m, bins = np.histogram(x[i], bins, weights=w[i], **hist_kwargs)\n   8325             m = m.astype(float) # causes problems later if it's an int\n   8326             if mlast is None:\n\n    C:\\Python27\\lib\\site-packages\\numpy\\lib\\function_base.pyc in histogram(a, bins, range,     normed, weights, density)\n    158         if (mn > mx):\n    159             raise AttributeError(\n--> 160                 'max must be larger than min in range parameter.')\n    161 \n    162     if not iterable(bins):\n\nAttributeError: max must be larger than min in range parameter.\n""]";"[""type(dfj2_MARKET1['VSPD2_perc'])\n"", ""fig, axes = plt.subplots(1, 7, figsize=(30,4))\naxes[0].hist(dfj2_MARKET1['VSPD1_perc'],alpha=0.9, color='blue')\naxes[0].grid(True)\naxes[0].set_title(MARKET1 + '  5-40 km / h')\n"", ""    AttributeError                            Traceback (most recent call last)\n    <ipython-input-75-3810c361db30> in <module>()\n      1 fig, axes = plt.subplots(1, 7, figsize=(30,4))\n      2 \n    ----> 3 axes[1].hist(dfj2_MARKET1['VSPD2_perc'],alpha=0.9, color='blue')\n      4 axes[1].grid(True)\n      5 axes[1].set_xlabel('Time spent [%]')\n\n    C:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in hist(self, x, bins, range, normed,          weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label,    stacked, **kwargs)\n   8322             # this will automatically overwrite bins,\n   8323             # so that each histogram uses the same bins\n-> 8324             m, bins = np.histogram(x[i], bins, weights=w[i], **hist_kwargs)\n   8325             m = m.astype(float) # causes problems later if it's an int\n   8326             if mlast is None:\n\n    C:\\Python27\\lib\\site-packages\\numpy\\lib\\function_base.pyc in histogram(a, bins, range,     normed, weights, density)\n    158         if (mn > mx):\n    159             raise AttributeError(\n--> 160                 'max must be larger than min in range parameter.')\n    161 \n    162     if not iterable(bins):\n\nAttributeError: max must be larger than min in range parameter.\n""]";"[""type(dfj2_MARKET1['VSPD2_perc'])\n"", 'pandas.core.series.Series', ""fig, axes = plt.subplots(1, 7, figsize=(30,4))\naxes[0].hist(dfj2_MARKET1['VSPD1_perc'],alpha=0.9, color='blue')\naxes[0].grid(True)\naxes[0].set_title(MARKET1 + '  5-40 km / h')\n"", ""    AttributeError                            Traceback (most recent call last)\n    <ipython-input-75-3810c361db30> in <module>()\n      1 fig, axes = plt.subplots(1, 7, figsize=(30,4))\n      2 \n    ----> 3 axes[1].hist(dfj2_MARKET1['VSPD2_perc'],alpha=0.9, color='blue')\n      4 axes[1].grid(True)\n      5 axes[1].set_xlabel('Time spent [%]')\n\n    C:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in hist(self, x, bins, range, normed,          weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label,    stacked, **kwargs)\n   8322             # this will automatically overwrite bins,\n   8323             # so that each histogram uses the same bins\n-> 8324             m, bins = np.histogram(x[i], bins, weights=w[i], **hist_kwargs)\n   8325             m = m.astype(float) # causes problems later if it's an int\n   8326             if mlast is None:\n\n    C:\\Python27\\lib\\site-packages\\numpy\\lib\\function_base.pyc in histogram(a, bins, range,     normed, weights, density)\n    158         if (mn > mx):\n    159             raise AttributeError(\n--> 160                 'max must be larger than min in range parameter.')\n    161 \n    162     if not iterable(bins):\n\nAttributeError: max must be larger than min in range parameter.\n""]";"[""type(dfj2_MARKET1['VSPD2_perc'])\nfig, axes = plt.subplots(1, 7, figsize=(30,4))\naxes[0].hist(dfj2_MARKET1['VSPD1_perc'],alpha=0.9, color='blue')\naxes[0].grid(True)\naxes[0].set_title(MARKET1 + '  5-40 km / h')\n\n\n\n""]";"[""type(dfj2_MARKET1['VSPD2_perc'])\nfig, axes = plt.subplots(1, 7, figsize=(30,4))\naxes[0].hist(dfj2_MARKET1['VSPD1_perc'],alpha=0.9, color='blue')\naxes[0].grid(True)\naxes[0].set_title(MARKET1 + '  5-40 km / h')\n\n\n\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ntype(dfj2_MARKET1['VSPD2_perc'])\nfig, axes = plt.subplots(1, 7, figsize=(30,4))\naxes[0].hist(dfj2_MARKET1['VSPD1_perc'],alpha=0.9, color='blue')\naxes[0].grid(True)\naxes[0].set_title(MARKET1 + '  5-40 km / h')\n\n\n\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
623;623;623;623;2.0;1;20670370;;1;11;<python><unicode><pandas>;Pandas and unicode;18168.0;"['DFJ {""args"":{""0"":""[]"",""1"":""[]"",""2"":""[]"",""3"":""[]"",""4"":""[]"",""5"":""[]"",""6"":""[]"",""7"":""[]""},""date"":{""0"":1385944439000000000,""1"":1385944439000000000,""2"":1385944440000000000,""3"":1385944440000000000,""4"":1385944440000000000,""5"":1385944440000000000,""6"":1385944440000000000,""7"":1385944440000000000},""host"":{""0"":""yy38.segm1.org"",""1"":""kyy1.segm1.org"",""2"":""yy10.segm1.org"",""3"":""yy24.segm1.org"",""4"":""yy24.segm1.org"",""5"":""yy34.segm1.org"",""6"":""yy15.segm1.org"",""7"":""yy15.segm1.org""},""kwargs"":{""0"":""{}"",""1"":""{}"",""2"":""{}"",""3"":""{}"",""4"":""{}"",""5"":""{}"",""6"":""{}"",""7"":""{}""},""operation"":{""0"":""x_gbinf"",""1"":""x_initobj"",""2"":""x_gobjParams"",""3"":""gtfull"",""4"":""x_gbinf"",""5"":""gxyzinf"",""6"":""deletemfg"",""7"":""gxyzinf""},""thingy"":{""0"":""a13yy38"",""1"":""a19kyy1"",""2"":""a14yy10"",""3"":""a14yy24"",""4"":""a14yy24"",""5"":""a12yy34"",""6"":""a15yy15"",""7"":""a15yy15""},""status"":{""0"":-101,""1"":1,""2"":-101,""3"":-101,""4"":-101,""5"":-101,""6"":1,""7"":-101},""time"":{""0"":0.000801,""1"":0.003244,""2"":0.002247,""3"":0.002787,""4"":0.001067,""5"":0.002652,""6"":0.004371,""7"":0.000602}}\nTraceback (most recent call last):\n  File ""./sqlprofile.py"", line 160, in <module>\n    maybe_save_dataframes(rconn, configd, results)\n  File ""./sqlprofile.py"", line 140, in maybe_save_dataframes\n    h5store.append(out_queue, df)\n  File ""/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py"", line 658, in append\n    self._write_to_group(key, value, table=True, append=True, **kwargs)\n  File ""/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py"", line 923, in _write_to_group\n    s.write(obj = value, append=append, complib=complib, **kwargs)\n  File ""/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py"", line 2985, in write\n    **kwargs)\n  File ""/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py"", line 2717, in create_axes\n    raise e\nTypeError: [unicode] is not implemented as a table column\n> /home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py(2717)create_axes()\n-> raise e\n(Pdb) locals()\n{\'append_axis\': [u\'args\', u\'date\', u\'host\', u\'kwargs\', u\'operation\', u\'thingy\', u\'status\', u\'time\'], \'existing_table\': None, \'blocks\': [FloatBlock: [time], 1 x 8, dtype float64, ObjectBlock: [args, host, kwargs, operation, thingy], 5 x 8, dtype object, IntBlock: [status], 1 x 8, dtype int64, DatetimeBlock: [date], 1 x 8, dtype datetime64[ns]], \'axis\': 1, \'self\': frame_table  (typ->appendable,nrows->None,ncols->1,indexers->[index]), \'axes\': [0], \'kwargs\': {}, \'klass\': <class \'pandas.io.pytables.DataCol\'>, \'block_obj\':   args                date            host kwargs              operation      thingy  status      time\n0   [] 2013-12-02 00:33:59  yy38.segm1.org     {}       x_gbinf  a13yy38    -101  0.000801\n1   [] 2013-12-02 00:33:59  kyy1.segm1.org     {}         x_initobj  a19kyy1       1  0.003244\n2   [] 2013-12-02 00:34:00  yy10.segm1.org     {}    x_gobjParams  a14yy10    -101  0.002247\n3   [] 2013-12-02 00:34:00  yy24.segm1.org     {}        gtfull  a14yy24    -101  0.002787\n4   [] 2013-12-02 00:34:00  yy24.segm1.org     {}       x_gbinf  a14yy24    -101  0.001067\n5   [] 2013-12-02 00:34:00  yy34.segm1.org     {}           gxyzinf  a12yy34    -101  0.002652\n6   [] 2013-12-02 00:34:00  yy15.segm1.org     {}  deletemfg  a15yy15       1  0.004371\n7   [] 2013-12-02 00:34:00  yy15.segm1.org     {}           gxyzinf  a15yy15    -101  0.000602, \'axis_labels\': [u\'args\', u\'date\', u\'host\', u\'kwargs\', u\'operation\', u\'thingy\', u\'status\', u\'time\'], \'nan_rep\': \'nan\', \'data_columns\': [], \'obj\':   args                date            host kwargs              operation      thingy  status      time\n0   [] 2013-12-02 00:33:59  yy38.segm1.org     {}       x_gbinf  a13yy38    -101  0.000801\n1   [] 2013-12-02 00:33:59  kyy1.segm1.org     {}         x_initobj  a19kyy1       1  0.003244\n2   [] 2013-12-02 00:34:00  yy10.segm1.org     {}    x_gobjParams  a14yy10    -101  0.002247\n3   [] 2013-12-02 00:34:00  yy24.segm1.org     {}        gtfull  a14yy24    -101  0.002787\n4   [] 2013-12-02 00:34:00  yy24.segm1.org     {}       x_gbinf  a14yy24    -101  0.001067\n5   [] 2013-12-02 00:34:00  yy34.segm1.org     {}           gxyzinf  a12yy34    -101  0.002652\n6   [] 2013-12-02 00:34:00  yy15.segm1.org     {}  deletemfg  a15yy15       1  0.004371\n7   [] 2013-12-02 00:34:00  yy15.segm1.org     {}           gxyzinf  a15yy15    -101  0.000602, \'validate\': True, \'a\': (1, [u\'args\', u\'date\', u\'host\', u\'kwargs\', u\'operation\', u\'thingy\', u\'status\', u\'time\']), \'index_axes_map\': {0: name->index,cname->index,axis->0,pos->0,kind->integer}, \'b\': ObjectBlock: [args, host, kwargs, operation, thingy], 5 x 8, dtype object, \'e\': TypeError(\'[unicode] is not implemented as a table column\',), \'name\': None, \'existing_col\': None, \'j\': 2, \'i\': 1, \'min_itemsize\': None, \'col\': name->values_block_1,cname->values_block_1,dtype->None,shape->None}\n']";"['DFJ {""args"":{""0"":""[]"",""1"":""[]"",""2"":""[]"",""3"":""[]"",""4"":""[]"",""5"":""[]"",""6"":""[]"",""7"":""[]""},""date"":{""0"":1385944439000000000,""1"":1385944439000000000,""2"":1385944440000000000,""3"":1385944440000000000,""4"":1385944440000000000,""5"":1385944440000000000,""6"":1385944440000000000,""7"":1385944440000000000},""host"":{""0"":""yy38.segm1.org"",""1"":""kyy1.segm1.org"",""2"":""yy10.segm1.org"",""3"":""yy24.segm1.org"",""4"":""yy24.segm1.org"",""5"":""yy34.segm1.org"",""6"":""yy15.segm1.org"",""7"":""yy15.segm1.org""},""kwargs"":{""0"":""{}"",""1"":""{}"",""2"":""{}"",""3"":""{}"",""4"":""{}"",""5"":""{}"",""6"":""{}"",""7"":""{}""},""operation"":{""0"":""x_gbinf"",""1"":""x_initobj"",""2"":""x_gobjParams"",""3"":""gtfull"",""4"":""x_gbinf"",""5"":""gxyzinf"",""6"":""deletemfg"",""7"":""gxyzinf""},""thingy"":{""0"":""a13yy38"",""1"":""a19kyy1"",""2"":""a14yy10"",""3"":""a14yy24"",""4"":""a14yy24"",""5"":""a12yy34"",""6"":""a15yy15"",""7"":""a15yy15""},""status"":{""0"":-101,""1"":1,""2"":-101,""3"":-101,""4"":-101,""5"":-101,""6"":1,""7"":-101},""time"":{""0"":0.000801,""1"":0.003244,""2"":0.002247,""3"":0.002787,""4"":0.001067,""5"":0.002652,""6"":0.004371,""7"":0.000602}}\n', 'Traceback (most recent call last):\n  File ""./sqlprofile.py"", line 160, in <module>\n    maybe_save_dataframes(rconn, configd, results)\n  File ""./sqlprofile.py"", line 140, in maybe_save_dataframes\n    h5store.append(out_queue, df)\n  File ""/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py"", line 658, in append\n    self._write_to_group(key, value, table=True, append=True, **kwargs)\n  File ""/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py"", line 923, in _write_to_group\n    s.write(obj = value, append=append, complib=complib, **kwargs)\n  File ""/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py"", line 2985, in write\n    **kwargs)\n  File ""/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py"", line 2717, in create_axes\n    raise e\nTypeError: [unicode] is not implemented as a table column\n> /home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py(2717)create_axes()\n-> raise e\n(Pdb) locals()\n', ""{'append_axis': [u'args', u'date', u'host', u'kwargs', u'operation', u'thingy', u'status', u'time'], 'existing_table': None, 'blocks': [FloatBlock: [time], 1 x 8, dtype float64, ObjectBlock: [args, host, kwargs, operation, thingy], 5 x 8, dtype object, IntBlock: [status], 1 x 8, dtype int64, DatetimeBlock: [date], 1 x 8, dtype datetime64[ns]], 'axis': 1, 'self': frame_table  (typ->appendable,nrows->None,ncols->1,indexers->[index]), 'axes': [0], 'kwargs': {}, 'klass': <class 'pandas.io.pytables.DataCol'>, 'block_obj':   args                date            host kwargs              operation      thingy  status      time\n0   [] 2013-12-02 00:33:59  yy38.segm1.org     {}       x_gbinf  a13yy38    -101  0.000801\n1   [] 2013-12-02 00:33:59  kyy1.segm1.org     {}         x_initobj  a19kyy1       1  0.003244\n2   [] 2013-12-02 00:34:00  yy10.segm1.org     {}    x_gobjParams  a14yy10    -101  0.002247\n3   [] 2013-12-02 00:34:00  yy24.segm1.org     {}        gtfull  a14yy24    -101  0.002787\n4   [] 2013-12-02 00:34:00  yy24.segm1.org     {}       x_gbinf  a14yy24    -101  0.001067\n5   [] 2013-12-02 00:34:00  yy34.segm1.org     {}           gxyzinf  a12yy34    -101  0.002652\n6   [] 2013-12-02 00:34:00  yy15.segm1.org     {}  deletemfg  a15yy15       1  0.004371\n7   [] 2013-12-02 00:34:00  yy15.segm1.org     {}           gxyzinf  a15yy15    -101  0.000602, 'axis_labels': [u'args', u'date', u'host', u'kwargs', u'operation', u'thingy', u'status', u'time'], 'nan_rep': 'nan', 'data_columns': [], 'obj':   args                date            host kwargs              operation      thingy  status      time\n0   [] 2013-12-02 00:33:59  yy38.segm1.org     {}       x_gbinf  a13yy38    -101  0.000801\n1   [] 2013-12-02 00:33:59  kyy1.segm1.org     {}         x_initobj  a19kyy1       1  0.003244\n2   [] 2013-12-02 00:34:00  yy10.segm1.org     {}    x_gobjParams  a14yy10    -101  0.002247\n3   [] 2013-12-02 00:34:00  yy24.segm1.org     {}        gtfull  a14yy24    -101  0.002787\n4   [] 2013-12-02 00:34:00  yy24.segm1.org     {}       x_gbinf  a14yy24    -101  0.001067\n5   [] 2013-12-02 00:34:00  yy34.segm1.org     {}           gxyzinf  a12yy34    -101  0.002652\n6   [] 2013-12-02 00:34:00  yy15.segm1.org     {}  deletemfg  a15yy15       1  0.004371\n7   [] 2013-12-02 00:34:00  yy15.segm1.org     {}           gxyzinf  a15yy15    -101  0.000602, 'validate': True, 'a': (1, [u'args', u'date', u'host', u'kwargs', u'operation', u'thingy', u'status', u'time']), 'index_axes_map': {0: name->index,cname->index,axis->0,pos->0,kind->integer}, 'b': ObjectBlock: [args, host, kwargs, operation, thingy], 5 x 8, dtype object, 'e': TypeError('[unicode] is not implemented as a table column',), 'name': None, 'existing_col': None, 'j': 2, 'i': 1, 'min_itemsize': None, 'col': name->values_block_1,cname->values_block_1,dtype->None,shape->None}\n""]";"['pandas.DataFrame.to_json()', 'pandas.read_json()', 'DFJ {""args"":{""0"":""[]"",""1"":""[]"",""2"":""[]"",""3"":""[]"",""4"":""[]"",""5"":""[]"",""6"":""[]"",""7"":""[]""},""date"":{""0"":1385944439000000000,""1"":1385944439000000000,""2"":1385944440000000000,""3"":1385944440000000000,""4"":1385944440000000000,""5"":1385944440000000000,""6"":1385944440000000000,""7"":1385944440000000000},""host"":{""0"":""yy38.segm1.org"",""1"":""kyy1.segm1.org"",""2"":""yy10.segm1.org"",""3"":""yy24.segm1.org"",""4"":""yy24.segm1.org"",""5"":""yy34.segm1.org"",""6"":""yy15.segm1.org"",""7"":""yy15.segm1.org""},""kwargs"":{""0"":""{}"",""1"":""{}"",""2"":""{}"",""3"":""{}"",""4"":""{}"",""5"":""{}"",""6"":""{}"",""7"":""{}""},""operation"":{""0"":""x_gbinf"",""1"":""x_initobj"",""2"":""x_gobjParams"",""3"":""gtfull"",""4"":""x_gbinf"",""5"":""gxyzinf"",""6"":""deletemfg"",""7"":""gxyzinf""},""thingy"":{""0"":""a13yy38"",""1"":""a19kyy1"",""2"":""a14yy10"",""3"":""a14yy24"",""4"":""a14yy24"",""5"":""a12yy34"",""6"":""a15yy15"",""7"":""a15yy15""},""status"":{""0"":-101,""1"":1,""2"":-101,""3"":-101,""4"":-101,""5"":-101,""6"":1,""7"":-101},""time"":{""0"":0.000801,""1"":0.003244,""2"":0.002247,""3"":0.002787,""4"":0.001067,""5"":0.002652,""6"":0.004371,""7"":0.000602}}\n', '.read_json()', 'Traceback (most recent call last):\n  File ""./sqlprofile.py"", line 160, in <module>\n    maybe_save_dataframes(rconn, configd, results)\n  File ""./sqlprofile.py"", line 140, in maybe_save_dataframes\n    h5store.append(out_queue, df)\n  File ""/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py"", line 658, in append\n    self._write_to_group(key, value, table=True, append=True, **kwargs)\n  File ""/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py"", line 923, in _write_to_group\n    s.write(obj = value, append=append, complib=complib, **kwargs)\n  File ""/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py"", line 2985, in write\n    **kwargs)\n  File ""/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py"", line 2717, in create_axes\n    raise e\nTypeError: [unicode] is not implemented as a table column\n> /home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py(2717)create_axes()\n-> raise e\n(Pdb) locals()\n', 'locals()', 'append_axis', ""{'append_axis': [u'args', u'date', u'host', u'kwargs', u'operation', u'thingy', u'status', u'time'], 'existing_table': None, 'blocks': [FloatBlock: [time], 1 x 8, dtype float64, ObjectBlock: [args, host, kwargs, operation, thingy], 5 x 8, dtype object, IntBlock: [status], 1 x 8, dtype int64, DatetimeBlock: [date], 1 x 8, dtype datetime64[ns]], 'axis': 1, 'self': frame_table  (typ->appendable,nrows->None,ncols->1,indexers->[index]), 'axes': [0], 'kwargs': {}, 'klass': <class 'pandas.io.pytables.DataCol'>, 'block_obj':   args                date            host kwargs              operation      thingy  status      time\n0   [] 2013-12-02 00:33:59  yy38.segm1.org     {}       x_gbinf  a13yy38    -101  0.000801\n1   [] 2013-12-02 00:33:59  kyy1.segm1.org     {}         x_initobj  a19kyy1       1  0.003244\n2   [] 2013-12-02 00:34:00  yy10.segm1.org     {}    x_gobjParams  a14yy10    -101  0.002247\n3   [] 2013-12-02 00:34:00  yy24.segm1.org     {}        gtfull  a14yy24    -101  0.002787\n4   [] 2013-12-02 00:34:00  yy24.segm1.org     {}       x_gbinf  a14yy24    -101  0.001067\n5   [] 2013-12-02 00:34:00  yy34.segm1.org     {}           gxyzinf  a12yy34    -101  0.002652\n6   [] 2013-12-02 00:34:00  yy15.segm1.org     {}  deletemfg  a15yy15       1  0.004371\n7   [] 2013-12-02 00:34:00  yy15.segm1.org     {}           gxyzinf  a15yy15    -101  0.000602, 'axis_labels': [u'args', u'date', u'host', u'kwargs', u'operation', u'thingy', u'status', u'time'], 'nan_rep': 'nan', 'data_columns': [], 'obj':   args                date            host kwargs              operation      thingy  status      time\n0   [] 2013-12-02 00:33:59  yy38.segm1.org     {}       x_gbinf  a13yy38    -101  0.000801\n1   [] 2013-12-02 00:33:59  kyy1.segm1.org     {}         x_initobj  a19kyy1       1  0.003244\n2   [] 2013-12-02 00:34:00  yy10.segm1.org     {}    x_gobjParams  a14yy10    -101  0.002247\n3   [] 2013-12-02 00:34:00  yy24.segm1.org     {}        gtfull  a14yy24    -101  0.002787\n4   [] 2013-12-02 00:34:00  yy24.segm1.org     {}       x_gbinf  a14yy24    -101  0.001067\n5   [] 2013-12-02 00:34:00  yy34.segm1.org     {}           gxyzinf  a12yy34    -101  0.002652\n6   [] 2013-12-02 00:34:00  yy15.segm1.org     {}  deletemfg  a15yy15       1  0.004371\n7   [] 2013-12-02 00:34:00  yy15.segm1.org     {}           gxyzinf  a15yy15    -101  0.000602, 'validate': True, 'a': (1, [u'args', u'date', u'host', u'kwargs', u'operation', u'thingy', u'status', u'time']), 'index_axes_map': {0: name->index,cname->index,axis->0,pos->0,kind->integer}, 'b': ObjectBlock: [args, host, kwargs, operation, thingy], 5 x 8, dtype object, 'e': TypeError('[unicode] is not implemented as a table column',), 'name': None, 'existing_col': None, 'j': 2, 'i': 1, 'min_itemsize': None, 'col': name->values_block_1,cname->values_block_1,dtype->None,shape->None}\n""]";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 's' is not defined""]";['NameError'];0;1;"[""name 's' is not defined""]";['NameError']
624;624;624;624;2.0;0;20670726;;1;18;<python><pandas>;Computing diffs in Pandas after using groupby leads to unexpected result;5454.0;"[""import pandas as pd\nimport numpy as np\nimport random\nfrom itertools import product\n\nrandom.seed(1)       # so you can play along at home\nnp.random.seed(2)    # ditto\n\n# make a list of dates for a few periods\ndates = pd.date_range(start='2013-10-01', periods=4).to_native_types()\n# make a list of tickers\ntickers = ['ticker_%d' % i for i in range(3)]\n# make a list of all the possible (date, ticker) tuples\npairs = list(product(dates, tickers))\n# put them in a random order\nrandom.shuffle(pairs)\n# exclude a few possible pairs\npairs = pairs[:-3]\n# make some data for all of our selected (date, ticker) tuples\nvalues = np.random.rand(len(pairs))\n\nmydates, mytickers = zip(*pairs)\ndata = pd.DataFrame({'date': mydates, 'ticker': mytickers, 'value':values})\n     date        ticker      value\n0    2013-10-03  ticker_2    0.435995\n1    2013-10-04  ticker_2    0.025926\n2    2013-10-02  ticker_1    0.549662\n3    2013-10-01  ticker_0    0.435322\n4    2013-10-02  ticker_2    0.420368\n5    2013-10-03  ticker_0    0.330335\n6    2013-10-04  ticker_1    0.204649\n7    2013-10-02  ticker_0    0.619271\n8    2013-10-01  ticker_2    0.299655\ndata1 = data.copy() #let's leave the original data alone for later experiments\ndata1.sort(['ticker', 'date'], inplace=True)\ndata1['diffs'] = data1.groupby(['ticker'])['value'].transform(lambda x: x.diff())\ndata1.sort_index(inplace=True)\ndata1\n     date        ticker      value       diffs\n0    2013-10-03  ticker_2    0.435995    0.015627\n1    2013-10-04  ticker_2    0.025926   -0.410069\n2    2013-10-02  ticker_1    0.549662    NaN\n3    2013-10-01  ticker_0    0.435322    NaN\n4    2013-10-02  ticker_2    0.420368    0.120713\n5    2013-10-03  ticker_0    0.330335   -0.288936\n6    2013-10-04  ticker_1    0.204649   -0.345014\n7    2013-10-02  ticker_0    0.619271    0.183949\n8    2013-10-01  ticker_2    0.299655    NaN\ndata2 = data.copy()\ndata2.sort(['ticker', 'date'], inplace=True)\ndata2['diffs'] = data2.groupby('ticker')['value'].diff()\ndata2.sort_index(inplace=True)\ndata2\ndata3 = data.copy()\ndata3.sort(['ticker', 'date'], inplace=True)\ndata3['diffs'] = data3.groupby('ticker')['value'].transform(np.diff)\ndata3.sort_index(inplace=True)\ndata3\n     date        ticker     value       diffs\n0    2013-10-03  ticker_2    0.435995    0\n1    2013-10-04  ticker_2    0.025926   NaN\n2    2013-10-02  ticker_1    0.549662   NaN\n3    2013-10-01  ticker_0    0.435322   NaN\n4    2013-10-02  ticker_2    0.420368   NaN\n5    2013-10-03  ticker_0    0.330335    0\n6    2013-10-04  ticker_1    0.204649   NaN\n7    2013-10-02  ticker_0    0.619271   NaN\n8    2013-10-01  ticker_2    0.299655    0\n""]";"[""import pandas as pd\nimport numpy as np\nimport random\nfrom itertools import product\n\nrandom.seed(1)       # so you can play along at home\nnp.random.seed(2)    # ditto\n\n# make a list of dates for a few periods\ndates = pd.date_range(start='2013-10-01', periods=4).to_native_types()\n# make a list of tickers\ntickers = ['ticker_%d' % i for i in range(3)]\n# make a list of all the possible (date, ticker) tuples\npairs = list(product(dates, tickers))\n# put them in a random order\nrandom.shuffle(pairs)\n# exclude a few possible pairs\npairs = pairs[:-3]\n# make some data for all of our selected (date, ticker) tuples\nvalues = np.random.rand(len(pairs))\n\nmydates, mytickers = zip(*pairs)\ndata = pd.DataFrame({'date': mydates, 'ticker': mytickers, 'value':values})\n"", '     date        ticker      value\n0    2013-10-03  ticker_2    0.435995\n1    2013-10-04  ticker_2    0.025926\n2    2013-10-02  ticker_1    0.549662\n3    2013-10-01  ticker_0    0.435322\n4    2013-10-02  ticker_2    0.420368\n5    2013-10-03  ticker_0    0.330335\n6    2013-10-04  ticker_1    0.204649\n7    2013-10-02  ticker_0    0.619271\n8    2013-10-01  ticker_2    0.299655\n', ""data1 = data.copy() #let's leave the original data alone for later experiments\ndata1.sort(['ticker', 'date'], inplace=True)\ndata1['diffs'] = data1.groupby(['ticker'])['value'].transform(lambda x: x.diff())\ndata1.sort_index(inplace=True)\ndata1\n"", '     date        ticker      value       diffs\n0    2013-10-03  ticker_2    0.435995    0.015627\n1    2013-10-04  ticker_2    0.025926   -0.410069\n2    2013-10-02  ticker_1    0.549662    NaN\n3    2013-10-01  ticker_0    0.435322    NaN\n4    2013-10-02  ticker_2    0.420368    0.120713\n5    2013-10-03  ticker_0    0.330335   -0.288936\n6    2013-10-04  ticker_1    0.204649   -0.345014\n7    2013-10-02  ticker_0    0.619271    0.183949\n8    2013-10-01  ticker_2    0.299655    NaN\n', ""data2 = data.copy()\ndata2.sort(['ticker', 'date'], inplace=True)\ndata2['diffs'] = data2.groupby('ticker')['value'].diff()\ndata2.sort_index(inplace=True)\ndata2\n"", ""data3 = data.copy()\ndata3.sort(['ticker', 'date'], inplace=True)\ndata3['diffs'] = data3.groupby('ticker')['value'].transform(np.diff)\ndata3.sort_index(inplace=True)\ndata3\n"", '     date        ticker     value       diffs\n0    2013-10-03  ticker_2    0.435995    0\n1    2013-10-04  ticker_2    0.025926   NaN\n2    2013-10-02  ticker_1    0.549662   NaN\n3    2013-10-01  ticker_0    0.435322   NaN\n4    2013-10-02  ticker_2    0.420368   NaN\n5    2013-10-03  ticker_0    0.330335    0\n6    2013-10-04  ticker_1    0.204649   NaN\n7    2013-10-02  ticker_0    0.619271   NaN\n8    2013-10-01  ticker_2    0.299655    0\n']";"[""import pandas as pd\nimport numpy as np\nimport random\nfrom itertools import product\n\nrandom.seed(1)       # so you can play along at home\nnp.random.seed(2)    # ditto\n\n# make a list of dates for a few periods\ndates = pd.date_range(start='2013-10-01', periods=4).to_native_types()\n# make a list of tickers\ntickers = ['ticker_%d' % i for i in range(3)]\n# make a list of all the possible (date, ticker) tuples\npairs = list(product(dates, tickers))\n# put them in a random order\nrandom.shuffle(pairs)\n# exclude a few possible pairs\npairs = pairs[:-3]\n# make some data for all of our selected (date, ticker) tuples\nvalues = np.random.rand(len(pairs))\n\nmydates, mytickers = zip(*pairs)\ndata = pd.DataFrame({'date': mydates, 'ticker': mytickers, 'value':values})\n"", '     date        ticker      value\n0    2013-10-03  ticker_2    0.435995\n1    2013-10-04  ticker_2    0.025926\n2    2013-10-02  ticker_1    0.549662\n3    2013-10-01  ticker_0    0.435322\n4    2013-10-02  ticker_2    0.420368\n5    2013-10-03  ticker_0    0.330335\n6    2013-10-04  ticker_1    0.204649\n7    2013-10-02  ticker_0    0.619271\n8    2013-10-01  ticker_2    0.299655\n', ""data1 = data.copy() #let's leave the original data alone for later experiments\ndata1.sort(['ticker', 'date'], inplace=True)\ndata1['diffs'] = data1.groupby(['ticker'])['value'].transform(lambda x: x.diff())\ndata1.sort_index(inplace=True)\ndata1\n"", '     date        ticker      value       diffs\n0    2013-10-03  ticker_2    0.435995    0.015627\n1    2013-10-04  ticker_2    0.025926   -0.410069\n2    2013-10-02  ticker_1    0.549662    NaN\n3    2013-10-01  ticker_0    0.435322    NaN\n4    2013-10-02  ticker_2    0.420368    0.120713\n5    2013-10-03  ticker_0    0.330335   -0.288936\n6    2013-10-04  ticker_1    0.204649   -0.345014\n7    2013-10-02  ticker_0    0.619271    0.183949\n8    2013-10-01  ticker_2    0.299655    NaN\n', ""data2 = data.copy()\ndata2.sort(['ticker', 'date'], inplace=True)\ndata2['diffs'] = data2.groupby('ticker')['value'].diff()\ndata2.sort_index(inplace=True)\ndata2\n"", 'data1', 'data2', ""data3 = data.copy()\ndata3.sort(['ticker', 'date'], inplace=True)\ndata3['diffs'] = data3.groupby('ticker')['value'].transform(np.diff)\ndata3.sort_index(inplace=True)\ndata3\n"", '     date        ticker     value       diffs\n0    2013-10-03  ticker_2    0.435995    0\n1    2013-10-04  ticker_2    0.025926   NaN\n2    2013-10-02  ticker_1    0.549662   NaN\n3    2013-10-01  ticker_0    0.435322   NaN\n4    2013-10-02  ticker_2    0.420368   NaN\n5    2013-10-03  ticker_0    0.330335    0\n6    2013-10-04  ticker_1    0.204649   NaN\n7    2013-10-02  ticker_0    0.619271   NaN\n8    2013-10-01  ticker_2    0.299655    0\n', '.diff', 'np.diff', 'diff', 'DataFrame', 'transform', 'lambda', 'data1', 'diffs', 'data3', 'diff', 'transform', 'lambda']";"[""import pandas as pd\nimport numpy as np\nimport random\nfrom itertools import product\n\nrandom.seed(1)       # so you can play along at home\nnp.random.seed(2)    # ditto\n\n# make a list of dates for a few periods\ndates = pd.date_range(start='2013-10-01', periods=4).to_native_types()\n# make a list of tickers\ntickers = ['ticker_%d' % i for i in range(3)]\n# make a list of all the possible (date, ticker) tuples\npairs = list(product(dates, tickers))\n# put them in a random order\nrandom.shuffle(pairs)\n# exclude a few possible pairs\npairs = pairs[:-3]\n# make some data for all of our selected (date, ticker) tuples\nvalues = np.random.rand(len(pairs))\n\nmydates, mytickers = zip(*pairs)\ndata = pd.DataFrame({'date': mydates, 'ticker': mytickers, 'value':values})\ndata1 = data.copy() #let's leave the original data alone for later experiments\ndata1.sort(['ticker', 'date'], inplace=True)\ndata1['diffs'] = data1.groupby(['ticker'])['value'].transform(lambda x: x.diff())\ndata1.sort_index(inplace=True)\ndata1\ndata2 = data.copy()\ndata2.sort(['ticker', 'date'], inplace=True)\ndata2['diffs'] = data2.groupby('ticker')['value'].diff()\ndata2.sort_index(inplace=True)\ndata2\ndata3 = data.copy()\ndata3.sort(['ticker', 'date'], inplace=True)\ndata3['diffs'] = data3.groupby('ticker')['value'].transform(np.diff)\ndata3.sort_index(inplace=True)\ndata3\n""]";"[""import pandas as pd\nimport numpy as np\nimport random\nfrom itertools import product\n\nrandom.seed(1)       # so you can play along at home\nnp.random.seed(2)    # ditto\n\n# make a list of dates for a few periods\ndates = pd.date_range(start='2013-10-01', periods=4).to_native_types()\n# make a list of tickers\ntickers = ['ticker_%d' % i for i in range(3)]\n# make a list of all the possible (date, ticker) tuples\npairs = list(product(dates, tickers))\n# put them in a random order\nrandom.shuffle(pairs)\n# exclude a few possible pairs\npairs = pairs[:-3]\n# make some data for all of our selected (date, ticker) tuples\nvalues = np.random.rand(len(pairs))\n\nmydates, mytickers = zip(*pairs)\ndata = pd.DataFrame({'date': mydates, 'ticker': mytickers, 'value':values})\ndata1 = data.copy() #let's leave the original data alone for later experiments\ndata1.sort(['ticker', 'date'], inplace=True)\ndata1['diffs'] = data1.groupby(['ticker'])['value'].transform(lambda x: x.diff())\ndata1.sort_index(inplace=True)\ndata1\ndata2 = data.copy()\ndata2.sort(['ticker', 'date'], inplace=True)\ndata2['diffs'] = data2.groupby('ticker')['value'].diff()\ndata2.sort_index(inplace=True)\ndata2\ndata3 = data.copy()\ndata3.sort(['ticker', 'date'], inplace=True)\ndata3['diffs'] = data3.groupby('ticker')['value'].transform(np.diff)\ndata3.sort_index(inplace=True)\ndata3\n""]";False;"[""import pandas as pd\nimport pandas as pd\nimport numpy as np\nimport random\nfrom itertools import product\n\nrandom.seed(1)       # so you can play along at home\nnp.random.seed(2)    # ditto\n\n# make a list of dates for a few periods\ndates = pd.date_range(start='2013-10-01', periods=4).to_native_types()\n# make a list of tickers\ntickers = ['ticker_%d' % i for i in range(3)]\n# make a list of all the possible (date, ticker) tuples\npairs = list(product(dates, tickers))\n# put them in a random order\nrandom.shuffle(pairs)\n# exclude a few possible pairs\npairs = pairs[:-3]\n# make some data for all of our selected (date, ticker) tuples\nvalues = np.random.rand(len(pairs))\n\nmydates, mytickers = zip(*pairs)\ndata = pd.DataFrame({'date': mydates, 'ticker': mytickers, 'value':values})\ndata1 = data.copy() #let's leave the original data alone for later experiments\ndata1.sort(['ticker', 'date'], inplace=True)\ndata1['diffs'] = data1.groupby(['ticker'])['value'].transform(lambda x: x.diff())\ndata1.sort_index(inplace=True)\ndata1\ndata2 = data.copy()\ndata2.sort(['ticker', 'date'], inplace=True)\ndata2['diffs'] = data2.groupby('ticker')['value'].diff()\ndata2.sort_index(inplace=True)\ndata2\ndata3 = data.copy()\ndata3.sort(['ticker', 'date'], inplace=True)\ndata3['diffs'] = data3.groupby('ticker')['value'].transform(np.diff)\ndata3.sort_index(inplace=True)\ndata3\n""]";False;0;1;"[""name 'data3' is not defined""]";['NameError'];0;1;"[""name 'data3' is not defined""]";['NameError'];0;1;"[""name 'data3' is not defined""]";['NameError']
625;625;625;625;3.0;2;20701484;;1;15;<python><pandas><linear-regression><statsmodels>;Why do I get only one parameter from a statsmodels OLS fit;8197.0;"[""$ python\nPython 2.7.6 (v2.7.6:3a1db0d2747e, Nov 10 2013, 00:42:54) \n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\n>>> import statsmodels.api as sm\n>>> statsmodels.__version__\n'0.5.0'\n>>> import numpy \n>>> y = numpy.array([1,2,3,4,5,6,7,8,9])\n>>> X = numpy.array([1,1,2,2,3,3,4,4,5])\n>>> res_ols = sm.OLS(y, X).fit()\n>>> res_ols.params\narray([ 1.82352941])\n""]";"[""$ python\nPython 2.7.6 (v2.7.6:3a1db0d2747e, Nov 10 2013, 00:42:54) \n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\n>>> import statsmodels.api as sm\n>>> statsmodels.__version__\n'0.5.0'\n>>> import numpy \n>>> y = numpy.array([1,2,3,4,5,6,7,8,9])\n>>> X = numpy.array([1,1,2,2,3,3,4,4,5])\n>>> res_ols = sm.OLS(y, X).fit()\n>>> res_ols.params\narray([ 1.82352941])\n""]";"[""$ python\nPython 2.7.6 (v2.7.6:3a1db0d2747e, Nov 10 2013, 00:42:54) \n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\n>>> import statsmodels.api as sm\n>>> statsmodels.__version__\n'0.5.0'\n>>> import numpy \n>>> y = numpy.array([1,2,3,4,5,6,7,8,9])\n>>> X = numpy.array([1,1,2,2,3,3,4,4,5])\n>>> res_ols = sm.OLS(y, X).fit()\n>>> res_ols.params\narray([ 1.82352941])\n""]";"[""'0.5.0'\narray([ 1.82352941])\n""]";"[""'0.5.0'\narray([ 1.82352941])\n""]";False;"[""import pandas as pd\n'0.5.0'\narray([ 1.82352941])\n""]";False;0;1;"[""name 'sm' is not defined""]";['NameError'];0;1;"[""name 'sm' is not defined""]";['NameError'];0;1;"[""name 'sm' is not defined""]";['NameError']
626;626;626;626;1.0;0;20738357;;1;12;<python><numpy><pandas>;getting seconds from numpy timedelta64;4626.0;"[""index = np.array(['2013-11-11T12:36:00.078757888-0800',\n                  '2013-11-11T12:36:03.692692992-0800',\n                  '2013-11-11T12:36:07.085489920-0800',\n                  '2013-11-11T12:36:08.957488128-0800'], dtype='datetime64[ns]')\ndiff(index).astype('float64')/1e9\n""]";"[""index = np.array(['2013-11-11T12:36:00.078757888-0800',\n                  '2013-11-11T12:36:03.692692992-0800',\n                  '2013-11-11T12:36:07.085489920-0800',\n                  '2013-11-11T12:36:08.957488128-0800'], dtype='datetime64[ns]')\n"", ""diff(index).astype('float64')/1e9\n""]";"[""index = np.array(['2013-11-11T12:36:00.078757888-0800',\n                  '2013-11-11T12:36:03.692692992-0800',\n                  '2013-11-11T12:36:07.085489920-0800',\n                  '2013-11-11T12:36:08.957488128-0800'], dtype='datetime64[ns]')\n"", ""diff(index).astype('float64')/1e9\n""]";"[""diff(index).astype('float64')/1e9\n""]";"[""diff(index).astype('float64')/1e9\n""]";False;"[""import pandas as pd\ndiff(index).astype('float64')/1e9\n""]";False;0;1;"[""name 'array' is not defined""]";['NameError'];0;1;"[""name 'array' is not defined""]";['NameError'];0;1;"[""name 'array' is not defined""]";['NameError']
627;627;627;627;2.0;1;20763012;;1;62;<numpy><pandas>;Creating a Pandas DataFrame from a Numpy array: How do I specify the index column and column headers?;128433.0;"[""data = array([['','Col1','Col2'],['Row1',1,2],['Row2',3,4]])\ndf = pd.DataFrame(data,index=data[:,0]),\n""]";"[""data = array([['','Col1','Col2'],['Row1',1,2],['Row2',3,4]])\n"", 'df = pd.DataFrame(data,index=data[:,0]),\n']";"[""data = array([['','Col1','Col2'],['Row1',1,2],['Row2',3,4]])\n"", 'df = pd.DataFrame(data,index=data[:,0]),\n']";"[""data = array([['','Col1','Col2'],['Row1',1,2],['Row2',3,4]])\ndf = pd.DataFrame(data,index=data[:,0]),\n""]";"[""import pandas as pd\ndata = array([['','Col1','Col2'],['Row1',1,2],['Row2',3,4]])\ndf = pd.DataFrame(data,index=data[:,0]),\n""]";True;"[""import pandas as pd\ndata = array([['','Col1','Col2'],['Row1',1,2],['Row2',3,4]])\ndf = pd.DataFrame(data,index=data[:,0]),\n""]";False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
628;628;628;628;5.0;0;20804673;;1;16;<python><pandas>;Appending column totals to a Pandas DataFrame;14494.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;['Sucess', 'cannot set a frame with no defined columns'];['Sucess', 'ValueError']
629;629;629;629;2.0;2;20829748;;1;11;<python><pandas>;Pandas: Assigning multiple *new* columns simultaneously;6380.0;"[""import pandas as pd\nimport numpy as np\n\nnp.random.seed(1)\nn = 10\n\nlabels = list('abcdef')\ncolors = ['red', 'green', 'blue']\nsizes = ['small', 'medium', 'large']\n\nlabeldict = {c: (np.random.choice(colors), np.random.choice(sizes)) for c in labels}\n\ndf = pd.DataFrame({'label': np.random.choice(labels, n), \n                   'somedata': np.random.randn(n)})\ndf['color'], df['size'] = zip(*df['label'].map(labeldict))\nprint df\n\n  label  somedata  color    size\n0     b  0.196643    red  medium\n1     c -1.545214  green   small\n2     a -0.088104  green   small\n3     c  0.852239  green   small\n4     b  0.677234    red  medium\n5     c -0.106878  green   small\n6     a  0.725274  green   small\n7     d  0.934889    red  medium\n8     a  1.118297  green   small\n9     c  0.055613  green   small\n# set up attrlist for later use\nattrlist = ['color', 'size']\n\n# non-working idea 1)\ndf[attrlist] = zip(*df['label'].map(labeldict))\n\n# non-working idea 2)\ndf.loc[:, attrlist] = zip(*df['label'].map(labeldict))\nfor a in attrlist:\n    df[a] = 0\ndf[attrlist] = zip(*df['label'].map(labeldict))\n""]";"[""import pandas as pd\nimport numpy as np\n\nnp.random.seed(1)\nn = 10\n\nlabels = list('abcdef')\ncolors = ['red', 'green', 'blue']\nsizes = ['small', 'medium', 'large']\n\nlabeldict = {c: (np.random.choice(colors), np.random.choice(sizes)) for c in labels}\n\ndf = pd.DataFrame({'label': np.random.choice(labels, n), \n                   'somedata': np.random.randn(n)})\n"", ""df['color'], df['size'] = zip(*df['label'].map(labeldict))\nprint df\n\n  label  somedata  color    size\n0     b  0.196643    red  medium\n1     c -1.545214  green   small\n2     a -0.088104  green   small\n3     c  0.852239  green   small\n4     b  0.677234    red  medium\n5     c -0.106878  green   small\n6     a  0.725274  green   small\n7     d  0.934889    red  medium\n8     a  1.118297  green   small\n9     c  0.055613  green   small\n"", ""# set up attrlist for later use\nattrlist = ['color', 'size']\n\n# non-working idea 1)\ndf[attrlist] = zip(*df['label'].map(labeldict))\n\n# non-working idea 2)\ndf.loc[:, attrlist] = zip(*df['label'].map(labeldict))\n"", ""for a in attrlist:\n    df[a] = 0\ndf[attrlist] = zip(*df['label'].map(labeldict))\n""]";"[""import pandas as pd\nimport numpy as np\n\nnp.random.seed(1)\nn = 10\n\nlabels = list('abcdef')\ncolors = ['red', 'green', 'blue']\nsizes = ['small', 'medium', 'large']\n\nlabeldict = {c: (np.random.choice(colors), np.random.choice(sizes)) for c in labels}\n\ndf = pd.DataFrame({'label': np.random.choice(labels, n), \n                   'somedata': np.random.randn(n)})\n"", ""df['color'], df['size'] = zip(*df['label'].map(labeldict))\nprint df\n\n  label  somedata  color    size\n0     b  0.196643    red  medium\n1     c -1.545214  green   small\n2     a -0.088104  green   small\n3     c  0.852239  green   small\n4     b  0.677234    red  medium\n5     c -0.106878  green   small\n6     a  0.725274  green   small\n7     d  0.934889    red  medium\n8     a  1.118297  green   small\n9     c  0.055613  green   small\n"", 'labeldict', ""# set up attrlist for later use\nattrlist = ['color', 'size']\n\n# non-working idea 1)\ndf[attrlist] = zip(*df['label'].map(labeldict))\n\n# non-working idea 2)\ndf.loc[:, attrlist] = zip(*df['label'].map(labeldict))\n"", ""for a in attrlist:\n    df[a] = 0\ndf[attrlist] = zip(*df['label'].map(labeldict))\n""]";"[""import pandas as pd\nimport numpy as np\n\nnp.random.seed(1)\nn = 10\n\nlabels = list('abcdef')\ncolors = ['red', 'green', 'blue']\nsizes = ['small', 'medium', 'large']\n\nlabeldict = {c: (np.random.choice(colors), np.random.choice(sizes)) for c in labels}\n\ndf['color'], df['size'] = zip(*df['label'].map(labeldict))\n\n# set up attrlist for later use\nattrlist = ['color', 'size']\n\n# non-working idea 1)\ndf[attrlist] = zip(*df['label'].map(labeldict))\n\n# non-working idea 2)\ndf.loc[:, attrlist] = zip(*df['label'].map(labeldict))\ndf[attrlist] = zip(*df['label'].map(labeldict))\n""]";"[""import pandas as pd\nimport numpy as np\n\nnp.random.seed(1)\nn = 10\n\nlabels = list('abcdef')\ncolors = ['red', 'green', 'blue']\nsizes = ['small', 'medium', 'large']\n\nlabeldict = {c: (np.random.choice(colors), np.random.choice(sizes)) for c in labels}\n\ndf['color'], df['size'] = zip(*df['label'].map(labeldict))\n\n# set up attrlist for later use\nattrlist = ['color', 'size']\n\n# non-working idea 1)\ndf[attrlist] = zip(*df['label'].map(labeldict))\n\n# non-working idea 2)\ndf.loc[:, attrlist] = zip(*df['label'].map(labeldict))\ndf[attrlist] = zip(*df['label'].map(labeldict))\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(1)\nn = 10\n\nlabels = list('abcdef')\ncolors = ['red', 'green', 'blue']\nsizes = ['small', 'medium', 'large']\n\nlabeldict = {c: (np.random.choice(colors), np.random.choice(sizes)) for c in labels}\n\ndf['color'], df['size'] = zip(*df['label'].map(labeldict))\n\n# set up attrlist for later use\nattrlist = ['color', 'size']\n\n# non-working idea 1)\ndf[attrlist] = zip(*df['label'].map(labeldict))\n\n# non-working idea 2)\ndf.loc[:, attrlist] = zip(*df['label'].map(labeldict))\ndf[attrlist] = zip(*df['label'].map(labeldict))\n""]";True;0;0;[];[];0;0;[];[];0;0;[];[]
630;630;630;630;1.0;0;20838395;;1;18;<python><pandas>;What is the point of .ix indexing for pandas Series;17681.0;"[""# play data ...\nimport string\nidx = [i for i in string.uppercase] # A, B, C .. Z\nt = pd.Series(range(26), index=idx) # 0, 1, 2 .. 25\n\n# examples ...\nt[0]              # --> 0\nt['A']            # --> 0\nt[['A','M']]      # --> [0, 12]\nt['A':'D']        # --> [0, 1, 2, 3]\nt.iloc[25]        # --> 25\nt.loc['Z']        # --> 25\nt.loc[['A','Z']]  # --> [0, 25]\nt.ix['A':'C']     # --> [0, 1, 2]\nt.ix[0:2]         # --> [0, 1]\n""]";"[""# play data ...\nimport string\nidx = [i for i in string.uppercase] # A, B, C .. Z\nt = pd.Series(range(26), index=idx) # 0, 1, 2 .. 25\n\n# examples ...\nt[0]              # --> 0\nt['A']            # --> 0\nt[['A','M']]      # --> [0, 12]\nt['A':'D']        # --> [0, 1, 2, 3]\nt.iloc[25]        # --> 25\nt.loc['Z']        # --> 25\nt.loc[['A','Z']]  # --> [0, 25]\nt.ix['A':'C']     # --> [0, 1, 2]\nt.ix[0:2]         # --> [0, 1]\n""]";"[""# play data ...\nimport string\nidx = [i for i in string.uppercase] # A, B, C .. Z\nt = pd.Series(range(26), index=idx) # 0, 1, 2 .. 25\n\n# examples ...\nt[0]              # --> 0\nt['A']            # --> 0\nt[['A','M']]      # --> [0, 12]\nt['A':'D']        # --> [0, 1, 2, 3]\nt.iloc[25]        # --> 25\nt.loc['Z']        # --> 25\nt.loc[['A','Z']]  # --> [0, 25]\nt.ix['A':'C']     # --> [0, 1, 2]\nt.ix[0:2]         # --> [0, 1]\n""]";"[""# play data ...\nimport string\nidx = [i for i in string.uppercase] # A, B, C .. Z\nt = pd.Series(range(26), index=idx) # 0, 1, 2 .. 25\n\n# examples ...\nt[0]              # --> 0\nt['A']            # --> 0\nt[['A','M']]      # --> [0, 12]\nt['A':'D']        # --> [0, 1, 2, 3]\nt.iloc[25]        # --> 25\nt.loc['Z']        # --> 25\nt.loc[['A','Z']]  # --> [0, 25]\nt.ix['A':'C']     # --> [0, 1, 2]\nt.ix[0:2]         # --> [0, 1]\n""]";"[""import pandas as pd\n# play data ...\nimport string\nidx = [i for i in string.uppercase] # A, B, C .. Z\nt = pd.Series(range(26), index=idx) # 0, 1, 2 .. 25\n\n# examples ...\nt[0]              # --> 0\nt['A']            # --> 0\nt[['A','M']]      # --> [0, 12]\nt['A':'D']        # --> [0, 1, 2, 3]\nt.iloc[25]        # --> 25\nt.loc['Z']        # --> 25\nt.loc[['A','Z']]  # --> [0, 25]\nt.ix['A':'C']     # --> [0, 1, 2]\nt.ix[0:2]         # --> [0, 1]\n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\n# play data ...\nimport string\nidx = [i for i in string.uppercase] # A, B, C .. Z\nt = pd.Series(range(26), index=idx) # 0, 1, 2 .. 25\n\n# examples ...\nt[0]              # --> 0\nt['A']            # --> 0\nt[['A','M']]      # --> [0, 12]\nt['A':'D']        # --> [0, 1, 2, 3]\nt.iloc[25]        # --> 25\nt.loc['Z']        # --> 25\nt.loc[['A','Z']]  # --> [0, 25]\nt.ix['A':'C']     # --> [0, 1, 2]\nt.ix[0:2]         # --> [0, 1]\n""]";True;0;1;"[""name 'DataFrame' is not defined""]";['NameError'];0;1;"[""name 'DataFrame' is not defined""]";['NameError'];0;1;"[""name 'DataFrame' is not defined""]";['NameError']
631;631;631;631;3.0;2;20845213;;1;100;<python><csv><indexing><pandas>;How to avoid Python/Pandas creating an index in a saved csv?;47670.0;"[""pd.read_csv('C:/Path to file to edit.csv', index_col = False)\npd.to_csv('C:/Path to save edited file.csv', index_col = False)\n""]";"[""pd.read_csv('C:/Path to file to edit.csv', index_col = False)\n"", ""pd.to_csv('C:/Path to save edited file.csv', index_col = False)\n""]";"[""pd.to_csv('C:/Path of file.csv')"", ""pd.read_csv('C:/Path to file to edit.csv', index_col = False)\n"", ""pd.to_csv('C:/Path to save edited file.csv', index_col = False)\n""]";"[""pd.read_csv('C:/Path to file to edit.csv', index_col = False)\npd.to_csv('C:/Path to save edited file.csv', index_col = False)\n""]";"[""import pandas as pd\npd.read_csv('C:/Path to file to edit.csv', index_col = False)\npd.to_csv('C:/Path to save edited file.csv', index_col = False)\n""]";True;"[""import pandas as pd\npd.read_csv('C:/Path to file to edit.csv', index_col = False)\npd.to_csv('C:/Path to save edited file.csv', index_col = False)\n""]";False;1;2;"[""name 'pd' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""module 'pandas' has no attribute 'to_csv'"", 'Sucess']";['AttributeError', 'Sucess'];1;2;"[""module 'pandas' has no attribute 'to_csv'"", 'Sucess']";['AttributeError', 'Sucess']
632;632;632;632;4.0;6;20853474;;1;59;<python><pandas><pip>;ImportError: No module named dateutil.parser;72499.0;"['monas-mbp:book mona$ sudo pip install python-dateutil\nRequirement already satisfied (use --upgrade to upgrade): python-dateutil in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python\nCleaning up...\nmonas-mbp:book mona$ python t1.py\nNo module named dateutil.parser\nTraceback (most recent call last):\n  File ""t1.py"", line 4, in <module>\n    import pandas as pd\n  File ""/Library/Python/2.7/site-packages/pandas/__init__.py"", line 6, in <module>\n    from . import hashtable, tslib, lib\n  File ""tslib.pyx"", line 31, in init pandas.tslib (pandas/tslib.c:48782)\nImportError: No module named dateutil.parser\nimport codecs \nfrom math import sqrt\nimport numpy as np\nimport pandas as pd\n\nusers = {""Angelica"": {""Blues Traveler"": 3.5, ""Broken Bells"": 2.0,\n                      ""Norah Jones"": 4.5, ""Phoenix"": 5.0,\n                      ""Slightly Stoopid"": 1.5,\n                      ""The Strokes"": 2.5, ""Vampire Weekend"": 2.0},\n\n         ""Bill"":{""Blues Traveler"": 2.0, ""Broken Bells"": 3.5,\n                 ""Deadmau5"": 4.0, ""Phoenix"": 2.0,\n                 ""Slightly Stoopid"": 3.5, ""Vampire Weekend"": 3.0},\n\n         ""Chan"": {""Blues Traveler"": 5.0, ""Broken Bells"": 1.0,\n                  ""Deadmau5"": 1.0, ""Norah Jones"": 3.0, ""Phoenix"": 5,\n                  ""Slightly Stoopid"": 1.0},\n\n         ""Dan"": {""Blues Traveler"": 3.0, ""Broken Bells"": 4.0,\n                 ""Deadmau5"": 4.5, ""Phoenix"": 3.0,\n                 ""Slightly Stoopid"": 4.5, ""The Strokes"": 4.0,\n                 ""Vampire Weekend"": 2.0},\n\n         ""Hailey"": {""Broken Bells"": 4.0, ""Deadmau5"": 1.0,\n                    ""Norah Jones"": 4.0, ""The Strokes"": 4.0,\n                    ""Vampire Weekend"": 1.0},\n\n         ""Jordyn"":  {""Broken Bells"": 4.5, ""Deadmau5"": 4.0,\n                     ""Norah Jones"": 5.0, ""Phoenix"": 5.0,\n                     ""Slightly Stoopid"": 4.5, ""The Strokes"": 4.0,\n                     ""Vampire Weekend"": 4.0},\n\n         ""Sam"": {""Blues Traveler"": 5.0, ""Broken Bells"": 2.0,\n                 ""Norah Jones"": 3.0, ""Phoenix"": 5.0,\n                 ""Slightly Stoopid"": 4.0, ""The Strokes"": 5.0},\n\n         ""Veronica"": {""Blues Traveler"": 3.0, ""Norah Jones"": 5.0,\n                      ""Phoenix"": 4.0, ""Slightly Stoopid"": 2.5,\n                      ""The Strokes"": 3.0}\n        }\n\n\n\nclass recommender:\n\n    def __init__(self, data, k=1, metric=\'pearson\', n=5):\n        """""" initialize recommender\n        currently, if data is dictionary the recommender is initialized\n        to it.\n        For all other data types of data, no initialization occurs\n        k is the k value for k nearest neighbor\n        metric is which distance formula to use\n        n is the maximum number of recommendations to make""""""\n        self.k = k\n        self.n = n\n        self.username2id = {}\n        self.userid2name = {}\n        self.productid2name = {}\n        # for some reason I want to save the name of the metric\n        self.metric = metric\n        if self.metric == \'pearson\':\n            self.fn = self.pearson\n        #\n        # if data is dictionary set recommender data to it\n        #\n        if type(data).__name__ == \'dict\':\n            self.data = data\n\n    def convertProductID2name(self, id):\n        """"""Given product id number return product name""""""\n        if id in self.productid2name:\n            return self.productid2name[id]\n        else:\n            return id\n\n\n    def userRatings(self, id, n):\n        """"""Return n top ratings for user with id""""""\n        print (""Ratings for "" + self.userid2name[id])\n        ratings = self.data[id]\n        print(len(ratings))\n        ratings = list(ratings.items())\n        ratings = [(self.convertProductID2name(k), v)\n                   for (k, v) in ratings]\n        # finally sort and return\n        ratings.sort(key=lambda artistTuple: artistTuple[1],\n                     reverse = True)\n        ratings = ratings[:n]\n        for rating in ratings:\n            print(""%s\\t%i"" % (rating[0], rating[1]))\n\n\n\n\n    def loadBookDB(self, path=\'\'):\n        """"""loads the BX book dataset. Path is where the BX files are\n        located""""""\n        self.data = {}\n        i = 0\n        #\n        # First load book ratings into self.data\n        #\n        f = codecs.open(path + ""BX-Book-Ratings.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #separate line into fields\n            fields = line.split(\';\')\n            user = fields[0].strip(\'""\')\n            book = fields[1].strip(\'""\')\n            rating = int(fields[2].strip().strip(\'""\'))\n            if user in self.data:\n                currentRatings = self.data[user]\n            else:\n                currentRatings = {}\n            currentRatings[book] = rating\n            self.data[user] = currentRatings\n        f.close()\n        #\n        # Now load books into self.productid2name\n        # Books contains isbn, title, and author among other fields\n        #\n        f = codecs.open(path + ""BX-Books.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #separate line into fields\n            fields = line.split(\';\')\n            isbn = fields[0].strip(\'""\')\n            title = fields[1].strip(\'""\')\n            author = fields[2].strip().strip(\'""\')\n            title = title + \' by \' + author\n            self.productid2name[isbn] = title\n        f.close()\n        #\n        #  Now load user info into both self.userid2name and\n        #  self.username2id\n        #\n        f = codecs.open(path + ""BX-Users.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #print(line)\n            #separate line into fields\n            fields = line.split(\';\')\n            userid = fields[0].strip(\'""\')\n            location = fields[1].strip(\'""\')\n            if len(fields) > 3:\n                age = fields[2].strip().strip(\'""\')\n            else:\n                age = \'NULL\'\n            if age != \'NULL\':\n                value = location + \'  (age: \' + age + \')\'\n            else:\n                value = location\n            self.userid2name[userid] = value\n            self.username2id[location] = userid\n        f.close()\n        print(i)\n\n\n    def pearson(self, rating1, rating2):\n        sum_xy = 0\n        sum_x = 0\n        sum_y = 0\n        sum_x2 = 0\n        sum_y2 = 0\n        n = 0\n        for key in rating1:\n            if key in rating2:\n                n += 1\n                x = rating1[key]\n                y = rating2[key]\n                sum_xy += x * y\n                sum_x += x\n                sum_y += y\n                sum_x2 += pow(x, 2)\n                sum_y2 += pow(y, 2)\n        if n == 0:\n            return 0\n        # now compute denominator\n        denominator = (sqrt(sum_x2 - pow(sum_x, 2) / n)\n                       * sqrt(sum_y2 - pow(sum_y, 2) / n))\n        if denominator == 0:\n            return 0\n        else:\n            return (sum_xy - (sum_x * sum_y) / n) / denominator\n\n\n    def computeNearestNeighbor(self, username):\n        """"""creates a sorted list of users based on their distance to\n        username""""""\n        distances = []\n        for instance in self.data:\n            if instance != username:\n                distance = self.fn(self.data[username],\n                                   self.data[instance])\n                distances.append((instance, distance))\n        # sort based on distance -- closest first\n        distances.sort(key=lambda artistTuple: artistTuple[1],\n                       reverse=True)\n        return distances\n\n    def recommend(self, user):\n       """"""Give list of recommendations""""""\n       recommendations = {}\n       # first get list of users  ordered by nearness\n       nearest = self.computeNearestNeighbor(user)\n       #\n       # now get the ratings for the user\n       #\n       userRatings = self.data[user]\n       #\n       # determine the total distance\n       totalDistance = 0.0\n       for i in range(self.k):\n          totalDistance += nearest[i][1]\n       # now iterate through the k nearest neighbors\n       # accumulating their ratings\n       for i in range(self.k):\n          # compute slice of pie \n          weight = nearest[i][1] / totalDistance\n          # get the name of the person\n          name = nearest[i][0]\n          # get the ratings for this person\n          neighborRatings = self.data[name]\n          # get the name of the person\n          # now find bands neighbor rated that user didn\'t\n          for artist in neighborRatings:\n             if not artist in userRatings:\n                if artist not in recommendations:\n                   recommendations[artist] = (neighborRatings[artist]\n                                              * weight)\n                else:\n                   recommendations[artist] = (recommendations[artist]\n                                              + neighborRatings[artist]\n                                              * weight)\n       # now make list from dictionary\n       recommendations = list(recommendations.items())\n       recommendations = [(self.convertProductID2name(k), v)\n                          for (k, v) in recommendations]\n       # finally sort and return\n       recommendations.sort(key=lambda artistTuple: artistTuple[1],\n                            reverse = True)\n       # Return the first n items\n       return recommendations[:self.n]\n\nr = recommender(users)\n# The author implementation\nr.loadBookDB(\'/Users/mona/Downloads/BX-Dump/\')\n\nratings = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Book-Ratings.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nbooks = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Books.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nusers = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Users.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\n\n\n\npivot_rating = ratings.pivot(index=\'User-ID\', columns=\'ISBN\', values=\'Book-Rating\')\n']";"['monas-mbp:book mona$ sudo pip install python-dateutil\nRequirement already satisfied (use --upgrade to upgrade): python-dateutil in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python\nCleaning up...\nmonas-mbp:book mona$ python t1.py\nNo module named dateutil.parser\nTraceback (most recent call last):\n  File ""t1.py"", line 4, in <module>\n    import pandas as pd\n  File ""/Library/Python/2.7/site-packages/pandas/__init__.py"", line 6, in <module>\n    from . import hashtable, tslib, lib\n  File ""tslib.pyx"", line 31, in init pandas.tslib (pandas/tslib.c:48782)\nImportError: No module named dateutil.parser\n', 'import codecs \nfrom math import sqrt\nimport numpy as np\nimport pandas as pd\n\nusers = {""Angelica"": {""Blues Traveler"": 3.5, ""Broken Bells"": 2.0,\n                      ""Norah Jones"": 4.5, ""Phoenix"": 5.0,\n                      ""Slightly Stoopid"": 1.5,\n                      ""The Strokes"": 2.5, ""Vampire Weekend"": 2.0},\n\n         ""Bill"":{""Blues Traveler"": 2.0, ""Broken Bells"": 3.5,\n                 ""Deadmau5"": 4.0, ""Phoenix"": 2.0,\n                 ""Slightly Stoopid"": 3.5, ""Vampire Weekend"": 3.0},\n\n         ""Chan"": {""Blues Traveler"": 5.0, ""Broken Bells"": 1.0,\n                  ""Deadmau5"": 1.0, ""Norah Jones"": 3.0, ""Phoenix"": 5,\n                  ""Slightly Stoopid"": 1.0},\n\n         ""Dan"": {""Blues Traveler"": 3.0, ""Broken Bells"": 4.0,\n                 ""Deadmau5"": 4.5, ""Phoenix"": 3.0,\n                 ""Slightly Stoopid"": 4.5, ""The Strokes"": 4.0,\n                 ""Vampire Weekend"": 2.0},\n\n         ""Hailey"": {""Broken Bells"": 4.0, ""Deadmau5"": 1.0,\n                    ""Norah Jones"": 4.0, ""The Strokes"": 4.0,\n                    ""Vampire Weekend"": 1.0},\n\n         ""Jordyn"":  {""Broken Bells"": 4.5, ""Deadmau5"": 4.0,\n                     ""Norah Jones"": 5.0, ""Phoenix"": 5.0,\n                     ""Slightly Stoopid"": 4.5, ""The Strokes"": 4.0,\n                     ""Vampire Weekend"": 4.0},\n\n         ""Sam"": {""Blues Traveler"": 5.0, ""Broken Bells"": 2.0,\n                 ""Norah Jones"": 3.0, ""Phoenix"": 5.0,\n                 ""Slightly Stoopid"": 4.0, ""The Strokes"": 5.0},\n\n         ""Veronica"": {""Blues Traveler"": 3.0, ""Norah Jones"": 5.0,\n                      ""Phoenix"": 4.0, ""Slightly Stoopid"": 2.5,\n                      ""The Strokes"": 3.0}\n        }\n\n\n\nclass recommender:\n\n    def __init__(self, data, k=1, metric=\'pearson\', n=5):\n        """""" initialize recommender\n        currently, if data is dictionary the recommender is initialized\n        to it.\n        For all other data types of data, no initialization occurs\n        k is the k value for k nearest neighbor\n        metric is which distance formula to use\n        n is the maximum number of recommendations to make""""""\n        self.k = k\n        self.n = n\n        self.username2id = {}\n        self.userid2name = {}\n        self.productid2name = {}\n        # for some reason I want to save the name of the metric\n        self.metric = metric\n        if self.metric == \'pearson\':\n            self.fn = self.pearson\n        #\n        # if data is dictionary set recommender data to it\n        #\n        if type(data).__name__ == \'dict\':\n            self.data = data\n\n    def convertProductID2name(self, id):\n        """"""Given product id number return product name""""""\n        if id in self.productid2name:\n            return self.productid2name[id]\n        else:\n            return id\n\n\n    def userRatings(self, id, n):\n        """"""Return n top ratings for user with id""""""\n        print (""Ratings for "" + self.userid2name[id])\n        ratings = self.data[id]\n        print(len(ratings))\n        ratings = list(ratings.items())\n        ratings = [(self.convertProductID2name(k), v)\n                   for (k, v) in ratings]\n        # finally sort and return\n        ratings.sort(key=lambda artistTuple: artistTuple[1],\n                     reverse = True)\n        ratings = ratings[:n]\n        for rating in ratings:\n            print(""%s\\t%i"" % (rating[0], rating[1]))\n\n\n\n\n    def loadBookDB(self, path=\'\'):\n        """"""loads the BX book dataset. Path is where the BX files are\n        located""""""\n        self.data = {}\n        i = 0\n        #\n        # First load book ratings into self.data\n        #\n        f = codecs.open(path + ""BX-Book-Ratings.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #separate line into fields\n            fields = line.split(\';\')\n            user = fields[0].strip(\'""\')\n            book = fields[1].strip(\'""\')\n            rating = int(fields[2].strip().strip(\'""\'))\n            if user in self.data:\n                currentRatings = self.data[user]\n            else:\n                currentRatings = {}\n            currentRatings[book] = rating\n            self.data[user] = currentRatings\n        f.close()\n        #\n        # Now load books into self.productid2name\n        # Books contains isbn, title, and author among other fields\n        #\n        f = codecs.open(path + ""BX-Books.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #separate line into fields\n            fields = line.split(\';\')\n            isbn = fields[0].strip(\'""\')\n            title = fields[1].strip(\'""\')\n            author = fields[2].strip().strip(\'""\')\n            title = title + \' by \' + author\n            self.productid2name[isbn] = title\n        f.close()\n        #\n        #  Now load user info into both self.userid2name and\n        #  self.username2id\n        #\n        f = codecs.open(path + ""BX-Users.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #print(line)\n            #separate line into fields\n            fields = line.split(\';\')\n            userid = fields[0].strip(\'""\')\n            location = fields[1].strip(\'""\')\n            if len(fields) > 3:\n                age = fields[2].strip().strip(\'""\')\n            else:\n                age = \'NULL\'\n            if age != \'NULL\':\n                value = location + \'  (age: \' + age + \')\'\n            else:\n                value = location\n            self.userid2name[userid] = value\n            self.username2id[location] = userid\n        f.close()\n        print(i)\n\n\n    def pearson(self, rating1, rating2):\n        sum_xy = 0\n        sum_x = 0\n        sum_y = 0\n        sum_x2 = 0\n        sum_y2 = 0\n        n = 0\n        for key in rating1:\n            if key in rating2:\n                n += 1\n                x = rating1[key]\n                y = rating2[key]\n                sum_xy += x * y\n                sum_x += x\n                sum_y += y\n                sum_x2 += pow(x, 2)\n                sum_y2 += pow(y, 2)\n        if n == 0:\n            return 0\n        # now compute denominator\n        denominator = (sqrt(sum_x2 - pow(sum_x, 2) / n)\n                       * sqrt(sum_y2 - pow(sum_y, 2) / n))\n        if denominator == 0:\n            return 0\n        else:\n            return (sum_xy - (sum_x * sum_y) / n) / denominator\n\n\n    def computeNearestNeighbor(self, username):\n        """"""creates a sorted list of users based on their distance to\n        username""""""\n        distances = []\n        for instance in self.data:\n            if instance != username:\n                distance = self.fn(self.data[username],\n                                   self.data[instance])\n                distances.append((instance, distance))\n        # sort based on distance -- closest first\n        distances.sort(key=lambda artistTuple: artistTuple[1],\n                       reverse=True)\n        return distances\n\n    def recommend(self, user):\n       """"""Give list of recommendations""""""\n       recommendations = {}\n       # first get list of users  ordered by nearness\n       nearest = self.computeNearestNeighbor(user)\n       #\n       # now get the ratings for the user\n       #\n       userRatings = self.data[user]\n       #\n       # determine the total distance\n       totalDistance = 0.0\n       for i in range(self.k):\n          totalDistance += nearest[i][1]\n       # now iterate through the k nearest neighbors\n       # accumulating their ratings\n       for i in range(self.k):\n          # compute slice of pie \n          weight = nearest[i][1] / totalDistance\n          # get the name of the person\n          name = nearest[i][0]\n          # get the ratings for this person\n          neighborRatings = self.data[name]\n          # get the name of the person\n          # now find bands neighbor rated that user didn\'t\n          for artist in neighborRatings:\n             if not artist in userRatings:\n                if artist not in recommendations:\n                   recommendations[artist] = (neighborRatings[artist]\n                                              * weight)\n                else:\n                   recommendations[artist] = (recommendations[artist]\n                                              + neighborRatings[artist]\n                                              * weight)\n       # now make list from dictionary\n       recommendations = list(recommendations.items())\n       recommendations = [(self.convertProductID2name(k), v)\n                          for (k, v) in recommendations]\n       # finally sort and return\n       recommendations.sort(key=lambda artistTuple: artistTuple[1],\n                            reverse = True)\n       # Return the first n items\n       return recommendations[:self.n]\n\nr = recommender(users)\n# The author implementation\nr.loadBookDB(\'/Users/mona/Downloads/BX-Dump/\')\n\nratings = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Book-Ratings.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nbooks = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Books.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nusers = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Users.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\n\n\n\npivot_rating = ratings.pivot(index=\'User-ID\', columns=\'ISBN\', values=\'Book-Rating\')\n']";"['pandas', 'Python', 'monas-mbp:book mona$ sudo pip install python-dateutil\nRequirement already satisfied (use --upgrade to upgrade): python-dateutil in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python\nCleaning up...\nmonas-mbp:book mona$ python t1.py\nNo module named dateutil.parser\nTraceback (most recent call last):\n  File ""t1.py"", line 4, in <module>\n    import pandas as pd\n  File ""/Library/Python/2.7/site-packages/pandas/__init__.py"", line 6, in <module>\n    from . import hashtable, tslib, lib\n  File ""tslib.pyx"", line 31, in init pandas.tslib (pandas/tslib.c:48782)\nImportError: No module named dateutil.parser\n', 'import codecs \nfrom math import sqrt\nimport numpy as np\nimport pandas as pd\n\nusers = {""Angelica"": {""Blues Traveler"": 3.5, ""Broken Bells"": 2.0,\n                      ""Norah Jones"": 4.5, ""Phoenix"": 5.0,\n                      ""Slightly Stoopid"": 1.5,\n                      ""The Strokes"": 2.5, ""Vampire Weekend"": 2.0},\n\n         ""Bill"":{""Blues Traveler"": 2.0, ""Broken Bells"": 3.5,\n                 ""Deadmau5"": 4.0, ""Phoenix"": 2.0,\n                 ""Slightly Stoopid"": 3.5, ""Vampire Weekend"": 3.0},\n\n         ""Chan"": {""Blues Traveler"": 5.0, ""Broken Bells"": 1.0,\n                  ""Deadmau5"": 1.0, ""Norah Jones"": 3.0, ""Phoenix"": 5,\n                  ""Slightly Stoopid"": 1.0},\n\n         ""Dan"": {""Blues Traveler"": 3.0, ""Broken Bells"": 4.0,\n                 ""Deadmau5"": 4.5, ""Phoenix"": 3.0,\n                 ""Slightly Stoopid"": 4.5, ""The Strokes"": 4.0,\n                 ""Vampire Weekend"": 2.0},\n\n         ""Hailey"": {""Broken Bells"": 4.0, ""Deadmau5"": 1.0,\n                    ""Norah Jones"": 4.0, ""The Strokes"": 4.0,\n                    ""Vampire Weekend"": 1.0},\n\n         ""Jordyn"":  {""Broken Bells"": 4.5, ""Deadmau5"": 4.0,\n                     ""Norah Jones"": 5.0, ""Phoenix"": 5.0,\n                     ""Slightly Stoopid"": 4.5, ""The Strokes"": 4.0,\n                     ""Vampire Weekend"": 4.0},\n\n         ""Sam"": {""Blues Traveler"": 5.0, ""Broken Bells"": 2.0,\n                 ""Norah Jones"": 3.0, ""Phoenix"": 5.0,\n                 ""Slightly Stoopid"": 4.0, ""The Strokes"": 5.0},\n\n         ""Veronica"": {""Blues Traveler"": 3.0, ""Norah Jones"": 5.0,\n                      ""Phoenix"": 4.0, ""Slightly Stoopid"": 2.5,\n                      ""The Strokes"": 3.0}\n        }\n\n\n\nclass recommender:\n\n    def __init__(self, data, k=1, metric=\'pearson\', n=5):\n        """""" initialize recommender\n        currently, if data is dictionary the recommender is initialized\n        to it.\n        For all other data types of data, no initialization occurs\n        k is the k value for k nearest neighbor\n        metric is which distance formula to use\n        n is the maximum number of recommendations to make""""""\n        self.k = k\n        self.n = n\n        self.username2id = {}\n        self.userid2name = {}\n        self.productid2name = {}\n        # for some reason I want to save the name of the metric\n        self.metric = metric\n        if self.metric == \'pearson\':\n            self.fn = self.pearson\n        #\n        # if data is dictionary set recommender data to it\n        #\n        if type(data).__name__ == \'dict\':\n            self.data = data\n\n    def convertProductID2name(self, id):\n        """"""Given product id number return product name""""""\n        if id in self.productid2name:\n            return self.productid2name[id]\n        else:\n            return id\n\n\n    def userRatings(self, id, n):\n        """"""Return n top ratings for user with id""""""\n        print (""Ratings for "" + self.userid2name[id])\n        ratings = self.data[id]\n        print(len(ratings))\n        ratings = list(ratings.items())\n        ratings = [(self.convertProductID2name(k), v)\n                   for (k, v) in ratings]\n        # finally sort and return\n        ratings.sort(key=lambda artistTuple: artistTuple[1],\n                     reverse = True)\n        ratings = ratings[:n]\n        for rating in ratings:\n            print(""%s\\t%i"" % (rating[0], rating[1]))\n\n\n\n\n    def loadBookDB(self, path=\'\'):\n        """"""loads the BX book dataset. Path is where the BX files are\n        located""""""\n        self.data = {}\n        i = 0\n        #\n        # First load book ratings into self.data\n        #\n        f = codecs.open(path + ""BX-Book-Ratings.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #separate line into fields\n            fields = line.split(\';\')\n            user = fields[0].strip(\'""\')\n            book = fields[1].strip(\'""\')\n            rating = int(fields[2].strip().strip(\'""\'))\n            if user in self.data:\n                currentRatings = self.data[user]\n            else:\n                currentRatings = {}\n            currentRatings[book] = rating\n            self.data[user] = currentRatings\n        f.close()\n        #\n        # Now load books into self.productid2name\n        # Books contains isbn, title, and author among other fields\n        #\n        f = codecs.open(path + ""BX-Books.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #separate line into fields\n            fields = line.split(\';\')\n            isbn = fields[0].strip(\'""\')\n            title = fields[1].strip(\'""\')\n            author = fields[2].strip().strip(\'""\')\n            title = title + \' by \' + author\n            self.productid2name[isbn] = title\n        f.close()\n        #\n        #  Now load user info into both self.userid2name and\n        #  self.username2id\n        #\n        f = codecs.open(path + ""BX-Users.csv"", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #print(line)\n            #separate line into fields\n            fields = line.split(\';\')\n            userid = fields[0].strip(\'""\')\n            location = fields[1].strip(\'""\')\n            if len(fields) > 3:\n                age = fields[2].strip().strip(\'""\')\n            else:\n                age = \'NULL\'\n            if age != \'NULL\':\n                value = location + \'  (age: \' + age + \')\'\n            else:\n                value = location\n            self.userid2name[userid] = value\n            self.username2id[location] = userid\n        f.close()\n        print(i)\n\n\n    def pearson(self, rating1, rating2):\n        sum_xy = 0\n        sum_x = 0\n        sum_y = 0\n        sum_x2 = 0\n        sum_y2 = 0\n        n = 0\n        for key in rating1:\n            if key in rating2:\n                n += 1\n                x = rating1[key]\n                y = rating2[key]\n                sum_xy += x * y\n                sum_x += x\n                sum_y += y\n                sum_x2 += pow(x, 2)\n                sum_y2 += pow(y, 2)\n        if n == 0:\n            return 0\n        # now compute denominator\n        denominator = (sqrt(sum_x2 - pow(sum_x, 2) / n)\n                       * sqrt(sum_y2 - pow(sum_y, 2) / n))\n        if denominator == 0:\n            return 0\n        else:\n            return (sum_xy - (sum_x * sum_y) / n) / denominator\n\n\n    def computeNearestNeighbor(self, username):\n        """"""creates a sorted list of users based on their distance to\n        username""""""\n        distances = []\n        for instance in self.data:\n            if instance != username:\n                distance = self.fn(self.data[username],\n                                   self.data[instance])\n                distances.append((instance, distance))\n        # sort based on distance -- closest first\n        distances.sort(key=lambda artistTuple: artistTuple[1],\n                       reverse=True)\n        return distances\n\n    def recommend(self, user):\n       """"""Give list of recommendations""""""\n       recommendations = {}\n       # first get list of users  ordered by nearness\n       nearest = self.computeNearestNeighbor(user)\n       #\n       # now get the ratings for the user\n       #\n       userRatings = self.data[user]\n       #\n       # determine the total distance\n       totalDistance = 0.0\n       for i in range(self.k):\n          totalDistance += nearest[i][1]\n       # now iterate through the k nearest neighbors\n       # accumulating their ratings\n       for i in range(self.k):\n          # compute slice of pie \n          weight = nearest[i][1] / totalDistance\n          # get the name of the person\n          name = nearest[i][0]\n          # get the ratings for this person\n          neighborRatings = self.data[name]\n          # get the name of the person\n          # now find bands neighbor rated that user didn\'t\n          for artist in neighborRatings:\n             if not artist in userRatings:\n                if artist not in recommendations:\n                   recommendations[artist] = (neighborRatings[artist]\n                                              * weight)\n                else:\n                   recommendations[artist] = (recommendations[artist]\n                                              + neighborRatings[artist]\n                                              * weight)\n       # now make list from dictionary\n       recommendations = list(recommendations.items())\n       recommendations = [(self.convertProductID2name(k), v)\n                          for (k, v) in recommendations]\n       # finally sort and return\n       recommendations.sort(key=lambda artistTuple: artistTuple[1],\n                            reverse = True)\n       # Return the first n items\n       return recommendations[:self.n]\n\nr = recommender(users)\n# The author implementation\nr.loadBookDB(\'/Users/mona/Downloads/BX-Dump/\')\n\nratings = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Book-Ratings.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nbooks = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Books.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nusers = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Users.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\n\n\n\npivot_rating = ratings.pivot(index=\'User-ID\', columns=\'ISBN\', values=\'Book-Rating\')\n']";"['import codecs \nfrom math import sqrt\nimport numpy as np\nimport pandas as pd\n\n\n\n\n\n\n\n\n\n\n\n\n        # for some reason I want to save the name of the metric\n        #\n        # if data is dictionary set recommender data to it\n        #\n\n\n\n        # finally sort and return\n\n\n\n\n        #\n        # First load book ratings into self.data\n        #\n            #separate line into fields\n        #\n        # Now load books into self.productid2name\n        # Books contains isbn, title, and author among other fields\n        #\n            #separate line into fields\n        #\n        #  Now load user info into both self.userid2name and\n        #  self.username2id\n        #\n            #print(line)\n            #separate line into fields\n\n\n        # now compute denominator\n\n\n        # sort based on distance -- closest first\n\n       # first get list of users  ordered by nearness\n       #\n       # now get the ratings for the user\n       #\n       #\n       # determine the total distance\n       # now iterate through the k nearest neighbors\n       # accumulating their ratings\n          # compute slice of pie \n          # get the name of the person\n          # get the ratings for this person\n          # get the name of the person\n          # now find bands neighbor rated that user didn\'t\n       # now make list from dictionary\n       # finally sort and return\n       # Return the first n items\n\nr = recommender(users)\n# The author implementation\nr.loadBookDB(\'/Users/mona/Downloads/BX-Dump/\')\n\nratings = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Book-Ratings.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nbooks = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Books.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nusers = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Users.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\n\n\n\npivot_rating = ratings.pivot(index=\'User-ID\', columns=\'ISBN\', values=\'Book-Rating\')\n']";"['import codecs \nfrom math import sqrt\nimport numpy as np\nimport pandas as pd\n\n\n\n\n\n\n\n\n\n\n\n\n        # for some reason I want to save the name of the metric\n        #\n        # if data is dictionary set recommender data to it\n        #\n\n\n\n        # finally sort and return\n\n\n\n\n        #\n        # First load book ratings into self.data\n        #\n            #separate line into fields\n        #\n        # Now load books into self.productid2name\n        # Books contains isbn, title, and author among other fields\n        #\n            #separate line into fields\n        #\n        #  Now load user info into both self.userid2name and\n        #  self.username2id\n        #\n            #print(line)\n            #separate line into fields\n\n\n        # now compute denominator\n\n\n        # sort based on distance -- closest first\n\n       # first get list of users  ordered by nearness\n       #\n       # now get the ratings for the user\n       #\n       #\n       # determine the total distance\n       # now iterate through the k nearest neighbors\n       # accumulating their ratings\n          # compute slice of pie \n          # get the name of the person\n          # get the ratings for this person\n          # get the name of the person\n          # now find bands neighbor rated that user didn\'t\n       # now make list from dictionary\n       # finally sort and return\n       # Return the first n items\n\nr = recommender(users)\n# The author implementation\nr.loadBookDB(\'/Users/mona/Downloads/BX-Dump/\')\n\nratings = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Book-Ratings.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nbooks = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Books.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nusers = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Users.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\n\n\n\npivot_rating = ratings.pivot(index=\'User-ID\', columns=\'ISBN\', values=\'Book-Rating\')\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\nimport codecs \nfrom math import sqrt\nimport numpy as np\nimport pandas as pd\n\n\n\n\n\n\n\n\n\n\n\n\n        # for some reason I want to save the name of the metric\n        #\n        # if data is dictionary set recommender data to it\n        #\n\n\n\n        # finally sort and return\n\n\n\n\n        #\n        # First load book ratings into self.data\n        #\n            #separate line into fields\n        #\n        # Now load books into self.productid2name\n        # Books contains isbn, title, and author among other fields\n        #\n            #separate line into fields\n        #\n        #  Now load user info into both self.userid2name and\n        #  self.username2id\n        #\n            #print(line)\n            #separate line into fields\n\n\n        # now compute denominator\n\n\n        # sort based on distance -- closest first\n\n       # first get list of users  ordered by nearness\n       #\n       # now get the ratings for the user\n       #\n       #\n       # determine the total distance\n       # now iterate through the k nearest neighbors\n       # accumulating their ratings\n          # compute slice of pie \n          # get the name of the person\n          # get the ratings for this person\n          # get the name of the person\n          # now find bands neighbor rated that user didn\'t\n       # now make list from dictionary\n       # finally sort and return\n       # Return the first n items\n\nr = recommender(users)\n# The author implementation\nr.loadBookDB(\'/Users/mona/Downloads/BX-Dump/\')\n\nratings = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Book-Ratings.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nbooks = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Books.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\nusers = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Users.csv\', sep="";"", quotechar=""\\"""", escapechar=""\\\\"")\n\n\n\npivot_rating = ratings.pivot(index=\'User-ID\', columns=\'ISBN\', values=\'Book-Rating\')\n']";True;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
633;633;633;633;3.0;0;20868394;;1;99;<python><pandas>;Changing a specific column name in pandas DataFrame;115075.0;"[""import pandas as pd\nd = {\n         'one': [1, 2, 3, 4, 5],\n         'two': [9, 8, 7, 6, 5],\n         'three': ['a', 'b', 'c', 'd', 'e']\n    }\ndf = pd.DataFrame(d)\nnames = df.columns.tolist()\nnames[names.index('two')] = 'new_name'\ndf.columns = names\ndf.columns[df.columns.tolist().index('one')] = 'another_name'\n""]";"[""import pandas as pd\nd = {\n         'one': [1, 2, 3, 4, 5],\n         'two': [9, 8, 7, 6, 5],\n         'three': ['a', 'b', 'c', 'd', 'e']\n    }\ndf = pd.DataFrame(d)\n"", ""names = df.columns.tolist()\nnames[names.index('two')] = 'new_name'\ndf.columns = names\n"", ""df.columns[df.columns.tolist().index('one')] = 'another_name'\n""]";"['DataFrame', ""import pandas as pd\nd = {\n         'one': [1, 2, 3, 4, 5],\n         'two': [9, 8, 7, 6, 5],\n         'three': ['a', 'b', 'c', 'd', 'e']\n    }\ndf = pd.DataFrame(d)\n"", ""names = df.columns.tolist()\nnames[names.index('two')] = 'new_name'\ndf.columns = names\n"", ""df.columns[df.columns.tolist().index('one')] = 'another_name'\n""]";"[""import pandas as pd\ndf = pd.DataFrame(d)\nnames = df.columns.tolist()\nnames[names.index('two')] = 'new_name'\ndf.columns = names\ndf.columns[df.columns.tolist().index('one')] = 'another_name'\n""]";"[""import pandas as pd\ndf = pd.DataFrame(d)\nnames = df.columns.tolist()\nnames[names.index('two')] = 'new_name'\ndf.columns = names\ndf.columns[df.columns.tolist().index('one')] = 'another_name'\n""]";False;"[""import pandas as pd\nimport pandas as pd\ndf = pd.DataFrame(d)\nnames = df.columns.tolist()\nnames[names.index('two')] = 'new_name'\ndf.columns = names\ndf.columns[df.columns.tolist().index('one')] = 'another_name'\n""]";False;0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];2;3;['Sucess', 'Sucess', 'index 2 is out of bounds for axis 0 with size 0'];['Sucess', 'Sucess', 'IndexError']
634;634;634;634;3.0;3;20868664;;1;13;<pandas><scikit-learn>;Should a pandas dataframe column be converted in some way before passing it to a scikit learn regressor?;8083.0;['DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().   probas = cfr.fit(trainset_X, trainset_Y).predict(testset_X)\n'];['DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().   probas = cfr.fit(trainset_X, trainset_Y).predict(testset_X)\n'];['df[list_of_columns]', 'df[[single_column]]', 'Y', 'DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().   probas = cfr.fit(trainset_X, trainset_Y).predict(testset_X)\n'];[''];[''];False;['import pandas as pd\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
635;635;635;635;2.0;0;20888954;;1;11;<python><pandas>;Add subtotal columns in pandas with multi-index;2813.0;"[""import numpy as np\nimport pandas as pd\nfrom itertools import product\n\nnp.random.seed(0)\n\ncolors = ['red', 'green']\nshapes = ['square', 'circle']\nobsnum = range(5)\n\nrows = list(product(colors, shapes, obsnum))\nidx = pd.MultiIndex.from_tuples(rows)\nidx.names = ['color', 'shape', 'obsnum']\n\ndf = pd.DataFrame({'attr1': np.random.randn(len(rows)), \n                   'attr2': 100 * np.random.randn(len(rows))},\n                  index=idx)\n\ndf.columns.names = ['attribute']\n\ndf = df.unstack(['color', 'shape'])\ntots = df.sum(axis=1, level=['attribute', 'color'])\ntots = df.sum(axis=1, level=['attribute', 'color'])\nnewcols = pd.MultiIndex.from_tuples(list((i[0], i[1], 'sum(shape)') for i in tots.columns))\ntots.columns = newcols\nbigframe = pd.concat([df, tots], axis=1).sort_index(axis=1)\n""]";"[""import numpy as np\nimport pandas as pd\nfrom itertools import product\n\nnp.random.seed(0)\n\ncolors = ['red', 'green']\nshapes = ['square', 'circle']\nobsnum = range(5)\n\nrows = list(product(colors, shapes, obsnum))\nidx = pd.MultiIndex.from_tuples(rows)\nidx.names = ['color', 'shape', 'obsnum']\n\ndf = pd.DataFrame({'attr1': np.random.randn(len(rows)), \n                   'attr2': 100 * np.random.randn(len(rows))},\n                  index=idx)\n\ndf.columns.names = ['attribute']\n\ndf = df.unstack(['color', 'shape'])\n"", ""tots = df.sum(axis=1, level=['attribute', 'color'])\n"", ""tots = df.sum(axis=1, level=['attribute', 'color'])\nnewcols = pd.MultiIndex.from_tuples(list((i[0], i[1], 'sum(shape)') for i in tots.columns))\ntots.columns = newcols\nbigframe = pd.concat([df, tots], axis=1).sort_index(axis=1)\n""]";"['sum(axis=1)', 'level', 'pd.DataFrame.sum', ""import numpy as np\nimport pandas as pd\nfrom itertools import product\n\nnp.random.seed(0)\n\ncolors = ['red', 'green']\nshapes = ['square', 'circle']\nobsnum = range(5)\n\nrows = list(product(colors, shapes, obsnum))\nidx = pd.MultiIndex.from_tuples(rows)\nidx.names = ['color', 'shape', 'obsnum']\n\ndf = pd.DataFrame({'attr1': np.random.randn(len(rows)), \n                   'attr2': 100 * np.random.randn(len(rows))},\n                  index=idx)\n\ndf.columns.names = ['attribute']\n\ndf = df.unstack(['color', 'shape'])\n"", 'shape', ""tots = df.sum(axis=1, level=['attribute', 'color'])\n"", ""tots = df.sum(axis=1, level=['attribute', 'color'])\nnewcols = pd.MultiIndex.from_tuples(list((i[0], i[1], 'sum(shape)') for i in tots.columns))\ntots.columns = newcols\nbigframe = pd.concat([df, tots], axis=1).sort_index(axis=1)\n""]";"[""import numpy as np\nimport pandas as pd\nfrom itertools import product\n\nnp.random.seed(0)\n\ncolors = ['red', 'green']\nshapes = ['square', 'circle']\nobsnum = range(5)\n\nrows = list(product(colors, shapes, obsnum))\nidx = pd.MultiIndex.from_tuples(rows)\nidx.names = ['color', 'shape', 'obsnum']\n\n\ndf.columns.names = ['attribute']\n\ndf = df.unstack(['color', 'shape'])\ntots = df.sum(axis=1, level=['attribute', 'color'])\ntots = df.sum(axis=1, level=['attribute', 'color'])\nnewcols = pd.MultiIndex.from_tuples(list((i[0], i[1], 'sum(shape)') for i in tots.columns))\ntots.columns = newcols\nbigframe = pd.concat([df, tots], axis=1).sort_index(axis=1)\n""]";"[""import numpy as np\nimport pandas as pd\nfrom itertools import product\n\nnp.random.seed(0)\n\ncolors = ['red', 'green']\nshapes = ['square', 'circle']\nobsnum = range(5)\n\nrows = list(product(colors, shapes, obsnum))\nidx = pd.MultiIndex.from_tuples(rows)\nidx.names = ['color', 'shape', 'obsnum']\n\n\ndf.columns.names = ['attribute']\n\ndf = df.unstack(['color', 'shape'])\ntots = df.sum(axis=1, level=['attribute', 'color'])\ntots = df.sum(axis=1, level=['attribute', 'color'])\nnewcols = pd.MultiIndex.from_tuples(list((i[0], i[1], 'sum(shape)') for i in tots.columns))\ntots.columns = newcols\nbigframe = pd.concat([df, tots], axis=1).sort_index(axis=1)\n""]";False;"[""import pandas as pd\nimport numpy as np\nimport pandas as pd\nfrom itertools import product\n\nnp.random.seed(0)\n\ncolors = ['red', 'green']\nshapes = ['square', 'circle']\nobsnum = range(5)\n\nrows = list(product(colors, shapes, obsnum))\nidx = pd.MultiIndex.from_tuples(rows)\nidx.names = ['color', 'shape', 'obsnum']\n\n\ndf.columns.names = ['attribute']\n\ndf = df.unstack(['color', 'shape'])\ntots = df.sum(axis=1, level=['attribute', 'color'])\ntots = df.sum(axis=1, level=['attribute', 'color'])\nnewcols = pd.MultiIndex.from_tuples(list((i[0], i[1], 'sum(shape)') for i in tots.columns))\ntots.columns = newcols\nbigframe = pd.concat([df, tots], axis=1).sort_index(axis=1)\n""]";False;0;0;[];[];0;0;[];[];0;0;[];[]
636;636;636;636;6.0;14;20906474;;1;104;<python><csv><pandas><concatenation>;Import multiple csv files into pandas and concatenate into one DataFrame;80289.0;"['import glob\nimport pandas as pd\n\n# get data file names\npath =r\'C:\\DRO\\DCL_rawdata_files\'\nfilenames = glob.glob(path + ""/*.csv"")\n\ndfs = []\nfor filename in filenames:\n    dfs.append(pd.read_csv(filename))\n\n# Concatenate all data into one DataFrame\nbig_frame = pd.concat(dfs, ignore_index=True)\n']";"['import glob\nimport pandas as pd\n\n# get data file names\npath =r\'C:\\DRO\\DCL_rawdata_files\'\nfilenames = glob.glob(path + ""/*.csv"")\n\ndfs = []\nfor filename in filenames:\n    dfs.append(pd.read_csv(filename))\n\n# Concatenate all data into one DataFrame\nbig_frame = pd.concat(dfs, ignore_index=True)\n']";"['import glob\nimport pandas as pd\n\n# get data file names\npath =r\'C:\\DRO\\DCL_rawdata_files\'\nfilenames = glob.glob(path + ""/*.csv"")\n\ndfs = []\nfor filename in filenames:\n    dfs.append(pd.read_csv(filename))\n\n# Concatenate all data into one DataFrame\nbig_frame = pd.concat(dfs, ignore_index=True)\n']";"['import glob\nimport pandas as pd\n\n# get data file names\npath =r\'C:\\DRO\\DCL_rawdata_files\'\nfilenames = glob.glob(path + ""/*.csv"")\n\ndfs = []\n\n# Concatenate all data into one DataFrame\nbig_frame = pd.concat(dfs, ignore_index=True)\n']";"['from pandas import DataFrame\nimport glob\nimport pandas as pd\n\n# get data file names\npath =r\'C:\\DRO\\DCL_rawdata_files\'\nfilenames = glob.glob(path + ""/*.csv"")\n\ndfs = []\n\n# Concatenate all data into one DataFrame\nbig_frame = pd.concat(dfs, ignore_index=True)\n']";True;"['import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\nimport glob\nimport pandas as pd\n\n# get data file names\npath =r\'C:\\DRO\\DCL_rawdata_files\'\nfilenames = glob.glob(path + ""/*.csv"")\n\ndfs = []\n\n# Concatenate all data into one DataFrame\nbig_frame = pd.concat(dfs, ignore_index=True)\n']";True;0;2;"[""name 'glob' is not defined"", ""name 'glob' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'glob' is not defined"", ""name 'glob' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'glob' is not defined"", ""name 'glob' is not defined""]";['NameError', 'NameError']
637;637;637;637;3.0;2;20937538;;1;60;<python><python-2.7><pandas><ipython><dataframe>;How to display pandas DataFrame of floats using a format string for columns?;56902.0;"[""df = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\nprint df\n\n         cost\nfoo   123.4567\nbar   234.5678\nbaz   345.6789\nquux  456.7890\n         cost\nfoo   $123.46\nbar   $234.57\nbaz   $345.68\nquux  $456.79\n""]";"[""df = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\nprint df\n\n         cost\nfoo   123.4567\nbar   234.5678\nbaz   345.6789\nquux  456.7890\n"", '         cost\nfoo   $123.46\nbar   $234.57\nbaz   $345.68\nquux  $456.79\n']";"['print()', 'display()', ""df = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\nprint df\n\n         cost\nfoo   123.4567\nbar   234.5678\nbaz   345.6789\nquux  456.7890\n"", '         cost\nfoo   $123.46\nbar   $234.57\nbaz   $345.68\nquux  $456.79\n']";['\n'];['\n'];False;['import pandas as pd\n\n'];False;1;3;"[""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""'cost'"", 'Sucess', ""name 'df' is not defined""]";['KeyError', 'Sucess', 'NameError']
638;638;638;638;5.0;5;20940805;;1;17;<numpy><pandas><pytables><h5py><blaze>;Python particles simulator: out-of-core processing;1262.0;"['counts = np.random.poisson(lam=emission).astype(np.uint8)\n# Loop across the particles\ntimestamps = [np.nonzero(c) for c in counts]\n    dt = np.dtype([(\'counts\', \'u1\')])        \n    for ip in xrange(n_particles):\n        name = ""particle_%d"" % ip\n        data_file.create_table(\n                    group, name, description=dt, chunkshape=chunksize,\n                    expectedrows=time_size,\n                    title=\'Binned timetrace of emitted ph (bin = t_step)\'\n                        \' - particle_%d\' % particle)\n']";"['counts = np.random.poisson(lam=emission).astype(np.uint8)\n', '# Loop across the particles\ntimestamps = [np.nonzero(c) for c in counts]\n', '    dt = np.dtype([(\'counts\', \'u1\')])        \n    for ip in xrange(n_particles):\n        name = ""particle_%d"" % ip\n        data_file.create_table(\n                    group, name, description=dt, chunkshape=chunksize,\n                    expectedrows=time_size,\n                    title=\'Binned timetrace of emitted ph (bin = t_step)\'\n                        \' - particle_%d\' % particle)\n']";"['n_particles', 'time_size', 'emission', 'n_particles', 'time_size', 'counts', 'n_particles', 'time_size', 'counts = np.random.poisson(lam=emission).astype(np.uint8)\n', '# Loop across the particles\ntimestamps = [np.nonzero(c) for c in counts]\n', 'emission', 'cumsum', 'counts', 'max', 'cumsum', 'chunksize', ""'/parameters'"", 'emission', 'EArray', 'counts', "".get_where_list('counts >= 2')"", '    dt = np.dtype([(\'counts\', \'u1\')])        \n    for ip in xrange(n_particles):\n        name = ""particle_%d"" % ip\n        data_file.create_table(\n                    group, name, description=dt, chunkshape=chunksize,\n                    expectedrows=time_size,\n                    title=\'Binned timetrace of emitted ph (bin = t_step)\'\n                        \' - particle_%d\' % particle)\n', 'name = ""particle_%d"" % ip']";['counts = np.random.poisson(lam=emission).astype(np.uint8)\n# Loop across the particles\ntimestamps = [np.nonzero(c) for c in counts]\n'];['counts = np.random.poisson(lam=emission).astype(np.uint8)\n# Loop across the particles\ntimestamps = [np.nonzero(c) for c in counts]\n'];False;['import pandas as pd\ncounts = np.random.poisson(lam=emission).astype(np.uint8)\n# Loop across the particles\ntimestamps = [np.nonzero(c) for c in counts]\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
639;639;639;639;1.0;0;20965046;;1;23;<python><pandas><dataframe><cumulative-sum>;Cumulative sum and percentage on column?;27193.0;[' fruit    val1 val2\n0 orange    15    3\n1 apple     10   13\n2 mango     5    5 \n fruit    val1 val2   cum_sum    cum_perc\n0 orange    15    3    15          50.00\n1 apple     10   13    25          83.33\n2 mango     5    5     30          100.00\n'];[' fruit    val1 val2\n0 orange    15    3\n1 apple     10   13\n2 mango     5    5 \n', ' fruit    val1 val2   cum_sum    cum_perc\n0 orange    15    3    15          50.00\n1 apple     10   13    25          83.33\n2 mango     5    5     30          100.00\n'];['DataFrame', 'df', ' fruit    val1 val2\n0 orange    15    3\n1 apple     10   13\n2 mango     5    5 \n', 'val1', 'df_with_cumsum', ' fruit    val1 val2   cum_sum    cum_perc\n0 orange    15    3    15          50.00\n1 apple     10   13    25          83.33\n2 mango     5    5     30          100.00\n', 'df.cumsum()'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'DataFrame' object has no attribute 'val1'""]";['AttributeError']
640;640;640;640;1.0;0;20970279;;1;24;<python><pandas>;how to do a left,right and mid of a string in a pandas dataframe;23529.0;"[""data = {'state': ['Auckland', 'Otago', 'Wellington', 'Dunedin', 'Hamilton'],\n'year': [2000, 2001, 2002, 2001, 2002],\n'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\ndf = pd.DataFrame(data)\n\nprint df\n\n     pop       state  year\n 0  1.5    Auckland  2000\n 1  1.7       Otago  2001\n 2  3.6  Wellington  2002\n 3  2.4     Dunedin  2001\n 4  2.9    Hamilton  2002\n    pop       state     year  StateInitial\n 0  1.5       Auckland    2000     Au\n 1  1.7       Otago       2001     Ot\n 2  3.6       Wellington  2002     We\n 3  2.4       Dunedin     2001     Du\n 4  2.9       Hamilton    2002     Ha\n""]";"[""data = {'state': ['Auckland', 'Otago', 'Wellington', 'Dunedin', 'Hamilton'],\n'year': [2000, 2001, 2002, 2001, 2002],\n'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\ndf = pd.DataFrame(data)\n\nprint df\n\n     pop       state  year\n 0  1.5    Auckland  2000\n 1  1.7       Otago  2001\n 2  3.6  Wellington  2002\n 3  2.4     Dunedin  2001\n 4  2.9    Hamilton  2002\n"", '    pop       state     year  StateInitial\n 0  1.5       Auckland    2000     Au\n 1  1.7       Otago       2001     Ot\n 2  3.6       Wellington  2002     We\n 3  2.4       Dunedin     2001     Du\n 4  2.9       Hamilton    2002     Ha\n']";"[""data = {'state': ['Auckland', 'Otago', 'Wellington', 'Dunedin', 'Hamilton'],\n'year': [2000, 2001, 2002, 2001, 2002],\n'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\ndf = pd.DataFrame(data)\n\nprint df\n\n     pop       state  year\n 0  1.5    Auckland  2000\n 1  1.7       Otago  2001\n 2  3.6  Wellington  2002\n 3  2.4     Dunedin  2001\n 4  2.9    Hamilton  2002\n"", '    pop       state     year  StateInitial\n 0  1.5       Auckland    2000     Au\n 1  1.7       Otago       2001     Ot\n 2  3.6       Wellington  2002     We\n 3  2.4       Dunedin     2001     Du\n 4  2.9       Hamilton    2002     Ha\n']";['df = pd.DataFrame(data)\n\n\n'];['import pandas as pd\ndf = pd.DataFrame(data)\n\n\n'];True;['import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame(data)\n\n\n'];True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
641;641;641;641;2.0;0;20995196;;1;23;<python><pandas><sum>;Python Pandas counting and summing specific conditions;38942.0;[''];[];['sumif', '(df.map(lambda x: condition), or df.size())', '.sum()', 'countif', '(groupby functions', '.count())'];[''];[''];False;['import pandas as pd\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
642;642;642;642;1.0;0;21018654;;1;51;<python><types><pandas>;Strings in a DataFrame, but dtype is object;21985.0;"['<class \'pandas.core.frame.DataFrame\'>\nInt64Index: 56992 entries, 0 to 56991\nData columns (total 7 columns):\nid            56992  non-null values\nattr1         56992  non-null values\nattr2         56992  non-null values\nattr3         56992  non-null values\nattr4         56992  non-null values\nattr5         56992  non-null values\nattr6         56992  non-null values\ndtypes: int64(2), object(5)\nfor c in df.columns:\n    if df[c].dtype == object:\n        print ""convert "", df[c].name, "" to string""\n        df[c] = df[c].astype(str)\n']";"[""<class 'pandas.core.frame.DataFrame'>\nInt64Index: 56992 entries, 0 to 56991\nData columns (total 7 columns):\nid            56992  non-null values\nattr1         56992  non-null values\nattr2         56992  non-null values\nattr3         56992  non-null values\nattr4         56992  non-null values\nattr5         56992  non-null values\nattr6         56992  non-null values\ndtypes: int64(2), object(5)\n"", 'for c in df.columns:\n    if df[c].dtype == object:\n        print ""convert "", df[c].name, "" to string""\n        df[c] = df[c].astype(str)\n']";"[""<class 'pandas.core.frame.DataFrame'>\nInt64Index: 56992 entries, 0 to 56991\nData columns (total 7 columns):\nid            56992  non-null values\nattr1         56992  non-null values\nattr2         56992  non-null values\nattr3         56992  non-null values\nattr4         56992  non-null values\nattr5         56992  non-null values\nattr6         56992  non-null values\ndtypes: int64(2), object(5)\n"", 'dtype object', 'for c in df.columns:\n    if df[c].dtype == object:\n        print ""convert "", df[c].name, "" to string""\n        df[c] = df[c].astype(str)\n', 'df[""attr2""]', 'dtype object', 'type(df[""attr2""].ix[0]', 'str', 'int64', 'float64', 'object', 'dtype str', 'str', 'object']";[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
643;643;643;643;3.0;0;21022865;;1;11;<python><pandas><multiplication><dataframe>;Pandas: Elementwise multiplication of two dataframes;17543.0;"[""df = pd.DataFrame({'col1' : [1.0] * 5, \n                   'col2' : [2.0] * 5, \n                   'col3' : [3.0] * 5 }, index = range(1,6),)\ndf2 = pd.DataFrame({'col1' : [10.0] * 5, \n                    'col2' : [100.0] * 5, \n                    'col3' : [1000.0] * 5 }, index = range(1,6),)\ndf3 = pd.DataFrame({'col1' : [0.1] * 5}, index = range(1,6),)\n\ndf.mul(df2, 1) # element by element multiplication no problems\n\ndf.mul(df3, 1) # df(row*col) is not equal to df3(row*col)\n   col1  col2  col3\n1   0.1   NaN   NaN\n2   0.1   NaN   NaN\n3   0.1   NaN   NaN\n4   0.1   NaN   NaN\n5   0.1   NaN   NaN\ndf3 = pd.DataFrame([df3.col1 for n in range(len(df.columns.values)) ])\ndf3\n        1    2    3    4    5\ncol1  0.1  0.1  0.1  0.1  0.1\ncol1  0.1  0.1  0.1  0.1  0.1\ncol1  0.1  0.1  0.1  0.1  0.1\n""]";"[""df = pd.DataFrame({'col1' : [1.0] * 5, \n                   'col2' : [2.0] * 5, \n                   'col3' : [3.0] * 5 }, index = range(1,6),)\ndf2 = pd.DataFrame({'col1' : [10.0] * 5, \n                    'col2' : [100.0] * 5, \n                    'col3' : [1000.0] * 5 }, index = range(1,6),)\ndf3 = pd.DataFrame({'col1' : [0.1] * 5}, index = range(1,6),)\n\ndf.mul(df2, 1) # element by element multiplication no problems\n\ndf.mul(df3, 1) # df(row*col) is not equal to df3(row*col)\n   col1  col2  col3\n1   0.1   NaN   NaN\n2   0.1   NaN   NaN\n3   0.1   NaN   NaN\n4   0.1   NaN   NaN\n5   0.1   NaN   NaN\n"", 'df3 = pd.DataFrame([df3.col1 for n in range(len(df.columns.values)) ])\ndf3\n        1    2    3    4    5\ncol1  0.1  0.1  0.1  0.1  0.1\ncol1  0.1  0.1  0.1  0.1  0.1\ncol1  0.1  0.1  0.1  0.1  0.1\n']";"['df * df2', 'df * df3', ""df = pd.DataFrame({'col1' : [1.0] * 5, \n                   'col2' : [2.0] * 5, \n                   'col3' : [3.0] * 5 }, index = range(1,6),)\ndf2 = pd.DataFrame({'col1' : [10.0] * 5, \n                    'col2' : [100.0] * 5, \n                    'col3' : [1000.0] * 5 }, index = range(1,6),)\ndf3 = pd.DataFrame({'col1' : [0.1] * 5}, index = range(1,6),)\n\ndf.mul(df2, 1) # element by element multiplication no problems\n\ndf.mul(df3, 1) # df(row*col) is not equal to df3(row*col)\n   col1  col2  col3\n1   0.1   NaN   NaN\n2   0.1   NaN   NaN\n3   0.1   NaN   NaN\n4   0.1   NaN   NaN\n5   0.1   NaN   NaN\n"", 'df3.col1', 'len(df.columns.values)', 'df', 'df3 = pd.DataFrame([df3.col1 for n in range(len(df.columns.values)) ])\ndf3\n        1    2    3    4    5\ncol1  0.1  0.1  0.1  0.1  0.1\ncol1  0.1  0.1  0.1  0.1  0.1\ncol1  0.1  0.1  0.1  0.1  0.1\n', 'df3.T()']";"[""df3 = pd.DataFrame({'col1' : [0.1] * 5}, index = range(1,6),)\n\ndf.mul(df2, 1) # element by element multiplication no problems\n\ndf.mul(df3, 1) # df(row*col) is not equal to df3(row*col)\ndf3 = pd.DataFrame([df3.col1 for n in range(len(df.columns.values)) ])\ndf3\n""]";"[""import pandas as pd\ndf3 = pd.DataFrame({'col1' : [0.1] * 5}, index = range(1,6),)\n\ndf.mul(df2, 1) # element by element multiplication no problems\n\ndf.mul(df3, 1) # df(row*col) is not equal to df3(row*col)\ndf3 = pd.DataFrame([df3.col1 for n in range(len(df.columns.values)) ])\ndf3\n""]";True;"[""import pandas as pd\ndf2 = pd.DataFrame()\ndf = pd.DataFrame()\ndf3 = pd.DataFrame({'col1' : [0.1] * 5}, index = range(1,6),)\n\ndf.mul(df2, 1) # element by element multiplication no problems\n\ndf.mul(df3, 1) # df(row*col) is not equal to df3(row*col)\ndf3 = pd.DataFrame([df3.col1 for n in range(len(df.columns.values)) ])\ndf3\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
644;644;644;644;3.0;0;21033720;;1;20;<python><pandas>;Python Pandas Histogram Log Scale;11275.0;"[""fig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111)\nplt.plot(np.random.rand(100))\nax.set_yscale('log')\nplt.show()\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111)\nresults.val1.hist(bins=120)\nax.set_yscale('log')\nplt.show()\nJan  9 15:53:07 BLARG.local python[6917] <Error>: CGContextClosePath: no current point.\n""]";"[""fig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111)\nplt.plot(np.random.rand(100))\nax.set_yscale('log')\nplt.show()\n"", ""fig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111)\nresults.val1.hist(bins=120)\nax.set_yscale('log')\nplt.show()\n"", 'Jan  9 15:53:07 BLARG.local python[6917] <Error>: CGContextClosePath: no current point.\n']";"['results.val1.hist(bins=120)', ""fig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111)\nplt.plot(np.random.rand(100))\nax.set_yscale('log')\nplt.show()\n"", 'plt', ""fig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111)\nresults.val1.hist(bins=120)\nax.set_yscale('log')\nplt.show()\n"", 'Jan  9 15:53:07 BLARG.local python[6917] <Error>: CGContextClosePath: no current point.\n', 'bottom=0.1', 'hist']";"[""fig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111)\nplt.plot(np.random.rand(100))\nax.set_yscale('log')\nplt.show()\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111)\nresults.val1.hist(bins=120)\nax.set_yscale('log')\nplt.show()\n""]";"[""fig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111)\nplt.plot(np.random.rand(100))\nax.set_yscale('log')\nplt.show()\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111)\nresults.val1.hist(bins=120)\nax.set_yscale('log')\nplt.show()\n""]";False;"[""import pandas as pd\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111)\nplt.plot(np.random.rand(100))\nax.set_yscale('log')\nplt.show()\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111)\nresults.val1.hist(bins=120)\nax.set_yscale('log')\nplt.show()\n""]";False;0;3;"[""No module named 'matplotlib'"", ""No module named 'matplotlib'"", ""name 'results' is not defined""]";['ImportError', 'ImportError', 'NameError'];0;3;"[""No module named 'matplotlib'"", ""No module named 'matplotlib'"", ""name 'results' is not defined""]";['ImportError', 'ImportError', 'NameError'];0;3;"[""No module named 'matplotlib'"", ""No module named 'matplotlib'"", ""name 'results' is not defined""]";['ImportError', 'ImportError', 'NameError']
645;645;645;645;2.0;0;21055068;;1;20;<python><string><python-2.7><csv><pandas>;Reversal of string.contains In python, pandas;3980.0;[''];[];"['df2 = df[df[\'A\'].str.contains(""Hello|World"")]']";[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""'A'"", 'Sucess']";['KeyError', 'Sucess']
646;646;646;646;4.0;0;21058333;;1;15;<python><algorithm><numpy><pandas>;Compute *rolling* maximum drawdown of pandas Series;10064.0;"[""import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef max_dd(ser):\n    max2here = pd.expanding_max(ser)\n    dd2here = ser - max2here\n    return dd2here.min()\nnp.random.seed(0)\nn = 100\ns = pd.Series(np.random.randn(n).cumsum())\ns.plot()\nplt.show()\nrolling_dd = pd.rolling_apply(s, 10, max_dd, min_periods=0)\ndf = pd.concat([s, rolling_dd], axis=1)\ndf.columns = ['s', 'rol_dd_10']\ndf.plot()\n""]";"['import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef max_dd(ser):\n    max2here = pd.expanding_max(ser)\n    dd2here = ser - max2here\n    return dd2here.min()\n', 'np.random.seed(0)\nn = 100\ns = pd.Series(np.random.randn(n).cumsum())\ns.plot()\nplt.show()\n', ""rolling_dd = pd.rolling_apply(s, 10, max_dd, min_periods=0)\ndf = pd.concat([s, rolling_dd], axis=1)\ndf.columns = ['s', 'rol_dd_10']\ndf.plot()\n""]";"['O(n)', 'O(n^2)', 'import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef max_dd(ser):\n    max2here = pd.expanding_max(ser)\n    dd2here = ser - max2here\n    return dd2here.min()\n', 'np.random.seed(0)\nn = 100\ns = pd.Series(np.random.randn(n).cumsum())\ns.plot()\nplt.show()\n', 'max_dd(s)', 'pd.rolling_apply', ""rolling_dd = pd.rolling_apply(s, 10, max_dd, min_periods=0)\ndf = pd.concat([s, rolling_dd], axis=1)\ndf.columns = ['s', 'rol_dd_10']\ndf.plot()\n"", 'rolling_dd_custom']";"[""import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\nn = 100\ns = pd.Series(np.random.randn(n).cumsum())\ns.plot()\nplt.show()\nrolling_dd = pd.rolling_apply(s, 10, max_dd, min_periods=0)\ndf = pd.concat([s, rolling_dd], axis=1)\ndf.columns = ['s', 'rol_dd_10']\ndf.plot()\n""]";"[""import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\nn = 100\ns = pd.Series(np.random.randn(n).cumsum())\ns.plot()\nplt.show()\nrolling_dd = pd.rolling_apply(s, 10, max_dd, min_periods=0)\ndf = pd.concat([s, rolling_dd], axis=1)\ndf.columns = ['s', 'rol_dd_10']\ndf.plot()\n""]";False;"[""import pandas as pd\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\nn = 100\ns = pd.Series(np.random.randn(n).cumsum())\ns.plot()\nplt.show()\nrolling_dd = pd.rolling_apply(s, 10, max_dd, min_periods=0)\ndf = pd.concat([s, rolling_dd], axis=1)\ndf.columns = ['s', 'rol_dd_10']\ndf.plot()\n""]";False;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
647;647;647;647;4.0;0;21104592;;1;44;<python><json><google-maps><pandas>;JSON to pandas DataFrame;64504.0;"['from urllib2 import Request, urlopen\nimport json\n\npath1 = \'42.974049,-81.205203|42.974298,-81.195755\'\nrequest=Request(\'http://maps.googleapis.com/maps/api/elevation/json?locations=\'+path1+\'&sensor=false\')\nresponse = urlopen(request)\nelevations = response.read()\nelevations.splitlines()\n\n[\'{\',\n \'   ""results"" : [\',\n \'      {\',\n \'         ""elevation"" : 243.3462677001953,\',\n \'         ""location"" : {\',\n \'            ""lat"" : 42.974049,\',\n \'            ""lng"" : -81.205203\',\n \'         },\',\n \'         ""resolution"" : 19.08790397644043\',\n \'      },\',\n \'      {\',\n \'         ""elevation"" : 244.1318664550781,\',\n \'         ""location"" : {\',\n \'            ""lat"" : 42.974298,\',\n \'            ""lng"" : -81.19575500000001\',\n \'         },\',\n \'         ""resolution"" : 19.08790397644043\',\n \'      }\',\n \'   ],\',\n \'   ""status"" : ""OK""\',\n \'}\']\npd.read_json(elevations)\ndata = json.loads(elevations)\nlat,lng,el = [],[],[]\nfor result in data[\'results\']:\n    lat.append(result[u\'location\'][u\'lat\'])\n    lng.append(result[u\'location\'][u\'lng\'])\n    el.append(result[u\'elevation\'])\ndf = pd.DataFrame([lat,lng,el]).T\n']";"[""from urllib2 import Request, urlopen\nimport json\n\npath1 = '42.974049,-81.205203|42.974298,-81.195755'\nrequest=Request('http://maps.googleapis.com/maps/api/elevation/json?locations='+path1+'&sensor=false')\nresponse = urlopen(request)\nelevations = response.read()\n"", 'elevations.splitlines()\n\n[\'{\',\n \'   ""results"" : [\',\n \'      {\',\n \'         ""elevation"" : 243.3462677001953,\',\n \'         ""location"" : {\',\n \'            ""lat"" : 42.974049,\',\n \'            ""lng"" : -81.205203\',\n \'         },\',\n \'         ""resolution"" : 19.08790397644043\',\n \'      },\',\n \'      {\',\n \'         ""elevation"" : 244.1318664550781,\',\n \'         ""location"" : {\',\n \'            ""lat"" : 42.974298,\',\n \'            ""lng"" : -81.19575500000001\',\n \'         },\',\n \'         ""resolution"" : 19.08790397644043\',\n \'      }\',\n \'   ],\',\n \'   ""status"" : ""OK""\',\n \'}\']\n', 'pd.read_json(elevations)\n', ""data = json.loads(elevations)\nlat,lng,el = [],[],[]\nfor result in data['results']:\n    lat.append(result[u'location'][u'lat'])\n    lng.append(result[u'location'][u'lng'])\n    el.append(result[u'elevation'])\ndf = pd.DataFrame([lat,lng,el]).T\n""]";"[""from urllib2 import Request, urlopen\nimport json\n\npath1 = '42.974049,-81.205203|42.974298,-81.195755'\nrequest=Request('http://maps.googleapis.com/maps/api/elevation/json?locations='+path1+'&sensor=false')\nresponse = urlopen(request)\nelevations = response.read()\n"", 'elevations.splitlines()\n\n[\'{\',\n \'   ""results"" : [\',\n \'      {\',\n \'         ""elevation"" : 243.3462677001953,\',\n \'         ""location"" : {\',\n \'            ""lat"" : 42.974049,\',\n \'            ""lng"" : -81.205203\',\n \'         },\',\n \'         ""resolution"" : 19.08790397644043\',\n \'      },\',\n \'      {\',\n \'         ""elevation"" : 244.1318664550781,\',\n \'         ""location"" : {\',\n \'            ""lat"" : 42.974298,\',\n \'            ""lng"" : -81.19575500000001\',\n \'         },\',\n \'         ""resolution"" : 19.08790397644043\',\n \'      }\',\n \'   ],\',\n \'   ""status"" : ""OK""\',\n \'}\']\n', 'pd.read_json(elevations)\n', ""data = json.loads(elevations)\nlat,lng,el = [],[],[]\nfor result in data['results']:\n    lat.append(result[u'location'][u'lat'])\n    lng.append(result[u'location'][u'lng'])\n    el.append(result[u'elevation'])\ndf = pd.DataFrame([lat,lng,el]).T\n""]";"[""from urllib2 import Request, urlopen\nimport json\n\npath1 = '42.974049,-81.205203|42.974298,-81.195755'\nrequest=Request('http://maps.googleapis.com/maps/api/elevation/json?locations='+path1+'&sensor=false')\nresponse = urlopen(request)\nelevations = response.read()\nelevations.splitlines()\n\npd.read_json(elevations)\ndata = json.loads(elevations)\nlat,lng,el = [],[],[]\ndf = pd.DataFrame([lat,lng,el]).T\n""]";"[""import pandas as pd\nfrom urllib2 import Request, urlopen\nimport json\n\npath1 = '42.974049,-81.205203|42.974298,-81.195755'\nrequest=Request('http://maps.googleapis.com/maps/api/elevation/json?locations='+path1+'&sensor=false')\nresponse = urlopen(request)\nelevations = response.read()\nelevations.splitlines()\n\npd.read_json(elevations)\ndata = json.loads(elevations)\nlat,lng,el = [],[],[]\ndf = pd.DataFrame([lat,lng,el]).T\n""]";True;"[""import pandas as pd\nfrom urllib2 import Request, urlopen\nimport json\n\npath1 = '42.974049,-81.205203|42.974298,-81.195755'\nrequest=Request('http://maps.googleapis.com/maps/api/elevation/json?locations='+path1+'&sensor=false')\nresponse = urlopen(request)\nelevations = response.read()\nelevations.splitlines()\n\npd.read_json(elevations)\ndata = json.loads(elevations)\nlat,lng,el = [],[],[]\ndf = pd.DataFrame([lat,lng,el]).T\n""]";False;0;1;"[""No module named 'urllib2'""]";['ImportError'];0;1;"[""No module named 'urllib2'""]";['ImportError'];0;1;"[""No module named 'urllib2'""]";['ImportError']
648;648;648;648;1.0;3;21137150;;1;36;<python><pandas><floating-point><scientific-notation><numeric-format>;Format / Suppress Scientific Notation from Python Pandas Aggregation Results;25062.0;"[""df1.groupby('dept')['data1'].sum()\n\ndept\nvalue1       1.192433e+08\nvalue2       1.293066e+08\nvalue3       1.077142e+08\nsum_sales_dept.astype(str)\n""]";"[""df1.groupby('dept')['data1'].sum()\n\ndept\nvalue1       1.192433e+08\nvalue2       1.293066e+08\nvalue3       1.077142e+08\n"", 'sum_sales_dept.astype(str)\n']";"[""df1.groupby('dept')['data1'].sum()\n\ndept\nvalue1       1.192433e+08\nvalue2       1.293066e+08\nvalue3       1.077142e+08\n"", 'sum_sales_dept.astype(str)\n']";"[""df1.groupby('dept')['data1'].sum()\n\ndept\nsum_sales_dept.astype(str)\n""]";"[""df1.groupby('dept')['data1'].sum()\n\ndept\nsum_sales_dept.astype(str)\n""]";False;"[""import pandas as pd\ndata1 = pd.DataFrame()\ndata = pd.DataFrame()\ndf1 = pd.DataFrame()\ndf = pd.DataFrame()\ndf1.groupby('dept')['data1'].sum()\n\ndept\nsum_sales_dept.astype(str)\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'Series' is not defined""]";['NameError'];0;1;"[""name 'Series' is not defined""]";['NameError']
649;649;649;649;1.0;0;21164910;;1;14;<python><pandas>;Delete Column in Pandas if it is All Zeros;11895.0;['ones = []\nzeros = []\nfor year in years:\n    for i in range(0,599):\n        if year[str(i)].values.any() == 1:\n            ones.append(i)\n        if year[str(i)].values.all() == 0:\n            zeros.append(i)\n    for j in ones:\n        if j in zeros:\n            zeros.remove(j)\n    for q in zeros:\n        del year[str(q)]\n'];['ones = []\nzeros = []\nfor year in years:\n    for i in range(0,599):\n        if year[str(i)].values.any() == 1:\n            ones.append(i)\n        if year[str(i)].values.all() == 0:\n            zeros.append(i)\n    for j in ones:\n        if j in zeros:\n            zeros.remove(j)\n    for q in zeros:\n        del year[str(q)]\n'];['ones = []\nzeros = []\nfor year in years:\n    for i in range(0,599):\n        if year[str(i)].values.any() == 1:\n            ones.append(i)\n        if year[str(i)].values.all() == 0:\n            zeros.append(i)\n    for j in ones:\n        if j in zeros:\n            zeros.remove(j)\n    for q in zeros:\n        del year[str(q)]\n'];['ones = []\nzeros = []\n'];['ones = []\nzeros = []\n'];False;['import pandas as pd\nones = []\nzeros = []\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
650;650;650;650;5.0;4;21197774;;1;48;<python><pandas>;Assign pandas dataframe column dtypes;50855.0;"[""import pandas as pd\nprint pd.DataFrame([['a','1'],['b','2']],\n                   dtype={'x':'object','y':'int'},\n                   columns=['x','y'])\nValueError: entry not a 2- or 3- tuple\ndtypes = {'x':'object','y':'int'}\nmydata = pd.DataFrame([['a','1'],['b','2']],\n                      columns=['x','y'])\nfor c in mydata.columns:\n    mydata[c] = mydata[c].astype(dtypes[c])\nprint mydata['y'].dtype   #=> int64\n""]";"[""import pandas as pd\nprint pd.DataFrame([['a','1'],['b','2']],\n                   dtype={'x':'object','y':'int'},\n                   columns=['x','y'])\n"", 'ValueError: entry not a 2- or 3- tuple\n', ""dtypes = {'x':'object','y':'int'}\nmydata = pd.DataFrame([['a','1'],['b','2']],\n                      columns=['x','y'])\nfor c in mydata.columns:\n    mydata[c] = mydata[c].astype(dtypes[c])\nprint mydata['y'].dtype   #=> int64\n""]";"['dtype', 'pd.Dataframe', 'pd.read_csv', ""import pandas as pd\nprint pd.DataFrame([['a','1'],['b','2']],\n                   dtype={'x':'object','y':'int'},\n                   columns=['x','y'])\n"", 'ValueError: entry not a 2- or 3- tuple\n', 'astype', ""dtypes = {'x':'object','y':'int'}\nmydata = pd.DataFrame([['a','1'],['b','2']],\n                      columns=['x','y'])\nfor c in mydata.columns:\n    mydata[c] = mydata[c].astype(dtypes[c])\nprint mydata['y'].dtype   #=> int64\n""]";"[""import pandas as pd\ndtypes = {'x':'object','y':'int'}\n""]";"[""import pandas as pd\ndtypes = {'x':'object','y':'int'}\n""]";False;"[""import pandas as pd\nimport pandas as pd\ndtypes = {'x':'object','y':'int'}\n""]";False;2;4;"[""name 'df' is not defined"", 'Sucess', 'Sucess', ""name 'data_df' is not defined""]";['NameError', 'Sucess', 'Sucess', 'NameError'];2;4;"[""name 'df' is not defined"", 'Sucess', 'Sucess', ""name 'data_df' is not defined""]";['NameError', 'Sucess', 'Sucess', 'NameError'];2;4;"['convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\nFor all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.', 'Sucess', 'Sucess', ""name 'data_df' is not defined""]";['FutureWarning', 'Sucess', 'Sucess', 'NameError']
651;651;651;651;3.0;0;21201618;;1;15;<python><pandas>;pandas.merge: match the nearest time stamp >= the series of timestamps;6959.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""No module named 'pylab'""]";['ImportError'];0;1;"[""No module named 'pylab'""]";['ImportError'];0;1;"[""No module named 'pylab'""]";['ImportError']
652;652;652;652;5.0;0;21231478;;1;15;<python><pandas><dataframe>;Get rows that have the same value across its columns in pandas;7328.0;['+-----+--------+--------+--------+   \n|     |    1   |    2   |    3   |\n+-----+--------+--------+--------+\n|  0  | apple  | banana | banana |\n|  1  | orange | orange | orange |\n|  2  | banana | apple  | orange |\n|  3  | NaN    | NaN    | NaN    |\n|  4  | apple  | apple  | apple  |\n+-----+--------+--------+--------+\n+-----+--------+--------+--------+   \n|     |    1   |    2   |    3   |\n+-----+--------+--------+--------+\n|  1  | orange | orange | orange |\n|  4  | apple  | apple  | apple  |\n+-----+--------+--------+--------+\n'];['+-----+--------+--------+--------+   \n|     |    1   |    2   |    3   |\n+-----+--------+--------+--------+\n|  0  | apple  | banana | banana |\n|  1  | orange | orange | orange |\n|  2  | banana | apple  | orange |\n|  3  | NaN    | NaN    | NaN    |\n|  4  | apple  | apple  | apple  |\n+-----+--------+--------+--------+\n', '+-----+--------+--------+--------+   \n|     |    1   |    2   |    3   |\n+-----+--------+--------+--------+\n|  1  | orange | orange | orange |\n|  4  | apple  | apple  | apple  |\n+-----+--------+--------+--------+\n'];['+-----+--------+--------+--------+   \n|     |    1   |    2   |    3   |\n+-----+--------+--------+--------+\n|  0  | apple  | banana | banana |\n|  1  | orange | orange | orange |\n|  2  | banana | apple  | orange |\n|  3  | NaN    | NaN    | NaN    |\n|  4  | apple  | apple  | apple  |\n+-----+--------+--------+--------+\n', '+-----+--------+--------+--------+   \n|     |    1   |    2   |    3   |\n+-----+--------+--------+--------+\n|  1  | orange | orange | orange |\n|  4  | apple  | apple  | apple  |\n+-----+--------+--------+--------+\n', 'D[D[1]==D[2]]'];[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
653;653;653;653;2.0;0;21231834;;1;20;<python><pandas><dataframe>;Creating a pandas DataFrame from columns of other DataFrames with similar indexes;41549.0;"[""df1 = pd.DataFrame(np.random.randn(5,3), index=pd.date_range('01/02/2014',periods=5,freq='D'), columns=['a','b','c'] )\ndf2 = pd.DataFrame(np.random.randn(8,3), index=pd.date_range('01/01/2014',periods=8,freq='D'), columns=['a','b','c'] )\ndf1\n                 a        b            c\n2014-01-02   0.580550    0.480814    1.135899\n2014-01-03  -1.961033    0.546013    1.093204\n2014-01-04   2.063441   -0.627297    2.035373\n2014-01-05   0.319570    0.058588    0.350060\n2014-01-06   1.318068   -0.802209   -0.939962\n\ndf2\n                 a        b            c\n2014-01-01   0.772482    0.899337    0.808630\n2014-01-02   0.518431   -1.582113    0.323425\n2014-01-03   0.112109    1.056705   -1.355067\n2014-01-04   0.767257   -2.311014    0.340701\n2014-01-05   0.794281   -1.954858    0.200922\n2014-01-06   0.156088    0.718658   -1.030077\n2014-01-07   1.621059    0.106656   -0.472080\n2014-01-08  -2.061138   -2.023157    0.257151\ndf3\n                 df1        df2\n2014-01-01   NaN        0.808630\n2014-01-02   1.135899   0.323425\n2014-01-03   1.093204   -1.355067\n2014-01-04   2.035373   0.340701\n2014-01-05   0.350060   0.200922\n2014-01-06   -0.939962  -1.030077\n2014-01-07   NaN        -0.472080\n2014-01-08   NaN        0.257151\n""]";"[""df1 = pd.DataFrame(np.random.randn(5,3), index=pd.date_range('01/02/2014',periods=5,freq='D'), columns=['a','b','c'] )\ndf2 = pd.DataFrame(np.random.randn(8,3), index=pd.date_range('01/01/2014',periods=8,freq='D'), columns=['a','b','c'] )\ndf1\n                 a        b            c\n2014-01-02   0.580550    0.480814    1.135899\n2014-01-03  -1.961033    0.546013    1.093204\n2014-01-04   2.063441   -0.627297    2.035373\n2014-01-05   0.319570    0.058588    0.350060\n2014-01-06   1.318068   -0.802209   -0.939962\n\ndf2\n                 a        b            c\n2014-01-01   0.772482    0.899337    0.808630\n2014-01-02   0.518431   -1.582113    0.323425\n2014-01-03   0.112109    1.056705   -1.355067\n2014-01-04   0.767257   -2.311014    0.340701\n2014-01-05   0.794281   -1.954858    0.200922\n2014-01-06   0.156088    0.718658   -1.030077\n2014-01-07   1.621059    0.106656   -0.472080\n2014-01-08  -2.061138   -2.023157    0.257151\n"", 'df3\n                 df1        df2\n2014-01-01   NaN        0.808630\n2014-01-02   1.135899   0.323425\n2014-01-03   1.093204   -1.355067\n2014-01-04   2.035373   0.340701\n2014-01-05   0.350060   0.200922\n2014-01-06   -0.939962  -1.030077\n2014-01-07   NaN        -0.472080\n2014-01-08   NaN        0.257151\n']";"[""df1 = pd.DataFrame(np.random.randn(5,3), index=pd.date_range('01/02/2014',periods=5,freq='D'), columns=['a','b','c'] )\ndf2 = pd.DataFrame(np.random.randn(8,3), index=pd.date_range('01/01/2014',periods=8,freq='D'), columns=['a','b','c'] )\ndf1\n                 a        b            c\n2014-01-02   0.580550    0.480814    1.135899\n2014-01-03  -1.961033    0.546013    1.093204\n2014-01-04   2.063441   -0.627297    2.035373\n2014-01-05   0.319570    0.058588    0.350060\n2014-01-06   1.318068   -0.802209   -0.939962\n\ndf2\n                 a        b            c\n2014-01-01   0.772482    0.899337    0.808630\n2014-01-02   0.518431   -1.582113    0.323425\n2014-01-03   0.112109    1.056705   -1.355067\n2014-01-04   0.767257   -2.311014    0.340701\n2014-01-05   0.794281   -1.954858    0.200922\n2014-01-06   0.156088    0.718658   -1.030077\n2014-01-07   1.621059    0.106656   -0.472080\n2014-01-08  -2.061138   -2.023157    0.257151\n"", 'df3\n                 df1        df2\n2014-01-01   NaN        0.808630\n2014-01-02   1.135899   0.323425\n2014-01-03   1.093204   -1.355067\n2014-01-04   2.035373   0.340701\n2014-01-05   0.350060   0.200922\n2014-01-06   -0.939962  -1.030077\n2014-01-07   NaN        -0.472080\n2014-01-08   NaN        0.257151\n']";"[""df1 = pd.DataFrame(np.random.randn(5,3), index=pd.date_range('01/02/2014',periods=5,freq='D'), columns=['a','b','c'] )\ndf2 = pd.DataFrame(np.random.randn(8,3), index=pd.date_range('01/01/2014',periods=8,freq='D'), columns=['a','b','c'] )\ndf1\n\ndf2\ndf3\n""]";"[""import pandas as pd\ndf1 = pd.DataFrame(np.random.randn(5,3), index=pd.date_range('01/02/2014',periods=5,freq='D'), columns=['a','b','c'] )\ndf2 = pd.DataFrame(np.random.randn(8,3), index=pd.date_range('01/01/2014',periods=8,freq='D'), columns=['a','b','c'] )\ndf1\n\ndf2\ndf3\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\ndf1 = pd.DataFrame(np.random.randn(5,3), index=pd.date_range('01/02/2014',periods=5,freq='D'), columns=['a','b','c'] )\ndf2 = pd.DataFrame(np.random.randn(8,3), index=pd.date_range('01/01/2014',periods=8,freq='D'), columns=['a','b','c'] )\ndf1\n\ndf2\ndf3\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'df1' is not defined""]";['NameError'];0;1;"[""'c'""]";['KeyError']
654;654;654;654;4.0;3;21247203;;1;19;<python><pandas><crosstab>;How to make a pandas crosstab with percentages?;15719.0;"[""df = pd.DataFrame({'A' : ['one', 'one', 'two', 'three'] * 6,\n                   'B' : ['A', 'B', 'C'] * 8,\n                   'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n                   'D' : np.random.randn(24),\n                   'E' : np.random.randn(24)})\n\n\npd.crosstab(df.A,df.B)\n\n\nB       A    B    C\nA               \none     4    4    4\nthree   2    2    2\ntwo     2    2    2\nB       A     B    C\nA               \none     .33  .33  .33\nthree   .33  .33  .33\ntwo     .33  .33  .33\n""]";"[""df = pd.DataFrame({'A' : ['one', 'one', 'two', 'three'] * 6,\n                   'B' : ['A', 'B', 'C'] * 8,\n                   'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n                   'D' : np.random.randn(24),\n                   'E' : np.random.randn(24)})\n\n\npd.crosstab(df.A,df.B)\n\n\nB       A    B    C\nA               \none     4    4    4\nthree   2    2    2\ntwo     2    2    2\n"", 'B       A     B    C\nA               \none     .33  .33  .33\nthree   .33  .33  .33\ntwo     .33  .33  .33\n']";"[""df = pd.DataFrame({'A' : ['one', 'one', 'two', 'three'] * 6,\n                   'B' : ['A', 'B', 'C'] * 8,\n                   'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n                   'D' : np.random.randn(24),\n                   'E' : np.random.randn(24)})\n\n\npd.crosstab(df.A,df.B)\n\n\nB       A    B    C\nA               \none     4    4    4\nthree   2    2    2\ntwo     2    2    2\n"", 'B       A     B    C\nA               \none     .33  .33  .33\nthree   .33  .33  .33\ntwo     .33  .33  .33\n']";['\n\npd.crosstab(df.A,df.B)\n\n\nA               \nA               \n'];['import pandas as pd\n\n\npd.crosstab(df.A,df.B)\n\n\nA               \nA               \n'];True;['import pandas as pd\ndf = pd.DataFrame()\n\n\npd.crosstab(df.A,df.B)\n\n\nA               \nA               \n'];True;0;2;"[""name 'pd' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'DataFrame' object has no attribute 'A'"", ""'DataFrame' object has no attribute 'A'""]";['AttributeError', 'AttributeError']
655;655;655;655;1.0;2;21247992;;1;11;<python><pandas><dataframe>;grouping pandas dataframe by two columns (or more)?;10280.0;"['mydf = pandas.DataFrame({""cat"": [""first"", ""first"", ""first"", ""second"", ""second"", ""third""], ""class"": [""A"", ""A"", ""A"", ""B"", ""B"", ""C""], ""name"": [""a1"", ""a2"", ""a3"", ""b1"", ""b2"", ""c1""], ""val"": [1,5,1,1,2,10]})\nmydf.groupby(""class"").val.sum()\nmydf.groupby([""cat"", ""class""]).val.sum()\ncat     class    val\nfirst   A         7\nsecond  B         3\nthird   C        10\n']";"['mydf = pandas.DataFrame({""cat"": [""first"", ""first"", ""first"", ""second"", ""second"", ""third""], ""class"": [""A"", ""A"", ""A"", ""B"", ""B"", ""C""], ""name"": [""a1"", ""a2"", ""a3"", ""b1"", ""b2"", ""c1""], ""val"": [1,5,1,1,2,10]})\n', 'mydf.groupby(""class"").val.sum()\n', 'mydf.groupby([""cat"", ""class""]).val.sum()\n', 'cat     class    val\nfirst   A         7\nsecond  B         3\nthird   C        10\n']";"['mydf = pandas.DataFrame({""cat"": [""first"", ""first"", ""first"", ""second"", ""second"", ""third""], ""class"": [""A"", ""A"", ""A"", ""B"", ""B"", ""C""], ""name"": [""a1"", ""a2"", ""a3"", ""b1"", ""b2"", ""c1""], ""val"": [1,5,1,1,2,10]})\n', 'val', 'class', 'groupby', 'mydf.groupby(""class"").val.sum()\n', 'cat', 'merge/join', 'mydf.groupby([""cat"", ""class""]).val.sum()\n', 'cat', 'class', 'val', 'class', 'cat     class    val\nfirst   A         7\nsecond  B         3\nthird   C        10\n']";"['mydf = pandas.DataFrame({""cat"": [""first"", ""first"", ""first"", ""second"", ""second"", ""third""], ""class"": [""A"", ""A"", ""A"", ""B"", ""B"", ""C""], ""name"": [""a1"", ""a2"", ""a3"", ""b1"", ""b2"", ""c1""], ""val"": [1,5,1,1,2,10]})\nmydf.groupby(""class"").val.sum()\nmydf.groupby([""cat"", ""class""]).val.sum()\n']";"['mydf = pandas.DataFrame({""cat"": [""first"", ""first"", ""first"", ""second"", ""second"", ""third""], ""class"": [""A"", ""A"", ""A"", ""B"", ""B"", ""C""], ""name"": [""a1"", ""a2"", ""a3"", ""b1"", ""b2"", ""c1""], ""val"": [1,5,1,1,2,10]})\nmydf.groupby(""class"").val.sum()\nmydf.groupby([""cat"", ""class""]).val.sum()\n']";False;"['import pandas as pd\nmydf = pandas.DataFrame({""cat"": [""first"", ""first"", ""first"", ""second"", ""second"", ""third""], ""class"": [""A"", ""A"", ""A"", ""B"", ""B"", ""C""], ""name"": [""a1"", ""a2"", ""a3"", ""b1"", ""b2"", ""c1""], ""val"": [1,5,1,1,2,10]})\nmydf.groupby(""class"").val.sum()\nmydf.groupby([""cat"", ""class""]).val.sum()\n']";False;0;1;"[""name 'mydf' is not defined""]";['NameError'];0;1;"[""name 'mydf' is not defined""]";['NameError'];0;1;"[""name 'mydf' is not defined""]";['NameError']
656;656;656;656;1.0;10;21249206;;1;19;<python><terminal><pandas><ipython>;How to configure display output in IPython pandas;12295.0;"[""import pandas as pd\n\npd.set_option('display.expand_max_repr', False)\npd.set_option('display.max_columns', 30)\npd.set_option('display.width', None)\npd.set_option('display.line_width', 200)\n    id       type\n106125       puzzle       gameplay_id  sitting_id  user_id           ...\n106253       frames       gameplay_id  sitting_id  user_id           ...\n106260       trivia       gameplay_id  sitting_id  user_id           ...\n""]";"[""import pandas as pd\n\npd.set_option('display.expand_max_repr', False)\npd.set_option('display.max_columns', 30)\npd.set_option('display.width', None)\npd.set_option('display.line_width', 200)\n"", '    id       type\n106125       puzzle       gameplay_id  sitting_id  user_id           ...\n106253       frames       gameplay_id  sitting_id  user_id           ...\n106260       trivia       gameplay_id  sitting_id  user_id           ...\n']";"['DataFrame', '%run', ""import pandas as pd\n\npd.set_option('display.expand_max_repr', False)\npd.set_option('display.max_columns', 30)\npd.set_option('display.width', None)\npd.set_option('display.line_width', 200)\n"", 'to_string()', 'describe()', 'to_string()', 'groupby', '    id       type\n106125       puzzle       gameplay_id  sitting_id  user_id           ...\n106253       frames       gameplay_id  sitting_id  user_id           ...\n106260       trivia       gameplay_id  sitting_id  user_id           ...\n', 'pd.util.terminal.get_terminal_size()']";"[""import pandas as pd\n\npd.set_option('display.expand_max_repr', False)\npd.set_option('display.max_columns', 30)\npd.set_option('display.width', None)\npd.set_option('display.line_width', 200)\n""]";"[""import pandas as pd\n\npd.set_option('display.expand_max_repr', False)\npd.set_option('display.max_columns', 30)\npd.set_option('display.width', None)\npd.set_option('display.line_width', 200)\n""]";False;"[""import pandas as pd\nimport pandas as pd\n\npd.set_option('display.expand_max_repr', False)\npd.set_option('display.max_columns', 30)\npd.set_option('display.width', None)\npd.set_option('display.line_width', 200)\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
657;657;657;657;3.0;1;21263020;;1;12;<python><pandas>;pandas : update value if condition in 3 columns are met;16650.0;"[""In[1]: df\nOut[1]:\n      A      B       C            D\n1   blue    red    square        NaN\n2  orange  yellow  circle        NaN\n3  black   grey    circle        NaN\ndf.ix[ np.logical_and(df.A=='blue', df.B=='red', df.C=='square'), ['D'] ] = 'succeed'\ndf.ix[ np.logical_and(df.A=='blue', df.B=='red', df.C=='triangle'), ['D'] ] = 'succeed'\nIn[1]: df\nOut[1]:\n      A      B       C            D\n1   blue    red    square        succeed\n2  orange  yellow  circle        NaN\n3  black   grey    circle        NaN\n""]";"['In[1]: df\nOut[1]:\n      A      B       C            D\n1   blue    red    square        NaN\n2  orange  yellow  circle        NaN\n3  black   grey    circle        NaN\n', ""df.ix[ np.logical_and(df.A=='blue', df.B=='red', df.C=='square'), ['D'] ] = 'succeed'\n"", ""df.ix[ np.logical_and(df.A=='blue', df.B=='red', df.C=='triangle'), ['D'] ] = 'succeed'\n"", 'In[1]: df\nOut[1]:\n      A      B       C            D\n1   blue    red    square        succeed\n2  orange  yellow  circle        NaN\n3  black   grey    circle        NaN\n']";"['In[1]: df\nOut[1]:\n      A      B       C            D\n1   blue    red    square        NaN\n2  orange  yellow  circle        NaN\n3  black   grey    circle        NaN\n', ""df.ix[ np.logical_and(df.A=='blue', df.B=='red', df.C=='square'), ['D'] ] = 'succeed'\n"", ""df.ix[ np.logical_and(df.A=='blue', df.B=='red', df.C=='triangle'), ['D'] ] = 'succeed'\n"", 'In[1]: df\nOut[1]:\n      A      B       C            D\n1   blue    red    square        succeed\n2  orange  yellow  circle        NaN\n3  black   grey    circle        NaN\n']";"[""df.ix[ np.logical_and(df.A=='blue', df.B=='red', df.C=='square'), ['D'] ] = 'succeed'\ndf.ix[ np.logical_and(df.A=='blue', df.B=='red', df.C=='triangle'), ['D'] ] = 'succeed'\n""]";"[""df.ix[ np.logical_and(df.A=='blue', df.B=='red', df.C=='square'), ['D'] ] = 'succeed'\ndf.ix[ np.logical_and(df.A=='blue', df.B=='red', df.C=='triangle'), ['D'] ] = 'succeed'\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf.ix[ np.logical_and(df.A=='blue', df.B=='red', df.C=='square'), ['D'] ] = 'succeed'\ndf.ix[ np.logical_and(df.A=='blue', df.B=='red', df.C=='triangle'), ['D'] ] = 'succeed'\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'DataFrame' object has no attribute 'A'""]";['AttributeError']
658;658;658;658;3.0;0;21269399;;1;24;<python><csv><datetime><pandas><dataframe>;datetime dtypes in pandas read_csv;28062.0;"['headers = [\'col1\', \'col2\', \'col3\', \'col4\']\ndtypes = [\'datetime\', \'datetime\', \'str\', \'float\']\npd.read_csv(file, sep=\'\\t\', header=None, names=headers, dtype=dtypes)\nTypeError: data type ""datetime"" not understood\n']";"[""headers = ['col1', 'col2', 'col3', 'col4']\ndtypes = ['datetime', 'datetime', 'str', 'float']\npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes)\n"", 'TypeError: data type ""datetime"" not understood\n']";"[""headers = ['col1', 'col2', 'col3', 'col4']\ndtypes = ['datetime', 'datetime', 'str', 'float']\npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes)\n"", 'TypeError: data type ""datetime"" not understood\n']";"[""headers = ['col1', 'col2', 'col3', 'col4']\ndtypes = ['datetime', 'datetime', 'str', 'float']\npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes)\n""]";"[""import pandas as pd\nheaders = ['col1', 'col2', 'col3', 'col4']\ndtypes = ['datetime', 'datetime', 'str', 'float']\npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes)\n""]";True;"[""import pandas as pd\nheaders = ['col1', 'col2', 'col3', 'col4']\ndtypes = ['datetime', 'datetime', 'str', 'float']\npd.read_csv(file, sep='\\t', header=None, names=headers, dtype=dtypes)\n""]";False;0;3;"[""name 'file' is not defined"", ""name 'file' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'file' is not defined"", ""name 'file' is not defined"", ""name 'file' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'file' is not defined"", ""name 'file' is not defined"", ""name 'file' is not defined""]";['NameError', 'NameError', 'NameError']
659;659;659;659;3.0;0;21271581;;1;12;<python><pandas><scipy>;Selecting Pandas Columns by dtype;5181.0;['df.select_columns(dtype=float64)\n'];['df.select_columns(dtype=float64)\n'];['df.select_columns(dtype=float64)\n'];['df.select_columns(dtype=float64)\n'];['df.select_columns(dtype=float64)\n'];False;['import pandas as pd\ndf = pd.DataFrame()\ndf.select_columns(dtype=float64)\n'];True;0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'np' is not defined"", ""name 'np' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError', 'NameError']
660;660;660;660;5.0;8;21271727;;1;12;<python><pandas>;Sorting in pandas for large datasets;2877.0;"['data = data.sort(columns=[""P_VALUE""], ascending=True, axis=0)\n']";"['data = data.sort(columns=[""P_VALUE""], ascending=True, axis=0)\n']";"['data = data.sort(columns=[""P_VALUE""], ascending=True, axis=0)\n']";"['data = data.sort(columns=[""P_VALUE""], ascending=True, axis=0)\n']";"['data = data.sort(columns=[""P_VALUE""], ascending=True, axis=0)\n']";False;"['import pandas as pd\ndata = data.sort(columns=[""P_VALUE""], ascending=True, axis=0)\n']";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
661;661;661;661;2.0;0;21285380;;1;31;<string><python-3.x><pandas><find>;Pandas: find column whose name contains a specific string;28697.0;[''];[];"[""'spike'"", ""'spike-2'"", ""'hey spike'"", ""'spiked-in'"", ""'spike'"", ""df['name']"", 'df[name]']";[''];[''];False;['import pandas as pd\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
662;662;662;662;3.0;7;21287624;;1;46;<python><pandas><na>;Convert Pandas column containing NaNs to dtype `int`;42262.0;"['df= pd.read_csv(""data.csv"", dtype={\'id\': int}) \nerror: Integer column has NA values\ndf= pd.read_csv(""data.csv"") \ndf[[\'id\']] = df[[\'id\']].astype(int)\nerror: Cannot convert NA to integer\n']";"['df= pd.read_csv(""data.csv"", dtype={\'id\': int}) \nerror: Integer column has NA values\n', 'df= pd.read_csv(""data.csv"") \ndf[[\'id\']] = df[[\'id\']].astype(int)\nerror: Cannot convert NA to integer\n']";"['id', 'int', 'id', 'id', 'df= pd.read_csv(""data.csv"", dtype={\'id\': int}) \nerror: Integer column has NA values\n', 'df= pd.read_csv(""data.csv"") \ndf[[\'id\']] = df[[\'id\']].astype(int)\nerror: Cannot convert NA to integer\n']";"['df= pd.read_csv(""data.csv"", dtype={\'id\': int}) \ndf= pd.read_csv(""data.csv"") \ndf[[\'id\']] = df[[\'id\']].astype(int)\n']";"['import pandas as pd\ndf= pd.read_csv(""data.csv"", dtype={\'id\': int}) \ndf= pd.read_csv(""data.csv"") \ndf[[\'id\']] = df[[\'id\']].astype(int)\n']";True;"['import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\ndf= pd.read_csv(""data.csv"", dtype={\'id\': int}) \ndf= pd.read_csv(""data.csv"") \ndf[[\'id\']] = df[[\'id\']].astype(int)\n']";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
663;663;663;663;5.0;2;21291259;;1;58;<python><pandas>;Convert floats to ints in Pandas?;84803.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;2;3;"['Sucess', ""name 'pd' is not defined"", 'Sucess']";['Sucess', 'NameError', 'Sucess'];2;3;['Sucess', 'DataFrame constructor not properly called!', 'Sucess'];['Sucess', 'ValueError', 'Sucess'];2;3;['Sucess', 'DataFrame constructor not properly called!', 'Sucess'];['Sucess', 'ValueError', 'Sucess']
664;664;664;664;2.0;1;21295334;;1;16;<python><pandas>;Find the length of the longest string in a Pandas DataFrame column;10209.0;"[""import numpy as np\nimport pandas as pd\n\nx = ['ab', 'bcd', 'dfe', 'efghik']\nx = np.repeat(x, 1e7)\ndf = pd.DataFrame(x, columns=['col1'])\n\nprint df.col1.map(lambda x: len(x)).max()\n# result --> 6\n""]";"[""import numpy as np\nimport pandas as pd\n\nx = ['ab', 'bcd', 'dfe', 'efghik']\nx = np.repeat(x, 1e7)\ndf = pd.DataFrame(x, columns=['col1'])\n\nprint df.col1.map(lambda x: len(x)).max()\n# result --> 6\n""]";"[""import numpy as np\nimport pandas as pd\n\nx = ['ab', 'bcd', 'dfe', 'efghik']\nx = np.repeat(x, 1e7)\ndf = pd.DataFrame(x, columns=['col1'])\n\nprint df.col1.map(lambda x: len(x)).max()\n# result --> 6\n"", 'df.col1.map(lambda x: len(x)).max()', '%timit']";"[""import numpy as np\nimport pandas as pd\n\nx = ['ab', 'bcd', 'dfe', 'efghik']\nx = np.repeat(x, 1e7)\ndf = pd.DataFrame(x, columns=['col1'])\n\n# result --> 6\n""]";"[""import numpy as np\nimport pandas as pd\n\nx = ['ab', 'bcd', 'dfe', 'efghik']\nx = np.repeat(x, 1e7)\ndf = pd.DataFrame(x, columns=['col1'])\n\n# result --> 6\n""]";False;"[""import pandas as pd\nimport numpy as np\nimport pandas as pd\n\nx = ['ab', 'bcd', 'dfe', 'efghik']\nx = np.repeat(x, 1e7)\ndf = pd.DataFrame(x, columns=['col1'])\n\n# result --> 6\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
665;665;665;665;1.0;1;21316628;;1;12;<python><pandas>;Make a Pandas MultiIndex from a product of iterables?;1871.0;"['import pandas as pd\nimport itertools\n\ndef product_index(values, names=None):\n    """"""Make a MultiIndex from the combinatorial product of the values.""""""\n    iterable = itertools.product(*values)\n    idx = pd.MultiIndex.from_tuples(list(iterable), names=names)\n    return idx\na = range(3)\nb = list(""ab"")\nproduct_index([a, b])\nMultiIndex(levels=[[0, 1, 2], [u\'a\', u\'b\']],\n           labels=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]])\n']";"['import pandas as pd\nimport itertools\n\ndef product_index(values, names=None):\n    """"""Make a MultiIndex from the combinatorial product of the values.""""""\n    iterable = itertools.product(*values)\n    idx = pd.MultiIndex.from_tuples(list(iterable), names=names)\n    return idx\n', 'a = range(3)\nb = list(""ab"")\nproduct_index([a, b])\n', ""MultiIndex(levels=[[0, 1, 2], [u'a', u'b']],\n           labels=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]])\n""]";"['import pandas as pd\nimport itertools\n\ndef product_index(values, names=None):\n    """"""Make a MultiIndex from the combinatorial product of the values.""""""\n    iterable = itertools.product(*values)\n    idx = pd.MultiIndex.from_tuples(list(iterable), names=names)\n    return idx\n', 'a = range(3)\nb = list(""ab"")\nproduct_index([a, b])\n', ""MultiIndex(levels=[[0, 1, 2], [u'a', u'b']],\n           labels=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]])\n"", 'MultiIndex.from_product']";"['import pandas as pd\nimport itertools\n\na = range(3)\nb = list(""ab"")\nproduct_index([a, b])\n']";"['import pandas as pd\nimport itertools\n\na = range(3)\nb = list(""ab"")\nproduct_index([a, b])\n']";False;"['import pandas as pd\nimport pandas as pd\nimport itertools\n\na = range(3)\nb = list(""ab"")\nproduct_index([a, b])\n']";False;0;1;"[""No module named 'pandas.tools.util'""]";['ImportError'];0;1;"[""No module named 'pandas.tools.util'""]";['ImportError'];0;1;"[""No module named 'pandas.tools.util'""]";['ImportError']
666;666;666;666;1.0;5;21317384;;1;11;<python><pandas>;Pandas/Python: How to concatenate two dataframes without duplicates?;12187.0;['   I    II    I    II\n0  1    2     5    6\n1  3    1     3    1\n     I    II\n  0  1    2\n  1  3    1\n  2  5    6\n'];['   I    II    I    II\n0  1    2     5    6\n1  3    1     3    1\n', '     I    II\n  0  1    2\n  1  3    1\n  2  5    6\n'];['   I    II    I    II\n0  1    2     5    6\n1  3    1     3    1\n', '     I    II\n  0  1    2\n  1  3    1\n  2  5    6\n'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
667;667;667;667;1.0;0;21318865;;1;12;<csv><pandas><multi-index>;Read multi-index on the columns from csv file;5225.0;"[""Male, Male, Male, Female, Female\nR, R, L, R, R\n.86, .67, .88, .78, .81\n    Male        Female\n    R       L   R\n0   .86 .67 .88 .78 .81\ndf = pd.read_csv('file.csv', header=[0,1])\nEmpty DataFrame\nColumns: [(Male, R), (Male, R), (Male, L), (Female, R), (Female, R)]\nIndex: []\n(...)Can be a list of integers that specify row\nlocations for a multi-index on the columns E.g. [0,1,3]\n""]";"['Male, Male, Male, Female, Female\nR, R, L, R, R\n.86, .67, .88, .78, .81\n', '    Male        Female\n    R       L   R\n0   .86 .67 .88 .78 .81\n', ""df = pd.read_csv('file.csv', header=[0,1])\n"", 'Empty DataFrame\nColumns: [(Male, R), (Male, R), (Male, L), (Female, R), (Female, R)]\nIndex: []\n', '(...)Can be a list of integers that specify row\nlocations for a multi-index on the columns E.g. [0,1,3]\n']";"['Male, Male, Male, Female, Female\nR, R, L, R, R\n.86, .67, .88, .78, .81\n', '    Male        Female\n    R       L   R\n0   .86 .67 .88 .78 .81\n', ""df = pd.read_csv('file.csv', header=[0,1])\n"", 'headers', 'Empty DataFrame\nColumns: [(Male, R), (Male, R), (Male, L), (Female, R), (Female, R)]\nIndex: []\n', '(...)Can be a list of integers that specify row\nlocations for a multi-index on the columns E.g. [0,1,3]\n']";"[""Male, Male, Male, Female, Female\nR, R, L, R, R\n.86, .67, .88, .78, .81\ndf = pd.read_csv('file.csv', header=[0,1])\n""]";"[""import pandas as pd\nMale, Male, Male, Female, Female\nR, R, L, R, R\n.86, .67, .88, .78, .81\ndf = pd.read_csv('file.csv', header=[0,1])\n""]";True;"[""import pandas as pd\nMale, Male, Male, Female, Female\nR, R, L, R, R\n.86, .67, .88, .78, .81\ndf = pd.read_csv('file.csv', header=[0,1])\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""The 'tupleize_cols' argument has been deprecated and will be removed in a future version. Column tuples will then always be converted to MultiIndex.\n\n""]";['FutureWarning'];0;1;"[""The 'tupleize_cols' argument has been deprecated and will be removed in a future version. Column tuples will then always be converted to MultiIndex.\n\n""]";['FutureWarning']
668;668;668;668;2.0;0;21319929;;1;26;<python><pandas>;How to determine whether a Pandas Column contains a particular value;42502.0;[''];[];"[""if x in df['id']"", ""43 in df['id']"", 'True', ""df[df['id'] == 43]""]";[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'pd' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
669;669;669;669;4.0;0;21360361;;1;43;<python><matplotlib><pandas><ipython><ipython-notebook>;how to dynamically update a plot in a loop in ipython notebook (within one cell);30112.0;"[""i = pd.date_range('2013-1-1',periods=100,freq='s')\nwhile True:\n    plot(pd.Series(data=np.random.randn(100), index=i))\n    #pd.Series(data=np.random.randn(100), index=i).plot() also tried this one\n    time.sleep(5)\n""]";"[""i = pd.date_range('2013-1-1',periods=100,freq='s')\nwhile True:\n    plot(pd.Series(data=np.random.randn(100), index=i))\n    #pd.Series(data=np.random.randn(100), index=i).plot() also tried this one\n    time.sleep(5)\n""]";"['--pylab=inline', ""i = pd.date_range('2013-1-1',periods=100,freq='s')\nwhile True:\n    plot(pd.Series(data=np.random.randn(100), index=i))\n    #pd.Series(data=np.random.randn(100), index=i).plot() also tried this one\n    time.sleep(5)\n"", 'plot()', 'plot()']";"[""i = pd.date_range('2013-1-1',periods=100,freq='s')\n    #pd.Series(data=np.random.randn(100), index=i).plot() also tried this one\n""]";"[""import pandas as pd\ni = pd.date_range('2013-1-1',periods=100,freq='s')\n    #pd.Series(data=np.random.randn(100), index=i).plot() also tried this one\n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\ni = pd.date_range('2013-1-1',periods=100,freq='s')\n    #pd.Series(data=np.random.randn(100), index=i).plot() also tried this one\n""]";True;0;3;"[""No module named 'pylab'"", ""'function' object has no attribute 'clear_output'"", ""No module named 'matplotlib'""]";['ImportError', 'AttributeError', 'ImportError'];0;3;"[""No module named 'pylab'"", ""'function' object has no attribute 'clear_output'"", ""No module named 'matplotlib'""]";['ImportError', 'AttributeError', 'ImportError'];0;3;"[""No module named 'pylab'"", ""'function' object has no attribute 'clear_output'"", ""No module named 'matplotlib'""]";['ImportError', 'AttributeError', 'ImportError']
670;670;670;670;1.0;9;21390035;;1;20;<python-2.7><pandas><group-by>;Python pandas groupby object apply method duplicates first group;3453.0;"["">>> from pandas import Series, DataFrame\n>>> import pandas as pd\n>>> df = pd.DataFrame({'class': ['A', 'B', 'C'], 'count':[1,0,2]})\n>>> print(df)\n   class  count  \n0     A      1  \n1     B      0    \n2     C      2\n>>> for group in df.groupby('class', group_keys = True):\n>>>     print(group)\n('A',   class  count\n0     A      1)\n('B',   class  count\n1     B      0)\n('C',   class  count\n2     C      2)\n>>> def checkit(group):\n>>>     print(group)\n>>> df.groupby('class', group_keys = True).apply(checkit)\n  class  count\n0     A      1\n  class  count\n0     A      1\n  class  count\n1     B      0\n  class  count\n2     C      2\n>>> def addone(group):\n>>>     group['count'] += 1\n>>>     return group\n\n>>> df.groupby('class', group_keys = True).apply(addone)\n>>> print(df)\n\n      class  count\n0     A      1\n1     B      0\n2     C      2\n      class  count\n0     A      2\n1     B      1\n2     C      3\n""]";"["">>> from pandas import Series, DataFrame\n>>> import pandas as pd\n>>> df = pd.DataFrame({'class': ['A', 'B', 'C'], 'count':[1,0,2]})\n>>> print(df)\n   class  count  \n0     A      1  \n1     B      0    \n2     C      2\n"", "">>> for group in df.groupby('class', group_keys = True):\n>>>     print(group)\n('A',   class  count\n0     A      1)\n('B',   class  count\n1     B      0)\n('C',   class  count\n2     C      2)\n"", "">>> def checkit(group):\n>>>     print(group)\n>>> df.groupby('class', group_keys = True).apply(checkit)\n  class  count\n0     A      1\n  class  count\n0     A      1\n  class  count\n1     B      0\n  class  count\n2     C      2\n"", "">>> def addone(group):\n>>>     group['count'] += 1\n>>>     return group\n\n>>> df.groupby('class', group_keys = True).apply(addone)\n>>> print(df)\n\n      class  count\n0     A      1\n1     B      0\n2     C      2\n"", '      class  count\n0     A      2\n1     B      1\n2     C      3\n']";"["">>> from pandas import Series, DataFrame\n>>> import pandas as pd\n>>> df = pd.DataFrame({'class': ['A', 'B', 'C'], 'count':[1,0,2]})\n>>> print(df)\n   class  count  \n0     A      1  \n1     B      0    \n2     C      2\n"", "">>> for group in df.groupby('class', group_keys = True):\n>>>     print(group)\n('A',   class  count\n0     A      1)\n('B',   class  count\n1     B      0)\n('C',   class  count\n2     C      2)\n"", "">>> def checkit(group):\n>>>     print(group)\n>>> df.groupby('class', group_keys = True).apply(checkit)\n  class  count\n0     A      1\n  class  count\n0     A      1\n  class  count\n1     B      0\n  class  count\n2     C      2\n"", "">>> def addone(group):\n>>>     group['count'] += 1\n>>>     return group\n\n>>> df.groupby('class', group_keys = True).apply(addone)\n>>> print(df)\n\n      class  count\n0     A      1\n1     B      0\n2     C      2\n"", '      class  count\n0     A      2\n1     B      1\n2     C      3\n']";['\n\n'];['\n\n'];False;['import pandas as pd\n\n\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
671;671;671;671;1.0;3;21415661;;1;32;<python><pandas><boolean-logic>;Logic operator for boolean indexing in Pandas;38894.0;"[""a[(a['some_column']==some_number) & (a['some_other_column']==some_other_number)]\na[(a['some_column']==some_number) and (a['some_other_column']==some_other_number)]\na=pd.DataFrame({'x':[1,1],'y':[10,20]})\n\nIn: a[(a['x']==1)&(a['y']==10)]\nOut:    x   y\n     0  1  10\n\nIn: a[(a['x']==1) and (a['y']==10)]\nOut: ValueError: The truth value of an array with more than one element is ambiguous.     Use a.any() or a.all()\n""]";"[""a[(a['some_column']==some_number) & (a['some_other_column']==some_other_number)]\n"", ""a[(a['some_column']==some_number) and (a['some_other_column']==some_other_number)]\n"", ""a=pd.DataFrame({'x':[1,1],'y':[10,20]})\n\nIn: a[(a['x']==1)&(a['y']==10)]\nOut:    x   y\n     0  1  10\n\nIn: a[(a['x']==1) and (a['y']==10)]\nOut: ValueError: The truth value of an array with more than one element is ambiguous.     Use a.any() or a.all()\n""]";"[""a[(a['some_column']==some_number) & (a['some_other_column']==some_other_number)]\n"", ""a[(a['some_column']==some_number) and (a['some_other_column']==some_other_number)]\n"", ""a=pd.DataFrame({'x':[1,1],'y':[10,20]})\n\nIn: a[(a['x']==1)&(a['y']==10)]\nOut:    x   y\n     0  1  10\n\nIn: a[(a['x']==1) and (a['y']==10)]\nOut: ValueError: The truth value of an array with more than one element is ambiguous.     Use a.any() or a.all()\n""]";"[""a[(a['some_column']==some_number) & (a['some_other_column']==some_other_number)]\na[(a['some_column']==some_number) and (a['some_other_column']==some_other_number)]\na=pd.DataFrame({'x':[1,1],'y':[10,20]})\n\n\n""]";"[""import pandas as pd\na[(a['some_column']==some_number) & (a['some_other_column']==some_other_number)]\na[(a['some_column']==some_number) and (a['some_other_column']==some_other_number)]\na=pd.DataFrame({'x':[1,1],'y':[10,20]})\n\n\n""]";True;"[""import pandas as pd\na[(a['some_column']==some_number) & (a['some_other_column']==some_other_number)]\na[(a['some_column']==some_number) and (a['some_other_column']==some_other_number)]\na=pd.DataFrame({'x':[1,1],'y':[10,20]})\n\n\n""]";False;0;1;"[""name 'a' is not defined""]";['NameError'];0;1;"[""name 'a' is not defined""]";['NameError'];0;1;"[""name 'a' is not defined""]";['NameError']
672;672;672;672;2.0;0;21441259;;1;33;<python><group-by><pandas>;Pandas Groupby Range of Values;13283.0;"[""import numpy as np\nimport pandas as pd\ndf=pd.DataFrame({'A':np.random.random(20),'B':np.random.random(20)})\n\n     A         B\n0  0.383493  0.250785\n1  0.572949  0.139555\n2  0.652391  0.401983\n3  0.214145  0.696935\n4  0.848551  0.516692\n""]";"[""import numpy as np\nimport pandas as pd\ndf=pd.DataFrame({'A':np.random.random(20),'B':np.random.random(20)})\n\n     A         B\n0  0.383493  0.250785\n1  0.572949  0.139555\n2  0.652391  0.401983\n3  0.214145  0.696935\n4  0.848551  0.516692\n""]";"['groupby', 'B', '0.155', 'B', '0, -0.155, 0.155 - 0.31 ...', ""import numpy as np\nimport pandas as pd\ndf=pd.DataFrame({'A':np.random.random(20),'B':np.random.random(20)})\n\n     A         B\n0  0.383493  0.250785\n1  0.572949  0.139555\n2  0.652391  0.401983\n3  0.214145  0.696935\n4  0.848551  0.516692\n"", 'groupby', 'A']";"[""import numpy as np\nimport pandas as pd\ndf=pd.DataFrame({'A':np.random.random(20),'B':np.random.random(20)})\n\n""]";"[""import numpy as np\nimport pandas as pd\ndf=pd.DataFrame({'A':np.random.random(20),'B':np.random.random(20)})\n\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport numpy as np\nimport pandas as pd\ndf=pd.DataFrame({'A':np.random.random(20),'B':np.random.random(20)})\n\n""]";True;0;1;"[""name 'B' is not defined""]";['NameError'];0;1;"[""name 'B' is not defined""]";['NameError'];0;1;"[""name 'B' is not defined""]";['NameError']
673;673;673;673;1.0;4;21443963;;1;12;<python><pandas>;Pandas: Multilevel column names;8434.0;"["">>>  x = pd.DataFrame({'instance':['first','first','first'],'foo':['a','b','c'],'bar':rand(3)})\n>>> x = x.set_index(['instance','foo']).transpose()\n>>> x.columns\nMultiIndex\n[(u'first', u'a'), (u'first', u'b'), (u'first', u'c')]\n>>> x\ninstance     first                    \nfoo              a         b         c\nbar       0.102885  0.937838  0.907467\n                 a         b         c\nbar       0.102885  0.937838  0.907467\nx['instance'] = 'first'\nx.set_level('instance',append=True)\n""]";"["">>>  x = pd.DataFrame({'instance':['first','first','first'],'foo':['a','b','c'],'bar':rand(3)})\n>>> x = x.set_index(['instance','foo']).transpose()\n>>> x.columns\nMultiIndex\n[(u'first', u'a'), (u'first', u'b'), (u'first', u'c')]\n>>> x\ninstance     first                    \nfoo              a         b         c\nbar       0.102885  0.937838  0.907467\n"", '                 a         b         c\nbar       0.102885  0.937838  0.907467\n', ""x['instance'] = 'first'\nx.set_level('instance',append=True)\n""]";"['pandas', "">>>  x = pd.DataFrame({'instance':['first','first','first'],'foo':['a','b','c'],'bar':rand(3)})\n>>> x = x.set_index(['instance','foo']).transpose()\n>>> x.columns\nMultiIndex\n[(u'first', u'a'), (u'first', u'b'), (u'first', u'c')]\n>>> x\ninstance     first                    \nfoo              a         b         c\nbar       0.102885  0.937838  0.907467\n"", 'instance', '                 a         b         c\nbar       0.102885  0.937838  0.907467\n', ""x['instance'] = 'first'\nx.set_level('instance',append=True)\n""]";"[""MultiIndex\n[(u'first', u'a'), (u'first', u'b'), (u'first', u'c')]\nx['instance'] = 'first'\nx.set_level('instance',append=True)\n""]";"[""MultiIndex\n[(u'first', u'a'), (u'first', u'b'), (u'first', u'c')]\nx['instance'] = 'first'\nx.set_level('instance',append=True)\n""]";False;"[""import pandas as pd\nMultiIndex\n[(u'first', u'a'), (u'first', u'b'), (u'first', u'c')]\nx['instance'] = 'first'\nx.set_level('instance',append=True)\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
674;674;674;674;1.0;0;21463589;;1;13;<python><pandas><copy>;Pandas: Chained assignments;5402.0;"['data[\'amount\'] = data[\'amount\'].astype(float)\n\ndata[""amount""].fillna(data.groupby(""num"")[""amount""].transform(""mean""), inplace=True)\n\ndata[""amount""].fillna(mean_avg, inplace=True)\ndata[\'amount\'] = data.apply(lambda row: function1(row,date,qty), axis=1) \ndata[\'amount\'] = data[\'amount\'].astype(float)\n\ndef function1(row,date,qty):\n    try:\n        if(row[\'currency\'] == \'A\'):\n            result = row[qty]\n        else:\n            rate = lookup[lookup[\'Date\']==row[date]][row[\'currency\'] ]\n            result = float(rate) * float(row[qty])\n        return result\n    except ValueError: # generic exception clause\n        print ""The current row causes an exception:""\n']";"['data[\'amount\'] = data[\'amount\'].astype(float)\n\ndata[""amount""].fillna(data.groupby(""num"")[""amount""].transform(""mean""), inplace=True)\n\ndata[""amount""].fillna(mean_avg, inplace=True)\n', 'data[\'amount\'] = data.apply(lambda row: function1(row,date,qty), axis=1) \ndata[\'amount\'] = data[\'amount\'].astype(float)\n\ndef function1(row,date,qty):\n    try:\n        if(row[\'currency\'] == \'A\'):\n            result = row[qty]\n        else:\n            rate = lookup[lookup[\'Date\']==row[date]][row[\'currency\'] ]\n            result = float(rate) * float(row[qty])\n        return result\n    except ValueError: # generic exception clause\n        print ""The current row causes an exception:""\n']";"['.ix()', '.iloc()', '.loc()', 'SettingWithCopyWarning', 'data', 'amount', 'data[\'amount\'] = data[\'amount\'].astype(float)\n\ndata[""amount""].fillna(data.groupby(""num"")[""amount""].transform(""mean""), inplace=True)\n\ndata[""amount""].fillna(mean_avg, inplace=True)\n', 'data[\'amount\'] = data.apply(lambda row: function1(row,date,qty), axis=1) \ndata[\'amount\'] = data[\'amount\'].astype(float)\n\ndef function1(row,date,qty):\n    try:\n        if(row[\'currency\'] == \'A\'):\n            result = row[qty]\n        else:\n            rate = lookup[lookup[\'Date\']==row[date]][row[\'currency\'] ]\n            result = float(rate) * float(row[qty])\n        return result\n    except ValueError: # generic exception clause\n        print ""The current row causes an exception:""\n']";"['data[\'amount\'] = data[\'amount\'].astype(float)\n\ndata[""amount""].fillna(data.groupby(""num"")[""amount""].transform(""mean""), inplace=True)\n\ndata[""amount""].fillna(mean_avg, inplace=True)\ndata[\'amount\'] = data.apply(lambda row: function1(row,date,qty), axis=1) \ndata[\'amount\'] = data[\'amount\'].astype(float)\n\n']";"['data[\'amount\'] = data[\'amount\'].astype(float)\n\ndata[""amount""].fillna(data.groupby(""num"")[""amount""].transform(""mean""), inplace=True)\n\ndata[""amount""].fillna(mean_avg, inplace=True)\ndata[\'amount\'] = data.apply(lambda row: function1(row,date,qty), axis=1) \ndata[\'amount\'] = data[\'amount\'].astype(float)\n\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\ndata[\'amount\'] = data[\'amount\'].astype(float)\n\ndata[""amount""].fillna(data.groupby(""num"")[""amount""].transform(""mean""), inplace=True)\n\ndata[""amount""].fillna(mean_avg, inplace=True)\ndata[\'amount\'] = data.apply(lambda row: function1(row,date,qty), axis=1) \ndata[\'amount\'] = data[\'amount\'].astype(float)\n\n']";True;0;1;"[""name 'data' is not defined""]";['NameError'];0;1;"[""name 'data' is not defined""]";['NameError'];0;1;"[""'amount'""]";['KeyError']
675;675;675;675;5.0;0;21487329;;1;62;<python><matplotlib><pandas>;Add x and y labels to a pandas plot;62710.0;"[""import pandas as pd\nvalues = [[1,2], [2,5]]\ndf2 = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])\ndf2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')\n""]";"[""import pandas as pd\nvalues = [[1,2], [2,5]]\ndf2 = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])\ndf2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')\n""]";"[""import pandas as pd\nvalues = [[1,2], [2,5]]\ndf2 = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])\ndf2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')\n""]";"[""import pandas as pd\nvalues = [[1,2], [2,5]]\ndf2 = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])\ndf2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')\n""]";"[""import pandas as pd\nvalues = [[1,2], [2,5]]\ndf2 = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])\ndf2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\nvalues = [[1,2], [2,5]]\ndf2 = pd.DataFrame(values, columns=['Type A', 'Type B'], index=['Index 1','Index 2'])\ndf2.plot(lw=2,colormap='jet',marker='.',markersize=10,title='Video streaming dropout by category')\n""]";True;0;3;"[""name 'df2' is not defined"", ""No module named 'matplotlib'"", ""name '_converter' is not defined""]";['NameError', 'ImportError', 'NameError'];0;3;"[""name 'df2' is not defined"", ""No module named 'matplotlib'"", ""name '_converter' is not defined""]";['NameError', 'ImportError', 'NameError'];0;3;"[""name '_converter' is not defined"", ""No module named 'matplotlib'"", ""name '_converter' is not defined""]";['NameError', 'ImportError', 'NameError']
676;676;676;676;1.0;3;21488085;;1;11;<python><matplotlib><pandas>;Pandas graphing a timeseries, with vertical lines at selected dates;6872.0;"[""In [555]:\ncum_edits.head()\nOut[555]:\n2001-08-31 23:37:28    1\n2001-09-01 05:09:28    2\n2001-09-18 10:01:17    3\n2001-10-27 06:52:45    4\n2001-10-27 07:01:45    5\nName: edits, dtype: int64\nIn [565]:\ncum_edits.tail()\nOut[565]:\n2014-01-29 16:05:15    53254\n2014-01-29 16:07:09    53255\n2014-01-29 16:11:43    53256\n2014-01-29 18:09:44    53257\n2014-01-29 18:12:09    53258\nName: edits, dtype: int64\nIn [567]:\n\ncum_edits.plot()\n\nOut[567]:\n\n<matplotlib.axes.AxesSubplot at 0x1359c810>\nIn [568]:\n\ndates\n\nOut[568]:\n\n[Timestamp('2006-06-04 04:46:22', tz=None),\n Timestamp('2007-01-28 23:53:02', tz=None),\n Timestamp('2007-09-16 10:52:02', tz=None),\n Timestamp('2008-04-28 21:20:40', tz=None),\n Timestamp('2009-04-12 22:07:13', tz=None),\n Timestamp('2010-04-09 18:45:37', tz=None),\n Timestamp('2011-03-28 23:38:12', tz=None),\n Timestamp('2012-05-24 13:44:35', tz=None),\n Timestamp('2013-03-05 17:57:29', tz=None),\n Timestamp('2014-01-29 16:05:15', tz=None)]\n""]";"['In [555]:\ncum_edits.head()\nOut[555]:\n2001-08-31 23:37:28    1\n2001-09-01 05:09:28    2\n2001-09-18 10:01:17    3\n2001-10-27 06:52:45    4\n2001-10-27 07:01:45    5\nName: edits, dtype: int64\nIn [565]:\ncum_edits.tail()\nOut[565]:\n2014-01-29 16:05:15    53254\n2014-01-29 16:07:09    53255\n2014-01-29 16:11:43    53256\n2014-01-29 18:09:44    53257\n2014-01-29 18:12:09    53258\nName: edits, dtype: int64\n', 'In [567]:\n\ncum_edits.plot()\n\nOut[567]:\n\n<matplotlib.axes.AxesSubplot at 0x1359c810>\n', ""In [568]:\n\ndates\n\nOut[568]:\n\n[Timestamp('2006-06-04 04:46:22', tz=None),\n Timestamp('2007-01-28 23:53:02', tz=None),\n Timestamp('2007-09-16 10:52:02', tz=None),\n Timestamp('2008-04-28 21:20:40', tz=None),\n Timestamp('2009-04-12 22:07:13', tz=None),\n Timestamp('2010-04-09 18:45:37', tz=None),\n Timestamp('2011-03-28 23:38:12', tz=None),\n Timestamp('2012-05-24 13:44:35', tz=None),\n Timestamp('2013-03-05 17:57:29', tz=None),\n Timestamp('2014-01-29 16:05:15', tz=None)]\n""]";"['In [555]:\ncum_edits.head()\nOut[555]:\n2001-08-31 23:37:28    1\n2001-09-01 05:09:28    2\n2001-09-18 10:01:17    3\n2001-10-27 06:52:45    4\n2001-10-27 07:01:45    5\nName: edits, dtype: int64\nIn [565]:\ncum_edits.tail()\nOut[565]:\n2014-01-29 16:05:15    53254\n2014-01-29 16:07:09    53255\n2014-01-29 16:11:43    53256\n2014-01-29 18:09:44    53257\n2014-01-29 18:12:09    53258\nName: edits, dtype: int64\n', 'In [567]:\n\ncum_edits.plot()\n\nOut[567]:\n\n<matplotlib.axes.AxesSubplot at 0x1359c810>\n', 'total_edits/n ; e.g. n=10', ""In [568]:\n\ndates\n\nOut[568]:\n\n[Timestamp('2006-06-04 04:46:22', tz=None),\n Timestamp('2007-01-28 23:53:02', tz=None),\n Timestamp('2007-09-16 10:52:02', tz=None),\n Timestamp('2008-04-28 21:20:40', tz=None),\n Timestamp('2009-04-12 22:07:13', tz=None),\n Timestamp('2010-04-09 18:45:37', tz=None),\n Timestamp('2011-03-28 23:38:12', tz=None),\n Timestamp('2012-05-24 13:44:35', tz=None),\n Timestamp('2013-03-05 17:57:29', tz=None),\n Timestamp('2014-01-29 16:05:15', tz=None)]\n"", 'axvline()', ""plt.axvline(x=0.5, color='r')"", '%pylab inline', 'cum_edits.plot()']";['cum_edits.head()\ncum_edits.tail()\ncum_edits.plot()\n\ndates\n\n'];['cum_edits.head()\ncum_edits.tail()\ncum_edits.plot()\n\ndates\n\n'];False;['import pandas as pd\ncum_edits.head()\ncum_edits.tail()\ncum_edits.plot()\n\ndates\n\n'];False;0;1;"[""name 'cum_edits' is not defined""]";['NameError'];0;1;"[""name 'cum_edits' is not defined""]";['NameError'];0;1;"[""name 'cum_edits' is not defined""]";['NameError']
677;677;677;677;1.0;1;21494030;;1;14;<python><json><pandas>;Create a Pandas DataFrame from deeply nested JSON;11613.0;"['{""intervals"": [\n{\npivots: ""Jane Smith"",\n""series"": [\n    {\n        ""interval_id"": 0,\n        ""p_value"": 1\n       },\n     {\n         ""interval_id"": 1,\n         ""p_value"": 1.1162791357932633e-8\n     },\n   {\n        ""interval_id"": 2,\n        ""p_value"": 0.0000028675012051504467\n     }\n    ],\n   },\n  {\n\n""pivots"": ""Bob Smith"",\n  ""series"": [\n       {\n            ""interval_id"": 0,\n            ""p_value"": 1\n           },\n         {\n             ""interval_id"": 1,\n            ""p_value"": 1.1162791357932633e-8\n         },\n       {\n            ""interval_id"": 2,\n            ""p_value"": 0.0000028675012051504467\n         }\n       ]\n     }\n    ]\n }\nActor Interval_id Interval_id Interval_id ... \nJane Smith      1         1.1162        0.00000 ... \nBob Smith       1         1.1162        0.00000 ... \nimport requests as r\nimport pandas as pd\nactor_data = r.get(""url/to/data"").json[\'data\'][\'intervals\']\ndf = pd.DataFrame(actor_data)\n<bound method DataFrame.describe of  pivots             Series\n0           Jane Smith  [{u\'p_value\': 1.0, u\'interval_id\': 0}, {u\'p_va...\n1           Bob Smith  [{u\'p_value\': 1.0, u\'interval_id\': 0}, {u\'p_va...\n.\n.\n.\npvalue_list = [i[\'p_value\'] for i in json_data[\'series\']]\nvalue_list = []\nfor i in pvalue_list:\n    pvs = [j[\'p_value\'] for j in i]\n    value_list = value_list.append(pvs)\nreturn value_list\ndef get_hypthesis_data():\n    raw_data = r.get(""/url/to/data"").json()[\'data\']\n    actor_dict = {}\n    for actor_series in raw_data[\'intervals\']:\n        actor = actor_series[\'pivots\']\n        p_values = []\n        for interval in actor_series[\'series\']:\n            p_values.append(interval[\'p_value\'])\n        actor_dict[actor] = p_values\n    return pd.DataFrame(actor_dict).T\n']";"['{""intervals"": [\n{\npivots: ""Jane Smith"",\n""series"": [\n    {\n        ""interval_id"": 0,\n        ""p_value"": 1\n       },\n     {\n         ""interval_id"": 1,\n         ""p_value"": 1.1162791357932633e-8\n     },\n   {\n        ""interval_id"": 2,\n        ""p_value"": 0.0000028675012051504467\n     }\n    ],\n   },\n  {\n\n""pivots"": ""Bob Smith"",\n  ""series"": [\n       {\n            ""interval_id"": 0,\n            ""p_value"": 1\n           },\n         {\n             ""interval_id"": 1,\n            ""p_value"": 1.1162791357932633e-8\n         },\n       {\n            ""interval_id"": 2,\n            ""p_value"": 0.0000028675012051504467\n         }\n       ]\n     }\n    ]\n }\n', 'Actor Interval_id Interval_id Interval_id ... \nJane Smith      1         1.1162        0.00000 ... \nBob Smith       1         1.1162        0.00000 ... \n', 'import requests as r\nimport pandas as pd\nactor_data = r.get(""url/to/data"").json[\'data\'][\'intervals\']\ndf = pd.DataFrame(actor_data)\n', ""<bound method DataFrame.describe of  pivots             Series\n0           Jane Smith  [{u'p_value': 1.0, u'interval_id': 0}, {u'p_va...\n1           Bob Smith  [{u'p_value': 1.0, u'interval_id': 0}, {u'p_va...\n.\n.\n.\n"", ""pvalue_list = [i['p_value'] for i in json_data['series']]\n"", ""value_list = []\nfor i in pvalue_list:\n    pvs = [j['p_value'] for j in i]\n    value_list = value_list.append(pvs)\nreturn value_list\n"", 'def get_hypthesis_data():\n    raw_data = r.get(""/url/to/data"").json()[\'data\']\n    actor_dict = {}\n    for actor_series in raw_data[\'intervals\']:\n        actor = actor_series[\'pivots\']\n        p_values = []\n        for interval in actor_series[\'series\']:\n            p_values.append(interval[\'p_value\'])\n        actor_dict[actor] = p_values\n    return pd.DataFrame(actor_dict).T\n']";"['{""intervals"": [\n{\npivots: ""Jane Smith"",\n""series"": [\n    {\n        ""interval_id"": 0,\n        ""p_value"": 1\n       },\n     {\n         ""interval_id"": 1,\n         ""p_value"": 1.1162791357932633e-8\n     },\n   {\n        ""interval_id"": 2,\n        ""p_value"": 0.0000028675012051504467\n     }\n    ],\n   },\n  {\n\n""pivots"": ""Bob Smith"",\n  ""series"": [\n       {\n            ""interval_id"": 0,\n            ""p_value"": 1\n           },\n         {\n             ""interval_id"": 1,\n            ""p_value"": 1.1162791357932633e-8\n         },\n       {\n            ""interval_id"": 2,\n            ""p_value"": 0.0000028675012051504467\n         }\n       ]\n     }\n    ]\n }\n', 'Actor Interval_id Interval_id Interval_id ... \nJane Smith      1         1.1162        0.00000 ... \nBob Smith       1         1.1162        0.00000 ... \n', 'Pivots', 'interval_id', 'p_value', 'series', 'import requests as r\nimport pandas as pd\nactor_data = r.get(""url/to/data"").json[\'data\'][\'intervals\']\ndf = pd.DataFrame(actor_data)\n', 'actor_data', 'pivots.values()', ""<bound method DataFrame.describe of  pivots             Series\n0           Jane Smith  [{u'p_value': 1.0, u'interval_id': 0}, {u'p_va...\n1           Bob Smith  [{u'p_value': 1.0, u'interval_id': 0}, {u'p_va...\n.\n.\n.\n"", 'series', 'series', ""pvalue_list = [i['p_value'] for i in json_data['series']]\n"", ""value_list = []\nfor i in pvalue_list:\n    pvs = [j['p_value'] for j in i]\n    value_list = value_list.append(pvs)\nreturn value_list\n"", 'def get_hypthesis_data():\n    raw_data = r.get(""/url/to/data"").json()[\'data\']\n    actor_dict = {}\n    for actor_series in raw_data[\'intervals\']:\n        actor = actor_series[\'pivots\']\n        p_values = []\n        for interval in actor_series[\'series\']:\n            p_values.append(interval[\'p_value\'])\n        actor_dict[actor] = p_values\n    return pd.DataFrame(actor_dict).T\n']";"['\nimport requests as r\nimport pandas as pd\nactor_data = r.get(""url/to/data"").json[\'data\'][\'intervals\']\ndf = pd.DataFrame(actor_data)\npvalue_list = [i[\'p_value\'] for i in json_data[\'series\']]\nvalue_list = []\nreturn value_list\n']";"['\nimport requests as r\nimport pandas as pd\nactor_data = r.get(""url/to/data"").json[\'data\'][\'intervals\']\ndf = pd.DataFrame(actor_data)\npvalue_list = [i[\'p_value\'] for i in json_data[\'series\']]\nvalue_list = []\nreturn value_list\n']";False;"['import pandas as pd\n\nimport requests as r\nimport pandas as pd\nactor_data = r.get(""url/to/data"").json[\'data\'][\'intervals\']\ndf = pd.DataFrame(actor_data)\npvalue_list = [i[\'p_value\'] for i in json_data[\'series\']]\nvalue_list = []\nreturn value_list\n']";False;0;1;"[""name 'res' is not defined""]";['NameError'];0;1;"[""name 'res' is not defined""]";['NameError'];0;1;"[""name 'res' is not defined""]";['NameError']
678;678;678;678;3.0;0;21546739;;1;15;<python><io><pandas>;Load data from txt with pandas;57146.0;"[""import pandas as pd\n\ndata = pd.read_csv('output_list.txt', header = None)\nprint data\n""]";"[""import pandas as pd\n\ndata = pd.read_csv('output_list.txt', header = None)\nprint data\n""]";"[""import pandas as pd\n\ndata = pd.read_csv('output_list.txt', header = None)\nprint data\n"", '1 0 2000.0 70.2836942112 1347.28369421 /file_address.txt', 'data[i,j]']";"[""import pandas as pd\n\ndata = pd.read_csv('output_list.txt', header = None)\n""]";"[""import pandas as pd\n\ndata = pd.read_csv('output_list.txt', header = None)\n""]";False;"[""import pandas as pd\nimport pandas as pd\n\ndata = pd.read_csv('output_list.txt', header = None)\n""]";False;0;2;"[""name 'pd' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];0;2;"[""File b'output_list.txt' does not exist"", ""File b'output_list.txt' does not exist""]";['FileNotFoundError', 'FileNotFoundError'];0;2;"[""File b'output_list.txt' does not exist"", ""File b'output_list.txt' does not exist""]";['FileNotFoundError', 'FileNotFoundError']
679;679;679;679;2.0;3;21567842;;1;18;<python><numpy><pandas>;Is there a difference in computation for Numpy vs Pandas?;13667.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
680;680;680;680;2.0;0;21606987;;1;26;<pandas>;How can I strip the whitespace from Pandas DataFrame headers?;14508.0;[''];[];"['df.columns', ""Index(['Year', 'Month ', 'Value'])"", 'df[""Month""]']";[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
681;681;681;681;2.0;1;21608228;;1;26;<python><replace><pandas><conditional>;Conditional Replace Pandas;30186.0;['df[df.my_channel > 20000].my_channel = 0\ndf2 = df.my_channel \n\ndf2[df2 > 20000] = 0\n'];['df[df.my_channel > 20000].my_channel = 0\n', 'df2 = df.my_channel \n\ndf2[df2 > 20000] = 0\n'];['df[df.my_channel > 20000].my_channel = 0\n', 'df2 = df.my_channel \n\ndf2[df2 > 20000] = 0\n'];['df[df.my_channel > 20000].my_channel = 0\ndf2 = df.my_channel \n\ndf2[df2 > 20000] = 0\n'];['df[df.my_channel > 20000].my_channel = 0\ndf2 = df.my_channel \n\ndf2[df2 > 20000] = 0\n'];False;['import pandas as pd\ndf = pd.DataFrame()\ndf[df.my_channel > 20000].my_channel = 0\ndf2 = df.my_channel \n\ndf2[df2 > 20000] = 0\n'];True;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"['\n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated', ""'DataFrame' object has no attribute 'my_channel'""]";['DeprecationWarning', 'AttributeError']
682;682;682;682;4.0;0;21654635;;1;42;<python><matplotlib><pandas>;Scatter plots in Pandas/Pyplot: How to plot by category;44949.0;"[""import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range('2010-01-01', freq = 'M', periods = 10), columns = ('one', 'two', 'three'))\ndf['key1'] = (4,4,4,6,6,6,8,8,8,8)\nfig1 = plt.figure(1)\nax1 = fig1.add_subplot(111)\nax1.scatter(df['one'], df['two'], marker = 'o', c = df['key1'], alpha = 0.8)\nplt.show()\n""]";"[""import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range('2010-01-01', freq = 'M', periods = 10), columns = ('one', 'two', 'three'))\ndf['key1'] = (4,4,4,6,6,6,8,8,8,8)\nfig1 = plt.figure(1)\nax1 = fig1.add_subplot(111)\nax1.scatter(df['one'], df['two'], marker = 'o', c = df['key1'], alpha = 0.8)\nplt.show()\n""]";"[""import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range('2010-01-01', freq = 'M', periods = 10), columns = ('one', 'two', 'three'))\ndf['key1'] = (4,4,4,6,6,6,8,8,8,8)\nfig1 = plt.figure(1)\nax1 = fig1.add_subplot(111)\nax1.scatter(df['one'], df['two'], marker = 'o', c = df['key1'], alpha = 0.8)\nplt.show()\n""]";"[""import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range('2010-01-01', freq = 'M', periods = 10), columns = ('one', 'two', 'three'))\ndf['key1'] = (4,4,4,6,6,6,8,8,8,8)\nfig1 = plt.figure(1)\nax1 = fig1.add_subplot(111)\nax1.scatter(df['one'], df['two'], marker = 'o', c = df['key1'], alpha = 0.8)\nplt.show()\n""]";"[""import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range('2010-01-01', freq = 'M', periods = 10), columns = ('one', 'two', 'three'))\ndf['key1'] = (4,4,4,6,6,6,8,8,8,8)\nfig1 = plt.figure(1)\nax1 = fig1.add_subplot(111)\nax1.scatter(df['one'], df['two'], marker = 'o', c = df['key1'], alpha = 0.8)\nplt.show()\n""]";False;"[""import pandas as pd\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range('2010-01-01', freq = 'M', periods = 10), columns = ('one', 'two', 'three'))\ndf['key1'] = (4,4,4,6,6,6,8,8,8,8)\nfig1 = plt.figure(1)\nax1 = fig1.add_subplot(111)\nax1.scatter(df['one'], df['two'], marker = 'o', c = df['key1'], alpha = 0.8)\nplt.show()\n""]";False;0;3;"[""name 'pd' is not defined"", ""No module named 'matplotlib'"", ""No module named 'seaborn'""]";['NameError', 'ImportError', 'ImportError'];0;3;"[""name 'np' is not defined"", ""No module named 'matplotlib'"", ""No module named 'seaborn'""]";['NameError', 'ImportError', 'ImportError'];0;3;"[""name 'np' is not defined"", ""No module named 'matplotlib'"", ""No module named 'seaborn'""]";['NameError', 'ImportError', 'ImportError']
683;683;683;683;3.0;0;21689423;;1;16;<python><python-2.7><pandas>;pandas attribute error : no attribute 'Factor' found;6525.0;"['File ""test_iris_with_rf.py"", line 11, in <module>\n    df[\'species\'] = pd.Factor(iris.target, iris.target_names)\nAttributeError: \'module\' object has no attribute \'Factor\'\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport numpy as np\n\niris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\nprint df\nprint iris.target_names\ndf[\'is_train\'] = np.random.uniform(0, 1, len(df)) <= .75\n\ndf[\'species\'] = pd.Factor(iris.target, iris.target_names)\n\ndf.head()\n']";"['File ""test_iris_with_rf.py"", line 11, in <module>\n    df[\'species\'] = pd.Factor(iris.target, iris.target_names)\nAttributeError: \'module\' object has no attribute \'Factor\'\n', ""from sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport numpy as np\n\niris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\nprint df\nprint iris.target_names\ndf['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n\ndf['species'] = pd.Factor(iris.target, iris.target_names)\n\ndf.head()\n""]";"['File ""test_iris_with_rf.py"", line 11, in <module>\n    df[\'species\'] = pd.Factor(iris.target, iris.target_names)\nAttributeError: \'module\' object has no attribute \'Factor\'\n', ""from sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport numpy as np\n\niris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\nprint df\nprint iris.target_names\ndf['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n\ndf['species'] = pd.Factor(iris.target, iris.target_names)\n\ndf.head()\n""]";"[""from sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport numpy as np\n\niris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n\ndf['species'] = pd.Factor(iris.target, iris.target_names)\n\ndf.head()\n""]";"[""from sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport numpy as np\n\niris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n\ndf['species'] = pd.Factor(iris.target, iris.target_names)\n\ndf.head()\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport numpy as np\n\niris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n\ndf['species'] = pd.Factor(iris.target, iris.target_names)\n\ndf.head()\n""]";True;0;2;"[""name 'pd' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'iris' is not defined"", ""name 'iris' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'iris' is not defined"", ""name 'iris' is not defined""]";['NameError', 'NameError']
684;684;684;684;2.0;0;21720022;;1;17;<python><pandas><dataframe><data-cleansing>;Find all columns of dataframe in Pandas whose type is float, or a particular type?;12649.0;"['df.fillna(\'unknown\') #getting error ""ValueError: could not convert string to float:""\nfor col in df.columns[<dtype == object>]:\n    df[col] = df[col].fillna(""unknown"")\n for col in df.columns:\n        if (df[col].dtype == dtype(\'O\')): # for object type\n            df[col] = df[col].fillna(\'\') \n            # still puzzled, only empty string works as replacement, \'unknown\' would not work for certain value leading to error of ""ValueError: Error parsing datetime string ""unknown"" at position 0"" \n']";"['df.fillna(\'unknown\') #getting error ""ValueError: could not convert string to float:""\n', 'for col in df.columns[<dtype == object>]:\n    df[col] = df[col].fillna(""unknown"")\n', ' for col in df.columns:\n        if (df[col].dtype == dtype(\'O\')): # for object type\n            df[col] = df[col].fillna(\'\') \n            # still puzzled, only empty string works as replacement, \'unknown\' would not work for certain value leading to error of ""ValueError: Error parsing datetime string ""unknown"" at position 0"" \n']";"['df.fillna(\'unknown\') #getting error ""ValueError: could not convert string to float:""\n', 'for col in df.columns[<dtype == object>]:\n    df[col] = df[col].fillna(""unknown"")\n', ' for col in df.columns:\n        if (df[col].dtype == dtype(\'O\')): # for object type\n            df[col] = df[col].fillna(\'\') \n            # still puzzled, only empty string works as replacement, \'unknown\' would not work for certain value leading to error of ""ValueError: Error parsing datetime string ""unknown"" at position 0"" \n']";"['df.fillna(\'unknown\') #getting error ""ValueError: could not convert string to float:""\n            # still puzzled, only empty string works as replacement, \'unknown\' would not work for certain value leading to error of ""ValueError: Error parsing datetime string ""unknown"" at position 0"" \n']";"['df.fillna(\'unknown\') #getting error ""ValueError: could not convert string to float:""\n            # still puzzled, only empty string works as replacement, \'unknown\' would not work for certain value leading to error of ""ValueError: Error parsing datetime string ""unknown"" at position 0"" \n']";False;"['import pandas as pd\ndf = pd.DataFrame()\ndf.fillna(\'unknown\') #getting error ""ValueError: could not convert string to float:""\n            # still puzzled, only empty string works as replacement, \'unknown\' would not work for certain value leading to error of ""ValueError: Error parsing datetime string ""unknown"" at position 0"" \n']";True;0;2;"[""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'np' is not defined""]";['Sucess', 'NameError']
685;685;685;685;2.0;0;21733893;;1;22;<python><lambda>;Pandas dataframe add a field based on multiple if statements;32357.0;"[""import pandas as pd\nimport numpy as n\n\nd = {'Age' : pd.Series([36., 42., 6., 66., 38.]) }\n\ndf = pd.DataFrame(d)\n\ndf['Age_Group'] =  df['Age'].map(lambda x: '<18' if x < 19 else '>18')\n\nprint(df)\n""]";"[""import pandas as pd\nimport numpy as n\n\nd = {'Age' : pd.Series([36., 42., 6., 66., 38.]) }\n\ndf = pd.DataFrame(d)\n\ndf['Age_Group'] =  df['Age'].map(lambda x: '<18' if x < 19 else '>18')\n\nprint(df)\n""]";"[""if age < 18 then 'under 18' elif age < 40 then 'under 40' else '>40'"", ""import pandas as pd\nimport numpy as n\n\nd = {'Age' : pd.Series([36., 42., 6., 66., 38.]) }\n\ndf = pd.DataFrame(d)\n\ndf['Age_Group'] =  df['Age'].map(lambda x: '<18' if x < 19 else '>18')\n\nprint(df)\n""]";"[""import pandas as pd\nimport numpy as n\n\nd = {'Age' : pd.Series([36., 42., 6., 66., 38.]) }\n\ndf = pd.DataFrame(d)\n\ndf['Age_Group'] =  df['Age'].map(lambda x: '<18' if x < 19 else '>18')\n\nprint(df)\n""]";"[""import pandas as pd\nimport numpy as n\n\nd = {'Age' : pd.Series([36., 42., 6., 66., 38.]) }\n\ndf = pd.DataFrame(d)\n\ndf['Age_Group'] =  df['Age'].map(lambda x: '<18' if x < 19 else '>18')\n\nprint(df)\n""]";False;"[""import pandas as pd\nimport pandas as pd\nimport numpy as n\n\nd = {'Age' : pd.Series([36., 42., 6., 66., 38.]) }\n\ndf = pd.DataFrame(d)\n\ndf['Age_Group'] =  df['Age'].map(lambda x: '<18' if x < 19 else '>18')\n\nprint(df)\n""]";False;1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""'Age'"", 'Sucess']";['KeyError', 'Sucess']
686;686;686;686;6.0;3;21738566;;1;17;<python><date><datetime><formatting><pandas>;"How to set a variable to be ""Today's"" date in Python/Pandas";42573.0;"[""dt.date.today\nimport datetime as dt     \ndate = dt.date.today\nprint date\n <built-in method today of type object at 0x000000001E2658B0>\n\n Df['Date'] = date\n""]";"['dt.date.today\n', ""import datetime as dt     \ndate = dt.date.today\nprint date\n <built-in method today of type object at 0x000000001E2658B0>\n\n Df['Date'] = date\n""]";"['dt.date.today\n', ""import datetime as dt     \ndate = dt.date.today\nprint date\n <built-in method today of type object at 0x000000001E2658B0>\n\n Df['Date'] = date\n""]";['dt.date.today\nimport datetime as dt     \ndate = dt.date.today\n\n'];['dt.date.today\nimport datetime as dt     \ndate = dt.date.today\n\n'];False;['import pandas as pd\ndt.date.today\nimport datetime as dt     \ndate = dt.date.today\n\n'];False;1;2;"['Sucess', ""name 'Timestamp' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'Timestamp' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'Timestamp' is not defined""]";['Sucess', 'NameError']
687;687;687;687;3.0;1;21752399;;1;18;<python><pandas>;Pandas dataframe total row;14345.0;"[""     foo  bar  qux\n0    a    1    3.14\n1    b    3    2.72\n2    c    2    1.62\n3    d    9    1.41\n4    e    3    0.58\n     foo  bar  qux\n0    a    1    3.14\n1    b    3    2.72\n2    c    2    1.62\n3    d    9    1.41\n4    e    3    0.58\n5    tot  15   9.47\ntot_row = pd.DataFrame(df.sum()).T\ntot_row['foo'] = 'tot'\ntot_row.dtypes:\n     foo    object\n     bar    object\n     qux    object\nbaz = 2*tot_row['qux'] + 3*tot_row['bar']\n""]";"['     foo  bar  qux\n0    a    1    3.14\n1    b    3    2.72\n2    c    2    1.62\n3    d    9    1.41\n4    e    3    0.58\n', '     foo  bar  qux\n0    a    1    3.14\n1    b    3    2.72\n2    c    2    1.62\n3    d    9    1.41\n4    e    3    0.58\n5    tot  15   9.47\n', ""tot_row = pd.DataFrame(df.sum()).T\ntot_row['foo'] = 'tot'\ntot_row.dtypes:\n     foo    object\n     bar    object\n     qux    object\n"", ""baz = 2*tot_row['qux'] + 3*tot_row['bar']\n""]";"['     foo  bar  qux\n0    a    1    3.14\n1    b    3    2.72\n2    c    2    1.62\n3    d    9    1.41\n4    e    3    0.58\n', '     foo  bar  qux\n0    a    1    3.14\n1    b    3    2.72\n2    c    2    1.62\n3    d    9    1.41\n4    e    3    0.58\n5    tot  15   9.47\n', 'sum', ""tot_row = pd.DataFrame(df.sum()).T\ntot_row['foo'] = 'tot'\ntot_row.dtypes:\n     foo    object\n     bar    object\n     qux    object\n"", ""baz = 2*tot_row['qux'] + 3*tot_row['bar']\n""]";"[""tot_row = pd.DataFrame(df.sum()).T\ntot_row['foo'] = 'tot'\nbaz = 2*tot_row['qux'] + 3*tot_row['bar']\n""]";"[""import pandas as pd\ntot_row = pd.DataFrame(df.sum()).T\ntot_row['foo'] = 'tot'\nbaz = 2*tot_row['qux'] + 3*tot_row['bar']\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\ntot_row = pd.DataFrame(df.sum()).T\ntot_row['foo'] = 'tot'\nbaz = 2*tot_row['qux'] + 3*tot_row['bar']\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'qux'""]";['KeyError']
688;688;688;688;1.0;0;21767900;;1;13;<python><pandas>;How to move pandas data from index to column after multiple groupby;8218.0;"[""In [8]:  dfalph.head()\n\nOut[8]:         token    year    uses  books\n         386    xanthos  1830    3     3\n         387    xanthos  1840    1     1\n         388    xanthos  1840    2     2\n         389    xanthos  1868    2     2\n         390    xanthos  1875    1     1\nIn [63]:  dfalph = dfalph[['token', 'year', 'uses', 'books']].groupby(['token', 'year']).agg([np.sum])\n          dfalph.columns = dfalph.columns.droplevel(1)\n          dfalph.head()\n\nOut[63]:                 uses  books\n          token    year     \n          xanthos  1830  3     3\n                   1840  3     3\n                   1867  2     2\n                   1868  2     2\n                   1875  1     1\n""]";"['In [8]:  dfalph.head()\n\nOut[8]:         token    year    uses  books\n         386    xanthos  1830    3     3\n         387    xanthos  1840    1     1\n         388    xanthos  1840    2     2\n         389    xanthos  1868    2     2\n         390    xanthos  1875    1     1\n', ""In [63]:  dfalph = dfalph[['token', 'year', 'uses', 'books']].groupby(['token', 'year']).agg([np.sum])\n          dfalph.columns = dfalph.columns.droplevel(1)\n          dfalph.head()\n\nOut[63]:                 uses  books\n          token    year     \n          xanthos  1830  3     3\n                   1840  3     3\n                   1867  2     2\n                   1868  2     2\n                   1875  1     1\n""]";"['In [8]:  dfalph.head()\n\nOut[8]:         token    year    uses  books\n         386    xanthos  1830    3     3\n         387    xanthos  1840    1     1\n         388    xanthos  1840    2     2\n         389    xanthos  1868    2     2\n         390    xanthos  1875    1     1\n', ""In [63]:  dfalph = dfalph[['token', 'year', 'uses', 'books']].groupby(['token', 'year']).agg([np.sum])\n          dfalph.columns = dfalph.columns.droplevel(1)\n          dfalph.head()\n\nOut[63]:                 uses  books\n          token    year     \n          xanthos  1830  3     3\n                   1840  3     3\n                   1867  2     2\n                   1868  2     2\n                   1875  1     1\n""]";"[""dfalph.head()\n\ndfalph = dfalph[['token', 'year', 'uses', 'books']].groupby(['token', 'year']).agg([np.sum])\n\n""]";"[""dfalph.head()\n\ndfalph = dfalph[['token', 'year', 'uses', 'books']].groupby(['token', 'year']).agg([np.sum])\n\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndfalph.head()\n\ndfalph = dfalph[['token', 'year', 'uses', 'books']].groupby(['token', 'year']).agg([np.sum])\n\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
689;689;689;689;4.0;0;21771133;;1;18;<python><pandas><dataframe>;Finding non-numeric rows in dataframe in pandas?;26480.0;"['df = pandas.DataFrame({""item"": [""a"", ""b"", ""c"", ""d"", ""e""], ""a"": [1,2,3,""bad"",5], ""b"":[0.1,0.2,0.3,0.4,0.5]})\ndf = df.set_index(""item"")\n']";"['df = pandas.DataFrame({""item"": [""a"", ""b"", ""c"", ""d"", ""e""], ""a"": [1,2,3,""bad"",5], ""b"":[0.1,0.2,0.3,0.4,0.5]})\ndf = df.set_index(""item"")\n']";"['df = pandas.DataFrame({""item"": [""a"", ""b"", ""c"", ""d"", ""e""], ""a"": [1,2,3,""bad"",5], ""b"":[0.1,0.2,0.3,0.4,0.5]})\ndf = df.set_index(""item"")\n', 'df', '""bad""', 'a']";"['df = pandas.DataFrame({""item"": [""a"", ""b"", ""c"", ""d"", ""e""], ""a"": [1,2,3,""bad"",5], ""b"":[0.1,0.2,0.3,0.4,0.5]})\ndf = df.set_index(""item"")\n']";"['df = pandas.DataFrame({""item"": [""a"", ""b"", ""c"", ""d"", ""e""], ""a"": [1,2,3,""bad"",5], ""b"":[0.1,0.2,0.3,0.4,0.5]})\ndf = df.set_index(""item"")\n']";False;"['import pandas as pd\ndf = pandas.DataFrame({""item"": [""a"", ""b"", ""c"", ""d"", ""e""], ""a"": [1,2,3,""bad"",5], ""b"":[0.1,0.2,0.3,0.4,0.5]})\ndf = df.set_index(""item"")\n']";False;0;2;"[""name 'np' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError']
690;690;690;690;1.0;0;21786490;;1;30;<python><sql><merge><pandas>;Pandas left outer join multiple dataframes on multiple columns;45960.0;['df1: \nYear    Week    Colour    Val1 \n2014       A       Red      50\n2014       B       Red      60\n2014       B     Black      70\n2014       C       Red      10\n2014       D     Green      20\n\ndf2:\nYear    Week    Colour    Val2\n2014       A     Black      30\n2014       B     Black     100\n2014       C     Green      50\n2014       C       Red      20\n2014       D       Red      40\n\ndf3:\nYear    Week    Colour    Val3\n2013       B       Red      60\n2013       C     Black      80\n2013       B     Black      10\n2013       D     Green      20\n2013       D       Red      50\nSELECT df1.*, df2.Val2, df3.Val3\nFROM df1\n  LEFT OUTER JOIN df2\n    ON df1.Year = df2.Year\n    AND df1.Week = df2.Week\n    AND df1.Colour = df2.Colour\n  LEFT OUTER JOIN df3\n    ON df1.Week = df3.Week\n    AND df1.Colour = df3.Colour\nYear    Week    Colour    Val1    Val2    Val3\n2014       A       Red      50    Null    Null\n2014       B       Red      60    Null      60\n2014       B     Black      70     100    Null\n2014       C       Red      10      20    Null\n2014       D     Green      20    Null    Null\n'];['df1: \nYear    Week    Colour    Val1 \n2014       A       Red      50\n2014       B       Red      60\n2014       B     Black      70\n2014       C       Red      10\n2014       D     Green      20\n\ndf2:\nYear    Week    Colour    Val2\n2014       A     Black      30\n2014       B     Black     100\n2014       C     Green      50\n2014       C       Red      20\n2014       D       Red      40\n\ndf3:\nYear    Week    Colour    Val3\n2013       B       Red      60\n2013       C     Black      80\n2013       B     Black      10\n2013       D     Green      20\n2013       D       Red      50\n', 'SELECT df1.*, df2.Val2, df3.Val3\nFROM df1\n  LEFT OUTER JOIN df2\n    ON df1.Year = df2.Year\n    AND df1.Week = df2.Week\n    AND df1.Colour = df2.Colour\n  LEFT OUTER JOIN df3\n    ON df1.Week = df3.Week\n    AND df1.Colour = df3.Colour\n', 'Year    Week    Colour    Val1    Val2    Val3\n2014       A       Red      50    Null    Null\n2014       B       Red      60    Null      60\n2014       B     Black      70     100    Null\n2014       C       Red      10      20    Null\n2014       D     Green      20    Null    Null\n'];['df1: \nYear    Week    Colour    Val1 \n2014       A       Red      50\n2014       B       Red      60\n2014       B     Black      70\n2014       C       Red      10\n2014       D     Green      20\n\ndf2:\nYear    Week    Colour    Val2\n2014       A     Black      30\n2014       B     Black     100\n2014       C     Green      50\n2014       C       Red      20\n2014       D       Red      40\n\ndf3:\nYear    Week    Colour    Val3\n2013       B       Red      60\n2013       C     Black      80\n2013       B     Black      10\n2013       D     Green      20\n2013       D       Red      50\n', 'SELECT df1.*, df2.Val2, df3.Val3\nFROM df1\n  LEFT OUTER JOIN df2\n    ON df1.Year = df2.Year\n    AND df1.Week = df2.Week\n    AND df1.Colour = df2.Colour\n  LEFT OUTER JOIN df3\n    ON df1.Week = df3.Week\n    AND df1.Colour = df3.Colour\n', 'Year    Week    Colour    Val1    Val2    Val3\n2014       A       Red      50    Null    Null\n2014       B       Red      60    Null      60\n2014       B     Black      70     100    Null\n2014       C       Red      10      20    Null\n2014       D     Green      20    Null    Null\n'];['\n\n'];['\n\n'];False;['import pandas as pd\n\n\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'df1' is not defined""]";['NameError'];0;1;"[""'Year'""]";['KeyError']
691;691;691;691;2.0;0;21800169;;1;66;<python><indexing><pandas>;Python Pandas: Get index of rows which column matches certain value;149908.0;"[""for i in range(100,3000):\n    if df.iloc[i]['BoolCol']== True:\n         print i,df.iloc[i]['BoolCol']\ndf[df['BoolCol'] == True].index.tolist()\ndf.iloc[i]['BoolCol']\n""]";"[""for i in range(100,3000):\n    if df.iloc[i]['BoolCol']== True:\n         print i,df.iloc[i]['BoolCol']\n"", ""df[df['BoolCol'] == True].index.tolist()\n"", ""df.iloc[i]['BoolCol']\n""]";"[""for i in range(100,3000):\n    if df.iloc[i]['BoolCol']== True:\n         print i,df.iloc[i]['BoolCol']\n"", ""df[df['BoolCol'] == True].index.tolist()\n"", ""df.iloc[i]['BoolCol']\n""]";"[""df[df['BoolCol'] == True].index.tolist()\ndf.iloc[i]['BoolCol']\n""]";"[""df[df['BoolCol'] == True].index.tolist()\ndf.iloc[i]['BoolCol']\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf[df['BoolCol'] == True].index.tolist()\ndf.iloc[i]['BoolCol']\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'BoolCol'""]";['KeyError']
692;692;692;692;3.0;6;21834676;;1;12;<python><r><pandas><pycharm>;equivalent of R's View for Python's pandas;3070.0;[''];[];['View', 'View', 'DataFrame', 'RStudio', 'PyCharm'];[''];[''];False;['import pandas as pd\n'];False;0;1;['from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls'];['FutureWarning'];0;1;['from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls'];['FutureWarning'];0;1;['from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls'];['FutureWarning']
693;693;693;693;3.0;7;21868369;;1;15;<pandas><pycharm><dataframe>;PyCharm hanging for a long time in iPython console with big data;1146.0;[''];[];['my_data.'];[''];[''];False;['import pandas as pd\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
694;694;694;694;2.0;2;21902080;;1;17;<python><csv><pandas>;python pandas not reading first column from csv file;12247.0;"[""GRID    St1  \n1457    614  \n1458    657  \n1459    679  \n1460    732  \n1461    754  \n1462    811  \n1463    748  \na = pandas.DataFrame.from_csv('st1.csv')  \na.columns\n Index([u'ST1'], dtype=object)\n""]";"['GRID    St1  \n1457    614  \n1458    657  \n1459    679  \n1460    732  \n1461    754  \n1462    811  \n1463    748  \n', ""a = pandas.DataFrame.from_csv('st1.csv')  \na.columns\n"", "" Index([u'ST1'], dtype=object)\n""]";"['GRID    St1  \n1457    614  \n1458    657  \n1459    679  \n1460    732  \n1461    754  \n1462    811  \n1463    748  \n', ""a = pandas.DataFrame.from_csv('st1.csv')  \na.columns\n"", "" Index([u'ST1'], dtype=object)\n""]";"[""a = pandas.DataFrame.from_csv('st1.csv')  \na.columns\n""]";"[""a = pandas.DataFrame.from_csv('st1.csv')  \na.columns\n""]";False;"[""import pandas as pd\na = pandas.DataFrame.from_csv('st1.csv')  \na.columns\n""]";False;0;1;"[""name 'pandas' is not defined""]";['NameError'];0;1;"[""name 'pandas' is not defined""]";['NameError'];0;1;"[""name 'pandas' is not defined""]";['NameError']
695;695;695;695;1.0;7;21938932;;1;13;<python><numpy><pandas>;How do I convert a numpy array into a pandas dataframe?;41911.0;"[""px[:,:,0]\npx[:,:,1]\npx[:,:,0]\ndf = pd.DataFrame(px, columns=['R', 'G', 'B'])\n""]";"['px[:,:,0]\npx[:,:,1]\npx[:,:,0]\n', ""df = pd.DataFrame(px, columns=['R', 'G', 'B'])\n""]";"['px[:,:,0]\npx[:,:,1]\npx[:,:,0]\n', ""df = pd.DataFrame(px, columns=['R', 'G', 'B'])\n""]";"[""px[:,:,0]\npx[:,:,1]\npx[:,:,0]\ndf = pd.DataFrame(px, columns=['R', 'G', 'B'])\n""]";"[""import pandas as pd\npx[:,:,0]\npx[:,:,1]\npx[:,:,0]\ndf = pd.DataFrame(px, columns=['R', 'G', 'B'])\n""]";True;"[""import pandas as pd\npx[:,:,0]\npx[:,:,1]\npx[:,:,0]\ndf = pd.DataFrame(px, columns=['R', 'G', 'B'])\n""]";False;0;1;"[""name 'px' is not defined""]";['NameError'];0;1;"[""name 'px' is not defined""]";['NameError'];0;1;"[""name 'px' is not defined""]";['NameError']
696;696;696;696;2.0;0;21961360;;1;11;<python><matplotlib><pandas>;matplotlib plot datetime in pandas DataFrame;16168.0;[''];[];"['training.head()', ""training.plot(x='date',y='rate')"", ""training.plot(kind='scatter',x='date',y='rate')""]";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'DataFrame' object has no attribute 'date'""]";['AttributeError']
697;697;697;697;3.0;0;21988196;;1;14;<python><matplotlib><plot><pandas>;Legend only shows one label when plotting with pandas;11326.0;"[""%pylab inline\nimport pandas as pd\n\n#creating data\n\nprng = pd.period_range('1/1/2011', '1/1/2012', freq='M')\nvar=pd.DataFrame(randn(len(prng)),index=prng,columns=['total'])\nshares=pd.DataFrame(randn(len(prng)),index=index,columns=['average'])\n\n#plotting\n\nax=var.total.plot(label='Variance')\nax=shares.average.plot(secondary_y=True,label='Average Age')\nax.left_ax.set_ylabel('Variance of log wages')\nax.right_ax.set_ylabel('Average age')\nplt.legend(loc='upper center')\nplt.title('Wage Variance and Mean Age')\nplt.show()\n""]";"[""%pylab inline\nimport pandas as pd\n\n#creating data\n\nprng = pd.period_range('1/1/2011', '1/1/2012', freq='M')\nvar=pd.DataFrame(randn(len(prng)),index=prng,columns=['total'])\nshares=pd.DataFrame(randn(len(prng)),index=index,columns=['average'])\n\n#plotting\n\nax=var.total.plot(label='Variance')\nax=shares.average.plot(secondary_y=True,label='Average Age')\nax.left_ax.set_ylabel('Variance of log wages')\nax.right_ax.set_ylabel('Average age')\nplt.legend(loc='upper center')\nplt.title('Wage Variance and Mean Age')\nplt.show()\n""]";"[""%pylab inline\nimport pandas as pd\n\n#creating data\n\nprng = pd.period_range('1/1/2011', '1/1/2012', freq='M')\nvar=pd.DataFrame(randn(len(prng)),index=prng,columns=['total'])\nshares=pd.DataFrame(randn(len(prng)),index=index,columns=['average'])\n\n#plotting\n\nax=var.total.plot(label='Variance')\nax=shares.average.plot(secondary_y=True,label='Average Age')\nax.left_ax.set_ylabel('Variance of log wages')\nax.right_ax.set_ylabel('Average age')\nplt.legend(loc='upper center')\nplt.title('Wage Variance and Mean Age')\nplt.show()\n""]";"[""import pandas as pd\n\n#creating data\n\nprng = pd.period_range('1/1/2011', '1/1/2012', freq='M')\nvar=pd.DataFrame(randn(len(prng)),index=prng,columns=['total'])\nshares=pd.DataFrame(randn(len(prng)),index=index,columns=['average'])\n\n#plotting\n\nax=var.total.plot(label='Variance')\nax=shares.average.plot(secondary_y=True,label='Average Age')\nax.left_ax.set_ylabel('Variance of log wages')\nax.right_ax.set_ylabel('Average age')\nplt.legend(loc='upper center')\nplt.title('Wage Variance and Mean Age')\nplt.show()\n""]";"[""import pandas as pd\n\n#creating data\n\nprng = pd.period_range('1/1/2011', '1/1/2012', freq='M')\nvar=pd.DataFrame(randn(len(prng)),index=prng,columns=['total'])\nshares=pd.DataFrame(randn(len(prng)),index=index,columns=['average'])\n\n#plotting\n\nax=var.total.plot(label='Variance')\nax=shares.average.plot(secondary_y=True,label='Average Age')\nax.left_ax.set_ylabel('Variance of log wages')\nax.right_ax.set_ylabel('Average age')\nplt.legend(loc='upper center')\nplt.title('Wage Variance and Mean Age')\nplt.show()\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nimport pandas as pd\n\n#creating data\n\nprng = pd.period_range('1/1/2011', '1/1/2012', freq='M')\nvar=pd.DataFrame(randn(len(prng)),index=prng,columns=['total'])\nshares=pd.DataFrame(randn(len(prng)),index=index,columns=['average'])\n\n#plotting\n\nax=var.total.plot(label='Variance')\nax=shares.average.plot(secondary_y=True,label='Average Age')\nax.left_ax.set_ylabel('Variance of log wages')\nax.right_ax.set_ylabel('Average age')\nplt.legend(loc='upper center')\nplt.title('Wage Variance and Mean Age')\nplt.show()\n""]";True;0;1;"[""name 'var' is not defined""]";['NameError'];0;1;"[""name 'var' is not defined""]";['NameError'];0;1;"[""name 'var' is not defined""]";['NameError']
698;698;698;698;2.0;0;22005911;;1;42;<python><numpy><pandas>;Convert Columns to String in Pandas;74333.0;"[""(Pdb) pp total_rows\n     ColumnID  RespondentCount\n0          -1                2\n1  3030096843                1\n2  3030096845                1\ntotal_data = total_rows.pivot_table(cols=['ColumnID'])\n\n(Pdb) pp total_data\nColumnID         -1            3030096843   3030096845\nRespondentCount            2            1            1\n\n[1 rows x 3 columns]\n\n\ntotal_rows.pivot_table(cols=['ColumnID']).to_dict('records')[0]\n\n{3030096843: 1, 3030096845: 1, -1: 2}\n{'3030096843': 1, '3030096845': 1, -1: 2}\n""]";"['(Pdb) pp total_rows\n     ColumnID  RespondentCount\n0          -1                2\n1  3030096843                1\n2  3030096845                1\n', ""total_data = total_rows.pivot_table(cols=['ColumnID'])\n\n(Pdb) pp total_data\nColumnID         -1            3030096843   3030096845\nRespondentCount            2            1            1\n\n[1 rows x 3 columns]\n\n\ntotal_rows.pivot_table(cols=['ColumnID']).to_dict('records')[0]\n\n{3030096843: 1, 3030096845: 1, -1: 2}\n"", ""{'3030096843': 1, '3030096845': 1, -1: 2}\n""]";"['(Pdb) pp total_rows\n     ColumnID  RespondentCount\n0          -1                2\n1  3030096843                1\n2  3030096845                1\n', ""total_data = total_rows.pivot_table(cols=['ColumnID'])\n\n(Pdb) pp total_data\nColumnID         -1            3030096843   3030096845\nRespondentCount            2            1            1\n\n[1 rows x 3 columns]\n\n\ntotal_rows.pivot_table(cols=['ColumnID']).to_dict('records')[0]\n\n{3030096843: 1, 3030096845: 1, -1: 2}\n"", ""{'3030096843': 1, '3030096845': 1, -1: 2}\n""]";"[""total_data = total_rows.pivot_table(cols=['ColumnID'])\n\n\n\n\ntotal_rows.pivot_table(cols=['ColumnID']).to_dict('records')[0]\n\n{3030096843: 1, 3030096845: 1, -1: 2}\n{'3030096843': 1, '3030096845': 1, -1: 2}\n""]";"[""total_data = total_rows.pivot_table(cols=['ColumnID'])\n\n\n\n\ntotal_rows.pivot_table(cols=['ColumnID']).to_dict('records')[0]\n\n{3030096843: 1, 3030096845: 1, -1: 2}\n{'3030096843': 1, '3030096845': 1, -1: 2}\n""]";False;"[""import pandas as pd\ntotal_data = total_rows.pivot_table(cols=['ColumnID'])\n\n\n\n\ntotal_rows.pivot_table(cols=['ColumnID']).to_dict('records')[0]\n\n{3030096843: 1, 3030096845: 1, -1: 2}\n{'3030096843': 1, '3030096845': 1, -1: 2}\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
699;699;699;699;1.0;0;22019763;;1;14;<python><csv><pandas>;Pandas Writing Dataframe Columns to csv;24908.0;"['import pandas\nimport csv\n\ndf = pandas.read_csv(\'C:\\\\Python27\\\\Work\\\\spoofing.csv\')\n\ntime = df[""InviteTime (Oracle)""]\norignum = df[""Orig Number""]\norigip = df[""Orig IP Address""]\ndestnum = df[""Dest Number""]\n\ndf.to_csv(\'output.csv\', header=[time,orignum,origip,destnum])\nValueError: Writing 102 cols but got 4 aliases\n']";"['import pandas\nimport csv\n\ndf = pandas.read_csv(\'C:\\\\Python27\\\\Work\\\\spoofing.csv\')\n\ntime = df[""InviteTime (Oracle)""]\norignum = df[""Orig Number""]\norigip = df[""Orig IP Address""]\ndestnum = df[""Dest Number""]\n\ndf.to_csv(\'output.csv\', header=[time,orignum,origip,destnum])\n', 'ValueError: Writing 102 cols but got 4 aliases\n']";"['import pandas\nimport csv\n\ndf = pandas.read_csv(\'C:\\\\Python27\\\\Work\\\\spoofing.csv\')\n\ntime = df[""InviteTime (Oracle)""]\norignum = df[""Orig Number""]\norigip = df[""Orig IP Address""]\ndestnum = df[""Dest Number""]\n\ndf.to_csv(\'output.csv\', header=[time,orignum,origip,destnum])\n', 'ValueError: Writing 102 cols but got 4 aliases\n']";"['import pandas\nimport csv\n\ndf = pandas.read_csv(\'C:\\\\Python27\\\\Work\\\\spoofing.csv\')\n\ntime = df[""InviteTime (Oracle)""]\norignum = df[""Orig Number""]\norigip = df[""Orig IP Address""]\ndestnum = df[""Dest Number""]\n\ndf.to_csv(\'output.csv\', header=[time,orignum,origip,destnum])\n']";"['import pandas\nimport csv\n\ndf = pandas.read_csv(\'C:\\\\Python27\\\\Work\\\\spoofing.csv\')\n\ntime = df[""InviteTime (Oracle)""]\norignum = df[""Orig Number""]\norigip = df[""Orig IP Address""]\ndestnum = df[""Dest Number""]\n\ndf.to_csv(\'output.csv\', header=[time,orignum,origip,destnum])\n']";False;"['import pandas as pd\nimport pandas\nimport csv\n\ndf = pandas.read_csv(\'C:\\\\Python27\\\\Work\\\\spoofing.csv\')\n\ntime = df[""InviteTime (Oracle)""]\norignum = df[""Orig Number""]\norigip = df[""Orig IP Address""]\ndestnum = df[""Dest Number""]\n\ndf.to_csv(\'output.csv\', header=[time,orignum,origip,destnum])\n']";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"['""None of [[\'InviteTime (Oracle)\', \'Orig Number\', \'Orig IP Address\', \'Dest Number\']] are in the [columns]""']";['KeyError']
700;700;700;700;3.0;1;22032668;;1;11;<python><numpy><pandas>;Numpy: Drop rows with all nan or 0 values;9273.0;[''];[];"['nan', '0', ""pandas.dropna(how = 'all')"", 'nan', '0']";[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'np' is not defined"", ""name 'a' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""name 'a' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""name 'a' is not defined""]";['NameError', 'NameError']
701;701;701;701;1.0;3;22070263;;1;11;<python><matplotlib><pandas>;Create a legend with pandas and matplotlib.pyplot;13734.0;"['import matplotlib.pyplot as plt\nimport pandas\ndata = pandas.read_csv( \'data/output/limits.dat\', sep=r""\\s+"", encoding = \'utf-8\' )\naxdata = data.plot( label = \'$|U|^{2}$\' , x = \'mass\', y = \'U2\',\n                    style = \'-s\', markeredgecolor = \'none\' )\nplt.legend( (line1), (\'label1\') )\nplt.legend( [axdata], [\'U2\'])\n~/.virtualenvs/science/lib/python3.3/site-packages/matplotlib/legend.py:613:\nUserWarning: Legend does not support Axes(0.125,0.1;0.775x0.8)\nUse proxy artist instead.\n\nhttp://matplotlib.sourceforge.net/users/legend_guide.html#using-proxy-artist\n\n(str(orig_handle),))\n']";"['import matplotlib.pyplot as plt\nimport pandas\n', 'data = pandas.read_csv( \'data/output/limits.dat\', sep=r""\\s+"", encoding = \'utf-8\' )\n', ""axdata = data.plot( label = '$|U|^{2}$' , x = 'mass', y = 'U2',\n                    style = '-s', markeredgecolor = 'none' )\n"", ""plt.legend( (line1), ('label1') )\n"", ""plt.legend( [axdata], ['U2'])\n"", '~/.virtualenvs/science/lib/python3.3/site-packages/matplotlib/legend.py:613:\nUserWarning: Legend does not support Axes(0.125,0.1;0.775x0.8)\nUse proxy artist instead.\n\nhttp://matplotlib.sourceforge.net/users/legend_guide.html#using-proxy-artist\n\n(str(orig_handle),))\n']";"['import matplotlib.pyplot as plt\nimport pandas\n', 'data = pandas.read_csv( \'data/output/limits.dat\', sep=r""\\s+"", encoding = \'utf-8\' )\n', ""axdata = data.plot( label = '$|U|^{2}$' , x = 'mass', y = 'U2',\n                    style = '-s', markeredgecolor = 'none' )\n"", 'AxesSubplot', ""plt.legend( (line1), ('label1') )\n"", 'line', 'AxesSubplot', 'plt.legend()', ""plt.legend( [axdata], ['U2'])\n"", '~/.virtualenvs/science/lib/python3.3/site-packages/matplotlib/legend.py:613:\nUserWarning: Legend does not support Axes(0.125,0.1;0.775x0.8)\nUse proxy artist instead.\n\nhttp://matplotlib.sourceforge.net/users/legend_guide.html#using-proxy-artist\n\n(str(orig_handle),))\n', 'plt.legend()']";"['import matplotlib.pyplot as plt\nimport pandas\ndata = pandas.read_csv( \'data/output/limits.dat\', sep=r""\\s+"", encoding = \'utf-8\' )\nplt.legend( (line1), (\'label1\') )\nplt.legend( [axdata], [\'U2\'])\n\n\n']";"['import matplotlib.pyplot as plt\nimport pandas\ndata = pandas.read_csv( \'data/output/limits.dat\', sep=r""\\s+"", encoding = \'utf-8\' )\nplt.legend( (line1), (\'label1\') )\nplt.legend( [axdata], [\'U2\'])\n\n\n']";False;"['import pandas as pd\nimport matplotlib.pyplot as plt\nimport pandas\ndata = pandas.read_csv( \'data/output/limits.dat\', sep=r""\\s+"", encoding = \'utf-8\' )\nplt.legend( (line1), (\'label1\') )\nplt.legend( [axdata], [\'U2\'])\n\n\n']";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
702;702;702;702;1.0;0;22081878;;1;13;<python><pandas>;get previous row's value and calculate new column pandas python;21522.0;"["">>> df\n  ChangeEvent StartEvent  case              change      open  \n0    Homeless   Homeless     1 2014-03-08 00:00:00 2014-02-08  \n1       other   Homeless     1 2014-04-08 00:00:00 2014-02-08     \n2    Homeless   Homeless     1 2014-05-08 00:00:00 2014-02-08      \n3        Jail   Homeless     1 2014-06-08 00:00:00 2014-02-08     \n4        Jail       Jail     2 2014-06-08 00:00:00 2014-02-08   \nJail  Homeless case\n 0    6        1\n 0    30       1\n 0    0        1\nimport pandas as pd\nimport datetime as DT\nd = {'case' : pd.Series([1,1,1,1,2]),\n'open' : pd.Series([DT.datetime(2014, 3, 2), DT.datetime(2014, 3, 2),DT.datetime(2014, 3, 2),DT.datetime(2014, 3, 2),DT.datetime(2014, 3, 2)]),\n'change' : pd.Series([DT.datetime(2014, 3, 8), DT.datetime(2014, 4, 8),DT.datetime(2014, 5, 8),DT.datetime(2014, 6, 8),DT.datetime(2014, 6, 8)]),\n'StartEvent' : pd.Series(['Homeless','Homeless','Homeless','Homeless','Jail']),\n'ChangeEvent' : pd.Series(['Homeless','irrelivant','Homeless','Jail','Jail']),\n'close' : pd.Series([DT.datetime(2015, 3, 2), DT.datetime(2015, 3, 2),DT.datetime(2015, 3, 2),DT.datetime(2015, 3, 2),DT.datetime(2015, 3, 2)])}\ndf=pd.DataFrame(d)\n""]";"['>>> df\n  ChangeEvent StartEvent  case              change      open  \n0    Homeless   Homeless     1 2014-03-08 00:00:00 2014-02-08  \n1       other   Homeless     1 2014-04-08 00:00:00 2014-02-08     \n2    Homeless   Homeless     1 2014-05-08 00:00:00 2014-02-08      \n3        Jail   Homeless     1 2014-06-08 00:00:00 2014-02-08     \n4        Jail       Jail     2 2014-06-08 00:00:00 2014-02-08   \n', 'Jail  Homeless case\n 0    6        1\n 0    30       1\n 0    0        1\n', ""import pandas as pd\nimport datetime as DT\nd = {'case' : pd.Series([1,1,1,1,2]),\n'open' : pd.Series([DT.datetime(2014, 3, 2), DT.datetime(2014, 3, 2),DT.datetime(2014, 3, 2),DT.datetime(2014, 3, 2),DT.datetime(2014, 3, 2)]),\n'change' : pd.Series([DT.datetime(2014, 3, 8), DT.datetime(2014, 4, 8),DT.datetime(2014, 5, 8),DT.datetime(2014, 6, 8),DT.datetime(2014, 6, 8)]),\n'StartEvent' : pd.Series(['Homeless','Homeless','Homeless','Homeless','Jail']),\n'ChangeEvent' : pd.Series(['Homeless','irrelivant','Homeless','Jail','Jail']),\n'close' : pd.Series([DT.datetime(2015, 3, 2), DT.datetime(2015, 3, 2),DT.datetime(2015, 3, 2),DT.datetime(2015, 3, 2),DT.datetime(2015, 3, 2)])}\ndf=pd.DataFrame(d)\n""]";"['>>> df\n  ChangeEvent StartEvent  case              change      open  \n0    Homeless   Homeless     1 2014-03-08 00:00:00 2014-02-08  \n1       other   Homeless     1 2014-04-08 00:00:00 2014-02-08     \n2    Homeless   Homeless     1 2014-05-08 00:00:00 2014-02-08      \n3        Jail   Homeless     1 2014-06-08 00:00:00 2014-02-08     \n4        Jail       Jail     2 2014-06-08 00:00:00 2014-02-08   \n', 'Jail  Homeless case\n 0    6        1\n 0    30       1\n 0    0        1\n', ""import pandas as pd\nimport datetime as DT\nd = {'case' : pd.Series([1,1,1,1,2]),\n'open' : pd.Series([DT.datetime(2014, 3, 2), DT.datetime(2014, 3, 2),DT.datetime(2014, 3, 2),DT.datetime(2014, 3, 2),DT.datetime(2014, 3, 2)]),\n'change' : pd.Series([DT.datetime(2014, 3, 8), DT.datetime(2014, 4, 8),DT.datetime(2014, 5, 8),DT.datetime(2014, 6, 8),DT.datetime(2014, 6, 8)]),\n'StartEvent' : pd.Series(['Homeless','Homeless','Homeless','Homeless','Jail']),\n'ChangeEvent' : pd.Series(['Homeless','irrelivant','Homeless','Jail','Jail']),\n'close' : pd.Series([DT.datetime(2015, 3, 2), DT.datetime(2015, 3, 2),DT.datetime(2015, 3, 2),DT.datetime(2015, 3, 2),DT.datetime(2015, 3, 2)])}\ndf=pd.DataFrame(d)\n""]";['import pandas as pd\nimport datetime as DT\ndf=pd.DataFrame(d)\n'];['import pandas as pd\nimport datetime as DT\ndf=pd.DataFrame(d)\n'];False;['import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\nimport datetime as DT\ndf=pd.DataFrame(d)\n'];True;0;1;"[""name 'df1' is not defined""]";['NameError'];0;1;"[""name 'df1' is not defined""]";['NameError'];0;1;"[""'DataFrame' object has no attribute 'change'""]";['AttributeError']
703;703;703;703;5.0;0;22084338;;1;28;<python><dictionary><pandas>;Pandas DataFrame performance;13142.0;"[""import timeit\n\nsetup = '''\nimport numpy, pandas\ndf = pandas.DataFrame(numpy.zeros(shape=[10, 10]))\ndictionary = df.to_dict()\n'''\n\nf = ['value = dictionary[5][5]', 'value = df.loc[5, 5]', 'value = df.iloc[5, 5]']\n\nfor func in f:\n    print func\n    print min(timeit.Timer(func, setup).repeat(3, 100000))\n""]";"[""import timeit\n\nsetup = '''\nimport numpy, pandas\ndf = pandas.DataFrame(numpy.zeros(shape=[10, 10]))\ndictionary = df.to_dict()\n'''\n\nf = ['value = dictionary[5][5]', 'value = df.loc[5, 5]', 'value = df.iloc[5, 5]']\n\nfor func in f:\n    print func\n    print min(timeit.Timer(func, setup).repeat(3, 100000))\n""]";"[""import timeit\n\nsetup = '''\nimport numpy, pandas\ndf = pandas.DataFrame(numpy.zeros(shape=[10, 10]))\ndictionary = df.to_dict()\n'''\n\nf = ['value = dictionary[5][5]', 'value = df.loc[5, 5]', 'value = df.iloc[5, 5]']\n\nfor func in f:\n    print func\n    print min(timeit.Timer(func, setup).repeat(3, 100000))\n""]";"[""import timeit\n\nimport numpy, pandas\ndf = pandas.DataFrame(numpy.zeros(shape=[10, 10]))\ndictionary = df.to_dict()\n\nf = ['value = dictionary[5][5]', 'value = df.loc[5, 5]', 'value = df.iloc[5, 5]']\n\n""]";"[""import timeit\n\nimport numpy, pandas\ndf = pandas.DataFrame(numpy.zeros(shape=[10, 10]))\ndictionary = df.to_dict()\n\nf = ['value = dictionary[5][5]', 'value = df.loc[5, 5]', 'value = df.iloc[5, 5]']\n\n""]";False;"[""import pandas as pd\nimport timeit\n\nimport numpy, pandas\ndf = pandas.DataFrame(numpy.zeros(shape=[10, 10]))\ndictionary = df.to_dict()\n\nf = ['value = dictionary[5][5]', 'value = df.loc[5, 5]', 'value = df.iloc[5, 5]']\n\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
704;704;704;704;2.0;1;22086116;;1;31;<python><filter><pandas>;how do you filter pandas dataframes by multiple columns;29798.0;"['males = df[df[Gender]==\'Male\']\nif A = ""Male"" and if B = ""2014"" then \nfor y in year:\n\nfor g in gender:\n\ndf = .....\n']";"[""males = df[df[Gender]=='Male']\n"", 'if A = ""Male"" and if B = ""2014"" then \n', 'for y in year:\n\nfor g in gender:\n\ndf = .....\n']";"[""males = df[df[Gender]=='Male']\n"", 'if A = ""Male"" and if B = ""2014"" then \n', 'for y in year:\n\nfor g in gender:\n\ndf = .....\n']";"[""males = df[df[Gender]=='Male']\n\n\n""]";"[""males = df[df[Gender]=='Male']\n\n\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nmales = df[df[Gender]=='Male']\n\n\n""]";True;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'Gender' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError']
705;705;705;705;1.0;3;22089317;;1;11;<python-2.7><pandas><export-to-excel>;Python to_excel without row names (index)?;5479.0;['0   6/6/2021 0:00   8/6/2021 0:00\n1   4/10/2024 0:00  6/10/2024 0:00\n2   4/14/2024 0:00  6/14/2024 0:00\n'];['0   6/6/2021 0:00   8/6/2021 0:00\n1   4/10/2024 0:00  6/10/2024 0:00\n2   4/14/2024 0:00  6/14/2024 0:00\n'];['0   6/6/2021 0:00   8/6/2021 0:00\n1   4/10/2024 0:00  6/10/2024 0:00\n2   4/14/2024 0:00  6/14/2024 0:00\n'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
706;706;706;706;1.0;2;22127569;;1;24;<python><pandas>;Opposite of melt in python pandas;3241.0;"[""import pandas as pd\n\nfrom StringIO import StringIO\n\norigin = pd.read_table(StringIO('''label    type    value\nx   a   1\nx   b   2\nx   c   3\ny   a   4\ny   b   5\ny   c   6\nz   a   7\nz   b   8\nz   c   9'''))\n\norigin\nOut[5]: \n  label type  value\n0     x    a      1\n1     x    b      2\n2     x    c      3\n3     y    a      4\n4     y    b      5\n5     y    c      6\n6     z    a      7\n7     z    b      8\n8     z    c      9\n    label   a   b   c\n        x   1   2   3\n        y   4   5   6\n        z   7   8   9\n""]";"[""import pandas as pd\n\nfrom StringIO import StringIO\n\norigin = pd.read_table(StringIO('''label    type    value\nx   a   1\nx   b   2\nx   c   3\ny   a   4\ny   b   5\ny   c   6\nz   a   7\nz   b   8\nz   c   9'''))\n\norigin\nOut[5]: \n  label type  value\n0     x    a      1\n1     x    b      2\n2     x    c      3\n3     y    a      4\n4     y    b      5\n5     y    c      6\n6     z    a      7\n7     z    b      8\n8     z    c      9\n"", '    label   a   b   c\n        x   1   2   3\n        y   4   5   6\n        z   7   8   9\n']";"[""import pandas as pd\n\nfrom StringIO import StringIO\n\norigin = pd.read_table(StringIO('''label    type    value\nx   a   1\nx   b   2\nx   c   3\ny   a   4\ny   b   5\ny   c   6\nz   a   7\nz   b   8\nz   c   9'''))\n\norigin\nOut[5]: \n  label type  value\n0     x    a      1\n1     x    b      2\n2     x    c      3\n3     y    a      4\n4     y    b      5\n5     y    c      6\n6     z    a      7\n7     z    b      8\n8     z    c      9\n"", '    label   a   b   c\n        x   1   2   3\n        y   4   5   6\n        z   7   8   9\n']";['import pandas as pd\n\nfrom StringIO import StringIO\n\n\norigin\n'];['import pandas as pd\n\nfrom StringIO import StringIO\n\n\norigin\n'];False;['import pandas as pd\nimport pandas as pd\n\nfrom StringIO import StringIO\n\n\norigin\n'];False;0;1;"[""name 'label' is not defined""]";['NameError'];0;1;"[""name 'label' is not defined""]";['NameError'];0;1;"[""name 'label' is not defined""]";['NameError']
707;707;707;707;4.0;0;22132525;;1;18;<pandas><date-difference>;Add column with number of days between dates in DataFrame pandas;23577.0;"['df\n          A        B\none 2014-01-01  2014-02-28 \ntwo 2014-02-03  2014-03-01\nimport datetime\ndate1=df[\'A\'][0]\ndate2=df[\'B\'][0]\nmdate1 = datetime.datetime.strptime(date1, ""%Y-%m-%d"").date()\nrdate1 = datetime.datetime.strptime(date2, ""%Y-%m-%d"").date()\ndelta =  (mdate1 - rdate1).days\nprint delta\n']";"['df\n          A        B\none 2014-01-01  2014-02-28 \ntwo 2014-02-03  2014-03-01\n', 'import datetime\ndate1=df[\'A\'][0]\ndate2=df[\'B\'][0]\nmdate1 = datetime.datetime.strptime(date1, ""%Y-%m-%d"").date()\nrdate1 = datetime.datetime.strptime(date2, ""%Y-%m-%d"").date()\ndelta =  (mdate1 - rdate1).days\nprint delta\n']";"['df\n          A        B\none 2014-01-01  2014-02-28 \ntwo 2014-02-03  2014-03-01\n', 'import datetime\ndate1=df[\'A\'][0]\ndate2=df[\'B\'][0]\nmdate1 = datetime.datetime.strptime(date1, ""%Y-%m-%d"").date()\nrdate1 = datetime.datetime.strptime(date2, ""%Y-%m-%d"").date()\ndelta =  (mdate1 - rdate1).days\nprint delta\n']";"['df\nimport datetime\ndate1=df[\'A\'][0]\ndate2=df[\'B\'][0]\nmdate1 = datetime.datetime.strptime(date1, ""%Y-%m-%d"").date()\nrdate1 = datetime.datetime.strptime(date2, ""%Y-%m-%d"").date()\ndelta =  (mdate1 - rdate1).days\n']";"['df\nimport datetime\ndate1=df[\'A\'][0]\ndate2=df[\'B\'][0]\nmdate1 = datetime.datetime.strptime(date1, ""%Y-%m-%d"").date()\nrdate1 = datetime.datetime.strptime(date2, ""%Y-%m-%d"").date()\ndelta =  (mdate1 - rdate1).days\n']";False;"['import pandas as pd\ndf = pd.DataFrame()\ndf\nimport datetime\ndate1=df[\'A\'][0]\ndate2=df[\'B\'][0]\nmdate1 = datetime.datetime.strptime(date1, ""%Y-%m-%d"").date()\nrdate1 = datetime.datetime.strptime(date2, ""%Y-%m-%d"").date()\ndelta =  (mdate1 - rdate1).days\n']";True;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'A'"", ""'DataFrame' object has no attribute 'B'""]";['KeyError', 'AttributeError']
708;708;708;708;1.0;0;22137723;;1;23;<python><pandas>;Convert number strings with commas in pandas DataFrame to float;15553.0;"[""a = [['1,200', '4,200'], ['7,000', '-0.03'], [ '5', '0']]\ndf=pandas.DataFrame(a)\ndf[0].apply(locale.atof)\ndf.apply(locale.atof)\ndf[0:1].apply(locale.atof)\n""]";"[""a = [['1,200', '4,200'], ['7,000', '-0.03'], [ '5', '0']]\ndf=pandas.DataFrame(a)\n"", 'df[0].apply(locale.atof)\n', 'df.apply(locale.atof)\n', 'df[0:1].apply(locale.atof)\n']";"[""a = [['1,200', '4,200'], ['7,000', '-0.03'], [ '5', '0']]\ndf=pandas.DataFrame(a)\n"", 'df[0].apply(locale.atof)\n', 'df.apply(locale.atof)\n', 'df[0:1].apply(locale.atof)\n']";"[""a = [['1,200', '4,200'], ['7,000', '-0.03'], [ '5', '0']]\ndf=pandas.DataFrame(a)\ndf[0].apply(locale.atof)\ndf.apply(locale.atof)\ndf[0:1].apply(locale.atof)\n""]";"[""a = [['1,200', '4,200'], ['7,000', '-0.03'], [ '5', '0']]\ndf=pandas.DataFrame(a)\ndf[0].apply(locale.atof)\ndf.apply(locale.atof)\ndf[0:1].apply(locale.atof)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\na = [['1,200', '4,200'], ['7,000', '-0.03'], [ '5', '0']]\ndf=pandas.DataFrame(a)\ndf[0].apply(locale.atof)\ndf.apply(locale.atof)\ndf[0:1].apply(locale.atof)\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'atof' is not defined""]";['NameError']
709;709;709;709;6.0;0;22149584;;1;86;<python><pandas>;What does axis in pandas mean?;39852.0;"[""import pandas as pd\nimport numpy as np\n\ndff = pd.DataFrame(np.random.randn(1,2),columns=list('AB'))\n+------------+---------+--------+\n|            |  A      |  B     |\n+------------+---------+---------\n|      0     | 0.626386| 1.52325|\n+------------+---------+--------+\ndff.mean(axis=1)\n0    1.074821\ndtype: float64\nA    0.626386\nB    1.523255\ndtype: float64\n""]";"[""import pandas as pd\nimport numpy as np\n\ndff = pd.DataFrame(np.random.randn(1,2),columns=list('AB'))\n"", '+------------+---------+--------+\n|            |  A      |  B     |\n+------------+---------+---------\n|      0     | 0.626386| 1.52325|\n+------------+---------+--------+\n', 'dff.mean(axis=1)\n', '0    1.074821\ndtype: float64\n', 'A    0.626386\nB    1.523255\ndtype: float64\n']";"[""import pandas as pd\nimport numpy as np\n\ndff = pd.DataFrame(np.random.randn(1,2),columns=list('AB'))\n"", '+------------+---------+--------+\n|            |  A      |  B     |\n+------------+---------+---------\n|      0     | 0.626386| 1.52325|\n+------------+---------+--------+\n', 'dff.mean(axis=1)\n', '0    1.074821\ndtype: float64\n', 'A    0.626386\nB    1.523255\ndtype: float64\n']";"[""import pandas as pd\nimport numpy as np\n\ndff = pd.DataFrame(np.random.randn(1,2),columns=list('AB'))\ndff.mean(axis=1)\n""]";"[""import pandas as pd\nimport numpy as np\n\ndff = pd.DataFrame(np.random.randn(1,2),columns=list('AB'))\ndff.mean(axis=1)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\nimport numpy as np\n\ndff = pd.DataFrame(np.random.randn(1,2),columns=list('AB'))\ndff.mean(axis=1)\n""]";True;1;2;"['Sucess', ""name 'np' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'np' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'np' is not defined""]";['Sucess', 'NameError']
710;710;710;710;2.0;10;22155951;;1;16;<python><pandas><dataframe><subclassing>;How to subclass pandas DataFrame?;4155.0;"[""import numpy as np\nimport pandas as pd\n\nclass MyDF(pd.DataFrame):\n    # how to subclass pandas DataFrame?\n    pass\n\nmydf = MyDF(np.random.randn(3,4), columns=['A','B','C','D'])\nprint type(mydf)  # <class '__main__.MyDF'>\n\n# Requirement 1: Instances of MyDF, when calling standard methods of DataFrame,\n# should produce instances of MyDF.\nmydf_sub = mydf[['A','C']]\nprint type(mydf_sub)  # <class 'pandas.core.frame.DataFrame'>\n\n# Requirement 2: Attributes attached to instances of MyDF, when calling standard \n# methods of DataFrame, should still attach to the output.\nmydf.myattr = 1\nmydf_cp1 = MyDF(mydf)\nmydf_cp2 = mydf.copy()\nprint hasattr(mydf_cp1, 'myattr')  # False\nprint hasattr(mydf_cp2, 'myattr')  # False\n""]";"[""import numpy as np\nimport pandas as pd\n\nclass MyDF(pd.DataFrame):\n    # how to subclass pandas DataFrame?\n    pass\n\nmydf = MyDF(np.random.randn(3,4), columns=['A','B','C','D'])\nprint type(mydf)  # <class '__main__.MyDF'>\n\n# Requirement 1: Instances of MyDF, when calling standard methods of DataFrame,\n# should produce instances of MyDF.\nmydf_sub = mydf[['A','C']]\nprint type(mydf_sub)  # <class 'pandas.core.frame.DataFrame'>\n\n# Requirement 2: Attributes attached to instances of MyDF, when calling standard \n# methods of DataFrame, should still attach to the output.\nmydf.myattr = 1\nmydf_cp1 = MyDF(mydf)\nmydf_cp2 = mydf.copy()\nprint hasattr(mydf_cp1, 'myattr')  # False\nprint hasattr(mydf_cp2, 'myattr')  # False\n""]";"[""import numpy as np\nimport pandas as pd\n\nclass MyDF(pd.DataFrame):\n    # how to subclass pandas DataFrame?\n    pass\n\nmydf = MyDF(np.random.randn(3,4), columns=['A','B','C','D'])\nprint type(mydf)  # <class '__main__.MyDF'>\n\n# Requirement 1: Instances of MyDF, when calling standard methods of DataFrame,\n# should produce instances of MyDF.\nmydf_sub = mydf[['A','C']]\nprint type(mydf_sub)  # <class 'pandas.core.frame.DataFrame'>\n\n# Requirement 2: Attributes attached to instances of MyDF, when calling standard \n# methods of DataFrame, should still attach to the output.\nmydf.myattr = 1\nmydf_cp1 = MyDF(mydf)\nmydf_cp2 = mydf.copy()\nprint hasattr(mydf_cp1, 'myattr')  # False\nprint hasattr(mydf_cp2, 'myattr')  # False\n""]";"[""import numpy as np\nimport pandas as pd\n\n    # how to subclass pandas DataFrame?\n\nmydf = MyDF(np.random.randn(3,4), columns=['A','B','C','D'])\n\n# Requirement 1: Instances of MyDF, when calling standard methods of DataFrame,\n# should produce instances of MyDF.\nmydf_sub = mydf[['A','C']]\n\n# Requirement 2: Attributes attached to instances of MyDF, when calling standard \n# methods of DataFrame, should still attach to the output.\nmydf.myattr = 1\nmydf_cp1 = MyDF(mydf)\nmydf_cp2 = mydf.copy()\n""]";"[""from pandas import DataFrame\nimport numpy as np\nimport pandas as pd\n\n    # how to subclass pandas DataFrame?\n\nmydf = MyDF(np.random.randn(3,4), columns=['A','B','C','D'])\n\n# Requirement 1: Instances of MyDF, when calling standard methods of DataFrame,\n# should produce instances of MyDF.\nmydf_sub = mydf[['A','C']]\n\n# Requirement 2: Attributes attached to instances of MyDF, when calling standard \n# methods of DataFrame, should still attach to the output.\nmydf.myattr = 1\nmydf_cp1 = MyDF(mydf)\nmydf_cp2 = mydf.copy()\n""]";True;"[""import pandas as pd\nimport numpy as np\nimport pandas as pd\n\n    # how to subclass pandas DataFrame?\n\nmydf = MyDF(np.random.randn(3,4), columns=['A','B','C','D'])\n\n# Requirement 1: Instances of MyDF, when calling standard methods of DataFrame,\n# should produce instances of MyDF.\nmydf_sub = mydf[['A','C']]\n\n# Requirement 2: Attributes attached to instances of MyDF, when calling standard \n# methods of DataFrame, should still attach to the output.\nmydf.myattr = 1\nmydf_cp1 = MyDF(mydf)\nmydf_cp2 = mydf.copy()\n""]";False;1;2;"[""name 'MyDF' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'MyDF' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'MyDF' is not defined"", 'Sucess']";['NameError', 'Sucess']
711;711;711;711;1.0;0;22156258;;1;14;<python><pandas>;Pandas MultiIndex versus Panel;956.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
712;712;712;712;2.0;4;22180993;;1;22;<python><pandas><flask>;Pandas Dataframe display on a webpage;13560.0;"['@app.route(\'/analysis/<filename>\')\ndef analysis(filename):\n    x = pd.DataFrame(np.random.randn(20, 5))\n    return render_template(""analysis.html"", name=filename, data=x)\n{% extends ""base.html"" %}\n{% block content %}\n<h1>{{name}}</h1>\n{{data}}\n{% endblock %}\n']";"['@app.route(\'/analysis/<filename>\')\ndef analysis(filename):\n    x = pd.DataFrame(np.random.randn(20, 5))\n    return render_template(""analysis.html"", name=filename, data=x)\n', '{% extends ""base.html"" %}\n{% block content %}\n<h1>{{name}}</h1>\n{{data}}\n{% endblock %}\n']";"['@app.route(\'/analysis/<filename>\')\ndef analysis(filename):\n    x = pd.DataFrame(np.random.randn(20, 5))\n    return render_template(""analysis.html"", name=filename, data=x)\n', '{% extends ""base.html"" %}\n{% block content %}\n<h1>{{name}}</h1>\n{{data}}\n{% endblock %}\n', 'data.to_html()', 'data.to_string()']";['{{data}}\n'];['{{data}}\n'];False;['import pandas as pd\ndata = pd.DataFrame()\n{{data}}\n'];True;0;2;"[""name 'data' is not defined"", ""name 'data' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'data' is not defined"", ""name 'data' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'safe' is not defined"", ""name 'safe' is not defined""]";['NameError', 'NameError']
713;713;713;713;2.0;0;22211737;;1;27;<python><pandas>;Python, pandas: how to sort dataframe by index;32238.0;"[""import pandas as pd\ndf = pd.DataFrame([1, 1, 1, 1, 1], index=[100, 29, 234, 1, 150], columns=['A'])\n""]";"[""import pandas as pd\ndf = pd.DataFrame([1, 1, 1, 1, 1], index=[100, 29, 234, 1, 150], columns=['A'])\n""]";"[""import pandas as pd\ndf = pd.DataFrame([1, 1, 1, 1, 1], index=[100, 29, 234, 1, 150], columns=['A'])\n""]";"[""import pandas as pd\ndf = pd.DataFrame([1, 1, 1, 1, 1], index=[100, 29, 234, 1, 150], columns=['A'])\n""]";"[""import pandas as pd\ndf = pd.DataFrame([1, 1, 1, 1, 1], index=[100, 29, 234, 1, 150], columns=['A'])\n""]";False;"[""import pandas as pd\nimport pandas as pd\ndf = pd.DataFrame([1, 1, 1, 1, 1], index=[100, 29, 234, 1, 150], columns=['A'])\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
714;714;714;714;3.0;3;22219004;;1;42;<python><pandas>;grouping rows in list in pandas groupby;24488.0;['A 1\nA 2\nB 5\nB 5\nB 4\nC 6\nA [1,2]\nB [5,5,4]\nC [6]\n'];['A 1\nA 2\nB 5\nB 5\nB 4\nC 6\n', 'A [1,2]\nB [5,5,4]\nC [6]\n'];['A 1\nA 2\nB 5\nB 5\nB 4\nC 6\n', 'A [1,2]\nB [5,5,4]\nC [6]\n'];['A [1,2]\nB [5,5,4]\nC [6]\n'];['A [1,2]\nB [5,5,4]\nC [6]\n'];False;['import pandas as pd\nA [1,2]\nB [5,5,4]\nC [6]\n'];False;1;3;"['Sucess', ""name 'pd' is not defined"", ""name 'pd' is not defined""]";['Sucess', 'NameError', 'NameError'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess']
715;715;715;715;4.0;0;22233488;;1;67;<python><pandas>;Pandas: drop a level from a multi-level column index?;36471.0;"['>>> cols = pd.MultiIndex.from_tuples([(""a"", ""b""), (""a"", ""c"")])\n>>> pd.DataFrame([[1,2], [3,4]], columns=cols)\n\n    a\n   ---+--\n    b | c\n--+---+--\n0 | 1 | 2\n1 | 3 | 4\n\n    b | c\n--+---+--\n0 | 1 | 2\n1 | 3 | 4\n']";"['>>> cols = pd.MultiIndex.from_tuples([(""a"", ""b""), (""a"", ""c"")])\n>>> pd.DataFrame([[1,2], [3,4]], columns=cols)\n', '\n    a\n   ---+--\n    b | c\n--+---+--\n0 | 1 | 2\n1 | 3 | 4\n', '\n    b | c\n--+---+--\n0 | 1 | 2\n1 | 3 | 4\n']";"['>>> cols = pd.MultiIndex.from_tuples([(""a"", ""b""), (""a"", ""c"")])\n>>> pd.DataFrame([[1,2], [3,4]], columns=cols)\n']";['\n0 | 1 | 2\n1 | 3 | 4\n\n0 | 1 | 2\n1 | 3 | 4\n'];['\n0 | 1 | 2\n1 | 3 | 4\n\n0 | 1 | 2\n1 | 3 | 4\n'];False;['import pandas as pd\n\n0 | 1 | 2\n1 | 3 | 4\n\n0 | 1 | 2\n1 | 3 | 4\n'];False;3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess']
716;716;716;716;1.0;1;22235245;;1;21;<python><csv><pandas><dataframe>;Calculate summary statistics of columns in dataframe;35787.0;['shopper_num,is_martian,number_of_items,count_pineapples,birth_country,tranpsortation_method\n1,FALSE,0,0,MX,\n2,FALSE,1,0,MX,\n3,FALSE,0,0,MX,\n4,FALSE,22,0,MX,\n5,FALSE,0,0,MX,\n6,FALSE,0,0,MX,\n7,FALSE,5,0,MX,\n8,FALSE,0,0,MX,\n9,FALSE,4,0,MX,\n10,FALSE,2,0,MX,\n11,FALSE,0,0,MX,\n12,FALSE,13,0,MX,\n13,FALSE,0,0,CA,\n14,FALSE,0,0,US,\ncolumnname, max, min, median,\n\nis_martian, NA, NA, FALSE\n'];['shopper_num,is_martian,number_of_items,count_pineapples,birth_country,tranpsortation_method\n1,FALSE,0,0,MX,\n2,FALSE,1,0,MX,\n3,FALSE,0,0,MX,\n4,FALSE,22,0,MX,\n5,FALSE,0,0,MX,\n6,FALSE,0,0,MX,\n7,FALSE,5,0,MX,\n8,FALSE,0,0,MX,\n9,FALSE,4,0,MX,\n10,FALSE,2,0,MX,\n11,FALSE,0,0,MX,\n12,FALSE,13,0,MX,\n13,FALSE,0,0,CA,\n14,FALSE,0,0,US,\n', 'columnname, max, min, median,\n\nis_martian, NA, NA, FALSE\n'];['shopper_num,is_martian,number_of_items,count_pineapples,birth_country,tranpsortation_method\n1,FALSE,0,0,MX,\n2,FALSE,1,0,MX,\n3,FALSE,0,0,MX,\n4,FALSE,22,0,MX,\n5,FALSE,0,0,MX,\n6,FALSE,0,0,MX,\n7,FALSE,5,0,MX,\n8,FALSE,0,0,MX,\n9,FALSE,4,0,MX,\n10,FALSE,2,0,MX,\n11,FALSE,0,0,MX,\n12,FALSE,13,0,MX,\n13,FALSE,0,0,CA,\n14,FALSE,0,0,US,\n', 'columnname, max, min, median,\n\nis_martian, NA, NA, FALSE\n'];['shopper_num,is_martian,number_of_items,count_pineapples,birth_country,tranpsortation_method\n1,FALSE,0,0,MX,\n2,FALSE,1,0,MX,\n3,FALSE,0,0,MX,\n4,FALSE,22,0,MX,\n5,FALSE,0,0,MX,\n6,FALSE,0,0,MX,\n7,FALSE,5,0,MX,\n8,FALSE,0,0,MX,\n9,FALSE,4,0,MX,\n10,FALSE,2,0,MX,\n11,FALSE,0,0,MX,\n12,FALSE,13,0,MX,\n13,FALSE,0,0,CA,\n14,FALSE,0,0,US,\ncolumnname, max, min, median,\n\nis_martian, NA, NA, FALSE\n'];['shopper_num,is_martian,number_of_items,count_pineapples,birth_country,tranpsortation_method\n1,FALSE,0,0,MX,\n2,FALSE,1,0,MX,\n3,FALSE,0,0,MX,\n4,FALSE,22,0,MX,\n5,FALSE,0,0,MX,\n6,FALSE,0,0,MX,\n7,FALSE,5,0,MX,\n8,FALSE,0,0,MX,\n9,FALSE,4,0,MX,\n10,FALSE,2,0,MX,\n11,FALSE,0,0,MX,\n12,FALSE,13,0,MX,\n13,FALSE,0,0,CA,\n14,FALSE,0,0,US,\ncolumnname, max, min, median,\n\nis_martian, NA, NA, FALSE\n'];False;['import pandas as pd\nshopper_num,is_martian,number_of_items,count_pineapples,birth_country,tranpsortation_method\n1,FALSE,0,0,MX,\n2,FALSE,1,0,MX,\n3,FALSE,0,0,MX,\n4,FALSE,22,0,MX,\n5,FALSE,0,0,MX,\n6,FALSE,0,0,MX,\n7,FALSE,5,0,MX,\n8,FALSE,0,0,MX,\n9,FALSE,4,0,MX,\n10,FALSE,2,0,MX,\n11,FALSE,0,0,MX,\n12,FALSE,13,0,MX,\n13,FALSE,0,0,CA,\n14,FALSE,0,0,US,\ncolumnname, max, min, median,\n\nis_martian, NA, NA, FALSE\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;['Cannot describe a DataFrame without columns'];['ValueError']
717;717;717;717;2.0;0;22245171;;1;16;<python><string><pandas><missing-data>;How to lowercase a python dataframe string column if it has missing values?;22959.0;"['import pandas as pd\nimport numpy as np\ndf=pd.DataFrame([\'ONE\',\'Two\', np.nan],columns=[\'x\']) \nxLower = df[""x""].map(lambda x: x.lower())\n']";"['import pandas as pd\nimport numpy as np\ndf=pd.DataFrame([\'ONE\',\'Two\', np.nan],columns=[\'x\']) \nxLower = df[""x""].map(lambda x: x.lower())\n']";"['import pandas as pd\nimport numpy as np\ndf=pd.DataFrame([\'ONE\',\'Two\', np.nan],columns=[\'x\']) \nxLower = df[""x""].map(lambda x: x.lower())\n']";"['import pandas as pd\nimport numpy as np\ndf=pd.DataFrame([\'ONE\',\'Two\', np.nan],columns=[\'x\']) \nxLower = df[""x""].map(lambda x: x.lower())\n']";"['import pandas as pd\nimport numpy as np\ndf=pd.DataFrame([\'ONE\',\'Two\', np.nan],columns=[\'x\']) \nxLower = df[""x""].map(lambda x: x.lower())\n']";False;"['import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\nimport numpy as np\ndf=pd.DataFrame([\'ONE\',\'Two\', np.nan],columns=[\'x\']) \nxLower = df[""x""].map(lambda x: x.lower())\n']";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
718;718;718;718;4.0;1;22257527;;1;12;<r><pandas><reporting><nan><missing-data>;How do I get a summary count of missing/NaN data by column in 'pandas'?;12292.0;['len(mydata.index) - mydata.count()\n'];['len(mydata.index) - mydata.count()\n'];['summary', 'pandas', 'describe', 'len(mydata.index) - mydata.count()\n'];['len(mydata.index) - mydata.count()\n'];['len(mydata.index) - mydata.count()\n'];False;['import pandas as pd\ndata = pd.DataFrame()\nlen(mydata.index) - mydata.count()\n'];True;0;1;"[""name 'DataFrame' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'DataFrame' is not defined""]";['NameError']
719;719;719;719;3.0;1;22257836;;1;15;<python><arrays><numpy><pandas><scikit-learn>;"Numpy hstack - ""ValueError: all the input arrays must have same number of dimensions"" - but they do";28086.0;"['  #reading in test/train data for TF-IDF\n  traindata = list(np.array(p.read_csv(\'FinalCSVFin.csv\', delimiter="";""))[:,2])\n  testdata = list(np.array(p.read_csv(\'FinalTestCSVFin.csv\', delimiter="";""))[:,2])\n\n  #reading in labels for training\n  y = np.array(p.read_csv(\'FinalCSVFin.csv\', delimiter="";""))[:,-2]\n\n  #reading in single integer column to join\n  AlexaTrainData = p.read_csv(\'FinalCSVFin.csv\', delimiter="";"")[[""alexarank""]]\n  AlexaTestData = p.read_csv(\'FinalTestCSVFin.csv\', delimiter="";"")[[""alexarank""]]\n  AllAlexaAndGoogleInfo = AlexaTestData.append(AlexaTrainData)\n\n  tfv = TfidfVectorizer(min_df=3,  max_features=None, strip_accents=\'unicode\',  \n        analyzer=\'word\',token_pattern=r\'\\w{1,}\',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1) #tf-idf object\n  rd = lm.LogisticRegression(penalty=\'l2\', dual=True, tol=0.0001, \n                             C=1, fit_intercept=True, intercept_scaling=1.0, \n                             class_weight=None, random_state=None) #Classifier\n  X_all = traindata + testdata #adding test and train data to put into tf-idf\n  lentrain = len(traindata) #find length of train data\n  tfv.fit(X_all) #fit tf-idf on all our text\n  X_all = tfv.transform(X_all) #transform it\n  X = X_all[:lentrain] #reduce to size of training set\n  AllAlexaAndGoogleInfo = AllAlexaAndGoogleInfo[:lentrain] #reduce to size of training set\n  X_test = X_all[lentrain:] #reduce to size of training set\n\n  #printing debug info, output below : \n  print ""X.shape => "" + str(X.shape)\n  print ""AllAlexaAndGoogleInfo.shape => "" + str(AllAlexaAndGoogleInfo.shape)\n  print ""X_all.shape => "" + str(X_all.shape)\n\n  #line we get error on\n  X = np.hstack((X, AllAlexaAndGoogleInfo))\nX.shape => (7395, 238377)\nAllAlexaAndGoogleInfo.shape => (7395, 1)\nX_all.shape => (10566, 238377)\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-12-2b310887b5e4> in <module>()\n     31 print ""X_all.shape => "" + str(X_all.shape)\n     32 #X = np.column_stack((X, AllAlexaAndGoogleInfo))\n---> 33 X = np.hstack((X, AllAlexaAndGoogleInfo))\n     34 sc = preprocessing.StandardScaler().fit(X)\n     35 X = sc.transform(X)\n\nC:\\Users\\Simon\\Anaconda\\lib\\site-packages\\numpy\\core\\shape_base.pyc in hstack(tup)\n    271     # As a special case, dimension 0 of 1-dimensional arrays is ""horizontal""\n    272     if arrs[0].ndim == 1:\n--> 273         return _nx.concatenate(arrs, 0)\n    274     else:\n    275         return _nx.concatenate(arrs, 1)\n\nValueError: all the input arrays must have same number of dimensions\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-16-640ef6dd335d> in <module>()\n---> 36 X = np.column_stack((X, AllAlexaAndGoogleInfo))\n     37 sc = preprocessing.StandardScaler().fit(X)\n     38 X = sc.transform(X)\n\nC:\\Users\\Simon\\Anaconda\\lib\\site-packages\\numpy\\lib\\shape_base.pyc in column_stack(tup)\n    294             arr = array(arr,copy=False,subok=True,ndmin=2).T\n    295         arrays.append(arr)\n--> 296     return _nx.concatenate(arrays,1)\n    297 \n    298 def dstack(tup):\n\nValueError: all the input array dimensions except for the concatenation axis must match exactly\nX.dtype => float64\nprint ""AllAlexaAndGoogleInfo.dtype => "" + str(AllAlexaAndGoogleInfo.dtype) \n\'DataFrame\' object has no attribute \'dtype\'\n']";"['  #reading in test/train data for TF-IDF\n  traindata = list(np.array(p.read_csv(\'FinalCSVFin.csv\', delimiter="";""))[:,2])\n  testdata = list(np.array(p.read_csv(\'FinalTestCSVFin.csv\', delimiter="";""))[:,2])\n\n  #reading in labels for training\n  y = np.array(p.read_csv(\'FinalCSVFin.csv\', delimiter="";""))[:,-2]\n\n  #reading in single integer column to join\n  AlexaTrainData = p.read_csv(\'FinalCSVFin.csv\', delimiter="";"")[[""alexarank""]]\n  AlexaTestData = p.read_csv(\'FinalTestCSVFin.csv\', delimiter="";"")[[""alexarank""]]\n  AllAlexaAndGoogleInfo = AlexaTestData.append(AlexaTrainData)\n\n  tfv = TfidfVectorizer(min_df=3,  max_features=None, strip_accents=\'unicode\',  \n        analyzer=\'word\',token_pattern=r\'\\w{1,}\',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1) #tf-idf object\n  rd = lm.LogisticRegression(penalty=\'l2\', dual=True, tol=0.0001, \n                             C=1, fit_intercept=True, intercept_scaling=1.0, \n                             class_weight=None, random_state=None) #Classifier\n  X_all = traindata + testdata #adding test and train data to put into tf-idf\n  lentrain = len(traindata) #find length of train data\n  tfv.fit(X_all) #fit tf-idf on all our text\n  X_all = tfv.transform(X_all) #transform it\n  X = X_all[:lentrain] #reduce to size of training set\n  AllAlexaAndGoogleInfo = AllAlexaAndGoogleInfo[:lentrain] #reduce to size of training set\n  X_test = X_all[lentrain:] #reduce to size of training set\n\n  #printing debug info, output below : \n  print ""X.shape => "" + str(X.shape)\n  print ""AllAlexaAndGoogleInfo.shape => "" + str(AllAlexaAndGoogleInfo.shape)\n  print ""X_all.shape => "" + str(X_all.shape)\n\n  #line we get error on\n  X = np.hstack((X, AllAlexaAndGoogleInfo))\n', 'X.shape => (7395, 238377)\nAllAlexaAndGoogleInfo.shape => (7395, 1)\nX_all.shape => (10566, 238377)\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-12-2b310887b5e4> in <module>()\n     31 print ""X_all.shape => "" + str(X_all.shape)\n     32 #X = np.column_stack((X, AllAlexaAndGoogleInfo))\n---> 33 X = np.hstack((X, AllAlexaAndGoogleInfo))\n     34 sc = preprocessing.StandardScaler().fit(X)\n     35 X = sc.transform(X)\n\nC:\\Users\\Simon\\Anaconda\\lib\\site-packages\\numpy\\core\\shape_base.pyc in hstack(tup)\n    271     # As a special case, dimension 0 of 1-dimensional arrays is ""horizontal""\n    272     if arrs[0].ndim == 1:\n--> 273         return _nx.concatenate(arrs, 0)\n    274     else:\n    275         return _nx.concatenate(arrs, 1)\n\nValueError: all the input arrays must have same number of dimensions\n', '---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-16-640ef6dd335d> in <module>()\n---> 36 X = np.column_stack((X, AllAlexaAndGoogleInfo))\n     37 sc = preprocessing.StandardScaler().fit(X)\n     38 X = sc.transform(X)\n\nC:\\Users\\Simon\\Anaconda\\lib\\site-packages\\numpy\\lib\\shape_base.pyc in column_stack(tup)\n    294             arr = array(arr,copy=False,subok=True,ndmin=2).T\n    295         arrays.append(arr)\n--> 296     return _nx.concatenate(arrays,1)\n    297 \n    298 def dstack(tup):\n\nValueError: all the input array dimensions except for the concatenation axis must match exactly\n', 'X.dtype => float64\n', 'print ""AllAlexaAndGoogleInfo.dtype => "" + str(AllAlexaAndGoogleInfo.dtype) \n', ""'DataFrame' object has no attribute 'dtype'\n""]";"['hstack', '  #reading in test/train data for TF-IDF\n  traindata = list(np.array(p.read_csv(\'FinalCSVFin.csv\', delimiter="";""))[:,2])\n  testdata = list(np.array(p.read_csv(\'FinalTestCSVFin.csv\', delimiter="";""))[:,2])\n\n  #reading in labels for training\n  y = np.array(p.read_csv(\'FinalCSVFin.csv\', delimiter="";""))[:,-2]\n\n  #reading in single integer column to join\n  AlexaTrainData = p.read_csv(\'FinalCSVFin.csv\', delimiter="";"")[[""alexarank""]]\n  AlexaTestData = p.read_csv(\'FinalTestCSVFin.csv\', delimiter="";"")[[""alexarank""]]\n  AllAlexaAndGoogleInfo = AlexaTestData.append(AlexaTrainData)\n\n  tfv = TfidfVectorizer(min_df=3,  max_features=None, strip_accents=\'unicode\',  \n        analyzer=\'word\',token_pattern=r\'\\w{1,}\',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1) #tf-idf object\n  rd = lm.LogisticRegression(penalty=\'l2\', dual=True, tol=0.0001, \n                             C=1, fit_intercept=True, intercept_scaling=1.0, \n                             class_weight=None, random_state=None) #Classifier\n  X_all = traindata + testdata #adding test and train data to put into tf-idf\n  lentrain = len(traindata) #find length of train data\n  tfv.fit(X_all) #fit tf-idf on all our text\n  X_all = tfv.transform(X_all) #transform it\n  X = X_all[:lentrain] #reduce to size of training set\n  AllAlexaAndGoogleInfo = AllAlexaAndGoogleInfo[:lentrain] #reduce to size of training set\n  X_test = X_all[lentrain:] #reduce to size of training set\n\n  #printing debug info, output below : \n  print ""X.shape => "" + str(X.shape)\n  print ""AllAlexaAndGoogleInfo.shape => "" + str(AllAlexaAndGoogleInfo.shape)\n  print ""X_all.shape => "" + str(X_all.shape)\n\n  #line we get error on\n  X = np.hstack((X, AllAlexaAndGoogleInfo))\n', 'X.shape => (7395, 238377)\nAllAlexaAndGoogleInfo.shape => (7395, 1)\nX_all.shape => (10566, 238377)\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-12-2b310887b5e4> in <module>()\n     31 print ""X_all.shape => "" + str(X_all.shape)\n     32 #X = np.column_stack((X, AllAlexaAndGoogleInfo))\n---> 33 X = np.hstack((X, AllAlexaAndGoogleInfo))\n     34 sc = preprocessing.StandardScaler().fit(X)\n     35 X = sc.transform(X)\n\nC:\\Users\\Simon\\Anaconda\\lib\\site-packages\\numpy\\core\\shape_base.pyc in hstack(tup)\n    271     # As a special case, dimension 0 of 1-dimensional arrays is ""horizontal""\n    272     if arrs[0].ndim == 1:\n--> 273         return _nx.concatenate(arrs, 0)\n    274     else:\n    275         return _nx.concatenate(arrs, 1)\n\nValueError: all the input arrays must have same number of dimensions\n', '---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-16-640ef6dd335d> in <module>()\n---> 36 X = np.column_stack((X, AllAlexaAndGoogleInfo))\n     37 sc = preprocessing.StandardScaler().fit(X)\n     38 X = sc.transform(X)\n\nC:\\Users\\Simon\\Anaconda\\lib\\site-packages\\numpy\\lib\\shape_base.pyc in column_stack(tup)\n    294             arr = array(arr,copy=False,subok=True,ndmin=2).T\n    295         arrays.append(arr)\n--> 296     return _nx.concatenate(arrays,1)\n    297 \n    298 def dstack(tup):\n\nValueError: all the input array dimensions except for the concatenation axis must match exactly\n', 'dtype', 'X.dtype => float64\n', 'AllAlexaAndGoogleInfo', 'print ""AllAlexaAndGoogleInfo.dtype => "" + str(AllAlexaAndGoogleInfo.dtype) \n', ""'DataFrame' object has no attribute 'dtype'\n""]";['  #reading in test/train data for TF-IDF\n\n  #reading in labels for training\n\n  #reading in single integer column to join\n\n\n  #printing debug info, output below : \n\n  #line we get error on\n\n\n\n\n\n\n\n'];['  #reading in test/train data for TF-IDF\n\n  #reading in labels for training\n\n  #reading in single integer column to join\n\n\n  #printing debug info, output below : \n\n  #line we get error on\n\n\n\n\n\n\n\n'];False;['import pandas as pd\ndata = pd.DataFrame()\n  #reading in test/train data for TF-IDF\n\n  #reading in labels for training\n\n  #reading in single integer column to join\n\n\n  #printing debug info, output below : \n\n  #line we get error on\n\n\n\n\n\n\n\n'];True;0;2;"[""name 'np' is not defined"", ""No module named 'scipy'""]";['NameError', 'ImportError'];0;2;"[""name 'np' is not defined"", ""No module named 'scipy'""]";['NameError', 'ImportError'];0;2;"[""name 'np' is not defined"", ""No module named 'scipy'""]";['NameError', 'ImportError']
720;720;720;720;6.0;3;22258491;;1;20;<python><pandas><random><io><import-from-csv>;Read a small random sample from a big CSV file into a Python data frame;9360.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'xrange' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'xrange' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'xrange' is not defined"", 'Sucess']";['NameError', 'Sucess']
721;721;721;721;3.0;0;22276503;;1;16;<python><string><floating-point><pandas><format>;How to I change data-type of pandas data frame to string with a defined format;76176.0;"[""print image_name_data\n     id           image_name\n0  1001  1001_mar2014_report\n1  1002  1002_mar2014_report\n2  1003  1003_mar2014_report\n\n[3 rows x 2 columns]\nprint image_name_data.dtypes\nid            float64\nimage_name     object\ndtype: object\nimage_name_data['id'] = image_name_data['id'].astype('str')\nprint image_name_data.dyptes\nid            object\nimage_name    object\ndtype: object\nprint image_name_data\n       id           image_name\n0  1001.0  1001_mar2014_report\n1  1002.0  1002_mar2014_report\n2  1003.0  1003_mar2014_report\n\n[3 rows x 2 columns]\n""]";"['print image_name_data\n     id           image_name\n0  1001  1001_mar2014_report\n1  1002  1002_mar2014_report\n2  1003  1003_mar2014_report\n\n[3 rows x 2 columns]\n', 'print image_name_data.dtypes\nid            float64\nimage_name     object\ndtype: object\n', ""image_name_data['id'] = image_name_data['id'].astype('str')\n"", 'print image_name_data.dyptes\nid            object\nimage_name    object\ndtype: object\n', 'print image_name_data\n       id           image_name\n0  1001.0  1001_mar2014_report\n1  1002.0  1002_mar2014_report\n2  1003.0  1003_mar2014_report\n\n[3 rows x 2 columns]\n']";"['print image_name_data\n     id           image_name\n0  1001  1001_mar2014_report\n1  1002  1002_mar2014_report\n2  1003  1003_mar2014_report\n\n[3 rows x 2 columns]\n', 'print image_name_data.dtypes\nid            float64\nimage_name     object\ndtype: object\n', ""image_name_data['id'] = image_name_data['id'].astype('str')\n"", 'print image_name_data.dyptes\nid            object\nimage_name    object\ndtype: object\n', 'print image_name_data\n       id           image_name\n0  1001.0  1001_mar2014_report\n1  1002.0  1002_mar2014_report\n2  1003.0  1003_mar2014_report\n\n[3 rows x 2 columns]\n']";"[""\nimage_name_data['id'] = image_name_data['id'].astype('str')\n\n""]";"[""\nimage_name_data['id'] = image_name_data['id'].astype('str')\n\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\n\nimage_name_data['id'] = image_name_data['id'].astype('str')\n\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
722;722;722;722;2.0;7;22341271;;1;60;<python><list><pandas>;get list from pandas dataframe column;117410.0;['cluster load_date   budget  actual  fixed_price\nA   1/1/2014    1000    4000    Y\nA   2/1/2014    12000   10000   Y\nA   3/1/2014    36000   2000    Y\nB   4/1/2014    15000   10000   N\nB   4/1/2014    12000   11500   N\nB   4/1/2014    90000   11000   N\nC   7/1/2014    22000   18000   N\nC   8/1/2014    30000   28960   N\nC   9/1/2014    53000   51200   N\nlist = [], list[column1] or list[df.ix(row1)]\n'];['cluster load_date   budget  actual  fixed_price\nA   1/1/2014    1000    4000    Y\nA   2/1/2014    12000   10000   Y\nA   3/1/2014    36000   2000    Y\nB   4/1/2014    15000   10000   N\nB   4/1/2014    12000   11500   N\nB   4/1/2014    90000   11000   N\nC   7/1/2014    22000   18000   N\nC   8/1/2014    30000   28960   N\nC   9/1/2014    53000   51200   N\n', 'list = [], list[column1] or list[df.ix(row1)]\n'];['cluster load_date   budget  actual  fixed_price\nA   1/1/2014    1000    4000    Y\nA   2/1/2014    12000   10000   Y\nA   3/1/2014    36000   2000    Y\nB   4/1/2014    15000   10000   N\nB   4/1/2014    12000   11500   N\nB   4/1/2014    90000   11000   N\nC   7/1/2014    22000   18000   N\nC   8/1/2014    30000   28960   N\nC   9/1/2014    53000   51200   N\n', 'list = [], list[column1] or list[df.ix(row1)]\n'];['list = [], list[column1] or list[df.ix(row1)]\n'];['list = [], list[column1] or list[df.ix(row1)]\n'];False;['import pandas as pd\ndf = pd.DataFrame()\nlist = [], list[column1] or list[df.ix(row1)]\n'];True;0;1;"[""name 'd' is not defined""]";['NameError'];0;1;"[""name 'd' is not defined""]";['NameError'];0;1;"[""name 'd' is not defined""]";['NameError']
723;723;723;723;9.0;7;22391433;;1;51;<python><pandas>;count the frequency that a value occurs in a dataframe column;98963.0;['|category|\ncat a\ncat b\ncat a\ncategory | freq |\ncat a       2\ncat b       1\n'];['|category|\ncat a\ncat b\ncat a\n', 'category | freq |\ncat a       2\ncat b       1\n'];['|category|\ncat a\ncat b\ncat a\n', 'category | freq |\ncat a       2\ncat b       1\n'];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'pd' is not defined"", ""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];2;3;"['Sucess', 'Sucess', ""name 'df' is not defined""]";['Sucess', 'Sucess', 'NameError'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess']
724;724;724;724;1.0;3;22403469;;1;21;<python><datetime><pandas>;Locate first and last non NaN values in a Pandas DataFrame;9357.0;[''];[];['DataFrame', 'NaN'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
725;725;725;725;5.0;0;22470690;;1;65;<python><pandas>;get list of pandas dataframe columns based on data type;133397.0;"['1. NAME                                     object\n2. On_Time                                      object\n3. On_Budget                                    object\n4. %actual_hr                                  float64\n5. Baseline Start Date                  datetime64[ns]\n6. Forecast Start Date                  datetime64[ns] \nFor c in col_list: if c.dtype = ""Something""\nlist[]\nList.append(c)?\n']";"['1. NAME                                     object\n2. On_Time                                      object\n3. On_Budget                                    object\n4. %actual_hr                                  float64\n5. Baseline Start Date                  datetime64[ns]\n6. Forecast Start Date                  datetime64[ns] \n', 'For c in col_list: if c.dtype = ""Something""\nlist[]\nList.append(c)?\n']";"['1. NAME                                     object\n2. On_Time                                      object\n3. On_Budget                                    object\n4. %actual_hr                                  float64\n5. Baseline Start Date                  datetime64[ns]\n6. Forecast Start Date                  datetime64[ns] \n', 'For c in col_list: if c.dtype = ""Something""\nlist[]\nList.append(c)?\n']";[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'pd' is not defined"", ""name 'dtype' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'np' is not defined"", ""name 'dtype' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];1;3;"[""name 'np' is not defined"", ""name 'dtype' is not defined"", 'Sucess']";['NameError', 'NameError', 'Sucess']
726;726;726;726;4.0;0;22483588;;1;38;<python><matplotlib><pandas>;How can I plot separate Pandas DataFrames as subplots?;34397.0;[''];[];['df.plot()'];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""No module named 'matplotlib'"", ""name 'df' is not defined"", ""name 'plt' is not defined""]";['ImportError', 'NameError', 'NameError'];0;3;"[""No module named 'matplotlib'"", ""name 'df' is not defined"", ""name 'plt' is not defined""]";['ImportError', 'NameError', 'NameError'];0;3;"[""No module named 'matplotlib'"", ""name '_converter' is not defined"", ""name 'plt' is not defined""]";['ImportError', 'NameError', 'NameError']
727;727;727;727;2.0;2;22485375;;1;19;<python><pandas>;Efficiently select rows that match one of several values in Pandas DataFrame;19621.0;['Name     Amount\n---------------\nAlice       100\nBob          50\nCharlie     200\nAlice        30\nCharlie      10\nName     Amount\n---------------\nAlice       100\nBob          50\nAlice        30\nmerge(df[df.name = specific_name] for specific_name in names) # something like this\n'];['Name     Amount\n---------------\nAlice       100\nBob          50\nCharlie     200\nAlice        30\nCharlie      10\n', 'Name     Amount\n---------------\nAlice       100\nBob          50\nAlice        30\n', 'merge(df[df.name = specific_name] for specific_name in names) # something like this\n'];['Name     Amount\n---------------\nAlice       100\nBob          50\nCharlie     200\nAlice        30\nCharlie      10\n', 'Name', '{Alice, Bob}', 'Name     Amount\n---------------\nAlice       100\nBob          50\nAlice        30\n', 'merge(df[df.name = specific_name] for specific_name in names) # something like this\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'Name'""]";['KeyError']
728;728;728;728;1.0;1;22487296;;1;15;<python><pandas><multiprocessing>;multiprocessing in python - sharing large object (e.g. pandas dataframe) between multiple processes;6405.0;['from multiprocessing import Pool\np = Pool(15)\n\nargs = [(df, config1), (df, config2), ...] #list of args - df is the same object in each tuple\nres = p.map_async(func, args) #func is some arbitrary function\np.close()\np.join()\nshared_df = multiprocessing.Value(pandas.DataFrame, df)\nargs = [(shared_df, config1), (shared_df, config2), ...] \n'];['from multiprocessing import Pool\np = Pool(15)\n\nargs = [(df, config1), (df, config2), ...] #list of args - df is the same object in each tuple\nres = p.map_async(func, args) #func is some arbitrary function\np.close()\np.join()\n', 'shared_df = multiprocessing.Value(pandas.DataFrame, df)\nargs = [(shared_df, config1), (shared_df, config2), ...] \n'];['from multiprocessing import Pool\np = Pool(15)\n\nargs = [(df, config1), (df, config2), ...] #list of args - df is the same object in each tuple\nres = p.map_async(func, args) #func is some arbitrary function\np.close()\np.join()\n', 'df', 'multiprocessing.Value', 'shared_df = multiprocessing.Value(pandas.DataFrame, df)\nargs = [(shared_df, config1), (shared_df, config2), ...] \n', 'TypeError: this type has no size', 'multiprocessing.Value'];['from multiprocessing import Pool\np = Pool(15)\n\nargs = [(df, config1), (df, config2), ...] #list of args - df is the same object in each tuple\nres = p.map_async(func, args) #func is some arbitrary function\np.close()\np.join()\nshared_df = multiprocessing.Value(pandas.DataFrame, df)\nargs = [(shared_df, config1), (shared_df, config2), ...] \n'];['from multiprocessing import Pool\np = Pool(15)\n\nargs = [(df, config1), (df, config2), ...] #list of args - df is the same object in each tuple\nres = p.map_async(func, args) #func is some arbitrary function\np.close()\np.join()\nshared_df = multiprocessing.Value(pandas.DataFrame, df)\nargs = [(shared_df, config1), (shared_df, config2), ...] \n'];False;['import pandas as pd\nfrom multiprocessing import Pool\np = Pool(15)\n\nargs = [(df, config1), (df, config2), ...] #list of args - df is the same object in each tuple\nres = p.map_async(func, args) #func is some arbitrary function\np.close()\np.join()\nshared_df = multiprocessing.Value(pandas.DataFrame, df)\nargs = [(shared_df, config1), (shared_df, config2), ...] \n'];False;0;1;"[""name 'my_dataframe' is not defined""]";['NameError'];0;1;"[""name 'my_dataframe' is not defined""]";['NameError'];0;1;"[""name 'my_dataframe' is not defined""]";['NameError']
729;729;729;729;4.0;3;22543208;;1;21;<python><matplotlib><pandas><ipython>;ggplot styles in Python;12579.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
730;730;730;730;2.0;0;22546425;;1;28;<python><pandas>;using pandas to select rows conditional on multiple equivalencies;35625.0;"[""SELECT * FROM df WHERE column1 = 'a' OR column2 = 'b' OR column3 = 'c' etc...\nfoo = df.ix[df['column']==value]\n""]";"[""SELECT * FROM df WHERE column1 = 'a' OR column2 = 'b' OR column3 = 'c' etc...\n"", ""foo = df.ix[df['column']==value]\n""]";"[""SELECT * FROM df WHERE column1 = 'a' OR column2 = 'b' OR column3 = 'c' etc...\n"", ""foo = df.ix[df['column']==value]\n""]";"[""foo = df.ix[df['column']==value]\n""]";"[""foo = df.ix[df['column']==value]\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nfoo = df.ix[df['column']==value]\n""]";True;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;['\n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated', 'Sucess'];['DeprecationWarning', 'Sucess']
731;731;731;731;1.0;0;22551403;;1;38;<python><pandas><dataframe>;Python pandas Filtering out nan from a data selection of a column of strings;55836.0;"[""import pandas as pd\nimport numpy as np\n\n\ndf = pd.DataFrame({'movie': ['thg', 'thg', 'mol', 'mol', 'lob', 'lob'],\n                  'rating': [3., 4., 5., np.nan, np.nan, np.nan],\n                  'name': ['John', np.nan, 'N/A', 'Graham', np.nan, np.nan]})\n\nnbs = df['name'].str.extract('^(N/A|NA|na|n/a)')\nnms=df[(df['name'] != nbs) ]\n>>> nms\n  movie    name  rating\n0   thg    John       3\n1   thg     NaN       4\n3   mol  Graham     NaN\n4   lob     NaN     NaN\n5   lob     NaN     NaN\n  movie    name  rating\n0   thg    John       3\n3   mol  Graham     NaN\n""]";"[""import pandas as pd\nimport numpy as np\n\n\ndf = pd.DataFrame({'movie': ['thg', 'thg', 'mol', 'mol', 'lob', 'lob'],\n                  'rating': [3., 4., 5., np.nan, np.nan, np.nan],\n                  'name': ['John', np.nan, 'N/A', 'Graham', np.nan, np.nan]})\n\nnbs = df['name'].str.extract('^(N/A|NA|na|n/a)')\nnms=df[(df['name'] != nbs) ]\n"", '>>> nms\n  movie    name  rating\n0   thg    John       3\n1   thg     NaN       4\n3   mol  Graham     NaN\n4   lob     NaN     NaN\n5   lob     NaN     NaN\n', '  movie    name  rating\n0   thg    John       3\n3   mol  Graham     NaN\n']";"['groupby', 'NaN', ""import pandas as pd\nimport numpy as np\n\n\ndf = pd.DataFrame({'movie': ['thg', 'thg', 'mol', 'mol', 'lob', 'lob'],\n                  'rating': [3., 4., 5., np.nan, np.nan, np.nan],\n                  'name': ['John', np.nan, 'N/A', 'Graham', np.nan, np.nan]})\n\nnbs = df['name'].str.extract('^(N/A|NA|na|n/a)')\nnms=df[(df['name'] != nbs) ]\n"", '>>> nms\n  movie    name  rating\n0   thg    John       3\n1   thg     NaN       4\n3   mol  Graham     NaN\n4   lob     NaN     NaN\n5   lob     NaN     NaN\n', '  movie    name  rating\n0   thg    John       3\n3   mol  Graham     NaN\n', '~np.isnan']";"[""import pandas as pd\nimport numpy as np\n\n\n\nnbs = df['name'].str.extract('^(N/A|NA|na|n/a)')\nnms=df[(df['name'] != nbs) ]\n""]";"[""import pandas as pd\nimport numpy as np\n\n\n\nnbs = df['name'].str.extract('^(N/A|NA|na|n/a)')\nnms=df[(df['name'] != nbs) ]\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\nimport numpy as np\n\n\n\nnbs = df['name'].str.extract('^(N/A|NA|na|n/a)')\nnms=df[(df['name'] != nbs) ]\n""]";True;0;1;"[""name 'nms' is not defined""]";['NameError'];0;1;"[""name 'nms' is not defined""]";['NameError'];0;1;"[""name 'nms' is not defined""]";['NameError']
732;732;732;732;4.0;0;22588316;;1;25;<python><regex><pandas>;pandas applying regex to replace values;22689.0;['$40,000*\n$40000 conditions attached\n[0-9]+\n'];['$40,000*\n$40000 conditions attached\n', '[0-9]+\n'];['$40,000*\n$40000 conditions attached\n', '[0-9]+\n'];[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 're' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 're' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 're' is not defined"", 'Sucess']";['NameError', 'Sucess']
733;733;733;733;2.0;1;22591174;;1;26;<python><pandas><boolean-logic>;pandas: multiple conditions while indexing data frame - unexpected behavior;47480.0;"[""import pandas as pd\n\ndf = pd.DataFrame({'a': range(5), 'b': range(5) })\n\n# let's insert some -1 values\ndf['a'][1] = -1\ndf['b'][1] = -1\ndf['a'][3] = -1\ndf['b'][4] = -1\n\ndf1 = df[(df.a != -1) & (df.b != -1)]\ndf2 = df[(df.a != -1) | (df.b != -1)]\n\nprint pd.concat([df, df1, df2], axis=1,\n                keys = [ 'original df', 'using AND (&)', 'using OR (|)',])\n      original df      using AND (&)      using OR (|)    \n             a  b              a   b             a   b\n0            0  0              0   0             0   0\n1           -1 -1            NaN NaN           NaN NaN\n2            2  2              2   2             2   2\n3           -1  3            NaN NaN            -1   3\n4            4 -1            NaN NaN             4  -1\n\n[5 rows x 6 columns]\n""]";"[""import pandas as pd\n\ndf = pd.DataFrame({'a': range(5), 'b': range(5) })\n\n# let's insert some -1 values\ndf['a'][1] = -1\ndf['b'][1] = -1\ndf['a'][3] = -1\ndf['b'][4] = -1\n\ndf1 = df[(df.a != -1) & (df.b != -1)]\ndf2 = df[(df.a != -1) | (df.b != -1)]\n\nprint pd.concat([df, df1, df2], axis=1,\n                keys = [ 'original df', 'using AND (&)', 'using OR (|)',])\n"", '      original df      using AND (&)      using OR (|)    \n             a  b              a   b             a   b\n0            0  0              0   0             0   0\n1           -1 -1            NaN NaN           NaN NaN\n2            2  2              2   2             2   2\n3           -1  3            NaN NaN            -1   3\n4            4 -1            NaN NaN             4  -1\n\n[5 rows x 6 columns]\n']";"[""import pandas as pd\n\ndf = pd.DataFrame({'a': range(5), 'b': range(5) })\n\n# let's insert some -1 values\ndf['a'][1] = -1\ndf['b'][1] = -1\ndf['a'][3] = -1\ndf['b'][4] = -1\n\ndf1 = df[(df.a != -1) & (df.b != -1)]\ndf2 = df[(df.a != -1) | (df.b != -1)]\n\nprint pd.concat([df, df1, df2], axis=1,\n                keys = [ 'original df', 'using AND (&)', 'using OR (|)',])\n"", '      original df      using AND (&)      using OR (|)    \n             a  b              a   b             a   b\n0            0  0              0   0             0   0\n1           -1 -1            NaN NaN           NaN NaN\n2            2  2              2   2             2   2\n3           -1  3            NaN NaN            -1   3\n4            4 -1            NaN NaN             4  -1\n\n[5 rows x 6 columns]\n', 'AND', '-1', 'OR', '-1']";"[""import pandas as pd\n\ndf = pd.DataFrame({'a': range(5), 'b': range(5) })\n\n# let's insert some -1 values\ndf['a'][1] = -1\ndf['b'][1] = -1\ndf['a'][3] = -1\ndf['b'][4] = -1\n\ndf1 = df[(df.a != -1) & (df.b != -1)]\ndf2 = df[(df.a != -1) | (df.b != -1)]\n\n\n""]";"[""import pandas as pd\n\ndf = pd.DataFrame({'a': range(5), 'b': range(5) })\n\n# let's insert some -1 values\ndf['a'][1] = -1\ndf['b'][1] = -1\ndf['a'][3] = -1\ndf['b'][4] = -1\n\ndf1 = df[(df.a != -1) & (df.b != -1)]\ndf2 = df[(df.a != -1) | (df.b != -1)]\n\n\n""]";False;"[""import pandas as pd\nimport pandas as pd\n\ndf = pd.DataFrame({'a': range(5), 'b': range(5) })\n\n# let's insert some -1 values\ndf['a'][1] = -1\ndf['b'][1] = -1\ndf['a'][3] = -1\ndf['b'][4] = -1\n\ndf1 = df[(df.a != -1) & (df.b != -1)]\ndf2 = df[(df.a != -1) | (df.b != -1)]\n\n\n""]";False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'DataFrame' object has no attribute 'a'"", ""name 'a' is not defined""]";['AttributeError', 'UndefinedVariableError']
734;734;734;734;1.0;0;22604564;;1;76;<python><csv><pandas>;How to create a Pandas DataFrame from String;32697.0;"['TESTDATA=""""""col1;col2;col3\n1;4.4;99\n2;4.5;200\n3;4.7;65\n4;3.2;140\n""""""\n']";"['TESTDATA=""""""col1;col2;col3\n1;4.4;99\n2;4.5;200\n3;4.7;65\n4;3.2;140\n""""""\n']";"['DataFrame', 'TESTDATA=""""""col1;col2;col3\n1;4.4;99\n2;4.5;200\n3;4.7;65\n4;3.2;140\n""""""\n', 'DataFrame']";"['1;4.4;99\n2;4.5;200\n3;4.7;65\n4;3.2;140\n']";"['1;4.4;99\n2;4.5;200\n3;4.7;65\n4;3.2;140\n']";False;"['import pandas as pd\n1;4.4;99\n2;4.5;200\n3;4.7;65\n4;3.2;140\n']";False;0;1;"[""name 'TESTDATA' is not defined""]";['NameError'];0;1;"[""name 'TESTDATA' is not defined""]";['NameError'];0;1;"[""name 'TESTDATA' is not defined""]";['NameError']
735;735;735;735;5.0;0;22642162;;1;14;<python><pandas>;Python: Divide each row of a DataFrame by another DataFrame vector;20234.0;[''];[];"['2000 rows x 500 columns', '1 rows X 500 columns', 'df.divide(df2)', ""df.divide(df2, axis='index')"", 'nan', 'df.divide']";[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'pd' is not defined"", ""name 'data1' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df2' is not defined"", ""name 'data1' is not defined""]";['NameError', 'NameError'];0;2;['single positional indexer is out-of-bounds', '\n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated'];['IndexError', 'DeprecationWarning']
736;736;736;736;7.0;0;22649693;;1;22;<python><pandas>;Drop rows with all zeros in pandas data frame;25347.0;['P   kt  b   tt  mky depth\n1   0   0   0   0   0\n2   0   0   0   0   0\n3   0   0   0   0   0\n4   0   0   0   0   0\n5   1.1 3   4.5 2.3 9.0\n'];['P   kt  b   tt  mky depth\n1   0   0   0   0   0\n2   0   0   0   0   0\n3   0   0   0   0   0\n4   0   0   0   0   0\n5   1.1 3   4.5 2.3 9.0\n'];['pandas', 'dropna()', 'NA', 'P   kt  b   tt  mky depth\n1   0   0   0   0   0\n2   0   0   0   0   0\n3   0   0   0   0   0\n4   0   0   0   0   0\n5   1.1 3   4.5 2.3 9.0\n'];[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
737;737;737;737;4.0;0;22650833;;1;14;<python><pandas>;Pandas groupby cumulative sum;13444.0;['Jack | Monday | 10\nJack | Tuesday | 20\nJack | Tuesday | 10\nJack | Wednesday | 50\nJill | Monday | 40\nJill Wednesday | 110\nJack | Monday | 10 | 10\nJack | Tuesday | 30 | 40\nJack | Wednesday | 50 | 100\nJill | Monday | 40 | 40\nJill | Wednesday | 40 | 150\n'];['Jack | Monday | 10\nJack | Tuesday | 20\nJack | Tuesday | 10\nJack | Wednesday | 50\nJill | Monday | 40\nJill Wednesday | 110\n', 'Jack | Monday | 10 | 10\nJack | Tuesday | 30 | 40\nJack | Wednesday | 50 | 100\nJill | Monday | 40 | 40\nJill | Wednesday | 40 | 150\n'];['Jack | Monday | 10\nJack | Tuesday | 20\nJack | Tuesday | 10\nJack | Wednesday | 50\nJill | Monday | 40\nJill Wednesday | 110\n', 'Jack | Monday | 10 | 10\nJack | Tuesday | 30 | 40\nJack | Wednesday | 50 | 100\nJill | Monday | 40 | 40\nJill | Wednesday | 40 | 150\n', 'df.groupby', 'df.agg(lambda x: cumsum(x))'];['Jack | Monday | 10\nJack | Tuesday | 20\nJack | Tuesday | 10\nJack | Wednesday | 50\nJill | Monday | 40\nJack | Monday | 10 | 10\nJack | Tuesday | 30 | 40\nJack | Wednesday | 50 | 100\nJill | Monday | 40 | 40\nJill | Wednesday | 40 | 150\n'];['Jack | Monday | 10\nJack | Tuesday | 20\nJack | Tuesday | 10\nJack | Wednesday | 50\nJill | Monday | 40\nJack | Monday | 10 | 10\nJack | Tuesday | 30 | 40\nJack | Wednesday | 50 | 100\nJill | Monday | 40 | 40\nJill | Wednesday | 40 | 150\n'];False;['import pandas as pd\nJack | Monday | 10\nJack | Tuesday | 20\nJack | Tuesday | 10\nJack | Wednesday | 50\nJill | Monday | 40\nJack | Monday | 10 | 10\nJack | Tuesday | 30 | 40\nJack | Wednesday | 50 | 100\nJill | Monday | 40 | 40\nJill | Wednesday | 40 | 150\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
738;738;738;738;4.0;1;22676081;;1;45;<python><pandas>;Pandas - The difference between join and merge;22067.0;"[""left = pd.DataFrame({'key1': ['foo', 'bar'], 'lval': [1, 2]})\n\nright = pd.DataFrame({'key2': ['foo', 'bar'], 'rval': [4, 5]})\npd.merge(left, right, left_on='key1', right_on='key2')\n    key1    lval    key2    rval\n0   foo     1       foo     4\n1   bar     2       bar     5\nleft.join(right, on=['key1', 'key2'])\n//anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _validate_specification(self)\n    406             if self.right_index:\n    407                 if not ((len(self.left_on) == self.right.index.nlevels)):\n--> 408                     raise AssertionError()\n    409                 self.right_on = [None] * n\n    410         elif self.right_on is not None:\n\nAssertionError: \n""]";"[""left = pd.DataFrame({'key1': ['foo', 'bar'], 'lval': [1, 2]})\n\nright = pd.DataFrame({'key2': ['foo', 'bar'], 'rval': [4, 5]})\n"", ""pd.merge(left, right, left_on='key1', right_on='key2')\n"", '    key1    lval    key2    rval\n0   foo     1       foo     4\n1   bar     2       bar     5\n', ""left.join(right, on=['key1', 'key2'])\n"", '//anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _validate_specification(self)\n    406             if self.right_index:\n    407                 if not ((len(self.left_on) == self.right.index.nlevels)):\n--> 408                     raise AssertionError()\n    409                 self.right_on = [None] * n\n    410         elif self.right_on is not None:\n\nAssertionError: \n']";"[""left = pd.DataFrame({'key1': ['foo', 'bar'], 'lval': [1, 2]})\n\nright = pd.DataFrame({'key2': ['foo', 'bar'], 'rval': [4, 5]})\n"", ""pd.merge(left, right, left_on='key1', right_on='key2')\n"", '    key1    lval    key2    rval\n0   foo     1       foo     4\n1   bar     2       bar     5\n', ""left.join(right, on=['key1', 'key2'])\n"", '//anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _validate_specification(self)\n    406             if self.right_index:\n    407                 if not ((len(self.left_on) == self.right.index.nlevels)):\n--> 408                     raise AssertionError()\n    409                 self.right_on = [None] * n\n    410         elif self.right_on is not None:\n\nAssertionError: \n']";"[""left = pd.DataFrame({'key1': ['foo', 'bar'], 'lval': [1, 2]})\n\nright = pd.DataFrame({'key2': ['foo', 'bar'], 'rval': [4, 5]})\npd.merge(left, right, left_on='key1', right_on='key2')\nleft.join(right, on=['key1', 'key2'])\n\n""]";"[""import pandas as pd\nleft = pd.DataFrame({'key1': ['foo', 'bar'], 'lval': [1, 2]})\n\nright = pd.DataFrame({'key2': ['foo', 'bar'], 'rval': [4, 5]})\npd.merge(left, right, left_on='key1', right_on='key2')\nleft.join(right, on=['key1', 'key2'])\n\n""]";True;"[""import pandas as pd\nleft = pd.DataFrame({'key1': ['foo', 'bar'], 'lval': [1, 2]})\n\nright = pd.DataFrame({'key2': ['foo', 'bar'], 'rval': [4, 5]})\npd.merge(left, right, left_on='key1', right_on='key2')\nleft.join(right, on=['key1', 'key2'])\n\n""]";False;0;2;"[""name 'key' is not defined"", ""name 'left' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'key' is not defined"", ""name 'left' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'key' is not defined"", ""name 'left' is not defined""]";['NameError', 'NameError']
739;739;739;739;4.0;3;22691010;;1;25;<python><pandas>;How to print a groupby object;15872.0;"[""import pandas as pd\ndf = pd.DataFrame({'A': ['one', 'one', 'two', 'three', 'three', 'one'], 'B': range(6)})\nprint df\n\n       A  B\n0    one  0\n1    one  1\n2    two  2\n3  three  3\n4  three  4\n5    one  5\nprint df.groupby('A')\n\n<pandas.core.groupby.DataFrameGroupBy object at 0x05416E90>\nprint df.groupby('A').head()\n             A  B\nA                \none   0    one  0\n      1    one  1\ntwo   2    two  2\nthree 3  three  3\n      4  three  4\none   5    one  5\n             A  B\nA                \none   0    one  0\n      1    one  1\n      5    one  5\ntwo   2    two  2\nthree 3  three  3\n      4  three  4\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'A': ['one', 'one', 'two', 'three', 'three', 'one'], 'B': range(6)})\nprint df\n\n       A  B\n0    one  0\n1    one  1\n2    two  2\n3  three  3\n4  three  4\n5    one  5\n"", ""print df.groupby('A')\n\n<pandas.core.groupby.DataFrameGroupBy object at 0x05416E90>\n"", ""print df.groupby('A').head()\n"", '             A  B\nA                \none   0    one  0\n      1    one  1\ntwo   2    two  2\nthree 3  three  3\n      4  three  4\none   5    one  5\n', '             A  B\nA                \none   0    one  0\n      1    one  1\n      5    one  5\ntwo   2    two  2\nthree 3  three  3\n      4  three  4\n']";"[""import pandas as pd\ndf = pd.DataFrame({'A': ['one', 'one', 'two', 'three', 'three', 'one'], 'B': range(6)})\nprint df\n\n       A  B\n0    one  0\n1    one  1\n2    two  2\n3  three  3\n4  three  4\n5    one  5\n"", ""print df.groupby('A')\n\n<pandas.core.groupby.DataFrameGroupBy object at 0x05416E90>\n"", ""print df.groupby('A').head()\n"", '             A  B\nA                \none   0    one  0\n      1    one  1\ntwo   2    two  2\nthree 3  three  3\n      4  three  4\none   5    one  5\n', '             A  B\nA                \none   0    one  0\n      1    one  1\n      5    one  5\ntwo   2    two  2\nthree 3  three  3\n      4  three  4\n']";"[""import pandas as pd\ndf = pd.DataFrame({'A': ['one', 'one', 'two', 'three', 'three', 'one'], 'B': range(6)})\n\n\nA                \nA                \n""]";"[""import pandas as pd\ndf = pd.DataFrame({'A': ['one', 'one', 'two', 'three', 'three', 'one'], 'B': range(6)})\n\n\nA                \nA                \n""]";False;"[""import pandas as pd\nimport pandas as pd\ndf = pd.DataFrame({'A': ['one', 'one', 'two', 'three', 'three', 'one'], 'B': range(6)})\n\n\nA                \nA                \n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError']
740;740;740;740;3.0;1;22697773;;1;31;<python><pandas>;how to check the dtype of a column in python pandas;62516.0;"[""allc = list((agg.loc[:, (agg.dtypes==np.float64)|(agg.dtypes==np.int)]).columns)\nfor y in allc:\n    treat_numeric(agg[y])    \n\nallc = list((agg.loc[:, (agg.dtypes!=np.float64)&(agg.dtypes!=np.int)]).columns)\nfor y in allc:\n    treat_str(agg[y])    \nfor y in agg.columns:\n    if(dtype(agg[y]) == 'string'):\n          treat_str(agg[y])\n    elif(dtype(agg[y]) != 'string'):\n          treat_numeric(agg[y])\n""]";"['allc = list((agg.loc[:, (agg.dtypes==np.float64)|(agg.dtypes==np.int)]).columns)\nfor y in allc:\n    treat_numeric(agg[y])    \n\nallc = list((agg.loc[:, (agg.dtypes!=np.float64)&(agg.dtypes!=np.int)]).columns)\nfor y in allc:\n    treat_str(agg[y])    \n', ""for y in agg.columns:\n    if(dtype(agg[y]) == 'string'):\n          treat_str(agg[y])\n    elif(dtype(agg[y]) != 'string'):\n          treat_numeric(agg[y])\n""]";"['allc = list((agg.loc[:, (agg.dtypes==np.float64)|(agg.dtypes==np.int)]).columns)\nfor y in allc:\n    treat_numeric(agg[y])    \n\nallc = list((agg.loc[:, (agg.dtypes!=np.float64)&(agg.dtypes!=np.int)]).columns)\nfor y in allc:\n    treat_str(agg[y])    \n', ""for y in agg.columns:\n    if(dtype(agg[y]) == 'string'):\n          treat_str(agg[y])\n    elif(dtype(agg[y]) != 'string'):\n          treat_numeric(agg[y])\n""]";['allc = list((agg.loc[:, (agg.dtypes==np.float64)|(agg.dtypes==np.int)]).columns)\n\nallc = list((agg.loc[:, (agg.dtypes!=np.float64)&(agg.dtypes!=np.int)]).columns)\n'];['allc = list((agg.loc[:, (agg.dtypes==np.float64)|(agg.dtypes==np.int)]).columns)\n\nallc = list((agg.loc[:, (agg.dtypes!=np.float64)&(agg.dtypes!=np.int)]).columns)\n'];False;['import pandas as pd\nallc = list((agg.loc[:, (agg.dtypes==np.float64)|(agg.dtypes==np.int)]).columns)\n\nallc = list((agg.loc[:, (agg.dtypes!=np.float64)&(agg.dtypes!=np.int)]).columns)\n'];False;1;3;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name 'your_function' is not defined"", ""'A'""]";['Sucess', 'NameError', 'KeyError']
741;741;741;741;1.0;0;22702760;;1;17;<python><pandas>;how to multiply multiple columns by a column in Pandas;8143.0;"[""df[['income_1', 'income_2']] * df['mtaz_proportion']\ndf[['mtaz_income_1', 'mtaz_income_2']] = \ndf[['income_1', 'income_2']] * df['mtaz_proportion']\nincome_1    income_2    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \n0   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...\n1   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...\n2   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...\n""]";"[""df[['income_1', 'income_2']] * df['mtaz_proportion']\n"", ""df[['mtaz_income_1', 'mtaz_income_2']] = \ndf[['income_1', 'income_2']] * df['mtaz_proportion']\n"", 'income_1    income_2    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \n0   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...\n1   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...\n2   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...\n']";"[""df[['income_1', 'income_2']] * df['mtaz_proportion']\n"", ""df['mtaz_proportion']"", ""df[['mtaz_income_1', 'mtaz_income_2']] = \ndf[['income_1', 'income_2']] * df['mtaz_proportion']\n"", 'income_1    income_2    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \n0   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...\n1   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...\n2   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...\n']";"[""df[['income_1', 'income_2']] * df['mtaz_proportion']\ndf[['income_1', 'income_2']] * df['mtaz_proportion']\n""]";"[""df[['income_1', 'income_2']] * df['mtaz_proportion']\ndf[['income_1', 'income_2']] * df['mtaz_proportion']\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf[['income_1', 'income_2']] * df['mtaz_proportion']\ndf[['income_1', 'income_2']] * df['mtaz_proportion']\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"['""[\'A\' \'B\'] not in index""']";['KeyError']
742;742;742;742;1.0;2;22720739;;1;17;<python><pandas>;Pandas Left Outer Join results in table larger than left table;6022.0;"[""combined = pd.merge(a,b,how='left',left_on='id',right_on='key')\n""]";"[""combined = pd.merge(a,b,how='left',left_on='id',right_on='key')\n""]";"[""combined = pd.merge(a,b,how='left',left_on='id',right_on='key')\n""]";"[""combined = pd.merge(a,b,how='left',left_on='id',right_on='key')\n""]";"[""import pandas as pd\ncombined = pd.merge(a,b,how='left',left_on='id',right_on='key')\n""]";True;"[""import pandas as pd\ncombined = pd.merge(a,b,how='left',left_on='id',right_on='key')\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'df2' is not defined""]";['NameError'];0;1;['not enough values to unpack (expected 2, got 0)'];['ValueError']
743;743;743;743;4.0;0;22787209;;1;24;<python><pandas><matplotlib><plot><seaborn>;How to have clusters of stacked bars with python (Pandas);7938.0;"['In [1]: df1=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])\n\nIn [2]: df2=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])\n\nIn [3]: df1\nOut[3]: \n          I         J\nA  0.675616  0.177597\nB  0.675693  0.598682\nC  0.631376  0.598966\nD  0.229858  0.378817\n\nIn [4]: df2\nOut[4]: \n          I         J\nA  0.939620  0.984616\nB  0.314818  0.456252\nC  0.630907  0.656341\nD  0.020994  0.538303\nIn [5]: ax = df1.plot(kind=""bar"", stacked=True)\n\nIn [5]: ax2 = df2.plot(kind=""bar"", stacked=True, ax = ax)\npd.concat(dict(df1 = df1, df2 = df2),axis = 1).plot(kind=""bar"", stacked=True)\n pd.concat(dict(df1 = df1, df2 = df2),axis = 0).plot(kind=""bar"", stacked=True)\ndf1 df2    df1 df2\n_______    _______ ...\n   A          B\n']";"['In [1]: df1=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])\n\nIn [2]: df2=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])\n\nIn [3]: df1\nOut[3]: \n          I         J\nA  0.675616  0.177597\nB  0.675693  0.598682\nC  0.631376  0.598966\nD  0.229858  0.378817\n\nIn [4]: df2\nOut[4]: \n          I         J\nA  0.939620  0.984616\nB  0.314818  0.456252\nC  0.630907  0.656341\nD  0.020994  0.538303\n', 'In [5]: ax = df1.plot(kind=""bar"", stacked=True)\n\nIn [5]: ax2 = df2.plot(kind=""bar"", stacked=True, ax = ax)\n', 'pd.concat(dict(df1 = df1, df2 = df2),axis = 1).plot(kind=""bar"", stacked=True)\n', ' pd.concat(dict(df1 = df1, df2 = df2),axis = 0).plot(kind=""bar"", stacked=True)\n', 'df1 df2    df1 df2\n_______    _______ ...\n   A          B\n']";"['In [1]: df1=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])\n\nIn [2]: df2=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])\n\nIn [3]: df1\nOut[3]: \n          I         J\nA  0.675616  0.177597\nB  0.675693  0.598682\nC  0.631376  0.598966\nD  0.229858  0.378817\n\nIn [4]: df2\nOut[4]: \n          I         J\nA  0.939620  0.984616\nB  0.314818  0.456252\nC  0.630907  0.656341\nD  0.020994  0.538303\n', 'In [5]: ax = df1.plot(kind=""bar"", stacked=True)\n\nIn [5]: ax2 = df2.plot(kind=""bar"", stacked=True, ax = ax)\n', 'pd.concat(dict(df1 = df1, df2 = df2),axis = 1).plot(kind=""bar"", stacked=True)\n', ' pd.concat(dict(df1 = df1, df2 = df2),axis = 0).plot(kind=""bar"", stacked=True)\n', 'df1 df2    df1 df2\n_______    _______ ...\n   A          B\n']";"['df1=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])\n\n\ndf2\n']";"['import pandas as pd\ndf1=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])\n\n\ndf2\n']";True;"['import pandas as pd\ndf2 = pd.DataFrame()\ndf1 = pd.DataFrame()\ndf = pd.DataFrame()\ndf1=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])\n\n\ndf2\n']";True;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
744;744;744;744;1.0;7;22795348;;1;14;<python><matplotlib><pandas><seaborn>;Plotting time-series data with seaborn;22238.0;"['from pandas.util import testing\nfrom random import randrange\n\ndef random_date(start, end):\n    delta = end - start\n    int_delta = (delta.days * 24 * 60 * 60) + delta.seconds\n    random_second = randrange(int_delta)\n    return start + timedelta(seconds=random_second)\n\ndef rand_dataframe():\n  df = testing.makeDataFrame()\n  df[\'date\'] = [random_date(datetime.date(2014,3,18),datetime.date(2014,4,1)) for x in xrange(df.shape[0])]\n  df.sort(columns=[\'date\'], inplace=True)      \n  return df\n\ndf = rand_dataframe()\nsns.tsplot(df, time=""time"", unit=""unit"", condition=""condition"", value=""value"")\n      date         A         B         C         D\n\n2014-03-18  1.223777  0.356887  1.201624  1.968612\n2014-03-18  0.160730  1.888415  0.306334  0.203939\n2014-03-18 -0.203101 -0.161298  2.426540  0.056791\n2014-03-18 -1.350102  0.990093  0.495406  0.036215\n2014-03-18 -1.862960  2.673009 -0.545336 -0.925385\n2014-03-19  0.238281  0.468102 -0.150869  0.955069\n2014-03-20  1.575317  0.811892  0.198165  1.117805\n2014-03-20  0.822698 -0.398840 -1.277511  0.811691\n2014-03-20  2.143201 -0.827853 -0.989221  1.088297\n2014-03-20  0.299331  1.144311 -0.387854  0.209612\n2014-03-20  1.284111 -0.470287 -0.172949 -0.792020\n2014-03-22  1.031994  1.059394  0.037627  0.101246\n2014-03-22  0.889149  0.724618  0.459405  1.023127\n2014-03-23 -1.136320 -0.396265 -1.833737  1.478656\n2014-03-23 -0.740400 -0.644395 -1.221330  0.321805\n2014-03-23 -0.443021 -0.172013  0.020392 -2.368532\n2014-03-23  1.063545  0.039607  1.673722  1.707222\n2014-03-24  0.865192 -0.036810 -1.162648  0.947431\n2014-03-24 -1.671451  0.979238 -0.701093 -1.204192\n2014-03-26 -1.903534 -1.550349  0.267547 -0.585541\n2014-03-27  2.515671 -0.271228 -1.993744 -0.671797\n2014-03-27  1.728133 -0.423410 -0.620908  1.430503\n2014-03-28 -1.446037 -0.229452 -0.996486  0.120554\n2014-03-28 -0.664443 -0.665207  0.512771  0.066071\n2014-03-29 -1.093379 -0.936449 -0.930999  0.389743\n2014-03-29  1.205712 -0.356070 -0.595944  0.702238\n2014-03-29 -1.069506  0.358093  1.217409 -2.286798\n2014-03-29  2.441311  1.391739 -0.838139  0.226026\n2014-03-31  1.471447 -0.987615  0.201999  1.228070\n2014-03-31 -0.050524  0.539846  0.133359 -0.833252\n']";"[""from pandas.util import testing\nfrom random import randrange\n\ndef random_date(start, end):\n    delta = end - start\n    int_delta = (delta.days * 24 * 60 * 60) + delta.seconds\n    random_second = randrange(int_delta)\n    return start + timedelta(seconds=random_second)\n\ndef rand_dataframe():\n  df = testing.makeDataFrame()\n  df['date'] = [random_date(datetime.date(2014,3,18),datetime.date(2014,4,1)) for x in xrange(df.shape[0])]\n  df.sort(columns=['date'], inplace=True)      \n  return df\n\ndf = rand_dataframe()\n"", 'sns.tsplot(df, time=""time"", unit=""unit"", condition=""condition"", value=""value"")\n', '      date         A         B         C         D\n\n2014-03-18  1.223777  0.356887  1.201624  1.968612\n2014-03-18  0.160730  1.888415  0.306334  0.203939\n2014-03-18 -0.203101 -0.161298  2.426540  0.056791\n2014-03-18 -1.350102  0.990093  0.495406  0.036215\n2014-03-18 -1.862960  2.673009 -0.545336 -0.925385\n2014-03-19  0.238281  0.468102 -0.150869  0.955069\n2014-03-20  1.575317  0.811892  0.198165  1.117805\n2014-03-20  0.822698 -0.398840 -1.277511  0.811691\n2014-03-20  2.143201 -0.827853 -0.989221  1.088297\n2014-03-20  0.299331  1.144311 -0.387854  0.209612\n2014-03-20  1.284111 -0.470287 -0.172949 -0.792020\n2014-03-22  1.031994  1.059394  0.037627  0.101246\n2014-03-22  0.889149  0.724618  0.459405  1.023127\n2014-03-23 -1.136320 -0.396265 -1.833737  1.478656\n2014-03-23 -0.740400 -0.644395 -1.221330  0.321805\n2014-03-23 -0.443021 -0.172013  0.020392 -2.368532\n2014-03-23  1.063545  0.039607  1.673722  1.707222\n2014-03-24  0.865192 -0.036810 -1.162648  0.947431\n2014-03-24 -1.671451  0.979238 -0.701093 -1.204192\n2014-03-26 -1.903534 -1.550349  0.267547 -0.585541\n2014-03-27  2.515671 -0.271228 -1.993744 -0.671797\n2014-03-27  1.728133 -0.423410 -0.620908  1.430503\n2014-03-28 -1.446037 -0.229452 -0.996486  0.120554\n2014-03-28 -0.664443 -0.665207  0.512771  0.066071\n2014-03-29 -1.093379 -0.936449 -0.930999  0.389743\n2014-03-29  1.205712 -0.356070 -0.595944  0.702238\n2014-03-29 -1.069506  0.358093  1.217409 -2.286798\n2014-03-29  2.441311  1.391739 -0.838139  0.226026\n2014-03-31  1.471447 -0.987615  0.201999  1.228070\n2014-03-31 -0.050524  0.539846  0.133359 -0.833252\n']";"['Dataframe', ""from pandas.util import testing\nfrom random import randrange\n\ndef random_date(start, end):\n    delta = end - start\n    int_delta = (delta.days * 24 * 60 * 60) + delta.seconds\n    random_second = randrange(int_delta)\n    return start + timedelta(seconds=random_second)\n\ndef rand_dataframe():\n  df = testing.makeDataFrame()\n  df['date'] = [random_date(datetime.date(2014,3,18),datetime.date(2014,4,1)) for x in xrange(df.shape[0])]\n  df.sort(columns=['date'], inplace=True)      \n  return df\n\ndf = rand_dataframe()\n"", 'A', 'B', 'C', 'D', 'seaborn', 'sns.tsplot(df, time=""time"", unit=""unit"", condition=""condition"", value=""value"")\n', 'time', 'unit', 'condition', 'value', '      date         A         B         C         D\n\n2014-03-18  1.223777  0.356887  1.201624  1.968612\n2014-03-18  0.160730  1.888415  0.306334  0.203939\n2014-03-18 -0.203101 -0.161298  2.426540  0.056791\n2014-03-18 -1.350102  0.990093  0.495406  0.036215\n2014-03-18 -1.862960  2.673009 -0.545336 -0.925385\n2014-03-19  0.238281  0.468102 -0.150869  0.955069\n2014-03-20  1.575317  0.811892  0.198165  1.117805\n2014-03-20  0.822698 -0.398840 -1.277511  0.811691\n2014-03-20  2.143201 -0.827853 -0.989221  1.088297\n2014-03-20  0.299331  1.144311 -0.387854  0.209612\n2014-03-20  1.284111 -0.470287 -0.172949 -0.792020\n2014-03-22  1.031994  1.059394  0.037627  0.101246\n2014-03-22  0.889149  0.724618  0.459405  1.023127\n2014-03-23 -1.136320 -0.396265 -1.833737  1.478656\n2014-03-23 -0.740400 -0.644395 -1.221330  0.321805\n2014-03-23 -0.443021 -0.172013  0.020392 -2.368532\n2014-03-23  1.063545  0.039607  1.673722  1.707222\n2014-03-24  0.865192 -0.036810 -1.162648  0.947431\n2014-03-24 -1.671451  0.979238 -0.701093 -1.204192\n2014-03-26 -1.903534 -1.550349  0.267547 -0.585541\n2014-03-27  2.515671 -0.271228 -1.993744 -0.671797\n2014-03-27  1.728133 -0.423410 -0.620908  1.430503\n2014-03-28 -1.446037 -0.229452 -0.996486  0.120554\n2014-03-28 -0.664443 -0.665207  0.512771  0.066071\n2014-03-29 -1.093379 -0.936449 -0.930999  0.389743\n2014-03-29  1.205712 -0.356070 -0.595944  0.702238\n2014-03-29 -1.069506  0.358093  1.217409 -2.286798\n2014-03-29  2.441311  1.391739 -0.838139  0.226026\n2014-03-31  1.471447 -0.987615  0.201999  1.228070\n2014-03-31 -0.050524  0.539846  0.133359 -0.833252\n']";"['from pandas.util import testing\nfrom random import randrange\n\n\n\ndf = rand_dataframe()\nsns.tsplot(df, time=""time"", unit=""unit"", condition=""condition"", value=""value"")\n\n']";"['from pandas.util import testing\nfrom random import randrange\n\n\n\ndf = rand_dataframe()\nsns.tsplot(df, time=""time"", unit=""unit"", condition=""condition"", value=""value"")\n\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\nfrom pandas.util import testing\nfrom random import randrange\n\n\n\ndf = rand_dataframe()\nsns.tsplot(df, time=""time"", unit=""unit"", condition=""condition"", value=""value"")\n\n']";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;['\n    Pyperclip could not find a copy/paste mechanism for your system.\n    For more information, please visit https://pyperclip.readthedocs.org '];['PyperclipException'];0;1;['\n    Pyperclip could not find a copy/paste mechanism for your system.\n    For more information, please visit https://pyperclip.readthedocs.org '];['PyperclipException']
745;745;745;745;5.0;0;22798934;;1;11;<python><pandas>;Pandas long to wide reshape;11082.0;['Salesman  Height   product      price\n  Knut      6        bat          5\n  Knut      6        ball         1\n  Knut      6        wand         3\n  Steve     5        pen          2\nSalesman  Height    product_1  price_1  product_2 price_2 product_3 price_3  \n  Knut      6        bat          5       ball      1        wand      3\n  Steve     5        pen          2        NA       NA        NA       NA\n'];['Salesman  Height   product      price\n  Knut      6        bat          5\n  Knut      6        ball         1\n  Knut      6        wand         3\n  Steve     5        pen          2\n', 'Salesman  Height    product_1  price_1  product_2 price_2 product_3 price_3  \n  Knut      6        bat          5       ball      1        wand      3\n  Steve     5        pen          2        NA       NA        NA       NA\n'];['Salesman  Height   product      price\n  Knut      6        bat          5\n  Knut      6        ball         1\n  Knut      6        wand         3\n  Steve     5        pen          2\n', 'Salesman  Height    product_1  price_1  product_2 price_2 product_3 price_3  \n  Knut      6        bat          5       ball      1        wand      3\n  Steve     5        pen          2        NA       NA        NA       NA\n'];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""No module named 'StringIO'""]";['NameError', 'NameError', 'ImportError'];0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""No module named 'StringIO'""]";['NameError', 'NameError', 'ImportError'];0;3;"[""'salesman'"", ""'Salesman'"", ""No module named 'StringIO'""]";['KeyError', 'KeyError', 'ImportError']
746;746;746;746;3.0;1;22799300;;1;14;<python><pandas>;How to unpack a Series of tuples in Pandas?;6603.0;"['import numpy as np\nfrom scipy import stats\ndf = pd.DataFrame(dict(x=np.random.randn(100),\n                       y=np.repeat(list(""abcd""), 25)))\nout = df.groupby(""y"").x.apply(stats.ttest_1samp, 0)\nprint out\n\ny\na       (1.3066417476, 0.203717485506)\nb    (0.0801133382517, 0.936811414675)\nc      (1.55784329113, 0.132360504653)\nd     (0.267999459642, 0.790989680709)\ndtype: object\nt, p = zip(*out)\n (array(1.3066417475999257),\n array(0.08011333825171714),\n array(1.557843291126335),\n array(0.267999459641651))\n']";"['import numpy as np\nfrom scipy import stats\ndf = pd.DataFrame(dict(x=np.random.randn(100),\n                       y=np.repeat(list(""abcd""), 25)))\nout = df.groupby(""y"").x.apply(stats.ttest_1samp, 0)\nprint out\n\ny\na       (1.3066417476, 0.203717485506)\nb    (0.0801133382517, 0.936811414675)\nc      (1.55784329113, 0.132360504653)\nd     (0.267999459642, 0.790989680709)\ndtype: object\n', 't, p = zip(*out)\n', ' (array(1.3066417475999257),\n array(0.08011333825171714),\n array(1.557843291126335),\n array(0.267999459641651))\n']";"['import numpy as np\nfrom scipy import stats\ndf = pd.DataFrame(dict(x=np.random.randn(100),\n                       y=np.repeat(list(""abcd""), 25)))\nout = df.groupby(""y"").x.apply(stats.ttest_1samp, 0)\nprint out\n\ny\na       (1.3066417476, 0.203717485506)\nb    (0.0801133382517, 0.936811414675)\nc      (1.55784329113, 0.132360504653)\nd     (0.267999459642, 0.790989680709)\ndtype: object\n', 't, p = zip(*out)\n', 't', ' (array(1.3066417475999257),\n array(0.08011333825171714),\n array(1.557843291126335),\n array(0.267999459641651))\n']";"['import numpy as np\nfrom scipy import stats\nout = df.groupby(""y"").x.apply(stats.ttest_1samp, 0)\n\ny\na       (1.3066417476, 0.203717485506)\nb    (0.0801133382517, 0.936811414675)\nc      (1.55784329113, 0.132360504653)\nd     (0.267999459642, 0.790989680709)\nt, p = zip(*out)\n']";"['import numpy as np\nfrom scipy import stats\nout = df.groupby(""y"").x.apply(stats.ttest_1samp, 0)\n\ny\na       (1.3066417476, 0.203717485506)\nb    (0.0801133382517, 0.936811414675)\nc      (1.55784329113, 0.132360504653)\nd     (0.267999459642, 0.790989680709)\nt, p = zip(*out)\n']";False;"['import pandas as pd\ndf = pd.DataFrame()\nimport numpy as np\nfrom scipy import stats\nout = df.groupby(""y"").x.apply(stats.ttest_1samp, 0)\n\ny\na       (1.3066417476, 0.203717485506)\nb    (0.0801133382517, 0.936811414675)\nc      (1.55784329113, 0.132360504653)\nd     (0.267999459642, 0.790989680709)\nt, p = zip(*out)\n']";True;0;2;"[""name 'y' is not defined"", ""name 'out' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'y' is not defined"", ""name 'out' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'y' is not defined"", ""name 'out' is not defined""]";['NameError', 'NameError']
747;747;747;747;2.0;0;22825349;;1;11;<python><datetime><pandas>;Converting between datetime and Pandas Timestamp objects;10290.0;"[""> date1\nTimestamp('2014-01-23 00:00:00', tz=None)\n\n> date2\ndatetime.date(2014, 3, 26)\n> pd.to_datetime(date1)   \nTimestamp('2014-01-23 00:00:00', tz=None)\n""]";"[""> date1\nTimestamp('2014-01-23 00:00:00', tz=None)\n\n> date2\ndatetime.date(2014, 3, 26)\n"", ""> pd.to_datetime(date1)   \nTimestamp('2014-01-23 00:00:00', tz=None)\n""]";"[""> date1\nTimestamp('2014-01-23 00:00:00', tz=None)\n\n> date2\ndatetime.date(2014, 3, 26)\n"", 'pandas.to_datetime()', 'Timestamps', 'datetime', ""> pd.to_datetime(date1)   \nTimestamp('2014-01-23 00:00:00', tz=None)\n""]";"[""Timestamp('2014-01-23 00:00:00', tz=None)\n\ndatetime.date(2014, 3, 26)\nTimestamp('2014-01-23 00:00:00', tz=None)\n""]";"[""Timestamp('2014-01-23 00:00:00', tz=None)\n\ndatetime.date(2014, 3, 26)\nTimestamp('2014-01-23 00:00:00', tz=None)\n""]";False;"[""import pandas as pd\nTimestamp('2014-01-23 00:00:00', tz=None)\n\ndatetime.date(2014, 3, 26)\nTimestamp('2014-01-23 00:00:00', tz=None)\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
748;748;748;748;1.0;0;22833404;;1;11;<python><matplotlib><plot><pandas>;How do I plot hatched bars using pandas?;1573.0;"[""df = pd.DataFrame(rand(10, 4), columns=['a', 'b', 'c', 'd'])\ndf.plot(kind='bar', hatch='/');\n""]";"[""df = pd.DataFrame(rand(10, 4), columns=['a', 'b', 'c', 'd'])\ndf.plot(kind='bar', hatch='/');\n""]";"['hatch', 'plot', 'DataFrame', ""df = pd.DataFrame(rand(10, 4), columns=['a', 'b', 'c', 'd'])\ndf.plot(kind='bar', hatch='/');\n"", 'colormap', 'Axes', 'plot']";"[""df = pd.DataFrame(rand(10, 4), columns=['a', 'b', 'c', 'd'])\ndf.plot(kind='bar', hatch='/');\n""]";"[""import pandas as pd\ndf = pd.DataFrame(rand(10, 4), columns=['a', 'b', 'c', 'd'])\ndf.plot(kind='bar', hatch='/');\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame(rand(10, 4), columns=['a', 'b', 'c', 'd'])\ndf.plot(kind='bar', hatch='/');\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
749;749;749;749;1.0;1;22840449;;1;12;<python><eclipse><pandas>;How to update Pandas from Anaconda and is it possible to use eclipse with this last?;17480.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
750;750;750;750;4.0;0;22898824;;1;23;<python><datetime><pandas><filtering><dataframe>;filtering pandas dataframes on dates;50394.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"['\n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated', '\n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated', ""'date'""]";['DeprecationWarning', 'DeprecationWarning', 'KeyError']
751;751;751;751;3.0;2;22918212;;1;13;<python><pandas><duplicate-removal>;Fastest Way to Drop Duplicated Index in a Pandas DataFrame;11999.0;"[""myDF.drop_duplicates(cols=index)\nmyDF.drop_duplicates(cols='index') \nmyDF['index'] = myDF.index\nmyDF= myDF.drop_duplicates(cols='index')\nmyDF.set_index = myDF['index']\nmyDF= myDF.drop('index', axis =1)\n""]";"['myDF.drop_duplicates(cols=index)\n', ""myDF.drop_duplicates(cols='index') \n"", ""myDF['index'] = myDF.index\nmyDF= myDF.drop_duplicates(cols='index')\nmyDF.set_index = myDF['index']\nmyDF= myDF.drop('index', axis =1)\n""]";"['myDF.drop_duplicates(cols=index)\n', ""myDF.drop_duplicates(cols='index') \n"", ""myDF['index'] = myDF.index\nmyDF= myDF.drop_duplicates(cols='index')\nmyDF.set_index = myDF['index']\nmyDF= myDF.drop('index', axis =1)\n""]";"[""myDF.drop_duplicates(cols=index)\nmyDF.drop_duplicates(cols='index') \nmyDF['index'] = myDF.index\nmyDF= myDF.drop_duplicates(cols='index')\nmyDF.set_index = myDF['index']\nmyDF= myDF.drop('index', axis =1)\n""]";"[""myDF.drop_duplicates(cols=index)\nmyDF.drop_duplicates(cols='index') \nmyDF['index'] = myDF.index\nmyDF= myDF.drop_duplicates(cols='index')\nmyDF.set_index = myDF['index']\nmyDF= myDF.drop('index', axis =1)\n""]";False;"[""import pandas as pd\nmyDF.drop_duplicates(cols=index)\nmyDF.drop_duplicates(cols='index') \nmyDF['index'] = myDF.index\nmyDF= myDF.drop_duplicates(cols='index')\nmyDF.set_index = myDF['index']\nmyDF= myDF.drop('index', axis =1)\n""]";False;1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
752;752;752;752;2.0;0;22923775;;1;21;<python><datetime><pandas>;Calculate Pandas DataFrame Time Difference Between Two Columns in Hours and Minutes;31742.0;"['df[\'diff\'] = df[\'todate\'] - df[\'fromdate\']\n2014-01-24 13:03:12.050000,2014-01-26 23:41:21.870000,""2 days, 10:38:09.820000""\n2014-01-27 11:57:18.240000,2014-01-27 15:38:22.540000,03:41:04.300000\n2014-01-23 10:07:47.660000,2014-01-23 18:50:41.420000,08:42:53.760000\n']";"[""df['diff'] = df['todate'] - df['fromdate']\n"", '2014-01-24 13:03:12.050000,2014-01-26 23:41:21.870000,""2 days, 10:38:09.820000""\n2014-01-27 11:57:18.240000,2014-01-27 15:38:22.540000,03:41:04.300000\n2014-01-23 10:07:47.660000,2014-01-23 18:50:41.420000,08:42:53.760000\n']";"[""df['diff'] = df['todate'] - df['fromdate']\n"", '2014-01-24 13:03:12.050000,2014-01-26 23:41:21.870000,""2 days, 10:38:09.820000""\n2014-01-27 11:57:18.240000,2014-01-27 15:38:22.540000,03:41:04.300000\n2014-01-23 10:07:47.660000,2014-01-23 18:50:41.420000,08:42:53.760000\n']";"[""df['diff'] = df['todate'] - df['fromdate']\n""]";"[""df['diff'] = df['todate'] - df['fromdate']\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf['diff'] = df['todate'] - df['fromdate']\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
753;753;753;753;4.0;3;22963263;;1;22;<python><pandas><dataframe>;Creating a zero-filled pandas data frame;29486.0;['zero_data = np.zeros(shape=(len(data),len(feature_list)))\nd = pd.DataFrame(zero_data, columns=feature_list)\n'];['zero_data = np.zeros(shape=(len(data),len(feature_list)))\nd = pd.DataFrame(zero_data, columns=feature_list)\n'];['zero_data = np.zeros(shape=(len(data),len(feature_list)))\nd = pd.DataFrame(zero_data, columns=feature_list)\n'];['zero_data = np.zeros(shape=(len(data),len(feature_list)))\nd = pd.DataFrame(zero_data, columns=feature_list)\n'];['import pandas as pd\nzero_data = np.zeros(shape=(len(data),len(feature_list)))\nd = pd.DataFrame(zero_data, columns=feature_list)\n'];True;['import pandas as pd\nzero_data = np.zeros(shape=(len(data),len(feature_list)))\nd = pd.DataFrame(zero_data, columns=feature_list)\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
754;754;754;754;8.0;3;22991567;;1;11;<python><pandas>;Pandas yahoo finance DataReader;26300.0;"[""stocks = ['ORCL', 'TSLA', 'IBM','YELP', 'MSFT']\nls_key = 'Adj Close'\nstart = datetime(2014,1,1)\nend = datetime(2014,3,28)    \nf = web.DataReader(stocks, 'yahoo',start,end)\n\n\ncleanData = f.ix[ls_key]\ndataFrame = pd.DataFrame(cleanData)\n\nprint dataFrame[:5]\n              IBM   MSFT   ORCL    TSLA   YELP\nDate                                           \n2014-01-02  184.52  36.88  37.61  150.10  67.92\n2014-01-03  185.62  36.64  37.51  149.56  67.66\n2014-01-06  184.99  35.86  37.36  147.00  71.72\n2014-01-07  188.68  36.14  37.74  149.36  72.66\n2014-01-08  186.95  35.49  37.61  151.28  78.42\nprint dataFrame['Date']\nKeyError: u'no item named Date'\n""]";"[""stocks = ['ORCL', 'TSLA', 'IBM','YELP', 'MSFT']\nls_key = 'Adj Close'\nstart = datetime(2014,1,1)\nend = datetime(2014,3,28)    \nf = web.DataReader(stocks, 'yahoo',start,end)\n\n\ncleanData = f.ix[ls_key]\ndataFrame = pd.DataFrame(cleanData)\n\nprint dataFrame[:5]\n"", '              IBM   MSFT   ORCL    TSLA   YELP\nDate                                           \n2014-01-02  184.52  36.88  37.61  150.10  67.92\n2014-01-03  185.62  36.64  37.51  149.56  67.66\n2014-01-06  184.99  35.86  37.36  147.00  71.72\n2014-01-07  188.68  36.14  37.74  149.36  72.66\n2014-01-08  186.95  35.49  37.61  151.28  78.42\n', ""print dataFrame['Date']\n"", ""KeyError: u'no item named Date'\n""]";"[""stocks = ['ORCL', 'TSLA', 'IBM','YELP', 'MSFT']\nls_key = 'Adj Close'\nstart = datetime(2014,1,1)\nend = datetime(2014,3,28)    \nf = web.DataReader(stocks, 'yahoo',start,end)\n\n\ncleanData = f.ix[ls_key]\ndataFrame = pd.DataFrame(cleanData)\n\nprint dataFrame[:5]\n"", '              IBM   MSFT   ORCL    TSLA   YELP\nDate                                           \n2014-01-02  184.52  36.88  37.61  150.10  67.92\n2014-01-03  185.62  36.64  37.51  149.56  67.66\n2014-01-06  184.99  35.86  37.36  147.00  71.72\n2014-01-07  188.68  36.14  37.74  149.36  72.66\n2014-01-08  186.95  35.49  37.61  151.28  78.42\n', ""print dataFrame['Date']\n"", ""KeyError: u'no item named Date'\n""]";"[""stocks = ['ORCL', 'TSLA', 'IBM','YELP', 'MSFT']\nls_key = 'Adj Close'\nstart = datetime(2014,1,1)\nend = datetime(2014,3,28)    \nf = web.DataReader(stocks, 'yahoo',start,end)\n\n\ncleanData = f.ix[ls_key]\ndataFrame = pd.DataFrame(cleanData)\n\nDate                                           \n""]";"[""import pandas as pd\nstocks = ['ORCL', 'TSLA', 'IBM','YELP', 'MSFT']\nls_key = 'Adj Close'\nstart = datetime(2014,1,1)\nend = datetime(2014,3,28)    \nf = web.DataReader(stocks, 'yahoo',start,end)\n\n\ncleanData = f.ix[ls_key]\ndataFrame = pd.DataFrame(cleanData)\n\nDate                                           \n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\nstocks = ['ORCL', 'TSLA', 'IBM','YELP', 'MSFT']\nls_key = 'Adj Close'\nstart = datetime(2014,1,1)\nend = datetime(2014,3,28)    \nf = web.DataReader(stocks, 'yahoo',start,end)\n\n\ncleanData = f.ix[ls_key]\ndataFrame = pd.DataFrame(cleanData)\n\nDate                                           \n""]";True;0;0;[];[];0;0;[];[];0;0;[];[]
755;755;755;755;4.0;0;23002762;;1;17;<python><python-2.7><pandas>;Pandas: SettingWithCopyWarning;28360.0;"[""sve2_all[sve2_all[' Hgtot ng/l'] > 100] = np.nan\n-c:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\nC:\\Users\\AppData\\Local\\Enthought\\Canopy32\\User\\lib\\site-packages\\pandas\\core\\indexing.py:346: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\nself.obj[item] = s\n""]";"[""sve2_all[sve2_all[' Hgtot ng/l'] > 100] = np.nan\n"", '-c:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\nC:\\Users\\AppData\\Local\\Enthought\\Canopy32\\User\\lib\\site-packages\\pandas\\core\\indexing.py:346: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\nself.obj[item] = s\n']";"['Pandas', 'DataFrame', 'NaN', ""sve2_all[sve2_all[' Hgtot ng/l'] > 100] = np.nan\n"", '-c:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\nC:\\Users\\AppData\\Local\\Enthought\\Canopy32\\User\\lib\\site-packages\\pandas\\core\\indexing.py:346: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\nself.obj[item] = s\n', 'DataFrame']";"[""sve2_all[sve2_all[' Hgtot ng/l'] > 100] = np.nan\nself.obj[item] = s\n""]";"[""sve2_all[sve2_all[' Hgtot ng/l'] > 100] = np.nan\nself.obj[item] = s\n""]";False;"[""import pandas as pd\nsve2_all[sve2_all[' Hgtot ng/l'] > 100] = np.nan\nself.obj[item] = s\n""]";False;0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
756;756;756;756;1.0;0;23103962;;1;16;<python><postgresql><pandas><sqlalchemy>;How to write DataFrame to postgres table?;19350.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""No module named 'sqlalchemy'""]";['ImportError'];0;1;"[""No module named 'sqlalchemy'""]";['ImportError'];0;1;"[""No module named 'sqlalchemy'""]";['ImportError']
757;757;757;757;2.0;7;23111990;;1;17;<python><string><list><pandas><dataframe>;Pandas DataFrame stored list as string: How to convert back to list?;8688.0;"["">>> df = DataFrame(columns=['col1'])\n>>> df.append(Series([None]), ignore_index=True)\n>>> df\nEmpty DataFrame\nColumns: [col1]\nIndex: []\n>>> df['column1'][0] = [1.23, 2.34]\n>>> df\n     col1\n0  [1, 2]\n>>> df['column1'][0]\n'[1.23, 2.34]'\n""]";"["">>> df = DataFrame(columns=['col1'])\n>>> df.append(Series([None]), ignore_index=True)\n>>> df\nEmpty DataFrame\nColumns: [col1]\nIndex: []\n"", "">>> df['column1'][0] = [1.23, 2.34]\n>>> df\n     col1\n0  [1, 2]\n"", "">>> df['column1'][0]\n'[1.23, 2.34]'\n""]";"['df', "">>> df = DataFrame(columns=['col1'])\n>>> df.append(Series([None]), ignore_index=True)\n>>> df\nEmpty DataFrame\nColumns: [col1]\nIndex: []\n"", "">>> df['column1'][0] = [1.23, 2.34]\n>>> df\n     col1\n0  [1, 2]\n"", "">>> df['column1'][0]\n'[1.23, 2.34]'\n""]";"[""0  [1, 2]\n'[1.23, 2.34]'\n""]";"[""0  [1, 2]\n'[1.23, 2.34]'\n""]";False;"[""import pandas as pd\n0  [1, 2]\n'[1.23, 2.34]'\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
758;758;758;758;2.0;0;23142967;;1;26;<pandas><dataframe><series>;Adding a column thats result of difference in consecutive rows in pandas;15505.0;['    A   B\n0   a   b\n1   c   d\n2   e   f \n3   g   h\n    A   B   dA\n0   a   b  (a-c)\n1   c   d  (c-e)\n2   e   f  (e-g)\n3   g   h   Nan\n'];['    A   B\n0   a   b\n1   c   d\n2   e   f \n3   g   h\n', '    A   B   dA\n0   a   b  (a-c)\n1   c   d  (c-e)\n2   e   f  (e-g)\n3   g   h   Nan\n'];['    A   B\n0   a   b\n1   c   d\n2   e   f \n3   g   h\n', '    A   B   dA\n0   a   b  (a-c)\n1   c   d  (c-e)\n2   e   f  (e-g)\n3   g   h   Nan\n'];[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""'A'"", 'Sucess']";['KeyError', 'Sucess']
759;759;759;759;2.0;0;23145928;;1;12;<python><pandas>;python and pandas - how to access a column using iterrows;18257.0;"['print df\nInt64Index: 152 entries, 0 to 151\nData columns:\nDate          152  non-null values\nTime          152  non-null values\nTime Zone     152  non-null values\nCurrency      152  non-null values\nEvent         152  non-null values\nImportance    152  non-null values\nActual        127  non-null values\nForecast      86  non-null values\nPrevious      132  non-null values\ndtypes: object(9)\n\nfor row in df.iterrows():\n    print row[\'Date\']\n\nTraceback (most recent call last):\n  File ""/home/ubuntu/workspace/calandar.py"", line 34, in <module>\n    print row[\'Date\']\nTypeError: tuple indices must be integers, not str\n(0, Date                                                 Sun Apr 13\nTime                                                      17:30\nTime Zone                                                   GMT\nCurrency                                                    USD\nEvent         USD Fed\'s Stein Speaks on Financial Stability ...\nImportance                                                  Low\nActual                                                      NaN\nForecast                                                    NaN\nPrevious                                                    NaN\nName: 0)\n']";"['print df\nInt64Index: 152 entries, 0 to 151\nData columns:\nDate          152  non-null values\nTime          152  non-null values\nTime Zone     152  non-null values\nCurrency      152  non-null values\nEvent         152  non-null values\nImportance    152  non-null values\nActual        127  non-null values\nForecast      86  non-null values\nPrevious      132  non-null values\ndtypes: object(9)\n\nfor row in df.iterrows():\n    print row[\'Date\']\n\nTraceback (most recent call last):\n  File ""/home/ubuntu/workspace/calandar.py"", line 34, in <module>\n    print row[\'Date\']\nTypeError: tuple indices must be integers, not str\n', ""(0, Date                                                 Sun Apr 13\nTime                                                      17:30\nTime Zone                                                   GMT\nCurrency                                                    USD\nEvent         USD Fed's Stein Speaks on Financial Stability ...\nImportance                                                  Low\nActual                                                      NaN\nForecast                                                    NaN\nPrevious                                                    NaN\nName: 0)\n""]";"['print df\nInt64Index: 152 entries, 0 to 151\nData columns:\nDate          152  non-null values\nTime          152  non-null values\nTime Zone     152  non-null values\nCurrency      152  non-null values\nEvent         152  non-null values\nImportance    152  non-null values\nActual        127  non-null values\nForecast      86  non-null values\nPrevious      132  non-null values\ndtypes: object(9)\n\nfor row in df.iterrows():\n    print row[\'Date\']\n\nTraceback (most recent call last):\n  File ""/home/ubuntu/workspace/calandar.py"", line 34, in <module>\n    print row[\'Date\']\nTypeError: tuple indices must be integers, not str\n', ""(0, Date                                                 Sun Apr 13\nTime                                                      17:30\nTime Zone                                                   GMT\nCurrency                                                    USD\nEvent         USD Fed's Stein Speaks on Financial Stability ...\nImportance                                                  Low\nActual                                                      NaN\nForecast                                                    NaN\nPrevious                                                    NaN\nName: 0)\n""]";['\n\n'];['\n\n'];False;['import pandas as pd\n\n\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
760;760;760;760;3.0;0;23151246;;1;18;<python><pandas><next>;iterrows pandas get next rows value;40825.0;"[""import pandas as pd\ndf = pd.DataFrame(['AA', 'BB', 'CC'], columns = ['value'])\nfor i, row in df.iterrows():\n     print row['value']\n     i1, row1 = next(df.iterrows())\n     print row1['value']\n'AA'\n'BB'\n'BB'\n'CC'\n'CC'\n*Wrong index error here  \nfor i in range(0, df.shape[0])\n   print df.irow(i)['value']\n   print df.irow(i+1)['value']\n""]";"[""import pandas as pd\ndf = pd.DataFrame(['AA', 'BB', 'CC'], columns = ['value'])\n"", ""for i, row in df.iterrows():\n     print row['value']\n     i1, row1 = next(df.iterrows())\n     print row1['value']\n"", ""'AA'\n'BB'\n'BB'\n'CC'\n'CC'\n*Wrong index error here  \n"", ""for i in range(0, df.shape[0])\n   print df.irow(i)['value']\n   print df.irow(i+1)['value']\n""]";"[""import pandas as pd\ndf = pd.DataFrame(['AA', 'BB', 'CC'], columns = ['value'])\n"", 's value and next row', ""for i, row in df.iterrows():\n     print row['value']\n     i1, row1 = next(df.iterrows())\n     print row1['value']\n"", ""'AA'\n'BB'\n'BB'\n'CC'\n'CC'\n*Wrong index error here  \n"", ""for i in range(0, df.shape[0])\n   print df.irow(i)['value']\n   print df.irow(i+1)['value']\n""]";"[""import pandas as pd\ndf = pd.DataFrame(['AA', 'BB', 'CC'], columns = ['value'])\n'AA'\n'BB'\n'BB'\n'CC'\n'CC'\n""]";"[""import pandas as pd\ndf = pd.DataFrame(['AA', 'BB', 'CC'], columns = ['value'])\n'AA'\n'BB'\n'BB'\n'CC'\n'CC'\n""]";False;"[""import pandas as pd\nimport pandas as pd\ndf = pd.DataFrame(['AA', 'BB', 'CC'], columns = ['value'])\n'AA'\n'BB'\n'BB'\n'CC'\n'CC'\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'generator' object has no attribute 'next'""]";['AttributeError']
761;761;761;761;2.0;0;23178129;;1;12;<python><datetime><pandas>;Getting min and max Date's from a pandas dataframe;11701.0;['           value\nDate                                           \n2014-03-13  10000.000 \n2014-03-21   2000.000 \n2014-03-27   2000.000 \n2014-03-17    200.000 \n2014-03-17      5.000 \n2014-03-17     70.000 \n2014-03-21    200.000 \n2014-03-27      5.000 \n2014-03-27     25.000 \n2014-03-31      0.020 \n2014-03-31     12.000 \n2014-03-31      0.022\n'];['           value\nDate                                           \n2014-03-13  10000.000 \n2014-03-21   2000.000 \n2014-03-27   2000.000 \n2014-03-17    200.000 \n2014-03-17      5.000 \n2014-03-17     70.000 \n2014-03-21    200.000 \n2014-03-27      5.000 \n2014-03-27     25.000 \n2014-03-31      0.020 \n2014-03-31     12.000 \n2014-03-31      0.022\n'];['           value\nDate                                           \n2014-03-13  10000.000 \n2014-03-21   2000.000 \n2014-03-27   2000.000 \n2014-03-17    200.000 \n2014-03-17      5.000 \n2014-03-17     70.000 \n2014-03-21    200.000 \n2014-03-27      5.000 \n2014-03-27     25.000 \n2014-03-31      0.020 \n2014-03-31     12.000 \n2014-03-31      0.022\n', '2014-03-13', '2014-03-31', 'numpy.min', 'df.min(axis=0)'];['Date                                           \n'];['Date                                           \n'];False;['import pandas as pd\nDate                                           \n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
762;762;762;762;1.0;0;23198053;;1;13;<python><pandas>;How do you shift Pandas DataFrame with a multiindex?;6890.0;['                  line_date  line_race  beyer\nhorse                                        \nLast Gunfighter  2013-09-28         10     99\nLast Gunfighter  2013-08-18         10    102\nLast Gunfighter  2013-07-06          8    103\n.....\nPaynter          2013-09-28         10    103\nPaynter          2013-08-31         10     88\nPaynter          2013-07-27          8    100\n                  line_date  line_race  beyer  beyer_shifted\nhorse                                                       \nLast Gunfighter  2013-09-28         10     99            NaN\nLast Gunfighter  2013-08-18         10    102             99\nLast Gunfighter  2013-07-06          8    103            102\n.....\nPaynter          2013-09-28         10    103             71\nPaynter          2013-08-31         10     88            103\nPaynter          2013-07-27          8    100             88\n                  line_date  line_race  beyer  beyer_shifted\nhorse                                                       \nLast Gunfighter  2013-09-28         10     99            NaN\nLast Gunfighter  2013-08-18         10    102             99\nLast Gunfighter  2013-07-06          8    103            102\n.....\nPaynter          2013-09-28         10    103            NaN\nPaynter          2013-08-31         10     88            103\nPaynter          2013-07-27          8    100             88\n'];['                  line_date  line_race  beyer\nhorse                                        \nLast Gunfighter  2013-09-28         10     99\nLast Gunfighter  2013-08-18         10    102\nLast Gunfighter  2013-07-06          8    103\n.....\nPaynter          2013-09-28         10    103\nPaynter          2013-08-31         10     88\nPaynter          2013-07-27          8    100\n', '                  line_date  line_race  beyer  beyer_shifted\nhorse                                                       \nLast Gunfighter  2013-09-28         10     99            NaN\nLast Gunfighter  2013-08-18         10    102             99\nLast Gunfighter  2013-07-06          8    103            102\n.....\nPaynter          2013-09-28         10    103             71\nPaynter          2013-08-31         10     88            103\nPaynter          2013-07-27          8    100             88\n', '                  line_date  line_race  beyer  beyer_shifted\nhorse                                                       \nLast Gunfighter  2013-09-28         10     99            NaN\nLast Gunfighter  2013-08-18         10    102             99\nLast Gunfighter  2013-07-06          8    103            102\n.....\nPaynter          2013-09-28         10    103            NaN\nPaynter          2013-08-31         10     88            103\nPaynter          2013-07-27          8    100             88\n'];"['                  line_date  line_race  beyer\nhorse                                        \nLast Gunfighter  2013-09-28         10     99\nLast Gunfighter  2013-08-18         10    102\nLast Gunfighter  2013-07-06          8    103\n.....\nPaynter          2013-09-28         10    103\nPaynter          2013-08-31         10     88\nPaynter          2013-07-27          8    100\n', ""df['beyer'].shift(1)"", '                  line_date  line_race  beyer  beyer_shifted\nhorse                                                       \nLast Gunfighter  2013-09-28         10     99            NaN\nLast Gunfighter  2013-08-18         10    102             99\nLast Gunfighter  2013-07-06          8    103            102\n.....\nPaynter          2013-09-28         10    103             71\nPaynter          2013-08-31         10     88            103\nPaynter          2013-07-27          8    100             88\n', '                  line_date  line_race  beyer  beyer_shifted\nhorse                                                       \nLast Gunfighter  2013-09-28         10     99            NaN\nLast Gunfighter  2013-08-18         10    102             99\nLast Gunfighter  2013-07-06          8    103            102\n.....\nPaynter          2013-09-28         10    103            NaN\nPaynter          2013-08-31         10     88            103\nPaynter          2013-07-27          8    100             88\n']";['horse                                        \nhorse                                                       \nhorse                                                       \n'];['horse                                        \nhorse                                                       \nhorse                                                       \n'];False;['import pandas as pd\nhorse                                        \nhorse                                                       \nhorse                                                       \n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'Column not found: beyer'""]";['KeyError']
763;763;763;763;6.0;1;23199796;;1;51;<python><pandas><filtering><dataframe><outliers>;Detect and exclude outliers in Pandas dataframe;40223.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;4;"[""name 'pd' is not defined"", ""name 'pd' is not defined"", ""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""name 'np' is not defined"", ""name 'np' is not defined"", ""name 'np' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""name 'np' is not defined"", ""name 'np' is not defined"", ""name 'np' is not defined"", ""'col'""]";['NameError', 'NameError', 'NameError', 'KeyError']
764;764;764;764;1.0;1;23282130;;1;25;<python><pandas><pca><scientific-computing><principal-components>;Principal components analysis using pandas dataframe;13077.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""No module named 'sklearn'""]";['ImportError'];0;1;"[""No module named 'sklearn'""]";['ImportError'];0;1;"[""No module named 'sklearn'""]";['ImportError']
765;765;765;765;1.0;0;23296282;;1;39;<python><pandas><indexing><dataframe><slice>;What rules does Pandas use to generate a view vs a copy?;8967.0;"[""df = pd.DataFrame(np.random.randn(8,8), columns=list('ABCDEFGH'), index=[1, 2, 3, 4, 5, 6, 7, 8])\nfoo = df.query('2 < index <= 5')\nfoo.loc[:,'E'] = 40\ndf.iloc[3] = 70\ndf.ix[1,'B':'E'] = 222\ndf[df.C <= df.B]  = 7654321\ndf[df.C <= df.B].ix[:,'B':'E']\n""]";"[""df = pd.DataFrame(np.random.randn(8,8), columns=list('ABCDEFGH'), index=[1, 2, 3, 4, 5, 6, 7, 8])\n"", ""foo = df.query('2 < index <= 5')\nfoo.loc[:,'E'] = 40\n"", 'df.iloc[3] = 70\n', ""df.ix[1,'B':'E'] = 222\n"", 'df[df.C <= df.B]  = 7654321\n', ""df[df.C <= df.B].ix[:,'B':'E']\n""]";"[""df = pd.DataFrame(np.random.randn(8,8), columns=list('ABCDEFGH'), index=[1, 2, 3, 4, 5, 6, 7, 8])\n"", 'query', ""foo = df.query('2 < index <= 5')\nfoo.loc[:,'E'] = 40\n"", 'df', 'df.iloc[3] = 70\n', ""df.ix[1,'B':'E'] = 222\n"", 'df', 'df[df.C <= df.B]  = 7654321\n', 'df', ""df[df.C <= df.B].ix[:,'B':'E']\n""]";"[""df = pd.DataFrame(np.random.randn(8,8), columns=list('ABCDEFGH'), index=[1, 2, 3, 4, 5, 6, 7, 8])\nfoo = df.query('2 < index <= 5')\nfoo.loc[:,'E'] = 40\ndf.iloc[3] = 70\ndf.ix[1,'B':'E'] = 222\ndf[df.C <= df.B]  = 7654321\ndf[df.C <= df.B].ix[:,'B':'E']\n""]";"[""import pandas as pd\ndf = pd.DataFrame(np.random.randn(8,8), columns=list('ABCDEFGH'), index=[1, 2, 3, 4, 5, 6, 7, 8])\nfoo = df.query('2 < index <= 5')\nfoo.loc[:,'E'] = 40\ndf.iloc[3] = 70\ndf.ix[1,'B':'E'] = 222\ndf[df.C <= df.B]  = 7654321\ndf[df.C <= df.B].ix[:,'B':'E']\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame(np.random.randn(8,8), columns=list('ABCDEFGH'), index=[1, 2, 3, 4, 5, 6, 7, 8])\nfoo = df.query('2 < index <= 5')\nfoo.loc[:,'E'] = 40\ndf.iloc[3] = 70\ndf.ix[1,'B':'E'] = 222\ndf[df.C <= df.B]  = 7654321\ndf[df.C <= df.B].ix[:,'B':'E']\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'DataFrame' object has no attribute 'C'""]";['AttributeError']
766;766;766;766;6.0;1;23307301;;1;33;<python><pandas>;Pandas: Replacing column values in dataframe;84170.0;"[""w['female']['female']='1'\nw['female']['male']='0' \nif w['female'] =='female':\n    w['female'] = '1';\nelse:\n    w['female'] = '0';\n""]";"[""w['female']['female']='1'\nw['female']['male']='0' \n"", ""if w['female'] =='female':\n    w['female'] = '1';\nelse:\n    w['female'] = '0';\n""]";"[""w['female']['female']='1'\nw['female']['male']='0' \n"", ""if w['female'] =='female':\n    w['female'] = '1';\nelse:\n    w['female'] = '0';\n""]";"[""w['female']['female']='1'\nw['female']['male']='0' \n""]";"[""w['female']['female']='1'\nw['female']['male']='0' \n""]";False;"[""import pandas as pd\nw['female']['female']='1'\nw['female']['male']='0' \n""]";False;0;4;"[""name 'w' is not defined"", ""name 'w' is not defined"", ""name 'w' is not defined"", ""name 'w' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""name 'w' is not defined"", ""name 'w' is not defined"", ""name 'w' is not defined"", ""name 'w' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""name 'w' is not defined"", ""name 'w' is not defined"", ""name 'w' is not defined"", ""name 'w' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError']
767;767;767;767;2.0;0;23317342;;1;26;<python><split><pandas>;Pandas Dataframe: split column into multiple columns, right-align inconsistent cell entries;31355.0;"[""0                 HUN\n1                 ESP\n2                 GBR\n3                 ESP\n4                 FRA\n5             ID, USA\n6             GA, USA\n7    Hoboken, NJ, USA\n8             NJ, USA\n9                 AUS\nlocation_df = df['City, State, Country'].apply(lambda x: pd.Series(x.split(',')))\n     0       1       2\n0    HUN     NaN     NaN\n1    ESP     NaN     NaN\n2    GBR     NaN     NaN\n3    ESP     NaN     NaN\n4    FRA     NaN     NaN\n5    ID      USA     NaN\n6    GA      USA     NaN\n7    Hoboken  NJ     USA\n8    NJ      USA     NaN\n9    AUS     NaN     NaN\n""]";"['0                 HUN\n1                 ESP\n2                 GBR\n3                 ESP\n4                 FRA\n5             ID, USA\n6             GA, USA\n7    Hoboken, NJ, USA\n8             NJ, USA\n9                 AUS\n', ""location_df = df['City, State, Country'].apply(lambda x: pd.Series(x.split(',')))\n"", '     0       1       2\n0    HUN     NaN     NaN\n1    ESP     NaN     NaN\n2    GBR     NaN     NaN\n3    ESP     NaN     NaN\n4    FRA     NaN     NaN\n5    ID      USA     NaN\n6    GA      USA     NaN\n7    Hoboken  NJ     USA\n8    NJ      USA     NaN\n9    AUS     NaN     NaN\n']";"['0                 HUN\n1                 ESP\n2                 GBR\n3                 ESP\n4                 FRA\n5             ID, USA\n6             GA, USA\n7    Hoboken, NJ, USA\n8             NJ, USA\n9                 AUS\n', ""location_df = df['City, State, Country'].apply(lambda x: pd.Series(x.split(',')))\n"", '     0       1       2\n0    HUN     NaN     NaN\n1    ESP     NaN     NaN\n2    GBR     NaN     NaN\n3    ESP     NaN     NaN\n4    FRA     NaN     NaN\n5    ID      USA     NaN\n6    GA      USA     NaN\n7    Hoboken  NJ     USA\n8    NJ      USA     NaN\n9    AUS     NaN     NaN\n']";"[""location_df = df['City, State, Country'].apply(lambda x: pd.Series(x.split(',')))\n""]";"[""import pandas as pd\nlocation_df = df['City, State, Country'].apply(lambda x: pd.Series(x.split(',')))\n""]";True;"[""import pandas as pd\nlocation_df = df['City, State, Country'].apply(lambda x: pd.Series(x.split(',')))\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'City, State, Country'""]";['KeyError']
768;768;768;768;2.0;3;23330654;;1;38;<python><pandas><updates><dataframe>;Update a dataframe in pandas while iterating row by row;31600.0;"[""           date      exer exp     ifor         mat  \n1092  2014-03-17  American   M  528.205  2014-04-19 \n1093  2014-03-17  American   M  528.205  2014-04-19 \n1094  2014-03-17  American   M  528.205  2014-04-19 \n1095  2014-03-17  American   M  528.205  2014-04-19    \n1096  2014-03-17  American   M  528.205  2014-05-17 \nfor i, row in df.iterrows():\n    if <something>:\n        row['ifor'] = x\n    else:\n        row['ifor'] = y\n\n    df.ix[i]['ifor'] = x\n""]";"['           date      exer exp     ifor         mat  \n1092  2014-03-17  American   M  528.205  2014-04-19 \n1093  2014-03-17  American   M  528.205  2014-04-19 \n1094  2014-03-17  American   M  528.205  2014-04-19 \n1095  2014-03-17  American   M  528.205  2014-04-19    \n1096  2014-03-17  American   M  528.205  2014-05-17 \n', ""for i, row in df.iterrows():\n    if <something>:\n        row['ifor'] = x\n    else:\n        row['ifor'] = y\n\n    df.ix[i]['ifor'] = x\n""]";"['           date      exer exp     ifor         mat  \n1092  2014-03-17  American   M  528.205  2014-04-19 \n1093  2014-03-17  American   M  528.205  2014-04-19 \n1094  2014-03-17  American   M  528.205  2014-04-19 \n1095  2014-03-17  American   M  528.205  2014-04-19    \n1096  2014-03-17  American   M  528.205  2014-05-17 \n', 'ifor', ""for i, row in df.iterrows():\n    if <something>:\n        row['ifor'] = x\n    else:\n        row['ifor'] = y\n\n    df.ix[i]['ifor'] = x\n""]";['\n'];['\n'];False;['import pandas as pd\n\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
769;769;769;769;1.0;0;23354124;;1;14;<python><pandas><pivot-table>;"How can I ""unpivot"" specific columns from a pandas DataFrame?";4373.0;"[""x = DataFrame.from_dict({'farm' : ['A','B','A','B'], \n                         'fruit':['apple','apple','pear','pear'], \n                         '2014':[10,12,6,8], \n                         '2015':[11,13,7,9]})\n   2014  2015 farm  fruit\n0    10    11    A  apple\n1    12    13    B  apple\n2     6     7    A   pear\n3     8     9    B   pear\n  farm  fruit  value  year\n0    A  apple     10  2014\n1    B  apple     12  2014\n2    A   pear      6  2014\n3    B   pear      8  2014\n4    A  apple     11  2015\n5    B  apple     13  2015\n6    A   pear      7  2015\n7    B   pear      9  2015\n""]";"[""x = DataFrame.from_dict({'farm' : ['A','B','A','B'], \n                         'fruit':['apple','apple','pear','pear'], \n                         '2014':[10,12,6,8], \n                         '2015':[11,13,7,9]})\n"", '   2014  2015 farm  fruit\n0    10    11    A  apple\n1    12    13    B  apple\n2     6     7    A   pear\n3     8     9    B   pear\n', '  farm  fruit  value  year\n0    A  apple     10  2014\n1    B  apple     12  2014\n2    A   pear      6  2014\n3    B   pear      8  2014\n4    A  apple     11  2015\n5    B  apple     13  2015\n6    A   pear      7  2015\n7    B   pear      9  2015\n']";"[""x = DataFrame.from_dict({'farm' : ['A','B','A','B'], \n                         'fruit':['apple','apple','pear','pear'], \n                         '2014':[10,12,6,8], \n                         '2015':[11,13,7,9]})\n"", '   2014  2015 farm  fruit\n0    10    11    A  apple\n1    12    13    B  apple\n2     6     7    A   pear\n3     8     9    B   pear\n', '  farm  fruit  value  year\n0    A  apple     10  2014\n1    B  apple     12  2014\n2    A   pear      6  2014\n3    B   pear      8  2014\n4    A  apple     11  2015\n5    B  apple     13  2015\n6    A   pear      7  2015\n7    B   pear      9  2015\n', 'stack', 'unstack']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'x' is not defined""]";['NameError'];0;1;"[""name 'x' is not defined""]";['NameError']
770;770;770;770;3.0;0;23377108;;1;29;<python><pandas>;Pandas percentage of total with groupby;34307.0;"[""df = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,\n                   'office_id': range(1, 7) * 2,\n                   'sales': [np.random.randint(100000, 999999)\n                             for _ in range(12)]})\n\ndf.groupby(['state', 'office_id']).agg({'sales': 'sum'})\n                  sales\nstate office_id        \nAZ    2          839507\n      4          373917\n      6          347225\nCA    1          798585\n      3          890850\n      5          454423\nCO    1          819975\n      3          202969\n      5          614011\nWA    2          163942\n      4          369858\n      6          959285\n""]";"[""df = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,\n                   'office_id': range(1, 7) * 2,\n                   'sales': [np.random.randint(100000, 999999)\n                             for _ in range(12)]})\n\ndf.groupby(['state', 'office_id']).agg({'sales': 'sum'})\n"", '                  sales\nstate office_id        \nAZ    2          839507\n      4          373917\n      6          347225\nCA    1          798585\n      3          890850\n      5          454423\nCO    1          819975\n      3          202969\n      5          614011\nWA    2          163942\n      4          369858\n      6          959285\n']";"[""df = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,\n                   'office_id': range(1, 7) * 2,\n                   'sales': [np.random.randint(100000, 999999)\n                             for _ in range(12)]})\n\ndf.groupby(['state', 'office_id']).agg({'sales': 'sum'})\n"", '                  sales\nstate office_id        \nAZ    2          839507\n      4          373917\n      6          347225\nCA    1          798585\n      3          890850\n      5          454423\nCO    1          819975\n      3          202969\n      5          614011\nWA    2          163942\n      4          369858\n      6          959285\n', 'state', 'groupby', 'sales', 'state']";"[""\ndf.groupby(['state', 'office_id']).agg({'sales': 'sum'})\n""]";"[""\ndf.groupby(['state', 'office_id']).agg({'sales': 'sum'})\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\n\ndf.groupby(['state', 'office_id']).agg({'sales': 'sum'})\n""]";True;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'state'"", ""'state'""]";['KeyError', 'KeyError']
771;771;771;771;1.0;0;23394476;;1;16;<python><pandas>;Keep other columns when using min() with groupby;8277.0;"['df1 = df.groupby(""item"", as_index=False)[""diff""].min()\n    item    diff   otherstuff\n   0   1       2            1\n   1   1       1            2\n   2   1       3            7\n   3   2      -1            0\n   4   2       1            3\n   5   2       4            9\n   6   2      -6            2\n   7   3       0            0\n   8   3       2            9\n    item   diff  otherstuff\n   0   1      1           2\n   1   2     -6           2\n   2   3      0           0\n    item   diff\n   0   1      1           \n   1   2     -6           \n   2   3      0           \ndf1 = df.groupby([""item"", ""otherstuff""], as_index=false)[""diff""].min()\n\ndf1 = df.groupby(""item"", as_index=false)[""diff""].min()[""otherstuff""]\n\ndf1 = df.groupby(""item"", as_index=false)[""otherstuff"", ""diff""].min()\n']";"['df1 = df.groupby(""item"", as_index=False)[""diff""].min()\n', '    item    diff   otherstuff\n   0   1       2            1\n   1   1       1            2\n   2   1       3            7\n   3   2      -1            0\n   4   2       1            3\n   5   2       4            9\n   6   2      -6            2\n   7   3       0            0\n   8   3       2            9\n', '    item   diff  otherstuff\n   0   1      1           2\n   1   2     -6           2\n   2   3      0           0\n', '    item   diff\n   0   1      1           \n   1   2     -6           \n   2   3      0           \n', 'df1 = df.groupby([""item"", ""otherstuff""], as_index=false)[""diff""].min()\n\ndf1 = df.groupby(""item"", as_index=false)[""diff""].min()[""otherstuff""]\n\ndf1 = df.groupby(""item"", as_index=false)[""otherstuff"", ""diff""].min()\n']";"['groupby', 'df1 = df.groupby(""item"", as_index=False)[""diff""].min()\n', '    item    diff   otherstuff\n   0   1       2            1\n   1   1       1            2\n   2   1       3            7\n   3   2      -1            0\n   4   2       1            3\n   5   2       4            9\n   6   2      -6            2\n   7   3       0            0\n   8   3       2            9\n', '    item   diff  otherstuff\n   0   1      1           2\n   1   2     -6           2\n   2   3      0           0\n', '    item   diff\n   0   1      1           \n   1   2     -6           \n   2   3      0           \n', 'df1 = df.groupby([""item"", ""otherstuff""], as_index=false)[""diff""].min()\n\ndf1 = df.groupby(""item"", as_index=false)[""diff""].min()[""otherstuff""]\n\ndf1 = df.groupby(""item"", as_index=false)[""otherstuff"", ""diff""].min()\n']";"['df1 = df.groupby(""item"", as_index=False)[""diff""].min()\ndf1 = df.groupby([""item"", ""otherstuff""], as_index=false)[""diff""].min()\n\ndf1 = df.groupby(""item"", as_index=false)[""diff""].min()[""otherstuff""]\n\ndf1 = df.groupby(""item"", as_index=false)[""otherstuff"", ""diff""].min()\n']";"['df1 = df.groupby(""item"", as_index=False)[""diff""].min()\ndf1 = df.groupby([""item"", ""otherstuff""], as_index=false)[""diff""].min()\n\ndf1 = df.groupby(""item"", as_index=false)[""diff""].min()[""otherstuff""]\n\ndf1 = df.groupby(""item"", as_index=false)[""otherstuff"", ""diff""].min()\n']";False;"['import pandas as pd\ndf = pd.DataFrame()\ndf1 = df.groupby(""item"", as_index=False)[""diff""].min()\ndf1 = df.groupby([""item"", ""otherstuff""], as_index=false)[""diff""].min()\n\ndf1 = df.groupby(""item"", as_index=false)[""diff""].min()[""otherstuff""]\n\ndf1 = df.groupby(""item"", as_index=false)[""otherstuff"", ""diff""].min()\n']";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
772;772;772;772;1.0;1;23415500;;1;11;<python><matplotlib><pandas><ipython-notebook><python-3.4>;Pandas - Plotting a stacked Bar Chart;17781.0;"[""      Site Name    Abuse/NFF\n0    NORTH ACTON       ABUSE\n1    WASHINGTON         -\n2    WASHINGTON        NFF\n3    BELFAST            -\n4    CROYDON            - \ntest5 = faultdf.groupby(['Site Name', 'Abuse/NFF'])['Site Name'].count().unstack('Abuse/NFF').fillna(0)\n\ntest5.plot(kind='bar', stacked=True)\n""]";"['      Site Name    Abuse/NFF\n0    NORTH ACTON       ABUSE\n1    WASHINGTON         -\n2    WASHINGTON        NFF\n3    BELFAST            -\n4    CROYDON            - \n', ""test5 = faultdf.groupby(['Site Name', 'Abuse/NFF'])['Site Name'].count().unstack('Abuse/NFF').fillna(0)\n\ntest5.plot(kind='bar', stacked=True)\n""]";"['      Site Name    Abuse/NFF\n0    NORTH ACTON       ABUSE\n1    WASHINGTON         -\n2    WASHINGTON        NFF\n3    BELFAST            -\n4    CROYDON            - \n', ""test5 = faultdf.groupby(['Site Name', 'Abuse/NFF'])['Site Name'].count().unstack('Abuse/NFF').fillna(0)\n\ntest5.plot(kind='bar', stacked=True)\n""]";"[""test5 = faultdf.groupby(['Site Name', 'Abuse/NFF'])['Site Name'].count().unstack('Abuse/NFF').fillna(0)\n\ntest5.plot(kind='bar', stacked=True)\n""]";"[""test5 = faultdf.groupby(['Site Name', 'Abuse/NFF'])['Site Name'].count().unstack('Abuse/NFF').fillna(0)\n\ntest5.plot(kind='bar', stacked=True)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ntest5 = faultdf.groupby(['Site Name', 'Abuse/NFF'])['Site Name'].count().unstack('Abuse/NFF').fillna(0)\n\ntest5.plot(kind='bar', stacked=True)\n""]";True;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
773;773;773;773;3.0;6;23450735;;1;15;<python><pandas><categorical-data>;Categorical Variables In A Pandas Dataframe?;4988.0;"[""In [4]:\n\nimport pandas as pd\n\nraw_data = {'name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger', 'Riani', 'Ali'], \n        'score': [25, 94, 57, 62, 70, 25, 94, 57, 62, 70, 62, 70]}\ndf = pd.DataFrame(raw_data, columns = ['name', 'score'])\n\nbins = [0, 25, 50, 75, 100]\ngroup_names = ['Low', 'Okay', 'Good', 'Great']\n\nIn [5]:\ncat_obj = pd.cut(df['score'], bins, labels=group_names)\ndf['cat'] = pd.cut(df['score'], bins, labels=group_names)\nIn [7]:\n\ntype(cat_obj)\nOut[7]:\npandas.core.categorical.Categorical\nIn [8]:\n\ntype(df['cat'])\nOut[8]:\npandas.core.series.Series\n""]";"[""In [4]:\n\nimport pandas as pd\n\nraw_data = {'name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger', 'Riani', 'Ali'], \n        'score': [25, 94, 57, 62, 70, 25, 94, 57, 62, 70, 62, 70]}\ndf = pd.DataFrame(raw_data, columns = ['name', 'score'])\n\nbins = [0, 25, 50, 75, 100]\ngroup_names = ['Low', 'Okay', 'Good', 'Great']\n\nIn [5]:\ncat_obj = pd.cut(df['score'], bins, labels=group_names)\ndf['cat'] = pd.cut(df['score'], bins, labels=group_names)\nIn [7]:\n\ntype(cat_obj)\nOut[7]:\npandas.core.categorical.Categorical\nIn [8]:\n\ntype(df['cat'])\nOut[8]:\npandas.core.series.Series\n""]";"['pd.cut()', 'cat_obj', 'cat_obj', 'pd.cut()', ""df['cat']"", ""In [4]:\n\nimport pandas as pd\n\nraw_data = {'name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger', 'Riani', 'Ali'], \n        'score': [25, 94, 57, 62, 70, 25, 94, 57, 62, 70, 62, 70]}\ndf = pd.DataFrame(raw_data, columns = ['name', 'score'])\n\nbins = [0, 25, 50, 75, 100]\ngroup_names = ['Low', 'Okay', 'Good', 'Great']\n\nIn [5]:\ncat_obj = pd.cut(df['score'], bins, labels=group_names)\ndf['cat'] = pd.cut(df['score'], bins, labels=group_names)\nIn [7]:\n\ntype(cat_obj)\nOut[7]:\npandas.core.categorical.Categorical\nIn [8]:\n\ntype(df['cat'])\nOut[8]:\npandas.core.series.Series\n""]";"[""import pandas as pd\n\ndf = pd.DataFrame(raw_data, columns = ['name', 'score'])\n\nbins = [0, 25, 50, 75, 100]\ngroup_names = ['Low', 'Okay', 'Good', 'Great']\n\ncat_obj = pd.cut(df['score'], bins, labels=group_names)\ndf['cat'] = pd.cut(df['score'], bins, labels=group_names)\n\ntype(cat_obj)\ntype(df['cat'])\n""]";"[""import pandas as pd\n\ndf = pd.DataFrame(raw_data, columns = ['name', 'score'])\n\nbins = [0, 25, 50, 75, 100]\ngroup_names = ['Low', 'Okay', 'Good', 'Great']\n\ncat_obj = pd.cut(df['score'], bins, labels=group_names)\ndf['cat'] = pd.cut(df['score'], bins, labels=group_names)\n\ntype(cat_obj)\ntype(df['cat'])\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nimport pandas as pd\n\ndf = pd.DataFrame(raw_data, columns = ['name', 'score'])\n\nbins = [0, 25, 50, 75, 100]\ngroup_names = ['Low', 'Okay', 'Good', 'Great']\n\ncat_obj = pd.cut(df['score'], bins, labels=group_names)\ndf['cat'] = pd.cut(df['score'], bins, labels=group_names)\n\ntype(cat_obj)\ntype(df['cat'])\n""]";True;0;0;[];[];0;0;[];[];0;0;[];[]
774;774;774;774;2.0;0;23451244;;1;13;<python><numpy><pandas><scipy>;how to zscore normalize pandas column with nans?;9629.0;"['>> a\narray([    nan,  0.0767,  0.4383,  0.7866,  0.8091,  0.1954,  0.6307,\n        0.6599,  0.1065,  0.0508])\n>> df = pandas.DataFrame({""a"": a})\n>> from scipy.stats import zscore\n>> zscore(df[""a""])\narray([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan])\nzscore = lambda x: (x - scipy.stats.nanmean(x)) / scipy.stats.nanstd(x)\n']";"['>> a\narray([    nan,  0.0767,  0.4383,  0.7866,  0.8091,  0.1954,  0.6307,\n        0.6599,  0.1065,  0.0508])\n>> df = pandas.DataFrame({""a"": a})\n', '>> from scipy.stats import zscore\n>> zscore(df[""a""])\narray([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan])\n', 'zscore = lambda x: (x - scipy.stats.nanmean(x)) / scipy.stats.nanstd(x)\n']";"['>> a\narray([    nan,  0.0767,  0.4383,  0.7866,  0.8091,  0.1954,  0.6307,\n        0.6599,  0.1065,  0.0508])\n>> df = pandas.DataFrame({""a"": a})\n', 'nan', 'nan', '>> from scipy.stats import zscore\n>> zscore(df[""a""])\narray([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan])\n', 'zscore', 'nan', 'np.nan', 'scipy.stats.nanmean', 'scipy.stats.nanstd', 'std', 'zscore = lambda x: (x - scipy.stats.nanmean(x)) / scipy.stats.nanstd(x)\n']";['array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan])\nzscore = lambda x: (x - scipy.stats.nanmean(x)) / scipy.stats.nanstd(x)\n'];['array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan])\nzscore = lambda x: (x - scipy.stats.nanmean(x)) / scipy.stats.nanstd(x)\n'];False;['import pandas as pd\narray([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan])\nzscore = lambda x: (x - scipy.stats.nanmean(x)) / scipy.stats.nanstd(x)\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'DataFrame' object has no attribute 'a'""]";['AttributeError']
775;775;775;775;9.0;2;23455728;;1;20;<python><pandas><scikit-learn><subsampling>;Scikit-learn balanced subsampling;13375.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
776;776;776;776;2.0;2;23482668;;1;15;<python><sorting><pandas>;sorting by a custom list in pandas;8245.0;"[""             Player      Year   Age   Tm     G\n2967     Cedric Hunter   1991    27  CHH     6\n5335     Maurice Baker   2004    25  VAN     7\n13950    Ratko Varda     2001    22  TOT     60\n6141     Ryan Bowen      2009    34  OKC     52\n6169     Adrian Caldwell 1997    31  DAL     81\nsorter = ['TOT', 'ATL', 'BOS', 'BRK', 'CHA', 'CHH', 'CHI', 'CLE', 'DAL', 'DEN',\n   'DET', 'GSW', 'HOU', 'IND', 'LAC', 'LAL', 'MEM', 'MIA', 'MIL',\n   'MIN', 'NJN', 'NOH', 'NOK', 'NOP', 'NYK', 'OKC', 'ORL', 'PHI',\n   'PHO', 'POR', 'SAC', 'SAS', 'SEA', 'TOR', 'UTA', 'VAN',\n   'WAS', 'WSB']\ndf.sort(['Player', 'Year', 'Tm'], ascending = [True, True, sorter])\n""]";"['             Player      Year   Age   Tm     G\n2967     Cedric Hunter   1991    27  CHH     6\n5335     Maurice Baker   2004    25  VAN     7\n13950    Ratko Varda     2001    22  TOT     60\n6141     Ryan Bowen      2009    34  OKC     52\n6169     Adrian Caldwell 1997    31  DAL     81\n', ""sorter = ['TOT', 'ATL', 'BOS', 'BRK', 'CHA', 'CHH', 'CHI', 'CLE', 'DAL', 'DEN',\n   'DET', 'GSW', 'HOU', 'IND', 'LAC', 'LAL', 'MEM', 'MIA', 'MIL',\n   'MIN', 'NJN', 'NOH', 'NOK', 'NOP', 'NYK', 'OKC', 'ORL', 'PHI',\n   'PHO', 'POR', 'SAC', 'SAS', 'SEA', 'TOR', 'UTA', 'VAN',\n   'WAS', 'WSB']\n"", ""df.sort(['Player', 'Year', 'Tm'], ascending = [True, True, sorter])\n""]";"['             Player      Year   Age   Tm     G\n2967     Cedric Hunter   1991    27  CHH     6\n5335     Maurice Baker   2004    25  VAN     7\n13950    Ratko Varda     2001    22  TOT     60\n6141     Ryan Bowen      2009    34  OKC     52\n6169     Adrian Caldwell 1997    31  DAL     81\n', ""sorter = ['TOT', 'ATL', 'BOS', 'BRK', 'CHA', 'CHH', 'CHI', 'CLE', 'DAL', 'DEN',\n   'DET', 'GSW', 'HOU', 'IND', 'LAC', 'LAL', 'MEM', 'MIA', 'MIL',\n   'MIN', 'NJN', 'NOH', 'NOK', 'NOP', 'NYK', 'OKC', 'ORL', 'PHI',\n   'PHO', 'POR', 'SAC', 'SAS', 'SEA', 'TOR', 'UTA', 'VAN',\n   'WAS', 'WSB']\n"", ""df.sort(['Player', 'Year', 'Tm'], ascending = [True, True, sorter])\n""]";"[""df.sort(['Player', 'Year', 'Tm'], ascending = [True, True, sorter])\n""]";"[""df.sort(['Player', 'Year', 'Tm'], ascending = [True, True, sorter])\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf.sort(['Player', 'Year', 'Tm'], ascending = [True, True, sorter])\n""]";True;0;2;"[""name 'sorter' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'sorter' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'sorter' is not defined"", ""'DataFrame' object has no attribute 'Tm'""]";['NameError', 'AttributeError']
777;777;777;777;1.0;2;23508351;;1;13;<python><join><pandas>;How to do a conditional join in python Pandas?;8135.0;"['    COMPANY_ID  DATE            MEASURE\n    1   2010-01-01 00:00:00     10\n    1   2010-01-02 00:00:00     10\n    1   2010-01-03 00:00:00     10\n    1   2010-01-04 00:00:00     10\n    1   2010-01-05 00:00:00     10\n    table_a = pd.concat(\\\n    [pd.DataFrame({\'DATE\': pd.date_range(""01/01/2010"", ""12/31/2010"", freq=""D""),\\\n    \'COMPANY_ID\': 1 , \'MEASURE\': 10}),\\\n    pd.DataFrame({\'DATE\': pd.date_range(""01/01/2010"", ""12/31/2010"", freq=""D""),\\\n    \'COMPANY_ID\': 2 , \'MEASURE\': 10})])\n        COMPANY     END_DATE\n        1   2010-03-01 00:00:00\n        1   2010-06-02 00:00:00\n        2   2010-03-01 00:00:00\n        2   2010-06-02 00:00:00\n    table_b = pd.DataFrame({\'END_DATE\':pd.to_datetime([\'03/01/2010\',\'06/02/2010\',\'03/01/2010\',\'06/02/2010\']),\\\n                    \'COMPANY\':(1,1,2,2)})\n      select\n b.COMPANY_ID,\n b.DATE\n sum(a.MEASURE) AS MEASURE_TO_END_DATE\n from table_a a, table_b b\n where a.COMPANY = b.COMPANY and\n       a.DATE < b.DATE and\n       a.DATE > b.DATE - 30  \n group by b.COMPANY;\n']";"['    COMPANY_ID  DATE            MEASURE\n    1   2010-01-01 00:00:00     10\n    1   2010-01-02 00:00:00     10\n    1   2010-01-03 00:00:00     10\n    1   2010-01-04 00:00:00     10\n    1   2010-01-05 00:00:00     10\n', '    table_a = pd.concat(\\\n    [pd.DataFrame({\'DATE\': pd.date_range(""01/01/2010"", ""12/31/2010"", freq=""D""),\\\n    \'COMPANY_ID\': 1 , \'MEASURE\': 10}),\\\n    pd.DataFrame({\'DATE\': pd.date_range(""01/01/2010"", ""12/31/2010"", freq=""D""),\\\n    \'COMPANY_ID\': 2 , \'MEASURE\': 10})])\n', '        COMPANY     END_DATE\n        1   2010-03-01 00:00:00\n        1   2010-06-02 00:00:00\n        2   2010-03-01 00:00:00\n        2   2010-06-02 00:00:00\n', ""    table_b = pd.DataFrame({'END_DATE':pd.to_datetime(['03/01/2010','06/02/2010','03/01/2010','06/02/2010']),\\\n                    'COMPANY':(1,1,2,2)})\n"", '      select\n b.COMPANY_ID,\n b.DATE\n sum(a.MEASURE) AS MEASURE_TO_END_DATE\n from table_a a, table_b b\n where a.COMPANY = b.COMPANY and\n       a.DATE < b.DATE and\n       a.DATE > b.DATE - 30  \n group by b.COMPANY;\n']";"['    COMPANY_ID  DATE            MEASURE\n    1   2010-01-01 00:00:00     10\n    1   2010-01-02 00:00:00     10\n    1   2010-01-03 00:00:00     10\n    1   2010-01-04 00:00:00     10\n    1   2010-01-05 00:00:00     10\n', '    table_a = pd.concat(\\\n    [pd.DataFrame({\'DATE\': pd.date_range(""01/01/2010"", ""12/31/2010"", freq=""D""),\\\n    \'COMPANY_ID\': 1 , \'MEASURE\': 10}),\\\n    pd.DataFrame({\'DATE\': pd.date_range(""01/01/2010"", ""12/31/2010"", freq=""D""),\\\n    \'COMPANY_ID\': 2 , \'MEASURE\': 10})])\n', '        COMPANY     END_DATE\n        1   2010-03-01 00:00:00\n        1   2010-06-02 00:00:00\n        2   2010-03-01 00:00:00\n        2   2010-06-02 00:00:00\n', ""    table_b = pd.DataFrame({'END_DATE':pd.to_datetime(['03/01/2010','06/02/2010','03/01/2010','06/02/2010']),\\\n                    'COMPANY':(1,1,2,2)})\n"", '      select\n b.COMPANY_ID,\n b.DATE\n sum(a.MEASURE) AS MEASURE_TO_END_DATE\n from table_a a, table_b b\n where a.COMPANY = b.COMPANY and\n       a.DATE < b.DATE and\n       a.DATE > b.DATE - 30  \n group by b.COMPANY;\n']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError']
778;778;778;778;2.0;6;23519135;;1;11;<python><matplotlib><pandas><seaborn>;Dot-boxplots from DataFrames;4100.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""No module named 'matplotlib'"", ""No module named 'seaborn'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'matplotlib'"", ""No module named 'seaborn'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'matplotlib'"", ""No module named 'seaborn'""]";['ImportError', 'ImportError']
779;779;779;779;3.0;0;23521511;;1;11;<python><pandas><mat>;Pandas: Creating DataFrame from Series;16290.0;['mat = loadmat(file_path)  # load mat-file\nVariables = mat.keys()    # identify variable names\n\ndf = pd.DataFrame         # Initialise DataFrame\n\nfor name in Variables:\n\n    B = mat[name]\n    s = pd.Series (B[:,1])\n'];['mat = loadmat(file_path)  # load mat-file\nVariables = mat.keys()    # identify variable names\n\ndf = pd.DataFrame         # Initialise DataFrame\n\nfor name in Variables:\n\n    B = mat[name]\n    s = pd.Series (B[:,1])\n'];['mat = loadmat(file_path)  # load mat-file\nVariables = mat.keys()    # identify variable names\n\ndf = pd.DataFrame         # Initialise DataFrame\n\nfor name in Variables:\n\n    B = mat[name]\n    s = pd.Series (B[:,1])\n'];['mat = loadmat(file_path)  # load mat-file\nVariables = mat.keys()    # identify variable names\n\ndf = pd.DataFrame         # Initialise DataFrame\n\n\n'];['from pandas import DataFrame\nimport pandas as pd\nmat = loadmat(file_path)  # load mat-file\nVariables = mat.keys()    # identify variable names\n\ndf = pd.DataFrame         # Initialise DataFrame\n\n\n'];True;['import pandas as pd\nmat = loadmat(file_path)  # load mat-file\nVariables = mat.keys()    # identify variable names\n\ndf = pd.DataFrame         # Initialise DataFrame\n\n\n'];False;0;2;"[""name 'Variables' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];1;2;"[""name 'Variables' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'Variables' is not defined"", 'Sucess']";['NameError', 'Sucess']
780;780;780;780;1.0;1;23543909;;1;18;<python><matplotlib><pandas>;Plotting pandas timedelta;9760.0;[''];[];"[""df['time_delta']"", ""TypeError: ufunc add cannot use operands with types dtype('<m8[ns]') and dtype('float64')"", ""float--> df2 = df1['time_delta'].astype(float)"", 'TypeError: cannot astype a timedelta from [timedelta64[ns]] to [float64]']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
781;781;781;781;4.0;1;23549231;;1;41;<python><pandas><ipython>;Check if a value exists in pandas dataframe index;45304.0;"[""df = pandas.DataFrame({'test':[1,2,3,4]}, index=['a','b','c','d'])\n\ndf.loc['g']  # (should give False)\nsum(df.index == 'g')\n""]";"[""df = pandas.DataFrame({'test':[1,2,3,4]}, index=['a','b','c','d'])\n\ndf.loc['g']  # (should give False)\n"", ""sum(df.index == 'g')\n""]";"['True', 'False', 'df', ""df = pandas.DataFrame({'test':[1,2,3,4]}, index=['a','b','c','d'])\n\ndf.loc['g']  # (should give False)\n"", ""sum(df.index == 'g')\n""]";"[""df = pandas.DataFrame({'test':[1,2,3,4]}, index=['a','b','c','d'])\n\ndf.loc['g']  # (should give False)\nsum(df.index == 'g')\n""]";"[""df = pandas.DataFrame({'test':[1,2,3,4]}, index=['a','b','c','d'])\n\ndf.loc['g']  # (should give False)\nsum(df.index == 'g')\n""]";False;"[""import pandas as pd\ndf = pandas.DataFrame({'test':[1,2,3,4]}, index=['a','b','c','d'])\n\ndf.loc['g']  # (should give False)\nsum(df.index == 'g')\n""]";False;0;2;"[""name 'df' is not defined"", ""name 'g' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'g' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""name 'g' is not defined""]";['Sucess', 'NameError']
782;782;782;782;4.0;3;23550056;;1;14;<python><matplotlib><pandas><financial>;FigureCanvasAgg' object has no attribute 'invalidate' ? python plotting;5778.0;"[""names = ['AAPL','MSFT', 'DELL', 'MS', 'BAC', 'C'] #goog and SF did not work\ndef get_px(stock, start, end):\n    return web.get_data_yahoo(stock, start, end)['Adj Close']\npx = pd.DataFrame({n: get_px(n, '1/1/2009', '6/1/2012') for n in names})\n\n#fillna method pad uses last valid observation to fill\npx = px.asfreq('B').fillna(method='pad')\nrets = px.pct_change()\ndf2 = ((1 + rets).cumprod() - 1)\n\ndf2.ix[0] = 1\n\ndf2.plot()\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-122-df192c0432be> in <module>()\n      6 df2.ix[0] = 1\n      7 \n----> 8 df2.plot()\n\n//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in plot_frame(frame, x, y, subplots, sharex, sharey, use_index, figsize, grid, legend, rot, ax, style, title, xlim, ylim, logx, logy, xticks, yticks, kind, sort_columns, fontsize, secondary_y, **kwds)\n   1634                      logy=logy, sort_columns=sort_columns,\n   1635                      secondary_y=secondary_y, **kwds)\n-> 1636     plot_obj.generate()\n   1637     plot_obj.draw()\n   1638     if subplots:\n\n//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in generate(self)\n    854         self._compute_plot_data()\n    855         self._setup_subplots()\n--> 856         self._make_plot()\n    857         self._post_plot_logic()\n    858         self._adorn_subplots()\n\n//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in _make_plot(self)\n   1238         if not self.x_compat and self.use_index and self._use_dynamic_x():\n   1239             data = self._maybe_convert_index(self.data)\n-> 1240             self._make_ts_plot(data, **self.kwds)\n   1241         else:\n   1242             lines = []\n\n//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in _make_ts_plot(self, data, **kwargs)\n   1319                 self._maybe_add_color(colors, kwds, style, i)\n   1320 \n-> 1321                 _plot(data[col], i, ax, label, style, **kwds)\n   1322 \n   1323         self._make_legend(lines, labels)\n\n//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in _plot(data, col_num, ax, label, style, **kwds)\n   1293         def _plot(data, col_num, ax, label, style, **kwds):\n   1294             newlines = tsplot(data, plotf, ax=ax, label=label,\n-> 1295                                 style=style, **kwds)\n   1296             ax.grid(self.grid)\n   1297             lines.append(newlines[0])\n\n//anaconda/lib/python2.7/site-packages/pandas/tseries/plotting.pyc in tsplot(series, plotf, **kwargs)\n     79 \n     80     # set date formatter, locators and rescale limits\n---> 81     format_dateaxis(ax, ax.freq)\n     82     left, right = _get_xlim(ax.get_lines())\n     83     ax.set_xlim(left, right)\n\n//anaconda/lib/python2.7/site-packages/pandas/tseries/plotting.pyc in format_dateaxis(subplot, freq)\n    258     subplot.xaxis.set_major_formatter(majformatter)\n    259     subplot.xaxis.set_minor_formatter(minformatter)\n--> 260     pylab.draw_if_interactive()\n\n//anaconda/lib/python2.7/site-packages/IPython/utils/decorators.pyc in wrapper(*args, **kw)\n     41     def wrapper(*args,**kw):\n     42         wrapper.called = False\n---> 43         out = func(*args,**kw)\n     44         wrapper.called = True\n     45         return out\n\n//anaconda/lib/python2.7/site-packages/matplotlib/backends/backend_macosx.pyc in draw_if_interactive()\n    227         figManager =  Gcf.get_active()\n    228         if figManager is not None:\n--> 229             figManager.canvas.invalidate()\n    230 \n    231 \n\nAttributeError: 'FigureCanvasAgg' object has no attribute 'invalidate'\n""]";"[""names = ['AAPL','MSFT', 'DELL', 'MS', 'BAC', 'C'] #goog and SF did not work\ndef get_px(stock, start, end):\n    return web.get_data_yahoo(stock, start, end)['Adj Close']\npx = pd.DataFrame({n: get_px(n, '1/1/2009', '6/1/2012') for n in names})\n\n#fillna method pad uses last valid observation to fill\npx = px.asfreq('B').fillna(method='pad')\nrets = px.pct_change()\ndf2 = ((1 + rets).cumprod() - 1)\n\ndf2.ix[0] = 1\n\ndf2.plot()\n"", ""---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-122-df192c0432be> in <module>()\n      6 df2.ix[0] = 1\n      7 \n----> 8 df2.plot()\n\n//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in plot_frame(frame, x, y, subplots, sharex, sharey, use_index, figsize, grid, legend, rot, ax, style, title, xlim, ylim, logx, logy, xticks, yticks, kind, sort_columns, fontsize, secondary_y, **kwds)\n   1634                      logy=logy, sort_columns=sort_columns,\n   1635                      secondary_y=secondary_y, **kwds)\n-> 1636     plot_obj.generate()\n   1637     plot_obj.draw()\n   1638     if subplots:\n\n//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in generate(self)\n    854         self._compute_plot_data()\n    855         self._setup_subplots()\n--> 856         self._make_plot()\n    857         self._post_plot_logic()\n    858         self._adorn_subplots()\n\n//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in _make_plot(self)\n   1238         if not self.x_compat and self.use_index and self._use_dynamic_x():\n   1239             data = self._maybe_convert_index(self.data)\n-> 1240             self._make_ts_plot(data, **self.kwds)\n   1241         else:\n   1242             lines = []\n\n//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in _make_ts_plot(self, data, **kwargs)\n   1319                 self._maybe_add_color(colors, kwds, style, i)\n   1320 \n-> 1321                 _plot(data[col], i, ax, label, style, **kwds)\n   1322 \n   1323         self._make_legend(lines, labels)\n\n//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in _plot(data, col_num, ax, label, style, **kwds)\n   1293         def _plot(data, col_num, ax, label, style, **kwds):\n   1294             newlines = tsplot(data, plotf, ax=ax, label=label,\n-> 1295                                 style=style, **kwds)\n   1296             ax.grid(self.grid)\n   1297             lines.append(newlines[0])\n\n//anaconda/lib/python2.7/site-packages/pandas/tseries/plotting.pyc in tsplot(series, plotf, **kwargs)\n     79 \n     80     # set date formatter, locators and rescale limits\n---> 81     format_dateaxis(ax, ax.freq)\n     82     left, right = _get_xlim(ax.get_lines())\n     83     ax.set_xlim(left, right)\n\n//anaconda/lib/python2.7/site-packages/pandas/tseries/plotting.pyc in format_dateaxis(subplot, freq)\n    258     subplot.xaxis.set_major_formatter(majformatter)\n    259     subplot.xaxis.set_minor_formatter(minformatter)\n--> 260     pylab.draw_if_interactive()\n\n//anaconda/lib/python2.7/site-packages/IPython/utils/decorators.pyc in wrapper(*args, **kw)\n     41     def wrapper(*args,**kw):\n     42         wrapper.called = False\n---> 43         out = func(*args,**kw)\n     44         wrapper.called = True\n     45         return out\n\n//anaconda/lib/python2.7/site-packages/matplotlib/backends/backend_macosx.pyc in draw_if_interactive()\n    227         figManager =  Gcf.get_active()\n    228         if figManager is not None:\n--> 229             figManager.canvas.invalidate()\n    230 \n    231 \n\nAttributeError: 'FigureCanvasAgg' object has no attribute 'invalidate'\n""]";"[""names = ['AAPL','MSFT', 'DELL', 'MS', 'BAC', 'C'] #goog and SF did not work\ndef get_px(stock, start, end):\n    return web.get_data_yahoo(stock, start, end)['Adj Close']\npx = pd.DataFrame({n: get_px(n, '1/1/2009', '6/1/2012') for n in names})\n\n#fillna method pad uses last valid observation to fill\npx = px.asfreq('B').fillna(method='pad')\nrets = px.pct_change()\ndf2 = ((1 + rets).cumprod() - 1)\n\ndf2.ix[0] = 1\n\ndf2.plot()\n"", ""---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-122-df192c0432be> in <module>()\n      6 df2.ix[0] = 1\n      7 \n----> 8 df2.plot()\n\n//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in plot_frame(frame, x, y, subplots, sharex, sharey, use_index, figsize, grid, legend, rot, ax, style, title, xlim, ylim, logx, logy, xticks, yticks, kind, sort_columns, fontsize, secondary_y, **kwds)\n   1634                      logy=logy, sort_columns=sort_columns,\n   1635                      secondary_y=secondary_y, **kwds)\n-> 1636     plot_obj.generate()\n   1637     plot_obj.draw()\n   1638     if subplots:\n\n//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in generate(self)\n    854         self._compute_plot_data()\n    855         self._setup_subplots()\n--> 856         self._make_plot()\n    857         self._post_plot_logic()\n    858         self._adorn_subplots()\n\n//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in _make_plot(self)\n   1238         if not self.x_compat and self.use_index and self._use_dynamic_x():\n   1239             data = self._maybe_convert_index(self.data)\n-> 1240             self._make_ts_plot(data, **self.kwds)\n   1241         else:\n   1242             lines = []\n\n//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in _make_ts_plot(self, data, **kwargs)\n   1319                 self._maybe_add_color(colors, kwds, style, i)\n   1320 \n-> 1321                 _plot(data[col], i, ax, label, style, **kwds)\n   1322 \n   1323         self._make_legend(lines, labels)\n\n//anaconda/lib/python2.7/site-packages/pandas/tools/plotting.pyc in _plot(data, col_num, ax, label, style, **kwds)\n   1293         def _plot(data, col_num, ax, label, style, **kwds):\n   1294             newlines = tsplot(data, plotf, ax=ax, label=label,\n-> 1295                                 style=style, **kwds)\n   1296             ax.grid(self.grid)\n   1297             lines.append(newlines[0])\n\n//anaconda/lib/python2.7/site-packages/pandas/tseries/plotting.pyc in tsplot(series, plotf, **kwargs)\n     79 \n     80     # set date formatter, locators and rescale limits\n---> 81     format_dateaxis(ax, ax.freq)\n     82     left, right = _get_xlim(ax.get_lines())\n     83     ax.set_xlim(left, right)\n\n//anaconda/lib/python2.7/site-packages/pandas/tseries/plotting.pyc in format_dateaxis(subplot, freq)\n    258     subplot.xaxis.set_major_formatter(majformatter)\n    259     subplot.xaxis.set_minor_formatter(minformatter)\n--> 260     pylab.draw_if_interactive()\n\n//anaconda/lib/python2.7/site-packages/IPython/utils/decorators.pyc in wrapper(*args, **kw)\n     41     def wrapper(*args,**kw):\n     42         wrapper.called = False\n---> 43         out = func(*args,**kw)\n     44         wrapper.called = True\n     45         return out\n\n//anaconda/lib/python2.7/site-packages/matplotlib/backends/backend_macosx.pyc in draw_if_interactive()\n    227         figManager =  Gcf.get_active()\n    228         if figManager is not None:\n--> 229             figManager.canvas.invalidate()\n    230 \n    231 \n\nAttributeError: 'FigureCanvasAgg' object has no attribute 'invalidate'\n""]";"[""names = ['AAPL','MSFT', 'DELL', 'MS', 'BAC', 'C'] #goog and SF did not work\npx = pd.DataFrame({n: get_px(n, '1/1/2009', '6/1/2012') for n in names})\n\n#fillna method pad uses last valid observation to fill\npx = px.asfreq('B').fillna(method='pad')\nrets = px.pct_change()\ndf2 = ((1 + rets).cumprod() - 1)\n\ndf2.ix[0] = 1\n\ndf2.plot()\n\n\n\n\n\n\n\n\n\n\n""]";"[""import pandas as pd\nnames = ['AAPL','MSFT', 'DELL', 'MS', 'BAC', 'C'] #goog and SF did not work\npx = pd.DataFrame({n: get_px(n, '1/1/2009', '6/1/2012') for n in names})\n\n#fillna method pad uses last valid observation to fill\npx = px.asfreq('B').fillna(method='pad')\nrets = px.pct_change()\ndf2 = ((1 + rets).cumprod() - 1)\n\ndf2.ix[0] = 1\n\ndf2.plot()\n\n\n\n\n\n\n\n\n\n\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\nnames = ['AAPL','MSFT', 'DELL', 'MS', 'BAC', 'C'] #goog and SF did not work\npx = pd.DataFrame({n: get_px(n, '1/1/2009', '6/1/2012') for n in names})\n\n#fillna method pad uses last valid observation to fill\npx = px.asfreq('B').fillna(method='pad')\nrets = px.pct_change()\ndf2 = ((1 + rets).cumprod() - 1)\n\ndf2.ix[0] = 1\n\ndf2.plot()\n\n\n\n\n\n\n\n\n\n\n""]";True;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
783;783;783;783;2.0;0;23600582;;1;17;<python><pandas><multi-index>;Concatenate Pandas columns under new multi-index level;6817.0;"[""dict = {'ABC': df1, 'XYZ' : df2}   # of any length...\ndata           Open     High      Low    Close   Volume\nDate                                                   \n2002-01-17  0.18077  0.18800  0.16993  0.18439  1720833\n2002-01-18  0.18439  0.21331  0.18077  0.19523  2027866\n2002-01-21  0.19523  0.20970  0.19162  0.20608   771149\nsymbol         ABC                                       XYZ\ndata           Open     High      Low    Close   Volume  Open ...\nDate                                                   \n2002-01-17  0.18077  0.18800  0.16993  0.18439  1720833  ...\n2002-01-18  0.18439  0.21331  0.18077  0.19523  2027866  ...\n2002-01-21  0.19523  0.20970  0.19162  0.20608   771149  ...\n""]";"[""dict = {'ABC': df1, 'XYZ' : df2}   # of any length...\n"", 'data           Open     High      Low    Close   Volume\nDate                                                   \n2002-01-17  0.18077  0.18800  0.16993  0.18439  1720833\n2002-01-18  0.18439  0.21331  0.18077  0.19523  2027866\n2002-01-21  0.19523  0.20970  0.19162  0.20608   771149\n', 'symbol         ABC                                       XYZ\ndata           Open     High      Low    Close   Volume  Open ...\nDate                                                   \n2002-01-17  0.18077  0.18800  0.16993  0.18439  1720833  ...\n2002-01-18  0.18439  0.21331  0.18077  0.19523  2027866  ...\n2002-01-21  0.19523  0.20970  0.19162  0.20608   771149  ...\n']";"[""dict = {'ABC': df1, 'XYZ' : df2}   # of any length...\n"", 'data           Open     High      Low    Close   Volume\nDate                                                   \n2002-01-17  0.18077  0.18800  0.16993  0.18439  1720833\n2002-01-18  0.18439  0.21331  0.18077  0.19523  2027866\n2002-01-21  0.19523  0.20970  0.19162  0.20608   771149\n', 'symbol         ABC                                       XYZ\ndata           Open     High      Low    Close   Volume  Open ...\nDate                                                   \n2002-01-17  0.18077  0.18800  0.16993  0.18439  1720833  ...\n2002-01-18  0.18439  0.21331  0.18077  0.19523  2027866  ...\n2002-01-21  0.19523  0.20970  0.19162  0.20608   771149  ...\n', "".from_product(['ABC', columns])"", 'axis=1']";"[""dict = {'ABC': df1, 'XYZ' : df2}   # of any length...\nDate                                                   \nDate                                                   \n""]";"[""dict = {'ABC': df1, 'XYZ' : df2}   # of any length...\nDate                                                   \nDate                                                   \n""]";False;"[""import pandas as pd\ndf2 = pd.DataFrame()\ndf1 = pd.DataFrame()\ndf = pd.DataFrame()\ndict = {'ABC': df1, 'XYZ' : df2}   # of any length...\nDate                                                   \nDate                                                   \n""]";True;0;1;"[""name 'df1' is not defined""]";['NameError'];0;1;"[""name 'df1' is not defined""]";['NameError'];0;1;"[""name 'Date' is not defined""]";['NameError']
784;784;784;784;3.0;1;23667369;;1;16;<python><pandas><duplicates>;Drop all duplicate rows in Python Pandas;36128.0;['    A   B   C\n0   foo 0   A\n1   foo 1   A\n2   foo 1   B\n3   bar 1   A\n'];['    A   B   C\n0   foo 0   A\n1   foo 1   A\n2   foo 1   B\n3   bar 1   A\n'];['pandas', 'drop_duplicates', 'take_last=True', 'take_last=False', '    A   B   C\n0   foo 0   A\n1   foo 1   A\n2   foo 1   B\n3   bar 1   A\n', 'A', 'C'];[''];[''];False;['import pandas as pd\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
785;785;785;785;5.0;3;23668427;;1;34;<python><pandas>;pandas joining multiple dataframes on columns;46068.0;[''];[];['join()'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'pd' is not defined"", ""name 'df0' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df1' is not defined"", ""name 'df0' is not defined""]";['NameError', 'NameError'];0;2;"[""'name'"", ""name 'df0' is not defined""]";['KeyError', 'NameError']
786;786;786;786;2.0;0;23691133;;1;14;<python><pandas>;Split pandas dataframe based on groupby;6273.0;['df = \n        N0_YLDF  ZZ        MAT\n    0  6.286333   2  11.669069\n    1  6.317000   6  11.669069\n    2  6.324889   6  11.516454\n    3  6.320667   5  11.516454\n    4  6.325556   5  11.516454\n    5  6.359000   6  11.516454\n    6  6.359000   6  11.516454\n    7  6.361111   7  11.516454\n    8  6.360778   7  11.516454\n    9  6.361111   6  11.516454\n'];['df = \n        N0_YLDF  ZZ        MAT\n    0  6.286333   2  11.669069\n    1  6.317000   6  11.669069\n    2  6.324889   6  11.516454\n    3  6.320667   5  11.516454\n    4  6.325556   5  11.516454\n    5  6.359000   6  11.516454\n    6  6.359000   6  11.516454\n    7  6.361111   7  11.516454\n    8  6.360778   7  11.516454\n    9  6.361111   6  11.516454\n'];['df = \n        N0_YLDF  ZZ        MAT\n    0  6.286333   2  11.669069\n    1  6.317000   6  11.669069\n    2  6.324889   6  11.516454\n    3  6.320667   5  11.516454\n    4  6.325556   5  11.516454\n    5  6.359000   6  11.516454\n    6  6.359000   6  11.516454\n    7  6.361111   7  11.516454\n    8  6.360778   7  11.516454\n    9  6.361111   6  11.516454\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'ZZ'""]";['KeyError']
787;787;787;787;2.0;1;23731564;;1;11;<python><pandas>;KeyError when indexing Pandas dataframe;30310.0;"[""import pandas as pd\ndf_ticks=pd.read_csv('values.csv', delimiter=',')\nprint(df_ticks.columns)\ndf_ticks['Date']\nIndex([u'Date', u'Open', u'High', u'Low', u'Close', u'Volume'], dtype='object')\nKeyError: u'no item named Date'\n""]";"[""import pandas as pd\ndf_ticks=pd.read_csv('values.csv', delimiter=',')\nprint(df_ticks.columns)\ndf_ticks['Date']\n"", ""Index([u'Date', u'Open', u'High', u'Low', u'Close', u'Volume'], dtype='object')\nKeyError: u'no item named Date'\n""]";"[""import pandas as pd\ndf_ticks=pd.read_csv('values.csv', delimiter=',')\nprint(df_ticks.columns)\ndf_ticks['Date']\n"", ""Index([u'Date', u'Open', u'High', u'Low', u'Close', u'Volume'], dtype='object')\nKeyError: u'no item named Date'\n""]";"[""import pandas as pd\ndf_ticks=pd.read_csv('values.csv', delimiter=',')\nprint(df_ticks.columns)\ndf_ticks['Date']\nIndex([u'Date', u'Open', u'High', u'Low', u'Close', u'Volume'], dtype='object')\n""]";"[""import pandas as pd\ndf_ticks=pd.read_csv('values.csv', delimiter=',')\nprint(df_ticks.columns)\ndf_ticks['Date']\nIndex([u'Date', u'Open', u'High', u'Low', u'Close', u'Volume'], dtype='object')\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\ndf_ticks=pd.read_csv('values.csv', delimiter=',')\nprint(df_ticks.columns)\ndf_ticks['Date']\nIndex([u'Date', u'Open', u'High', u'Low', u'Close', u'Volume'], dtype='object')\n""]";True;0;2;"[""name 'Index' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'Index' is not defined"", ""File b'values.csv' does not exist""]";['NameError', 'FileNotFoundError'];0;2;"[""name 'Index' is not defined"", ""File b'values.csv' does not exist""]";['NameError', 'FileNotFoundError']
788;788;788;788;1.0;0;23743460;;1;17;<pandas>;Replacing None with NaN in pandas;8731.0;"[""        website\n0   http://www.google.com/\n1   http://www.yahoo.com\n2   None\nx.replace(to_replace=None, value=np.nan)\nTypeError: 'regex' must be a string or a compiled regular expression or a list or dict of strings or regular expressions, you passed a 'bool'\n""]";"['        website\n0   http://www.google.com/\n1   http://www.yahoo.com\n2   None\n', 'x.replace(to_replace=None, value=np.nan)\n', ""TypeError: 'regex' must be a string or a compiled regular expression or a list or dict of strings or regular expressions, you passed a 'bool'\n""]";"['x', '        website\n0   http://www.google.com/\n1   http://www.yahoo.com\n2   None\n', 'x.replace(to_replace=None, value=np.nan)\n', ""TypeError: 'regex' must be a string or a compiled regular expression or a list or dict of strings or regular expressions, you passed a 'bool'\n""]";['x.replace(to_replace=None, value=np.nan)\n'];['x.replace(to_replace=None, value=np.nan)\n'];False;['import pandas as pd\nx.replace(to_replace=None, value=np.nan)\n'];False;0;1;"[""name 'my_dataframe' is not defined""]";['NameError'];0;1;"[""name 'my_dataframe' is not defined""]";['NameError'];0;1;"[""name 'my_dataframe' is not defined""]";['NameError']
789;789;789;789;3.0;2;23747451;;1;13;<python><pandas><dataframe>;Filtering all rows with NaT in a column in Dataframe python;16311.0;"[""    a b           c\n    1 NaT         w\n    2 2014-02-01  g\n    3 NaT         x   \n\n    df=df[df.b=='2014-02-01']\n    a  b          c\n    2 2014-02-01  g\n   df=df[df.b==None] #Doesn't work\n    a b           c\n    1 NaT         w\n    3 NaT         x    \n""]";"[""    a b           c\n    1 NaT         w\n    2 2014-02-01  g\n    3 NaT         x   \n\n    df=df[df.b=='2014-02-01']\n"", '    a  b          c\n    2 2014-02-01  g\n', ""   df=df[df.b==None] #Doesn't work\n"", '    a b           c\n    1 NaT         w\n    3 NaT         x    \n']";"[""    a b           c\n    1 NaT         w\n    2 2014-02-01  g\n    3 NaT         x   \n\n    df=df[df.b=='2014-02-01']\n"", '    a  b          c\n    2 2014-02-01  g\n', ""   df=df[df.b==None] #Doesn't work\n"", '    a b           c\n    1 NaT         w\n    3 NaT         x    \n']";['\n'];['\n'];False;['import pandas as pd\n\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'DataFrame' object has no attribute 'b'""]";['AttributeError']
790;790;790;790;4.0;1;23748995;;1;43;<python><pandas><tolist>;Pandas DataFrame to list;103159.0;"['import pandas as pd\n\ntst = pd.read_csv(\'C:\\\\SomeCSV.csv\')\n\nlookupValue = tst[\'SomeCol\'] == ""SomeValue""\nID = tst[lookupValue][[\'SomeCol\']]\n#How To convert ID to a list\n']";"['import pandas as pd\n\ntst = pd.read_csv(\'C:\\\\SomeCSV.csv\')\n\nlookupValue = tst[\'SomeCol\'] == ""SomeValue""\nID = tst[lookupValue][[\'SomeCol\']]\n#How To convert ID to a list\n']";"['import pandas as pd\n\ntst = pd.read_csv(\'C:\\\\SomeCSV.csv\')\n\nlookupValue = tst[\'SomeCol\'] == ""SomeValue""\nID = tst[lookupValue][[\'SomeCol\']]\n#How To convert ID to a list\n']";"['import pandas as pd\n\ntst = pd.read_csv(\'C:\\\\SomeCSV.csv\')\n\nlookupValue = tst[\'SomeCol\'] == ""SomeValue""\nID = tst[lookupValue][[\'SomeCol\']]\n#How To convert ID to a list\n']";"['import pandas as pd\n\ntst = pd.read_csv(\'C:\\\\SomeCSV.csv\')\n\nlookupValue = tst[\'SomeCol\'] == ""SomeValue""\nID = tst[lookupValue][[\'SomeCol\']]\n#How To convert ID to a list\n']";False;"['import pandas as pd\nimport pandas as pd\n\ntst = pd.read_csv(\'C:\\\\SomeCSV.csv\')\n\nlookupValue = tst[\'SomeCol\'] == ""SomeValue""\nID = tst[lookupValue][[\'SomeCol\']]\n#How To convert ID to a list\n']";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
791;791;791;791;1.0;8;23787072;;1;15;<python><pandas><indexing><data-analysis>;Python Pandas join dataframes on index;33759.0;"['import pandas as pd\nfrom datetime import datetime\ndf_train_csv = pd.read_csv(\'./train.csv\',parse_dates=[\'Date\'],index_col=\'Date\')\n\nstart = datetime(2010, 2, 5)\nend = datetime(2012, 10, 26)\n\ndf_train_fly = pd.date_range(start, end, freq=""W-FRI"")\ndf_train_fly = pd.DataFrame(pd.Series(df_train_fly), columns=[\'Date\'])\n\nmerged = df_train_csv.join(df_train_fly.set_index([\'Date\']), on = [\'Date\'], how = \'right\', lsuffix=\'_x\')\nDate,Weekly_Sales\n2010-02-05,24924.5\n2010-02-12,46039.49\n2010-02-19,41595.55\n2010-02-26,19403.54\n2010-03-05,21827.9\n2010-03-12,21043.39\n2010-03-19,22136.64\n2010-03-26,26229.21\n2010-04-02,57258.43\n2010-04-09,42960.91\n2010-04-16,17596.96\n2010-04-23,16145.35\n2010-04-30,16555.11\n2010-05-07,17413.94\n2010-05-14,18926.74\n2010-05-21,14773.04\n2010-05-28,15580.43\n2010-06-04,17558.09\n2010-06-11,16637.62\n2010-06-18,16216.27\n2010-06-25,16328.72\n2010-07-02,16333.14\n2010-07-09,17688.76\n2010-07-16,17150.84\n2010-07-23,15360.45\n2010-07-30,15381.82\n2010-08-06,17508.41\n2010-08-13,15536.4\n2010-08-20,15740.13\n2010-08-27,15793.87\n2010-09-03,16241.78\n2010-09-10,18194.74\n2010-09-17,19354.23\n2010-09-24,18122.52\n2010-10-01,20094.19\n2010-10-08,23388.03\n2010-10-15,26978.34\n2010-10-22,25543.04\n2010-10-29,38640.93\n2010-11-05,34238.88\n2010-11-12,19549.39\n2010-11-19,19552.84\n2010-11-26,18820.29\n2010-12-03,22517.56\n2010-12-10,31497.65\n2010-12-17,44912.86\n2010-12-24,55931.23\n2010-12-31,19124.58\n2011-01-07,15984.24\n2011-01-14,17359.7\n2011-01-21,17341.47\n2011-01-28,18461.18\n2011-02-04,21665.76\n2011-02-11,37887.17\n2011-02-18,46845.87\n2011-02-25,19363.83\n2011-03-04,20327.61\n2011-03-11,21280.4\n2011-03-18,20334.23\n2011-03-25,20881.1\n2011-04-01,20398.09\n2011-04-08,23873.79\n2011-04-15,28762.37\n2011-04-22,50510.31\n2011-04-29,41512.39\n2011-05-06,20138.19\n2011-05-13,17235.15\n2011-05-20,15136.78\n2011-05-27,15741.6\n2011-06-03,16434.15\n2011-06-10,15883.52\n2011-06-17,14978.09\n2011-06-24,15682.81\n2011-07-01,15363.5\n2011-07-08,16148.87\n2011-07-15,15654.85\n2011-07-22,15766.6\n2011-07-29,15922.41\n2011-08-05,15295.55\n2011-08-12,14539.79\n2011-08-19,14689.24\n2011-08-26,14537.37\n2011-09-02,15277.27\n2011-09-09,17746.68\n2011-09-16,18535.48\n2011-09-23,17859.3\n2011-09-30,18337.68\n2011-10-07,20797.58\n2011-10-14,23077.55\n2011-10-21,23351.8\n2011-10-28,31579.9\n2011-11-04,39886.06\n2011-11-11,18689.54\n2011-11-18,19050.66\n2011-11-25,20911.25\n2011-12-02,25293.49\n2011-12-09,33305.92\n2011-12-16,45773.03\n2011-12-23,46788.75\n2011-12-30,23350.88\n2012-01-06,16567.69\n2012-01-13,16894.4\n2012-01-20,18365.1\n2012-01-27,18378.16\n2012-02-03,23510.49\n2012-02-10,36988.49\n2012-02-17,54060.1\n2012-02-24,20124.22\n2012-03-02,20113.03\n2012-03-09,21140.07\n2012-03-16,22366.88\n2012-03-23,22107.7\n2012-03-30,28952.86\n2012-04-06,57592.12\n2012-04-13,34684.21\n2012-04-20,16976.19\n2012-04-27,16347.6\n2012-05-04,17147.44\n2012-05-11,18164.2\n2012-05-18,18517.79\n2012-05-25,16963.55\n2012-06-01,16065.49\n2012-06-08,17666\n2012-06-15,17558.82\n2012-06-22,16633.41\n2012-06-29,15722.82\n2012-07-06,17823.37\n2012-07-13,16566.18\n2012-07-20,16348.06\n2012-07-27,15731.18\n2012-08-03,16628.31\n2012-08-10,16119.92\n2012-08-17,17330.7\n2012-08-24,16286.4\n2012-08-31,16680.24\n2012-09-07,18322.37\n2012-09-14,19616.22\n2012-09-21,19251.5\n2012-09-28,18947.81\n2012-10-05,21904.47\n2012-10-12,22764.01\n2012-10-19,24185.27\n2012-10-26,27390.81\ndf_train_fly = pd.date_range(start, end, freq=""W-FRI"")\n']";"['import pandas as pd\nfrom datetime import datetime\ndf_train_csv = pd.read_csv(\'./train.csv\',parse_dates=[\'Date\'],index_col=\'Date\')\n\nstart = datetime(2010, 2, 5)\nend = datetime(2012, 10, 26)\n\ndf_train_fly = pd.date_range(start, end, freq=""W-FRI"")\ndf_train_fly = pd.DataFrame(pd.Series(df_train_fly), columns=[\'Date\'])\n\nmerged = df_train_csv.join(df_train_fly.set_index([\'Date\']), on = [\'Date\'], how = \'right\', lsuffix=\'_x\')\n', 'Date,Weekly_Sales\n2010-02-05,24924.5\n2010-02-12,46039.49\n2010-02-19,41595.55\n2010-02-26,19403.54\n2010-03-05,21827.9\n2010-03-12,21043.39\n2010-03-19,22136.64\n2010-03-26,26229.21\n2010-04-02,57258.43\n2010-04-09,42960.91\n2010-04-16,17596.96\n2010-04-23,16145.35\n2010-04-30,16555.11\n2010-05-07,17413.94\n2010-05-14,18926.74\n2010-05-21,14773.04\n2010-05-28,15580.43\n2010-06-04,17558.09\n2010-06-11,16637.62\n2010-06-18,16216.27\n2010-06-25,16328.72\n2010-07-02,16333.14\n2010-07-09,17688.76\n2010-07-16,17150.84\n2010-07-23,15360.45\n2010-07-30,15381.82\n2010-08-06,17508.41\n2010-08-13,15536.4\n2010-08-20,15740.13\n2010-08-27,15793.87\n2010-09-03,16241.78\n2010-09-10,18194.74\n2010-09-17,19354.23\n2010-09-24,18122.52\n2010-10-01,20094.19\n2010-10-08,23388.03\n2010-10-15,26978.34\n2010-10-22,25543.04\n2010-10-29,38640.93\n2010-11-05,34238.88\n2010-11-12,19549.39\n2010-11-19,19552.84\n2010-11-26,18820.29\n2010-12-03,22517.56\n2010-12-10,31497.65\n2010-12-17,44912.86\n2010-12-24,55931.23\n2010-12-31,19124.58\n2011-01-07,15984.24\n2011-01-14,17359.7\n2011-01-21,17341.47\n2011-01-28,18461.18\n2011-02-04,21665.76\n2011-02-11,37887.17\n2011-02-18,46845.87\n2011-02-25,19363.83\n2011-03-04,20327.61\n2011-03-11,21280.4\n2011-03-18,20334.23\n2011-03-25,20881.1\n2011-04-01,20398.09\n2011-04-08,23873.79\n2011-04-15,28762.37\n2011-04-22,50510.31\n2011-04-29,41512.39\n2011-05-06,20138.19\n2011-05-13,17235.15\n2011-05-20,15136.78\n2011-05-27,15741.6\n2011-06-03,16434.15\n2011-06-10,15883.52\n2011-06-17,14978.09\n2011-06-24,15682.81\n2011-07-01,15363.5\n2011-07-08,16148.87\n2011-07-15,15654.85\n2011-07-22,15766.6\n2011-07-29,15922.41\n2011-08-05,15295.55\n2011-08-12,14539.79\n2011-08-19,14689.24\n2011-08-26,14537.37\n2011-09-02,15277.27\n2011-09-09,17746.68\n2011-09-16,18535.48\n2011-09-23,17859.3\n2011-09-30,18337.68\n2011-10-07,20797.58\n2011-10-14,23077.55\n2011-10-21,23351.8\n2011-10-28,31579.9\n2011-11-04,39886.06\n2011-11-11,18689.54\n2011-11-18,19050.66\n2011-11-25,20911.25\n2011-12-02,25293.49\n2011-12-09,33305.92\n2011-12-16,45773.03\n2011-12-23,46788.75\n2011-12-30,23350.88\n2012-01-06,16567.69\n2012-01-13,16894.4\n2012-01-20,18365.1\n2012-01-27,18378.16\n2012-02-03,23510.49\n2012-02-10,36988.49\n2012-02-17,54060.1\n2012-02-24,20124.22\n2012-03-02,20113.03\n2012-03-09,21140.07\n2012-03-16,22366.88\n2012-03-23,22107.7\n2012-03-30,28952.86\n2012-04-06,57592.12\n2012-04-13,34684.21\n2012-04-20,16976.19\n2012-04-27,16347.6\n2012-05-04,17147.44\n2012-05-11,18164.2\n2012-05-18,18517.79\n2012-05-25,16963.55\n2012-06-01,16065.49\n2012-06-08,17666\n2012-06-15,17558.82\n2012-06-22,16633.41\n2012-06-29,15722.82\n2012-07-06,17823.37\n2012-07-13,16566.18\n2012-07-20,16348.06\n2012-07-27,15731.18\n2012-08-03,16628.31\n2012-08-10,16119.92\n2012-08-17,17330.7\n2012-08-24,16286.4\n2012-08-31,16680.24\n2012-09-07,18322.37\n2012-09-14,19616.22\n2012-09-21,19251.5\n2012-09-28,18947.81\n2012-10-05,21904.47\n2012-10-12,22764.01\n2012-10-19,24185.27\n2012-10-26,27390.81\n', 'df_train_fly = pd.date_range(start, end, freq=""W-FRI"")\n']";"['import pandas as pd\nfrom datetime import datetime\ndf_train_csv = pd.read_csv(\'./train.csv\',parse_dates=[\'Date\'],index_col=\'Date\')\n\nstart = datetime(2010, 2, 5)\nend = datetime(2012, 10, 26)\n\ndf_train_fly = pd.date_range(start, end, freq=""W-FRI"")\ndf_train_fly = pd.DataFrame(pd.Series(df_train_fly), columns=[\'Date\'])\n\nmerged = df_train_csv.join(df_train_fly.set_index([\'Date\']), on = [\'Date\'], how = \'right\', lsuffix=\'_x\')\n', 'Date,Weekly_Sales\n2010-02-05,24924.5\n2010-02-12,46039.49\n2010-02-19,41595.55\n2010-02-26,19403.54\n2010-03-05,21827.9\n2010-03-12,21043.39\n2010-03-19,22136.64\n2010-03-26,26229.21\n2010-04-02,57258.43\n2010-04-09,42960.91\n2010-04-16,17596.96\n2010-04-23,16145.35\n2010-04-30,16555.11\n2010-05-07,17413.94\n2010-05-14,18926.74\n2010-05-21,14773.04\n2010-05-28,15580.43\n2010-06-04,17558.09\n2010-06-11,16637.62\n2010-06-18,16216.27\n2010-06-25,16328.72\n2010-07-02,16333.14\n2010-07-09,17688.76\n2010-07-16,17150.84\n2010-07-23,15360.45\n2010-07-30,15381.82\n2010-08-06,17508.41\n2010-08-13,15536.4\n2010-08-20,15740.13\n2010-08-27,15793.87\n2010-09-03,16241.78\n2010-09-10,18194.74\n2010-09-17,19354.23\n2010-09-24,18122.52\n2010-10-01,20094.19\n2010-10-08,23388.03\n2010-10-15,26978.34\n2010-10-22,25543.04\n2010-10-29,38640.93\n2010-11-05,34238.88\n2010-11-12,19549.39\n2010-11-19,19552.84\n2010-11-26,18820.29\n2010-12-03,22517.56\n2010-12-10,31497.65\n2010-12-17,44912.86\n2010-12-24,55931.23\n2010-12-31,19124.58\n2011-01-07,15984.24\n2011-01-14,17359.7\n2011-01-21,17341.47\n2011-01-28,18461.18\n2011-02-04,21665.76\n2011-02-11,37887.17\n2011-02-18,46845.87\n2011-02-25,19363.83\n2011-03-04,20327.61\n2011-03-11,21280.4\n2011-03-18,20334.23\n2011-03-25,20881.1\n2011-04-01,20398.09\n2011-04-08,23873.79\n2011-04-15,28762.37\n2011-04-22,50510.31\n2011-04-29,41512.39\n2011-05-06,20138.19\n2011-05-13,17235.15\n2011-05-20,15136.78\n2011-05-27,15741.6\n2011-06-03,16434.15\n2011-06-10,15883.52\n2011-06-17,14978.09\n2011-06-24,15682.81\n2011-07-01,15363.5\n2011-07-08,16148.87\n2011-07-15,15654.85\n2011-07-22,15766.6\n2011-07-29,15922.41\n2011-08-05,15295.55\n2011-08-12,14539.79\n2011-08-19,14689.24\n2011-08-26,14537.37\n2011-09-02,15277.27\n2011-09-09,17746.68\n2011-09-16,18535.48\n2011-09-23,17859.3\n2011-09-30,18337.68\n2011-10-07,20797.58\n2011-10-14,23077.55\n2011-10-21,23351.8\n2011-10-28,31579.9\n2011-11-04,39886.06\n2011-11-11,18689.54\n2011-11-18,19050.66\n2011-11-25,20911.25\n2011-12-02,25293.49\n2011-12-09,33305.92\n2011-12-16,45773.03\n2011-12-23,46788.75\n2011-12-30,23350.88\n2012-01-06,16567.69\n2012-01-13,16894.4\n2012-01-20,18365.1\n2012-01-27,18378.16\n2012-02-03,23510.49\n2012-02-10,36988.49\n2012-02-17,54060.1\n2012-02-24,20124.22\n2012-03-02,20113.03\n2012-03-09,21140.07\n2012-03-16,22366.88\n2012-03-23,22107.7\n2012-03-30,28952.86\n2012-04-06,57592.12\n2012-04-13,34684.21\n2012-04-20,16976.19\n2012-04-27,16347.6\n2012-05-04,17147.44\n2012-05-11,18164.2\n2012-05-18,18517.79\n2012-05-25,16963.55\n2012-06-01,16065.49\n2012-06-08,17666\n2012-06-15,17558.82\n2012-06-22,16633.41\n2012-06-29,15722.82\n2012-07-06,17823.37\n2012-07-13,16566.18\n2012-07-20,16348.06\n2012-07-27,15731.18\n2012-08-03,16628.31\n2012-08-10,16119.92\n2012-08-17,17330.7\n2012-08-24,16286.4\n2012-08-31,16680.24\n2012-09-07,18322.37\n2012-09-14,19616.22\n2012-09-21,19251.5\n2012-09-28,18947.81\n2012-10-05,21904.47\n2012-10-12,22764.01\n2012-10-19,24185.27\n2012-10-26,27390.81\n', 'df_train_fly = pd.date_range(start, end, freq=""W-FRI"")\n']";"['import pandas as pd\nfrom datetime import datetime\ndf_train_csv = pd.read_csv(\'./train.csv\',parse_dates=[\'Date\'],index_col=\'Date\')\n\nstart = datetime(2010, 2, 5)\nend = datetime(2012, 10, 26)\n\ndf_train_fly = pd.date_range(start, end, freq=""W-FRI"")\ndf_train_fly = pd.DataFrame(pd.Series(df_train_fly), columns=[\'Date\'])\n\nmerged = df_train_csv.join(df_train_fly.set_index([\'Date\']), on = [\'Date\'], how = \'right\', lsuffix=\'_x\')\nDate,Weekly_Sales\n2010-10-15,26978.34\n2010-10-22,25543.04\n2010-10-29,38640.93\n2010-11-12,19549.39\n2010-11-19,19552.84\n2010-11-26,18820.29\n2010-12-10,31497.65\n2010-12-17,44912.86\n2010-12-24,55931.23\n2010-12-31,19124.58\n2011-10-14,23077.55\n2011-10-21,23351.8\n2011-10-28,31579.9\n2011-11-11,18689.54\n2011-11-18,19050.66\n2011-11-25,20911.25\n2011-12-16,45773.03\n2011-12-23,46788.75\n2011-12-30,23350.88\n2012-10-12,22764.01\n2012-10-19,24185.27\n2012-10-26,27390.81\ndf_train_fly = pd.date_range(start, end, freq=""W-FRI"")\n']";"['import pandas as pd\nfrom datetime import datetime\ndf_train_csv = pd.read_csv(\'./train.csv\',parse_dates=[\'Date\'],index_col=\'Date\')\n\nstart = datetime(2010, 2, 5)\nend = datetime(2012, 10, 26)\n\ndf_train_fly = pd.date_range(start, end, freq=""W-FRI"")\ndf_train_fly = pd.DataFrame(pd.Series(df_train_fly), columns=[\'Date\'])\n\nmerged = df_train_csv.join(df_train_fly.set_index([\'Date\']), on = [\'Date\'], how = \'right\', lsuffix=\'_x\')\nDate,Weekly_Sales\n2010-10-15,26978.34\n2010-10-22,25543.04\n2010-10-29,38640.93\n2010-11-12,19549.39\n2010-11-19,19552.84\n2010-11-26,18820.29\n2010-12-10,31497.65\n2010-12-17,44912.86\n2010-12-24,55931.23\n2010-12-31,19124.58\n2011-10-14,23077.55\n2011-10-21,23351.8\n2011-10-28,31579.9\n2011-11-11,18689.54\n2011-11-18,19050.66\n2011-11-25,20911.25\n2011-12-16,45773.03\n2011-12-23,46788.75\n2011-12-30,23350.88\n2012-10-12,22764.01\n2012-10-19,24185.27\n2012-10-26,27390.81\ndf_train_fly = pd.date_range(start, end, freq=""W-FRI"")\n']";False;"['import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\nfrom datetime import datetime\ndf_train_csv = pd.read_csv(\'./train.csv\',parse_dates=[\'Date\'],index_col=\'Date\')\n\nstart = datetime(2010, 2, 5)\nend = datetime(2012, 10, 26)\n\ndf_train_fly = pd.date_range(start, end, freq=""W-FRI"")\ndf_train_fly = pd.DataFrame(pd.Series(df_train_fly), columns=[\'Date\'])\n\nmerged = df_train_csv.join(df_train_fly.set_index([\'Date\']), on = [\'Date\'], how = \'right\', lsuffix=\'_x\')\nDate,Weekly_Sales\n2010-10-15,26978.34\n2010-10-22,25543.04\n2010-10-29,38640.93\n2010-11-12,19549.39\n2010-11-19,19552.84\n2010-11-26,18820.29\n2010-12-10,31497.65\n2010-12-17,44912.86\n2010-12-24,55931.23\n2010-12-31,19124.58\n2011-10-14,23077.55\n2011-10-21,23351.8\n2011-10-28,31579.9\n2011-11-11,18689.54\n2011-11-18,19050.66\n2011-11-25,20911.25\n2011-12-16,45773.03\n2011-12-23,46788.75\n2011-12-30,23350.88\n2012-10-12,22764.01\n2012-10-19,24185.27\n2012-10-26,27390.81\ndf_train_fly = pd.date_range(start, end, freq=""W-FRI"")\n']";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""File b'./train.csv' does not exist""]";['FileNotFoundError'];0;1;"[""File b'./train.csv' does not exist""]";['FileNotFoundError']
792;792;792;792;2.0;0;23836277;;1;12;<python><pandas>;Add Leading Zeros to Strings in Pandas Dataframe;6942.0;"[""         ID        text1    text 2\n0       2345656     blah      blah\n1          3456     blah      blah\n2        541304     blah      blah        \n3        201306       hi      blah        \n4   12313201308    hello      blah         \n                ID    text1    text 2\n0  000000002345656     blah      blah\n1  000000000003456     blah      blah\n2  000000000541304     blah      blah        \n3  000000000201306       hi      blah        \n4  000012313201308    hello      blah \ndf['ID'] = df.ID.zfill(15))\ndf['ID'] = '{0:0>15}'.format(df['ID']\n""]";"['         ID        text1    text 2\n0       2345656     blah      blah\n1          3456     blah      blah\n2        541304     blah      blah        \n3        201306       hi      blah        \n4   12313201308    hello      blah         \n', '                ID    text1    text 2\n0  000000002345656     blah      blah\n1  000000000003456     blah      blah\n2  000000000541304     blah      blah        \n3  000000000201306       hi      blah        \n4  000012313201308    hello      blah \n', ""df['ID'] = df.ID.zfill(15))\ndf['ID'] = '{0:0>15}'.format(df['ID']\n""]";"['         ID        text1    text 2\n0       2345656     blah      blah\n1          3456     blah      blah\n2        541304     blah      blah        \n3        201306       hi      blah        \n4   12313201308    hello      blah         \n', '                ID    text1    text 2\n0  000000002345656     blah      blah\n1  000000000003456     blah      blah\n2  000000000541304     blah      blah        \n3  000000000201306       hi      blah        \n4  000012313201308    hello      blah \n', ""df['ID'] = df.ID.zfill(15))\ndf['ID'] = '{0:0>15}'.format(df['ID']\n""]";[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'ID'"", ""'ID'""]";['KeyError', 'KeyError']
793;793;793;793;1.0;0;23853553;;1;23;<python><pandas>;Python Pandas: How to read only first n rows of CSV files in?;18338.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'read_csv' is not defined""]";['NameError'];0;1;"[""name 'read_csv' is not defined""]";['NameError'];0;1;"[""name 'read_csv' is not defined""]";['NameError']
794;794;794;794;3.0;0;23887881;;1;12;<python><pandas><duplicates><dataframe><repeat>;How to repeat Pandas data frame?;8822.0;"["">>> x = pd.DataFrame({'a':1,'b':2},index = range(1))\n>>> x\n   a  b\n0  1  2\n>>> x.append(x).append(x).append(x)\n   a  b\n0  1  2\n0  1  2\n0  1  2\n0  1  2\n""]";"["">>> x = pd.DataFrame({'a':1,'b':2},index = range(1))\n>>> x\n   a  b\n0  1  2\n"", '>>> x.append(x).append(x).append(x)\n   a  b\n0  1  2\n0  1  2\n0  1  2\n0  1  2\n']";"["">>> x = pd.DataFrame({'a':1,'b':2},index = range(1))\n>>> x\n   a  b\n0  1  2\n"", '>>> x.append(x).append(x).append(x)\n   a  b\n0  1  2\n0  1  2\n0  1  2\n0  1  2\n', 'np.repeat']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'x' is not defined""]";['NameError'];0;1;"[""name 'x' is not defined""]";['NameError']
795;795;795;795;2.0;1;23966152;;1;11;<python><datetime><numpy><pandas>;how to create a group ID based on 5 minutes interval in pandas timeseries?;3577.0;['                                id               val \n time                    \n2014-04-03 16:01:53             23              14389      \n2014-04-03 16:01:54             28              14391             \n2014-04-03 16:05:55             24              14393             \n2014-04-03 16:06:25             23              14395             \n2014-04-03 16:07:01             23              14395             \n2014-04-03 16:10:09             23              14395             \n2014-04-03 16:10:23             26              14397             \n2014-04-03 16:10:57             26              14397             \n2014-04-03 16:11:10             26              14397              \n                                id               val           period \ntime            \n2014-04-03 16:01:53             23              14389             1\n2014-04-03 16:01:54             28              14391             1\n2014-04-03 16:05:55             24              14393             2\n2014-04-03 16:06:25             23              14395             2\n2014-04-03 16:07:01             23              14395             2\n2014-04-03 16:10:09             23              14395             3\n2014-04-03 16:10:23             26              14397             3\n2014-04-03 16:10:57             26              14397             3\n2014-04-03 16:11:10             26              14397             3\n'];['                                id               val \n time                    \n2014-04-03 16:01:53             23              14389      \n2014-04-03 16:01:54             28              14391             \n2014-04-03 16:05:55             24              14393             \n2014-04-03 16:06:25             23              14395             \n2014-04-03 16:07:01             23              14395             \n2014-04-03 16:10:09             23              14395             \n2014-04-03 16:10:23             26              14397             \n2014-04-03 16:10:57             26              14397             \n2014-04-03 16:11:10             26              14397              \n', '                                id               val           period \ntime            \n2014-04-03 16:01:53             23              14389             1\n2014-04-03 16:01:54             28              14391             1\n2014-04-03 16:05:55             24              14393             2\n2014-04-03 16:06:25             23              14395             2\n2014-04-03 16:07:01             23              14395             2\n2014-04-03 16:10:09             23              14395             3\n2014-04-03 16:10:23             26              14397             3\n2014-04-03 16:10:57             26              14397             3\n2014-04-03 16:11:10             26              14397             3\n'];"['df', '                                id               val \n time                    \n2014-04-03 16:01:53             23              14389      \n2014-04-03 16:01:54             28              14391             \n2014-04-03 16:05:55             24              14393             \n2014-04-03 16:06:25             23              14395             \n2014-04-03 16:07:01             23              14395             \n2014-04-03 16:10:09             23              14395             \n2014-04-03 16:10:23             26              14397             \n2014-04-03 16:10:57             26              14397             \n2014-04-03 16:11:10             26              14397              \n', '16:00:00', '16:00:00', '16:05:00', 'period', '                                id               val           period \ntime            \n2014-04-03 16:01:53             23              14389             1\n2014-04-03 16:01:54             28              14391             1\n2014-04-03 16:05:55             24              14393             2\n2014-04-03 16:06:25             23              14395             2\n2014-04-03 16:07:01             23              14395             2\n2014-04-03 16:10:09             23              14395             3\n2014-04-03 16:10:23             26              14397             3\n2014-04-03 16:10:57             26              14397             3\n2014-04-03 16:11:10             26              14397             3\n', 'groupby', ""pd.resample(how=' ')"", 'period', ""df.groupby('period').apply(myfunc)""]";['time            \n'];['time            \n'];False;['import pandas as pd\ntime            \n'];False;0;1;"[""name 'time' is not defined""]";['NameError'];0;1;"[""name 'time' is not defined""]";['NameError'];0;1;"[""name 'time' is not defined""]";['NameError']
796;796;796;796;2.0;4;24007762;;1;12;<python><mysql><sql><pandas><sqlalchemy>;Python Pandas - Using to_sql to write large data frames in chunks;8266.0;"['def write_to_db(engine, frame, table_name, chunk_size):\n\n    start_index = 0\n    end_index = chunk_size if chunk_size < len(frame) else len(frame)\n\n    frame = frame.where(pd.notnull(frame), None)\n    if_exists_param = \'replace\'\n\n    while start_index != end_index:\n        print ""Writing rows %s through %s"" % (start_index, end_index)\n        frame.iloc[start_index:end_index, :].to_sql(con=engine, name=table_name, if_exists=if_exists_param)\n        if_exists_param = \'append\'\n\n        start_index = min(start_index + chunk_size, len(frame))\n        end_index = min(end_index + chunk_size, len(frame))\n\nengine = sqlalchemy.create_engine(\'mysql://...\') #database details omited\nwrite_to_db(engine, frame, \'retail_pendingcustomers\', 20000)\n']";"['def write_to_db(engine, frame, table_name, chunk_size):\n\n    start_index = 0\n    end_index = chunk_size if chunk_size < len(frame) else len(frame)\n\n    frame = frame.where(pd.notnull(frame), None)\n    if_exists_param = \'replace\'\n\n    while start_index != end_index:\n        print ""Writing rows %s through %s"" % (start_index, end_index)\n        frame.iloc[start_index:end_index, :].to_sql(con=engine, name=table_name, if_exists=if_exists_param)\n        if_exists_param = \'append\'\n\n        start_index = min(start_index + chunk_size, len(frame))\n        end_index = min(end_index + chunk_size, len(frame))\n\nengine = sqlalchemy.create_engine(\'mysql://...\') #database details omited\nwrite_to_db(engine, frame, \'retail_pendingcustomers\', 20000)\n']";"['to_sql', 'def write_to_db(engine, frame, table_name, chunk_size):\n\n    start_index = 0\n    end_index = chunk_size if chunk_size < len(frame) else len(frame)\n\n    frame = frame.where(pd.notnull(frame), None)\n    if_exists_param = \'replace\'\n\n    while start_index != end_index:\n        print ""Writing rows %s through %s"" % (start_index, end_index)\n        frame.iloc[start_index:end_index, :].to_sql(con=engine, name=table_name, if_exists=if_exists_param)\n        if_exists_param = \'append\'\n\n        start_index = min(start_index + chunk_size, len(frame))\n        end_index = min(end_index + chunk_size, len(frame))\n\nengine = sqlalchemy.create_engine(\'mysql://...\') #database details omited\nwrite_to_db(engine, frame, \'retail_pendingcustomers\', 20000)\n']";"[""\n\n\n\n\nengine = sqlalchemy.create_engine('mysql://...') #database details omited\nwrite_to_db(engine, frame, 'retail_pendingcustomers', 20000)\n""]";"[""\n\n\n\n\nengine = sqlalchemy.create_engine('mysql://...') #database details omited\nwrite_to_db(engine, frame, 'retail_pendingcustomers', 20000)\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\n\n\n\n\n\nengine = sqlalchemy.create_engine('mysql://...') #database details omited\nwrite_to_db(engine, frame, 'retail_pendingcustomers', 20000)\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'engine' is not defined""]";['NameError']
797;797;797;797;3.0;0;24029659;;1;14;<python><pandas><dataframe>;Python Pandas replicate rows in dataframe;13950.0;"[""Store,Dept,Date,Weekly_Sales,IsHoliday\n1,1,2010-02-05,24924.5,FALSE\n1,1,2010-02-12,46039.49,TRUE\n1,1,2010-02-19,41595.55,FALSE\n1,1,2010-02-26,19403.54,FALSE\n1,1,2010-03-05,21827.9,FALSE\n1,1,2010-03-12,21043.39,FALSE\n1,1,2010-03-19,22136.64,FALSE\n1,1,2010-03-26,26229.21,FALSE\n1,1,2010-04-02,57258.43,FALSE\nis_hol = df['IsHoliday'] == True\ndf_try = df[is_hol]\ndf=df.append(df_try*10)\n""]";"['Store,Dept,Date,Weekly_Sales,IsHoliday\n1,1,2010-02-05,24924.5,FALSE\n1,1,2010-02-12,46039.49,TRUE\n1,1,2010-02-19,41595.55,FALSE\n1,1,2010-02-26,19403.54,FALSE\n1,1,2010-03-05,21827.9,FALSE\n1,1,2010-03-12,21043.39,FALSE\n1,1,2010-03-19,22136.64,FALSE\n1,1,2010-03-26,26229.21,FALSE\n1,1,2010-04-02,57258.43,FALSE\n', ""is_hol = df['IsHoliday'] == True\ndf_try = df[is_hol]\ndf=df.append(df_try*10)\n""]";"['Store,Dept,Date,Weekly_Sales,IsHoliday\n1,1,2010-02-05,24924.5,FALSE\n1,1,2010-02-12,46039.49,TRUE\n1,1,2010-02-19,41595.55,FALSE\n1,1,2010-02-26,19403.54,FALSE\n1,1,2010-03-05,21827.9,FALSE\n1,1,2010-03-12,21043.39,FALSE\n1,1,2010-03-19,22136.64,FALSE\n1,1,2010-03-26,26229.21,FALSE\n1,1,2010-04-02,57258.43,FALSE\n', ""is_hol = df['IsHoliday'] == True\ndf_try = df[is_hol]\ndf=df.append(df_try*10)\n""]";"[""Store,Dept,Date,Weekly_Sales,IsHoliday\nis_hol = df['IsHoliday'] == True\ndf_try = df[is_hol]\ndf=df.append(df_try*10)\n""]";"[""Store,Dept,Date,Weekly_Sales,IsHoliday\nis_hol = df['IsHoliday'] == True\ndf_try = df[is_hol]\ndf=df.append(df_try*10)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nStore,Dept,Date,Weekly_Sales,IsHoliday\nis_hol = df['IsHoliday'] == True\ndf_try = df[is_hol]\ndf=df.append(df_try*10)\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
798;798;798;798;3.0;0;24036911;;1;16;<python><pandas>;How to update values in a specific row in a Python Pandas DataFrame?;26260.0;"[""import pandas as pd\ndf = pd.DataFrame({'filename' :  ['test0.dat', 'test2.dat'], \n                                  'm': [12, 13], 'n' : [None, None]})\ndf2 = pd.DataFrame({'filename' :  'test2.dat', 'n':16}, index=[0])\n\n# this overwrites the first row but we want to update the second\n# df.update(df2)\n\n# this does not update anything\ndf.loc[df.filename == 'test2.dat'].update(df2)\n\nprint(df)\n   filename   m     n\n0  test0.dat  12  None\n1  test2.dat  13  None\n\n[2 rows x 3 columns]\n    filename   m     n\n0  test0.dat  12  None\n1  test2.dat  13  16\n\n[2 rows x 3 columns]\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'filename' :  ['test0.dat', 'test2.dat'], \n                                  'm': [12, 13], 'n' : [None, None]})\ndf2 = pd.DataFrame({'filename' :  'test2.dat', 'n':16}, index=[0])\n\n# this overwrites the first row but we want to update the second\n# df.update(df2)\n\n# this does not update anything\ndf.loc[df.filename == 'test2.dat'].update(df2)\n\nprint(df)\n"", '   filename   m     n\n0  test0.dat  12  None\n1  test2.dat  13  None\n\n[2 rows x 3 columns]\n', '    filename   m     n\n0  test0.dat  12  None\n1  test2.dat  13  16\n\n[2 rows x 3 columns]\n']";"[""import pandas as pd\ndf = pd.DataFrame({'filename' :  ['test0.dat', 'test2.dat'], \n                                  'm': [12, 13], 'n' : [None, None]})\ndf2 = pd.DataFrame({'filename' :  'test2.dat', 'n':16}, index=[0])\n\n# this overwrites the first row but we want to update the second\n# df.update(df2)\n\n# this does not update anything\ndf.loc[df.filename == 'test2.dat'].update(df2)\n\nprint(df)\n"", '   filename   m     n\n0  test0.dat  12  None\n1  test2.dat  13  None\n\n[2 rows x 3 columns]\n', '    filename   m     n\n0  test0.dat  12  None\n1  test2.dat  13  16\n\n[2 rows x 3 columns]\n']";"[""import pandas as pd\ndf2 = pd.DataFrame({'filename' :  'test2.dat', 'n':16}, index=[0])\n\n# this overwrites the first row but we want to update the second\n# df.update(df2)\n\n# this does not update anything\ndf.loc[df.filename == 'test2.dat'].update(df2)\n\nprint(df)\n\n\n""]";"[""import pandas as pd\ndf2 = pd.DataFrame({'filename' :  'test2.dat', 'n':16}, index=[0])\n\n# this overwrites the first row but we want to update the second\n# df.update(df2)\n\n# this does not update anything\ndf.loc[df.filename == 'test2.dat'].update(df2)\n\nprint(df)\n\n\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\ndf2 = pd.DataFrame({'filename' :  'test2.dat', 'n':16}, index=[0])\n\n# this overwrites the first row but we want to update the second\n# df.update(df2)\n\n# this does not update anything\ndf.loc[df.filename == 'test2.dat'].update(df2)\n\nprint(df)\n\n\n""]";True;0;1;"[""name 'filename' is not defined""]";['NameError'];0;1;"[""name 'filename' is not defined""]";['NameError'];0;1;"[""name 'filename' is not defined""]";['NameError']
799;799;799;799;2.0;2;24037507;;1;16;<python><csv><pandas>;Converting string objects to int/float using pandas;46157.0;"['import pandas as pd\n\npath1 = ""/home/supertramp/Desktop/100&life_180_data.csv""\n\nmydf =  pd.read_csv(path1)\n\nnumcigar = {""Never"":0 ,""1-5 Cigarettes/day"" :1,""10-20 Cigarettes/day"":4}\n\nprint mydf[\'Cigarettes\']\n\nmydf[\'CigarNum\'] = mydf[\'Cigarettes\'].apply(numcigar.get).astype(float)\n\nprint mydf[\'CigarNum\']\n\nmydf.to_csv(\'/home/supertramp/Desktop/powerRangers.csv\')\nNo                int64\nAge               int64\nBMI             float64\nAlcohol          object\nCigarettes       object\ndtype: object\n0                     Never\n1                     Never\n2        1-5 Cigarettes/day\n3                     Never\n4                     Never\n5                     Never\n6                     Never\n7                     Never\n8                     Never\n9                     Never\n10                    Never\n11                    Never\n12     10-20 Cigarettes/day\n13       1-5 Cigarettes/day\n14                    Never\n...\n167                    Never\n168                    Never\n169     10-20 Cigarettes/day\n170                    Never\n171                    Never\n172                    Never\n173                    Never\n174                    Never\n175                    Never\n176                    Never\n177                    Never\n178                    Never\n179                    Never\n180                    Never\n181                    Never\nName: Cigarettes, Length: 182, dtype: object\n0      0\n1      0\n2      1\n3      0\n4      0\n5      0\n6      0\n7      0\n8      0\n9      0\n10   NaN\n11   NaN\n12   NaN\n13   NaN\n14     0\n...\n167   NaN\n168   NaN\n169   NaN\n170   NaN\n171   NaN\n172   NaN\n173   NaN\n174   NaN\n175   NaN\n176   NaN\n177   NaN\n178   NaN\n179   NaN\n180   NaN\n181   NaN\nName: CigarNum, Length: 182, dtype: float64\n']";"['import pandas as pd\n\npath1 = ""/home/supertramp/Desktop/100&life_180_data.csv""\n\nmydf =  pd.read_csv(path1)\n\nnumcigar = {""Never"":0 ,""1-5 Cigarettes/day"" :1,""10-20 Cigarettes/day"":4}\n\nprint mydf[\'Cigarettes\']\n\nmydf[\'CigarNum\'] = mydf[\'Cigarettes\'].apply(numcigar.get).astype(float)\n\nprint mydf[\'CigarNum\']\n\nmydf.to_csv(\'/home/supertramp/Desktop/powerRangers.csv\')\n', 'No                int64\nAge               int64\nBMI             float64\nAlcohol          object\nCigarettes       object\ndtype: object\n', '0                     Never\n1                     Never\n2        1-5 Cigarettes/day\n3                     Never\n4                     Never\n5                     Never\n6                     Never\n7                     Never\n8                     Never\n9                     Never\n10                    Never\n11                    Never\n12     10-20 Cigarettes/day\n13       1-5 Cigarettes/day\n14                    Never\n...\n167                    Never\n168                    Never\n169     10-20 Cigarettes/day\n170                    Never\n171                    Never\n172                    Never\n173                    Never\n174                    Never\n175                    Never\n176                    Never\n177                    Never\n178                    Never\n179                    Never\n180                    Never\n181                    Never\nName: Cigarettes, Length: 182, dtype: object\n', '0      0\n1      0\n2      1\n3      0\n4      0\n5      0\n6      0\n7      0\n8      0\n9      0\n10   NaN\n11   NaN\n12   NaN\n13   NaN\n14     0\n...\n167   NaN\n168   NaN\n169   NaN\n170   NaN\n171   NaN\n172   NaN\n173   NaN\n174   NaN\n175   NaN\n176   NaN\n177   NaN\n178   NaN\n179   NaN\n180   NaN\n181   NaN\nName: CigarNum, Length: 182, dtype: float64\n']";"['import pandas as pd\n\npath1 = ""/home/supertramp/Desktop/100&life_180_data.csv""\n\nmydf =  pd.read_csv(path1)\n\nnumcigar = {""Never"":0 ,""1-5 Cigarettes/day"" :1,""10-20 Cigarettes/day"":4}\n\nprint mydf[\'Cigarettes\']\n\nmydf[\'CigarNum\'] = mydf[\'Cigarettes\'].apply(numcigar.get).astype(float)\n\nprint mydf[\'CigarNum\']\n\nmydf.to_csv(\'/home/supertramp/Desktop/powerRangers.csv\')\n', 'No                int64\nAge               int64\nBMI             float64\nAlcohol          object\nCigarettes       object\ndtype: object\n', '0                     Never\n1                     Never\n2        1-5 Cigarettes/day\n3                     Never\n4                     Never\n5                     Never\n6                     Never\n7                     Never\n8                     Never\n9                     Never\n10                    Never\n11                    Never\n12     10-20 Cigarettes/day\n13       1-5 Cigarettes/day\n14                    Never\n...\n167                    Never\n168                    Never\n169     10-20 Cigarettes/day\n170                    Never\n171                    Never\n172                    Never\n173                    Never\n174                    Never\n175                    Never\n176                    Never\n177                    Never\n178                    Never\n179                    Never\n180                    Never\n181                    Never\nName: Cigarettes, Length: 182, dtype: object\n', '0      0\n1      0\n2      1\n3      0\n4      0\n5      0\n6      0\n7      0\n8      0\n9      0\n10   NaN\n11   NaN\n12   NaN\n13   NaN\n14     0\n...\n167   NaN\n168   NaN\n169   NaN\n170   NaN\n171   NaN\n172   NaN\n173   NaN\n174   NaN\n175   NaN\n176   NaN\n177   NaN\n178   NaN\n179   NaN\n180   NaN\n181   NaN\nName: CigarNum, Length: 182, dtype: float64\n']";"['import pandas as pd\n\npath1 = ""/home/supertramp/Desktop/100&life_180_data.csv""\n\nmydf =  pd.read_csv(path1)\n\nnumcigar = {""Never"":0 ,""1-5 Cigarettes/day"" :1,""10-20 Cigarettes/day"":4}\n\n\nmydf[\'CigarNum\'] = mydf[\'Cigarettes\'].apply(numcigar.get).astype(float)\n\n\nmydf.to_csv(\'/home/supertramp/Desktop/powerRangers.csv\')\n...\n...\n']";"['import pandas as pd\n\npath1 = ""/home/supertramp/Desktop/100&life_180_data.csv""\n\nmydf =  pd.read_csv(path1)\n\nnumcigar = {""Never"":0 ,""1-5 Cigarettes/day"" :1,""10-20 Cigarettes/day"":4}\n\n\nmydf[\'CigarNum\'] = mydf[\'Cigarettes\'].apply(numcigar.get).astype(float)\n\n\nmydf.to_csv(\'/home/supertramp/Desktop/powerRangers.csv\')\n...\n...\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\nimport pandas as pd\n\npath1 = ""/home/supertramp/Desktop/100&life_180_data.csv""\n\nmydf =  pd.read_csv(path1)\n\nnumcigar = {""Never"":0 ,""1-5 Cigarettes/day"" :1,""10-20 Cigarettes/day"":4}\n\n\nmydf[\'CigarNum\'] = mydf[\'Cigarettes\'].apply(numcigar.get).astype(float)\n\n\nmydf.to_csv(\'/home/supertramp/Desktop/powerRangers.csv\')\n...\n...\n']";True;0;1;"[""name 'mydf' is not defined""]";['NameError'];0;1;"[""name 'mydf' is not defined""]";['NameError'];0;1;"[""name 'mydf' is not defined""]";['NameError']
800;800;800;800;1.0;5;24039023;;1;32;<python><pandas>;add column with constant value to pandas dataframe;32111.0;"[""df['new'] = pd.Series([0 for x in range(len(df.index))])\ndf['new'] = 0 \n""]";"[""df['new'] = pd.Series([0 for x in range(len(df.index))])\n"", ""df['new'] = 0 \n""]";"[""df['new'] = pd.Series([0 for x in range(len(df.index))])\n"", ""df['new'] = 0 \n""]";"[""df['new'] = pd.Series([0 for x in range(len(df.index))])\ndf['new'] = 0 \n""]";"[""import pandas as pd\ndf['new'] = pd.Series([0 for x in range(len(df.index))])\ndf['new'] = 0 \n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\ndf['new'] = pd.Series([0 for x in range(len(df.index))])\ndf['new'] = 0 \n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'DataFrame' object has no attribute 'a'""]";['AttributeError']
801;801;801;801;1.0;2;24041436;;1;21;<python><pandas>;set multi index of an existing data frame in pandas;21648.0;"['  Emp1    Empl2           date       Company\n0    0        0     2012-05-01         apple\n1    0        1     2012-05-29         apple\n2    0        1     2013-05-02         apple\n3    0        1     2013-11-22         apple\n18   1        0     2011-09-09        google\n19   1        0     2012-02-02        google\n20   1        0     2012-11-26        google\n21   1        0     2013-05-11        google\ndf = pd.DataFrame()\nfor c in company_list:\n        row = pd.DataFrame([dict(company = \'%s\' %s, date = datetime.date(2012, 05, 01))])\n        df = df.append(row, ignore_index = True)\n        for e in emp_list:\n            dataset  = pd.read_sql(""select company, emp_name, date(date), count(*) from company_table where  = \'""+s+""\' and emp_name = \'""+b+""\' group by company, date, name LIMIT 5 "", con)\n                if len(dataset) == 0:\n                row = pd.DataFrame([dict(sitename=\'%s\' %s, name = \'%s\' %b, date = datetime.date(2012, 05, 01), count = np.nan)])\n                dataset = dataset.append(row, ignore_index=True)\n            dataset = dataset.rename(columns = {\'count\': \'%s\' %b})\n            dataset = dataset.groupby([\'company\', \'date\', \'emp_name\'], as_index = False).sum()\n\n            dataset = dataset.drop(\'emp_name\', 1)\n            df = pd.merge(df, dataset, how = \'\')\n            df = df.sort(\'date\', ascending = True)\n            df.fillna(0, inplace = True)\n\ndf.set_index([\'Company\', \'date\'], inplace=True)            \nprint df\n']";"['  Emp1    Empl2           date       Company\n0    0        0     2012-05-01         apple\n1    0        1     2012-05-29         apple\n2    0        1     2013-05-02         apple\n3    0        1     2013-11-22         apple\n18   1        0     2011-09-09        google\n19   1        0     2012-02-02        google\n20   1        0     2012-11-26        google\n21   1        0     2013-05-11        google\n', 'df = pd.DataFrame()\nfor c in company_list:\n        row = pd.DataFrame([dict(company = \'%s\' %s, date = datetime.date(2012, 05, 01))])\n        df = df.append(row, ignore_index = True)\n        for e in emp_list:\n            dataset  = pd.read_sql(""select company, emp_name, date(date), count(*) from company_table where  = \'""+s+""\' and emp_name = \'""+b+""\' group by company, date, name LIMIT 5 "", con)\n                if len(dataset) == 0:\n                row = pd.DataFrame([dict(sitename=\'%s\' %s, name = \'%s\' %b, date = datetime.date(2012, 05, 01), count = np.nan)])\n                dataset = dataset.append(row, ignore_index=True)\n            dataset = dataset.rename(columns = {\'count\': \'%s\' %b})\n            dataset = dataset.groupby([\'company\', \'date\', \'emp_name\'], as_index = False).sum()\n\n            dataset = dataset.drop(\'emp_name\', 1)\n            df = pd.merge(df, dataset, how = \'\')\n            df = df.sort(\'date\', ascending = True)\n            df.fillna(0, inplace = True)\n\ndf.set_index([\'Company\', \'date\'], inplace=True)            \nprint df\n']";"['DataFrame', '  Emp1    Empl2           date       Company\n0    0        0     2012-05-01         apple\n1    0        1     2012-05-29         apple\n2    0        1     2013-05-02         apple\n3    0        1     2013-11-22         apple\n18   1        0     2011-09-09        google\n19   1        0     2012-02-02        google\n20   1        0     2012-11-26        google\n21   1        0     2013-05-11        google\n', 'MultiIndex', 'DataFrame', ""df.set_index(['Company', 'date'], inplace=True)"", 'df = pd.DataFrame()\nfor c in company_list:\n        row = pd.DataFrame([dict(company = \'%s\' %s, date = datetime.date(2012, 05, 01))])\n        df = df.append(row, ignore_index = True)\n        for e in emp_list:\n            dataset  = pd.read_sql(""select company, emp_name, date(date), count(*) from company_table where  = \'""+s+""\' and emp_name = \'""+b+""\' group by company, date, name LIMIT 5 "", con)\n                if len(dataset) == 0:\n                row = pd.DataFrame([dict(sitename=\'%s\' %s, name = \'%s\' %b, date = datetime.date(2012, 05, 01), count = np.nan)])\n                dataset = dataset.append(row, ignore_index=True)\n            dataset = dataset.rename(columns = {\'count\': \'%s\' %b})\n            dataset = dataset.groupby([\'company\', \'date\', \'emp_name\'], as_index = False).sum()\n\n            dataset = dataset.drop(\'emp_name\', 1)\n            df = pd.merge(df, dataset, how = \'\')\n            df = df.sort(\'date\', ascending = True)\n            df.fillna(0, inplace = True)\n\ndf.set_index([\'Company\', \'date\'], inplace=True)            \nprint df\n', 'DataFrame', 'None']";"[""df = pd.DataFrame()\n\n\ndf.set_index(['Company', 'date'], inplace=True)            \n""]";"[""import pandas as pd\ndf = pd.DataFrame()\n\n\ndf.set_index(['Company', 'date'], inplace=True)            \n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\n\n\ndf.set_index(['Company', 'date'], inplace=True)            \n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError']
802;802;802;802;4.0;2;24080275;;1;15;<python><matplotlib><plot><pandas>;Plotting multiple line graph using pandas and matplotlib;25416.0;['       date  template     score\n0  20140605         0  0.138786\n1  20140605         1  0.846441\n2  20140605         2  0.766636\n3  20140605         3  0.259632\n4  20140605         4  0.497366\n5  20140606         0  0.138139\n6  20140606         1  0.845320\n7  20140606         2  0.762876\n8  20140606         3  0.261035\n9  20140606         4  0.498010\n'];['       date  template     score\n0  20140605         0  0.138786\n1  20140605         1  0.846441\n2  20140605         2  0.766636\n3  20140605         3  0.259632\n4  20140605         4  0.497366\n5  20140606         0  0.138139\n6  20140606         1  0.845320\n7  20140606         2  0.762876\n8  20140606         3  0.261035\n9  20140606         4  0.498010\n'];['       date  template     score\n0  20140605         0  0.138786\n1  20140605         1  0.846441\n2  20140605         2  0.766636\n3  20140605         3  0.259632\n4  20140605         4  0.497366\n5  20140606         0  0.138139\n6  20140606         1  0.845320\n7  20140606         2  0.762876\n8  20140606         3  0.261035\n9  20140606         4  0.498010\n'];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""No module named 'matplotlib'"", ""name 'data' is not defined"", ""name 'pandas' is not defined""]";['ImportError', 'NameError', 'NameError'];0;3;"[""No module named 'matplotlib'"", ""name 'data' is not defined"", ""name 'pandas' is not defined""]";['ImportError', 'NameError', 'NameError'];0;3;"[""No module named 'matplotlib'"", ""'template'"", ""name 'pandas' is not defined""]";['ImportError', 'KeyError', 'NameError']
803;803;803;803;1.0;0;24082784;;1;27;<python><datetime><pandas>;pandas dataframe groupby datetime month;26219.0;"[""string,date,number\na string,2/5/11 9:16am,1.0\na string,3/5/11 10:44pm,2.0\na string,4/22/11 12:07pm,3.0\na string,4/22/11 12:10pm,4.0\na string,4/29/11 11:59am,1.0\na string,5/2/11 1:41pm,2.0\na string,5/2/11 2:02pm,3.0\na string,5/2/11 2:56pm,4.0\na string,5/2/11 3:00pm,5.0\na string,5/2/14 3:02pm,6.0\na string,5/2/14 3:18pm,7.0\nb=pd.read_csv('b.dat')\nb['date']=pd.to_datetime(b['date'],format='%m/%d/%y %I:%M%p')\nb.index=b['date']\nb.index.month\n""]";"['string,date,number\na string,2/5/11 9:16am,1.0\na string,3/5/11 10:44pm,2.0\na string,4/22/11 12:07pm,3.0\na string,4/22/11 12:10pm,4.0\na string,4/29/11 11:59am,1.0\na string,5/2/11 1:41pm,2.0\na string,5/2/11 2:02pm,3.0\na string,5/2/11 2:56pm,4.0\na string,5/2/11 3:00pm,5.0\na string,5/2/14 3:02pm,6.0\na string,5/2/14 3:18pm,7.0\n', ""b=pd.read_csv('b.dat')\nb['date']=pd.to_datetime(b['date'],format='%m/%d/%y %I:%M%p')\n"", ""b.index=b['date']\n"", 'b.index.month\n']";"['string,date,number\na string,2/5/11 9:16am,1.0\na string,3/5/11 10:44pm,2.0\na string,4/22/11 12:07pm,3.0\na string,4/22/11 12:10pm,4.0\na string,4/29/11 11:59am,1.0\na string,5/2/11 1:41pm,2.0\na string,5/2/11 2:02pm,3.0\na string,5/2/11 2:56pm,4.0\na string,5/2/11 3:00pm,5.0\na string,5/2/14 3:02pm,6.0\na string,5/2/14 3:18pm,7.0\n', ""b=pd.read_csv('b.dat')\nb['date']=pd.to_datetime(b['date'],format='%m/%d/%y %I:%M%p')\n"", ""b.index=b['date']\n"", 'b.index.month\n']";"[""string,date,number\nb=pd.read_csv('b.dat')\nb['date']=pd.to_datetime(b['date'],format='%m/%d/%y %I:%M%p')\nb.index=b['date']\nb.index.month\n""]";"[""import pandas as pd\nstring,date,number\nb=pd.read_csv('b.dat')\nb['date']=pd.to_datetime(b['date'],format='%m/%d/%y %I:%M%p')\nb.index=b['date']\nb.index.month\n""]";True;"[""import pandas as pd\nstring,date,number\nb=pd.read_csv('b.dat')\nb['date']=pd.to_datetime(b['date'],format='%m/%d/%y %I:%M%p')\nb.index=b['date']\nb.index.month\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'b' is not defined""]";['NameError'];0;1;"[""name 'b' is not defined""]";['NameError']
804;804;804;804;6.0;3;24122850;;1;16;<python><numpy><pandas>;pandas ValueError: numpy.dtype has the wrong size, try recompiling;23340.0;"['\npip install pandas\npip install numpy\n\n\n    numpy.dtype has the wrong size, try recompiling Traceback (most recent call last): \n    File ""./moen.py"", line 7, in  import pandas File ""/Library/Python/2.7/site-packages/pandas/__init__.py"", line 6, in  from . import hashtable, tslib, lib \n    File ""numpy.pxd"", line 157, in init pandas.hashtable (pandas/hashtable.c:22331) \n    ValueError: numpy.dtype has the wrong size, try recompiling\n\n']";"['\npip install pandas\npip install numpy\n', '\n\n    numpy.dtype has the wrong size, try recompiling Traceback (most recent call last): \n    File ""./moen.py"", line 7, in  import pandas File ""/Library/Python/2.7/site-packages/pandas/__init__.py"", line 6, in  from . import hashtable, tslib, lib \n    File ""numpy.pxd"", line 157, in init pandas.hashtable (pandas/hashtable.c:22331) \n    ValueError: numpy.dtype has the wrong size, try recompiling\n\n']";[];['\n\n\n\n'];['\n\n\n\n'];False;['import pandas as pd\n\n\n\n\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
805;805;805;805;4.0;7;24123498;;1;14;<python><pandas><scikit-learn><random-forest><feature-selection>;Recursive feature elimination on Random Forest using scikit-learn;9257.0;"['from sklearn import datasets\nimport pandas as pd\nfrom pandas import Series\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFECV\n\niris = datasets.load_iris()\nx=pd.DataFrame(iris.data, columns=[\'var1\',\'var2\',\'var3\', \'var4\'])\ny=pd.Series(iris.target, name=\'target\')\nrf = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, n_jobs=-1)\nrfecv = RFECV(estimator=rf, step=1, cv=10, scoring=\'ROC\', verbose=2)\nselector=rfecv.fit(x, y)\n\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/rfe.py"", line 336, in fit\n    ranking_ = rfe.fit(X_train, y_train).ranking_\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/rfe.py"", line 148, in fit\n    if estimator.coef_.ndim > 1:\nAttributeError: \'RandomForestClassifier\' object has no attribute \'coef_\'\n']";"['from sklearn import datasets\nimport pandas as pd\nfrom pandas import Series\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFECV\n\niris = datasets.load_iris()\nx=pd.DataFrame(iris.data, columns=[\'var1\',\'var2\',\'var3\', \'var4\'])\ny=pd.Series(iris.target, name=\'target\')\nrf = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, n_jobs=-1)\nrfecv = RFECV(estimator=rf, step=1, cv=10, scoring=\'ROC\', verbose=2)\nselector=rfecv.fit(x, y)\n\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/rfe.py"", line 336, in fit\n    ranking_ = rfe.fit(X_train, y_train).ranking_\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/rfe.py"", line 148, in fit\n    if estimator.coef_.ndim > 1:\nAttributeError: \'RandomForestClassifier\' object has no attribute \'coef_\'\n']";"['scikit-learn', 'RFECV', ""AttributeError: 'RandomForestClassifier' object has no attribute 'coef_'"", 'pandas', 'from sklearn import datasets\nimport pandas as pd\nfrom pandas import Series\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFECV\n\niris = datasets.load_iris()\nx=pd.DataFrame(iris.data, columns=[\'var1\',\'var2\',\'var3\', \'var4\'])\ny=pd.Series(iris.target, name=\'target\')\nrf = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, n_jobs=-1)\nrfecv = RFECV(estimator=rf, step=1, cv=10, scoring=\'ROC\', verbose=2)\nselector=rfecv.fit(x, y)\n\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/rfe.py"", line 336, in fit\n    ranking_ = rfe.fit(X_train, y_train).ranking_\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/rfe.py"", line 148, in fit\n    if estimator.coef_.ndim > 1:\nAttributeError: \'RandomForestClassifier\' object has no attribute \'coef_\'\n']";"[""from sklearn import datasets\nimport pandas as pd\nfrom pandas import Series\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFECV\n\niris = datasets.load_iris()\nx=pd.DataFrame(iris.data, columns=['var1','var2','var3', 'var4'])\ny=pd.Series(iris.target, name='target')\nrf = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, n_jobs=-1)\nrfecv = RFECV(estimator=rf, step=1, cv=10, scoring='ROC', verbose=2)\nselector=rfecv.fit(x, y)\n\n""]";"[""from sklearn import datasets\nimport pandas as pd\nfrom pandas import Series\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFECV\n\niris = datasets.load_iris()\nx=pd.DataFrame(iris.data, columns=['var1','var2','var3', 'var4'])\ny=pd.Series(iris.target, name='target')\nrf = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, n_jobs=-1)\nrfecv = RFECV(estimator=rf, step=1, cv=10, scoring='ROC', verbose=2)\nselector=rfecv.fit(x, y)\n\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nfrom sklearn import datasets\nimport pandas as pd\nfrom pandas import Series\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFECV\n\niris = datasets.load_iris()\nx=pd.DataFrame(iris.data, columns=['var1','var2','var3', 'var4'])\ny=pd.Series(iris.target, name='target')\nrf = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, n_jobs=-1)\nrfecv = RFECV(estimator=rf, step=1, cv=10, scoring='ROC', verbose=2)\nselector=rfecv.fit(x, y)\n\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'iris' is not defined""]";['NameError']
806;806;806;806;12.0;0;24147278;;1;97;<python><python-2.7><pandas><dataframe>;How do I create test and train samples from one dataframe with pandas?;81694.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;4;"[""name 'pd' is not defined"", ""No module named 'sklearn'"", ""No module named 'sklearn'"", ""name 'df' is not defined""]";['NameError', 'ImportError', 'ImportError', 'NameError'];0;4;"[""name 'np' is not defined"", ""No module named 'sklearn'"", ""No module named 'sklearn'"", ""name 'df' is not defined""]";['NameError', 'ImportError', 'ImportError', 'NameError'];0;4;"[""name 'np' is not defined"", ""No module named 'sklearn'"", ""No module named 'sklearn'"", 'a must be greater than 0']";['NameError', 'ImportError', 'ImportError', 'ValueError']
807;807;807;807;5.0;0;24193174;;1;26;<python><matplotlib><pandas>;Reset color cycle in Matplotlib;10416.0;['fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\nfor c in with_transaction_frame.columns:\n    ax.plot(with_transaction_frame[c], label=c, alpha=1, linewidth=1)\n\n****SOME MAGIC GOES HERE TO RESET THE COLOR CYCLE\n\nfor c in no_transaction_frame.columns:\n    ax.plot(no_transaction_frame[c], label=c, alpha=0.25, linewidth=5)\n\nax.legend()\n'];['fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\nfor c in with_transaction_frame.columns:\n    ax.plot(with_transaction_frame[c], label=c, alpha=1, linewidth=1)\n\n****SOME MAGIC GOES HERE TO RESET THE COLOR CYCLE\n\nfor c in no_transaction_frame.columns:\n    ax.plot(no_transaction_frame[c], label=c, alpha=0.25, linewidth=5)\n\nax.legend()\n'];['alpha=1', 'linewidth=1', 'alpha=0.25', 'linewidth=5', 'fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\nfor c in with_transaction_frame.columns:\n    ax.plot(with_transaction_frame[c], label=c, alpha=1, linewidth=1)\n\n****SOME MAGIC GOES HERE TO RESET THE COLOR CYCLE\n\nfor c in no_transaction_frame.columns:\n    ax.plot(no_transaction_frame[c], label=c, alpha=0.25, linewidth=5)\n\nax.legend()\n'];['fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\n\n\n\nax.legend()\n'];['fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\n\n\n\nax.legend()\n'];False;['import pandas as pd\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\n\n\n\nax.legend()\n'];False;0;2;"[""No module named 'matplotlib'"", ""No module named 'matplotlib'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'matplotlib'"", ""No module named 'matplotlib'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'matplotlib'"", ""No module named 'matplotlib'""]";['ImportError', 'ImportError']
808;808;808;808;2.0;14;24195432;;1;11;<python><pandas><regression><statsmodels>;Fixed effect in Pandas or Statsmodels;5937.0;[''];[];['plm', 'pd.plm()'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
809;809;809;809;3.0;2;24203106;;1;11;<python><datetime><pandas><calendar><group-by>;Pandas: Group by calendar-week, then plot grouped barplots for the real datetime;12490.0;"[""codes = list('ABCDEFGH'); \ndates = pd.Series(pd.date_range('2013-11-01', '2014-01-31')); \ndates = dates.append(dates)\ndates.sort()\ndf = pd.DataFrame({'amount': np.random.randint(1, 10, dates.size), 'col1': np.random.choice(codes, dates.size), 'col2': np.random.choice(codes, dates.size), 'date': dates})\nIn [55]: df\nOut[55]:\n    amount col1 col2       date\n0        1    D    E 2013-11-01\n0        5    E    B 2013-11-01\n1        5    G    A 2013-11-02\n1        7    D    H 2013-11-02\n2        5    E    G 2013-11-03\n2        4    H    G 2013-11-03\n3        7    A    F 2013-11-04\n3        3    A    A 2013-11-04\n4        1    E    G 2013-11-05\n4        7    D    C 2013-11-05\n5        5    C    A 2013-11-06\n5        7    H    F 2013-11-06\n6        1    G    B 2013-11-07\n6        8    D    A 2013-11-07\n7        1    B    H 2013-11-08\n7        8    F    H 2013-11-08\n8        3    A    E 2013-11-09\n8        1    H    D 2013-11-09\n9        3    B    D 2013-11-10\n9        1    H    G 2013-11-10\n10       6    E    E 2013-11-11\n10       6    F    E 2013-11-11\n11       2    G    B 2013-11-12\n11       5    H    H 2013-11-12\n12       5    F    G 2013-11-13\n12       5    G    B 2013-11-13\n13       8    H    B 2013-11-14\n13       6    G    F 2013-11-14\n14       9    F    C 2013-11-15\n14       4    H    A 2013-11-15\n..     ...  ...  ...        ...\n77       9    A    B 2014-01-17\n77       7    E    B 2014-01-17\n78       4    F    E 2014-01-18\n78       6    B    E 2014-01-18\n79       6    A    H 2014-01-19\n79       3    G    D 2014-01-19\n80       7    E    E 2014-01-20\n80       6    G    C 2014-01-20\n81       9    H    G 2014-01-21\n81       9    C    B 2014-01-21\n82       2    D    D 2014-01-22\n82       7    D    A 2014-01-22\n83       6    G    B 2014-01-23\n83       1    A    G 2014-01-23\n84       9    B    D 2014-01-24\n84       7    G    D 2014-01-24\n85       7    A    F 2014-01-25\n85       9    B    H 2014-01-25\n86       9    C    D 2014-01-26\n86       5    E    B 2014-01-26\n87       3    C    H 2014-01-27\n87       7    F    D 2014-01-27\n88       3    D    G 2014-01-28\n88       4    A    D 2014-01-28\n89       2    F    A 2014-01-29\n89       8    D    A 2014-01-29\n90       1    A    G 2014-01-30\n90       6    C    A 2014-01-30\n91       6    H    C 2014-01-31\n91       2    G    F 2014-01-31\n\n[184 rows x 4 columns]\nkw = lambda x: x.isocalendar()[1]\ngrouped = df.groupby([df['date'].map(kw), 'col1'], sort=False).agg({'amount': 'sum'})\nIn [58]: grouped\nOut[58]:\n           amount\ndate col1\n44   D          8\n     E         10\n     G          5\n     H          4\n45   D         15\n     E          1\n     G          1\n     H          9\n     A         13\n     C          5\n     B          4\n     F          8\n46   E          7\n     G         13\n     H         17\n     B          9\n     F         23\n47   G         14\n     H          4\n     A         40\n     C          7\n     B         16\n     F         13\n48   D          7\n     E         16\n     G          9\n     H          2\n     A          7\n     C          7\n     B          2\n...           ...\n1    H         14\n     A         14\n     B         15\n     F         19\n2    D         13\n     H         13\n     A         13\n     B         10\n     F         32\n3    D          8\n     E         18\n     G          3\n     H          6\n     A         30\n     C          9\n     B          6\n     F          5\n4    D          9\n     E         12\n     G         19\n     H          9\n     A          8\n     C         18\n     B         18\n5    D         11\n     G          2\n     H          6\n     A          5\n     C          9\n     F          9\n\n[87 rows x 1 columns]\nA = grouped.reset_index().pivot(index='date', columns='col1', values='amount').fillna(0)\ncol1   A   B   C   D   E   F   G   H\ndate\n1      4  31   0   0   0  18  13   8\n2      0  12  13  22   1  17   0   8\n3      3  10   4  13  12   8   7   6\n4     17   0  10   7   0  25   7   4\n5      7   0   7   9   8   6   0   7\n44     0   0   2  11   7   0   0   2\n45     9   3   2  14   0  16  21   2\n46     0  14   7   2  17  13  11   8\n47     5  13   0  15  19   7   5  10\n48    15   8  12   2  20   4   7   6\n49    20   0   0  18  22  17  11   0\n50     7  11   8   6   5   6  13  10\n51     8  26   0   0   5   5  16   9\n52     8  13   7   5   4  10   0  11\nA. plot(kind='bar')\n""]";"[""codes = list('ABCDEFGH'); \ndates = pd.Series(pd.date_range('2013-11-01', '2014-01-31')); \ndates = dates.append(dates)\ndates.sort()\ndf = pd.DataFrame({'amount': np.random.randint(1, 10, dates.size), 'col1': np.random.choice(codes, dates.size), 'col2': np.random.choice(codes, dates.size), 'date': dates})\n"", 'In [55]: df\nOut[55]:\n    amount col1 col2       date\n0        1    D    E 2013-11-01\n0        5    E    B 2013-11-01\n1        5    G    A 2013-11-02\n1        7    D    H 2013-11-02\n2        5    E    G 2013-11-03\n2        4    H    G 2013-11-03\n3        7    A    F 2013-11-04\n3        3    A    A 2013-11-04\n4        1    E    G 2013-11-05\n4        7    D    C 2013-11-05\n5        5    C    A 2013-11-06\n5        7    H    F 2013-11-06\n6        1    G    B 2013-11-07\n6        8    D    A 2013-11-07\n7        1    B    H 2013-11-08\n7        8    F    H 2013-11-08\n8        3    A    E 2013-11-09\n8        1    H    D 2013-11-09\n9        3    B    D 2013-11-10\n9        1    H    G 2013-11-10\n10       6    E    E 2013-11-11\n10       6    F    E 2013-11-11\n11       2    G    B 2013-11-12\n11       5    H    H 2013-11-12\n12       5    F    G 2013-11-13\n12       5    G    B 2013-11-13\n13       8    H    B 2013-11-14\n13       6    G    F 2013-11-14\n14       9    F    C 2013-11-15\n14       4    H    A 2013-11-15\n..     ...  ...  ...        ...\n77       9    A    B 2014-01-17\n77       7    E    B 2014-01-17\n78       4    F    E 2014-01-18\n78       6    B    E 2014-01-18\n79       6    A    H 2014-01-19\n79       3    G    D 2014-01-19\n80       7    E    E 2014-01-20\n80       6    G    C 2014-01-20\n81       9    H    G 2014-01-21\n81       9    C    B 2014-01-21\n82       2    D    D 2014-01-22\n82       7    D    A 2014-01-22\n83       6    G    B 2014-01-23\n83       1    A    G 2014-01-23\n84       9    B    D 2014-01-24\n84       7    G    D 2014-01-24\n85       7    A    F 2014-01-25\n85       9    B    H 2014-01-25\n86       9    C    D 2014-01-26\n86       5    E    B 2014-01-26\n87       3    C    H 2014-01-27\n87       7    F    D 2014-01-27\n88       3    D    G 2014-01-28\n88       4    A    D 2014-01-28\n89       2    F    A 2014-01-29\n89       8    D    A 2014-01-29\n90       1    A    G 2014-01-30\n90       6    C    A 2014-01-30\n91       6    H    C 2014-01-31\n91       2    G    F 2014-01-31\n\n[184 rows x 4 columns]\n', ""kw = lambda x: x.isocalendar()[1]\ngrouped = df.groupby([df['date'].map(kw), 'col1'], sort=False).agg({'amount': 'sum'})\n"", 'In [58]: grouped\nOut[58]:\n           amount\ndate col1\n44   D          8\n     E         10\n     G          5\n     H          4\n45   D         15\n     E          1\n     G          1\n     H          9\n     A         13\n     C          5\n     B          4\n     F          8\n46   E          7\n     G         13\n     H         17\n     B          9\n     F         23\n47   G         14\n     H          4\n     A         40\n     C          7\n     B         16\n     F         13\n48   D          7\n     E         16\n     G          9\n     H          2\n     A          7\n     C          7\n     B          2\n...           ...\n1    H         14\n     A         14\n     B         15\n     F         19\n2    D         13\n     H         13\n     A         13\n     B         10\n     F         32\n3    D          8\n     E         18\n     G          3\n     H          6\n     A         30\n     C          9\n     B          6\n     F          5\n4    D          9\n     E         12\n     G         19\n     H          9\n     A          8\n     C         18\n     B         18\n5    D         11\n     G          2\n     H          6\n     A          5\n     C          9\n     F          9\n\n[87 rows x 1 columns]\n', ""A = grouped.reset_index().pivot(index='date', columns='col1', values='amount').fillna(0)\n"", 'col1   A   B   C   D   E   F   G   H\ndate\n1      4  31   0   0   0  18  13   8\n2      0  12  13  22   1  17   0   8\n3      3  10   4  13  12   8   7   6\n4     17   0  10   7   0  25   7   4\n5      7   0   7   9   8   6   0   7\n44     0   0   2  11   7   0   0   2\n45     9   3   2  14   0  16  21   2\n46     0  14   7   2  17  13  11   8\n47     5  13   0  15  19   7   5  10\n48    15   8  12   2  20   4   7   6\n49    20   0   0  18  22  17  11   0\n50     7  11   8   6   5   6  13  10\n51     8  26   0   0   5   5  16   9\n52     8  13   7   5   4  10   0  11\n', ""A. plot(kind='bar')\n""]";"[""codes = list('ABCDEFGH'); \ndates = pd.Series(pd.date_range('2013-11-01', '2014-01-31')); \ndates = dates.append(dates)\ndates.sort()\ndf = pd.DataFrame({'amount': np.random.randint(1, 10, dates.size), 'col1': np.random.choice(codes, dates.size), 'col2': np.random.choice(codes, dates.size), 'date': dates})\n"", 'In [55]: df\nOut[55]:\n    amount col1 col2       date\n0        1    D    E 2013-11-01\n0        5    E    B 2013-11-01\n1        5    G    A 2013-11-02\n1        7    D    H 2013-11-02\n2        5    E    G 2013-11-03\n2        4    H    G 2013-11-03\n3        7    A    F 2013-11-04\n3        3    A    A 2013-11-04\n4        1    E    G 2013-11-05\n4        7    D    C 2013-11-05\n5        5    C    A 2013-11-06\n5        7    H    F 2013-11-06\n6        1    G    B 2013-11-07\n6        8    D    A 2013-11-07\n7        1    B    H 2013-11-08\n7        8    F    H 2013-11-08\n8        3    A    E 2013-11-09\n8        1    H    D 2013-11-09\n9        3    B    D 2013-11-10\n9        1    H    G 2013-11-10\n10       6    E    E 2013-11-11\n10       6    F    E 2013-11-11\n11       2    G    B 2013-11-12\n11       5    H    H 2013-11-12\n12       5    F    G 2013-11-13\n12       5    G    B 2013-11-13\n13       8    H    B 2013-11-14\n13       6    G    F 2013-11-14\n14       9    F    C 2013-11-15\n14       4    H    A 2013-11-15\n..     ...  ...  ...        ...\n77       9    A    B 2014-01-17\n77       7    E    B 2014-01-17\n78       4    F    E 2014-01-18\n78       6    B    E 2014-01-18\n79       6    A    H 2014-01-19\n79       3    G    D 2014-01-19\n80       7    E    E 2014-01-20\n80       6    G    C 2014-01-20\n81       9    H    G 2014-01-21\n81       9    C    B 2014-01-21\n82       2    D    D 2014-01-22\n82       7    D    A 2014-01-22\n83       6    G    B 2014-01-23\n83       1    A    G 2014-01-23\n84       9    B    D 2014-01-24\n84       7    G    D 2014-01-24\n85       7    A    F 2014-01-25\n85       9    B    H 2014-01-25\n86       9    C    D 2014-01-26\n86       5    E    B 2014-01-26\n87       3    C    H 2014-01-27\n87       7    F    D 2014-01-27\n88       3    D    G 2014-01-28\n88       4    A    D 2014-01-28\n89       2    F    A 2014-01-29\n89       8    D    A 2014-01-29\n90       1    A    G 2014-01-30\n90       6    C    A 2014-01-30\n91       6    H    C 2014-01-31\n91       2    G    F 2014-01-31\n\n[184 rows x 4 columns]\n', 'col1', ""kw = lambda x: x.isocalendar()[1]\ngrouped = df.groupby([df['date'].map(kw), 'col1'], sort=False).agg({'amount': 'sum'})\n"", 'In [58]: grouped\nOut[58]:\n           amount\ndate col1\n44   D          8\n     E         10\n     G          5\n     H          4\n45   D         15\n     E          1\n     G          1\n     H          9\n     A         13\n     C          5\n     B          4\n     F          8\n46   E          7\n     G         13\n     H         17\n     B          9\n     F         23\n47   G         14\n     H          4\n     A         40\n     C          7\n     B         16\n     F         13\n48   D          7\n     E         16\n     G          9\n     H          2\n     A          7\n     C          7\n     B          2\n...           ...\n1    H         14\n     A         14\n     B         15\n     F         19\n2    D         13\n     H         13\n     A         13\n     B         10\n     F         32\n3    D          8\n     E         18\n     G          3\n     H          6\n     A         30\n     C          9\n     B          6\n     F          5\n4    D          9\n     E         12\n     G         19\n     H          9\n     A          8\n     C         18\n     B         18\n5    D         11\n     G          2\n     H          6\n     A          5\n     C          9\n     F          9\n\n[87 rows x 1 columns]\n', 'col1', 'pivot', 'reset_index', 'NaN', ""A = grouped.reset_index().pivot(index='date', columns='col1', values='amount').fillna(0)\n"", 'col1   A   B   C   D   E   F   G   H\ndate\n1      4  31   0   0   0  18  13   8\n2      0  12  13  22   1  17   0   8\n3      3  10   4  13  12   8   7   6\n4     17   0  10   7   0  25   7   4\n5      7   0   7   9   8   6   0   7\n44     0   0   2  11   7   0   0   2\n45     9   3   2  14   0  16  21   2\n46     0  14   7   2  17  13  11   8\n47     5  13   0  15  19   7   5  10\n48    15   8  12   2  20   4   7   6\n49    20   0   0  18  22  17  11   0\n50     7  11   8   6   5   6  13  10\n51     8  26   0   0   5   5  16   9\n52     8  13   7   5   4  10   0  11\n', ""A. plot(kind='bar')\n""]";['df\ngrouped\n'];['df\ngrouped\n'];False;['import pandas as pd\ndf = pd.DataFrame()\ndf\ngrouped\n'];True;0;1;"[""name 'grouped' is not defined""]";['NameError'];0;1;"[""name 'grouped' is not defined""]";['NameError'];0;1;"[""name 'grouped' is not defined""]";['NameError']
810;810;810;810;1.0;0;24216425;;1;28;<python><pandas>;Adding a new pandas column with mapped value from a dictionary;15904.0;"['import pandas as pd\nequiv = {7001:1, 8001:2, 9001:3}\ndf = pd.DataFrame( {""A"": [7001, 8001, 9001]} )\ndf[""B""] = equiv(df[""A""])\nprint(df)\n      A   B\n0  7001   1\n1  8001   2\n2  9001   3\ndf[""B""] = df[""A""].map(lambda x:equiv[x])\n']";"['import pandas as pd\nequiv = {7001:1, 8001:2, 9001:3}\ndf = pd.DataFrame( {""A"": [7001, 8001, 9001]} )\ndf[""B""] = equiv(df[""A""])\nprint(df)\n', '      A   B\n0  7001   1\n1  8001   2\n2  9001   3\n', 'df[""B""] = df[""A""].map(lambda x:equiv[x])\n']";"['import pandas as pd\nequiv = {7001:1, 8001:2, 9001:3}\ndf = pd.DataFrame( {""A"": [7001, 8001, 9001]} )\ndf[""B""] = equiv(df[""A""])\nprint(df)\n', '      A   B\n0  7001   1\n1  8001   2\n2  9001   3\n', 'df[""B""] = df[""A""].map(lambda x:equiv[x])\n']";"['import pandas as pd\nequiv = {7001:1, 8001:2, 9001:3}\ndf = pd.DataFrame( {""A"": [7001, 8001, 9001]} )\ndf[""B""] = equiv(df[""A""])\nprint(df)\ndf[""B""] = df[""A""].map(lambda x:equiv[x])\n']";"['import pandas as pd\nequiv = {7001:1, 8001:2, 9001:3}\ndf = pd.DataFrame( {""A"": [7001, 8001, 9001]} )\ndf[""B""] = equiv(df[""A""])\nprint(df)\ndf[""B""] = df[""A""].map(lambda x:equiv[x])\n']";False;"['import pandas as pd\nimport pandas as pd\nequiv = {7001:1, 8001:2, 9001:3}\ndf = pd.DataFrame( {""A"": [7001, 8001, 9001]} )\ndf[""B""] = equiv(df[""A""])\nprint(df)\ndf[""B""] = df[""A""].map(lambda x:equiv[x])\n']";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
811;811;811;811;4.0;2;24251219;;1;81;<python><parsing><numpy><pandas><dataframe>;Pandas read_csv low_memory and dtype options;53257.0;"[""df = pd.read_csv('somefile.csv')\n""]";"[""df = pd.read_csv('somefile.csv')\n""]";"[""df = pd.read_csv('somefile.csv')\n"", 'dtype', 'low_memory', 'False']";"[""df = pd.read_csv('somefile.csv')\n""]";"[""import pandas as pd\ndf = pd.read_csv('somefile.csv')\n""]";True;"[""import pandas as pd\ndf = pd.read_csv('somefile.csv')\n""]";False;0;3;"[""name 'pd' is not defined"", ""No module named 'StringIO'"", ""name 'pd' is not defined""]";['NameError', 'ImportError', 'NameError'];0;3;"[""name 'p_file' is not defined"", ""No module named 'StringIO'"", ""File b'somefile.csv' does not exist""]";['NameError', 'ImportError', 'FileNotFoundError'];0;3;"[""name 'p_file' is not defined"", ""No module named 'StringIO'"", ""File b'somefile.csv' does not exist""]";['NameError', 'ImportError', 'FileNotFoundError']
812;812;812;812;1.0;3;24273130;;1;12;<python><pandas>;Get first element of Series without have information on index;17707.0;"[""    import pandas as pd\n    key='MCS096'\n    SUBJECTS=pd.DataFrame({'ID':Series([146],index=[145]),\\\n                   'study':Series(['MCS'],index=[145]),\\\n                   'center':Series(['Mag'],index=[145]),\\\n                   'initials':Series(['MCS096'],index=[145])\n                   })\n    print (SUBJECTS[SUBJECTS.initials==key]['ID'])\n    145    146\n    Name: ID, dtype: int64\n""]";"[""    import pandas as pd\n    key='MCS096'\n    SUBJECTS=pd.DataFrame({'ID':Series([146],index=[145]),\\\n                   'study':Series(['MCS'],index=[145]),\\\n                   'center':Series(['Mag'],index=[145]),\\\n                   'initials':Series(['MCS096'],index=[145])\n                   })\n"", ""    print (SUBJECTS[SUBJECTS.initials==key]['ID'])\n    145    146\n    Name: ID, dtype: int64\n""]";"[""    import pandas as pd\n    key='MCS096'\n    SUBJECTS=pd.DataFrame({'ID':Series([146],index=[145]),\\\n                   'study':Series(['MCS'],index=[145]),\\\n                   'center':Series(['Mag'],index=[145]),\\\n                   'initials':Series(['MCS096'],index=[145])\n                   })\n"", ""    print (SUBJECTS[SUBJECTS.initials==key]['ID'])\n    145    146\n    Name: ID, dtype: int64\n""]";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
813;813;813;813;3.0;2;24284342;;1;27;<python><pandas>;Insert a row to pandas dataframe;87343.0;"['s1 = pd.Series([5, 6, 7])\ns2 = pd.Series([7, 8, 9])\n\ndf = pd.DataFrame([list(s1), list(s2)],  columns =  [""A"", ""B"", ""C""])\n\n   A  B  C\n0  5  6  7\n1  7  8  9\n\n[2 rows x 3 columns]\n   A  B  C\n0  2  3  4\n1  5  6  7\n2  7  8  9\n']";"['s1 = pd.Series([5, 6, 7])\ns2 = pd.Series([7, 8, 9])\n\ndf = pd.DataFrame([list(s1), list(s2)],  columns =  [""A"", ""B"", ""C""])\n\n   A  B  C\n0  5  6  7\n1  7  8  9\n\n[2 rows x 3 columns]\n', '   A  B  C\n0  2  3  4\n1  5  6  7\n2  7  8  9\n']";"['s1 = pd.Series([5, 6, 7])\ns2 = pd.Series([7, 8, 9])\n\ndf = pd.DataFrame([list(s1), list(s2)],  columns =  [""A"", ""B"", ""C""])\n\n   A  B  C\n0  5  6  7\n1  7  8  9\n\n[2 rows x 3 columns]\n', '   A  B  C\n0  2  3  4\n1  5  6  7\n2  7  8  9\n']";"['s1 = pd.Series([5, 6, 7])\ns2 = pd.Series([7, 8, 9])\n\ndf = pd.DataFrame([list(s1), list(s2)],  columns =  [""A"", ""B"", ""C""])\n\n\n']";"['import pandas as pd\ns1 = pd.Series([5, 6, 7])\ns2 = pd.Series([7, 8, 9])\n\ndf = pd.DataFrame([list(s1), list(s2)],  columns =  [""A"", ""B"", ""C""])\n\n\n']";True;"['import pandas as pd\ns1 = pd.Series([5, 6, 7])\ns2 = pd.Series([7, 8, 9])\n\ndf = pd.DataFrame([list(s1), list(s2)],  columns =  [""A"", ""B"", ""C""])\n\n\n']";False;3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess']
814;814;814;814;2.0;0;24386638;;1;11;<pandas>;Pandas sum two columns, skipping NaN;7005.0;"[""In [42]: frame = pd.DataFrame({'a': [1, 2, np.nan], 'b': [3, np.nan, 4]})\n\nIn [44]: frame['c'] = frame['a'] + frame['b']\n\nIn [45]: frame\nOut[45]: \n    a   b   c\n0   1   3   4\n1   2 NaN NaN\n2 NaN   4 NaN\n""]";"[""In [42]: frame = pd.DataFrame({'a': [1, 2, np.nan], 'b': [3, np.nan, 4]})\n\nIn [44]: frame['c'] = frame['a'] + frame['b']\n\nIn [45]: frame\nOut[45]: \n    a   b   c\n0   1   3   4\n1   2 NaN NaN\n2 NaN   4 NaN\n""]";"[""In [42]: frame = pd.DataFrame({'a': [1, 2, np.nan], 'b': [3, np.nan, 4]})\n\nIn [44]: frame['c'] = frame['a'] + frame['b']\n\nIn [45]: frame\nOut[45]: \n    a   b   c\n0   1   3   4\n1   2 NaN NaN\n2 NaN   4 NaN\n""]";"[""frame = pd.DataFrame({'a': [1, 2, np.nan], 'b': [3, np.nan, 4]})\n\n\n""]";"[""import pandas as pd\nframe = pd.DataFrame({'a': [1, 2, np.nan], 'b': [3, np.nan, 4]})\n\n\n""]";True;"[""import pandas as pd\nframe = pd.DataFrame({'a': [1, 2, np.nan], 'b': [3, np.nan, 4]})\n\n\n""]";False;1;2;"[""name 'frame' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'frame' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'frame' is not defined"", 'Sucess']";['NameError', 'Sucess']
815;815;815;815;2.0;0;24408557;;1;19;<python><sql><pandas>;Pandas read_sql with parameters;28919.0;"['df = psql.read_sql((\'select ""Timestamp"",""Value"" from ""MyTable"" \'\n                     \'where ""Timestamp"" BETWEEN %s AND %s\'),\n                   db,params=[datetime(2014,6,24,16,0),datetime(2014,6,24,17,0)],\n                   index_col=[\'Timestamp\'])\ndf = psql.read_sql((\'select ""Timestamp"",""Value"" from ""MyTable"" \'\n                     \'where ""Timestamp"" BETWEEN :dstart AND :dfinish\'),\n                   db,params={""dstart"":datetime(2014,6,24,16,0),""dfinish"":datetime(2014,6,24,17,0)},\n                   index_col=[\'Timestamp\'])\n']";"['df = psql.read_sql((\'select ""Timestamp"",""Value"" from ""MyTable"" \'\n                     \'where ""Timestamp"" BETWEEN %s AND %s\'),\n                   db,params=[datetime(2014,6,24,16,0),datetime(2014,6,24,17,0)],\n                   index_col=[\'Timestamp\'])\n', 'df = psql.read_sql((\'select ""Timestamp"",""Value"" from ""MyTable"" \'\n                     \'where ""Timestamp"" BETWEEN :dstart AND :dfinish\'),\n                   db,params={""dstart"":datetime(2014,6,24,16,0),""dfinish"":datetime(2014,6,24,17,0)},\n                   index_col=[\'Timestamp\'])\n']";"['df = psql.read_sql((\'select ""Timestamp"",""Value"" from ""MyTable"" \'\n                     \'where ""Timestamp"" BETWEEN %s AND %s\'),\n                   db,params=[datetime(2014,6,24,16,0),datetime(2014,6,24,17,0)],\n                   index_col=[\'Timestamp\'])\n', 'df = psql.read_sql((\'select ""Timestamp"",""Value"" from ""MyTable"" \'\n                     \'where ""Timestamp"" BETWEEN :dstart AND :dfinish\'),\n                   db,params={""dstart"":datetime(2014,6,24,16,0),""dfinish"":datetime(2014,6,24,17,0)},\n                   index_col=[\'Timestamp\'])\n']";[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
816;816;816;816;2.0;0;24435788;;1;11;<python><pandas><dataframe><multi-index>;Using .loc with a MultiIndex in pandas?;8091.0;['         Char    Dwell  Flight  ND_Offset  Offset\nQGram                                                           \nat    0     a      100     120   0.000000       0  \n      1     t      180       0   0.108363       5  \n      2     a      100     120   0.000000       0 \n      3     t      180       0   0.108363       5 \n      4     a       20     180   0.000000       0  \n      5     t       80     120   0.108363       5\n      6     a       20     180   0.000000       0   \n      7     t       80     120   0.108363       5  \n      8     a       20     180   0.000000       0  \n      9     t       80     120   0.108363       5   \n      10    a      120     180   0.000000       0  \n'];['         Char    Dwell  Flight  ND_Offset  Offset\nQGram                                                           \nat    0     a      100     120   0.000000       0  \n      1     t      180       0   0.108363       5  \n      2     a      100     120   0.000000       0 \n      3     t      180       0   0.108363       5 \n      4     a       20     180   0.000000       0  \n      5     t       80     120   0.108363       5\n      6     a       20     180   0.000000       0   \n      7     t       80     120   0.108363       5  \n      8     a       20     180   0.000000       0  \n      9     t       80     120   0.108363       5   \n      10    a      120     180   0.000000       0  \n'];"[""('at', 1)"", ""('at', 3)"", ""('at', 5)"", ""data.loc[['at',[1,3,5]], 'Dwell']"", ""data.loc[[1,3,5], 'Dwell']"", '         Char    Dwell  Flight  ND_Offset  Offset\nQGram                                                           \nat    0     a      100     120   0.000000       0  \n      1     t      180       0   0.108363       5  \n      2     a      100     120   0.000000       0 \n      3     t      180       0   0.108363       5 \n      4     a       20     180   0.000000       0  \n      5     t       80     120   0.108363       5\n      6     a       20     180   0.000000       0   \n      7     t       80     120   0.108363       5  \n      8     a       20     180   0.000000       0  \n      9     t       80     120   0.108363       5   \n      10    a      120     180   0.000000       0  \n']";['QGram                                                           \n'];['QGram                                                           \n'];False;['import pandas as pd\nQGram                                                           \n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""unhashable type: 'list'""]";['TypeError']
817;817;817;817;9.0;2;24458645;;1;67;<python><pandas><scikit-learn>;Label encoding across multiple columns in scikit-learn;27993.0;"['import pandas\nfrom sklearn import preprocessing \n\ndf = pandas.DataFrame({\'pets\':[\'cat\', \'dog\', \'cat\', \'monkey\', \'dog\', \'dog\'], \'owner\':[\'Champ\', \'Ron\', \'Brick\', \'Champ\', \'Veronica\', \'Ron\'], \'location\':[\'San_Diego\', \'New_York\', \'New_York\', \'San_Diego\', \'San_Diego\', \'New_York\']})\nle = preprocessing.LabelEncoder()\n\nle.fit(df)\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/label.py"", line 103, in fit\n    y = column_or_1d(y, warn=True)\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py"", line 306, in column_or_1d\n    raise ValueError(""bad input shape {0}"".format(shape))\nValueError: bad input shape (6, 3)\n']";"['import pandas\nfrom sklearn import preprocessing \n\ndf = pandas.DataFrame({\'pets\':[\'cat\', \'dog\', \'cat\', \'monkey\', \'dog\', \'dog\'], \'owner\':[\'Champ\', \'Ron\', \'Brick\', \'Champ\', \'Veronica\', \'Ron\'], \'location\':[\'San_Diego\', \'New_York\', \'New_York\', \'San_Diego\', \'San_Diego\', \'New_York\']})\nle = preprocessing.LabelEncoder()\n\nle.fit(df)\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/label.py"", line 103, in fit\n    y = column_or_1d(y, warn=True)\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py"", line 306, in column_or_1d\n    raise ValueError(""bad input shape {0}"".format(shape))\nValueError: bad input shape (6, 3)\n']";"['LabelEncoder', 'DataFrame', 'LabelEncoder', 'LabelEncoder', 'DataFrame', 'LabelEncoder', 'import pandas\nfrom sklearn import preprocessing \n\ndf = pandas.DataFrame({\'pets\':[\'cat\', \'dog\', \'cat\', \'monkey\', \'dog\', \'dog\'], \'owner\':[\'Champ\', \'Ron\', \'Brick\', \'Champ\', \'Veronica\', \'Ron\'], \'location\':[\'San_Diego\', \'New_York\', \'New_York\', \'San_Diego\', \'San_Diego\', \'New_York\']})\nle = preprocessing.LabelEncoder()\n\nle.fit(df)\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/label.py"", line 103, in fit\n    y = column_or_1d(y, warn=True)\n  File ""/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py"", line 306, in column_or_1d\n    raise ValueError(""bad input shape {0}"".format(shape))\nValueError: bad input shape (6, 3)\n']";"[""import pandas\nfrom sklearn import preprocessing \n\ndf = pandas.DataFrame({'pets':['cat', 'dog', 'cat', 'monkey', 'dog', 'dog'], 'owner':['Champ', 'Ron', 'Brick', 'Champ', 'Veronica', 'Ron'], 'location':['San_Diego', 'New_York', 'New_York', 'San_Diego', 'San_Diego', 'New_York']})\nle = preprocessing.LabelEncoder()\n\nle.fit(df)\n""]";"[""import pandas\nfrom sklearn import preprocessing \n\ndf = pandas.DataFrame({'pets':['cat', 'dog', 'cat', 'monkey', 'dog', 'dog'], 'owner':['Champ', 'Ron', 'Brick', 'Champ', 'Veronica', 'Ron'], 'location':['San_Diego', 'New_York', 'New_York', 'San_Diego', 'San_Diego', 'New_York']})\nle = preprocessing.LabelEncoder()\n\nle.fit(df)\n""]";False;"[""import pandas as pd\nimport pandas\nfrom sklearn import preprocessing \n\ndf = pandas.DataFrame({'pets':['cat', 'dog', 'cat', 'monkey', 'dog', 'dog'], 'owner':['Champ', 'Ron', 'Brick', 'Champ', 'Veronica', 'Ron'], 'location':['San_Diego', 'New_York', 'New_York', 'San_Diego', 'San_Diego', 'New_York']})\nle = preprocessing.LabelEncoder()\n\nle.fit(df)\n""]";False;0;2;"[""No module named 'sklearn'"", ""name 'df' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'sklearn'"", ""name 'df' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'sklearn'"", ""name 'LabelEncoder' is not defined""]";['ImportError', 'NameError']
818;818;818;818;4.0;0;24475094;;1;14;<python><numpy><pandas>;Set values on the diagonal of pandas.DataFrame;4336.0;['import numpy\nimport pandas\n\ndf = pandas.DataFrame(numpy.random.rand(5,5))\ndf\n\nOut[6]:\n     0           1           2           3               4\n0    0.536596    0.674319    0.032815    0.908086    0.215334\n1    0.735022    0.954506    0.889162    0.711610    0.415118\n2    0.119985    0.979056    0.901891    0.687829    0.947549\n3    0.186921    0.899178    0.296294    0.521104    0.638924\n4    0.354053    0.060022    0.275224    0.635054    0.075738\n5 rows × 5 columns\nfor i in range(len(df.index)):\n    for j in range(len(df.columns)):\n        if i==j:\n            df.loc[i,j] = 0\ndf\nOut[9]:\n     0           1           2           3           4\n0    0.000000    0.674319    0.032815    0.908086    0.215334\n1    0.735022    0.000000    0.889162    0.711610    0.415118\n2    0.119985    0.979056    0.000000    0.687829    0.947549\n3    0.186921    0.899178    0.296294    0.000000    0.638924\n4    0.354053    0.060022    0.275224    0.635054    0.000000\n5 rows × 5 columns\n'];['import numpy\nimport pandas\n\ndf = pandas.DataFrame(numpy.random.rand(5,5))\ndf\n\nOut[6]:\n     0           1           2           3               4\n0    0.536596    0.674319    0.032815    0.908086    0.215334\n1    0.735022    0.954506    0.889162    0.711610    0.415118\n2    0.119985    0.979056    0.901891    0.687829    0.947549\n3    0.186921    0.899178    0.296294    0.521104    0.638924\n4    0.354053    0.060022    0.275224    0.635054    0.075738\n5 rows × 5 columns\n', 'for i in range(len(df.index)):\n    for j in range(len(df.columns)):\n        if i==j:\n            df.loc[i,j] = 0\ndf\nOut[9]:\n     0           1           2           3           4\n0    0.000000    0.674319    0.032815    0.908086    0.215334\n1    0.735022    0.000000    0.889162    0.711610    0.415118\n2    0.119985    0.979056    0.000000    0.687829    0.947549\n3    0.186921    0.899178    0.296294    0.000000    0.638924\n4    0.354053    0.060022    0.275224    0.635054    0.000000\n5 rows × 5 columns\n'];['import numpy\nimport pandas\n\ndf = pandas.DataFrame(numpy.random.rand(5,5))\ndf\n\nOut[6]:\n     0           1           2           3               4\n0    0.536596    0.674319    0.032815    0.908086    0.215334\n1    0.735022    0.954506    0.889162    0.711610    0.415118\n2    0.119985    0.979056    0.901891    0.687829    0.947549\n3    0.186921    0.899178    0.296294    0.521104    0.638924\n4    0.354053    0.060022    0.275224    0.635054    0.075738\n5 rows × 5 columns\n', 'for i in range(len(df.index)):\n    for j in range(len(df.columns)):\n        if i==j:\n            df.loc[i,j] = 0\ndf\nOut[9]:\n     0           1           2           3           4\n0    0.000000    0.674319    0.032815    0.908086    0.215334\n1    0.735022    0.000000    0.889162    0.711610    0.415118\n2    0.119985    0.979056    0.000000    0.687829    0.947549\n3    0.186921    0.899178    0.296294    0.000000    0.638924\n4    0.354053    0.060022    0.275224    0.635054    0.000000\n5 rows × 5 columns\n'];['import numpy\nimport pandas\n\ndf = pandas.DataFrame(numpy.random.rand(5,5))\ndf\n\ndf\n'];['import numpy\nimport pandas\n\ndf = pandas.DataFrame(numpy.random.rand(5,5))\ndf\n\ndf\n'];False;['import pandas as pd\nimport numpy\nimport pandas\n\ndf = pandas.DataFrame(numpy.random.rand(5,5))\ndf\n\ndf\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
819;819;819;819;3.0;1;24495695;;1;30;<python><pandas>;Pandas: Get unique MultiIndex level values by label;18489.0;"[""df = pd.DataFrame({'co':['DE','DE','FR','FR'],\n                   'tp':['Lake','Forest','Lake','Forest'],\n                   'area':[10,20,30,40],\n                   'count':[7,5,2,3]})\ndf = df.set_index(['co','tp'])\n           area  count\nco tp\nDE Lake      10      7\n   Forest    20      5\nFR Lake      30      2\n   Forest    40      3\ndf.index.levels[0]  # returns ['DE', 'FR]\ndf.index.levels[1]  # returns ['Lake', 'Forest']\nlist(set(df.index.get_level_values('co')))  # returns ['DE', 'FR']\ndf.index.levels[df.index.names.index('co')]  # returns ['DE', 'FR']\n""]";"[""df = pd.DataFrame({'co':['DE','DE','FR','FR'],\n                   'tp':['Lake','Forest','Lake','Forest'],\n                   'area':[10,20,30,40],\n                   'count':[7,5,2,3]})\ndf = df.set_index(['co','tp'])\n"", '           area  count\nco tp\nDE Lake      10      7\n   Forest    20      5\nFR Lake      30      2\n   Forest    40      3\n', ""df.index.levels[0]  # returns ['DE', 'FR]\ndf.index.levels[1]  # returns ['Lake', 'Forest']\n"", ""list(set(df.index.get_level_values('co')))  # returns ['DE', 'FR']\ndf.index.levels[df.index.names.index('co')]  # returns ['DE', 'FR']\n""]";"[""df = pd.DataFrame({'co':['DE','DE','FR','FR'],\n                   'tp':['Lake','Forest','Lake','Forest'],\n                   'area':[10,20,30,40],\n                   'count':[7,5,2,3]})\ndf = df.set_index(['co','tp'])\n"", '           area  count\nco tp\nDE Lake      10      7\n   Forest    20      5\nFR Lake      30      2\n   Forest    40      3\n', ""df.index.levels[0]  # returns ['DE', 'FR]\ndf.index.levels[1]  # returns ['Lake', 'Forest']\n"", ""'co'"", ""'tp'"", ""list(set(df.index.get_level_values('co')))  # returns ['DE', 'FR']\ndf.index.levels[df.index.names.index('co')]  # returns ['DE', 'FR']\n""]";"[""df = df.set_index(['co','tp'])\ndf.index.levels[0]  # returns ['DE', 'FR]\ndf.index.levels[1]  # returns ['Lake', 'Forest']\nlist(set(df.index.get_level_values('co')))  # returns ['DE', 'FR']\ndf.index.levels[df.index.names.index('co')]  # returns ['DE', 'FR']\n""]";"[""df = df.set_index(['co','tp'])\ndf.index.levels[0]  # returns ['DE', 'FR]\ndf.index.levels[1]  # returns ['Lake', 'Forest']\nlist(set(df.index.get_level_values('co')))  # returns ['DE', 'FR']\ndf.index.levels[df.index.names.index('co')]  # returns ['DE', 'FR']\n""]";False;"[""import pandas as pd\ndf = df.set_index(['co','tp'])\ndf.index.levels[0]  # returns ['DE', 'FR]\ndf.index.levels[1]  # returns ['Lake', 'Forest']\nlist(set(df.index.get_level_values('co')))  # returns ['DE', 'FR']\ndf.index.levels[df.index.names.index('co')]  # returns ['DE', 'FR']\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'Level co must be same as name (None)'""]";['KeyError']
820;820;820;820;3.0;7;24524104;;1;11;<python><pandas>;Pandas 'describe' is not returning summary of all columns;13696.0;"[""df_test = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\ndf_test.dtypes\ndf_test.describe()\ndf_test['$a'] = df_test['$a'].astype(str)\ndf_test.describe()\ndf_test['$a'].describe()\ndf_test['$b'].describe()\ndef my_df_describe(df):\n    objects = []\n    numerics = []\n    for c in df:\n        if (df[c].dtype == object):\n            objects.append(c)\n        else:\n            numerics.append(c)\n\n    return df[numerics].describe(), df[objects].describe()\n""]";"[""df_test = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\ndf_test.dtypes\ndf_test.describe()\ndf_test['$a'] = df_test['$a'].astype(str)\ndf_test.describe()\ndf_test['$a'].describe()\ndf_test['$b'].describe()\n"", 'def my_df_describe(df):\n    objects = []\n    numerics = []\n    for c in df:\n        if (df[c].dtype == object):\n            objects.append(c)\n        else:\n            numerics.append(c)\n\n    return df[numerics].describe(), df[objects].describe()\n']";"[""df_test = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\ndf_test.dtypes\ndf_test.describe()\ndf_test['$a'] = df_test['$a'].astype(str)\ndf_test.describe()\ndf_test['$a'].describe()\ndf_test['$b'].describe()\n"", 'def my_df_describe(df):\n    objects = []\n    numerics = []\n    for c in df:\n        if (df[c].dtype == object):\n            objects.append(c)\n        else:\n            numerics.append(c)\n\n    return df[numerics].describe(), df[objects].describe()\n']";"[""df_test = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\ndf_test.dtypes\ndf_test.describe()\ndf_test['$a'] = df_test['$a'].astype(str)\ndf_test.describe()\ndf_test['$a'].describe()\ndf_test['$b'].describe()\n\n""]";"[""import pandas as pd\ndf_test = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\ndf_test.dtypes\ndf_test.describe()\ndf_test['$a'] = df_test['$a'].astype(str)\ndf_test.describe()\ndf_test['$a'].describe()\ndf_test['$b'].describe()\n\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\ndf_test = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\ndf_test.dtypes\ndf_test.describe()\ndf_test['$a'] = df_test['$a'].astype(str)\ndf_test.describe()\ndf_test['$a'].describe()\ndf_test['$b'].describe()\n\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
821;821;821;821;3.0;4;24574976;;1;18;<python><matplotlib><pandas><ipython><canopy>;"Save the ""Out[]"" table of a pandas dataframe as a figure";6231.0;"[""from matplotlib.backends.backend_pdf import PdfPages\n\npp = PdfPages('Output.pdf')\nfig = plt.figure() \nax = fig.add_subplot(1, 1, 1)\ndf.plot(how='table')\npp.savefig()\npp.close()\n""]";"[""from matplotlib.backends.backend_pdf import PdfPages\n\npp = PdfPages('Output.pdf')\nfig = plt.figure() \nax = fig.add_subplot(1, 1, 1)\ndf.plot(how='table')\npp.savefig()\npp.close()\n""]";"[""from matplotlib.backends.backend_pdf import PdfPages\n\npp = PdfPages('Output.pdf')\nfig = plt.figure() \nax = fig.add_subplot(1, 1, 1)\ndf.plot(how='table')\npp.savefig()\npp.close()\n""]";"[""from matplotlib.backends.backend_pdf import PdfPages\n\npp = PdfPages('Output.pdf')\nfig = plt.figure() \nax = fig.add_subplot(1, 1, 1)\ndf.plot(how='table')\npp.savefig()\npp.close()\n""]";"[""from matplotlib.backends.backend_pdf import PdfPages\n\npp = PdfPages('Output.pdf')\nfig = plt.figure() \nax = fig.add_subplot(1, 1, 1)\ndf.plot(how='table')\npp.savefig()\npp.close()\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nfrom matplotlib.backends.backend_pdf import PdfPages\n\npp = PdfPages('Output.pdf')\nfig = plt.figure() \nax = fig.add_subplot(1, 1, 1)\ndf.plot(how='table')\npp.savefig()\npp.close()\n""]";True;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
822;822;822;822;3.0;4;24644656;;1;32;<python><datetime><pandas><dataframe>;How to print dataframe without index;21964.0;['   User ID           Enter Time   Activity Number\n0      123  2014-07-08 00:09:00              1411\n1      123  2014-07-08 00:18:00               893\n2      123  2014-07-08 00:49:00              1041\nUser ID   Enter Time   Activity Number\n123         00:09:00              1411\n123         00:18:00               893\n123         00:49:00              1041\n'];['   User ID           Enter Time   Activity Number\n0      123  2014-07-08 00:09:00              1411\n1      123  2014-07-08 00:18:00               893\n2      123  2014-07-08 00:49:00              1041\n', 'User ID   Enter Time   Activity Number\n123         00:09:00              1411\n123         00:18:00               893\n123         00:49:00              1041\n'];['   User ID           Enter Time   Activity Number\n0      123  2014-07-08 00:09:00              1411\n1      123  2014-07-08 00:18:00               893\n2      123  2014-07-08 00:49:00              1041\n', 'User ID   Enter Time   Activity Number\n123         00:09:00              1411\n123         00:18:00               893\n123         00:49:00              1041\n'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
823;823;823;823;4.0;3;24645153;;1;25;<python><pandas><scikit-learn><dataframe>;pandas dataframe columns scaling with sklearn;19762.0;"[""import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\n\ndfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],'B':[103.02,107.26,110.35,114.23,114.68], 'C':['big','small','big','small','small']})\nmin_max_scaler = preprocessing.MinMaxScaler()\n\ndef scaleColumns(df, cols_to_scale):\n    for col in cols_to_scale:\n        df[col] = pd.DataFrame(min_max_scaler.fit_transform(pd.DataFrame(dfTest[col])),columns=[col])\n    return df\n\ndfTest\n\n    A   B   C\n0    14.00   103.02  big\n1    90.20   107.26  small\n2    90.95   110.35  big\n3    96.27   114.23  small\n4    91.21   114.68  small\n\nscaled_df = scaleColumns(dfTest,['A','B'])\nscaled_df\n\nA   B   C\n0    0.000000    0.000000    big\n1    0.926219    0.363636    small\n2    0.935335    0.628645    big\n3    1.000000    0.961407    small\n4    0.938495    1.000000    small\n""]";"[""import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\n\ndfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],'B':[103.02,107.26,110.35,114.23,114.68], 'C':['big','small','big','small','small']})\nmin_max_scaler = preprocessing.MinMaxScaler()\n\ndef scaleColumns(df, cols_to_scale):\n    for col in cols_to_scale:\n        df[col] = pd.DataFrame(min_max_scaler.fit_transform(pd.DataFrame(dfTest[col])),columns=[col])\n    return df\n\ndfTest\n\n    A   B   C\n0    14.00   103.02  big\n1    90.20   107.26  small\n2    90.95   110.35  big\n3    96.27   114.23  small\n4    91.21   114.68  small\n\nscaled_df = scaleColumns(dfTest,['A','B'])\nscaled_df\n\nA   B   C\n0    0.000000    0.000000    big\n1    0.926219    0.363636    small\n2    0.935335    0.628645    big\n3    1.000000    0.961407    small\n4    0.938495    1.000000    small\n""]";"[""import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\n\ndfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],'B':[103.02,107.26,110.35,114.23,114.68], 'C':['big','small','big','small','small']})\nmin_max_scaler = preprocessing.MinMaxScaler()\n\ndef scaleColumns(df, cols_to_scale):\n    for col in cols_to_scale:\n        df[col] = pd.DataFrame(min_max_scaler.fit_transform(pd.DataFrame(dfTest[col])),columns=[col])\n    return df\n\ndfTest\n\n    A   B   C\n0    14.00   103.02  big\n1    90.20   107.26  small\n2    90.95   110.35  big\n3    96.27   114.23  small\n4    91.21   114.68  small\n\nscaled_df = scaleColumns(dfTest,['A','B'])\nscaled_df\n\nA   B   C\n0    0.000000    0.000000    big\n1    0.926219    0.363636    small\n2    0.935335    0.628645    big\n3    1.000000    0.961407    small\n4    0.938495    1.000000    small\n"", ""bad_output = min_max_scaler.fit_transform(dfTest['A'])"", ""dfTest2 = dfTest.drop('C', axis = 1)\ngood_output = min_max_scaler.fit_transform(dfTest2)\ngood_output""]";"[""import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\n\ndfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],'B':[103.02,107.26,110.35,114.23,114.68], 'C':['big','small','big','small','small']})\nmin_max_scaler = preprocessing.MinMaxScaler()\n\n\ndfTest\n\n\nscaled_df = scaleColumns(dfTest,['A','B'])\nscaled_df\n\n""]";"[""import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\n\ndfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],'B':[103.02,107.26,110.35,114.23,114.68], 'C':['big','small','big','small','small']})\nmin_max_scaler = preprocessing.MinMaxScaler()\n\n\ndfTest\n\n\nscaled_df = scaleColumns(dfTest,['A','B'])\nscaled_df\n\n""]";False;"[""import pandas as pd\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\n\ndfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],'B':[103.02,107.26,110.35,114.23,114.68], 'C':['big','small','big','small','small']})\nmin_max_scaler = preprocessing.MinMaxScaler()\n\n\ndfTest\n\n\nscaled_df = scaleColumns(dfTest,['A','B'])\nscaled_df\n\n""]";False;1;2;"[""name 'dfTest' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'dfTest' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'dfTest' is not defined"", 'Sucess']";['NameError', 'Sucess']
824;824;824;824;2.0;0;24775648;;1;32;<python><pandas><boolean-logic><logical-operators><boolean-operations>;Element-wise logical OR in Pandas;17268.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
825;825;825;825;1.0;4;24786209;;1;13;<python><datetime><pandas><dataframe>;Dropping time from datetime <[M8] in Pandas;11942.0;['0    1998-08-26 04:00:00 \n'];['0    1998-08-26 04:00:00 \n'];['0    1998-08-26 04:00:00 \n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
826;826;826;826;1.0;0;24826368;;1;16;<python><pandas><statistics><multi-index>;Summing over a multiindex level in a pandas series;7387.0;"[""ind = [tuple(x) for x in ['ABC', 'ABc', 'AbC', 'Abc', 'aBC', 'aBc', 'abC', 'abc']]\nmi = pd.MultiIndex.from_tuples(ind)\ndata = pd.Series([264, 13, 29, 8, 152, 7, 15, 1], index=mi)\n\nA  B  C    264\n      c     13\n   b  C     29\n      c      8\na  B  C    152\n      c      7\n   b  C     15\n      c      1\nA  B    277\n   b     37\na  B    159\n   b     16\n""]";"[""ind = [tuple(x) for x in ['ABC', 'ABc', 'AbC', 'Abc', 'aBC', 'aBc', 'abC', 'abc']]\nmi = pd.MultiIndex.from_tuples(ind)\ndata = pd.Series([264, 13, 29, 8, 152, 7, 15, 1], index=mi)\n\nA  B  C    264\n      c     13\n   b  C     29\n      c      8\na  B  C    152\n      c      7\n   b  C     15\n      c      1\n"", 'A  B    277\n   b     37\na  B    159\n   b     16\n']";"[""ind = [tuple(x) for x in ['ABC', 'ABc', 'AbC', 'Abc', 'aBC', 'aBc', 'abC', 'abc']]\nmi = pd.MultiIndex.from_tuples(ind)\ndata = pd.Series([264, 13, 29, 8, 152, 7, 15, 1], index=mi)\n\nA  B  C    264\n      c     13\n   b  C     29\n      c      8\na  B  C    152\n      c      7\n   b  C     15\n      c      1\n"", 'A  B    277\n   b     37\na  B    159\n   b     16\n']";"[""ind = [tuple(x) for x in ['ABC', 'ABc', 'AbC', 'Abc', 'aBC', 'aBc', 'abC', 'abc']]\nmi = pd.MultiIndex.from_tuples(ind)\ndata = pd.Series([264, 13, 29, 8, 152, 7, 15, 1], index=mi)\n\n""]";"[""import pandas as pd\nind = [tuple(x) for x in ['ABC', 'ABc', 'AbC', 'Abc', 'aBC', 'aBc', 'abC', 'abc']]\nmi = pd.MultiIndex.from_tuples(ind)\ndata = pd.Series([264, 13, 29, 8, 152, 7, 15, 1], index=mi)\n\n""]";True;"[""import pandas as pd\nind = [tuple(x) for x in ['ABC', 'ABc', 'AbC', 'Abc', 'aBC', 'aBc', 'abC', 'abc']]\nmi = pd.MultiIndex.from_tuples(ind)\ndata = pd.Series([264, 13, 29, 8, 152, 7, 15, 1], index=mi)\n\n""]";False;0;1;"[""name 'data' is not defined""]";['NameError'];0;1;"[""name 'data' is not defined""]";['NameError'];0;1;['multiple levels only valid with MultiIndex'];['ValueError']
827;827;827;827;3.0;0;24870306;;1;71;<python><pandas><dataframe>;How to check if a column exists in Pandas;39441.0;"["">>> import pandas as pd\n>>> from random import randint\n>>> df = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],\n                       'B': [randint(1, 9)*10 for x in xrange(10)],\n                       'C': [randint(1, 9)*100 for x in xrange(10)]})\n>>> df\n   A   B    C\n0  3  40  100\n1  6  30  200\n2  7  70  800\n3  3  50  200\n4  7  50  400\n5  4  10  400\n6  3  70  500\n7  8  30  200\n8  3  40  800\n9  6  60  200\n""]";"["">>> import pandas as pd\n>>> from random import randint\n>>> df = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],\n                       'B': [randint(1, 9)*10 for x in xrange(10)],\n                       'C': [randint(1, 9)*100 for x in xrange(10)]})\n>>> df\n   A   B    C\n0  3  40  100\n1  6  30  200\n2  7  70  800\n3  3  50  200\n4  7  50  400\n5  4  10  400\n6  3  70  500\n7  8  30  200\n8  3  40  800\n9  6  60  200\n""]";"["">>> import pandas as pd\n>>> from random import randint\n>>> df = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],\n                       'B': [randint(1, 9)*10 for x in xrange(10)],\n                       'C': [randint(1, 9)*100 for x in xrange(10)]})\n>>> df\n   A   B    C\n0  3  40  100\n1  6  30  200\n2  7  70  800\n3  3  50  200\n4  7  50  400\n5  4  10  400\n6  3  70  500\n7  8  30  200\n8  3  40  800\n9  6  60  200\n"", ""df['sum'] = df['A'] + df['C']"", ""df['A']"", ""df['sum'] = df['B'] + df['C']""]";[''];[''];False;['import pandas as pd\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
828;828;828;828;4.0;6;24870953;;1;22;<python><performance><pandas><iteration>;Does iterrows have performance issues?;7339.0;"[""import pandas as pd\nimport numpy as np\nimport time\n\ns1 = np.random.randn(2000000)\ns2 = np.random.randn(2000000)\ndfa = pd.DataFrame({'s1': s1, 's2': s2})\n\nstart = time.time()\ni=0\nfor rowindex, row in dfa.iterrows():\n    i+=1\nend = time.time()\nprint end - start\nimport pandas as pd\nimport numpy as np\n\n#%% Create the original tables\nt1 = {'letter':['a','b'],\n      'number1':[50,-10]}\n\nt2 = {'letter':['a','a','b','b'],\n      'number2':[0.2,0.5,0.1,0.4]}\n\ntable1 = pd.DataFrame(t1)\ntable2 = pd.DataFrame(t2)\n\n#%% Create the body of the new table\ntable3 = pd.DataFrame(np.nan, columns=['letter','number2'], index=[0])\n\n#%% Iterate through filtering relevant data, optimizing, returning info\nfor row_index, row in table1.iterrows():   \n    t2info = table2[table2.letter == row['letter']].reset_index()\n    table3.ix[row_index,] = optimize(t2info,row['number1'])\n\n#%% Define optimization\ndef optimize(t2info, t1info):\n    calculation = []\n    for index, r in t2info.iterrows():\n        calculation.append(r['number2']*t1info)\n    maxrow = calculation.index(max(calculation))\n    return t2info.ix[maxrow]\n""]";"[""import pandas as pd\nimport numpy as np\nimport time\n\ns1 = np.random.randn(2000000)\ns2 = np.random.randn(2000000)\ndfa = pd.DataFrame({'s1': s1, 's2': s2})\n\nstart = time.time()\ni=0\nfor rowindex, row in dfa.iterrows():\n    i+=1\nend = time.time()\nprint end - start\n"", ""import pandas as pd\nimport numpy as np\n\n#%% Create the original tables\nt1 = {'letter':['a','b'],\n      'number1':[50,-10]}\n\nt2 = {'letter':['a','a','b','b'],\n      'number2':[0.2,0.5,0.1,0.4]}\n\ntable1 = pd.DataFrame(t1)\ntable2 = pd.DataFrame(t2)\n\n#%% Create the body of the new table\ntable3 = pd.DataFrame(np.nan, columns=['letter','number2'], index=[0])\n\n#%% Iterate through filtering relevant data, optimizing, returning info\nfor row_index, row in table1.iterrows():   \n    t2info = table2[table2.letter == row['letter']].reset_index()\n    table3.ix[row_index,] = optimize(t2info,row['number1'])\n\n#%% Define optimization\ndef optimize(t2info, t1info):\n    calculation = []\n    for index, r in t2info.iterrows():\n        calculation.append(r['number2']*t1info)\n    maxrow = calculation.index(max(calculation))\n    return t2info.ix[maxrow]\n""]";"[""import pandas as pd\nimport numpy as np\nimport time\n\ns1 = np.random.randn(2000000)\ns2 = np.random.randn(2000000)\ndfa = pd.DataFrame({'s1': s1, 's2': s2})\n\nstart = time.time()\ni=0\nfor rowindex, row in dfa.iterrows():\n    i+=1\nend = time.time()\nprint end - start\n"", ""import pandas as pd\nimport numpy as np\n\n#%% Create the original tables\nt1 = {'letter':['a','b'],\n      'number1':[50,-10]}\n\nt2 = {'letter':['a','a','b','b'],\n      'number2':[0.2,0.5,0.1,0.4]}\n\ntable1 = pd.DataFrame(t1)\ntable2 = pd.DataFrame(t2)\n\n#%% Create the body of the new table\ntable3 = pd.DataFrame(np.nan, columns=['letter','number2'], index=[0])\n\n#%% Iterate through filtering relevant data, optimizing, returning info\nfor row_index, row in table1.iterrows():   \n    t2info = table2[table2.letter == row['letter']].reset_index()\n    table3.ix[row_index,] = optimize(t2info,row['number1'])\n\n#%% Define optimization\ndef optimize(t2info, t1info):\n    calculation = []\n    for index, r in t2info.iterrows():\n        calculation.append(r['number2']*t1info)\n    maxrow = calculation.index(max(calculation))\n    return t2info.ix[maxrow]\n""]";"[""import pandas as pd\nimport numpy as np\nimport time\n\ns1 = np.random.randn(2000000)\ns2 = np.random.randn(2000000)\ndfa = pd.DataFrame({'s1': s1, 's2': s2})\n\nstart = time.time()\ni=0\nend = time.time()\nimport pandas as pd\nimport numpy as np\n\n#%% Create the original tables\n\n\ntable1 = pd.DataFrame(t1)\ntable2 = pd.DataFrame(t2)\n\n#%% Create the body of the new table\ntable3 = pd.DataFrame(np.nan, columns=['letter','number2'], index=[0])\n\n#%% Iterate through filtering relevant data, optimizing, returning info\n\n#%% Define optimization\n""]";"[""import pandas as pd\nimport numpy as np\nimport time\n\ns1 = np.random.randn(2000000)\ns2 = np.random.randn(2000000)\ndfa = pd.DataFrame({'s1': s1, 's2': s2})\n\nstart = time.time()\ni=0\nend = time.time()\nimport pandas as pd\nimport numpy as np\n\n#%% Create the original tables\n\n\ntable1 = pd.DataFrame(t1)\ntable2 = pd.DataFrame(t2)\n\n#%% Create the body of the new table\ntable3 = pd.DataFrame(np.nan, columns=['letter','number2'], index=[0])\n\n#%% Iterate through filtering relevant data, optimizing, returning info\n\n#%% Define optimization\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\nimport pandas as pd\nimport numpy as np\nimport time\n\ns1 = np.random.randn(2000000)\ns2 = np.random.randn(2000000)\ndfa = pd.DataFrame({'s1': s1, 's2': s2})\n\nstart = time.time()\ni=0\nend = time.time()\nimport pandas as pd\nimport numpy as np\n\n#%% Create the original tables\n\n\ntable1 = pd.DataFrame(t1)\ntable2 = pd.DataFrame(t2)\n\n#%% Create the body of the new table\ntable3 = pd.DataFrame(np.nan, columns=['letter','number2'], index=[0])\n\n#%% Iterate through filtering relevant data, optimizing, returning info\n\n#%% Define optimization\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
829;829;829;829;3.0;0;24901766;;1;20;<python><pandas>;python: How to get column names from pandas dataframe - but only for continuous data type?;63185.0;['names = df.columns.values \nnames = df.columns.values(column_type=float64) \n'];['names = df.columns.values \n', 'names = df.columns.values(column_type=float64) \n'];['names = df.columns.values \n', 'names = df.columns.values(column_type=float64) \n'];['names = df.columns.values \nnames = df.columns.values(column_type=float64) \n'];['names = df.columns.values \nnames = df.columns.values(column_type=float64) \n'];False;['import pandas as pd\ndf = pd.DataFrame()\nnames = df.columns.values \nnames = df.columns.values(column_type=float64) \n'];True;0;3;"[""name 'DF' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'DF' is not defined"", ""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];2;3;"[""name 'DF' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess']
830;830;830;830;1.0;0;24980437;;1;12;<python><python-2.7><pandas>;Pandas - GroupBy and then Merge on original table;10276.0;"[""SELECT EID,\n       PCODE,\n       SUM(PVALUE) AS PVALUE,\n       SUM(SQRT(SC*EXP(SC-1))) AS SC,\n       SUM(SI) AS SI,\n       SUM(EE) AS EE\nINTO foo_bar_grp\nFROM foo_bar\nGROUP BY EID, PCODE \nSELECT *\nFROM foo_bar_grp INNER JOIN \nfoo_bar ON foo_bar.EID = foo_bar_grp.EID \n        AND foo_bar.PCODE = foo_bar_grp.PCODE\npol_dict = {'PID':[1,1,2,2],\n             'EID':[123,123,123,123],\n             'PCODE':['GU','GR','GU','GR'],\n             'PVALUE':[100,50,150,300],\n             'SI':[400,40,140,140],\n             'SC':[230,23,213,213],\n             'EE':[10000,10000,2000,30000],\n             }\n\n\npol_df = DataFrame(pol_dict)\n\npol_df\n   EID    EE PCODE  PID  PVALUE   SC   SI\n0  123  10000    GU    1     100  230  400\n1  123  10000    GR    1      50   23   40\n2  123   2000    GU    2     150  213  140\n3  123  30000    GR    2     300  213  140\n#create aggregation dataframe\npoagg_df = pol_df\ndel poagg_df['PID']\npo_grouped_df = poagg_df.groupby(['EID','PCODE'])\n\n#generate acc level aggregate\nacc_df = po_grouped_df.agg({\n    'PVALUE' : np.sum,\n    'SI' : lambda x: np.sqrt(np.sum(x * np.exp(x-1))),\n    'SC' : np.sum,\n    'EE' : np.sum\n})\npo_account_df = pd.merge(acc_df, po_df, on=['EID','PCODE'], how='inner',suffixes=('_Acc','_Po'))\npol_acc_df['PVALUE_PCT'] = np.round(pol_acc_df.PVALUE_Po/pol_acc_df.PVALUE_Acc,4)\n""]";"['SELECT EID,\n       PCODE,\n       SUM(PVALUE) AS PVALUE,\n       SUM(SQRT(SC*EXP(SC-1))) AS SC,\n       SUM(SI) AS SI,\n       SUM(EE) AS EE\nINTO foo_bar_grp\nFROM foo_bar\nGROUP BY EID, PCODE \n', 'SELECT *\nFROM foo_bar_grp INNER JOIN \nfoo_bar ON foo_bar.EID = foo_bar_grp.EID \n        AND foo_bar.PCODE = foo_bar_grp.PCODE\n', ""pol_dict = {'PID':[1,1,2,2],\n             'EID':[123,123,123,123],\n             'PCODE':['GU','GR','GU','GR'],\n             'PVALUE':[100,50,150,300],\n             'SI':[400,40,140,140],\n             'SC':[230,23,213,213],\n             'EE':[10000,10000,2000,30000],\n             }\n\n\npol_df = DataFrame(pol_dict)\n\npol_df\n"", '   EID    EE PCODE  PID  PVALUE   SC   SI\n0  123  10000    GU    1     100  230  400\n1  123  10000    GR    1      50   23   40\n2  123   2000    GU    2     150  213  140\n3  123  30000    GR    2     300  213  140\n', ""#create aggregation dataframe\npoagg_df = pol_df\ndel poagg_df['PID']\npo_grouped_df = poagg_df.groupby(['EID','PCODE'])\n\n#generate acc level aggregate\nacc_df = po_grouped_df.agg({\n    'PVALUE' : np.sum,\n    'SI' : lambda x: np.sqrt(np.sum(x * np.exp(x-1))),\n    'SC' : np.sum,\n    'EE' : np.sum\n})\n"", ""po_account_df = pd.merge(acc_df, po_df, on=['EID','PCODE'], how='inner',suffixes=('_Acc','_Po'))\n"", ""pol_acc_df['PVALUE_PCT'] = np.round(pol_acc_df.PVALUE_Po/pol_acc_df.PVALUE_Acc,4)\n""]";"['SELECT EID,\n       PCODE,\n       SUM(PVALUE) AS PVALUE,\n       SUM(SQRT(SC*EXP(SC-1))) AS SC,\n       SUM(SI) AS SI,\n       SUM(EE) AS EE\nINTO foo_bar_grp\nFROM foo_bar\nGROUP BY EID, PCODE \n', 'SELECT *\nFROM foo_bar_grp INNER JOIN \nfoo_bar ON foo_bar.EID = foo_bar_grp.EID \n        AND foo_bar.PCODE = foo_bar_grp.PCODE\n', ""pol_dict = {'PID':[1,1,2,2],\n             'EID':[123,123,123,123],\n             'PCODE':['GU','GR','GU','GR'],\n             'PVALUE':[100,50,150,300],\n             'SI':[400,40,140,140],\n             'SC':[230,23,213,213],\n             'EE':[10000,10000,2000,30000],\n             }\n\n\npol_df = DataFrame(pol_dict)\n\npol_df\n"", '   EID    EE PCODE  PID  PVALUE   SC   SI\n0  123  10000    GU    1     100  230  400\n1  123  10000    GR    1      50   23   40\n2  123   2000    GU    2     150  213  140\n3  123  30000    GR    2     300  213  140\n', ""#create aggregation dataframe\npoagg_df = pol_df\ndel poagg_df['PID']\npo_grouped_df = poagg_df.groupby(['EID','PCODE'])\n\n#generate acc level aggregate\nacc_df = po_grouped_df.agg({\n    'PVALUE' : np.sum,\n    'SI' : lambda x: np.sqrt(np.sum(x * np.exp(x-1))),\n    'SC' : np.sum,\n    'EE' : np.sum\n})\n"", ""po_account_df = pd.merge(acc_df, po_df, on=['EID','PCODE'], how='inner',suffixes=('_Acc','_Po'))\n"", ""pol_acc_df['PVALUE_PCT'] = np.round(pol_acc_df.PVALUE_Po/pol_acc_df.PVALUE_Acc,4)\n""]";"[""\n\npol_df = DataFrame(pol_dict)\n\npol_df\n#create aggregation dataframe\npoagg_df = pol_df\ndel poagg_df['PID']\npo_grouped_df = poagg_df.groupby(['EID','PCODE'])\n\n#generate acc level aggregate\npo_account_df = pd.merge(acc_df, po_df, on=['EID','PCODE'], how='inner',suffixes=('_Acc','_Po'))\npol_acc_df['PVALUE_PCT'] = np.round(pol_acc_df.PVALUE_Po/pol_acc_df.PVALUE_Acc,4)\n""]";"[""from pandas import DataFrame\nimport pandas as pd\n\n\npol_df = DataFrame(pol_dict)\n\npol_df\n#create aggregation dataframe\npoagg_df = pol_df\ndel poagg_df['PID']\npo_grouped_df = poagg_df.groupby(['EID','PCODE'])\n\n#generate acc level aggregate\npo_account_df = pd.merge(acc_df, po_df, on=['EID','PCODE'], how='inner',suffixes=('_Acc','_Po'))\npol_acc_df['PVALUE_PCT'] = np.round(pol_acc_df.PVALUE_Po/pol_acc_df.PVALUE_Acc,4)\n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\n\n\npol_df = DataFrame(pol_dict)\n\npol_df\n#create aggregation dataframe\npoagg_df = pol_df\ndel poagg_df['PID']\npo_grouped_df = poagg_df.groupby(['EID','PCODE'])\n\n#generate acc level aggregate\npo_account_df = pd.merge(acc_df, po_df, on=['EID','PCODE'], how='inner',suffixes=('_Acc','_Po'))\npol_acc_df['PVALUE_PCT'] = np.round(pol_acc_df.PVALUE_Po/pol_acc_df.PVALUE_Acc,4)\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'acc_df' is not defined""]";['NameError'];0;1;"[""name 'acc_df' is not defined""]";['NameError']
831;831;831;831;2.0;0;24988131;;1;17;<python><dictionary><pandas><dataframe><multi-index>;Nested dictionary to multiindex dataframe where dictionary keys are column labels;6855.0;"[""dictionary = {'A' : {'a': [1,2,3,4,5],\n                     'b': [6,7,8,9,1]},\n\n              'B' : {'a': [2,3,4,5,6],\n                     'b': [7,8,9,1,2]}}\n     A   B\n     a b a b\n  0  1 6 2 7\n  1  2 7 3 8\n  2  3 8 4 9\n  3  4 9 5 1\n  4  5 1 6 2\nIn [99]:\n\nDataFrame(dictionary)\n\nOut[99]:\n     A               B\na   [1, 2, 3, 4, 5] [2, 3, 4, 5, 6]\nb   [6, 7, 8, 9, 1] [7, 8, 9, 1, 2]\n""]";"[""dictionary = {'A' : {'a': [1,2,3,4,5],\n                     'b': [6,7,8,9,1]},\n\n              'B' : {'a': [2,3,4,5,6],\n                     'b': [7,8,9,1,2]}}\n"", '     A   B\n     a b a b\n  0  1 6 2 7\n  1  2 7 3 8\n  2  3 8 4 9\n  3  4 9 5 1\n  4  5 1 6 2\n', 'In [99]:\n\nDataFrame(dictionary)\n\nOut[99]:\n     A               B\na   [1, 2, 3, 4, 5] [2, 3, 4, 5, 6]\nb   [6, 7, 8, 9, 1] [7, 8, 9, 1, 2]\n']";"[""dictionary = {'A' : {'a': [1,2,3,4,5],\n                     'b': [6,7,8,9,1]},\n\n              'B' : {'a': [2,3,4,5,6],\n                     'b': [7,8,9,1,2]}}\n"", '     A   B\n     a b a b\n  0  1 6 2 7\n  1  2 7 3 8\n  2  3 8 4 9\n  3  4 9 5 1\n  4  5 1 6 2\n', 'In [99]:\n\nDataFrame(dictionary)\n\nOut[99]:\n     A               B\na   [1, 2, 3, 4, 5] [2, 3, 4, 5, 6]\nb   [6, 7, 8, 9, 1] [7, 8, 9, 1, 2]\n']";['DataFrame(dictionary)\n\n'];['DataFrame(dictionary)\n\n'];False;['import pandas as pd\nDataFrame(dictionary)\n\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
832;832;832;832;2.0;0;25024797;;1;15;<python><pandas><dataframe>;Max and Min date in pandas groupby;11334.0;"[""data = {'index': ['2014-06-22 10:46:00', '2014-06-24 19:52:00', '2014-06-25 17:02:00', '2014-06-25 17:55:00', '2014-07-02 11:36:00', '2014-07-06 12:40:00', '2014-07-05 12:46:00', '2014-07-27 15:12:00'],\n    'type': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'C'],\n    'sum_col': [1, 2, 3, 1, 1, 3, 2, 1]}\ndf = pd.DataFrame(data, columns=['index', 'type', 'sum_col'])\ndf['index'] = pd.to_datetime(df['index'])\ndf = df.set_index('index')\ndf['weekofyear'] = df.index.weekofyear\ndf['date'] = df.index.date\ndf['date'] = pd.to_datetime(df['date'])\n\n\n\n                     type sum_col weekofyear   date\nindex               \n2014-06-22 10:46:00    A    1       25      2014-06-22\n2014-06-24 19:52:00    B    2       26      2014-06-24\n2014-06-25 17:02:00    C    3       26      2014-06-25\n2014-06-25 17:55:00    A    1       26      2014-06-25\n2014-07-02 11:36:00    B    1       27      2014-07-02\n2014-07-06 12:40:00    C    3       27      2014-07-06\n2014-07-05 12:46:00    A    2       27      2014-07-05\n2014-07-27 15:12:00    C    1       30      2014-07-27\ngb = df.groupby(['type', 'weekofyear'])\ngb['sum_col'].agg({'sum_col' : np.sum})\ngb = df.groupby(['type', 'weekofyear'])\ngb.agg({'sum_col' : np.sum,\n        'date' : np.min,\n        'date' : np.max})\n""]";"[""data = {'index': ['2014-06-22 10:46:00', '2014-06-24 19:52:00', '2014-06-25 17:02:00', '2014-06-25 17:55:00', '2014-07-02 11:36:00', '2014-07-06 12:40:00', '2014-07-05 12:46:00', '2014-07-27 15:12:00'],\n    'type': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'C'],\n    'sum_col': [1, 2, 3, 1, 1, 3, 2, 1]}\ndf = pd.DataFrame(data, columns=['index', 'type', 'sum_col'])\ndf['index'] = pd.to_datetime(df['index'])\ndf = df.set_index('index')\ndf['weekofyear'] = df.index.weekofyear\ndf['date'] = df.index.date\ndf['date'] = pd.to_datetime(df['date'])\n\n\n\n                     type sum_col weekofyear   date\nindex               \n2014-06-22 10:46:00    A    1       25      2014-06-22\n2014-06-24 19:52:00    B    2       26      2014-06-24\n2014-06-25 17:02:00    C    3       26      2014-06-25\n2014-06-25 17:55:00    A    1       26      2014-06-25\n2014-07-02 11:36:00    B    1       27      2014-07-02\n2014-07-06 12:40:00    C    3       27      2014-07-06\n2014-07-05 12:46:00    A    2       27      2014-07-05\n2014-07-27 15:12:00    C    1       30      2014-07-27\n"", ""gb = df.groupby(['type', 'weekofyear'])\ngb['sum_col'].agg({'sum_col' : np.sum})\n"", ""gb = df.groupby(['type', 'weekofyear'])\ngb.agg({'sum_col' : np.sum,\n        'date' : np.min,\n        'date' : np.max})\n""]";"[""data = {'index': ['2014-06-22 10:46:00', '2014-06-24 19:52:00', '2014-06-25 17:02:00', '2014-06-25 17:55:00', '2014-07-02 11:36:00', '2014-07-06 12:40:00', '2014-07-05 12:46:00', '2014-07-27 15:12:00'],\n    'type': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'C'],\n    'sum_col': [1, 2, 3, 1, 1, 3, 2, 1]}\ndf = pd.DataFrame(data, columns=['index', 'type', 'sum_col'])\ndf['index'] = pd.to_datetime(df['index'])\ndf = df.set_index('index')\ndf['weekofyear'] = df.index.weekofyear\ndf['date'] = df.index.date\ndf['date'] = pd.to_datetime(df['date'])\n\n\n\n                     type sum_col weekofyear   date\nindex               \n2014-06-22 10:46:00    A    1       25      2014-06-22\n2014-06-24 19:52:00    B    2       26      2014-06-24\n2014-06-25 17:02:00    C    3       26      2014-06-25\n2014-06-25 17:55:00    A    1       26      2014-06-25\n2014-07-02 11:36:00    B    1       27      2014-07-02\n2014-07-06 12:40:00    C    3       27      2014-07-06\n2014-07-05 12:46:00    A    2       27      2014-07-05\n2014-07-27 15:12:00    C    1       30      2014-07-27\n"", ""gb = df.groupby(['type', 'weekofyear'])\ngb['sum_col'].agg({'sum_col' : np.sum})\n"", ""gb = df.groupby(['type', 'weekofyear'])\ngb.agg({'sum_col' : np.sum,\n        'date' : np.min,\n        'date' : np.max})\n""]";"[""df = pd.DataFrame(data, columns=['index', 'type', 'sum_col'])\ndf['index'] = pd.to_datetime(df['index'])\ndf = df.set_index('index')\ndf['weekofyear'] = df.index.weekofyear\ndf['date'] = df.index.date\ndf['date'] = pd.to_datetime(df['date'])\n\n\n\nindex               \ngb = df.groupby(['type', 'weekofyear'])\ngb['sum_col'].agg({'sum_col' : np.sum})\ngb = df.groupby(['type', 'weekofyear'])\n""]";"[""import pandas as pd\ndf = pd.DataFrame(data, columns=['index', 'type', 'sum_col'])\ndf['index'] = pd.to_datetime(df['index'])\ndf = df.set_index('index')\ndf['weekofyear'] = df.index.weekofyear\ndf['date'] = df.index.date\ndf['date'] = pd.to_datetime(df['date'])\n\n\n\nindex               \ngb = df.groupby(['type', 'weekofyear'])\ngb['sum_col'].agg({'sum_col' : np.sum})\ngb = df.groupby(['type', 'weekofyear'])\n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame(data, columns=['index', 'type', 'sum_col'])\ndf['index'] = pd.to_datetime(df['index'])\ndf = df.set_index('index')\ndf['weekofyear'] = df.index.weekofyear\ndf['date'] = df.index.date\ndf['date'] = pd.to_datetime(df['date'])\n\n\n\nindex               \ngb = df.groupby(['type', 'weekofyear'])\ngb['sum_col'].agg({'sum_col' : np.sum})\ngb = df.groupby(['type', 'weekofyear'])\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
833;833;833;833;5.0;1;25039626;;1;37;<python><types><pandas>;find numeric columns in pandas (python);15852.0;['isNumeric = is_numeric(df)\n'];['isNumeric = is_numeric(df)\n'];['isNumeric = is_numeric(df)\n'];['isNumeric = is_numeric(df)\n'];['isNumeric = is_numeric(df)\n'];False;['import pandas as pd\ndf = pd.DataFrame()\nisNumeric = is_numeric(df)\n'];True;0;2;"[""name 'df' is not defined"", ""name 'data' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'data' is not defined""]";['NameError', 'NameError'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess']
834;834;834;834;2.0;2;25050141;;1;18;<python><pandas><nan>;How to filter in NaN (pandas)?;13993.0;"[""newdf = df[(df.var1 == 'a') & (df.var2 == NaN)]\n""]";"[""newdf = df[(df.var1 == 'a') & (df.var2 == NaN)]\n""]";"[""newdf = df[(df.var1 == 'a') & (df.var2 == NaN)]\n"", 'np.NaN', ""'NaN'"", ""'nan'"", 'pd.NaN', 'df.fillna(np.nan)']";"[""newdf = df[(df.var1 == 'a') & (df.var2 == NaN)]\n""]";"[""newdf = df[(df.var1 == 'a') & (df.var2 == NaN)]\n""]";False;"[""import pandas as pd\nnewdf = df[(df.var1 == 'a') & (df.var2 == NaN)]\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
835;835;835;835;1.0;0;25055712;;1;23;<pandas><resampling>;Pandas every nth row;10589.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
836;836;836;836;5.0;2;25057835;;1;12;<python><r><numpy><pandas>;Get the mean across multiple Pandas DataFrames;5655.0;['         Source.0  Source.1  Source.2  Source.3\ncluster                                        \n0        0.001182  0.184535  0.814230  0.000054\n1        0.000001  0.160490  0.839508  0.000001\n2        0.000001  0.173829  0.826114  0.000055\n3        0.000432  0.180065  0.819502  0.000001\n4        0.000152  0.157041  0.842694  0.000113\n5        0.000183  0.174142  0.825674  0.000001\n6        0.000001  0.151556  0.848405  0.000038\n7        0.000771  0.177583  0.821645  0.000001\n8        0.000001  0.202059  0.797939  0.000001\n9        0.000025  0.189537  0.810410  0.000028\n10       0.006142  0.003041  0.493912  0.496905\n11       0.003739  0.002367  0.514216  0.479678\n12       0.002334  0.001517  0.529041  0.467108\n13       0.003458  0.000001  0.532265  0.464276\n14       0.000405  0.005655  0.527576  0.466364\n15       0.002557  0.003233  0.507954  0.486256\n16       0.004161  0.000001  0.491271  0.504568\n17       0.001364  0.001330  0.528311  0.468996\n18       0.002886  0.000001  0.506392  0.490721\n19       0.001823  0.002498  0.509620  0.486059\n\n         Source.0  Source.1  Source.2  Source.3\ncluster                                        \n0        0.000001  0.197108  0.802495  0.000396\n1        0.000001  0.157860  0.842076  0.000063\n2        0.094956  0.203057  0.701662  0.000325\n3        0.000001  0.181948  0.817841  0.000210\n4        0.000003  0.169680  0.830316  0.000001\n5        0.000362  0.177194  0.822443  0.000001\n6        0.000001  0.146807  0.852924  0.000268\n7        0.001087  0.178994  0.819564  0.000354\n8        0.000001  0.202182  0.797333  0.000485\n9        0.000348  0.181399  0.818252  0.000001\n10       0.003050  0.000247  0.506777  0.489926\n11       0.004420  0.000001  0.513927  0.481652\n12       0.006488  0.001396  0.527197  0.464919\n13       0.001510  0.000001  0.525987  0.472502\n14       0.000001  0.000001  0.520737  0.479261\n15       0.000001  0.001765  0.515658  0.482575\n16       0.000001  0.000001  0.492550  0.507448\n17       0.002855  0.000199  0.526535  0.470411\n18       0.000001  0.001952  0.498303  0.499744\n19       0.001232  0.000001  0.506612  0.492155\n'];['         Source.0  Source.1  Source.2  Source.3\ncluster                                        \n0        0.001182  0.184535  0.814230  0.000054\n1        0.000001  0.160490  0.839508  0.000001\n2        0.000001  0.173829  0.826114  0.000055\n3        0.000432  0.180065  0.819502  0.000001\n4        0.000152  0.157041  0.842694  0.000113\n5        0.000183  0.174142  0.825674  0.000001\n6        0.000001  0.151556  0.848405  0.000038\n7        0.000771  0.177583  0.821645  0.000001\n8        0.000001  0.202059  0.797939  0.000001\n9        0.000025  0.189537  0.810410  0.000028\n10       0.006142  0.003041  0.493912  0.496905\n11       0.003739  0.002367  0.514216  0.479678\n12       0.002334  0.001517  0.529041  0.467108\n13       0.003458  0.000001  0.532265  0.464276\n14       0.000405  0.005655  0.527576  0.466364\n15       0.002557  0.003233  0.507954  0.486256\n16       0.004161  0.000001  0.491271  0.504568\n17       0.001364  0.001330  0.528311  0.468996\n18       0.002886  0.000001  0.506392  0.490721\n19       0.001823  0.002498  0.509620  0.486059\n\n         Source.0  Source.1  Source.2  Source.3\ncluster                                        \n0        0.000001  0.197108  0.802495  0.000396\n1        0.000001  0.157860  0.842076  0.000063\n2        0.094956  0.203057  0.701662  0.000325\n3        0.000001  0.181948  0.817841  0.000210\n4        0.000003  0.169680  0.830316  0.000001\n5        0.000362  0.177194  0.822443  0.000001\n6        0.000001  0.146807  0.852924  0.000268\n7        0.001087  0.178994  0.819564  0.000354\n8        0.000001  0.202182  0.797333  0.000485\n9        0.000348  0.181399  0.818252  0.000001\n10       0.003050  0.000247  0.506777  0.489926\n11       0.004420  0.000001  0.513927  0.481652\n12       0.006488  0.001396  0.527197  0.464919\n13       0.001510  0.000001  0.525987  0.472502\n14       0.000001  0.000001  0.520737  0.479261\n15       0.000001  0.001765  0.515658  0.482575\n16       0.000001  0.000001  0.492550  0.507448\n17       0.002855  0.000199  0.526535  0.470411\n18       0.000001  0.001952  0.498303  0.499744\n19       0.001232  0.000001  0.506612  0.492155\n'];['         Source.0  Source.1  Source.2  Source.3\ncluster                                        \n0        0.001182  0.184535  0.814230  0.000054\n1        0.000001  0.160490  0.839508  0.000001\n2        0.000001  0.173829  0.826114  0.000055\n3        0.000432  0.180065  0.819502  0.000001\n4        0.000152  0.157041  0.842694  0.000113\n5        0.000183  0.174142  0.825674  0.000001\n6        0.000001  0.151556  0.848405  0.000038\n7        0.000771  0.177583  0.821645  0.000001\n8        0.000001  0.202059  0.797939  0.000001\n9        0.000025  0.189537  0.810410  0.000028\n10       0.006142  0.003041  0.493912  0.496905\n11       0.003739  0.002367  0.514216  0.479678\n12       0.002334  0.001517  0.529041  0.467108\n13       0.003458  0.000001  0.532265  0.464276\n14       0.000405  0.005655  0.527576  0.466364\n15       0.002557  0.003233  0.507954  0.486256\n16       0.004161  0.000001  0.491271  0.504568\n17       0.001364  0.001330  0.528311  0.468996\n18       0.002886  0.000001  0.506392  0.490721\n19       0.001823  0.002498  0.509620  0.486059\n\n         Source.0  Source.1  Source.2  Source.3\ncluster                                        \n0        0.000001  0.197108  0.802495  0.000396\n1        0.000001  0.157860  0.842076  0.000063\n2        0.094956  0.203057  0.701662  0.000325\n3        0.000001  0.181948  0.817841  0.000210\n4        0.000003  0.169680  0.830316  0.000001\n5        0.000362  0.177194  0.822443  0.000001\n6        0.000001  0.146807  0.852924  0.000268\n7        0.001087  0.178994  0.819564  0.000354\n8        0.000001  0.202182  0.797333  0.000485\n9        0.000348  0.181399  0.818252  0.000001\n10       0.003050  0.000247  0.506777  0.489926\n11       0.004420  0.000001  0.513927  0.481652\n12       0.006488  0.001396  0.527197  0.464919\n13       0.001510  0.000001  0.525987  0.472502\n14       0.000001  0.000001  0.520737  0.479261\n15       0.000001  0.001765  0.515658  0.482575\n16       0.000001  0.000001  0.492550  0.507448\n17       0.002855  0.000199  0.526535  0.470411\n18       0.000001  0.001952  0.498303  0.499744\n19       0.001232  0.000001  0.506612  0.492155\n', '[0,Source.0]'];['cluster                                        \n\ncluster                                        \n'];['cluster                                        \n\ncluster                                        \n'];False;['import pandas as pd\ncluster                                        \n\ncluster                                        \n'];False;1;3;"['Sucess', ""name 'DataFrame' is not defined"", ""name 'pd' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name 'np' is not defined"", ""name 'np' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name 'DataFrame' is not defined"", ""name 'np' is not defined""]";['Sucess', 'NameError', 'NameError']
837;837;837;837;2.0;0;25122099;;1;18;<python><pandas><move><dataframe><shift>;Move column by name to front of table in pandas;14833.0;['                             Net   Upper   Lower  Mid  Zsore\nAnswer option                                                \nMore than once a day          0%   0.22%  -0.12%   2    65 \nOnce a day                    0%   0.32%  -0.19%   3    45\nSeveral times a week          2%   2.45%   1.10%   4    78\nOnce a week                   1%   1.63%  -0.40%   6    65\n                             Mid   Upper   Lower  Net  Zsore\nAnswer option                                                \nMore than once a day          2   0.22%  -0.12%   0%    65 \nOnce a day                    3   0.32%  -0.19%   0%    45\nSeveral times a week          4   2.45%   1.10%   2%    78\nOnce a week                   6   1.63%  -0.40%   1%    65\n'];['                             Net   Upper   Lower  Mid  Zsore\nAnswer option                                                \nMore than once a day          0%   0.22%  -0.12%   2    65 \nOnce a day                    0%   0.32%  -0.19%   3    45\nSeveral times a week          2%   2.45%   1.10%   4    78\nOnce a week                   1%   1.63%  -0.40%   6    65\n', '                             Mid   Upper   Lower  Net  Zsore\nAnswer option                                                \nMore than once a day          2   0.22%  -0.12%   0%    65 \nOnce a day                    3   0.32%  -0.19%   0%    45\nSeveral times a week          4   2.45%   1.10%   2%    78\nOnce a week                   6   1.63%  -0.40%   1%    65\n'];['                             Net   Upper   Lower  Mid  Zsore\nAnswer option                                                \nMore than once a day          0%   0.22%  -0.12%   2    65 \nOnce a day                    0%   0.32%  -0.19%   3    45\nSeveral times a week          2%   2.45%   1.10%   4    78\nOnce a week                   1%   1.63%  -0.40%   6    65\n', '                             Mid   Upper   Lower  Net  Zsore\nAnswer option                                                \nMore than once a day          2   0.22%  -0.12%   0%    65 \nOnce a day                    3   0.32%  -0.19%   0%    45\nSeveral times a week          4   2.45%   1.10%   2%    78\nOnce a week                   6   1.63%  -0.40%   1%    65\n'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError']
838;838;838;838;3.0;8;25129144;;1;14;<python><datetime><pandas>;Pandas: Return Hour from Datetime Column Directly;20259.0;"[""timestamp               sales_office\n2014-01-01 09:01:00     Cincinnati\n2014-01-01 09:11:00     San Francisco\n2014-01-01 15:22:00     Chicago\n2014-01-01 19:01:00     Chicago\ndef hr_func(ts):\n    return ts.hour\n\nsales['time_hour'] = sales['timestamp'].apply(hr_func)\ntimestamp               sales_office         time_hour\n2014-01-01 09:01:00     Cincinnati           9\n2014-01-01 09:11:00     San Francisco        9\n2014-01-01 15:22:00     Chicago              15\n2014-01-01 19:01:00     Chicago              19\nsales['time_hour'] = sales['timestamp'].hour\n""]";"['timestamp               sales_office\n2014-01-01 09:01:00     Cincinnati\n2014-01-01 09:11:00     San Francisco\n2014-01-01 15:22:00     Chicago\n2014-01-01 19:01:00     Chicago\n', ""def hr_func(ts):\n    return ts.hour\n\nsales['time_hour'] = sales['timestamp'].apply(hr_func)\n"", 'timestamp               sales_office         time_hour\n2014-01-01 09:01:00     Cincinnati           9\n2014-01-01 09:11:00     San Francisco        9\n2014-01-01 15:22:00     Chicago              15\n2014-01-01 19:01:00     Chicago              19\n', ""sales['time_hour'] = sales['timestamp'].hour\n""]";"['sales', 'timestamp               sales_office\n2014-01-01 09:01:00     Cincinnati\n2014-01-01 09:11:00     San Francisco\n2014-01-01 15:22:00     Chicago\n2014-01-01 19:01:00     Chicago\n', 'time_hour', 'apply()', ""def hr_func(ts):\n    return ts.hour\n\nsales['time_hour'] = sales['timestamp'].apply(hr_func)\n"", 'timestamp               sales_office         time_hour\n2014-01-01 09:01:00     Cincinnati           9\n2014-01-01 09:11:00     San Francisco        9\n2014-01-01 15:22:00     Chicago              15\n2014-01-01 19:01:00     Chicago              19\n', ""sales['time_hour'] = sales['timestamp'].hour\n"", 'Series']";"[""\nsales['time_hour'] = sales['timestamp'].apply(hr_func)\nsales['time_hour'] = sales['timestamp'].hour\n""]";"[""\nsales['time_hour'] = sales['timestamp'].apply(hr_func)\nsales['time_hour'] = sales['timestamp'].hour\n""]";False;"[""import pandas as pd\n\nsales['time_hour'] = sales['timestamp'].apply(hr_func)\nsales['time_hour'] = sales['timestamp'].hour\n""]";False;2;3;"[""name 'sales' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess'];2;3;"[""name 'sales' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess'];2;3;"[""name 'sales' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess']
839;839;839;839;7.0;1;25146121;;1;53;<python><pandas>;Extracting just Month and Year from Pandas Datetime column (Python);78275.0;"[""df['ArrivalDate'] =\n...\n936   2012-12-31\n938   2012-12-29\n965   2012-12-31\n966   2012-12-31\n967   2012-12-31\n968   2012-12-31\n969   2012-12-31\n970   2012-12-29\n971   2012-12-31\n972   2012-12-29\n973   2012-12-29\n...\ndf['ArrivalDate'].resample('M', how = 'mean')\nOnly valid with DatetimeIndex or PeriodIndex \ndf['ArrivalDate'].apply(lambda(x):x[:-2])\n'Timestamp' object has no attribute '__getitem__' \ndf.index = df['ArrivalDate']\n""]";"[""df['ArrivalDate'] =\n...\n936   2012-12-31\n938   2012-12-29\n965   2012-12-31\n966   2012-12-31\n967   2012-12-31\n968   2012-12-31\n969   2012-12-31\n970   2012-12-29\n971   2012-12-31\n972   2012-12-29\n973   2012-12-29\n...\n"", ""df['ArrivalDate'].resample('M', how = 'mean')\n"", 'Only valid with DatetimeIndex or PeriodIndex \n', ""df['ArrivalDate'].apply(lambda(x):x[:-2])\n"", ""'Timestamp' object has no attribute '__getitem__' \n"", ""df.index = df['ArrivalDate']\n""]";"[""df['ArrivalDate'] =\n...\n936   2012-12-31\n938   2012-12-29\n965   2012-12-31\n966   2012-12-31\n967   2012-12-31\n968   2012-12-31\n969   2012-12-31\n970   2012-12-29\n971   2012-12-31\n972   2012-12-29\n973   2012-12-29\n...\n"", ""df['ArrivalDate'].resample('M', how = 'mean')\n"", 'Only valid with DatetimeIndex or PeriodIndex \n', ""df['ArrivalDate'].apply(lambda(x):x[:-2])\n"", ""'Timestamp' object has no attribute '__getitem__' \n"", ""df.index = df['ArrivalDate']\n""]";"[""...\n...\ndf['ArrivalDate'].resample('M', how = 'mean')\ndf.index = df['ArrivalDate']\n""]";"[""...\n...\ndf['ArrivalDate'].resample('M', how = 'mean')\ndf.index = df['ArrivalDate']\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\n...\n...\ndf['ArrivalDate'].resample('M', how = 'mean')\ndf.index = df['ArrivalDate']\n""]";True;1;4;"[""name 'pandas' is not defined"", ""name 'pd' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'NameError', 'Sucess', 'NameError'];1;4;"[""name 'pandas' is not defined"", ""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'NameError', 'Sucess', 'NameError'];1;4;"[""name 'pandas' is not defined"", ""'ArrivalDate'"", 'Sucess', ""'DataFrame' object has no attribute 'date_column'""]";['NameError', 'KeyError', 'Sucess', 'AttributeError']
840;840;840;840;3.0;1;25189575;;1;15;<python><pandas><hierarchical><multi-index>;pandas dataframe select columns in multiindex;11209.0;['Name    0                       1                      ...\nCol     A           B           A            B         ...\n0       0.409511    -0.537108   -0.355529    0.212134  ...\n1       -0.332276   -1.087013    0.083684    0.529002  ...\n2       1.138159    -0.327212    0.570834    2.337718  ...\n'];['Name    0                       1                      ...\nCol     A           B           A            B         ...\n0       0.409511    -0.537108   -0.355529    0.212134  ...\n1       -0.332276   -1.087013    0.083684    0.529002  ...\n2       1.138159    -0.327212    0.570834    2.337718  ...\n'];"['Name    0                       1                      ...\nCol     A           B           A            B         ...\n0       0.409511    -0.537108   -0.355529    0.212134  ...\n1       -0.332276   -1.087013    0.083684    0.529002  ...\n2       1.138159    -0.327212    0.570834    2.337718  ...\n', ""names=['Name', 'Col']"", 'Name', 'A', 'B', 'A', 'B']";[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'np' is not defined"", ""'Index' object has no attribute 'get_loc_level'""]";['NameError', 'AttributeError']
841;841;841;841;1.0;8;25203883;;1;12;<python><pandas>;Sampling groups in Pandas;2102.0;['> df \n\n   X   Y  Z\n   1 123  a\n   2  89  b\n   1 234  a\n   4 893  a\n   6 234  b\n   2 893  b\n   3 200  c\n   5 583  c\n   2 583  c\n   6 100  c\n'];['> df \n\n   X   Y  Z\n   1 123  a\n   2  89  b\n   1 234  a\n   4 893  a\n   6 234  b\n   2 893  b\n   3 200  c\n   5 583  c\n   2 583  c\n   6 100  c\n'];['5%', '5%', 'Z', '> df \n\n   X   Y  Z\n   1 123  a\n   2  89  b\n   1 234  a\n   4 893  a\n   6 234  b\n   2 893  b\n   3 200  c\n   5 583  c\n   2 583  c\n   6 100  c\n'];['\n'];['\n'];False;['import pandas as pd\n\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
842;842;842;842;1.0;10;25211220;;1;14;<python><r><view><pandas>;Python equivalent of R's head and tail function;20701.0;"[""<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10 entries, 0 to 9\nData columns (total 14 columns):\n#Book_Date            10  non-null values\nItem_Qty              10  non-null values\nItem_id               10  non-null values\nLocation_id           10  non-null values\nMFG_Discount          10  non-null values\nSale_Revenue          10  non-null values\nSales_Flg             10  non-null values\nSell_Unit_Cost        5  non-null values\nStore_Discount        10  non-null values\nTransaction_Id        10  non-null values\nUnit_Cost_Amt         10  non-null values\nUnit_Received_Cost    5  non-null values\nUnnamed: 0            10  non-null values\nWeight                10  non-null values\n""]";"[""<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10 entries, 0 to 9\nData columns (total 14 columns):\n#Book_Date            10  non-null values\nItem_Qty              10  non-null values\nItem_id               10  non-null values\nLocation_id           10  non-null values\nMFG_Discount          10  non-null values\nSale_Revenue          10  non-null values\nSales_Flg             10  non-null values\nSell_Unit_Cost        5  non-null values\nStore_Discount        10  non-null values\nTransaction_Id        10  non-null values\nUnit_Cost_Amt         10  non-null values\nUnit_Received_Cost    5  non-null values\nUnnamed: 0            10  non-null values\nWeight                10  non-null values\n""]";"[""<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10 entries, 0 to 9\nData columns (total 14 columns):\n#Book_Date            10  non-null values\nItem_Qty              10  non-null values\nItem_id               10  non-null values\nLocation_id           10  non-null values\nMFG_Discount          10  non-null values\nSale_Revenue          10  non-null values\nSales_Flg             10  non-null values\nSell_Unit_Cost        5  non-null values\nStore_Discount        10  non-null values\nTransaction_Id        10  non-null values\nUnit_Cost_Amt         10  non-null values\nUnit_Received_Cost    5  non-null values\nUnnamed: 0            10  non-null values\nWeight                10  non-null values\n""]";['#Book_Date            10  non-null values\n'];['#Book_Date            10  non-null values\n'];False;['import pandas as pd\n#Book_Date            10  non-null values\n'];False;0;1;"[""name 'data' is not defined""]";['NameError'];0;1;"[""name 'data' is not defined""]";['NameError'];0;1;"[""name 'iris' is not defined""]";['NameError']
843;843;843;843;2.0;2;25212986;;1;30;<python><pandas><seaborn>;How to set some xlim and ylim in Seaborn lmplot facetgrid;25892.0;"[""import pandas as pd\nimport seaborn as sns\nimport random\n\nn = 200\nrandom.seed(2014)\nbase_x = [random.random() for i in range(n)]\nbase_y = [2*i for i in base_x]\nerrors = [random.uniform(0,1) for i in range(n)]\ny = [i+j for i,j in zip(base_y,errors)]\n\ndf = pd.DataFrame({'X': base_x,\n                   'Y': y,\n                   'Z': ['A','B']*(n/2)})\n\nmask_for_b = df.Z == 'B'\ndf.loc[mask_for_b,['X','Y']] = df.loc[mask_for_b,] *2\n\nsns.lmplot('X','Y',df,col='Z',sharex=False,sharey=False)\nsns.plt.ylim(0,)\nsns.plt.xlim(0,)\n""]";"[""import pandas as pd\nimport seaborn as sns\nimport random\n\nn = 200\nrandom.seed(2014)\nbase_x = [random.random() for i in range(n)]\nbase_y = [2*i for i in base_x]\nerrors = [random.uniform(0,1) for i in range(n)]\ny = [i+j for i,j in zip(base_y,errors)]\n\ndf = pd.DataFrame({'X': base_x,\n                   'Y': y,\n                   'Z': ['A','B']*(n/2)})\n\nmask_for_b = df.Z == 'B'\ndf.loc[mask_for_b,['X','Y']] = df.loc[mask_for_b,] *2\n\nsns.lmplot('X','Y',df,col='Z',sharex=False,sharey=False)\n"", 'sns.plt.ylim(0,)\nsns.plt.xlim(0,)\n']";"[""import pandas as pd\nimport seaborn as sns\nimport random\n\nn = 200\nrandom.seed(2014)\nbase_x = [random.random() for i in range(n)]\nbase_y = [2*i for i in base_x]\nerrors = [random.uniform(0,1) for i in range(n)]\ny = [i+j for i,j in zip(base_y,errors)]\n\ndf = pd.DataFrame({'X': base_x,\n                   'Y': y,\n                   'Z': ['A','B']*(n/2)})\n\nmask_for_b = df.Z == 'B'\ndf.loc[mask_for_b,['X','Y']] = df.loc[mask_for_b,] *2\n\nsns.lmplot('X','Y',df,col='Z',sharex=False,sharey=False)\n"", 'sns.plt.ylim(0,)\nsns.plt.xlim(0,)\n']";"[""import pandas as pd\nimport seaborn as sns\nimport random\n\nn = 200\nrandom.seed(2014)\nbase_x = [random.random() for i in range(n)]\nbase_y = [2*i for i in base_x]\nerrors = [random.uniform(0,1) for i in range(n)]\ny = [i+j for i,j in zip(base_y,errors)]\n\n\nmask_for_b = df.Z == 'B'\ndf.loc[mask_for_b,['X','Y']] = df.loc[mask_for_b,] *2\n\nsns.lmplot('X','Y',df,col='Z',sharex=False,sharey=False)\nsns.plt.ylim(0,)\nsns.plt.xlim(0,)\n""]";"[""import pandas as pd\nimport seaborn as sns\nimport random\n\nn = 200\nrandom.seed(2014)\nbase_x = [random.random() for i in range(n)]\nbase_y = [2*i for i in base_x]\nerrors = [random.uniform(0,1) for i in range(n)]\ny = [i+j for i,j in zip(base_y,errors)]\n\n\nmask_for_b = df.Z == 'B'\ndf.loc[mask_for_b,['X','Y']] = df.loc[mask_for_b,] *2\n\nsns.lmplot('X','Y',df,col='Z',sharex=False,sharey=False)\nsns.plt.ylim(0,)\nsns.plt.xlim(0,)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\nimport seaborn as sns\nimport random\n\nn = 200\nrandom.seed(2014)\nbase_x = [random.random() for i in range(n)]\nbase_y = [2*i for i in base_x]\nerrors = [random.uniform(0,1) for i in range(n)]\ny = [i+j for i,j in zip(base_y,errors)]\n\n\nmask_for_b = df.Z == 'B'\ndf.loc[mask_for_b,['X','Y']] = df.loc[mask_for_b,] *2\n\nsns.lmplot('X','Y',df,col='Z',sharex=False,sharey=False)\nsns.plt.ylim(0,)\nsns.plt.xlim(0,)\n""]";True;0;2;"[""name 'sns' is not defined"", ""name 'sns' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'sns' is not defined"", ""name 'sns' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'sns' is not defined"", ""name 'sns' is not defined""]";['NameError', 'NameError']
844;844;844;844;3.0;0;25234941;;1;12;<python><pandas><time-series><linear-interpolation>;Python regularise irregular time series with linear interpolation;3459.0;['                     Values\n1992-08-27 07:46:48    28.0  \n1992-08-27 08:00:48    28.2  \n1992-08-27 08:33:48    28.4  \n1992-08-27 08:43:48    28.8  \n1992-08-27 08:48:48    29.0  \n1992-08-27 08:51:48    29.2  \n1992-08-27 08:53:48    29.6  \n1992-08-27 08:56:48    29.8  \n1992-08-27 09:03:48    30.0\n                     Values\n1992-08-27 08:00:00    28.2  \n1992-08-27 08:15:00    28.3  \n1992-08-27 08:30:00    28.4  \n1992-08-27 08:45:00    28.8  \n1992-08-27 09:00:00    29.9\n                     Values\n1992-08-27 08:00:00   28.20  \n1992-08-27 08:15:00     NaN  \n1992-08-27 08:30:00   28.60  \n1992-08-27 08:45:00   29.40  \n1992-08-27 09:00:00   30.00  \n'];['                     Values\n1992-08-27 07:46:48    28.0  \n1992-08-27 08:00:48    28.2  \n1992-08-27 08:33:48    28.4  \n1992-08-27 08:43:48    28.8  \n1992-08-27 08:48:48    29.0  \n1992-08-27 08:51:48    29.2  \n1992-08-27 08:53:48    29.6  \n1992-08-27 08:56:48    29.8  \n1992-08-27 09:03:48    30.0\n', '                     Values\n1992-08-27 08:00:00    28.2  \n1992-08-27 08:15:00    28.3  \n1992-08-27 08:30:00    28.4  \n1992-08-27 08:45:00    28.8  \n1992-08-27 09:00:00    29.9\n', '                     Values\n1992-08-27 08:00:00   28.20  \n1992-08-27 08:15:00     NaN  \n1992-08-27 08:30:00   28.60  \n1992-08-27 08:45:00   29.40  \n1992-08-27 09:00:00   30.00  \n'];['                     Values\n1992-08-27 07:46:48    28.0  \n1992-08-27 08:00:48    28.2  \n1992-08-27 08:33:48    28.4  \n1992-08-27 08:43:48    28.8  \n1992-08-27 08:48:48    29.0  \n1992-08-27 08:51:48    29.2  \n1992-08-27 08:53:48    29.6  \n1992-08-27 08:56:48    29.8  \n1992-08-27 09:03:48    30.0\n', '                     Values\n1992-08-27 08:00:00    28.2  \n1992-08-27 08:15:00    28.3  \n1992-08-27 08:30:00    28.4  \n1992-08-27 08:45:00    28.8  \n1992-08-27 09:00:00    29.9\n', '                     Values\n1992-08-27 08:00:00   28.20  \n1992-08-27 08:15:00     NaN  \n1992-08-27 08:30:00   28.60  \n1992-08-27 08:45:00   29.40  \n1992-08-27 09:00:00   30.00  \n'];[''];[''];False;['import pandas as pd\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
845;845;845;845;4.0;3;25239958;;1;23;<python><pandas><scikit-learn>;Impute categorical missing values in scikit-learn;10847.0;"[""from sklearn.preprocessing import Imputer\nimp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\nimp.fit(df) \n""]";"[""from sklearn.preprocessing import Imputer\nimp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\nimp.fit(df) \n""]";"[""from sklearn.preprocessing import Imputer\nimp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\nimp.fit(df) \n""]";"[""from sklearn.preprocessing import Imputer\nimp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\nimp.fit(df) \n""]";"[""from sklearn.preprocessing import Imputer\nimp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\nimp.fit(df) \n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nfrom sklearn.preprocessing import Imputer\nimp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\nimp.fit(df) \n""]";True;0;1;"[""No module named 'sklearn'""]";['ImportError'];0;1;"[""No module named 'sklearn'""]";['ImportError'];0;1;"[""No module named 'sklearn'""]";['ImportError']
846;846;846;846;3.0;1;25254016;;1;62;<python><pandas>;Pandas - Get first row value of a given column;90823.0;['  ATime   X   Y   Z   Btime  C   D   E\n0    1.2  2  15   2    1.2  12  25  12\n1    1.4  3  12   1    1.3  13  22  11\n2    1.5  1  10   6    1.4  11  20  16\n3    1.6  2   9  10    1.7  12  29  12\n4    1.9  1   1   9    1.9  11  21  19\n5    2.0  0   0   0    2.0   8  10  11\n6    2.4  0   0   0    2.4  10  12  15\n'];['  ATime   X   Y   Z   Btime  C   D   E\n0    1.2  2  15   2    1.2  12  25  12\n1    1.4  3  12   1    1.3  13  22  11\n2    1.5  1  10   6    1.4  11  20  16\n3    1.6  2   9  10    1.7  12  29  12\n4    1.9  1   1   9    1.9  11  21  19\n5    2.0  0   0   0    2.0   8  10  11\n6    2.4  0   0   0    2.4  10  12  15\n'];['  ATime   X   Y   Z   Btime  C   D   E\n0    1.2  2  15   2    1.2  12  25  12\n1    1.4  3  12   1    1.3  13  22  11\n2    1.5  1  10   6    1.4  11  20  16\n3    1.6  2   9  10    1.7  12  29  12\n4    1.9  1   1   9    1.9  11  21  19\n5    2.0  0   0   0    2.0   8  10  11\n6    2.4  0   0   0    2.4  10  12  15\n'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df_test' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];1;2;"[""name 'df_test' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df_test' is not defined"", 'Sucess']";['NameError', 'Sucess']
847;847;847;847;3.0;0;25284859;;1;15;<matplotlib><pandas><seaborn>;Grouping boxplots in seaborn when input is a DataFrame;10099.0;"[""import seaborn as sns\nimport pandas as pd\ndf = pd.DataFrame(\n[\n[2, 4, 5, 6, 1],\n[4, 5, 6, 7, 2],\n[5, 4, 5, 5, 1],\n[10, 4, 7, 8, 2],\n[9, 3, 4, 6, 2],\n[3, 3, 4, 4, 1]\n], columns=['a1', 'a2', 'a3', 'a4', 'b'])\n\n#Plotting by seaborn\nsns.boxplot(df[['a1','a2', 'a3', 'a4']], groupby=df.b)\nsns.boxplot(df.a1, groupby=df.b)\n""]";"[""import seaborn as sns\nimport pandas as pd\ndf = pd.DataFrame(\n[\n[2, 4, 5, 6, 1],\n[4, 5, 6, 7, 2],\n[5, 4, 5, 5, 1],\n[10, 4, 7, 8, 2],\n[9, 3, 4, 6, 2],\n[3, 3, 4, 4, 1]\n], columns=['a1', 'a2', 'a3', 'a4', 'b'])\n\n#Plotting by seaborn\nsns.boxplot(df[['a1','a2', 'a3', 'a4']], groupby=df.b)\n"", 'sns.boxplot(df.a1, groupby=df.b)\n']";"['pandas dataframe', 'groupby', 'seaborn.boxplot', 'matplotlib', 'seaborn.boxplot', 'groupby', 'seaborn', ""import seaborn as sns\nimport pandas as pd\ndf = pd.DataFrame(\n[\n[2, 4, 5, 6, 1],\n[4, 5, 6, 7, 2],\n[5, 4, 5, 5, 1],\n[10, 4, 7, 8, 2],\n[9, 3, 4, 6, 2],\n[3, 3, 4, 4, 1]\n], columns=['a1', 'a2', 'a3', 'a4', 'b'])\n\n#Plotting by seaborn\nsns.boxplot(df[['a1','a2', 'a3', 'a4']], groupby=df.b)\n"", 'groupby', 'sns.boxplot(df.a1, groupby=df.b)\n']";"[""import seaborn as sns\nimport pandas as pd\n[2, 4, 5, 6, 1],\n[4, 5, 6, 7, 2],\n[5, 4, 5, 5, 1],\n[10, 4, 7, 8, 2],\n[9, 3, 4, 6, 2],\n[3, 3, 4, 4, 1]\n\n#Plotting by seaborn\nsns.boxplot(df[['a1','a2', 'a3', 'a4']], groupby=df.b)\nsns.boxplot(df.a1, groupby=df.b)\n""]";"[""import seaborn as sns\nimport pandas as pd\n[2, 4, 5, 6, 1],\n[4, 5, 6, 7, 2],\n[5, 4, 5, 5, 1],\n[10, 4, 7, 8, 2],\n[9, 3, 4, 6, 2],\n[3, 3, 4, 4, 1]\n\n#Plotting by seaborn\nsns.boxplot(df[['a1','a2', 'a3', 'a4']], groupby=df.b)\nsns.boxplot(df.a1, groupby=df.b)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport seaborn as sns\nimport pandas as pd\n[2, 4, 5, 6, 1],\n[4, 5, 6, 7, 2],\n[5, 4, 5, 5, 1],\n[10, 4, 7, 8, 2],\n[9, 3, 4, 6, 2],\n[3, 3, 4, 4, 1]\n\n#Plotting by seaborn\nsns.boxplot(df[['a1','a2', 'a3', 'a4']], groupby=df.b)\nsns.boxplot(df.a1, groupby=df.b)\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'b'""]";['KeyError']
848;848;848;848;2.0;0;25351968;;1;21;<python><html><pandas>;How to display full (non-truncated) dataframe information in html when converting from pandas dataframe to html?;13484.0;[''];[];['DataFrame.to_html', 'df.head(1)', 'DataFrame.to_html'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'pd' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
849;849;849;849;2.0;0;25386870;;1;25;<python><matplotlib><pandas><multi-index>;Pandas Plotting with Multi-Index;12929.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""No module named 'matplotlib'"", ""name 'summed_group' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'matplotlib'"", ""name 'summed_group' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'matplotlib'"", ""name 'summed_group' is not defined""]";['ImportError', 'NameError']
850;850;850;850;1.0;4;25435229;;1;17;<python><pandas><compare><series>;What happens when you compare 2 pandas Series;1056.0;"[""import pandas as pd\nx = pd.Series([1, 1, 1, 0, 0, 0], index=['a', 'b', 'c', 'd', 'e', 'f'], name='Value')\ny = pd.Series([0, 2, 0, 2, 0, 2], index=['c', 'f', 'a', 'e', 'b', 'd'], name='Value')\n\nx > y\na     True\nb    False\nc     True\nd    False\ne    False\nf    False\nName: Value, dtype: bool\nx > y.reindex_like(x)\na     True\nb     True\nc     True\nd    False\ne    False\nf    False\nName: Value, dtype: bool\nx + y\na    1\nb    1\nc    1\nd    2\ne    2\nf    2\nName: Value, dtype: int64\n""]";"[""import pandas as pd\nx = pd.Series([1, 1, 1, 0, 0, 0], index=['a', 'b', 'c', 'd', 'e', 'f'], name='Value')\ny = pd.Series([0, 2, 0, 2, 0, 2], index=['c', 'f', 'a', 'e', 'b', 'd'], name='Value')\n\nx > y\n"", 'a     True\nb    False\nc     True\nd    False\ne    False\nf    False\nName: Value, dtype: bool\n', 'x > y.reindex_like(x)\n', 'a     True\nb     True\nc     True\nd    False\ne    False\nf    False\nName: Value, dtype: bool\n', 'x + y\n', 'a    1\nb    1\nc    1\nd    2\ne    2\nf    2\nName: Value, dtype: int64\n']";"[""import pandas as pd\nx = pd.Series([1, 1, 1, 0, 0, 0], index=['a', 'b', 'c', 'd', 'e', 'f'], name='Value')\ny = pd.Series([0, 2, 0, 2, 0, 2], index=['c', 'f', 'a', 'e', 'b', 'd'], name='Value')\n\nx > y\n"", 'a     True\nb    False\nc     True\nd    False\ne    False\nf    False\nName: Value, dtype: bool\n', 'x > y.reindex_like(x)\n', 'a     True\nb     True\nc     True\nd    False\ne    False\nf    False\nName: Value, dtype: bool\n', 'x + y\n', 'a    1\nb    1\nc    1\nd    2\ne    2\nf    2\nName: Value, dtype: int64\n']";"[""import pandas as pd\nx = pd.Series([1, 1, 1, 0, 0, 0], index=['a', 'b', 'c', 'd', 'e', 'f'], name='Value')\ny = pd.Series([0, 2, 0, 2, 0, 2], index=['c', 'f', 'a', 'e', 'b', 'd'], name='Value')\n\nx > y\nx > y.reindex_like(x)\nx + y\n""]";"[""import pandas as pd\nx = pd.Series([1, 1, 1, 0, 0, 0], index=['a', 'b', 'c', 'd', 'e', 'f'], name='Value')\ny = pd.Series([0, 2, 0, 2, 0, 2], index=['c', 'f', 'a', 'e', 'b', 'd'], name='Value')\n\nx > y\nx > y.reindex_like(x)\nx + y\n""]";False;"[""import pandas as pd\nimport pandas as pd\nx = pd.Series([1, 1, 1, 0, 0, 0], index=['a', 'b', 'c', 'd', 'e', 'f'], name='Value')\ny = pd.Series([0, 2, 0, 2, 0, 2], index=['c', 'f', 'a', 'e', 'b', 'd'], name='Value')\n\nx > y\nx > y.reindex_like(x)\nx + y\n""]";False;0;0;[];[];0;0;[];[];0;0;[];[]
851;851;851;851;3.0;2;25440008;;1;12;<python><list><numpy><pandas><dataframe>;python pandas flatten a dataframe to a list;11919.0;"[""import pandas\na=[['1/2/2014', 'a', '6', 'z1'], \n   ['1/2/2014', 'a', '3', 'z1'], \n   ['1/3/2014', 'c', '1', 'x3'],\n   ]\ndf = pandas.DataFrame.from_records(a[1:],columns=a[0])\n""]";"[""import pandas\na=[['1/2/2014', 'a', '6', 'z1'], \n   ['1/2/2014', 'a', '3', 'z1'], \n   ['1/3/2014', 'c', '1', 'x3'],\n   ]\ndf = pandas.DataFrame.from_records(a[1:],columns=a[0])\n""]";"[""import pandas\na=[['1/2/2014', 'a', '6', 'z1'], \n   ['1/2/2014', 'a', '3', 'z1'], \n   ['1/3/2014', 'c', '1', 'x3'],\n   ]\ndf = pandas.DataFrame.from_records(a[1:],columns=a[0])\n"", ""['1/2/2014', 'a', '6', 'z1', '1/2/2014', 'a', '3', 'z1','1/3/2014', 'c', '1', 'x3']"", 'extend']";['import pandas\ndf = pandas.DataFrame.from_records(a[1:],columns=a[0])\n'];['import pandas\ndf = pandas.DataFrame.from_records(a[1:],columns=a[0])\n'];False;['import pandas as pd\nimport pandas\ndf = pandas.DataFrame.from_records(a[1:],columns=a[0])\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
852;852;852;852;2.0;1;25447700;;1;18;<python><matplotlib><plot><pandas><dataframe>;Annotate bars with values on Pandas bar plots;9512.0;"["">>> df=pd.DataFrame({'A':np.random.rand(2),'B':np.random.rand(2)},index=['value1','value2'] )         \n>>> df\n                 A         B\n  value1  0.440922  0.911800\n  value2  0.588242  0.797366\n>>> ax = df.plot(kind='bar') \n>>> for idx, label in enumerate(list(df.index)): \n        for acc in df.columns:\n            value = np.round(df.ix[idx][acc],decimals=2)\n            ax.annotate(value,\n                        (idx, value),\n                         xytext=(0, 15), \n                         textcoords='offset points')\n""]";"["">>> df=pd.DataFrame({'A':np.random.rand(2),'B':np.random.rand(2)},index=['value1','value2'] )         \n>>> df\n                 A         B\n  value1  0.440922  0.911800\n  value2  0.588242  0.797366\n"", "">>> ax = df.plot(kind='bar') \n>>> for idx, label in enumerate(list(df.index)): \n        for acc in df.columns:\n            value = np.round(df.ix[idx][acc],decimals=2)\n            ax.annotate(value,\n                        (idx, value),\n                         xytext=(0, 15), \n                         textcoords='offset points')\n""]";"["">>> df=pd.DataFrame({'A':np.random.rand(2),'B':np.random.rand(2)},index=['value1','value2'] )         \n>>> df\n                 A         B\n  value1  0.440922  0.911800\n  value2  0.588242  0.797366\n"", "">>> ax = df.plot(kind='bar') \n>>> for idx, label in enumerate(list(df.index)): \n        for acc in df.columns:\n            value = np.round(df.ix[idx][acc],decimals=2)\n            ax.annotate(value,\n                        (idx, value),\n                         xytext=(0, 15), \n                         textcoords='offset points')\n""]";[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'pd' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'np' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'np' is not defined""]";['Sucess', 'NameError']
853;853;853;853;1.0;2;25478528;;1;11;<python><loops><pandas><explicit>;Updating value in iterrow for pandas;12347.0;"[""for index, row in rche_df.iterrows():\n    if isinstance(row.wgs1984_latitude, float):\n        row = row.copy()\n        target = row.address_chi        \n        dict_temp = geocoding(target)\n        row.wgs1984_latitude = dict_temp['lat']\n        row.wgs1984_longitude = dict_temp['long']\n""]";"[""for index, row in rche_df.iterrows():\n    if isinstance(row.wgs1984_latitude, float):\n        row = row.copy()\n        target = row.address_chi        \n        dict_temp = geocoding(target)\n        row.wgs1984_latitude = dict_temp['lat']\n        row.wgs1984_longitude = dict_temp['long']\n""]";"['selenium', ""for index, row in rche_df.iterrows():\n    if isinstance(row.wgs1984_latitude, float):\n        row = row.copy()\n        target = row.address_chi        \n        dict_temp = geocoding(target)\n        row.wgs1984_latitude = dict_temp['lat']\n        row.wgs1984_longitude = dict_temp['long']\n"", 'lambda']";[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
854;854;854;854;1.0;0;25479607;;1;12;<python><pandas><row><minimum><calculated-columns>;Pandas min() of selected row and columns;15665.0;"[""    A0      A1      A2      B0      B1      B2      C0      C1\n0   0.84    0.47    0.55    0.46    0.76    0.42    0.24    0.75\n1   0.43    0.47    0.93    0.39    0.58    0.83    0.35    0.39\n2   0.12    0.17    0.35    0.00    0.19    0.22    0.93    0.73\n3   0.95    0.56    0.84    0.74    0.52    0.51    0.28    0.03\n4   0.73    0.19    0.88    0.51    0.73    0.69    0.74    0.61\n5   0.18    0.46    0.62    0.84    0.68    0.17    0.02    0.53\n6   0.38    0.55    0.80    0.87    0.01    0.88    0.56    0.72\n    A0      A1      A2      B0      B1      B2      C0      C1      Minimum\n0   0.84    0.47    0.55    0.46    0.76    0.42    0.24    0.75    0.42\n1   0.43    0.47    0.93    0.39    0.58    0.83    0.35    0.39    0.39\n2   0.12    0.17    0.35    0.00    0.19    0.22    0.93    0.73    0.00\n3   0.95    0.56    0.84    0.74    0.52    0.51    0.28    0.03    0.51\n4   0.73    0.19    0.88    0.51    0.73    0.69    0.74    0.61    0.51\n5   0.18    0.46    0.62    0.84    0.68    0.17    0.02    0.53    0.17\n6   0.38    0.55    0.80    0.87    0.01    0.88    0.56    0.72    0.01\nfor i in range(0,2):\n    df['Minimum'] = df.loc[0,'B'+str(i)].min()\n""]";"['    A0      A1      A2      B0      B1      B2      C0      C1\n0   0.84    0.47    0.55    0.46    0.76    0.42    0.24    0.75\n1   0.43    0.47    0.93    0.39    0.58    0.83    0.35    0.39\n2   0.12    0.17    0.35    0.00    0.19    0.22    0.93    0.73\n3   0.95    0.56    0.84    0.74    0.52    0.51    0.28    0.03\n4   0.73    0.19    0.88    0.51    0.73    0.69    0.74    0.61\n5   0.18    0.46    0.62    0.84    0.68    0.17    0.02    0.53\n6   0.38    0.55    0.80    0.87    0.01    0.88    0.56    0.72\n', '    A0      A1      A2      B0      B1      B2      C0      C1      Minimum\n0   0.84    0.47    0.55    0.46    0.76    0.42    0.24    0.75    0.42\n1   0.43    0.47    0.93    0.39    0.58    0.83    0.35    0.39    0.39\n2   0.12    0.17    0.35    0.00    0.19    0.22    0.93    0.73    0.00\n3   0.95    0.56    0.84    0.74    0.52    0.51    0.28    0.03    0.51\n4   0.73    0.19    0.88    0.51    0.73    0.69    0.74    0.61    0.51\n5   0.18    0.46    0.62    0.84    0.68    0.17    0.02    0.53    0.17\n6   0.38    0.55    0.80    0.87    0.01    0.88    0.56    0.72    0.01\n', ""for i in range(0,2):\n    df['Minimum'] = df.loc[0,'B'+str(i)].min()\n""]";"['    A0      A1      A2      B0      B1      B2      C0      C1\n0   0.84    0.47    0.55    0.46    0.76    0.42    0.24    0.75\n1   0.43    0.47    0.93    0.39    0.58    0.83    0.35    0.39\n2   0.12    0.17    0.35    0.00    0.19    0.22    0.93    0.73\n3   0.95    0.56    0.84    0.74    0.52    0.51    0.28    0.03\n4   0.73    0.19    0.88    0.51    0.73    0.69    0.74    0.61\n5   0.18    0.46    0.62    0.84    0.68    0.17    0.02    0.53\n6   0.38    0.55    0.80    0.87    0.01    0.88    0.56    0.72\n', '    A0      A1      A2      B0      B1      B2      C0      C1      Minimum\n0   0.84    0.47    0.55    0.46    0.76    0.42    0.24    0.75    0.42\n1   0.43    0.47    0.93    0.39    0.58    0.83    0.35    0.39    0.39\n2   0.12    0.17    0.35    0.00    0.19    0.22    0.93    0.73    0.00\n3   0.95    0.56    0.84    0.74    0.52    0.51    0.28    0.03    0.51\n4   0.73    0.19    0.88    0.51    0.73    0.69    0.74    0.61    0.51\n5   0.18    0.46    0.62    0.84    0.68    0.17    0.02    0.53    0.17\n6   0.38    0.55    0.80    0.87    0.01    0.88    0.56    0.72    0.01\n', ""for i in range(0,2):\n    df['Minimum'] = df.loc[0,'B'+str(i)].min()\n""]";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"['""None of [[\'B0\', \'B1\', \'B2\']] are in the [columns]""']";['KeyError']
855;855;855;855;2.0;0;25493625;;1;22;<python><join><pandas><vlookup>;vlookup in Pandas using join;21751.0;"[""Example1\nsku loc flag  \n122  61 True \n123  61 True\n113  62 True \n122  62 True \n123  62 False\n122  63 False\n301  63 True \n\nExample2 \nsku dept \n113 a\n122 b\n123 b\n301 c \nExample3\nsku loc flag   dept  \n122  61 True   b\n123  61 True   b\n113  62 True   a\n122  62 True   b\n123  62 False  b\n122  63 False  b\n301  63 True   c\n\nBoth \ndf_Example1.join(df_Example2,lsuffix='_ProdHier')\ndf_Example1.join(df_Example2,how='outer',lsuffix='_ProdHier')\n""]";"['Example1\nsku loc flag  \n122  61 True \n123  61 True\n113  62 True \n122  62 True \n123  62 False\n122  63 False\n301  63 True \n\nExample2 \nsku dept \n113 a\n122 b\n123 b\n301 c \n', ""Example3\nsku loc flag   dept  \n122  61 True   b\n123  61 True   b\n113  62 True   a\n122  62 True   b\n123  62 False  b\n122  63 False  b\n301  63 True   c\n\nBoth \ndf_Example1.join(df_Example2,lsuffix='_ProdHier')\ndf_Example1.join(df_Example2,how='outer',lsuffix='_ProdHier')\n""]";"['Example1\nsku loc flag  \n122  61 True \n123  61 True\n113  62 True \n122  62 True \n123  62 False\n122  63 False\n301  63 True \n\nExample2 \nsku dept \n113 a\n122 b\n123 b\n301 c \n', ""Example3\nsku loc flag   dept  \n122  61 True   b\n123  61 True   b\n113  62 True   a\n122  62 True   b\n123  62 False  b\n122  63 False  b\n301  63 True   c\n\nBoth \ndf_Example1.join(df_Example2,lsuffix='_ProdHier')\ndf_Example1.join(df_Example2,how='outer',lsuffix='_ProdHier')\n""]";"[""Example1\n\nExample2 \nExample3\n\nBoth \ndf_Example1.join(df_Example2,lsuffix='_ProdHier')\ndf_Example1.join(df_Example2,how='outer',lsuffix='_ProdHier')\n""]";"[""Example1\n\nExample2 \nExample3\n\nBoth \ndf_Example1.join(df_Example2,lsuffix='_ProdHier')\ndf_Example1.join(df_Example2,how='outer',lsuffix='_ProdHier')\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nExample1\n\nExample2 \nExample3\n\nBoth \ndf_Example1.join(df_Example2,lsuffix='_ProdHier')\ndf_Example1.join(df_Example2,how='outer',lsuffix='_ProdHier')\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'sku'""]";['KeyError']
856;856;856;856;2.0;6;25508510;;1;15;<python><pandas>;Fastest way to parse large CSV files in Pandas;11193.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
857;857;857;857;1.0;6;25537399;;1;21;<python><pandas><scipy><statsmodels><anova>;ANOVA in python using pandas dataframe with statsmodels or scipy?;10835.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""No module named 'statsmodels'""]";['ImportError'];0;1;"[""No module named 'statsmodels'""]";['ImportError'];0;1;"[""No module named 'statsmodels'""]";['ImportError']
858;858;858;858;4.0;4;25577352;;1;17;<python><pandas><series><cdf>;Plotting CDF of a pandas series in python;14521.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""No module named 'matplotlib'"", ""No module named 'matplotlib'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'matplotlib'"", ""No module named 'matplotlib'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'matplotlib'"", ""No module named 'matplotlib'""]";['ImportError', 'ImportError']
859;859;859;859;1.0;6;25610592;;1;17;<python><pandas><types>;How to set dtypes by column in pandas DataFrame;5800.0;"[""myarray = np.random.randint(0,5,size=(2,2))\nmydf = pd.DataFrame(myarray,columns=['a','b'], dtype=[float,int])\nmydf.dtypes\nTypeError: data type not understood\nmydf = pd.DataFrame(myarray,columns=['a','b'], dtype={'a': int})\n\nTypeError: object of type 'type' has no len()\n""]";"[""myarray = np.random.randint(0,5,size=(2,2))\nmydf = pd.DataFrame(myarray,columns=['a','b'], dtype=[float,int])\nmydf.dtypes\n"", 'TypeError: data type not understood\n', ""mydf = pd.DataFrame(myarray,columns=['a','b'], dtype={'a': int})\n\nTypeError: object of type 'type' has no len()\n""]";"[""myarray = np.random.randint(0,5,size=(2,2))\nmydf = pd.DataFrame(myarray,columns=['a','b'], dtype=[float,int])\nmydf.dtypes\n"", 'TypeError: data type not understood\n', ""mydf = pd.DataFrame(myarray,columns=['a','b'], dtype={'a': int})\n\nTypeError: object of type 'type' has no len()\n"", 'dtype=(float,int)']";"[""myarray = np.random.randint(0,5,size=(2,2))\nmydf = pd.DataFrame(myarray,columns=['a','b'], dtype=[float,int])\nmydf.dtypes\nmydf = pd.DataFrame(myarray,columns=['a','b'], dtype={'a': int})\n\n""]";"[""import pandas as pd\nmyarray = np.random.randint(0,5,size=(2,2))\nmydf = pd.DataFrame(myarray,columns=['a','b'], dtype=[float,int])\nmydf.dtypes\nmydf = pd.DataFrame(myarray,columns=['a','b'], dtype={'a': int})\n\n""]";True;"[""import pandas as pd\nmyarray = np.random.randint(0,5,size=(2,2))\nmydf = pd.DataFrame(myarray,columns=['a','b'], dtype=[float,int])\nmydf.dtypes\nmydf = pd.DataFrame(myarray,columns=['a','b'], dtype={'a': int})\n\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
860;860;860;860;1.0;2;25628496;;1;13;<python><pandas><ipython><pycharm>;Getting wider output in PyCharm's built-in console;4476.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
861;861;861;861;1.0;16;25631076;;1;46;<python><performance><numpy><pandas>;Is this the fastest way to group in Pandas?;2357.0;"['$ python3\nPython 3.4.0 (default, Apr 11 2014, 13:05:11) \n[GCC 4.8.2] on linux\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n>>> import pandas as pd\n>>> import numpy as np\n>>> import timeit\n>>> pd.__version__\n\'0.14.1\'\n\ndef randChar(f, numGrp, N) :\n   things = [f%x for x in range(numGrp)]\n   return [things[x] for x in np.random.choice(numGrp, N)]\n\ndef randFloat(numGrp, N) :\n   things = [round(100*np.random.random(),4) for x in range(numGrp)]\n   return [things[x] for x in np.random.choice(numGrp, N)]\n\nN=int(1e8)\nK=100\nDF = pd.DataFrame({\n  \'id1\' : randChar(""id%03d"", K, N),       # large groups (char)\n  \'id2\' : randChar(""id%03d"", K, N),       # large groups (char)\n  \'id3\' : randChar(""id%010d"", N//K, N),   # small groups (char)\n  \'id4\' : np.random.choice(K, N),         # large groups (int)\n  \'id5\' : np.random.choice(K, N),         # large groups (int)\n  \'id6\' : np.random.choice(N//K, N),      # small groups (int)            \n  \'v1\' :  np.random.choice(5, N),         # int in range [1,5]\n  \'v2\' :  np.random.choice(5, N),         # int in range [1,5]\n  \'v3\' :  randFloat(100,N)                # numeric e.g. 23.5749\n})\n>>> timeit.Timer(""DF.groupby([\'id1\']).agg({\'v1\':\'sum\'})""                            ,""from __main__ import DF"").timeit(1)\n5.604133386000285\n>>> timeit.Timer(""DF.groupby([\'id1\']).agg({\'v1\':\'sum\'})""                            ,""from __main__ import DF"").timeit(1)\n5.505057081000359\n\n>>> timeit.Timer(""DF.groupby([\'id1\',\'id2\']).agg({\'v1\':\'sum\'})""                      ,""from __main__ import DF"").timeit(1)\n14.232032927000091\n>>> timeit.Timer(""DF.groupby([\'id1\',\'id2\']).agg({\'v1\':\'sum\'})""                      ,""from __main__ import DF"").timeit(1)\n14.242601240999647\n\n>>> timeit.Timer(""DF.groupby([\'id3\']).agg({\'v1\':\'sum\', \'v3\':\'mean\'})""               ,""from __main__ import DF"").timeit(1)\n22.87025260900009\n>>> timeit.Timer(""DF.groupby([\'id3\']).agg({\'v1\':\'sum\', \'v3\':\'mean\'})""               ,""from __main__ import DF"").timeit(1)\n22.393589012999655\n\n>>> timeit.Timer(""DF.groupby([\'id4\']).agg({\'v1\':\'mean\', \'v2\':\'mean\', \'v3\':\'mean\'})"" ,""from __main__ import DF"").timeit(1)\n2.9725865330001398\n>>> timeit.Timer(""DF.groupby([\'id4\']).agg({\'v1\':\'mean\', \'v2\':\'mean\', \'v3\':\'mean\'})"" ,""from __main__ import DF"").timeit(1)\n2.9683854739996605\n\n>>> timeit.Timer(""DF.groupby([\'id6\']).agg({\'v1\':\'sum\', \'v2\':\'sum\', \'v3\':\'sum\'})""    ,""from __main__ import DF"").timeit(1)\n12.776488024999708\n>>> timeit.Timer(""DF.groupby([\'id6\']).agg({\'v1\':\'sum\', \'v2\':\'sum\', \'v3\':\'sum\'})""    ,""from __main__ import DF"").timeit(1)\n13.558292575999076\n$ lscpu\nArchitecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                32\nOn-line CPU(s) list:   0-31\nThread(s) per core:    2\nCore(s) per socket:    8\nSocket(s):             2\nNUMA node(s):          2\nVendor ID:             GenuineIntel\nCPU family:            6\nModel:                 62\nStepping:              4\nCPU MHz:               2500.048\nBogoMIPS:              5066.38\nHypervisor vendor:     Xen\nVirtualization type:   full\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              256K\nL3 cache:              25600K\nNUMA node0 CPU(s):     0-7,16-23\nNUMA node1 CPU(s):     8-15,24-31\n\n$ free -h\n             total       used       free     shared    buffers     cached\nMem:          240G        74G       166G       372K        33M       550M\n-/+ buffers/cache:        73G       166G\nSwap:           0B         0B         0B\n']";"['$ python3\nPython 3.4.0 (default, Apr 11 2014, 13:05:11) \n[GCC 4.8.2] on linux\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n>>> import pandas as pd\n>>> import numpy as np\n>>> import timeit\n>>> pd.__version__\n\'0.14.1\'\n\ndef randChar(f, numGrp, N) :\n   things = [f%x for x in range(numGrp)]\n   return [things[x] for x in np.random.choice(numGrp, N)]\n\ndef randFloat(numGrp, N) :\n   things = [round(100*np.random.random(),4) for x in range(numGrp)]\n   return [things[x] for x in np.random.choice(numGrp, N)]\n\nN=int(1e8)\nK=100\nDF = pd.DataFrame({\n  \'id1\' : randChar(""id%03d"", K, N),       # large groups (char)\n  \'id2\' : randChar(""id%03d"", K, N),       # large groups (char)\n  \'id3\' : randChar(""id%010d"", N//K, N),   # small groups (char)\n  \'id4\' : np.random.choice(K, N),         # large groups (int)\n  \'id5\' : np.random.choice(K, N),         # large groups (int)\n  \'id6\' : np.random.choice(N//K, N),      # small groups (int)            \n  \'v1\' :  np.random.choice(5, N),         # int in range [1,5]\n  \'v2\' :  np.random.choice(5, N),         # int in range [1,5]\n  \'v3\' :  randFloat(100,N)                # numeric e.g. 23.5749\n})\n', '>>> timeit.Timer(""DF.groupby([\'id1\']).agg({\'v1\':\'sum\'})""                            ,""from __main__ import DF"").timeit(1)\n5.604133386000285\n>>> timeit.Timer(""DF.groupby([\'id1\']).agg({\'v1\':\'sum\'})""                            ,""from __main__ import DF"").timeit(1)\n5.505057081000359\n\n>>> timeit.Timer(""DF.groupby([\'id1\',\'id2\']).agg({\'v1\':\'sum\'})""                      ,""from __main__ import DF"").timeit(1)\n14.232032927000091\n>>> timeit.Timer(""DF.groupby([\'id1\',\'id2\']).agg({\'v1\':\'sum\'})""                      ,""from __main__ import DF"").timeit(1)\n14.242601240999647\n\n>>> timeit.Timer(""DF.groupby([\'id3\']).agg({\'v1\':\'sum\', \'v3\':\'mean\'})""               ,""from __main__ import DF"").timeit(1)\n22.87025260900009\n>>> timeit.Timer(""DF.groupby([\'id3\']).agg({\'v1\':\'sum\', \'v3\':\'mean\'})""               ,""from __main__ import DF"").timeit(1)\n22.393589012999655\n\n>>> timeit.Timer(""DF.groupby([\'id4\']).agg({\'v1\':\'mean\', \'v2\':\'mean\', \'v3\':\'mean\'})"" ,""from __main__ import DF"").timeit(1)\n2.9725865330001398\n>>> timeit.Timer(""DF.groupby([\'id4\']).agg({\'v1\':\'mean\', \'v2\':\'mean\', \'v3\':\'mean\'})"" ,""from __main__ import DF"").timeit(1)\n2.9683854739996605\n\n>>> timeit.Timer(""DF.groupby([\'id6\']).agg({\'v1\':\'sum\', \'v2\':\'sum\', \'v3\':\'sum\'})""    ,""from __main__ import DF"").timeit(1)\n12.776488024999708\n>>> timeit.Timer(""DF.groupby([\'id6\']).agg({\'v1\':\'sum\', \'v2\':\'sum\', \'v3\':\'sum\'})""    ,""from __main__ import DF"").timeit(1)\n13.558292575999076\n', '$ lscpu\nArchitecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                32\nOn-line CPU(s) list:   0-31\nThread(s) per core:    2\nCore(s) per socket:    8\nSocket(s):             2\nNUMA node(s):          2\nVendor ID:             GenuineIntel\nCPU family:            6\nModel:                 62\nStepping:              4\nCPU MHz:               2500.048\nBogoMIPS:              5066.38\nHypervisor vendor:     Xen\nVirtualization type:   full\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              256K\nL3 cache:              25600K\nNUMA node0 CPU(s):     0-7,16-23\nNUMA node1 CPU(s):     8-15,24-31\n\n$ free -h\n             total       used       free     shared    buffers     cached\nMem:          240G        74G       166G       372K        33M       550M\n-/+ buffers/cache:        73G       166G\nSwap:           0B         0B         0B\n']";"['$ python3\nPython 3.4.0 (default, Apr 11 2014, 13:05:11) \n[GCC 4.8.2] on linux\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n>>> import pandas as pd\n>>> import numpy as np\n>>> import timeit\n>>> pd.__version__\n\'0.14.1\'\n\ndef randChar(f, numGrp, N) :\n   things = [f%x for x in range(numGrp)]\n   return [things[x] for x in np.random.choice(numGrp, N)]\n\ndef randFloat(numGrp, N) :\n   things = [round(100*np.random.random(),4) for x in range(numGrp)]\n   return [things[x] for x in np.random.choice(numGrp, N)]\n\nN=int(1e8)\nK=100\nDF = pd.DataFrame({\n  \'id1\' : randChar(""id%03d"", K, N),       # large groups (char)\n  \'id2\' : randChar(""id%03d"", K, N),       # large groups (char)\n  \'id3\' : randChar(""id%010d"", N//K, N),   # small groups (char)\n  \'id4\' : np.random.choice(K, N),         # large groups (int)\n  \'id5\' : np.random.choice(K, N),         # large groups (int)\n  \'id6\' : np.random.choice(N//K, N),      # small groups (int)            \n  \'v1\' :  np.random.choice(5, N),         # int in range [1,5]\n  \'v2\' :  np.random.choice(5, N),         # int in range [1,5]\n  \'v3\' :  randFloat(100,N)                # numeric e.g. 23.5749\n})\n', 'timeit(2)', 'htop', '>>> timeit.Timer(""DF.groupby([\'id1\']).agg({\'v1\':\'sum\'})""                            ,""from __main__ import DF"").timeit(1)\n5.604133386000285\n>>> timeit.Timer(""DF.groupby([\'id1\']).agg({\'v1\':\'sum\'})""                            ,""from __main__ import DF"").timeit(1)\n5.505057081000359\n\n>>> timeit.Timer(""DF.groupby([\'id1\',\'id2\']).agg({\'v1\':\'sum\'})""                      ,""from __main__ import DF"").timeit(1)\n14.232032927000091\n>>> timeit.Timer(""DF.groupby([\'id1\',\'id2\']).agg({\'v1\':\'sum\'})""                      ,""from __main__ import DF"").timeit(1)\n14.242601240999647\n\n>>> timeit.Timer(""DF.groupby([\'id3\']).agg({\'v1\':\'sum\', \'v3\':\'mean\'})""               ,""from __main__ import DF"").timeit(1)\n22.87025260900009\n>>> timeit.Timer(""DF.groupby([\'id3\']).agg({\'v1\':\'sum\', \'v3\':\'mean\'})""               ,""from __main__ import DF"").timeit(1)\n22.393589012999655\n\n>>> timeit.Timer(""DF.groupby([\'id4\']).agg({\'v1\':\'mean\', \'v2\':\'mean\', \'v3\':\'mean\'})"" ,""from __main__ import DF"").timeit(1)\n2.9725865330001398\n>>> timeit.Timer(""DF.groupby([\'id4\']).agg({\'v1\':\'mean\', \'v2\':\'mean\', \'v3\':\'mean\'})"" ,""from __main__ import DF"").timeit(1)\n2.9683854739996605\n\n>>> timeit.Timer(""DF.groupby([\'id6\']).agg({\'v1\':\'sum\', \'v2\':\'sum\', \'v3\':\'sum\'})""    ,""from __main__ import DF"").timeit(1)\n12.776488024999708\n>>> timeit.Timer(""DF.groupby([\'id6\']).agg({\'v1\':\'sum\', \'v2\':\'sum\', \'v3\':\'sum\'})""    ,""from __main__ import DF"").timeit(1)\n13.558292575999076\n', '$ lscpu\nArchitecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                32\nOn-line CPU(s) list:   0-31\nThread(s) per core:    2\nCore(s) per socket:    8\nSocket(s):             2\nNUMA node(s):          2\nVendor ID:             GenuineIntel\nCPU family:            6\nModel:                 62\nStepping:              4\nCPU MHz:               2500.048\nBogoMIPS:              5066.38\nHypervisor vendor:     Xen\nVirtualization type:   full\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              256K\nL3 cache:              25600K\nNUMA node0 CPU(s):     0-7,16-23\nNUMA node1 CPU(s):     8-15,24-31\n\n$ free -h\n             total       used       free     shared    buffers     cached\nMem:          240G        74G       166G       372K        33M       550M\n-/+ buffers/cache:        73G       166G\nSwap:           0B         0B         0B\n', 'randChar', 'mtrand.RandomState.choice']";"[""'0.14.1'\n\n\n\nN=int(1e8)\nK=100\n5.604133386000285\n5.505057081000359\n\n14.232032927000091\n14.242601240999647\n\n22.87025260900009\n22.393589012999655\n\n2.9725865330001398\n2.9683854739996605\n\n12.776488024999708\n13.558292575999076\n\n""]";"[""'0.14.1'\n\n\n\nN=int(1e8)\nK=100\n5.604133386000285\n5.505057081000359\n\n14.232032927000091\n14.242601240999647\n\n22.87025260900009\n22.393589012999655\n\n2.9725865330001398\n2.9683854739996605\n\n12.776488024999708\n13.558292575999076\n\n""]";False;"[""import pandas as pd\n'0.14.1'\n\n\n\nN=int(1e8)\nK=100\n5.604133386000285\n5.505057081000359\n\n14.232032927000091\n14.242601240999647\n\n22.87025260900009\n22.393589012999655\n\n2.9725865330001398\n2.9683854739996605\n\n12.776488024999708\n13.558292575999076\n\n""]";False;0;0;[];[];0;0;[];[];0;0;[];[]
862;862;862;862;2.0;1;25646200;;1;27;<python><pandas><timedelta>;Python: Convert timedelta to int in a dataframe;31951.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'td' is not defined"", ""name 'td' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'td' is not defined"", ""name 'td' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'td' is not defined"", ""name 'td' is not defined""]";['NameError', 'NameError']
863;863;863;863;3.0;0;25698710;;1;19;<python><replace><pandas><dataframe>;Replace all occurrences of a string in a pandas dataframe (Python);36857.0;"['df[\'columnname1\'] = df[\'columnname1\'].str.replace(""\\n"",""<br>"")\ndf[\'columnname2\'] = df[\'columnname2\'].str.replace(""\\n"",""<br>"")\ndf[\'columnname3\'] = df[\'columnname3\'].str.replace(""\\n"",""<br>"")\n...\ndf[\'columnname20\'] = df[\'columnname20\'].str.replace(""\\n"",""<br>"")\ndf = df.replace(""\\n"",""<br>"")\n']";"['df[\'columnname1\'] = df[\'columnname1\'].str.replace(""\\n"",""<br>"")\ndf[\'columnname2\'] = df[\'columnname2\'].str.replace(""\\n"",""<br>"")\ndf[\'columnname3\'] = df[\'columnname3\'].str.replace(""\\n"",""<br>"")\n...\ndf[\'columnname20\'] = df[\'columnname20\'].str.replace(""\\n"",""<br>"")\n', 'df = df.replace(""\\n"",""<br>"")\n']";"['df[\'columnname1\'] = df[\'columnname1\'].str.replace(""\\n"",""<br>"")\ndf[\'columnname2\'] = df[\'columnname2\'].str.replace(""\\n"",""<br>"")\ndf[\'columnname3\'] = df[\'columnname3\'].str.replace(""\\n"",""<br>"")\n...\ndf[\'columnname20\'] = df[\'columnname20\'].str.replace(""\\n"",""<br>"")\n', 'df = df.replace(""\\n"",""<br>"")\n']";"['df[\'columnname1\'] = df[\'columnname1\'].str.replace(""\\n"",""<br>"")\ndf[\'columnname2\'] = df[\'columnname2\'].str.replace(""\\n"",""<br>"")\ndf[\'columnname3\'] = df[\'columnname3\'].str.replace(""\\n"",""<br>"")\n...\ndf[\'columnname20\'] = df[\'columnname20\'].str.replace(""\\n"",""<br>"")\ndf = df.replace(""\\n"",""<br>"")\n']";"['df[\'columnname1\'] = df[\'columnname1\'].str.replace(""\\n"",""<br>"")\ndf[\'columnname2\'] = df[\'columnname2\'].str.replace(""\\n"",""<br>"")\ndf[\'columnname3\'] = df[\'columnname3\'].str.replace(""\\n"",""<br>"")\n...\ndf[\'columnname20\'] = df[\'columnname20\'].str.replace(""\\n"",""<br>"")\ndf = df.replace(""\\n"",""<br>"")\n']";False;"['import pandas as pd\ndf[\'columnname1\'] = df[\'columnname1\'].str.replace(""\\n"",""<br>"")\ndf[\'columnname2\'] = df[\'columnname2\'].str.replace(""\\n"",""<br>"")\ndf[\'columnname3\'] = df[\'columnname3\'].str.replace(""\\n"",""<br>"")\n...\ndf[\'columnname20\'] = df[\'columnname20\'].str.replace(""\\n"",""<br>"")\ndf = df.replace(""\\n"",""<br>"")\n']";False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
864;864;864;864;4.0;0;25699439;;1;15;<python><pandas><parallel-processing><ipython>;How to iterate over consecutive chunks of Pandas dataframe efficiently;10644.0;['# Generate a number from 0-9 for each row, indicating which tenth of the DF it belongs to\nmax_idx = dataframe.index.max()\ntenths = ((10 * dataframe.index) / (1 + max_idx)).astype(np.uint32)\n\n# Use this value to perform a groupby, yielding 10 consecutive chunks\ngroups = [g[1] for g in dataframe.groupby(tenths)]\n\n# Process chunks in parallel\nresults = dview.map_sync(my_function, groups)\n'];['# Generate a number from 0-9 for each row, indicating which tenth of the DF it belongs to\nmax_idx = dataframe.index.max()\ntenths = ((10 * dataframe.index) / (1 + max_idx)).astype(np.uint32)\n\n# Use this value to perform a groupby, yielding 10 consecutive chunks\ngroups = [g[1] for g in dataframe.groupby(tenths)]\n\n# Process chunks in parallel\nresults = dview.map_sync(my_function, groups)\n'];['# Generate a number from 0-9 for each row, indicating which tenth of the DF it belongs to\nmax_idx = dataframe.index.max()\ntenths = ((10 * dataframe.index) / (1 + max_idx)).astype(np.uint32)\n\n# Use this value to perform a groupby, yielding 10 consecutive chunks\ngroups = [g[1] for g in dataframe.groupby(tenths)]\n\n# Process chunks in parallel\nresults = dview.map_sync(my_function, groups)\n'];['# Generate a number from 0-9 for each row, indicating which tenth of the DF it belongs to\nmax_idx = dataframe.index.max()\ntenths = ((10 * dataframe.index) / (1 + max_idx)).astype(np.uint32)\n\n# Use this value to perform a groupby, yielding 10 consecutive chunks\ngroups = [g[1] for g in dataframe.groupby(tenths)]\n\n# Process chunks in parallel\nresults = dview.map_sync(my_function, groups)\n'];['# Generate a number from 0-9 for each row, indicating which tenth of the DF it belongs to\nmax_idx = dataframe.index.max()\ntenths = ((10 * dataframe.index) / (1 + max_idx)).astype(np.uint32)\n\n# Use this value to perform a groupby, yielding 10 consecutive chunks\ngroups = [g[1] for g in dataframe.groupby(tenths)]\n\n# Process chunks in parallel\nresults = dview.map_sync(my_function, groups)\n'];False;['import pandas as pd\ndata = pd.DataFrame()\n# Generate a number from 0-9 for each row, indicating which tenth of the DF it belongs to\nmax_idx = dataframe.index.max()\ntenths = ((10 * dataframe.index) / (1 + max_idx)).astype(np.uint32)\n\n# Use this value to perform a groupby, yielding 10 consecutive chunks\ngroups = [g[1] for g in dataframe.groupby(tenths)]\n\n# Process chunks in parallel\nresults = dview.map_sync(my_function, groups)\n'];True;2;3;"['Sucess', 'Sucess', ""No module named 'blaze'""]";['Sucess', 'Sucess', 'ImportError'];2;3;"['Sucess', 'Sucess', ""No module named 'blaze'""]";['Sucess', 'Sucess', 'ImportError'];2;3;"['Sucess', 'Sucess', ""No module named 'blaze'""]";['Sucess', 'Sucess', 'ImportError']
865;865;865;865;2.0;0;25748683;;1;35;<python><pandas><dataframe><sum>;Pandas: sum DataFrame rows for given columns;88000.0;"[""import pandas as pd\ndf = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})\ndf['e'] = df[['a','b','d']].map(sum)\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})\n"", ""df['e'] = df[['a','b','d']].map(sum)\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})\n"", ""df['e'] = df[['a','b','d']].map(sum)\n"", ""['a','b','d']"", 'df']";"[""import pandas as pd\ndf = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})\ndf['e'] = df[['a','b','d']].map(sum)\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})\ndf['e'] = df[['a','b','d']].map(sum)\n""]";False;"[""import pandas as pd\nimport pandas as pd\ndf = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})\ndf['e'] = df[['a','b','d']].map(sum)\n""]";False;0;2;"[""name 'df' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""'DataFrame' object has no attribute 'a'"", 'Sucess']";['AttributeError', 'Sucess']
866;866;866;866;2.0;0;25773245;;1;58;<python><arrays><pandas><numpy><dataframe>;"Ambiguity in Pandas Dataframe / Numpy Array ""axis"" definition";11210.0;"['>>> df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]], columns=[""col1"", ""col2"", ""col3"", ""col4""])\n>>> df\n   col1  col2  col3  col4\n0     1     1     1     1\n1     2     2     2     2\n2     3     3     3     3\n>>> df.mean(axis=1)\n0    1\n1    2\n2    3\n>>> df.drop(""col4"", axis=1)\n   col1  col2  col3\n0     1     1     1\n1     2     2     2\n2     3     3     3\n']";"['>>> df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]], columns=[""col1"", ""col2"", ""col3"", ""col4""])\n>>> df\n   col1  col2  col3  col4\n0     1     1     1     1\n1     2     2     2     2\n2     3     3     3     3\n', '>>> df.mean(axis=1)\n0    1\n1    2\n2    3\n', '>>> df.drop(""col4"", axis=1)\n   col1  col2  col3\n0     1     1     1\n1     2     2     2\n2     3     3     3\n']";"['>>> df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]], columns=[""col1"", ""col2"", ""col3"", ""col4""])\n>>> df\n   col1  col2  col3  col4\n0     1     1     1     1\n1     2     2     2     2\n2     3     3     3     3\n', 'df.mean(axis=1)', '>>> df.mean(axis=1)\n0    1\n1    2\n2    3\n', 'df.drop(name, axis=1)', '>>> df.drop(""col4"", axis=1)\n   col1  col2  col3\n0     1     1     1\n1     2     2     2\n2     3     3     3\n', 'DataFrame.mean', 'DataFrame.mean', 'axis=1']";[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
867;867;867;867;4.0;5;25789354;;1;13;<csv><pandas><int><nan><missing-data>;Exporting ints with missing values to csv in Pandas;1388.0;"['import pandas as pd\nimport numpy as np\ndf = pd.DataFrame([[1,2],[3,np.nan],[5,6]],\n                  columns=[""a"",""b""],\n                  index=[""i_1"",""i_2"",""i_3""])\ndf.to_csv(""file.csv"")\n,a,b\ni_1,1,2.0\ni_2,3,\ni_3,5,6.0\n,a,b\ni_1,1,2\ni_2,3,\ni_3,5,6\n']";"['import pandas as pd\nimport numpy as np\ndf = pd.DataFrame([[1,2],[3,np.nan],[5,6]],\n                  columns=[""a"",""b""],\n                  index=[""i_1"",""i_2"",""i_3""])\ndf.to_csv(""file.csv"")\n', ',a,b\ni_1,1,2.0\ni_2,3,\ni_3,5,6.0\n', ',a,b\ni_1,1,2\ni_2,3,\ni_3,5,6\n']";"['np.nan', 'import pandas as pd\nimport numpy as np\ndf = pd.DataFrame([[1,2],[3,np.nan],[5,6]],\n                  columns=[""a"",""b""],\n                  index=[""i_1"",""i_2"",""i_3""])\ndf.to_csv(""file.csv"")\n', ',a,b\ni_1,1,2.0\ni_2,3,\ni_3,5,6.0\n', ',a,b\ni_1,1,2\ni_2,3,\ni_3,5,6\n']";"['import pandas as pd\nimport numpy as np\ndf.to_csv(""file.csv"")\ni_1,1,2.0\ni_2,3,\ni_3,5,6.0\ni_1,1,2\ni_2,3,\ni_3,5,6\n']";"['import pandas as pd\nimport numpy as np\ndf.to_csv(""file.csv"")\ni_1,1,2.0\ni_2,3,\ni_3,5,6.0\ni_1,1,2\ni_2,3,\ni_3,5,6\n']";False;"['import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\nimport numpy as np\ndf.to_csv(""file.csv"")\ni_1,1,2.0\ni_2,3,\ni_3,5,6.0\ni_1,1,2\ni_2,3,\ni_3,5,6\n']";True;0;0;[];[];0;0;[];[];0;0;[];[]
868;868;868;868;1.0;0;25789445;;1;12;<python><pandas>;Pandas make new column from string slice of another column;12335.0;['Sample  Value  New_sample\nAAB     23     A\nBAB     25     B\n'];['Sample  Value  New_sample\nAAB     23     A\nBAB     25     B\n'];['Sample  Value  New_sample\nAAB     23     A\nBAB     25     B\n', 'New_sample', '[:1]', 'Sample'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'DataFrame' object has no attribute 'Sample'""]";['AttributeError']
869;869;869;869;1.0;0;25797245;;1;12;<python><pandas><indexing><time-series><dataframe>;How to properly add hours to a pandas.tseries.index.DatetimeIndex?;5659.0;"[""In [1]: test[1].index\nOut[2]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2010-03-11, ..., 2014-08-14]\nLength: 52, Freq: None, Timezone: None\nIn [1]: test[1].index[0]\nOut[2]: Timestamp('2010-03-11 00:00:00')\nIn [1]: test[1].index[0] + pd.tseries.timedeltas.to_timedelta(16, unit='h')\nOut[2]: Timestamp('2010-03-11 00:00:00.000000016')\nOut[2]: Timestamp('2010-03-11 16:00:00')\n""]";"[""In [1]: test[1].index\nOut[2]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2010-03-11, ..., 2014-08-14]\nLength: 52, Freq: None, Timezone: None\n"", ""In [1]: test[1].index[0]\nOut[2]: Timestamp('2010-03-11 00:00:00')\n"", ""In [1]: test[1].index[0] + pd.tseries.timedeltas.to_timedelta(16, unit='h')\n"", ""Out[2]: Timestamp('2010-03-11 00:00:00.000000016')\n"", ""Out[2]: Timestamp('2010-03-11 16:00:00')\n""]";"[""In [1]: test[1].index\nOut[2]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2010-03-11, ..., 2014-08-14]\nLength: 52, Freq: None, Timezone: None\n"", ""In [1]: test[1].index[0]\nOut[2]: Timestamp('2010-03-11 00:00:00')\n"", ""In [1]: test[1].index[0] + pd.tseries.timedeltas.to_timedelta(16, unit='h')\n"", ""Out[2]: Timestamp('2010-03-11 00:00:00.000000016')\n"", ""Out[2]: Timestamp('2010-03-11 16:00:00')\n""]";"[""test[1].index\ntest[1].index[0]\ntest[1].index[0] + pd.tseries.timedeltas.to_timedelta(16, unit='h')\n""]";"[""import pandas as pd\ntest[1].index\ntest[1].index[0]\ntest[1].index[0] + pd.tseries.timedeltas.to_timedelta(16, unit='h')\n""]";True;"[""import pandas as pd\ntest[1].index\ntest[1].index[0]\ntest[1].index[0] + pd.tseries.timedeltas.to_timedelta(16, unit='h')\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
870;870;870;870;3.0;1;25909984;;1;11;<python><numpy><pandas>;Missing data, insert rows in Pandas and fill with NAN;5165.0;['ind A    B  C  \n0   0.0  1  3  \n1   0.5  4  2  \n2   1.0  6  1  \n3   3.5  2  0  \n4   4.0  4  5  \n5   4.5  3  3  \nind A    B  C  \n0   0.0  1  3  \n1   0.5  4  2  \n2   1.0  6  1  \n3   1.5  NAN NAN  \n4   2.0  NAN NAN  \n5   2.5  NAN NAN  \n6   3.0  NAN NAN  \n7   3.5  2  0  \n8   4.0  4  5  \n9   4.5  3  3  \n'];['ind A    B  C  \n0   0.0  1  3  \n1   0.5  4  2  \n2   1.0  6  1  \n3   3.5  2  0  \n4   4.0  4  5  \n5   4.5  3  3  \n', 'ind A    B  C  \n0   0.0  1  3  \n1   0.5  4  2  \n2   1.0  6  1  \n3   1.5  NAN NAN  \n4   2.0  NAN NAN  \n5   2.5  NAN NAN  \n6   3.0  NAN NAN  \n7   3.5  2  0  \n8   4.0  4  5  \n9   4.5  3  3  \n'];['ind A    B  C  \n0   0.0  1  3  \n1   0.5  4  2  \n2   1.0  6  1  \n3   3.5  2  0  \n4   4.0  4  5  \n5   4.5  3  3  \n', 'ind A    B  C  \n0   0.0  1  3  \n1   0.5  4  2  \n2   1.0  6  1  \n3   1.5  NAN NAN  \n4   2.0  NAN NAN  \n5   2.5  NAN NAN  \n6   3.0  NAN NAN  \n7   3.5  2  0  \n8   4.0  4  5  \n9   4.5  3  3  \n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'A'""]";['KeyError']
871;871;871;871;2.0;0;25929319;;1;17;<python><pandas>;How to iterate over pandas multiindex dataframe using index;10633.0;['                           observation1   observation2\ndate          Time                             \n2012-11-02    9:15:00      79.373668      224\n              9:16:00      130.841316     477\n2012-11-03    9:15:00      45.312814      835\n              9:16:00      123.776946     623\n              9:17:00      153.76646      624\n              9:18:00      463.276946     626\n              9:19:00      663.176934     622\n              9:20:00      763.77333      621\n2012-11-04    9:15:00      115.449437     122\n              9:16:00      123.776946     555\n              9:17:00      153.76646      344\n              9:18:00      463.276946     212\n for count in df(level 0 index) :\n     new_df = get only chunk for count\n     complex_process(new_df)\n2012-11-03    9:15:00      45.312814      835\n              9:16:00      123.776946     623\n              9:17:00      153.76646      624\n              9:18:00      463.276946     626\n              9:19:00      663.176934     622\n              9:20:00      763.77333      621\n'];['                           observation1   observation2\ndate          Time                             \n2012-11-02    9:15:00      79.373668      224\n              9:16:00      130.841316     477\n2012-11-03    9:15:00      45.312814      835\n              9:16:00      123.776946     623\n              9:17:00      153.76646      624\n              9:18:00      463.276946     626\n              9:19:00      663.176934     622\n              9:20:00      763.77333      621\n2012-11-04    9:15:00      115.449437     122\n              9:16:00      123.776946     555\n              9:17:00      153.76646      344\n              9:18:00      463.276946     212\n', ' for count in df(level 0 index) :\n     new_df = get only chunk for count\n     complex_process(new_df)\n', '2012-11-03    9:15:00      45.312814      835\n              9:16:00      123.776946     623\n              9:17:00      153.76646      624\n              9:18:00      463.276946     626\n              9:19:00      663.176934     622\n              9:20:00      763.77333      621\n'];['                           observation1   observation2\ndate          Time                             \n2012-11-02    9:15:00      79.373668      224\n              9:16:00      130.841316     477\n2012-11-03    9:15:00      45.312814      835\n              9:16:00      123.776946     623\n              9:17:00      153.76646      624\n              9:18:00      463.276946     626\n              9:19:00      663.176934     622\n              9:20:00      763.77333      621\n2012-11-04    9:15:00      115.449437     122\n              9:16:00      123.776946     555\n              9:17:00      153.76646      344\n              9:18:00      463.276946     212\n', ' for count in df(level 0 index) :\n     new_df = get only chunk for count\n     complex_process(new_df)\n', '2012-11-03    9:15:00      45.312814      835\n              9:16:00      123.776946     623\n              9:17:00      153.76646      624\n              9:18:00      463.276946     626\n              9:19:00      663.176934     622\n              9:20:00      763.77333      621\n'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
872;872;872;872;5.0;1;25962114;;1;37;<python><memory><numpy><pandas>;How to read a 6 GB csv file with pandas;40201.0;"[""MemoryError                               Traceback (most recent call last)\n<ipython-input-58-67a72687871b> in <module>()\n----> 1 data=pd.read_csv('aphro.csv',sep=';')\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format)\n    450                     infer_datetime_format=infer_datetime_format)\n    451 \n--> 452         return _read(filepath_or_buffer, kwds)\n    453 \n    454     parser_f.__name__ = name\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in _read(filepath_or_buffer, kwds)\n    242         return parser\n    243 \n--> 244     return parser.read()\n    245 \n    246 _parser_defaults = {\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in read(self, nrows)\n    693                 raise ValueError('skip_footer not supported for iteration')\n    694 \n--> 695         ret = self._engine.read(nrows)\n    696 \n    697         if self.options.get('as_recarray'):\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in read(self, nrows)\n   1137 \n   1138         try:\n-> 1139             data = self._reader.read(nrows)\n   1140         except StopIteration:\n   1141             if nrows is None:\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader.read (pandas\\parser.c:7145)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._read_low_memory (pandas\\parser.c:7369)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._read_rows (pandas\\parser.c:8194)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_column_data (pandas\\parser.c:9402)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_tokens (pandas\\parser.c:10057)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_with_dtype (pandas\\parser.c:10361)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser._try_int64 (pandas\\parser.c:17806)()\n\nMemoryError: \n""]";"[""MemoryError                               Traceback (most recent call last)\n<ipython-input-58-67a72687871b> in <module>()\n----> 1 data=pd.read_csv('aphro.csv',sep=';')\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format)\n    450                     infer_datetime_format=infer_datetime_format)\n    451 \n--> 452         return _read(filepath_or_buffer, kwds)\n    453 \n    454     parser_f.__name__ = name\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in _read(filepath_or_buffer, kwds)\n    242         return parser\n    243 \n--> 244     return parser.read()\n    245 \n    246 _parser_defaults = {\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in read(self, nrows)\n    693                 raise ValueError('skip_footer not supported for iteration')\n    694 \n--> 695         ret = self._engine.read(nrows)\n    696 \n    697         if self.options.get('as_recarray'):\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in read(self, nrows)\n   1137 \n   1138         try:\n-> 1139             data = self._reader.read(nrows)\n   1140         except StopIteration:\n   1141             if nrows is None:\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader.read (pandas\\parser.c:7145)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._read_low_memory (pandas\\parser.c:7369)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._read_rows (pandas\\parser.c:8194)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_column_data (pandas\\parser.c:9402)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_tokens (pandas\\parser.c:10057)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_with_dtype (pandas\\parser.c:10361)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser._try_int64 (pandas\\parser.c:17806)()\n\nMemoryError: \n""]";"[""MemoryError                               Traceback (most recent call last)\n<ipython-input-58-67a72687871b> in <module>()\n----> 1 data=pd.read_csv('aphro.csv',sep=';')\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format)\n    450                     infer_datetime_format=infer_datetime_format)\n    451 \n--> 452         return _read(filepath_or_buffer, kwds)\n    453 \n    454     parser_f.__name__ = name\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in _read(filepath_or_buffer, kwds)\n    242         return parser\n    243 \n--> 244     return parser.read()\n    245 \n    246 _parser_defaults = {\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in read(self, nrows)\n    693                 raise ValueError('skip_footer not supported for iteration')\n    694 \n--> 695         ret = self._engine.read(nrows)\n    696 \n    697         if self.options.get('as_recarray'):\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in read(self, nrows)\n   1137 \n   1138         try:\n-> 1139             data = self._reader.read(nrows)\n   1140         except StopIteration:\n   1141             if nrows is None:\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader.read (pandas\\parser.c:7145)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._read_low_memory (pandas\\parser.c:7369)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._read_rows (pandas\\parser.c:8194)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_column_data (pandas\\parser.c:9402)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_tokens (pandas\\parser.c:10057)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_with_dtype (pandas\\parser.c:10361)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser._try_int64 (pandas\\parser.c:17806)()\n\nMemoryError: \n""]";['\n\n\n\n\n\n\n\n\n\n\n\n'];['\n\n\n\n\n\n\n\n\n\n\n\n'];False;['import pandas as pd\n\n\n\n\n\n\n\n\n\n\n\n\n'];False;1;2;"['Sucess', ""name 'pd' is not defined""]";['Sucess', 'NameError'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
873;873;873;873;2.0;4;26047209;;1;43;<python><pandas>;What is the difference between a pandas Series and a single-column DataFrame?;19282.0;[''];[];['Series', 'DataFrame', 'Series'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
874;874;874;874;1.0;6;26063231;;1;17;<python><csv><pandas>;Read specific columns with pandas or other python module;25214.0;"[""data = #read data in a clever way\nnames = data['star_name']\nras = data['ra']\n# name, mass, mass_error_min, mass_error_max, radius, radius_error_min, radius_error_max, orbital_period, orbital_period_err_min, orbital_period_err_max, semi_major_axis, semi_major_axis_error_min, semi_major_axis_error_max, eccentricity, eccentricity_error_min, eccentricity_error_max, angular_distance, inclination, inclination_error_min, inclination_error_max, tzero_tr, tzero_tr_error_min, tzero_tr_error_max, tzero_tr_sec, tzero_tr_sec_error_min, tzero_tr_sec_error_max, lambda_angle, lambda_angle_error_min, lambda_angle_error_max, impact_parameter, impact_parameter_error_min, impact_parameter_error_max, tzero_vr, tzero_vr_error_min, tzero_vr_error_max, K, K_error_min, K_error_max, temp_calculated, temp_measured, hot_point_lon, albedo, albedo_error_min, albedo_error_max, log_g, publication_status, discovered, updated, omega, omega_error_min, omega_error_max, tperi, tperi_error_min, tperi_error_max, detection_type, mass_detection_type, radius_detection_type, alternate_names, molecules, star_name, ra, dec, mag_v, mag_i, mag_j, mag_h, mag_k, star_distance, star_metallicity, star_mass, star_radius, star_sp_type, star_age, star_teff, star_detected_disc, star_magnetic_field\n11 Com b,19.4,1.5,1.5,,,,326.03,0.32,0.32,1.29,0.05,0.05,0.231,0.005,0.005,0.011664,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,2008,2011-12-23,94.8,1.5,1.5,2452899.6,1.6,1.6,Radial Velocity,,,,,11 Com,185.1791667,17.7927778,4.74,,,,,110.6,-0.35,2.7,19.0,G8 III,,4742.0,,\n11 UMi b,10.5,2.47,2.47,,,,516.22,3.25,3.25,1.54,0.07,0.07,0.08,0.03,0.03,0.012887,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,2009,2009-08-13,117.63,21.06,21.06,2452861.05,2.06,2.06,Radial Velocity,,,,,11 UMi,229.275,71.8238889,5.02,,,,,119.5,0.04,1.8,24.08,K4III,1.56,4340.0,,\n""]";"[""data = #read data in a clever way\nnames = data['star_name']\nras = data['ra']\n"", '# name, mass, mass_error_min, mass_error_max, radius, radius_error_min, radius_error_max, orbital_period, orbital_period_err_min, orbital_period_err_max, semi_major_axis, semi_major_axis_error_min, semi_major_axis_error_max, eccentricity, eccentricity_error_min, eccentricity_error_max, angular_distance, inclination, inclination_error_min, inclination_error_max, tzero_tr, tzero_tr_error_min, tzero_tr_error_max, tzero_tr_sec, tzero_tr_sec_error_min, tzero_tr_sec_error_max, lambda_angle, lambda_angle_error_min, lambda_angle_error_max, impact_parameter, impact_parameter_error_min, impact_parameter_error_max, tzero_vr, tzero_vr_error_min, tzero_vr_error_max, K, K_error_min, K_error_max, temp_calculated, temp_measured, hot_point_lon, albedo, albedo_error_min, albedo_error_max, log_g, publication_status, discovered, updated, omega, omega_error_min, omega_error_max, tperi, tperi_error_min, tperi_error_max, detection_type, mass_detection_type, radius_detection_type, alternate_names, molecules, star_name, ra, dec, mag_v, mag_i, mag_j, mag_h, mag_k, star_distance, star_metallicity, star_mass, star_radius, star_sp_type, star_age, star_teff, star_detected_disc, star_magnetic_field\n11 Com b,19.4,1.5,1.5,,,,326.03,0.32,0.32,1.29,0.05,0.05,0.231,0.005,0.005,0.011664,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,2008,2011-12-23,94.8,1.5,1.5,2452899.6,1.6,1.6,Radial Velocity,,,,,11 Com,185.1791667,17.7927778,4.74,,,,,110.6,-0.35,2.7,19.0,G8 III,,4742.0,,\n11 UMi b,10.5,2.47,2.47,,,,516.22,3.25,3.25,1.54,0.07,0.07,0.08,0.03,0.03,0.012887,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,2009,2009-08-13,117.63,21.06,21.06,2452861.05,2.06,2.06,Radial Velocity,,,,,11 UMi,229.275,71.8238889,5.02,,,,,119.5,0.04,1.8,24.08,K4III,1.56,4340.0,,\n']";"['star_name', 'ra', ""data = #read data in a clever way\nnames = data['star_name']\nras = data['ra']\n"", 'csv', 'pandas', '# name, mass, mass_error_min, mass_error_max, radius, radius_error_min, radius_error_max, orbital_period, orbital_period_err_min, orbital_period_err_max, semi_major_axis, semi_major_axis_error_min, semi_major_axis_error_max, eccentricity, eccentricity_error_min, eccentricity_error_max, angular_distance, inclination, inclination_error_min, inclination_error_max, tzero_tr, tzero_tr_error_min, tzero_tr_error_max, tzero_tr_sec, tzero_tr_sec_error_min, tzero_tr_sec_error_max, lambda_angle, lambda_angle_error_min, lambda_angle_error_max, impact_parameter, impact_parameter_error_min, impact_parameter_error_max, tzero_vr, tzero_vr_error_min, tzero_vr_error_max, K, K_error_min, K_error_max, temp_calculated, temp_measured, hot_point_lon, albedo, albedo_error_min, albedo_error_max, log_g, publication_status, discovered, updated, omega, omega_error_min, omega_error_max, tperi, tperi_error_min, tperi_error_max, detection_type, mass_detection_type, radius_detection_type, alternate_names, molecules, star_name, ra, dec, mag_v, mag_i, mag_j, mag_h, mag_k, star_distance, star_metallicity, star_mass, star_radius, star_sp_type, star_age, star_teff, star_detected_disc, star_magnetic_field\n11 Com b,19.4,1.5,1.5,,,,326.03,0.32,0.32,1.29,0.05,0.05,0.231,0.005,0.005,0.011664,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,2008,2011-12-23,94.8,1.5,1.5,2452899.6,1.6,1.6,Radial Velocity,,,,,11 Com,185.1791667,17.7927778,4.74,,,,,110.6,-0.35,2.7,19.0,G8 III,,4742.0,,\n11 UMi b,10.5,2.47,2.47,,,,516.22,3.25,3.25,1.54,0.07,0.07,0.08,0.03,0.03,0.012887,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,2009,2009-08-13,117.63,21.06,21.06,2452861.05,2.06,2.06,Radial Velocity,,,,,11 UMi,229.275,71.8238889,5.02,,,,,119.5,0.04,1.8,24.08,K4III,1.56,4340.0,,\n']";"[""names = data['star_name']\nras = data['ra']\n# name, mass, mass_error_min, mass_error_max, radius, radius_error_min, radius_error_max, orbital_period, orbital_period_err_min, orbital_period_err_max, semi_major_axis, semi_major_axis_error_min, semi_major_axis_error_max, eccentricity, eccentricity_error_min, eccentricity_error_max, angular_distance, inclination, inclination_error_min, inclination_error_max, tzero_tr, tzero_tr_error_min, tzero_tr_error_max, tzero_tr_sec, tzero_tr_sec_error_min, tzero_tr_sec_error_max, lambda_angle, lambda_angle_error_min, lambda_angle_error_max, impact_parameter, impact_parameter_error_min, impact_parameter_error_max, tzero_vr, tzero_vr_error_min, tzero_vr_error_max, K, K_error_min, K_error_max, temp_calculated, temp_measured, hot_point_lon, albedo, albedo_error_min, albedo_error_max, log_g, publication_status, discovered, updated, omega, omega_error_min, omega_error_max, tperi, tperi_error_min, tperi_error_max, detection_type, mass_detection_type, radius_detection_type, alternate_names, molecules, star_name, ra, dec, mag_v, mag_i, mag_j, mag_h, mag_k, star_distance, star_metallicity, star_mass, star_radius, star_sp_type, star_age, star_teff, star_detected_disc, star_magnetic_field\n""]";"[""names = data['star_name']\nras = data['ra']\n# name, mass, mass_error_min, mass_error_max, radius, radius_error_min, radius_error_max, orbital_period, orbital_period_err_min, orbital_period_err_max, semi_major_axis, semi_major_axis_error_min, semi_major_axis_error_max, eccentricity, eccentricity_error_min, eccentricity_error_max, angular_distance, inclination, inclination_error_min, inclination_error_max, tzero_tr, tzero_tr_error_min, tzero_tr_error_max, tzero_tr_sec, tzero_tr_sec_error_min, tzero_tr_sec_error_max, lambda_angle, lambda_angle_error_min, lambda_angle_error_max, impact_parameter, impact_parameter_error_min, impact_parameter_error_max, tzero_vr, tzero_vr_error_min, tzero_vr_error_max, K, K_error_min, K_error_max, temp_calculated, temp_measured, hot_point_lon, albedo, albedo_error_min, albedo_error_max, log_g, publication_status, discovered, updated, omega, omega_error_min, omega_error_max, tperi, tperi_error_min, tperi_error_max, detection_type, mass_detection_type, radius_detection_type, alternate_names, molecules, star_name, ra, dec, mag_v, mag_i, mag_j, mag_h, mag_k, star_distance, star_metallicity, star_mass, star_radius, star_sp_type, star_age, star_teff, star_detected_disc, star_magnetic_field\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nnames = data['star_name']\nras = data['ra']\n# name, mass, mass_error_min, mass_error_max, radius, radius_error_min, radius_error_max, orbital_period, orbital_period_err_min, orbital_period_err_max, semi_major_axis, semi_major_axis_error_min, semi_major_axis_error_max, eccentricity, eccentricity_error_min, eccentricity_error_max, angular_distance, inclination, inclination_error_min, inclination_error_max, tzero_tr, tzero_tr_error_min, tzero_tr_error_max, tzero_tr_sec, tzero_tr_sec_error_min, tzero_tr_sec_error_max, lambda_angle, lambda_angle_error_min, lambda_angle_error_max, impact_parameter, impact_parameter_error_min, impact_parameter_error_max, tzero_vr, tzero_vr_error_min, tzero_vr_error_max, K, K_error_min, K_error_max, temp_calculated, temp_measured, hot_point_lon, albedo, albedo_error_min, albedo_error_max, log_g, publication_status, discovered, updated, omega, omega_error_min, omega_error_max, tperi, tperi_error_min, tperi_error_max, detection_type, mass_detection_type, radius_detection_type, alternate_names, molecules, star_name, ra, dec, mag_v, mag_i, mag_j, mag_h, mag_k, star_distance, star_metallicity, star_mass, star_radius, star_sp_type, star_age, star_teff, star_detected_disc, star_magnetic_field\n""]";True;0;1;"[""File b'data.csv' does not exist""]";['FileNotFoundError'];0;1;"[""File b'data.csv' does not exist""]";['FileNotFoundError'];0;1;"[""File b'data.csv' does not exist""]";['FileNotFoundError']
875;875;875;875;3.0;12;26067692;;1;11;<python><numpy><pandas>;Numpy.dtype has the wrong size, try recompiling;14293.0;[''];[];['Numpy.dtype has the wrong size, try recompiling'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'mv' is not defined""]";['NameError'];0;1;"[""name 'mv' is not defined""]";['NameError'];0;1;"[""name 'mv' is not defined""]";['NameError']
876;876;876;876;2.0;3;26097916;;1;15;<python><pandas><dataframe><series>;python, best way to convert a pandas series into a pandas dataframe;19075.0;"[""email\nemail1@email.com    [1.0, 0.0, 0.0]\nemail2@email.com    [2.0, 0.0, 0.0]\nemail3@email.com    [1.0, 0.0, 0.0]\nemail4@email.com    [4.0, 0.0, 0.0]\nemail5@email.com    [1.0, 0.0, 3.0]\nemail6@email.com    [1.0, 5.0, 0.0]\nindex | email             | list\n_____________________________________________\n0     | email1@email.com  | [1.0, 0.0, 0.0]\n1     | email2@email.com  | [2.0, 0.0, 0.0]\n2     | email3@email.com  | [1.0, 0.0, 0.0]\n3     | email4@email.com  | [4.0, 0.0, 0.0]\n4     | email5@email.com  | [1.0, 0.0, 3.0]\n5     | email6@email.com  | [1.0, 5.0, 0.0]\ndf1 = pd.DataFrame(data=sf.index, columns=['email'])\ndf2 = pd.DataFrame(data=sf.values, columns=['list'])\ndf = pd.merge(df1, df2, left_index=True, right_index=True)\n""]";"['email\nemail1@email.com    [1.0, 0.0, 0.0]\nemail2@email.com    [2.0, 0.0, 0.0]\nemail3@email.com    [1.0, 0.0, 0.0]\nemail4@email.com    [4.0, 0.0, 0.0]\nemail5@email.com    [1.0, 0.0, 3.0]\nemail6@email.com    [1.0, 5.0, 0.0]\n', 'index | email             | list\n_____________________________________________\n0     | email1@email.com  | [1.0, 0.0, 0.0]\n1     | email2@email.com  | [2.0, 0.0, 0.0]\n2     | email3@email.com  | [1.0, 0.0, 0.0]\n3     | email4@email.com  | [4.0, 0.0, 0.0]\n4     | email5@email.com  | [1.0, 0.0, 3.0]\n5     | email6@email.com  | [1.0, 5.0, 0.0]\n', ""df1 = pd.DataFrame(data=sf.index, columns=['email'])\ndf2 = pd.DataFrame(data=sf.values, columns=['list'])\ndf = pd.merge(df1, df2, left_index=True, right_index=True)\n""]";"['email\nemail1@email.com    [1.0, 0.0, 0.0]\nemail2@email.com    [2.0, 0.0, 0.0]\nemail3@email.com    [1.0, 0.0, 0.0]\nemail4@email.com    [4.0, 0.0, 0.0]\nemail5@email.com    [1.0, 0.0, 3.0]\nemail6@email.com    [1.0, 5.0, 0.0]\n', 'index | email             | list\n_____________________________________________\n0     | email1@email.com  | [1.0, 0.0, 0.0]\n1     | email2@email.com  | [2.0, 0.0, 0.0]\n2     | email3@email.com  | [1.0, 0.0, 0.0]\n3     | email4@email.com  | [4.0, 0.0, 0.0]\n4     | email5@email.com  | [1.0, 0.0, 3.0]\n5     | email6@email.com  | [1.0, 5.0, 0.0]\n', ""df1 = pd.DataFrame(data=sf.index, columns=['email'])\ndf2 = pd.DataFrame(data=sf.values, columns=['list'])\ndf = pd.merge(df1, df2, left_index=True, right_index=True)\n""]";"[""email\nemail1@email.com    [1.0, 0.0, 0.0]\nemail2@email.com    [2.0, 0.0, 0.0]\nemail3@email.com    [1.0, 0.0, 0.0]\nemail4@email.com    [4.0, 0.0, 0.0]\nemail5@email.com    [1.0, 0.0, 3.0]\nemail6@email.com    [1.0, 5.0, 0.0]\nindex | email             | list\n_____________________________________________\n0     | email1@email.com  | [1.0, 0.0, 0.0]\n1     | email2@email.com  | [2.0, 0.0, 0.0]\n2     | email3@email.com  | [1.0, 0.0, 0.0]\n3     | email4@email.com  | [4.0, 0.0, 0.0]\n4     | email5@email.com  | [1.0, 0.0, 3.0]\n5     | email6@email.com  | [1.0, 5.0, 0.0]\ndf1 = pd.DataFrame(data=sf.index, columns=['email'])\ndf2 = pd.DataFrame(data=sf.values, columns=['list'])\ndf = pd.merge(df1, df2, left_index=True, right_index=True)\n""]";"[""import pandas as pd\nemail\nemail1@email.com    [1.0, 0.0, 0.0]\nemail2@email.com    [2.0, 0.0, 0.0]\nemail3@email.com    [1.0, 0.0, 0.0]\nemail4@email.com    [4.0, 0.0, 0.0]\nemail5@email.com    [1.0, 0.0, 3.0]\nemail6@email.com    [1.0, 5.0, 0.0]\nindex | email             | list\n_____________________________________________\n0     | email1@email.com  | [1.0, 0.0, 0.0]\n1     | email2@email.com  | [2.0, 0.0, 0.0]\n2     | email3@email.com  | [1.0, 0.0, 0.0]\n3     | email4@email.com  | [4.0, 0.0, 0.0]\n4     | email5@email.com  | [1.0, 0.0, 3.0]\n5     | email6@email.com  | [1.0, 5.0, 0.0]\ndf1 = pd.DataFrame(data=sf.index, columns=['email'])\ndf2 = pd.DataFrame(data=sf.values, columns=['list'])\ndf = pd.merge(df1, df2, left_index=True, right_index=True)\n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\nemail\nemail1@email.com    [1.0, 0.0, 0.0]\nemail2@email.com    [2.0, 0.0, 0.0]\nemail3@email.com    [1.0, 0.0, 0.0]\nemail4@email.com    [4.0, 0.0, 0.0]\nemail5@email.com    [1.0, 0.0, 3.0]\nemail6@email.com    [1.0, 5.0, 0.0]\nindex | email             | list\n_____________________________________________\n0     | email1@email.com  | [1.0, 0.0, 0.0]\n1     | email2@email.com  | [2.0, 0.0, 0.0]\n2     | email3@email.com  | [1.0, 0.0, 0.0]\n3     | email4@email.com  | [4.0, 0.0, 0.0]\n4     | email5@email.com  | [1.0, 0.0, 3.0]\n5     | email6@email.com  | [1.0, 5.0, 0.0]\ndf1 = pd.DataFrame(data=sf.index, columns=['email'])\ndf2 = pd.DataFrame(data=sf.values, columns=['list'])\ndf = pd.merge(df1, df2, left_index=True, right_index=True)\n""]";True;0;2;"[""name 'pd' is not defined"", ""name 'email' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'sf' is not defined"", ""name 'email' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'sf' is not defined"", ""name 'email' is not defined""]";['NameError', 'NameError']
877;877;877;877;1.0;2;26121009;;1;14;<python><python-3.x><pandas>;Python 3 - Zip is an iterator in a pandas dataframe;12260.0;"['In [11]: print(\'Python version \' + sys.version)\nPython version 3.4.1 |Anaconda 2.0.1 (64-bit)| (default, Jun 11 2014, 17:27:11)\n[MSC v.1600 64 bit (AMD64)]\n\nIn [12]: print(\'Pandas version \' + pd.__version__)\nPandas version 0.14.1\nIn [13]: names = [\'Bob\',\'Jessica\',\'Mary\',\'John\',\'Mel\']\n\nIn [14]: births = [968, 155, 77, 578, 973]\n\nIn [15]: zip?\nType:            type\nString form:     <class \'zip\'>\nNamespace:       Python builtin\nInit definition: zip(self, *args, **kwargs)\nDocstring:\nzip(iter1 [,iter2 [...]]) --> zip object\n\nReturn a zip object whose .__next__() method returns a tuple where\nthe i-th element comes from the i-th iterable argument.  The .__next__()\nmethod continues until the shortest iterable in the argument sequence\nis exhausted and then it raises StopIteration.\n\nIn [16]: BabyDataSet = zip(names,births)\nIn [17]: BabyDataSet\nOut[17]: <zip at 0x4f28848>\n\nIn [18]: print(BabyDataSet)\n<zip object at 0x0000000004F28848>\nIn [21]: df = pd.DataFrame(data = BabyDataSet, columns=[\'Names\', \'Births\'])\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-21-636a49c94b6e> in <module>()\n----> 1 df = pd.DataFrame(data = BabyDataSet, columns=[\'Names\', \'Births\'])\n\nc:\\Users\\Sayth\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in __init__(self\n, data, index, columns, dtype, copy)\n    255                                          copy=copy)\n    256         elif isinstance(data, collections.Iterator):\n--> 257             raise TypeError(""data argument can\'t be an iterator"")\n    258         else:\n    259             try:\n\nTypeError: data argument can\'t be an iterator\n\nIn [22]:\n']";"[""In [11]: print('Python version ' + sys.version)\nPython version 3.4.1 |Anaconda 2.0.1 (64-bit)| (default, Jun 11 2014, 17:27:11)\n[MSC v.1600 64 bit (AMD64)]\n\nIn [12]: print('Pandas version ' + pd.__version__)\nPandas version 0.14.1\n"", ""In [13]: names = ['Bob','Jessica','Mary','John','Mel']\n\nIn [14]: births = [968, 155, 77, 578, 973]\n\nIn [15]: zip?\nType:            type\nString form:     <class 'zip'>\nNamespace:       Python builtin\nInit definition: zip(self, *args, **kwargs)\nDocstring:\nzip(iter1 [,iter2 [...]]) --> zip object\n\nReturn a zip object whose .__next__() method returns a tuple where\nthe i-th element comes from the i-th iterable argument.  The .__next__()\nmethod continues until the shortest iterable in the argument sequence\nis exhausted and then it raises StopIteration.\n\nIn [16]: BabyDataSet = zip(names,births)\n"", 'In [17]: BabyDataSet\nOut[17]: <zip at 0x4f28848>\n\nIn [18]: print(BabyDataSet)\n<zip object at 0x0000000004F28848>\n', 'In [21]: df = pd.DataFrame(data = BabyDataSet, columns=[\'Names\', \'Births\'])\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-21-636a49c94b6e> in <module>()\n----> 1 df = pd.DataFrame(data = BabyDataSet, columns=[\'Names\', \'Births\'])\n\nc:\\Users\\Sayth\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in __init__(self\n, data, index, columns, dtype, copy)\n    255                                          copy=copy)\n    256         elif isinstance(data, collections.Iterator):\n--> 257             raise TypeError(""data argument can\'t be an iterator"")\n    258         else:\n    259             try:\n\nTypeError: data argument can\'t be an iterator\n\nIn [22]:\n']";"[""In [11]: print('Python version ' + sys.version)\nPython version 3.4.1 |Anaconda 2.0.1 (64-bit)| (default, Jun 11 2014, 17:27:11)\n[MSC v.1600 64 bit (AMD64)]\n\nIn [12]: print('Pandas version ' + pd.__version__)\nPandas version 0.14.1\n"", ""In [13]: names = ['Bob','Jessica','Mary','John','Mel']\n\nIn [14]: births = [968, 155, 77, 578, 973]\n\nIn [15]: zip?\nType:            type\nString form:     <class 'zip'>\nNamespace:       Python builtin\nInit definition: zip(self, *args, **kwargs)\nDocstring:\nzip(iter1 [,iter2 [...]]) --> zip object\n\nReturn a zip object whose .__next__() method returns a tuple where\nthe i-th element comes from the i-th iterable argument.  The .__next__()\nmethod continues until the shortest iterable in the argument sequence\nis exhausted and then it raises StopIteration.\n\nIn [16]: BabyDataSet = zip(names,births)\n"", 'In [17]: BabyDataSet\nOut[17]: <zip at 0x4f28848>\n\nIn [18]: print(BabyDataSet)\n<zip object at 0x0000000004F28848>\n', 'In [21]: df = pd.DataFrame(data = BabyDataSet, columns=[\'Names\', \'Births\'])\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-21-636a49c94b6e> in <module>()\n----> 1 df = pd.DataFrame(data = BabyDataSet, columns=[\'Names\', \'Births\'])\n\nc:\\Users\\Sayth\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in __init__(self\n, data, index, columns, dtype, copy)\n    255                                          copy=copy)\n    256         elif isinstance(data, collections.Iterator):\n--> 257             raise TypeError(""data argument can\'t be an iterator"")\n    258         else:\n    259             try:\n\nTypeError: data argument can\'t be an iterator\n\nIn [22]:\n']";"[""print('Python version ' + sys.version)\n\n\n\n\n\n""]";"[""print('Python version ' + sys.version)\n\n\n\n\n\n""]";False;"[""import pandas as pd\nprint('Python version ' + sys.version)\n\n\n\n\n\n""]";False;0;1;"[""name 'names' is not defined""]";['NameError'];0;1;"[""name 'names' is not defined""]";['NameError'];0;1;"[""name 'names' is not defined""]";['NameError']
878;878;878;878;2.0;0;26133538;;1;11;<python><pandas>;round a single column in pandas;9137.0;[' df:\n      item  value1  value2\n    0    a    1.12     1.3\n    1    a    1.50     2.5\n    2    a    0.10     0.0\n    3    b    3.30    -1.0\n    4    b    4.80    -1.0\n0    1\n1    2\n2    0\n3    3\n4    5\n5    5\n  item  value1  value2\n0    a       1     1.3\n1    a       2     2.5\n2    a       0     0.0\n3    b       3    -1.0\n4    b       5    -1.0\n5    c       5     5.0\n'];[' df:\n      item  value1  value2\n    0    a    1.12     1.3\n    1    a    1.50     2.5\n    2    a    0.10     0.0\n    3    b    3.30    -1.0\n    4    b    4.80    -1.0\n', '0    1\n1    2\n2    0\n3    3\n4    5\n5    5\n', '  item  value1  value2\n0    a       1     1.3\n1    a       2     2.5\n2    a       0     0.0\n3    b       3    -1.0\n4    b       5    -1.0\n5    c       5     5.0\n'];[' df:\n      item  value1  value2\n    0    a    1.12     1.3\n    1    a    1.50     2.5\n    2    a    0.10     0.0\n    3    b    3.30    -1.0\n    4    b    4.80    -1.0\n', '0    1\n1    2\n2    0\n3    3\n4    5\n5    5\n', '  item  value1  value2\n0    a       1     1.3\n1    a       2     2.5\n2    a       0     0.0\n3    b       3    -1.0\n4    b       5    -1.0\n5    c       5     5.0\n'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
879;879;879;879;4.0;0;26139423;;1;35;<matplotlib><pandas><visualization>;plot different color for different categorical levels using matplotlib;28342.0;"[""ggplot(aes(x=carat, y=price, color=color),  #by setting color=color, ggplot automatically draw in different colors\n       data=diamonds) + geom_point(stat='summary', fun.y=median)\n""]";"[""ggplot(aes(x=carat, y=price, color=color),  #by setting color=color, ggplot automatically draw in different colors\n       data=diamonds) + geom_point(stat='summary', fun.y=median)\n""]";"['diamonds', '(carat, price, color)', 'price', 'carat', 'color', 'color', 'R', 'ggplot', ""ggplot(aes(x=carat, y=price, color=color),  #by setting color=color, ggplot automatically draw in different colors\n       data=diamonds) + geom_point(stat='summary', fun.y=median)\n"", 'matplotlib', 'seaborn', 'ggplot for python', 'matplotlib']";[''];[''];False;['import pandas as pd\n'];False;0;2;"[""No module named 'matplotlib'"", ""name 'sns' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'matplotlib'"", ""name 'sns' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'matplotlib'"", ""name 'sns' is not defined""]";['ImportError', 'NameError']
880;880;880;880;2.0;0;26147180;;1;15;<python><pandas><rename><dataframe>;Convert row to column header for Pandas DataFrame,;24734.0;"[""header = df[df['old_header_name1'] == 'new_header_name1']\n\ndf.columns = header\n""]";"[""header = df[df['old_header_name1'] == 'new_header_name1']\n\ndf.columns = header\n""]";"[""header = df[df['old_header_name1'] == 'new_header_name1']\n\ndf.columns = header\n""]";"[""header = df[df['old_header_name1'] == 'new_header_name1']\n\ndf.columns = header\n""]";"[""header = df[df['old_header_name1'] == 'new_header_name1']\n\ndf.columns = header\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nheader = df[df['old_header_name1'] == 'new_header_name1']\n\ndf.columns = header\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
881;881;881;881;3.0;0;26187759;;1;22;<python><pandas><parallel-processing><rosetta>;Parallelize apply after pandas groupby;11709.0;"[""from rosetta.parallel.pandas_easy import groupby_to_series_to_frame\ndf = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])\ngroupby_to_series_to_frame(df, np.mean, n_jobs=8, use_apply=True, by=df.index)\ndef tmpFunc(df):\n    df['c'] = df.a + df.b\n    return df\n\ndf.groupby(df.index).apply(tmpFunc)\ngroupby_to_series_to_frame(df, tmpFunc, n_jobs=1, use_apply=True, by=df.index)\n""]";"[""from rosetta.parallel.pandas_easy import groupby_to_series_to_frame\ndf = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])\ngroupby_to_series_to_frame(df, np.mean, n_jobs=8, use_apply=True, by=df.index)\n"", ""def tmpFunc(df):\n    df['c'] = df.a + df.b\n    return df\n\ndf.groupby(df.index).apply(tmpFunc)\ngroupby_to_series_to_frame(df, tmpFunc, n_jobs=1, use_apply=True, by=df.index)\n""]";"[""from rosetta.parallel.pandas_easy import groupby_to_series_to_frame\ndf = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])\ngroupby_to_series_to_frame(df, np.mean, n_jobs=8, use_apply=True, by=df.index)\n"", ""def tmpFunc(df):\n    df['c'] = df.a + df.b\n    return df\n\ndf.groupby(df.index).apply(tmpFunc)\ngroupby_to_series_to_frame(df, tmpFunc, n_jobs=1, use_apply=True, by=df.index)\n""]";"[""from rosetta.parallel.pandas_easy import groupby_to_series_to_frame\ndf = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])\ngroupby_to_series_to_frame(df, np.mean, n_jobs=8, use_apply=True, by=df.index)\n\ndf.groupby(df.index).apply(tmpFunc)\ngroupby_to_series_to_frame(df, tmpFunc, n_jobs=1, use_apply=True, by=df.index)\n""]";"[""import pandas as pd\nfrom rosetta.parallel.pandas_easy import groupby_to_series_to_frame\ndf = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])\ngroupby_to_series_to_frame(df, np.mean, n_jobs=8, use_apply=True, by=df.index)\n\ndf.groupby(df.index).apply(tmpFunc)\ngroupby_to_series_to_frame(df, tmpFunc, n_jobs=1, use_apply=True, by=df.index)\n""]";True;"[""import pandas as pd\nfrom rosetta.parallel.pandas_easy import groupby_to_series_to_frame\ndf = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])\ngroupby_to_series_to_frame(df, np.mean, n_jobs=8, use_apply=True, by=df.index)\n\ndf.groupby(df.index).apply(tmpFunc)\ngroupby_to_series_to_frame(df, tmpFunc, n_jobs=1, use_apply=True, by=df.index)\n""]";False;1;3;"[""name 'np' is not defined"", ""No module named 'joblib'"", 'Sucess']";['NameError', 'ImportError', 'Sucess'];1;3;"[""name 'np' is not defined"", ""No module named 'joblib'"", 'Sucess']";['NameError', 'ImportError', 'Sucess'];1;3;"[""name 'np' is not defined"", ""No module named 'joblib'"", 'Sucess']";['NameError', 'ImportError', 'Sucess']
882;882;882;882;4.0;1;26205922;;1;15;<python><numpy><pandas>;Calculate weighted average using a pandas/dataframe;16744.0;['Date        ID      wt      value   w_avg\n01/01/2012  100     0.50    60      0.791666667\n01/01/2012  101     0.75    80\n01/01/2012  102     1.00    100\n01/02/2012  201     0.50    100     0.722222222\n01/02/2012  202     1.00    80\n'];['Date        ID      wt      value   w_avg\n01/01/2012  100     0.50    60      0.791666667\n01/01/2012  101     0.75    80\n01/01/2012  102     1.00    100\n01/02/2012  201     0.50    100     0.722222222\n01/02/2012  202     1.00    80\n'];['Date        ID      wt      value   w_avg\n01/01/2012  100     0.50    60      0.791666667\n01/01/2012  101     0.75    80\n01/01/2012  102     1.00    100\n01/02/2012  201     0.50    100     0.722222222\n01/02/2012  202     1.00    80\n'];[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""'Date'"", 'Sucess']";['KeyError', 'Sucess']
883;883;883;883;4.0;4;26226343;;1;17;<python><pandas><concat>;Pandas concat gives error ValueError: Plan shapes are not aligned;7420.0;['ValueError: Plan shapes are not aligned\ndfs = [npo_jun_df, npo_jul_df,npo_may_df,npo_apr_df,npo_feb_df, ]\nalpha = pd.concat(dfs)\n'];['ValueError: Plan shapes are not aligned\n', 'dfs = [npo_jun_df, npo_jul_df,npo_may_df,npo_apr_df,npo_feb_df, ]\nalpha = pd.concat(dfs)\n'];['ValueError: Plan shapes are not aligned\n', 'dfs = [npo_jun_df, npo_jul_df,npo_may_df,npo_apr_df,npo_feb_df, ]\nalpha = pd.concat(dfs)\n'];['dfs = [npo_jun_df, npo_jul_df,npo_may_df,npo_apr_df,npo_feb_df, ]\nalpha = pd.concat(dfs)\n'];['import pandas as pd\ndfs = [npo_jun_df, npo_jul_df,npo_may_df,npo_apr_df,npo_feb_df, ]\nalpha = pd.concat(dfs)\n'];True;['import pandas as pd\ndf = pd.DataFrame()\ndfs = [npo_jun_df, npo_jul_df,npo_may_df,npo_apr_df,npo_feb_df, ]\nalpha = pd.concat(dfs)\n'];True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
884;884;884;884;3.0;1;26244309;;1;18;<python><pandas><dataframe>;How to analyze all duplicate entries in this Pandas DataFrame?;22157.0;"[""import pandas as pd\ndata={'key1':[1,2,3,1,2,3,2,2],'key2':[2,2,1,2,2,4,2,2],'data':[5,6,2,6,1,6,2,8]}\nframe=pd.DataFrame(data,columns=['key1','key2','data'])\nprint frame\n\n\n     key1  key2  data\n0     1     2     5\n1     2     2     6\n2     3     1     2\n3     1     2     6\n4     2     2     1\n5     3     4     6\n6     2     2     2\n7     2     2     8\nframe[frame.duplicated(['key1','key2'])]\n   key1  key2  data\n3     1     2     6\n4     2     2     1\n6     2     2     2\n7     2     2     8\na.groupby(['key1','key2']).min()\n           key1  key2  data\nkey1 key2                  \n1    2        1     2     6\n2    2        2     2     1\n""]";"[""import pandas as pd\ndata={'key1':[1,2,3,1,2,3,2,2],'key2':[2,2,1,2,2,4,2,2],'data':[5,6,2,6,1,6,2,8]}\nframe=pd.DataFrame(data,columns=['key1','key2','data'])\nprint frame\n\n\n     key1  key2  data\n0     1     2     5\n1     2     2     6\n2     3     1     2\n3     1     2     6\n4     2     2     1\n5     3     4     6\n6     2     2     2\n7     2     2     8\n"", ""frame[frame.duplicated(['key1','key2'])]\n"", '   key1  key2  data\n3     1     2     6\n4     2     2     1\n6     2     2     2\n7     2     2     8\n', ""a.groupby(['key1','key2']).min()\n"", '           key1  key2  data\nkey1 key2                  \n1    2        1     2     6\n2    2        2     2     1\n']";"[""import pandas as pd\ndata={'key1':[1,2,3,1,2,3,2,2],'key2':[2,2,1,2,2,4,2,2],'data':[5,6,2,6,1,6,2,8]}\nframe=pd.DataFrame(data,columns=['key1','key2','data'])\nprint frame\n\n\n     key1  key2  data\n0     1     2     5\n1     2     2     6\n2     3     1     2\n3     1     2     6\n4     2     2     1\n5     3     4     6\n6     2     2     2\n7     2     2     8\n"", ""frame[frame.duplicated(['key1','key2'])]\n"", '   key1  key2  data\n3     1     2     6\n4     2     2     1\n6     2     2     2\n7     2     2     8\n', ""a.groupby(['key1','key2']).min()\n"", '           key1  key2  data\nkey1 key2                  \n1    2        1     2     6\n2    2        2     2     1\n']";"[""import pandas as pd\ndata={'key1':[1,2,3,1,2,3,2,2],'key2':[2,2,1,2,2,4,2,2],'data':[5,6,2,6,1,6,2,8]}\nframe=pd.DataFrame(data,columns=['key1','key2','data'])\n\n\nframe[frame.duplicated(['key1','key2'])]\na.groupby(['key1','key2']).min()\n""]";"[""import pandas as pd\ndata={'key1':[1,2,3,1,2,3,2,2],'key2':[2,2,1,2,2,4,2,2],'data':[5,6,2,6,1,6,2,8]}\nframe=pd.DataFrame(data,columns=['key1','key2','data'])\n\n\nframe[frame.duplicated(['key1','key2'])]\na.groupby(['key1','key2']).min()\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nimport pandas as pd\ndata={'key1':[1,2,3,1,2,3,2,2],'key2':[2,2,1,2,2,4,2,2],'data':[5,6,2,6,1,6,2,8]}\nframe=pd.DataFrame(data,columns=['key1','key2','data'])\n\n\nframe[frame.duplicated(['key1','key2'])]\na.groupby(['key1','key2']).min()\n""]";True;0;2;"[""name 'frame' is not defined"", ""name 'frame' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'frame' is not defined"", ""name 'frame' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'frame' is not defined"", ""name 'frame' is not defined""]";['NameError', 'NameError']
885;885;885;885;3.0;0;26265819;;1;16;<python><pandas>;How to merge Series to DataFrame as columns, broadcasting;20918.0;"[""df = pd.DataFrame({'a':[1, 2], 'b':[3, 4]})  # see EDIT below\ns = pd.Series({'s1':5, 's2':6})\n\nfor name in s.index:\n    df[name] = s[name]\n\n   a  b  s1  s2\n0  1  3   5   6\n1  2  4   5   6\ndf.merge(s)\nAttributeError: 'Series' object has no attribute 'columns'\ndf.join(s)\nValueError: Other Series must have a name\ndf = pd.DataFrame({'a':[np.nan, 2, 3], 'b':[4, 5, 6]}, index=[3, 5, 6])\n    a  b  s1  s2\n3 NaN  4   5   6\n5   2  5   5   6\n6   3  6   5   6\n""]";"[""df = pd.DataFrame({'a':[1, 2], 'b':[3, 4]})  # see EDIT below\ns = pd.Series({'s1':5, 's2':6})\n\nfor name in s.index:\n    df[name] = s[name]\n\n   a  b  s1  s2\n0  1  3   5   6\n1  2  4   5   6\n"", ""df.merge(s)\nAttributeError: 'Series' object has no attribute 'columns'\n"", 'df.join(s)\nValueError: Other Series must have a name\n', ""df = pd.DataFrame({'a':[np.nan, 2, 3], 'b':[4, 5, 6]}, index=[3, 5, 6])\n"", '    a  b  s1  s2\n3 NaN  4   5   6\n5   2  5   5   6\n6   3  6   5   6\n']";"[""df = pd.DataFrame({'a':[1, 2], 'b':[3, 4]})  # see EDIT below\ns = pd.Series({'s1':5, 's2':6})\n\nfor name in s.index:\n    df[name] = s[name]\n\n   a  b  s1  s2\n0  1  3   5   6\n1  2  4   5   6\n"", ""df.merge(s)\nAttributeError: 'Series' object has no attribute 'columns'\n"", 'df.join(s)\nValueError: Other Series must have a name\n', 'df', ""df = pd.DataFrame({'a':[np.nan, 2, 3], 'b':[4, 5, 6]}, index=[3, 5, 6])\n"", '    a  b  s1  s2\n3 NaN  4   5   6\n5   2  5   5   6\n6   3  6   5   6\n']";"[""df = pd.DataFrame({'a':[1, 2], 'b':[3, 4]})  # see EDIT below\ns = pd.Series({'s1':5, 's2':6})\n\n\ndf.merge(s)\ndf.join(s)\ndf = pd.DataFrame({'a':[np.nan, 2, 3], 'b':[4, 5, 6]}, index=[3, 5, 6])\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'a':[1, 2], 'b':[3, 4]})  # see EDIT below\ns = pd.Series({'s1':5, 's2':6})\n\n\ndf.merge(s)\ndf.join(s)\ndf = pd.DataFrame({'a':[np.nan, 2, 3], 'b':[4, 5, 6]}, index=[3, 5, 6])\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame({'a':[1, 2], 'b':[3, 4]})  # see EDIT below\ns = pd.Series({'s1':5, 's2':6})\n\n\ndf.merge(s)\ndf.join(s)\ndf = pd.DataFrame({'a':[np.nan, 2, 3], 'b':[4, 5, 6]}, index=[3, 5, 6])\n""]";False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 's' is not defined"", ""name 's' is not defined""]";['NameError', 'NameError']
886;886;886;886;9.0;0;26266362;;1;110;<python><pandas>;How to count the Nan values in the column in Panda Data frame;83763.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;5;"[""name 'df' is not defined"", ""name 'pd' is not defined"", ""name 'np' is not defined"", 'Sucess', ""name 'df1' is not defined""]";['NameError', 'NameError', 'NameError', 'Sucess', 'NameError'];1;5;"[""name 'df' is not defined"", ""name 'np' is not defined"", ""name 'np' is not defined"", 'Sucess', ""name 'df1' is not defined""]";['NameError', 'NameError', 'NameError', 'Sucess', 'NameError'];2;5;"['Sucess', ""name 'np' is not defined"", ""name 'np' is not defined"", 'Sucess', ""'col1'""]";['Sucess', 'NameError', 'NameError', 'Sucess', 'KeyError']
887;887;887;887;2.0;5;26277757;;1;32;<python><html><pandas>;Pandas to_html() truncates string contents;7560.0;"['import pandas\ndf = pandas.DataFrame({\'text\': [\'Lorem ipsum dolor sit amet, consectetur adipiscing elit.\']})\nprint (df.to_html())\n<table border=""1"" class=""dataframe"">\n  <thead>\n    <tr style=""text-align: right;"">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td> Lorem ipsum dolor sit amet, consectetur adipis...</td>\n    </tr>\n  </tbody>\n</table>\n']";"[""import pandas\ndf = pandas.DataFrame({'text': ['Lorem ipsum dolor sit amet, consectetur adipiscing elit.']})\nprint (df.to_html())\n"", '<table border=""1"" class=""dataframe"">\n  <thead>\n    <tr style=""text-align: right;"">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td> Lorem ipsum dolor sit amet, consectetur adipis...</td>\n    </tr>\n  </tbody>\n</table>\n']";"['DataFrame', 'to_html()', ""import pandas\ndf = pandas.DataFrame({'text': ['Lorem ipsum dolor sit amet, consectetur adipiscing elit.']})\nprint (df.to_html())\n"", 'adapis...', '<table border=""1"" class=""dataframe"">\n  <thead>\n    <tr style=""text-align: right;"">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td> Lorem ipsum dolor sit amet, consectetur adipis...</td>\n    </tr>\n  </tbody>\n</table>\n']";"[""import pandas\ndf = pandas.DataFrame({'text': ['Lorem ipsum dolor sit amet, consectetur adipiscing elit.']})\nprint (df.to_html())\n""]";"[""import pandas\ndf = pandas.DataFrame({'text': ['Lorem ipsum dolor sit amet, consectetur adipiscing elit.']})\nprint (df.to_html())\n""]";False;"[""import pandas as pd\nimport pandas\ndf = pandas.DataFrame({'text': ['Lorem ipsum dolor sit amet, consectetur adipiscing elit.']})\nprint (df.to_html())\n""]";False;1;2;"[""name 'pd' is not defined"", 'Sucess']";['NameError', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
888;888;888;888;5.0;1;26309962;;1;21;<python><pandas><append><dataframe>;Appending a list or series to a pandas DataFrame as a row?;40866.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess']
889;889;889;889;1.0;0;26347412;;1;35;<python><pandas>;Drop multiple columns in pandas;66476.0;"[""df.drop([df.columns[[1, 69]]], axis=1, inplace=True)\nTypeError: unhashable type: 'Index'\nExpected type 'Integral', got 'list[int]' instead\ndf.drop([df.columns[69]], axis=1, inplace=True)\ndf.drop([df.columns[1]], axis=1, inplace=True)\n""]";"['df.drop([df.columns[[1, 69]]], axis=1, inplace=True)\n', ""TypeError: unhashable type: 'Index'\n"", ""Expected type 'Integral', got 'list[int]' instead\n"", 'df.drop([df.columns[69]], axis=1, inplace=True)\ndf.drop([df.columns[1]], axis=1, inplace=True)\n']";"['df.drop([df.columns[[1, 69]]], axis=1, inplace=True)\n', ""TypeError: unhashable type: 'Index'\n"", ""Expected type 'Integral', got 'list[int]' instead\n"", 'df.drop([df.columns[69]], axis=1, inplace=True)\ndf.drop([df.columns[1]], axis=1, inplace=True)\n']";['df.drop([df.columns[[1, 69]]], axis=1, inplace=True)\ndf.drop([df.columns[69]], axis=1, inplace=True)\ndf.drop([df.columns[1]], axis=1, inplace=True)\n'];['df.drop([df.columns[[1, 69]]], axis=1, inplace=True)\ndf.drop([df.columns[69]], axis=1, inplace=True)\ndf.drop([df.columns[1]], axis=1, inplace=True)\n'];False;['import pandas as pd\ndf = pd.DataFrame()\ndf.drop([df.columns[[1, 69]]], axis=1, inplace=True)\ndf.drop([df.columns[69]], axis=1, inplace=True)\ndf.drop([df.columns[1]], axis=1, inplace=True)\n'];True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;['index 1 is out of bounds for axis 1 with size 0'];['IndexError']
890;890;890;890;6.0;1;26414913;;1;17;<python><pandas><normalize>;Normalize columns of pandas data frame;28751.0;['A     B   C\n1000  10  0.5\n765   5   0.35\n800   7   0.09\nA     B    C\n1     1    1\n0.765 0.5  0.7\n0.8   0.7  0.18(which is 0.09/0.5)\n'];['A     B   C\n1000  10  0.5\n765   5   0.35\n800   7   0.09\n', 'A     B    C\n1     1    1\n0.765 0.5  0.7\n0.8   0.7  0.18(which is 0.09/0.5)\n'];['A     B   C\n1000  10  0.5\n765   5   0.35\n800   7   0.09\n', 'A     B    C\n1     1    1\n0.765 0.5  0.7\n0.8   0.7  0.18(which is 0.09/0.5)\n'];[''];[''];False;['import pandas as pd\n'];False;1;4;"[""No module named 'sklearn'"", ""name 'frame' is not defined"", 'Sucess', ""name 'df' is not defined""]";['ImportError', 'NameError', 'Sucess', 'NameError'];1;4;"[""No module named 'sklearn'"", ""name 'frame' is not defined"", 'Sucess', ""name 'df' is not defined""]";['ImportError', 'NameError', 'Sucess', 'NameError'];2;4;"[""No module named 'sklearn'"", ""name 'frame' is not defined"", 'Sucess', 'Sucess']";['ImportError', 'NameError', 'Sucess', 'Sucess']
891;891;891;891;3.0;2;26456125;;1;15;<python><pandas><aggregate>;Python Pandas: Is Order Preserved When Using groupby() and agg()?;3852.0;"[""df = pd.DataFrame({'A': ['group1', 'group1', 'group2', 'group2', 'group3', 'group3'],\n                   'B': [10, 12, 10, 25, 10, 12],\n                   'C': [100, 102, 100, 250, 100, 102]})\n\n>>> df\n[output]\n        A   B    C\n0  group1  10  100\n1  group1  12  102\n2  group2  10  100\n3  group2  25  250\n4  group3  10  100\n5  group3  12  102\ndf.groupby('A').agg([np.mean, lambda x: x.iloc[1] ])\n\n[output]\n\n        mean  <lambda>  mean  <lambda>\nA                                     \ngroup1  11.0        12   101       102\ngroup2  17.5        25   175       250\ngroup3  11.0        12   101       102\n""]";"[""df = pd.DataFrame({'A': ['group1', 'group1', 'group2', 'group2', 'group3', 'group3'],\n                   'B': [10, 12, 10, 25, 10, 12],\n                   'C': [100, 102, 100, 250, 100, 102]})\n\n>>> df\n[output]\n        A   B    C\n0  group1  10  100\n1  group1  12  102\n2  group2  10  100\n3  group2  25  250\n4  group3  10  100\n5  group3  12  102\n"", ""df.groupby('A').agg([np.mean, lambda x: x.iloc[1] ])\n\n[output]\n\n        mean  <lambda>  mean  <lambda>\nA                                     \ngroup1  11.0        12   101       102\ngroup2  17.5        25   175       250\ngroup3  11.0        12   101       102\n""]";"['agg()', ""df = pd.DataFrame({'A': ['group1', 'group1', 'group2', 'group2', 'group3', 'group3'],\n                   'B': [10, 12, 10, 25, 10, 12],\n                   'C': [100, 102, 100, 250, 100, 102]})\n\n>>> df\n[output]\n        A   B    C\n0  group1  10  100\n1  group1  12  102\n2  group2  10  100\n3  group2  25  250\n4  group3  10  100\n5  group3  12  102\n"", ""df.groupby('A').agg([np.mean, lambda x: x.iloc[1] ])\n\n[output]\n\n        mean  <lambda>  mean  <lambda>\nA                                     \ngroup1  11.0        12   101       102\ngroup2  17.5        25   175       250\ngroup3  11.0        12   101       102\n"", 'agg()']";"[""\n[output]\ndf.groupby('A').agg([np.mean, lambda x: x.iloc[1] ])\n\n[output]\n\nA                                     \n""]";"[""\n[output]\ndf.groupby('A').agg([np.mean, lambda x: x.iloc[1] ])\n\n[output]\n\nA                                     \n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\n\n[output]\ndf.groupby('A').agg([np.mean, lambda x: x.iloc[1] ])\n\n[output]\n\nA                                     \n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'A'""]";['KeyError']
892;892;892;892;2.0;3;26456825;;1;12;<python><python-2.7><numpy><pandas>;Convert timedelta64[ns] column to seconds in Python Pandas DataFrame;9843.0;"['0   00:20:32\n1   00:23:10\n2   00:24:55\n3   00:13:17\n4   00:18:52\nName: duration, dtype: timedelta64[ns]\nprint df[:5][\'duration\'] / np.timedelta64(1, \'s\')\nTraceback (most recent call last):\n  File ""test.py"", line 16, in <module>\n    print df[0:5][\'duration\'] / np.timedelta64(1, \'s\')\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\series.py"", line 130, in wrapper\n    ""addition and subtraction, but the operator [%s] was passed"" % name)\nTypeError: can only operate on a timedeltas for addition and subtraction, but the operator [__div__] was passed\nprint df[:5][\'duration\'].astype(\'timedelta64[s]\')\nTraceback (most recent call last):\n  File ""test.py"", line 17, in <module>\n    print df[:5][\'duration\'].astype(\'timedelta64[s]\')\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\series.py"", line 934, in astype\n    values = com._astype_nansafe(self.values, dtype)\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\common.py"", line 1653, in _astype_nansafe\n    raise TypeError(""cannot astype a timedelta from [%s] to [%s]"" % (arr.dtype,dtype))\nTypeError: cannot astype a timedelta from [timedelta64[ns]] to [timedelta64[s]]\n']";"['0   00:20:32\n1   00:23:10\n2   00:24:55\n3   00:13:17\n4   00:18:52\nName: duration, dtype: timedelta64[ns]\n', ""print df[:5]['duration'] / np.timedelta64(1, 's')\n"", 'Traceback (most recent call last):\n  File ""test.py"", line 16, in <module>\n    print df[0:5][\'duration\'] / np.timedelta64(1, \'s\')\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\series.py"", line 130, in wrapper\n    ""addition and subtraction, but the operator [%s] was passed"" % name)\nTypeError: can only operate on a timedeltas for addition and subtraction, but the operator [__div__] was passed\n', ""print df[:5]['duration'].astype('timedelta64[s]')\n"", 'Traceback (most recent call last):\n  File ""test.py"", line 17, in <module>\n    print df[:5][\'duration\'].astype(\'timedelta64[s]\')\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\series.py"", line 934, in astype\n    values = com._astype_nansafe(self.values, dtype)\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\common.py"", line 1653, in _astype_nansafe\n    raise TypeError(""cannot astype a timedelta from [%s] to [%s]"" % (arr.dtype,dtype))\nTypeError: cannot astype a timedelta from [timedelta64[ns]] to [timedelta64[s]]\n']";"['duration', 'timedelta64[ns]', '0   00:20:32\n1   00:23:10\n2   00:24:55\n3   00:13:17\n4   00:18:52\nName: duration, dtype: timedelta64[ns]\n', ""print df[:5]['duration'] / np.timedelta64(1, 's')\n"", 'Traceback (most recent call last):\n  File ""test.py"", line 16, in <module>\n    print df[0:5][\'duration\'] / np.timedelta64(1, \'s\')\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\series.py"", line 130, in wrapper\n    ""addition and subtraction, but the operator [%s] was passed"" % name)\nTypeError: can only operate on a timedeltas for addition and subtraction, but the operator [__div__] was passed\n', ""print df[:5]['duration'].astype('timedelta64[s]')\n"", 'Traceback (most recent call last):\n  File ""test.py"", line 17, in <module>\n    print df[:5][\'duration\'].astype(\'timedelta64[s]\')\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\series.py"", line 934, in astype\n    values = com._astype_nansafe(self.values, dtype)\n  File ""C:\\Python27\\lib\\site-packages\\pandas\\core\\common.py"", line 1653, in _astype_nansafe\n    raise TypeError(""cannot astype a timedelta from [%s] to [%s]"" % (arr.dtype,dtype))\nTypeError: cannot astype a timedelta from [timedelta64[ns]] to [timedelta64[s]]\n']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'duration'""]";['KeyError']
893;893;893;893;14.0;4;26473681;;1;59;<python><numpy><pandas><pip>;"PIP Install Numpy throws an error ""ascii codec can't decode byte 0xe2""";46327.0;"['Traceback (most recent call last):\n  File ""/usr/bin/pip"", line 9, in <module>\n    load_entry_point(\'pip==1.5.4\', \'console_scripts\', \'pip\')()\n  File ""/usr/lib/python2.7/dist-packages/pip/__init__.py"", line 185, in main\n    return command.main(cmd_args)\n  File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 161, in main\n    text = \'\\n\'.join(complete_log)\nUnicodeDecodeError: \'ascii\' codec can\'t decode byte 0xe2 in position 72: ordinal not in range(128)\n']";"['Traceback (most recent call last):\n  File ""/usr/bin/pip"", line 9, in <module>\n    load_entry_point(\'pip==1.5.4\', \'console_scripts\', \'pip\')()\n  File ""/usr/lib/python2.7/dist-packages/pip/__init__.py"", line 185, in main\n    return command.main(cmd_args)\n  File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 161, in main\n    text = \'\\n\'.join(complete_log)\nUnicodeDecodeError: \'ascii\' codec can\'t decode byte 0xe2 in position 72: ordinal not in range(128)\n']";"['Traceback (most recent call last):\n  File ""/usr/bin/pip"", line 9, in <module>\n    load_entry_point(\'pip==1.5.4\', \'console_scripts\', \'pip\')()\n  File ""/usr/lib/python2.7/dist-packages/pip/__init__.py"", line 185, in main\n    return command.main(cmd_args)\n  File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 161, in main\n    text = \'\\n\'.join(complete_log)\nUnicodeDecodeError: \'ascii\' codec can\'t decode byte 0xe2 in position 72: ordinal not in range(128)\n']";[''];[''];False;['import pandas as pd\n'];False;2;3;"[""name 'system' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess'];2;3;"[""name 'system' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess'];2;3;"[""name 'system' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess']
894;894;894;894;1.0;3;26483254;;1;22;<python><list><pandas><insert><dataframe>;Python pandas insert list into a cell;17970.0;"[""abc = ['foo', 'bar']\ndf =\n    A  B\n0  12  NaN\n1  23  NaN\n    A  B\n0  12  NaN\n1  23  ['foo', 'bar']\ndf.ix[1,'B'] = abc\nValueError: Must have equal len keys and value when setting with an iterable\ndf.ix[1,'B'] = [abc]\ndf.ix[1,'B'] = ', '.join(abc)\ndf.ix[1,'B'] = [', '.join(abc)]\nabc = ['foo', 'bar']\ndf2 =\n    A    B         C\n0  12  NaN      'bla'\n1  23  NaN  'bla bla'\ndf3 =\n    A    B         C                    D\n0  12  NaN      'bla'  ['item1', 'item2']\n1  23  NaN  'bla bla'        [11, 12, 13]\ndf4 =\n          A     B\n0      'bla'  NaN\n1  'bla bla'  NaN\n""]";"[""abc = ['foo', 'bar']\ndf =\n    A  B\n0  12  NaN\n1  23  NaN\n"", ""    A  B\n0  12  NaN\n1  23  ['foo', 'bar']\n"", ""df.ix[1,'B'] = abc\n"", 'ValueError: Must have equal len keys and value when setting with an iterable\n', ""df.ix[1,'B'] = [abc]\n"", ""df.ix[1,'B'] = ', '.join(abc)\n"", ""df.ix[1,'B'] = [', '.join(abc)]\n"", ""abc = ['foo', 'bar']\ndf2 =\n    A    B         C\n0  12  NaN      'bla'\n1  23  NaN  'bla bla'\n"", ""df3 =\n    A    B         C                    D\n0  12  NaN      'bla'  ['item1', 'item2']\n1  23  NaN  'bla bla'        [11, 12, 13]\n"", ""df4 =\n          A     B\n0      'bla'  NaN\n1  'bla bla'  NaN\n""]";"[""abc = ['foo', 'bar']\ndf =\n    A  B\n0  12  NaN\n1  23  NaN\n"", ""    A  B\n0  12  NaN\n1  23  ['foo', 'bar']\n"", ""df.ix[1,'B'] = abc\n"", 'ValueError: Must have equal len keys and value when setting with an iterable\n', ""df.ix[1,'B'] = [abc]\n"", ""[['foo', 'bar']]"", ""df.ix[1,'B'] = ', '.join(abc)\n"", 'foo, bar', ""df.ix[1,'B'] = [', '.join(abc)]\n"", ""['foo, bar']"", ""['foo', 'bar']"", ""abc = ['foo', 'bar']\ndf2 =\n    A    B         C\n0  12  NaN      'bla'\n1  23  NaN  'bla bla'\n"", ""df3 =\n    A    B         C                    D\n0  12  NaN      'bla'  ['item1', 'item2']\n1  23  NaN  'bla bla'        [11, 12, 13]\n"", ""df2.loc[1,'B']"", ""df3.loc[1,'B']"", ""df2.loc[1,'B'] = abc"", ""df3.loc[1,'B'] = abc"", ""df4 =\n          A     B\n0      'bla'  NaN\n1  'bla bla'  NaN\n"", ""df.loc[1,'B'] = abc"", ""df4.loc[1,'B'] = abc""]";"[""abc = ['foo', 'bar']\ndf.ix[1,'B'] = abc\ndf.ix[1,'B'] = [abc]\ndf.ix[1,'B'] = ', '.join(abc)\ndf.ix[1,'B'] = [', '.join(abc)]\nabc = ['foo', 'bar']\n""]";"[""abc = ['foo', 'bar']\ndf.ix[1,'B'] = abc\ndf.ix[1,'B'] = [abc]\ndf.ix[1,'B'] = ', '.join(abc)\ndf.ix[1,'B'] = [', '.join(abc)]\nabc = ['foo', 'bar']\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nabc = ['foo', 'bar']\ndf.ix[1,'B'] = abc\ndf.ix[1,'B'] = [abc]\ndf.ix[1,'B'] = ', '.join(abc)\ndf.ix[1,'B'] = [', '.join(abc)]\nabc = ['foo', 'bar']\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
895;895;895;895;4.0;1;26489134;;1;11;<python><pandas><quantile>;what's the inverse of the quantile function on a pandas Series?;3802.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""No module named 'scipy'""]";['ImportError'];0;1;"[""No module named 'scipy'""]";['ImportError'];0;1;"[""No module named 'scipy'""]";['ImportError']
896;896;896;896;2.0;2;26495408;;1;15;<python><csv><pandas>;Pandas - pandas.DataFrame.from_csv vs pandas.read_csv;5198.0;[''];[];['pandas.DataFrame.from_csv', 'pandas.read_csv'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
897;897;897;897;3.0;0;26521266;;1;18;<python><excel><pandas><dataframe>;Using Pandas to pd.read_excel() for multiple worksheets of the same workbook;22157.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'pd' is not defined"", ""name 'pd' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;['Install xlrd >= 0.9.0 for Excel support', 'Install xlrd >= 0.9.0 for Excel support', 'The `sheetname` keyword is deprecated, use `sheet_name` instead'];['ImportError', 'ImportError', 'FutureWarning'];0;3;['Install xlrd >= 0.9.0 for Excel support', 'Install xlrd >= 0.9.0 for Excel support', 'The `sheetname` keyword is deprecated, use `sheet_name` instead'];['ImportError', 'ImportError', 'FutureWarning']
898;898;898;898;2.0;0;26535563;;1;12;<python><pandas>;Querying for NaN and other names in Pandas;5041.0;"[""df.query( '(value < 10) or (value == NaN)' )\n""]";"[""df.query( '(value < 10) or (value == NaN)' )\n""]";"['df', 'value', 'NaN', 'NaN', ""df.query( '(value < 10) or (value == NaN)' )\n"", 'name NaN is not defined', ""df.query('value ==NaN')"", 'inf', 'nan', 'pi', 'e']";"[""df.query( '(value < 10) or (value == NaN)' )\n""]";"[""df.query( '(value < 10) or (value == NaN)' )\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf.query( '(value < 10) or (value == NaN)' )\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
899;899;899;899;4.0;1;26537878;;1;12;<python><pandas><dataframe>;Pandas sum across columns and divide each cell from that value;19153.0;"[""pivoted = df.pivot('user_id', 'group', 'value')\nlookup = df.drop_duplicates('user_id')[['user_id', 'group']]\nlookup.set_index(['user_id'], inplace=True)\nresult = pivoted.join(lookup)\nresult = result.fillna(0) \n             0     1     2    3     4    5   6  7    8   9  10  11  12  13  group\nuser_id                                                                      \n2        33653  2325   916  720   867  187  31  0    6   3  42  56  92  15    l-1\n4        18895   414  1116  570  1190   55  92  0  122  23  78   6   4   2    l-2 \n16        1383    70    27   17    17    1   0  0    0   0   1   0   0   0    l-2\n50         396    72    34    5    18    0   0  0    0   0   0   0   0   0    l-3\n51        3915  1170   402  832  2791  316  12  5  118  51  32   9  62  27    l-4\n""]";"[""pivoted = df.pivot('user_id', 'group', 'value')\nlookup = df.drop_duplicates('user_id')[['user_id', 'group']]\nlookup.set_index(['user_id'], inplace=True)\nresult = pivoted.join(lookup)\nresult = result.fillna(0) \n"", '             0     1     2    3     4    5   6  7    8   9  10  11  12  13  group\nuser_id                                                                      \n2        33653  2325   916  720   867  187  31  0    6   3  42  56  92  15    l-1\n4        18895   414  1116  570  1190   55  92  0  122  23  78   6   4   2    l-2 \n16        1383    70    27   17    17    1   0  0    0   0   1   0   0   0    l-2\n50         396    72    34    5    18    0   0  0    0   0   0   0   0   0    l-3\n51        3915  1170   402  832  2791  316  12  5  118  51  32   9  62  27    l-4\n']";"[""pivoted = df.pivot('user_id', 'group', 'value')\nlookup = df.drop_duplicates('user_id')[['user_id', 'group']]\nlookup.set_index(['user_id'], inplace=True)\nresult = pivoted.join(lookup)\nresult = result.fillna(0) \n"", '             0     1     2    3     4    5   6  7    8   9  10  11  12  13  group\nuser_id                                                                      \n2        33653  2325   916  720   867  187  31  0    6   3  42  56  92  15    l-1\n4        18895   414  1116  570  1190   55  92  0  122  23  78   6   4   2    l-2 \n16        1383    70    27   17    17    1   0  0    0   0   1   0   0   0    l-2\n50         396    72    34    5    18    0   0  0    0   0   0   0   0   0    l-3\n51        3915  1170   402  832  2791  316  12  5  118  51  32   9  62  27    l-4\n']";"[""pivoted = df.pivot('user_id', 'group', 'value')\nlookup = df.drop_duplicates('user_id')[['user_id', 'group']]\nlookup.set_index(['user_id'], inplace=True)\nresult = pivoted.join(lookup)\nresult = result.fillna(0) \nuser_id                                                                      \n""]";"[""pivoted = df.pivot('user_id', 'group', 'value')\nlookup = df.drop_duplicates('user_id')[['user_id', 'group']]\nlookup.set_index(['user_id'], inplace=True)\nresult = pivoted.join(lookup)\nresult = result.fillna(0) \nuser_id                                                                      \n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\npivoted = df.pivot('user_id', 'group', 'value')\nlookup = df.drop_duplicates('user_id')[['user_id', 'group']]\nlookup.set_index(['user_id'], inplace=True)\nresult = pivoted.join(lookup)\nresult = result.fillna(0) \nuser_id                                                                      \n""]";True;1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess']
900;900;900;900;2.0;0;26577516;;1;15;<python><string><pandas><dataframe><match>;pandas: test if string contains one of the substrings in a list;8720.0;"[""searchfor = ['og', 'at']\nfound = [s.str.contains(x) for x in searchfor]\nresult = pd.DataFrame[found]\nresult.any()\n""]";"[""searchfor = ['og', 'at']\nfound = [s.str.contains(x) for x in searchfor]\nresult = pd.DataFrame[found]\nresult.any()\n""]";"['df.isin()', 'df[col].str.contains()', ""s = pd.Series(['cat','hat','dog','fog','pet'])"", 's', ""['og', 'at']"", ""searchfor = ['og', 'at']\nfound = [s.str.contains(x) for x in searchfor]\nresult = pd.DataFrame[found]\nresult.any()\n""]";"[""searchfor = ['og', 'at']\nfound = [s.str.contains(x) for x in searchfor]\nresult = pd.DataFrame[found]\nresult.any()\n""]";"[""import pandas as pd\nsearchfor = ['og', 'at']\nfound = [s.str.contains(x) for x in searchfor]\nresult = pd.DataFrame[found]\nresult.any()\n""]";True;"[""import pandas as pd\nsearchfor = ['og', 'at']\nfound = [s.str.contains(x) for x in searchfor]\nresult = pd.DataFrame[found]\nresult.any()\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
901;901;901;901;4.0;0;26640145;;1;24;<python><pandas><dataframe>;Python Pandas: How to get the row names from index of a dataframe?;41938.0;['        X  Y\n Row 1  0  5\n Row 2  8  1\n Row 3  3  0\n'];['        X  Y\n Row 1  0  5\n Row 2  8  1\n Row 3  3  0\n'];['        X  Y\n Row 1  0  5\n Row 2  8  1\n Row 3  3  0\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;['slice indices must be integers or None or have an __index__ method'];['TypeError']
902;902;902;902;3.0;5;26645515;;1;36;<python><join><pandas>;Pandas join issue: columns overlap but no suffix specified;30832.0;"[""df_a =\n\n     mukey  DI  PI\n0   100000  35  14\n1  1000005  44  14\n2  1000006  44  14\n3  1000007  43  13\n4  1000008  43  13\n\ndf_b = \n    mukey  niccdcd\n0  190236        4\n1  190237        6\n2  190238        7\n3  190239        4\n4  190240        7\njoin_df = df_a.join(df_b,on='mukey',how='left')\n*** ValueError: columns overlap but no suffix specified: Index([u'mukey'], dtype='object')\n""]";"['df_a =\n\n     mukey  DI  PI\n0   100000  35  14\n1  1000005  44  14\n2  1000006  44  14\n3  1000007  43  13\n4  1000008  43  13\n\ndf_b = \n    mukey  niccdcd\n0  190236        4\n1  190237        6\n2  190238        7\n3  190239        4\n4  190240        7\n', ""join_df = df_a.join(df_b,on='mukey',how='left')\n"", ""*** ValueError: columns overlap but no suffix specified: Index([u'mukey'], dtype='object')\n""]";"['df_a =\n\n     mukey  DI  PI\n0   100000  35  14\n1  1000005  44  14\n2  1000006  44  14\n3  1000007  43  13\n4  1000008  43  13\n\ndf_b = \n    mukey  niccdcd\n0  190236        4\n1  190237        6\n2  190238        7\n3  190239        4\n4  190240        7\n', ""join_df = df_a.join(df_b,on='mukey',how='left')\n"", ""*** ValueError: columns overlap but no suffix specified: Index([u'mukey'], dtype='object')\n""]";"[""\n\njoin_df = df_a.join(df_b,on='mukey',how='left')\n""]";"[""\n\njoin_df = df_a.join(df_b,on='mukey',how='left')\n""]";False;"[""import pandas as pd\n\n\njoin_df = df_a.join(df_b,on='mukey',how='left')\n""]";False;0;1;"[""name 'df_a' is not defined""]";['NameError'];0;1;"[""name 'df_a' is not defined""]";['NameError'];0;1;"[""name 'df_a' is not defined""]";['NameError']
903;903;903;903;3.0;0;26646191;;1;16;<python><pandas>;Pandas groupby month and year;19480.0;['Date        abc    xyz\n01-Jun-13   100    200\n03-Jun-13   -20    50\n15-Aug-13   40     -5\n20-Jan-14   25     15\n21-Feb-14   60     80\n'];['Date        abc    xyz\n01-Jun-13   100    200\n03-Jun-13   -20    50\n15-Aug-13   40     -5\n20-Jan-14   25     15\n21-Feb-14   60     80\n'];['Date        abc    xyz\n01-Jun-13   100    200\n03-Jun-13   -20    50\n15-Aug-13   40     -5\n20-Jan-14   25     15\n21-Feb-14   60     80\n'];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'df' is not defined"", ""name 'df1' is not defined"", ""name 'DF' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'df1' is not defined"", ""name 'DF' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""'Date'"", ""'DataFrame' object has no attribute 'Date'"", ""name 'DF' is not defined""]";['KeyError', 'AttributeError', 'NameError']
904;904;904;904;1.0;2;26658240;;1;26;<python-2.7><pandas><dataframe>;getting the index of a row in a pandas apply function;9080.0;"[""df = pandas.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])\n>>> df\n   a  b  c\n0  1  2  3\n1  4  5  6\ndef rowFunc(row):\n    return row['a'] + row['b'] * row['c']\ndf['d'] = df.apply(rowFunc, axis=1)\n>>> df\n   a  b  c   d\n0  1  2  3   7\n1  4  5  6  34\n""]";"[""df = pandas.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])\n>>> df\n   a  b  c\n0  1  2  3\n1  4  5  6\n"", ""def rowFunc(row):\n    return row['a'] + row['b'] * row['c']\n"", ""df['d'] = df.apply(rowFunc, axis=1)\n>>> df\n   a  b  c   d\n0  1  2  3   7\n1  4  5  6  34\n""]";"['DataFrame', ""df = pandas.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])\n>>> df\n   a  b  c\n0  1  2  3\n1  4  5  6\n"", ""def rowFunc(row):\n    return row['a'] + row['b'] * row['c']\n"", ""df['d'] = df.apply(rowFunc, axis=1)\n>>> df\n   a  b  c   d\n0  1  2  3   7\n1  4  5  6  34\n"", 'DataFrame', 'd', ""Index([u'a', u'b', u'c', u'd'], dtype='object')"", 'row.index']";"[""df = pandas.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])\ndf['d'] = df.apply(rowFunc, axis=1)\n""]";"[""df = pandas.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])\ndf['d'] = df.apply(rowFunc, axis=1)\n""]";False;"[""import pandas as pd\ndf = pandas.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])\ndf['d'] = df.apply(rowFunc, axis=1)\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'rowFunc' is not defined""]";['NameError'];0;1;"[""name 'rowFunc' is not defined""]";['NameError']
905;905;905;905;3.0;1;26666919;;1;18;<python><pandas><dataframe>;python pandas add column in dataframe from list;27924.0;['A   B   C  \n0   \n4\n5\n6\n7\n7\n6\n5\nList=[2,5,6,8,12,16,26,32]  //There are only 8 elements in this list\nA   B   C   D\n0           2\n4           12\n5           16\n6           26\n7           32\n7           32\n6           26\n5           16\n'];['A   B   C  \n0   \n4\n5\n6\n7\n7\n6\n5\n', 'List=[2,5,6,8,12,16,26,32]  //There are only 8 elements in this list\n', 'A   B   C   D\n0           2\n4           12\n5           16\n6           26\n7           32\n7           32\n6           26\n5           16\n'];['A   B   C  \n0   \n4\n5\n6\n7\n7\n6\n5\n', 'List=[2,5,6,8,12,16,26,32]  //There are only 8 elements in this list\n', 'A   B   C   D\n0           2\n4           12\n5           16\n6           26\n7           32\n7           32\n6           26\n5           16\n'];['0   \n4\n5\n6\n7\n7\n6\n5\n'];['0   \n4\n5\n6\n7\n7\n6\n5\n'];False;['import pandas as pd\n0   \n4\n5\n6\n7\n7\n6\n5\n'];False;0;2;"[""name 'array' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'array' is not defined"", ""name 'mylist' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'array' is not defined"", ""name 'mylist' is not defined""]";['NameError', 'NameError']
906;906;906;906;2.0;3;26678467;;1;11;<python><pandas>;Export a Pandas dataframe as a table image;5767.0;[''];[];"['df.to_png()', ""df.to_table().savefig('table.png')"", 'df.to_csv()', '.table()']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""[Errno 2] No such file or directory: 'pdflatex'""]";['FileNotFoundError'];0;1;"[""[Errno 2] No such file or directory: 'pdflatex'""]";['FileNotFoundError'];0;1;"[""[Errno 2] No such file or directory: 'pdflatex'""]";['FileNotFoundError']
907;907;907;907;4.0;2;26716616;;1;28;<python><pandas><dictionary><dataframe>;Convert a Pandas DataFrame to a dictionary;34674.0;"[""    ID   A   B   C\n0   p    1   3   2\n1   q    4   3   2\n2   r    4   0   9  \n{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}\n""]";"['    ID   A   B   C\n0   p    1   3   2\n1   q    4   3   2\n2   r    4   0   9  \n', ""{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}\n""]";"['keys', 'values', '    ID   A   B   C\n0   p    1   3   2\n1   q    4   3   2\n2   r    4   0   9  \n', ""{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}\n""]";"[""{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}\n""]";"[""{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}\n""]";False;"[""import pandas as pd\n{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}\n""]";False;1;2;"[""name 'pd' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""File b'file' does not exist"", 'Sucess']";['FileNotFoundError', 'Sucess'];1;2;"[""File b'file' does not exist"", 'Sucess']";['FileNotFoundError', 'Sucess']
908;908;908;908;2.0;0;26724378;;1;16;<python><pandas><warnings>;Pandas SettingWithCopyWarning;31586.0;"[""df.col1[df.col1 == 10] = 1000\ndf.col2[df.index == 151] = 500\n-c:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\ncols = ['col1', 'col2', 'col3']\ndf[cols] = df[cols].applymap(some_function)\nTry using .loc[row_indexer,col_indexer] = value instead\n""]";"['df.col1[df.col1 == 10] = 1000\n', 'df.col2[df.index == 151] = 500\n', '-c:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n', ""cols = ['col1', 'col2', 'col3']\ndf[cols] = df[cols].applymap(some_function)\n"", 'Try using .loc[row_indexer,col_indexer] = value instead\n']";"['df.col1[df.col1 == 10] = 1000\n', 'df.col2[df.index == 151] = 500\n', '-c:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n', ""cols = ['col1', 'col2', 'col3']\ndf[cols] = df[cols].applymap(some_function)\n"", 'Try using .loc[row_indexer,col_indexer] = value instead\n']";"[""df.col1[df.col1 == 10] = 1000\ndf.col2[df.index == 151] = 500\n\ncols = ['col1', 'col2', 'col3']\ndf[cols] = df[cols].applymap(some_function)\n""]";"[""df.col1[df.col1 == 10] = 1000\ndf.col2[df.index == 151] = 500\n\ncols = ['col1', 'col2', 'col3']\ndf[cols] = df[cols].applymap(some_function)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf.col1[df.col1 == 10] = 1000\ndf.col2[df.index == 151] = 500\n\ncols = ['col1', 'col2', 'col3']\ndf[cols] = df[cols].applymap(some_function)\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'DataFrame' object has no attribute 'col1'""]";['AttributeError']
909;909;909;909;3.0;0;26763344;;1;43;<python><datetime><pandas>;Convert Pandas Column to DateTime;63841.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'pd' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'raw_data' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'raw_data' is not defined"", 'Sucess']";['NameError', 'Sucess']
910;910;910;910;1.0;5;26777832;;1;12;<python><pandas>;Replicating rows in a pandas data frame by a column value;3782.0;"[""import pandas as pd\n\nwhat_i_have = pd.DataFrame(data={\n  'id': ['A', 'B', 'C'],\n  'n' : [  1,   2,   3],\n  'v' : [ 10,  13,   8]\n})\n\nwhat_i_want = pd.DataFrame(data={\n  'id': ['A', 'B', 'B', 'C', 'C', 'C'],\n  'v' : [ 10,  13,  13,   8,   8,   8]\n})\n""]";"[""import pandas as pd\n\nwhat_i_have = pd.DataFrame(data={\n  'id': ['A', 'B', 'C'],\n  'n' : [  1,   2,   3],\n  'v' : [ 10,  13,   8]\n})\n\nwhat_i_want = pd.DataFrame(data={\n  'id': ['A', 'B', 'B', 'C', 'C', 'C'],\n  'v' : [ 10,  13,  13,   8,   8,   8]\n})\n""]";"[""import pandas as pd\n\nwhat_i_have = pd.DataFrame(data={\n  'id': ['A', 'B', 'C'],\n  'n' : [  1,   2,   3],\n  'v' : [ 10,  13,   8]\n})\n\nwhat_i_want = pd.DataFrame(data={\n  'id': ['A', 'B', 'B', 'C', 'C', 'C'],\n  'v' : [ 10,  13,  13,   8,   8,   8]\n})\n""]";['import pandas as pd\n\n\n'];['import pandas as pd\n\n\n'];False;['import pandas as pd\nimport pandas as pd\n\n\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
911;911;911;911;2.0;6;26784164;;1;12;<python><pandas><multiprocessing>;pandas multiprocessing apply;5109.0;"[""import multiprocessing as mp\nimport pandas.util.testing as pdt\n\ndef process_apply(x):\n    # do some stuff to data here\n\ndef process(df):\n    res = df.apply(process_apply, axis=1)\n    return res\n\nif __name__ == '__main__':\n    p = mp.Pool(processes=8)\n    split_dfs = np.array_split(big_df,8)\n    pool_results = p.map(aoi_proc, split_dfs)\n    p.close()\n    p.join()\n\n    # merging parts processed by different processes\n    parts = pd.concat(pool_results, axis=0)\n\n    # merging newly calculated parts to big_df\n    big_df = pd.concat([big_df, parts], axis=1)\n\n    # checking if the dfs were merged correctly\n    pdt.assert_series_equal(parts['id'], big_df['id'])\n""]";"[""import multiprocessing as mp\nimport pandas.util.testing as pdt\n\ndef process_apply(x):\n    # do some stuff to data here\n\ndef process(df):\n    res = df.apply(process_apply, axis=1)\n    return res\n\nif __name__ == '__main__':\n    p = mp.Pool(processes=8)\n    split_dfs = np.array_split(big_df,8)\n    pool_results = p.map(aoi_proc, split_dfs)\n    p.close()\n    p.join()\n\n    # merging parts processed by different processes\n    parts = pd.concat(pool_results, axis=0)\n\n    # merging newly calculated parts to big_df\n    big_df = pd.concat([big_df, parts], axis=1)\n\n    # checking if the dfs were merged correctly\n    pdt.assert_series_equal(parts['id'], big_df['id'])\n""]";"[""import multiprocessing as mp\nimport pandas.util.testing as pdt\n\ndef process_apply(x):\n    # do some stuff to data here\n\ndef process(df):\n    res = df.apply(process_apply, axis=1)\n    return res\n\nif __name__ == '__main__':\n    p = mp.Pool(processes=8)\n    split_dfs = np.array_split(big_df,8)\n    pool_results = p.map(aoi_proc, split_dfs)\n    p.close()\n    p.join()\n\n    # merging parts processed by different processes\n    parts = pd.concat(pool_results, axis=0)\n\n    # merging newly calculated parts to big_df\n    big_df = pd.concat([big_df, parts], axis=1)\n\n    # checking if the dfs were merged correctly\n    pdt.assert_series_equal(parts['id'], big_df['id'])\n""]";['import multiprocessing as mp\nimport pandas.util.testing as pdt\n\n    # do some stuff to data here\n\n\n\n    # merging parts processed by different processes\n\n    # merging newly calculated parts to big_df\n\n    # checking if the dfs were merged correctly\n'];['import multiprocessing as mp\nimport pandas.util.testing as pdt\n\n    # do some stuff to data here\n\n\n\n    # merging parts processed by different processes\n\n    # merging newly calculated parts to big_df\n\n    # checking if the dfs were merged correctly\n'];False;['import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\nimport multiprocessing as mp\nimport pandas.util.testing as pdt\n\n    # do some stuff to data here\n\n\n\n    # merging parts processed by different processes\n\n    # merging newly calculated parts to big_df\n\n    # checking if the dfs were merged correctly\n'];True;0;0;[];[];0;0;[];[];0;0;[];[]
912;912;912;912;2.0;0;26786960;;1;28;<python><csv><pandas>;pandas to_csv first extra column remove, how to?;14508.0;"[""d = {'one' : pd.Series([1., 2., 3.]),'two' : pd.Series([1., 2., 3., 4.])}\ndf0_fa = pd.DataFrame(d)\ndf_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w')\n,one,two\n0,1.0,1.0\n1,2.0,2.0\n2,3.0,3.0\n3,,4.0\none,two\n1.0,1.0\n2.0,2.0\n3.0,3.0\n,4.0\n""]";"[""d = {'one' : pd.Series([1., 2., 3.]),'two' : pd.Series([1., 2., 3., 4.])}\ndf0_fa = pd.DataFrame(d)\ndf_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w')\n"", ',one,two\n0,1.0,1.0\n1,2.0,2.0\n2,3.0,3.0\n3,,4.0\n', 'one,two\n1.0,1.0\n2.0,2.0\n3.0,3.0\n,4.0\n']";"[""d = {'one' : pd.Series([1., 2., 3.]),'two' : pd.Series([1., 2., 3., 4.])}\ndf0_fa = pd.DataFrame(d)\ndf_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w')\n"", ',one,two\n0,1.0,1.0\n1,2.0,2.0\n2,3.0,3.0\n3,,4.0\n', 'one,two\n1.0,1.0\n2.0,2.0\n3.0,3.0\n,4.0\n']";"[""d = {'one' : pd.Series([1., 2., 3.]),'two' : pd.Series([1., 2., 3., 4.])}\ndf0_fa = pd.DataFrame(d)\ndf_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w')\n0,1.0,1.0\n1,2.0,2.0\n2,3.0,3.0\none,two\n1.0,1.0\n2.0,2.0\n3.0,3.0\n""]";"[""import pandas as pd\nd = {'one' : pd.Series([1., 2., 3.]),'two' : pd.Series([1., 2., 3., 4.])}\ndf0_fa = pd.DataFrame(d)\ndf_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w')\n0,1.0,1.0\n1,2.0,2.0\n2,3.0,3.0\none,two\n1.0,1.0\n2.0,2.0\n3.0,3.0\n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\nd = {'one' : pd.Series([1., 2., 3.]),'two' : pd.Series([1., 2., 3., 4.])}\ndf0_fa = pd.DataFrame(d)\ndf_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w')\n0,1.0,1.0\n1,2.0,2.0\n2,3.0,3.0\none,two\n1.0,1.0\n2.0,2.0\n3.0,3.0\n""]";True;0;1;"[""name 'df0_fa' is not defined""]";['NameError'];0;1;"[""name 'df0_fa' is not defined""]";['NameError'];0;1;"[""name 'df0_fa' is not defined""]";['NameError']
913;913;913;913;3.0;1;26837998;;1;40;<python><pandas><nan>;Pandas Replace NaN with blank/empty string;36883.0;"['    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n    1    2       3\n 0  a   """"    read\n 1  b    l  unread\n 2  c   """"    read\n']";"['    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n', '    1    2       3\n 0  a   """"    read\n 1  b    l  unread\n 2  c   """"    read\n']";"['    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n', '    1    2       3\n 0  a   """"    read\n 1  b    l  unread\n 2  c   """"    read\n']";[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError']
914;914;914;914;4.0;5;26868304;;1;12;<python><matplotlib><pandas><plot><seaborn>;How to get rid of grid lines when plotting with Seaborn + Pandas with secondary_y;14203.0;"[""import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n\ndata = pd.DataFrame(np.cumsum(np.random.normal(size=(100,2)),axis=0),columns=['A','B'])\ndata.plot(secondary_y=['B'],grid=False)\n""]";"[""import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n\ndata = pd.DataFrame(np.cumsum(np.random.normal(size=(100,2)),axis=0),columns=['A','B'])\ndata.plot(secondary_y=['B'],grid=False)\n""]";"[""import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n\ndata = pd.DataFrame(np.cumsum(np.random.normal(size=(100,2)),axis=0),columns=['A','B'])\ndata.plot(secondary_y=['B'],grid=False)\n""]";"[""import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n\ndata = pd.DataFrame(np.cumsum(np.random.normal(size=(100,2)),axis=0),columns=['A','B'])\ndata.plot(secondary_y=['B'],grid=False)\n""]";"[""import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n\ndata = pd.DataFrame(np.cumsum(np.random.normal(size=(100,2)),axis=0),columns=['A','B'])\ndata.plot(secondary_y=['B'],grid=False)\n""]";False;"[""import pandas as pd\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n\ndata = pd.DataFrame(np.cumsum(np.random.normal(size=(100,2)),axis=0),columns=['A','B'])\ndata.plot(secondary_y=['B'],grid=False)\n""]";False;0;3;"[""'You can only set the value of existing options'"", ""name 'sns' is not defined"", ""name 'data' is not defined""]";['OptionError', 'NameError', 'NameError'];0;3;"[""'You can only set the value of existing options'"", ""name 'sns' is not defined"", ""name 'data' is not defined""]";['OptionError', 'NameError', 'NameError'];0;3;"[""'You can only set the value of existing options'"", ""name 'sns' is not defined"", 'Ellipsis']";['OptionError', 'NameError', 'KeyError']
915;915;915;915;4.0;2;26873127;;1;78;<pandas><printing><ipython-notebook><jupyter-notebook><display>;Show DataFrame as table in iPython Notebook;52192.0;['df\ndf1\ndf2 \nprint df1\nprint df2\n'];['df\n', 'df1\ndf2 \n', 'print df1\nprint df2\n'];['df\n', 'df1\ndf2 \n', 'print df1\nprint df2\n'];['df\ndf1\ndf2 \n'];['df\ndf1\ndf2 \n'];False;['import pandas as pd\ndf2 = pd.DataFrame()\ndf1 = pd.DataFrame()\ndf = pd.DataFrame()\ndf\ndf1\ndf2 \n'];True;0;3;"[""name 'df' is not defined"", ""name 'df1' is not defined"", ""name 'df1' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'df1' is not defined"", ""name 'df1' is not defined""]";['NameError', 'NameError', 'NameError'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess']
916;916;916;916;3.0;0;26878476;;1;14;<python><r><pandas><plyr><dplyr>;plyr or dplyr in Python;7935.0;['   data %.% group_by(c(.....)) %.% summarise(new1 = ...., new2 = ...., ..... newn=....)\ndf[...].groupby(.....).sum() only sums columns, \n'];['   data %.% group_by(c(.....)) %.% summarise(new1 = ...., new2 = ...., ..... newn=....)\n', 'df[...].groupby(.....).sum() only sums columns, \n'];['   data %.% group_by(c(.....)) %.% summarise(new1 = ...., new2 = ...., ..... newn=....)\n', 'df[...].groupby(.....).sum() only sums columns, \n'];[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'flights' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'flights' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'flights' is not defined"", 'Sucess']";['NameError', 'Sucess']
917;917;917;917;2.0;5;26879073;;1;16;<python><pandas>;Checking whether data frame is copy or view in Pandas;3172.0;"[""# Make two data frames that are views of same data.\ndf = pd.DataFrame([[1,2,3,4],[5,6,7,8]], index = ['row1','row2'], \n       columns = ['a','b','c','d'])\ndf2 = df.iloc[0:2,:]\n\n# Demonstrate they are views:\ndf.iloc[0,0] = 99\ndf2.iloc[0,0]\nOut[70]: 99\n\n# Now try and compare the id on values attribute\n# Different despite being views! \n\nid(df.values)\nOut[71]: 4753564496\n\nid(df2.values)\nOut[72]: 4753603728\n\n# And we can of course compare df and df2\ndf is df2\nOut[73]: False\n""]";"[""# Make two data frames that are views of same data.\ndf = pd.DataFrame([[1,2,3,4],[5,6,7,8]], index = ['row1','row2'], \n       columns = ['a','b','c','d'])\ndf2 = df.iloc[0:2,:]\n\n# Demonstrate they are views:\ndf.iloc[0,0] = 99\ndf2.iloc[0,0]\nOut[70]: 99\n\n# Now try and compare the id on values attribute\n# Different despite being views! \n\nid(df.values)\nOut[71]: 4753564496\n\nid(df2.values)\nOut[72]: 4753603728\n\n# And we can of course compare df and df2\ndf is df2\nOut[73]: False\n""]";"[""# Make two data frames that are views of same data.\ndf = pd.DataFrame([[1,2,3,4],[5,6,7,8]], index = ['row1','row2'], \n       columns = ['a','b','c','d'])\ndf2 = df.iloc[0:2,:]\n\n# Demonstrate they are views:\ndf.iloc[0,0] = 99\ndf2.iloc[0,0]\nOut[70]: 99\n\n# Now try and compare the id on values attribute\n# Different despite being views! \n\nid(df.values)\nOut[71]: 4753564496\n\nid(df2.values)\nOut[72]: 4753603728\n\n# And we can of course compare df and df2\ndf is df2\nOut[73]: False\n"", 'df.values.base', 'df.values', 'df._is_copy']";['# Make two data frames that are views of same data.\ndf2 = df.iloc[0:2,:]\n\n# Demonstrate they are views:\ndf.iloc[0,0] = 99\ndf2.iloc[0,0]\n\n# Now try and compare the id on values attribute\n# Different despite being views! \n\nid(df.values)\n\nid(df2.values)\n\n# And we can of course compare df and df2\ndf is df2\n'];['# Make two data frames that are views of same data.\ndf2 = df.iloc[0:2,:]\n\n# Demonstrate they are views:\ndf.iloc[0,0] = 99\ndf2.iloc[0,0]\n\n# Now try and compare the id on values attribute\n# Different despite being views! \n\nid(df.values)\n\nid(df2.values)\n\n# And we can of course compare df and df2\ndf is df2\n'];False;['import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\n# Make two data frames that are views of same data.\ndf2 = df.iloc[0:2,:]\n\n# Demonstrate they are views:\ndf.iloc[0,0] = 99\ndf2.iloc[0,0]\n\n# Now try and compare the id on values attribute\n# Different despite being views! \n\nid(df.values)\n\nid(df2.values)\n\n# And we can of course compare df and df2\ndf is df2\n'];True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
918;918;918;918;1.0;0;26886653;;1;52;<python><numpy><pandas>;pandas create new column based on values from other columns;68245.0;['IF [ERI_Hispanic] = 1 THEN RETURN \x93Hispanic\x94\nELSE IF SUM([ERI_AmerInd_AKNatv] + [ERI_Asian] + [ERI_Black_Afr.Amer] + [ERI_HI_PacIsl] + [ERI_White]) > 1 THEN RETURN \x93Two or More\x94\nELSE IF [ERI_AmerInd_AKNatv] = 1 THEN RETURN \x93A/I AK Native\x94\nELSE IF [ERI_Asian] = 1 THEN RETURN \x93Asian\x94\nELSE IF [ERI_Black_Afr.Amer] = 1 THEN RETURN \x93Black/AA\x94\nELSE IF [ERI_HI_PacIsl] = 1 THEN RETURN \x93Haw/Pac Isl.\x94\nELSE IF [ERI_White] = 1 THEN RETURN \x93White\x94\n     lname          fname       rno_cd  eri_afr_amer    eri_asian   eri_hawaiian    eri_hispanic    eri_nat_amer    eri_white   rno_defined\n0    MOST           JEFF        E       0               0           0               0               0               1           White\n1    CRUISE         TOM         E       0               0           0               1               0               0           White\n2    DEPP           JOHNNY              0               0           0               0               0               1           Unknown\n3    DICAP          LEO                 0               0           0               0               0               1           Unknown\n4    BRANDO         MARLON      E       0               0           0               0               0               0           White\n5    HANKS          TOM         0                       0           0               0               0               1           Unknown\n6    DENIRO         ROBERT      E       0               1           0               0               0               1           White\n7    PACINO         AL          E       0               0           0               0               0               1           White\n8    WILLIAMS       ROBIN       E       0               0           1               0               0               0           White\n9    EASTWOOD       CLINT       E       0               0           0               0               0               1           White\n'];['IF [ERI_Hispanic] = 1 THEN RETURN \x93Hispanic\x94\nELSE IF SUM([ERI_AmerInd_AKNatv] + [ERI_Asian] + [ERI_Black_Afr.Amer] + [ERI_HI_PacIsl] + [ERI_White]) > 1 THEN RETURN \x93Two or More\x94\nELSE IF [ERI_AmerInd_AKNatv] = 1 THEN RETURN \x93A/I AK Native\x94\nELSE IF [ERI_Asian] = 1 THEN RETURN \x93Asian\x94\nELSE IF [ERI_Black_Afr.Amer] = 1 THEN RETURN \x93Black/AA\x94\nELSE IF [ERI_HI_PacIsl] = 1 THEN RETURN \x93Haw/Pac Isl.\x94\nELSE IF [ERI_White] = 1 THEN RETURN \x93White\x94\n', '     lname          fname       rno_cd  eri_afr_amer    eri_asian   eri_hawaiian    eri_hispanic    eri_nat_amer    eri_white   rno_defined\n0    MOST           JEFF        E       0               0           0               0               0               1           White\n1    CRUISE         TOM         E       0               0           0               1               0               0           White\n2    DEPP           JOHNNY              0               0           0               0               0               1           Unknown\n3    DICAP          LEO                 0               0           0               0               0               1           Unknown\n4    BRANDO         MARLON      E       0               0           0               0               0               0           White\n5    HANKS          TOM         0                       0           0               0               0               1           Unknown\n6    DENIRO         ROBERT      E       0               1           0               0               0               1           White\n7    PACINO         AL          E       0               0           0               0               0               1           White\n8    WILLIAMS       ROBIN       E       0               0           1               0               0               0           White\n9    EASTWOOD       CLINT       E       0               0           0               0               0               1           White\n'];['IF [ERI_Hispanic] = 1 THEN RETURN \x93Hispanic\x94\nELSE IF SUM([ERI_AmerInd_AKNatv] + [ERI_Asian] + [ERI_Black_Afr.Amer] + [ERI_HI_PacIsl] + [ERI_White]) > 1 THEN RETURN \x93Two or More\x94\nELSE IF [ERI_AmerInd_AKNatv] = 1 THEN RETURN \x93A/I AK Native\x94\nELSE IF [ERI_Asian] = 1 THEN RETURN \x93Asian\x94\nELSE IF [ERI_Black_Afr.Amer] = 1 THEN RETURN \x93Black/AA\x94\nELSE IF [ERI_HI_PacIsl] = 1 THEN RETURN \x93Haw/Pac Isl.\x94\nELSE IF [ERI_White] = 1 THEN RETURN \x93White\x94\n', '     lname          fname       rno_cd  eri_afr_amer    eri_asian   eri_hawaiian    eri_hispanic    eri_nat_amer    eri_white   rno_defined\n0    MOST           JEFF        E       0               0           0               0               0               1           White\n1    CRUISE         TOM         E       0               0           0               1               0               0           White\n2    DEPP           JOHNNY              0               0           0               0               0               1           Unknown\n3    DICAP          LEO                 0               0           0               0               0               1           Unknown\n4    BRANDO         MARLON      E       0               0           0               0               0               0           White\n5    HANKS          TOM         0                       0           0               0               0               1           Unknown\n6    DENIRO         ROBERT      E       0               1           0               0               0               1           White\n7    PACINO         AL          E       0               0           0               0               0               1           White\n8    WILLIAMS       ROBIN       E       0               0           1               0               0               0           White\n9    EASTWOOD       CLINT       E       0               0           0               0               0               1           White\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;['Wrong number of items passed 0, placement implies 1'];['ValueError']
919;919;919;919;2.0;0;26893419;;1;12;<python><pandas><null><na>;Selecting pandas cells with None value;12598.0;"['In [325]: yes_records_sample[\'name\']\nOut[325]: \n41055    John J Murphy Professional Building\n25260                                   None\n41757             Armand Bayou Nature Center\n31397                                   None\n33104               Hubert Humphrey Building\n16891                         Williams Hall\n29618                                   None\n3770                          Covenant House\n39618                                   None\n1342       Bhathal Student Services Building\n20506                                   None\nIn [332]: isnull(yes_records_sample[\'name\'])\nNameError Traceback (most recent call last)\n<ipython-input-332-55873906e7e6> in <module>()\n----> 1 isnull(yes_records_sample[\'name\'])\nNameError: name \'isnull\' is not defined\nyes_records_sample[\'name\'].replace(\'None\', ""--no value--"")\nyes_records_sample[\'name\'].replace(None, ""--no value--"")\n']";"[""In [325]: yes_records_sample['name']\nOut[325]: \n41055    John J Murphy Professional Building\n25260                                   None\n41757             Armand Bayou Nature Center\n31397                                   None\n33104               Hubert Humphrey Building\n16891                         Williams Hall\n29618                                   None\n3770                          Covenant House\n39618                                   None\n1342       Bhathal Student Services Building\n20506                                   None\n"", ""In [332]: isnull(yes_records_sample['name'])\n"", ""NameError Traceback (most recent call last)\n<ipython-input-332-55873906e7e6> in <module>()\n----> 1 isnull(yes_records_sample['name'])\nNameError: name 'isnull' is not defined\n"", 'yes_records_sample[\'name\'].replace(\'None\', ""--no value--"")\nyes_records_sample[\'name\'].replace(None, ""--no value--"")\n']";"[""In [325]: yes_records_sample['name']\nOut[325]: \n41055    John J Murphy Professional Building\n25260                                   None\n41757             Armand Bayou Nature Center\n31397                                   None\n33104               Hubert Humphrey Building\n16891                         Williams Hall\n29618                                   None\n3770                          Covenant House\n39618                                   None\n1342       Bhathal Student Services Building\n20506                                   None\n"", 'isnull()', ""In [332]: isnull(yes_records_sample['name'])\n"", ""NameError Traceback (most recent call last)\n<ipython-input-332-55873906e7e6> in <module>()\n----> 1 isnull(yes_records_sample['name'])\nNameError: name 'isnull' is not defined\n"", 'yes_records_sample[\'name\'].replace(\'None\', ""--no value--"")\nyes_records_sample[\'name\'].replace(None, ""--no value--"")\n', 'fillna', ""yes_records_sample.fillna('')"", ""yes_records_sample['name']==''""]";"[""yes_records_sample['name']\n""]";"[""yes_records_sample['name']\n""]";False;"[""import pandas as pd\nyes_records_sample['name']\n""]";False;0;1;"[""name 'yes_records_sample' is not defined""]";['NameError'];0;1;"[""name 'yes_records_sample' is not defined""]";['NameError'];0;1;"[""name 'yes_records_sample' is not defined""]";['NameError']
920;920;920;920;5.0;0;26977076;;1;35;<python><pandas><dataframe><unique>;pandas unique values multiple columns;43537.0;"[""df = pd.DataFrame({'Col1': ['Bob', 'Joe', 'Bill', 'Mary', 'Joe'],\n                   'Col2': ['Joe', 'Steve', 'Bob', 'Bob', 'Steve'],\n                   'Col3': np.random.random(5)})\n'Bob', 'Joe', 'Bill', 'Mary', 'Steve'\n""]";"[""df = pd.DataFrame({'Col1': ['Bob', 'Joe', 'Bill', 'Mary', 'Joe'],\n                   'Col2': ['Joe', 'Steve', 'Bob', 'Bob', 'Steve'],\n                   'Col3': np.random.random(5)})\n"", ""'Bob', 'Joe', 'Bill', 'Mary', 'Steve'\n""]";"[""df = pd.DataFrame({'Col1': ['Bob', 'Joe', 'Bill', 'Mary', 'Joe'],\n                   'Col2': ['Joe', 'Steve', 'Bob', 'Bob', 'Steve'],\n                   'Col3': np.random.random(5)})\n"", ""'Bob', 'Joe', 'Bill', 'Mary', 'Steve'\n""]";"[""'Bob', 'Joe', 'Bill', 'Mary', 'Steve'\n""]";"[""'Bob', 'Joe', 'Bill', 'Mary', 'Steve'\n""]";False;"[""import pandas as pd\n'Bob', 'Joe', 'Bill', 'Mary', 'Steve'\n""]";False;0;1;"[""name 'array' is not defined""]";['NameError'];0;1;"[""name 'array' is not defined""]";['NameError'];0;1;"[""name 'array' is not defined""]";['NameError']
921;921;921;921;2.0;0;27012151;;1;13;<python><pandas>;forward fill specific columns in pandas dataframe;8448.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"['""[\'X\' \'Y\'] not in index""']";['KeyError']
922;922;922;922;3.0;6;27050108;;1;12;<python><json><numpy><pandas>;Convert numpy type to python;6926.0;"['list_val = [{1.0: 685}, {2.0: 8}]\noutput = json.dumps(list_val)\n            new = df[df[label] == label_new] \n            ks_dict = json.loads(content)\n            ks_list = ks_dict[\'variables\']\n            freq_counts = []\n\n            for ks_var in ks_list:\n\n                    freq_var = dict()\n                    freq_var[""name""] = ks_var[""name""]\n                    ks_series = new[ks_var[""name""]]\n                    temp_df = ks_series.value_counts().to_dict()\n                    freq_var[""new""] = [{u: np.int32(v)} for (u, v) in temp_df.iteritems()]            \n                    freq_counts.append(freq_var)\n\n           out = json.dumps(freq_counts)\n']";"['list_val = [{1.0: 685}, {2.0: 8}]\noutput = json.dumps(list_val)\n', '            new = df[df[label] == label_new] \n            ks_dict = json.loads(content)\n            ks_list = ks_dict[\'variables\']\n            freq_counts = []\n\n            for ks_var in ks_list:\n\n                    freq_var = dict()\n                    freq_var[""name""] = ks_var[""name""]\n                    ks_series = new[ks_var[""name""]]\n                    temp_df = ks_series.value_counts().to_dict()\n                    freq_var[""new""] = [{u: np.int32(v)} for (u, v) in temp_df.iteritems()]            \n                    freq_counts.append(freq_var)\n\n           out = json.dumps(freq_counts)\n']";"['list_val = [{1.0: 685}, {2.0: 8}]\noutput = json.dumps(list_val)\n', '            new = df[df[label] == label_new] \n            ks_dict = json.loads(content)\n            ks_list = ks_dict[\'variables\']\n            freq_counts = []\n\n            for ks_var in ks_list:\n\n                    freq_var = dict()\n                    freq_var[""name""] = ks_var[""name""]\n                    ks_series = new[ks_var[""name""]]\n                    temp_df = ks_series.value_counts().to_dict()\n                    freq_var[""new""] = [{u: np.int32(v)} for (u, v) in temp_df.iteritems()]            \n                    freq_counts.append(freq_var)\n\n           out = json.dumps(freq_counts)\n']";['list_val = [{1.0: 685}, {2.0: 8}]\noutput = json.dumps(list_val)\n\n\n\n'];['list_val = [{1.0: 685}, {2.0: 8}]\noutput = json.dumps(list_val)\n\n\n\n'];False;['import pandas as pd\nlist_val = [{1.0: 685}, {2.0: 8}]\noutput = json.dumps(list_val)\n\n\n\n'];False;0;1;"[""name 'json' is not defined""]";['NameError'];0;1;"[""name 'json' is not defined""]";['NameError'];0;1;"[""name 'json' is not defined""]";['NameError']
923;923;923;923;2.0;0;27060098;;1;16;<python><replace><pandas><dataframe>;Replacing few values in a pandas dataframe column with another value;28802.0;['BrandName Specialty\nA          H\nB          I\nABC        J\nD          K\nAB         L\n'];['BrandName Specialty\nA          H\nB          I\nABC        J\nD          K\nAB         L\n'];['BrandName Specialty\nA          H\nB          I\nABC        J\nD          K\nAB         L\n'];[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""'BrandName'"", 'Sucess']";['KeyError', 'Sucess']
924;924;924;924;2.0;1;27065133;;1;17;<python><pandas><dataframe><data-structures>;"Pandas merge giving error ""Buffer has wrong number of dimensions (expected 1, got 2)""";10573.0;"['df = pd.merge(df, c, how=""left"",\n        left_on=[""section_term_ps_id"", ""section_school_id"", ""state""],\n        right_on=[""term_ps_id"", ""term_school_id"", ""state""])\n']";"['df = pd.merge(df, c, how=""left"",\n        left_on=[""section_term_ps_id"", ""section_school_id"", ""state""],\n        right_on=[""term_ps_id"", ""term_school_id"", ""state""])\n']";"['df = pd.merge(df, c, how=""left"",\n        left_on=[""section_term_ps_id"", ""section_school_id"", ""state""],\n        right_on=[""term_ps_id"", ""term_school_id"", ""state""])\n']";[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
925;925;925;925;1.0;0;27116739;;1;11;<python><numpy><pandas>;What exactly is the lexsort_depth of a multi-index Dataframe?;4723.0;"[""idx = pd.IndexSlice\ndf[idx['foo', 'bar']]\nKeyError: 'Key length (2) was greater than MultiIndex lexsort depth (0)'\ndf = df.sortlevel(0,axis=1)\n""]";"[""idx = pd.IndexSlice\ndf[idx['foo', 'bar']]\n"", ""KeyError: 'Key length (2) was greater than MultiIndex lexsort depth (0)'\n"", 'df = df.sortlevel(0,axis=1)\n']";"['lexsort_depth', 'df', ""idx = pd.IndexSlice\ndf[idx['foo', 'bar']]\n"", ""KeyError: 'Key length (2) was greater than MultiIndex lexsort depth (0)'\n"", 'df.columns.lexsort_depth', '0', 'df = df.sortlevel(0,axis=1)\n', 'lexsort_depth', 'sortlevel']";"[""idx = pd.IndexSlice\ndf[idx['foo', 'bar']]\ndf = df.sortlevel(0,axis=1)\n""]";"[""import pandas as pd\nidx = pd.IndexSlice\ndf[idx['foo', 'bar']]\ndf = df.sortlevel(0,axis=1)\n""]";True;"[""import pandas as pd\nidx = pd.IndexSlice\ndf[idx['foo', 'bar']]\ndf = df.sortlevel(0,axis=1)\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
926;926;926;926;2.0;0;27117773;;1;12;<python><replace><pandas>;Pandas replace values;20684.0;['     col\n0    pre\n1    post\n2    a\n3    b\n4    post\n5    pre\n6    pre\n     col\n0    pre\n1    nonpre\n2    nonpre\n3    nonpre\n4    nonpre\n5    pre\n6    pre\n'];['     col\n0    pre\n1    post\n2    a\n3    b\n4    post\n5    pre\n6    pre\n', '     col\n0    pre\n1    nonpre\n2    nonpre\n3    nonpre\n4    nonpre\n5    pre\n6    pre\n'];['     col\n0    pre\n1    post\n2    a\n3    b\n4    post\n5    pre\n6    pre\n', '     col\n0    pre\n1    nonpre\n2    nonpre\n3    nonpre\n4    nonpre\n5    pre\n6    pre\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'col'""]";['KeyError']
927;927;927;927;2.0;0;27126511;;1;11;<python><pandas>;add columns different length pandas;8799.0;"[""df['Name column'] = data    \n# type(data) = list\nAssertionError: Length of values does not match length of index   \n""]";"[""df['Name column'] = data    \n# type(data) = list\n"", 'AssertionError: Length of values does not match length of index   \n']";"[""df['Name column'] = data    \n# type(data) = list\n"", 'AssertionError: Length of values does not match length of index   \n']";"[""df['Name column'] = data    \n# type(data) = list\n""]";"[""df['Name column'] = data    \n# type(data) = list\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\ndf['Name column'] = data    \n# type(data) = list\n""]";True;0;2;"[""name 'pd' is not defined"", ""name 'original' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""name 'original' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'original' is not defined""]";['Sucess', 'NameError']
928;928;928;928;2.0;0;27203161;;1;17;<python><csv><pandas><hdf5><pytables>;Convert large csv to hdf5;8628.0;[''];[];['read_csv', 'to_hdf', 'io_tools'];[''];[''];False;['import pandas as pd\n'];False;0;2;"['HDFStore requires PyTables, ""No module named \'tables\'"" problem importing', ""No module named 'tables'""]";['ImportError', 'ImportError'];0;2;"['HDFStore requires PyTables, ""No module named \'tables\'"" problem importing', ""No module named 'tables'""]";['ImportError', 'ImportError'];0;2;"['HDFStore requires PyTables, ""No module named \'tables\'"" problem importing', ""No module named 'tables'""]";['ImportError', 'ImportError']
929;929;929;929;2.0;4;27236275;;1;56;<python><pandas>;What does `ValueError: cannot reindex from a duplicate axis` mean?;48018.0;"[""ipdb> type(affinity_matrix)\n<class 'pandas.core.frame.DataFrame'>\nipdb> affinity_matrix.shape\n(333, 10)\nipdb> affinity_matrix.columns\nInt64Index([9315684, 9315597, 9316591, 9320520, 9321163, 9320615, 9321187, 9319487, 9319467, 9320484], dtype='int64')\nipdb> affinity_matrix.index\nIndex([u'001', u'002', u'003', u'004', u'005', u'008', u'009', u'010', u'011', u'014', u'015', u'016', u'018', u'020', u'021', u'022', u'024', u'025', u'026', u'027', u'028', u'029', u'030', u'032', u'033', u'034', u'035', u'036', u'039', u'040', u'041', u'042', u'043', u'044', u'045', u'047', u'047', u'048', u'050', u'053', u'054', u'055', u'056', u'057', u'058', u'059', u'060', u'061', u'062', u'063', u'065', u'067', u'068', u'069', u'070', u'071', u'072', u'073', u'074', u'075', u'076', u'077', u'078', u'080', u'082', u'083', u'084', u'085', u'086', u'089', u'090', u'091', u'092', u'093', u'094', u'095', u'096', u'097', u'098', u'100', u'101', u'103', u'104', u'105', u'106', u'107', u'108', u'109', u'110', u'111', u'112', u'113', u'114', u'115', u'116', u'117', u'118', u'119', u'121', u'122', ...], dtype='object')\n\nipdb> affinity_matrix.values.dtype\ndtype('float64')\nipdb> 'sums' in affinity_matrix.index\nFalse\nipdb> affinity_matrix.loc['sums'] = affinity_matrix.sum(axis=0)\n*** ValueError: cannot reindex from a duplicate axis\nIn [32]: import pandas as pd\n\nIn [33]: import numpy as np\n\nIn [34]: a = np.arange(35).reshape(5,7)\n\nIn [35]: df = pd.DataFrame(a, ['x', 'y', 'u', 'z', 'w'], range(10, 17))\n\nIn [36]: df.values.dtype\nOut[36]: dtype('int64')\n\nIn [37]: df.loc['sums'] = df.sum(axis=0)\n\nIn [38]: df\nOut[38]: \n      10  11  12  13  14  15   16\nx      0   1   2   3   4   5    6\ny      7   8   9  10  11  12   13\nu     14  15  16  17  18  19   20\nz     21  22  23  24  25  26   27\nw     28  29  30  31  32  33   34\nsums  70  75  80  85  90  95  100\n""]";"[""ipdb> type(affinity_matrix)\n<class 'pandas.core.frame.DataFrame'>\nipdb> affinity_matrix.shape\n(333, 10)\nipdb> affinity_matrix.columns\nInt64Index([9315684, 9315597, 9316591, 9320520, 9321163, 9320615, 9321187, 9319487, 9319467, 9320484], dtype='int64')\nipdb> affinity_matrix.index\nIndex([u'001', u'002', u'003', u'004', u'005', u'008', u'009', u'010', u'011', u'014', u'015', u'016', u'018', u'020', u'021', u'022', u'024', u'025', u'026', u'027', u'028', u'029', u'030', u'032', u'033', u'034', u'035', u'036', u'039', u'040', u'041', u'042', u'043', u'044', u'045', u'047', u'047', u'048', u'050', u'053', u'054', u'055', u'056', u'057', u'058', u'059', u'060', u'061', u'062', u'063', u'065', u'067', u'068', u'069', u'070', u'071', u'072', u'073', u'074', u'075', u'076', u'077', u'078', u'080', u'082', u'083', u'084', u'085', u'086', u'089', u'090', u'091', u'092', u'093', u'094', u'095', u'096', u'097', u'098', u'100', u'101', u'103', u'104', u'105', u'106', u'107', u'108', u'109', u'110', u'111', u'112', u'113', u'114', u'115', u'116', u'117', u'118', u'119', u'121', u'122', ...], dtype='object')\n\nipdb> affinity_matrix.values.dtype\ndtype('float64')\nipdb> 'sums' in affinity_matrix.index\nFalse\n"", ""ipdb> affinity_matrix.loc['sums'] = affinity_matrix.sum(axis=0)\n*** ValueError: cannot reindex from a duplicate axis\n"", ""In [32]: import pandas as pd\n\nIn [33]: import numpy as np\n\nIn [34]: a = np.arange(35).reshape(5,7)\n\nIn [35]: df = pd.DataFrame(a, ['x', 'y', 'u', 'z', 'w'], range(10, 17))\n\nIn [36]: df.values.dtype\nOut[36]: dtype('int64')\n\nIn [37]: df.loc['sums'] = df.sum(axis=0)\n\nIn [38]: df\nOut[38]: \n      10  11  12  13  14  15   16\nx      0   1   2   3   4   5    6\ny      7   8   9  10  11  12   13\nu     14  15  16  17  18  19   20\nz     21  22  23  24  25  26   27\nw     28  29  30  31  32  33   34\nsums  70  75  80  85  90  95  100\n""]";"['ValueError: cannot reindex from a duplicate axis', 'ipdb', 'sum', 'ValueError: cannot reindex from a duplicate axis', 'ValueError: cannot reindex from a duplicate axis', ""ipdb> type(affinity_matrix)\n<class 'pandas.core.frame.DataFrame'>\nipdb> affinity_matrix.shape\n(333, 10)\nipdb> affinity_matrix.columns\nInt64Index([9315684, 9315597, 9316591, 9320520, 9321163, 9320615, 9321187, 9319487, 9319467, 9320484], dtype='int64')\nipdb> affinity_matrix.index\nIndex([u'001', u'002', u'003', u'004', u'005', u'008', u'009', u'010', u'011', u'014', u'015', u'016', u'018', u'020', u'021', u'022', u'024', u'025', u'026', u'027', u'028', u'029', u'030', u'032', u'033', u'034', u'035', u'036', u'039', u'040', u'041', u'042', u'043', u'044', u'045', u'047', u'047', u'048', u'050', u'053', u'054', u'055', u'056', u'057', u'058', u'059', u'060', u'061', u'062', u'063', u'065', u'067', u'068', u'069', u'070', u'071', u'072', u'073', u'074', u'075', u'076', u'077', u'078', u'080', u'082', u'083', u'084', u'085', u'086', u'089', u'090', u'091', u'092', u'093', u'094', u'095', u'096', u'097', u'098', u'100', u'101', u'103', u'104', u'105', u'106', u'107', u'108', u'109', u'110', u'111', u'112', u'113', u'114', u'115', u'116', u'117', u'118', u'119', u'121', u'122', ...], dtype='object')\n\nipdb> affinity_matrix.values.dtype\ndtype('float64')\nipdb> 'sums' in affinity_matrix.index\nFalse\n"", ""ipdb> affinity_matrix.loc['sums'] = affinity_matrix.sum(axis=0)\n*** ValueError: cannot reindex from a duplicate axis\n"", ""In [32]: import pandas as pd\n\nIn [33]: import numpy as np\n\nIn [34]: a = np.arange(35).reshape(5,7)\n\nIn [35]: df = pd.DataFrame(a, ['x', 'y', 'u', 'z', 'w'], range(10, 17))\n\nIn [36]: df.values.dtype\nOut[36]: dtype('int64')\n\nIn [37]: df.loc['sums'] = df.sum(axis=0)\n\nIn [38]: df\nOut[38]: \n      10  11  12  13  14  15   16\nx      0   1   2   3   4   5    6\ny      7   8   9  10  11  12   13\nu     14  15  16  17  18  19   20\nz     21  22  23  24  25  26   27\nw     28  29  30  31  32  33   34\nsums  70  75  80  85  90  95  100\n""]";"[""import pandas as pd\n\n\n\n\ndf.loc['sums'] = df.sum(axis=0)\n\n""]";"[""import pandas as pd\n\n\n\n\ndf.loc['sums'] = df.sum(axis=0)\n\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\n\n\n\n\ndf.loc['sums'] = df.sum(axis=0)\n\n""]";True;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
930;930;930;930;3.0;0;27263805;;1;31;<python><pandas>;pandas: When cell contents are lists, create a row for each element in the list;8222.0;"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {'trial_num': [1, 2, 3, 1, 2, 3],\n     'subject': [1, 1, 1, 2, 2, 2],\n     'samples': [list(np.random.randn(3).round(2)) for i in range(6)]\n    }\n)\n\ndf\nOut[10]: \n                 samples  subject  trial_num\n0    [0.57, -0.83, 1.44]        1          1\n1    [-0.01, 1.13, 0.36]        1          2\n2   [1.18, -1.46, -0.94]        1          3\n3  [-0.08, -4.22, -2.05]        2          1\n4     [0.72, 0.79, 0.53]        2          2\n5    [0.4, -0.32, -0.13]        2          3\n   subject  trial_num  sample  sample_num\n0        1          1    0.57           0\n1        1          1   -0.83           1\n2        1          1    1.44           2\n3        1          2   -0.01           0\n4        1          2    1.13           1\n5        1          2    0.36           2\n6        1          3    1.18           0\n# etc.\n""]";"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {'trial_num': [1, 2, 3, 1, 2, 3],\n     'subject': [1, 1, 1, 2, 2, 2],\n     'samples': [list(np.random.randn(3).round(2)) for i in range(6)]\n    }\n)\n\ndf\nOut[10]: \n                 samples  subject  trial_num\n0    [0.57, -0.83, 1.44]        1          1\n1    [-0.01, 1.13, 0.36]        1          2\n2   [1.18, -1.46, -0.94]        1          3\n3  [-0.08, -4.22, -2.05]        2          1\n4     [0.72, 0.79, 0.53]        2          2\n5    [0.4, -0.32, -0.13]        2          3\n"", '   subject  trial_num  sample  sample_num\n0        1          1    0.57           0\n1        1          1   -0.83           1\n2        1          1    1.44           2\n3        1          2   -0.01           0\n4        1          2    1.13           1\n5        1          2    0.36           2\n6        1          3    1.18           0\n# etc.\n']";"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {'trial_num': [1, 2, 3, 1, 2, 3],\n     'subject': [1, 1, 1, 2, 2, 2],\n     'samples': [list(np.random.randn(3).round(2)) for i in range(6)]\n    }\n)\n\ndf\nOut[10]: \n                 samples  subject  trial_num\n0    [0.57, -0.83, 1.44]        1          1\n1    [-0.01, 1.13, 0.36]        1          2\n2   [1.18, -1.46, -0.94]        1          3\n3  [-0.08, -4.22, -2.05]        2          1\n4     [0.72, 0.79, 0.53]        2          2\n5    [0.4, -0.32, -0.13]        2          3\n"", '   subject  trial_num  sample  sample_num\n0        1          1    0.57           0\n1        1          1   -0.83           1\n2        1          1    1.44           2\n3        1          2   -0.01           0\n4        1          2    1.13           1\n5        1          2    0.36           2\n6        1          3    1.18           0\n# etc.\n']";['import pandas as pd\nimport numpy as np\n\n\ndf\n# etc.\n'];['import pandas as pd\nimport numpy as np\n\n\ndf\n# etc.\n'];False;['import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\nimport numpy as np\n\n\ndf\n# etc.\n'];True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
931;931;931;931;4.0;0;27275236;;1;18;<python><pandas><dataframe><selection>;pandas: best way to select all columns starting with X;13469.0;"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'foo.aa': [1, 2.1, np.nan, 4.7, 5.6, 6.8],\n                   'foo.fighters': [0, 1, np.nan, 0, 0, 0],\n                   'foo.bars': [0, 0, 0, 0, 0, 1],\n                   'bar.baz': [5, 5, 6, 5, 5.6, 6.8],\n                   'foo.fox': [2, 4, 1, 0, 0, 5],\n                   'nas.foo': ['NA', 0, 1, 0, 0, 0],\n                   'foo.manchu': ['NA', 0, 0, 0, 0, 0],})\ndf2 = df[(df['foo.aa'] == 1)|\n(df['foo.fighters'] == 1)|\n(df['foo.bars'] == 1)|\n(df['foo.fox'] == 1)|\n(df['foo.manchu'] == 1)\n]\ndf2= df[df.STARTS_WITH_FOO == 1]\n   bar.baz  foo.aa  foo.bars  foo.fighters  foo.fox foo.manchu nas.foo\n0      5.0     1.0         0             0        2         NA      NA\n1      5.0     2.1         0             1        4          0       0\n2      6.0     NaN         0           NaN        1          0       1\n5      6.8     6.8         1             0        5          0       0\n\n[4 rows x 7 columns]\n""]";"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'foo.aa': [1, 2.1, np.nan, 4.7, 5.6, 6.8],\n                   'foo.fighters': [0, 1, np.nan, 0, 0, 0],\n                   'foo.bars': [0, 0, 0, 0, 0, 1],\n                   'bar.baz': [5, 5, 6, 5, 5.6, 6.8],\n                   'foo.fox': [2, 4, 1, 0, 0, 5],\n                   'nas.foo': ['NA', 0, 1, 0, 0, 0],\n                   'foo.manchu': ['NA', 0, 0, 0, 0, 0],})\n"", ""df2 = df[(df['foo.aa'] == 1)|\n(df['foo.fighters'] == 1)|\n(df['foo.bars'] == 1)|\n(df['foo.fox'] == 1)|\n(df['foo.manchu'] == 1)\n]\n"", 'df2= df[df.STARTS_WITH_FOO == 1]\n', '   bar.baz  foo.aa  foo.bars  foo.fighters  foo.fox foo.manchu nas.foo\n0      5.0     1.0         0             0        2         NA      NA\n1      5.0     2.1         0             1        4          0       0\n2      6.0     NaN         0           NaN        1          0       1\n5      6.8     6.8         1             0        5          0       0\n\n[4 rows x 7 columns]\n']";"[""import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'foo.aa': [1, 2.1, np.nan, 4.7, 5.6, 6.8],\n                   'foo.fighters': [0, 1, np.nan, 0, 0, 0],\n                   'foo.bars': [0, 0, 0, 0, 0, 1],\n                   'bar.baz': [5, 5, 6, 5, 5.6, 6.8],\n                   'foo.fox': [2, 4, 1, 0, 0, 5],\n                   'nas.foo': ['NA', 0, 1, 0, 0, 0],\n                   'foo.manchu': ['NA', 0, 0, 0, 0, 0],})\n"", 'foo.', ""df2 = df[(df['foo.aa'] == 1)|\n(df['foo.fighters'] == 1)|\n(df['foo.bars'] == 1)|\n(df['foo.fox'] == 1)|\n(df['foo.manchu'] == 1)\n]\n"", 'df2= df[df.STARTS_WITH_FOO == 1]\n', '   bar.baz  foo.aa  foo.bars  foo.fighters  foo.fox foo.manchu nas.foo\n0      5.0     1.0         0             0        2         NA      NA\n1      5.0     2.1         0             1        4          0       0\n2      6.0     NaN         0           NaN        1          0       1\n5      6.8     6.8         1             0        5          0       0\n\n[4 rows x 7 columns]\n']";"[""import pandas as pd\nimport numpy as np\n\n(df['foo.manchu'] == 1)\ndf2= df[df.STARTS_WITH_FOO == 1]\n\n""]";"[""import pandas as pd\nimport numpy as np\n\n(df['foo.manchu'] == 1)\ndf2= df[df.STARTS_WITH_FOO == 1]\n\n""]";False;"[""import pandas as pd\ndf2 = pd.DataFrame()\ndf = pd.DataFrame()\nimport pandas as pd\nimport numpy as np\n\n(df['foo.manchu'] == 1)\ndf2= df[df.STARTS_WITH_FOO == 1]\n\n""]";True;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""Can only use .str accessor with string values (i.e. inferred_type is 'string', 'unicode' or 'mixed')""]";['Sucess', 'AttributeError']
932;932;932;932;1.0;0;27325652;;1;19;<python><csv><pandas>;Python Pandas read_csv skip rows but keep header;12332.0;"[""data = pd.read_csv('test.csv', sep='|', header=0, skiprows=10, nrows=10)\n""]";"[""data = pd.read_csv('test.csv', sep='|', header=0, skiprows=10, nrows=10)\n""]";"['skiprows', ""data = pd.read_csv('test.csv', sep='|', header=0, skiprows=10, nrows=10)\n""]";"[""data = pd.read_csv('test.csv', sep='|', header=0, skiprows=10, nrows=10)\n""]";"[""import pandas as pd\ndata = pd.read_csv('test.csv', sep='|', header=0, skiprows=10, nrows=10)\n""]";True;"[""import pandas as pd\ndata = pd.read_csv('test.csv', sep='|', header=0, skiprows=10, nrows=10)\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""File b'test.csv' does not exist""]";['FileNotFoundError'];0;1;"[""File b'test.csv' does not exist""]";['FileNotFoundError']
933;933;933;933;4.0;1;27365467;;1;32;<python><pandas><matplotlib><time-series>;python pandas: plot histogram of dates?;17415.0;"[""import pandas as pd\ndf = pd.read_csv('somefile.csv')\ncolumn = df['date']\ncolumn = pd.to_datetime(column, coerce=True)\nipdb> column.plot(kind='hist')\n*** TypeError: ufunc add cannot use operands with types dtype('<M8[ns]') and dtype('float64')\n""]";"[""import pandas as pd\ndf = pd.read_csv('somefile.csv')\ncolumn = df['date']\ncolumn = pd.to_datetime(column, coerce=True)\n"", ""ipdb> column.plot(kind='hist')\n*** TypeError: ufunc add cannot use operands with types dtype('<M8[ns]') and dtype('float64')\n""]";"['datetime64[ns]', ""import pandas as pd\ndf = pd.read_csv('somefile.csv')\ncolumn = df['date']\ncolumn = pd.to_datetime(column, coerce=True)\n"", ""ipdb> column.plot(kind='hist')\n*** TypeError: ufunc add cannot use operands with types dtype('<M8[ns]') and dtype('float64')\n"", 'pandas']";"[""import pandas as pd\ndf = pd.read_csv('somefile.csv')\ncolumn = df['date']\ncolumn = pd.to_datetime(column, coerce=True)\nipdb> column.plot(kind='hist')\n""]";"[""import pandas as pd\ndf = pd.read_csv('somefile.csv')\ncolumn = df['date']\ncolumn = pd.to_datetime(column, coerce=True)\nipdb> column.plot(kind='hist')\n""]";False;"[""import pandas as pd\nimport pandas as pd\ndf = pd.read_csv('somefile.csv')\ncolumn = df['date']\ncolumn = pd.to_datetime(column, coerce=True)\nipdb> column.plot(kind='hist')\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'date'""]";['KeyError']
934;934;934;934;2.0;7;27405483;;1;29;<python><pandas>;How to loop over grouped Pandas dataframe?;27652.0;"[""  c_os_family_ss c_os_major_is l_customer_id_i\n0      Windows 7                         90418\n1      Windows 7                         90418\n2      Windows 7                         90418\nprint df\nfor name, group in df.groupby('l_customer_id_i').agg(lambda x: ','.join(x)):\n    print name\n    print group\n                                                    c_os_family_ss  \\\nl_customer_id_i\n131572           Windows 7,Windows 7,Windows 7,Windows 7,Window...\n135467           Windows 7,Windows 7,Windows 7,Windows 7,Window...\n\n                                                     c_os_major_is\nl_customer_id_i\n131572           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...\n135467           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...\n""]";"['  c_os_family_ss c_os_major_is l_customer_id_i\n0      Windows 7                         90418\n1      Windows 7                         90418\n2      Windows 7                         90418\n', ""print df\nfor name, group in df.groupby('l_customer_id_i').agg(lambda x: ','.join(x)):\n    print name\n    print group\n"", '                                                    c_os_family_ss  \\\nl_customer_id_i\n131572           Windows 7,Windows 7,Windows 7,Windows 7,Window...\n135467           Windows 7,Windows 7,Windows 7,Windows 7,Window...\n\n                                                     c_os_major_is\nl_customer_id_i\n131572           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...\n135467           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...\n']";"['  c_os_family_ss c_os_major_is l_customer_id_i\n0      Windows 7                         90418\n1      Windows 7                         90418\n2      Windows 7                         90418\n', ""print df\nfor name, group in df.groupby('l_customer_id_i').agg(lambda x: ','.join(x)):\n    print name\n    print group\n"", '                                                    c_os_family_ss  \\\nl_customer_id_i\n131572           Windows 7,Windows 7,Windows 7,Windows 7,Window...\n135467           Windows 7,Windows 7,Windows 7,Windows 7,Window...\n\n                                                     c_os_major_is\nl_customer_id_i\n131572           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...\n135467           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...\n']";['l_customer_id_i\n\nl_customer_id_i\n'];['l_customer_id_i\n\nl_customer_id_i\n'];False;['import pandas as pd\nl_customer_id_i\n\nl_customer_id_i\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'A'""]";['KeyError']
935;935;935;935;2.0;3;27467730;;1;16;<python><pandas><dataframe>;Is there a way to copy only the structure (not the data) of a Pandas DataFrame?;6323.0;"[""import pandas as pd\ndf1 = pd.DataFrame([[11,12],[21,22]],columns=['c1','c2'],index=['i1','i2'])\ndf2 = pd.DataFrame(columns=df1.columns,index=df1.index)    \nIn [23]: df1\nOut[23]: \n    c1  c2\ni1  11  12\ni2  21  22\n\nIn [24]: df2\nOut[24]: \n     c1   c2\ni1  NaN  NaN\ni2  NaN  NaN\n""]";"[""import pandas as pd\ndf1 = pd.DataFrame([[11,12],[21,22]],columns=['c1','c2'],index=['i1','i2'])\n"", 'df2 = pd.DataFrame(columns=df1.columns,index=df1.index)    \n', 'In [23]: df1\nOut[23]: \n    c1  c2\ni1  11  12\ni2  21  22\n\nIn [24]: df2\nOut[24]: \n     c1   c2\ni1  NaN  NaN\ni2  NaN  NaN\n']";"[""import pandas as pd\ndf1 = pd.DataFrame([[11,12],[21,22]],columns=['c1','c2'],index=['i1','i2'])\n"", 'df2 = pd.DataFrame(columns=df1.columns,index=df1.index)    \n', 'df2 = df1.copy()', 'In [23]: df1\nOut[23]: \n    c1  c2\ni1  11  12\ni2  21  22\n\nIn [24]: df2\nOut[24]: \n     c1   c2\ni1  NaN  NaN\ni2  NaN  NaN\n']";['df1\ndf2\n'];['df1\ndf2\n'];False;['import pandas as pd\ndf2 = pd.DataFrame()\ndf1 = pd.DataFrame()\ndf = pd.DataFrame()\ndf1\ndf2\n'];True;0;2;"[""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df1' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""name 'empty_copy_2' is not defined""]";['Sucess', 'NameError']
936;936;936;936;5.0;4;27472548;;1;12;<python><python-3.x><matplotlib><pandas>;pandas scatter plotting datetime;5660.0;"['df.plot(kind=\'scatter\', x=\'T1\', y=\'T2\')\nplt.plot_date(x=df.loc[:,\'T1\'], y=df.loc[:,\'T2\'])\nplt.show()\nreturn _from_ordinalf(x, tz)\n  File ""/usr/lib/python3/dist-packages/matplotlib/dates.py"", line 224, in _from_ordinalf\nmicrosecond, tzinfo=UTC).astimezone(tz)\nTypeError: tzinfo argument must be None or of a tzinfo subclass, not type \'str\'\n']";"[""df.plot(kind='scatter', x='T1', y='T2')\n"", ""plt.plot_date(x=df.loc[:,'T1'], y=df.loc[:,'T2'])\nplt.show()\n"", 'return _from_ordinalf(x, tz)\n  File ""/usr/lib/python3/dist-packages/matplotlib/dates.py"", line 224, in _from_ordinalf\nmicrosecond, tzinfo=UTC).astimezone(tz)\nTypeError: tzinfo argument must be None or of a tzinfo subclass, not type \'str\'\n']";"[""df.plot(kind='scatter', x='T1', y='T2')\n"", ""plt.plot_date(x=df.loc[:,'T1'], y=df.loc[:,'T2'])\nplt.show()\n"", 'return _from_ordinalf(x, tz)\n  File ""/usr/lib/python3/dist-packages/matplotlib/dates.py"", line 224, in _from_ordinalf\nmicrosecond, tzinfo=UTC).astimezone(tz)\nTypeError: tzinfo argument must be None or of a tzinfo subclass, not type \'str\'\n']";"[""df.plot(kind='scatter', x='T1', y='T2')\nplt.plot_date(x=df.loc[:,'T1'], y=df.loc[:,'T2'])\nplt.show()\nreturn _from_ordinalf(x, tz)\n""]";"[""df.plot(kind='scatter', x='T1', y='T2')\nplt.plot_date(x=df.loc[:,'T1'], y=df.loc[:,'T2'])\nplt.show()\nreturn _from_ordinalf(x, tz)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf.plot(kind='scatter', x='T1', y='T2')\nplt.plot_date(x=df.loc[:,'T1'], y=df.loc[:,'T2'])\nplt.show()\nreturn _from_ordinalf(x, tz)\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'x'""]";['KeyError']
937;937;937;937;5.0;4;27474921;;1;19;<python><pandas><if-statement><dataframe>;Compare two columns using pandas;35053.0;"[""a = [['10', '1.2', '4.2'], ['15', '70', '0.03'], ['8', '5', '0']]\ndf = pd.DataFrame(a, columns=['one', 'two', 'three'])\n\nOut[8]: \n  one  two three\n0   10  1.2   4.2\n1   15  70   0.03\n2    8   5     0\nif df['one'] >= df['two'] and df['one'] <= df['three']:\n    df['que'] = df['one']\n""]";"[""a = [['10', '1.2', '4.2'], ['15', '70', '0.03'], ['8', '5', '0']]\ndf = pd.DataFrame(a, columns=['one', 'two', 'three'])\n\nOut[8]: \n  one  two three\n0   10  1.2   4.2\n1   15  70   0.03\n2    8   5     0\n"", ""if df['one'] >= df['two'] and df['one'] <= df['three']:\n    df['que'] = df['one']\n""]";"[""a = [['10', '1.2', '4.2'], ['15', '70', '0.03'], ['8', '5', '0']]\ndf = pd.DataFrame(a, columns=['one', 'two', 'three'])\n\nOut[8]: \n  one  two three\n0   10  1.2   4.2\n1   15  70   0.03\n2    8   5     0\n"", 'if', ""if df['one'] >= df['two'] and df['one'] <= df['three']:\n    df['que'] = df['one']\n"", 'if', '.all']";"[""a = [['10', '1.2', '4.2'], ['15', '70', '0.03'], ['8', '5', '0']]\ndf = pd.DataFrame(a, columns=['one', 'two', 'three'])\n\n""]";"[""import pandas as pd\na = [['10', '1.2', '4.2'], ['15', '70', '0.03'], ['8', '5', '0']]\ndf = pd.DataFrame(a, columns=['one', 'two', 'three'])\n\n""]";True;"[""import pandas as pd\na = [['10', '1.2', '4.2'], ['15', '70', '0.03'], ['8', '5', '0']]\ndf = pd.DataFrame(a, columns=['one', 'two', 'three'])\n\n""]";False;2;3;"[""name 'df' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess'];2;3;"[""name 'df' is not defined"", 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess'];2;3;['Wrong number of items passed 0, placement implies 1', 'Sucess', 'Sucess'];['ValueError', 'Sucess', 'Sucess']
938;938;938;938;3.0;3;27488080;;1;13;<python><pandas><filter><lambda><group-by>;Python pandas - filter rows after groupby;21194.0;"['index,A,B\n0,0,0\n1,0,8\n2,0,8\n3,1,0\n4,1,5\n0:\nindex,A,B\n0,0,0\n1,0,8\n2,0,8\n\n1:\nindex,A,B\n3,1,5\n4,1,3\ndata = <example table>\ngrouped = data.groupby(""A"")\nfiltered = grouped.filter(lambda x: x[""B""] == x[""B""].max())\n']";"['index,A,B\n0,0,0\n1,0,8\n2,0,8\n3,1,0\n4,1,5\n', '0:\nindex,A,B\n0,0,0\n1,0,8\n2,0,8\n\n1:\nindex,A,B\n3,1,5\n4,1,3\n', 'data = <example table>\ngrouped = data.groupby(""A"")\nfiltered = grouped.filter(lambda x: x[""B""] == x[""B""].max())\n']";"['index,A,B\n0,0,0\n1,0,8\n2,0,8\n3,1,0\n4,1,5\n', 'A', '0:\nindex,A,B\n0,0,0\n1,0,8\n2,0,8\n\n1:\nindex,A,B\n3,1,5\n4,1,3\n', 'B', 'B', 'B', '0', '0', '1', '2', 'B', '1', '4', '3', 'data = <example table>\ngrouped = data.groupby(""A"")\nfiltered = grouped.filter(lambda x: x[""B""] == x[""B""].max())\n', 'DataFrame']";"['index,A,B\n0,0,0\n1,0,8\n2,0,8\n3,1,0\n4,1,5\nindex,A,B\n0,0,0\n1,0,8\n2,0,8\n\nindex,A,B\n3,1,5\n4,1,3\ngrouped = data.groupby(""A"")\nfiltered = grouped.filter(lambda x: x[""B""] == x[""B""].max())\n']";"['index,A,B\n0,0,0\n1,0,8\n2,0,8\n3,1,0\n4,1,5\nindex,A,B\n0,0,0\n1,0,8\n2,0,8\n\nindex,A,B\n3,1,5\n4,1,3\ngrouped = data.groupby(""A"")\nfiltered = grouped.filter(lambda x: x[""B""] == x[""B""].max())\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\nindex,A,B\n0,0,0\n1,0,8\n2,0,8\n3,1,0\n4,1,5\nindex,A,B\n0,0,0\n1,0,8\n2,0,8\n\nindex,A,B\n3,1,5\n4,1,3\ngrouped = data.groupby(""A"")\nfiltered = grouped.filter(lambda x: x[""B""] == x[""B""].max())\n']";True;0;2;"[""name 'csv' is not defined"", ""name 'df_lots_groups' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'csv' is not defined"", ""name 'df_lots_groups' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'csv' is not defined"", ""name 'df_lots_groups' is not defined""]";['NameError', 'NameError']
939;939;939;939;1.0;2;27517425;;1;38;<python><pandas>;Apply vs transform on a group object;19045.0;"[""     A      B         C         D\n0  foo    one  0.162003  0.087469\n1  bar    one -1.156319 -1.526272\n2  foo    two  0.833892 -1.666304\n3  bar  three -2.026673 -0.322057\n4  foo    two  0.411452 -0.954371\n5  bar    two  0.765878 -0.095968\n6  foo    one -0.654890  0.678091\n7  foo  three -1.789842 -1.130922\n> df.groupby('A').apply(lambda x: (x['C'] - x['D']))\n> df.groupby('A').apply(lambda x: (x['C'] - x['D']).mean())\n> df.groupby('A').transform(lambda x: (x['C'] - x['D']))\nValueError: could not broadcast input array from shape (5) into shape (5,3)\n\n> df.groupby('A').transform(lambda x: (x['C'] - x['D']).mean())\n TypeError: cannot concatenate a non-NDFrame object\n# Note that the following suggests row-wise operation (x.mean is the column mean)\nzscore = lambda x: (x - x.mean()) / x.std()\ntransformed = ts.groupby(key).transform(zscore)\ndf = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n                          'foo', 'bar', 'foo', 'foo'],\n                   'B' : ['one', 'one', 'two', 'three',\n                         'two', 'two', 'one', 'three'],\n                   'C' : randn(8), 'D' : randn(8)})\n""]";"['     A      B         C         D\n0  foo    one  0.162003  0.087469\n1  bar    one -1.156319 -1.526272\n2  foo    two  0.833892 -1.666304\n3  bar  three -2.026673 -0.322057\n4  foo    two  0.411452 -0.954371\n5  bar    two  0.765878 -0.095968\n6  foo    one -0.654890  0.678091\n7  foo  three -1.789842 -1.130922\n', ""> df.groupby('A').apply(lambda x: (x['C'] - x['D']))\n> df.groupby('A').apply(lambda x: (x['C'] - x['D']).mean())\n"", ""> df.groupby('A').transform(lambda x: (x['C'] - x['D']))\nValueError: could not broadcast input array from shape (5) into shape (5,3)\n\n> df.groupby('A').transform(lambda x: (x['C'] - x['D']).mean())\n TypeError: cannot concatenate a non-NDFrame object\n"", '# Note that the following suggests row-wise operation (x.mean is the column mean)\nzscore = lambda x: (x - x.mean()) / x.std()\ntransformed = ts.groupby(key).transform(zscore)\n', ""df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n                          'foo', 'bar', 'foo', 'foo'],\n                   'B' : ['one', 'one', 'two', 'three',\n                         'two', 'two', 'one', 'three'],\n                   'C' : randn(8), 'D' : randn(8)})\n""]";"['     A      B         C         D\n0  foo    one  0.162003  0.087469\n1  bar    one -1.156319 -1.526272\n2  foo    two  0.833892 -1.666304\n3  bar  three -2.026673 -0.322057\n4  foo    two  0.411452 -0.954371\n5  bar    two  0.765878 -0.095968\n6  foo    one -0.654890  0.678091\n7  foo  three -1.789842 -1.130922\n', ""> df.groupby('A').apply(lambda x: (x['C'] - x['D']))\n> df.groupby('A').apply(lambda x: (x['C'] - x['D']).mean())\n"", ""> df.groupby('A').transform(lambda x: (x['C'] - x['D']))\nValueError: could not broadcast input array from shape (5) into shape (5,3)\n\n> df.groupby('A').transform(lambda x: (x['C'] - x['D']).mean())\n TypeError: cannot concatenate a non-NDFrame object\n"", 'transform', '# Note that the following suggests row-wise operation (x.mean is the column mean)\nzscore = lambda x: (x - x.mean()) / x.std()\ntransformed = ts.groupby(key).transform(zscore)\n', ""df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n                          'foo', 'bar', 'foo', 'foo'],\n                   'B' : ['one', 'one', 'two', 'three',\n                         'two', 'two', 'one', 'three'],\n                   'C' : randn(8), 'D' : randn(8)})\n""]";['\n# Note that the following suggests row-wise operation (x.mean is the column mean)\nzscore = lambda x: (x - x.mean()) / x.std()\ntransformed = ts.groupby(key).transform(zscore)\n'];['\n# Note that the following suggests row-wise operation (x.mean is the column mean)\nzscore = lambda x: (x - x.mean()) / x.std()\ntransformed = ts.groupby(key).transform(zscore)\n'];False;['import pandas as pd\n\n# Note that the following suggests row-wise operation (x.mean is the column mean)\nzscore = lambda x: (x - x.mean()) / x.std()\ntransformed = ts.groupby(key).transform(zscore)\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'A'""]";['KeyError']
940;940;940;940;1.0;1;27667759;;1;53;<python><r><pandas>;Is .ix() always better than .loc() and .iloc() since it is faster and supports integer and label access?;38433.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
941;941;941;941;3.0;2;27673231;;1;36;<pandas>;why should I make a copy of a data frame in pandas;28042.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'DataFrame' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"['\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy', ""name 'df' is not defined""]";['SettingWithCopyWarning', 'NameError'];0;2;"[""name 'DataFrame' is not defined"", ""name 'func1' is not defined""]";['NameError', 'NameError']
942;942;942;942;3.0;1;27759084;;1;12;<python><replace><pandas>;How to replace negative numbers in Pandas Data Frame by zero;20051.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'DataFrame' is not defined""]";['NameError', 'NameError'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"['Sucess', ""name 'DataFrame' is not defined""]";['Sucess', 'NameError']
943;943;943;943;1.0;0;27787930;;1;14;<python><pandas><grouping>;How to get number of groups in a groupby object in pandas?;5235.0;[''];[];['dfgroup'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
944;944;944;944;3.0;0;27842613;;1;43;<python><sorting><pandas><group-by>;pandas groupby sort within groups;37771.0;"[""In [167]:\ndf\n\nOut[167]:\ncount   job source\n0   2   sales   A\n1   4   sales   B\n2   6   sales   C\n3   3   sales   D\n4   7   sales   E\n5   5   market  A\n6   3   market  B\n7   2   market  C\n8   4   market  D\n9   1   market  E\n\nIn [168]:\ndf.groupby(['job','source']).agg({'count':sum})\n\nOut[168]:\n            count\njob     source  \nmarket  A   5\n        B   3\n        C   2\n        D   4\n        E   1\nsales   A   2\n        B   4\n        C   6\n        D   3\n        E   7\n            count\njob     source  \nmarket  A   5\n        D   4\n        B   3\nsales   E   7\n        C   6\n        B   4\n""]";"[""In [167]:\ndf\n\nOut[167]:\ncount   job source\n0   2   sales   A\n1   4   sales   B\n2   6   sales   C\n3   3   sales   D\n4   7   sales   E\n5   5   market  A\n6   3   market  B\n7   2   market  C\n8   4   market  D\n9   1   market  E\n\nIn [168]:\ndf.groupby(['job','source']).agg({'count':sum})\n\nOut[168]:\n            count\njob     source  \nmarket  A   5\n        B   3\n        C   2\n        D   4\n        E   1\nsales   A   2\n        B   4\n        C   6\n        D   3\n        E   7\n"", '            count\njob     source  \nmarket  A   5\n        D   4\n        B   3\nsales   E   7\n        C   6\n        B   4\n']";"[""In [167]:\ndf\n\nOut[167]:\ncount   job source\n0   2   sales   A\n1   4   sales   B\n2   6   sales   C\n3   3   sales   D\n4   7   sales   E\n5   5   market  A\n6   3   market  B\n7   2   market  C\n8   4   market  D\n9   1   market  E\n\nIn [168]:\ndf.groupby(['job','source']).agg({'count':sum})\n\nOut[168]:\n            count\njob     source  \nmarket  A   5\n        B   3\n        C   2\n        D   4\n        E   1\nsales   A   2\n        B   4\n        C   6\n        D   3\n        E   7\n"", '            count\njob     source  \nmarket  A   5\n        D   4\n        B   3\nsales   E   7\n        C   6\n        B   4\n']";"[""df\n\ndf.groupby(['job','source']).agg({'count':sum})\n\n""]";"[""df\n\ndf.groupby(['job','source']).agg({'count':sum})\n\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf\n\ndf.groupby(['job','source']).agg({'count':sum})\n\n""]";True;1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""'job'"", 'Sucess']";['KeyError', 'Sucess']
945;945;945;945;2.0;0;27884268;;1;19;<python><postgresql><pandas><sqlalchemy>;Return Pandas dataframe from PostgreSQL query with sqlalchemy;16640.0;"['from sqlalchemy import create_engine\nengine = create_engine(\'postgresql://user@localhost:5432/mydb\')\ni=pd.read_csv(path)\ni.to_sql(\'Stat_Table\',engine,if_exists=\'replace\')\na=pd.read_sql_query(\'select * from Stat_Table\',con=engine)\nProgrammingError: (ProgrammingError) relation ""stat_table"" does not exist\n']";"[""from sqlalchemy import create_engine\nengine = create_engine('postgresql://user@localhost:5432/mydb')\n"", ""i=pd.read_csv(path)\ni.to_sql('Stat_Table',engine,if_exists='replace')\n"", ""a=pd.read_sql_query('select * from Stat_Table',con=engine)\n"", 'ProgrammingError: (ProgrammingError) relation ""stat_table"" does not exist\n']";"['sqlalchemy', ""from sqlalchemy import create_engine\nengine = create_engine('postgresql://user@localhost:5432/mydb')\n"", ""i=pd.read_csv(path)\ni.to_sql('Stat_Table',engine,if_exists='replace')\n"", ""a=pd.read_sql_query('select * from Stat_Table',con=engine)\n"", 'ProgrammingError: (ProgrammingError) relation ""stat_table"" does not exist\n']";"[""from sqlalchemy import create_engine\nengine = create_engine('postgresql://user@localhost:5432/mydb')\ni=pd.read_csv(path)\ni.to_sql('Stat_Table',engine,if_exists='replace')\na=pd.read_sql_query('select * from Stat_Table',con=engine)\n""]";"[""import pandas as pd\nfrom sqlalchemy import create_engine\nengine = create_engine('postgresql://user@localhost:5432/mydb')\ni=pd.read_csv(path)\ni.to_sql('Stat_Table',engine,if_exists='replace')\na=pd.read_sql_query('select * from Stat_Table',con=engine)\n""]";True;"[""import pandas as pd\nfrom sqlalchemy import create_engine\nengine = create_engine('postgresql://user@localhost:5432/mydb')\ni=pd.read_csv(path)\ni.to_sql('Stat_Table',engine,if_exists='replace')\na=pd.read_sql_query('select * from Stat_Table',con=engine)\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'engine' is not defined""]";['NameError'];0;1;"[""name 'engine' is not defined""]";['NameError']
946;946;946;946;1.0;6;27896214;;1;13;<python><osx><pandas><import><tab-delimited>;Reading tab-delimited file with Pandas - works on Windows, but not on Mac;23577.0;"[""df = pd.read_csv(myfile,sep='\\t',skiprows=(0,1,2),header=(0))\npandas.parser.CParserError: Error tokenizing data. C error: Expected 1\nfields in line 8, saw 39\nSkipping line 8: expected 1 fields, saw 39\nSkipping line 9: expected 1 fields, saw 125\nSkipping line 10: expected 1 fields, saw 125\nSkipping line 11: expected 1 fields, saw 125\nSkipping line 12: expected 1 fields, saw 125\nSkipping line 13: expected 1 fields, saw 125\nSkipping line 14: expected 1 fields, saw 125\nSkipping line 15: expected 1 fields, saw 125\nSkipping line 16: expected 1 fields, saw 125\nSkipping line 17: expected 1 fields, saw 125\n...\n""]";"[""df = pd.read_csv(myfile,sep='\\t',skiprows=(0,1,2),header=(0))\n"", 'pandas.parser.CParserError: Error tokenizing data. C error: Expected 1\nfields in line 8, saw 39\n', 'Skipping line 8: expected 1 fields, saw 39\nSkipping line 9: expected 1 fields, saw 125\nSkipping line 10: expected 1 fields, saw 125\nSkipping line 11: expected 1 fields, saw 125\nSkipping line 12: expected 1 fields, saw 125\nSkipping line 13: expected 1 fields, saw 125\nSkipping line 14: expected 1 fields, saw 125\nSkipping line 15: expected 1 fields, saw 125\nSkipping line 16: expected 1 fields, saw 125\nSkipping line 17: expected 1 fields, saw 125\n...\n']";"[""df = pd.read_csv(myfile,sep='\\t',skiprows=(0,1,2),header=(0))\n"", 'pandas.parser.CParserError: Error tokenizing data. C error: Expected 1\nfields in line 8, saw 39\n', 'Skipping line 8: expected 1 fields, saw 39\nSkipping line 9: expected 1 fields, saw 125\nSkipping line 10: expected 1 fields, saw 125\nSkipping line 11: expected 1 fields, saw 125\nSkipping line 12: expected 1 fields, saw 125\nSkipping line 13: expected 1 fields, saw 125\nSkipping line 14: expected 1 fields, saw 125\nSkipping line 15: expected 1 fields, saw 125\nSkipping line 16: expected 1 fields, saw 125\nSkipping line 17: expected 1 fields, saw 125\n...\n']";"[""df = pd.read_csv(myfile,sep='\\t',skiprows=(0,1,2),header=(0))\n...\n""]";"[""import pandas as pd\ndf = pd.read_csv(myfile,sep='\\t',skiprows=(0,1,2),header=(0))\n...\n""]";True;"[""import pandas as pd\ndf = pd.read_csv(myfile,sep='\\t',skiprows=(0,1,2),header=(0))\n...\n""]";False;0;1;"[""name 'pandas' is not defined""]";['NameError'];0;1;"[""name 'pandas' is not defined""]";['NameError'];0;1;"[""name 'pandas' is not defined""]";['NameError']
947;947;947;947;5.0;0;27905295;;1;23;<python><python-3.x><pandas><dataframe><nan>;How to replace NaNs by preceding values in pandas DataFrame?;14351.0;['>>> import pandas as pd\n>>> df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])\n>>> df\n    0   1   2\n0   1   2   3\n1   4 NaN NaN\n2 NaN NaN   9\n   0  1  2\n0  1  2  3\n1  4  2  3\n2  4  2  9\n'];['>>> import pandas as pd\n>>> df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])\n>>> df\n    0   1   2\n0   1   2   3\n1   4 NaN NaN\n2 NaN NaN   9\n', '   0  1  2\n0  1  2  3\n1  4  2  3\n2  4  2  9\n'];['NaN', '>>> import pandas as pd\n>>> df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])\n>>> df\n    0   1   2\n0   1   2   3\n1   4 NaN NaN\n2 NaN NaN   9\n', 'NaN', 'NaN', 'NaN', '   0  1  2\n0  1  2  3\n1  4  2  3\n2  4  2  9\n'];[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
948;948;948;948;4.0;1;27975069;;1;26;<python><pandas>;How to filter rows containing a string pattern from a Pandas dataframe;35532.0;"[""df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': [u'aball', u'bball', u'cnut', u'fball']})\nids    vals\naball   1\nbball   2\ncnut    3\nfball   4\nids    vals\naball   1\nbball   2\nfball   4\n""]";"[""df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': [u'aball', u'bball', u'cnut', u'fball']})\n"", 'ids    vals\naball   1\nbball   2\ncnut    3\nfball   4\n', 'ids    vals\naball   1\nbball   2\nfball   4\n']";"[""df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': [u'aball', u'bball', u'cnut', u'fball']})\n"", 'ids    vals\naball   1\nbball   2\ncnut    3\nfball   4\n', 'ids    vals\naball   1\nbball   2\nfball   4\n']";"[""df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': [u'aball', u'bball', u'cnut', u'fball']})\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': [u'aball', u'bball', u'cnut', u'fball']})\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': [u'aball', u'bball', u'cnut', u'fball']})\n""]";False;1;3;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""'ids'"", ""'ids'""]";['Sucess', 'KeyError', 'KeyError']
949;949;949;949;2.0;1;28006793;;1;27;<python><pandas>;Pandas DataFrame to List of Lists;27158.0;['import pandas as pd\ndf = pd.DataFrame([[1,2,3],[3,4,5]])\nlol = df.what_to_do_now?\nprint lol\n# [[1,2,3],[3,4,5]]\n'];['import pandas as pd\ndf = pd.DataFrame([[1,2,3],[3,4,5]])\n', 'lol = df.what_to_do_now?\nprint lol\n# [[1,2,3],[3,4,5]]\n'];['import pandas as pd\ndf = pd.DataFrame([[1,2,3],[3,4,5]])\n', 'lol = df.what_to_do_now?\nprint lol\n# [[1,2,3],[3,4,5]]\n'];['import pandas as pd\ndf = pd.DataFrame([[1,2,3],[3,4,5]])\n# [[1,2,3],[3,4,5]]\n'];['import pandas as pd\ndf = pd.DataFrame([[1,2,3],[3,4,5]])\n# [[1,2,3],[3,4,5]]\n'];False;['import pandas as pd\nimport pandas as pd\ndf = pd.DataFrame([[1,2,3],[3,4,5]])\n# [[1,2,3],[3,4,5]]\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
950;950;950;950;1.0;0;28009370;;1;16;<python><pandas>;Get weekday/day-of-week for Datetime column of DataFrame;17538.0;['Timestamp              Value\n2012-06-01 00:00:00     100\n2012-06-01 00:15:00     150\n2012-06-01 00:30:00     120\n2012-06-01 01:00:00     220\n2012-06-01 01:15:00      80\n...and so on.\n'];['Timestamp              Value\n2012-06-01 00:00:00     100\n2012-06-01 00:15:00     150\n2012-06-01 00:30:00     120\n2012-06-01 01:00:00     220\n2012-06-01 01:15:00      80\n...and so on.\n'];"['df', 'Timestamp              Value\n2012-06-01 00:00:00     100\n2012-06-01 00:15:00     150\n2012-06-01 00:30:00     120\n2012-06-01 01:00:00     220\n2012-06-01 01:15:00      80\n...and so on.\n', ""df['weekday']""]";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError']
951;951;951;951;2.0;1;28017091;;1;17;<python><pandas><scikit-learn><cluster-analysis><k-means>;Will pandas dataframe object work with sklearn kmeans clustering?;15064.0;[' km = KMeans(n_clusters = n_Clusters)\n\n km.fit(dataset)\n\n prediction = km.predict(dataset)\n for i in range(len(prediction)):\n     cluster_fit_dict[dataset.index[i]] = prediction[i]\n A 1 2 3 4 5 6\n B 2 3 4 5 6 7\n C 1 4 2 7 8 1\n ...\n'];[' km = KMeans(n_clusters = n_Clusters)\n\n km.fit(dataset)\n\n prediction = km.predict(dataset)\n', ' for i in range(len(prediction)):\n     cluster_fit_dict[dataset.index[i]] = prediction[i]\n', ' A 1 2 3 4 5 6\n B 2 3 4 5 6 7\n C 1 4 2 7 8 1\n ...\n'];[' km = KMeans(n_clusters = n_Clusters)\n\n km.fit(dataset)\n\n prediction = km.predict(dataset)\n', ' for i in range(len(prediction)):\n     cluster_fit_dict[dataset.index[i]] = prediction[i]\n', ' A 1 2 3 4 5 6\n B 2 3 4 5 6 7\n C 1 4 2 7 8 1\n ...\n'];['\n\n'];['\n\n'];False;['import pandas as pd\n\n\n'];False;0;2;"[""name 'dataset' is not defined"", ""name 'dataset' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'dataset' is not defined"", ""name 'dataset' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'dataset' is not defined"", ""name 'dataset' is not defined""]";['NameError', 'NameError']
952;952;952;952;1.0;0;28135436;;1;18;<python><pandas><dataframe>;Concatenate rows of two dataframes in pandas;37294.0;"[""import pandas as pd\ndict_data = {'Treatment': ['C', 'C', 'C'], 'Biorep': ['A', 'A', 'A'], 'Techrep': [1, 1, 1], 'AAseq': ['ELVISLIVES', 'ELVISLIVES', 'ELVISLIVES'], 'mz':[500.0, 500.5, 501.0]}\ndf_a = pd.DataFrame(dict_data)\ndict_data = {'Treatment1': ['C', 'C', 'C'], 'Biorep1': ['A', 'A', 'A'], 'Techrep1': [1, 1, 1], 'AAseq1': ['ELVISLIVES', 'ELVISLIVES', 'ELVISLIVES'], 'inte1':[1100.0, 1050.0, 1010.0]}\ndf_b = pd.DataFrame(dict_data)\n""]";"[""import pandas as pd\ndict_data = {'Treatment': ['C', 'C', 'C'], 'Biorep': ['A', 'A', 'A'], 'Techrep': [1, 1, 1], 'AAseq': ['ELVISLIVES', 'ELVISLIVES', 'ELVISLIVES'], 'mz':[500.0, 500.5, 501.0]}\ndf_a = pd.DataFrame(dict_data)\ndict_data = {'Treatment1': ['C', 'C', 'C'], 'Biorep1': ['A', 'A', 'A'], 'Techrep1': [1, 1, 1], 'AAseq1': ['ELVISLIVES', 'ELVISLIVES', 'ELVISLIVES'], 'inte1':[1100.0, 1050.0, 1010.0]}\ndf_b = pd.DataFrame(dict_data)\n""]";"['df_a', 'df_b', 'nRow', 'cbind', 'R programming language', 'nRow', ""import pandas as pd\ndict_data = {'Treatment': ['C', 'C', 'C'], 'Biorep': ['A', 'A', 'A'], 'Techrep': [1, 1, 1], 'AAseq': ['ELVISLIVES', 'ELVISLIVES', 'ELVISLIVES'], 'mz':[500.0, 500.5, 501.0]}\ndf_a = pd.DataFrame(dict_data)\ndict_data = {'Treatment1': ['C', 'C', 'C'], 'Biorep1': ['A', 'A', 'A'], 'Techrep1': [1, 1, 1], 'AAseq1': ['ELVISLIVES', 'ELVISLIVES', 'ELVISLIVES'], 'inte1':[1100.0, 1050.0, 1010.0]}\ndf_b = pd.DataFrame(dict_data)\n""]";"[""import pandas as pd\ndict_data = {'Treatment': ['C', 'C', 'C'], 'Biorep': ['A', 'A', 'A'], 'Techrep': [1, 1, 1], 'AAseq': ['ELVISLIVES', 'ELVISLIVES', 'ELVISLIVES'], 'mz':[500.0, 500.5, 501.0]}\ndf_a = pd.DataFrame(dict_data)\ndict_data = {'Treatment1': ['C', 'C', 'C'], 'Biorep1': ['A', 'A', 'A'], 'Techrep1': [1, 1, 1], 'AAseq1': ['ELVISLIVES', 'ELVISLIVES', 'ELVISLIVES'], 'inte1':[1100.0, 1050.0, 1010.0]}\ndf_b = pd.DataFrame(dict_data)\n""]";"[""import pandas as pd\ndict_data = {'Treatment': ['C', 'C', 'C'], 'Biorep': ['A', 'A', 'A'], 'Techrep': [1, 1, 1], 'AAseq': ['ELVISLIVES', 'ELVISLIVES', 'ELVISLIVES'], 'mz':[500.0, 500.5, 501.0]}\ndf_a = pd.DataFrame(dict_data)\ndict_data = {'Treatment1': ['C', 'C', 'C'], 'Biorep1': ['A', 'A', 'A'], 'Techrep1': [1, 1, 1], 'AAseq1': ['ELVISLIVES', 'ELVISLIVES', 'ELVISLIVES'], 'inte1':[1100.0, 1050.0, 1010.0]}\ndf_b = pd.DataFrame(dict_data)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\ndict_data = {'Treatment': ['C', 'C', 'C'], 'Biorep': ['A', 'A', 'A'], 'Techrep': [1, 1, 1], 'AAseq': ['ELVISLIVES', 'ELVISLIVES', 'ELVISLIVES'], 'mz':[500.0, 500.5, 501.0]}\ndf_a = pd.DataFrame(dict_data)\ndict_data = {'Treatment1': ['C', 'C', 'C'], 'Biorep1': ['A', 'A', 'A'], 'Techrep1': [1, 1, 1], 'AAseq1': ['ELVISLIVES', 'ELVISLIVES', 'ELVISLIVES'], 'inte1':[1100.0, 1050.0, 1010.0]}\ndf_b = pd.DataFrame(dict_data)\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'df_a' is not defined""]";['NameError'];0;1;"[""name 'df_a' is not defined""]";['NameError']
953;953;953;953;3.0;0;28142420;;1;11;<python><excel><pandas>;Can Pandas read and modify a single Excel file worksheet (tab) without modifying the rest of the file?;3756.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError']
954;954;954;954;2.0;0;28161356;;1;16;<python><pandas>;Sort Pandas Dataframe by Date;25652.0;['Symbol  Date\nA       02/20/2015\nA       01/15/2016\nA       08/21/2015\n'];['Symbol  Date\nA       02/20/2015\nA       01/15/2016\nA       08/21/2015\n'];['Symbol  Date\nA       02/20/2015\nA       01/15/2016\nA       08/21/2015\n', 'Date', 'object', '2015-02-20,'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
955;955;955;955;6.0;0;28199524;;1;11;<python><pandas><missing-data>;Best way to count the number of rows with missing values in a pandas DataFrame;14012.0;"[""from numpy.random import randn\ndf = pd.DataFrame(randn(5, 3), index=['a', 'c', 'e', 'f', 'h'],\n               columns=['one', 'two', 'three'])\ndf = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\n>>> sum(df.isnull().values.ravel())\n9\n>>> sum([True for idx,row in df.iterrows() if any(row.isnull())])\n3\n""]";"[""from numpy.random import randn\ndf = pd.DataFrame(randn(5, 3), index=['a', 'c', 'e', 'f', 'h'],\n               columns=['one', 'two', 'three'])\ndf = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\n"", '>>> sum(df.isnull().values.ravel())\n9\n', '>>> sum([True for idx,row in df.iterrows() if any(row.isnull())])\n3\n']";"['DataFrame', 'DataFrame', ""from numpy.random import randn\ndf = pd.DataFrame(randn(5, 3), index=['a', 'c', 'e', 'f', 'h'],\n               columns=['one', 'two', 'three'])\ndf = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\n"", '>>> sum(df.isnull().values.ravel())\n9\n', '>>> sum([True for idx,row in df.iterrows() if any(row.isnull())])\n3\n']";"[""from numpy.random import randn\ndf = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\n9\n3\n""]";"[""from numpy.random import randn\ndf = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\n9\n3\n""]";False;"[""import pandas as pd\nfrom numpy.random import randn\ndf = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\n9\n3\n""]";False;1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'df' is not defined"", 'Sucess']";['NameError', 'Sucess']
956;956;956;956;7.0;1;28218698;;1;42;<python><pandas><statsmodels>;How to iterate over columns of pandas dataframe to run regression;71906.0;"[""all_data = {}\nfor ticker in ['FIUIX', 'FSAIX', 'FSAVX', 'FSTMX']:\n    all_data[ticker] = web.get_data_yahoo(ticker, '1/1/2010', '1/1/2015')\n\nprices = DataFrame({tic: data['Adj Close'] for tic, data in all_data.iteritems()})  \nreturns = prices.pct_change()\nregs = sm.OLS(returns.FIUIX,returns.FSTMX).fit()\nresids = {}\nfor k in returns.keys():\n    reg = sm.OLS(returns[k],returns.FSTMX).fit()\n    resids[k] = reg.resid\n""]";"[""all_data = {}\nfor ticker in ['FIUIX', 'FSAIX', 'FSAVX', 'FSTMX']:\n    all_data[ticker] = web.get_data_yahoo(ticker, '1/1/2010', '1/1/2015')\n\nprices = DataFrame({tic: data['Adj Close'] for tic, data in all_data.iteritems()})  \nreturns = prices.pct_change()\n"", 'regs = sm.OLS(returns.FIUIX,returns.FSTMX).fit()\n', 'resids = {}\nfor k in returns.keys():\n    reg = sm.OLS(returns[k],returns.FSTMX).fit()\n    resids[k] = reg.resid\n']";"['pandas', ""all_data = {}\nfor ticker in ['FIUIX', 'FSAIX', 'FSAVX', 'FSTMX']:\n    all_data[ticker] = web.get_data_yahoo(ticker, '1/1/2010', '1/1/2015')\n\nprices = DataFrame({tic: data['Adj Close'] for tic, data in all_data.iteritems()})  \nreturns = prices.pct_change()\n"", 'regs = sm.OLS(returns.FIUIX,returns.FSTMX).fit()\n', 'resids = {}\nfor k in returns.keys():\n    reg = sm.OLS(returns[k],returns.FSTMX).fit()\n    resids[k] = reg.resid\n', 'returns[k]']";"[""all_data = {}\n\nprices = DataFrame({tic: data['Adj Close'] for tic, data in all_data.iteritems()})  \nreturns = prices.pct_change()\nregs = sm.OLS(returns.FIUIX,returns.FSTMX).fit()\nresids = {}\n""]";"[""from pandas import DataFrame\nall_data = {}\n\nprices = DataFrame({tic: data['Adj Close'] for tic, data in all_data.iteritems()})  \nreturns = prices.pct_change()\nregs = sm.OLS(returns.FIUIX,returns.FSTMX).fit()\nresids = {}\n""]";True;"[""import pandas as pd\nall_data = {}\n\nprices = DataFrame({tic: data['Adj Close'] for tic, data in all_data.iteritems()})  \nreturns = prices.pct_change()\nregs = sm.OLS(returns.FIUIX,returns.FSTMX).fit()\nresids = {}\n""]";False;3;4;"[""name 'df1' is not defined"", 'Sucess', 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess', 'Sucess'];3;4;"[""name 'df1' is not defined"", 'Sucess', 'Sucess', 'Sucess']";['NameError', 'Sucess', 'Sucess', 'Sucess'];3;4;['\n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated', 'Sucess', 'Sucess', 'Sucess'];['DeprecationWarning', 'Sucess', 'Sucess', 'Sucess']
957;957;957;957;1.0;23;28219902;;1;11;<python><parsing><pandas><numpy><ipython>;Pandas read_csv on 6.5 GB file consumes more than 170GB RAM;878.0;"['h = open(""ms.txt"")\nheader = h.readline().split(""\\t"")\nh.close()\nrows=1100\ndf = pd.DataFrame(columns=header, index=range(rows), dtype=int)\n']";"['h = open(""ms.txt"")\nheader = h.readline().split(""\\t"")\nh.close()\nrows=1100\ndf = pd.DataFrame(columns=header, index=range(rows), dtype=int)\n']";"['h = open(""ms.txt"")\nheader = h.readline().split(""\\t"")\nh.close()\nrows=1100\ndf = pd.DataFrame(columns=header, index=range(rows), dtype=int)\n']";"['h = open(""ms.txt"")\nheader = h.readline().split(""\\t"")\nh.close()\nrows=1100\ndf = pd.DataFrame(columns=header, index=range(rows), dtype=int)\n']";"['import pandas as pd\nh = open(""ms.txt"")\nheader = h.readline().split(""\\t"")\nh.close()\nrows=1100\ndf = pd.DataFrame(columns=header, index=range(rows), dtype=int)\n']";True;"['import pandas as pd\nh = open(""ms.txt"")\nheader = h.readline().split(""\\t"")\nh.close()\nrows=1100\ndf = pd.DataFrame(columns=header, index=range(rows), dtype=int)\n']";False;0;0;[];[];0;0;[];[];0;0;[];[]
958;958;958;958;2.0;0;28227612;;1;13;<python><pandas>;how to convert a list into a pandas dataframe;70273.0;"['rows =[]\nfor dt in new_info:\n    x =  dt[\'state\']\n    est = dt[\'estimates\']\n\n    col_R = [val[\'choice\'] for val in est if val[\'party\'] == \'Rep\']\n    col_D = [val[\'choice\'] for val in est if val[\'party\'] == \'Dem\']\n\n    incumb = [val[\'party\'] for val in est if val[\'incumbent\'] == True ]\n\n    rows.append((x, col_R, col_D, incumb))\npd.DataFrame(rows, columns=[""State"", ""R"", ""D"", ""incumbent""])  \n']";"[""rows =[]\nfor dt in new_info:\n    x =  dt['state']\n    est = dt['estimates']\n\n    col_R = [val['choice'] for val in est if val['party'] == 'Rep']\n    col_D = [val['choice'] for val in est if val['party'] == 'Dem']\n\n    incumb = [val['party'] for val in est if val['incumbent'] == True ]\n\n    rows.append((x, col_R, col_D, incumb))\n"", 'pd.DataFrame(rows, columns=[""State"", ""R"", ""D"", ""incumbent""])  \n']";"[""rows =[]\nfor dt in new_info:\n    x =  dt['state']\n    est = dt['estimates']\n\n    col_R = [val['choice'] for val in est if val['party'] == 'Rep']\n    col_D = [val['choice'] for val in est if val['party'] == 'Dem']\n\n    incumb = [val['party'] for val in est if val['incumbent'] == True ]\n\n    rows.append((x, col_R, col_D, incumb))\n"", 'pd.DataFrame(rows, columns=[""State"", ""R"", ""D"", ""incumbent""])  \n']";"['rows =[]\n\n\n\npd.DataFrame(rows, columns=[""State"", ""R"", ""D"", ""incumbent""])  \n']";"['import pandas as pd\nrows =[]\n\n\n\npd.DataFrame(rows, columns=[""State"", ""R"", ""D"", ""incumbent""])  \n']";True;"['import pandas as pd\nrows =[]\n\n\n\npd.DataFrame(rows, columns=[""State"", ""R"", ""D"", ""incumbent""])  \n']";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
959;959;959;959;2.0;2;28228781;;1;15;<python><join><pandas><inner-join>;Python Pandas inner join;13222.0;"['merged = DataFrameA.join(DataFrameB, on=[\'Code\',\'Date\'])\nValueError: len(left_on) must equal the number of levels in the index of ""right""\nDataFrameA:  Code, Date, ColA, ColB, ColC, ..., ColG, ColH (shape: 80514, 8 - no index)\nDataFrameB:  Date, Code, Col1, Col2, Col3, ..., Col15, Col16 (shape: 859, 16 - no index)\n']";"[""merged = DataFrameA.join(DataFrameB, on=['Code','Date'])\n"", 'ValueError: len(left_on) must equal the number of levels in the index of ""right""\n', 'DataFrameA:  Code, Date, ColA, ColB, ColC, ..., ColG, ColH (shape: 80514, 8 - no index)\nDataFrameB:  Date, Code, Col1, Col2, Col3, ..., Col15, Col16 (shape: 859, 16 - no index)\n']";"[""merged = DataFrameA.join(DataFrameB, on=['Code','Date'])\n"", 'ValueError: len(left_on) must equal the number of levels in the index of ""right""\n', 'DataFrameA:  Code, Date, ColA, ColB, ColC, ..., ColG, ColH (shape: 80514, 8 - no index)\nDataFrameB:  Date, Code, Col1, Col2, Col3, ..., Col15, Col16 (shape: 859, 16 - no index)\n']";"[""merged = DataFrameA.join(DataFrameB, on=['Code','Date'])\n""]";"[""from pandas import DataFrame\nmerged = DataFrameA.join(DataFrameB, on=['Code','Date'])\n""]";True;"[""import pandas as pd\nmerged = DataFrameA.join(DataFrameB, on=['Code','Date'])\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'DataFrameA' is not defined""]";['NameError']
960;960;960;960;1.0;1;28236305;;1;21;<python><pandas><dataframe><data-analysis>;How do I sum values in a column that match a given condition using pandas?;27037.0;['a   b  \n1   5   \n1   7\n2   3\n1   3\n2   5\n'];['a   b  \n1   5   \n1   7\n2   3\n1   3\n2   5\n'];['a   b  \n1   5   \n1   7\n2   3\n1   3\n2   5\n', 'b', 'a = 1', '5 + 7 + 3 = 15'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
961;961;961;961;2.0;2;28259301;;1;16;<python><xml><python-2.7><parsing><pandas>;How to convert an XML file to nice pandas dataframe?;22013.0;"['<type=""XXX"" language=""EN"" gender=""xx"" feature=""xx"" web=""foobar.com"">\n    <count=""N"">\n        <KEY=""e95a9a6c790ecb95e46cf15bee517651"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""bc360cfbafc39970587547215162f0db"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""19e71144c50a8b9160b3f0955e906fce"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""21d4af9021a174f61b884606c74d9e42"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""28a45eb2460899763d709ca00ddbb665"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""a0c0712a6a351f85d9f5757e9fff8946"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""626726ba8d34d15d02b6d043c55fe691"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""2cb473e0f102e2e4a40aa3006e412ae4"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...] [...]\n]]>\n        </document>\n    </documents>\n</author>\nkey                                         type     language    feature            web                             data\ne95324a9a6c790ecb95e46cf15bE232ee517651      XXX        EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\ne95324a9a6c790ecb95e46cf15bE232ee517651     XXX         EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\n19e71144c50a8b9160b3cvdf2324f0955e906fce    XXX         EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\n21d4af9021a174f61b8erf284606c74d9e42        XXX         EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\n28a45eb2460823499763d70vdf9ca00ddbb665       XXX        EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\nfrom lxml import objectify\nimport pandas as pd\n\npath = \'file_path\'\nxml = objectify.parse(open(path))\nroot = xml.getroot()\nroot.getchildren()[0].getchildren()\ndf = pd.DataFrame(columns=(\'key\',\'type\', \'language\', \'feature\', \'web\', \'data\'))\n\nfor i in range(0,len(xml)):\n    obj = root.getchildren()[i].getchildren()\n    row = dict(zip([\'key\',\'type\', \'language\', \'feature\', \'web\', \'data\'], [obj[0].text, obj[1].text]))\n    row_s = pd.Series(row)\n    row_s.name = i\n    df = df.append(row_s)\n']";"['<type=""XXX"" language=""EN"" gender=""xx"" feature=""xx"" web=""foobar.com"">\n    <count=""N"">\n        <KEY=""e95a9a6c790ecb95e46cf15bee517651"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""bc360cfbafc39970587547215162f0db"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""19e71144c50a8b9160b3f0955e906fce"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""21d4af9021a174f61b884606c74d9e42"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""28a45eb2460899763d709ca00ddbb665"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""a0c0712a6a351f85d9f5757e9fff8946"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""626726ba8d34d15d02b6d043c55fe691"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""2cb473e0f102e2e4a40aa3006e412ae4"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...] [...]\n]]>\n        </document>\n    </documents>\n</author>\n', 'key                                         type     language    feature            web                             data\ne95324a9a6c790ecb95e46cf15bE232ee517651      XXX        EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\ne95324a9a6c790ecb95e46cf15bE232ee517651     XXX         EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\n19e71144c50a8b9160b3cvdf2324f0955e906fce    XXX         EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\n21d4af9021a174f61b8erf284606c74d9e42        XXX         EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\n28a45eb2460823499763d70vdf9ca00ddbb665       XXX        EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\n', ""from lxml import objectify\nimport pandas as pd\n\npath = 'file_path'\nxml = objectify.parse(open(path))\nroot = xml.getroot()\nroot.getchildren()[0].getchildren()\ndf = pd.DataFrame(columns=('key','type', 'language', 'feature', 'web', 'data'))\n\nfor i in range(0,len(xml)):\n    obj = root.getchildren()[i].getchildren()\n    row = dict(zip(['key','type', 'language', 'feature', 'web', 'data'], [obj[0].text, obj[1].text]))\n    row_s = pd.Series(row)\n    row_s.name = i\n    df = df.append(row_s)\n""]";"['<type=""XXX"" language=""EN"" gender=""xx"" feature=""xx"" web=""foobar.com"">\n    <count=""N"">\n        <KEY=""e95a9a6c790ecb95e46cf15bee517651"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""bc360cfbafc39970587547215162f0db"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""19e71144c50a8b9160b3f0955e906fce"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""21d4af9021a174f61b884606c74d9e42"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""28a45eb2460899763d709ca00ddbb665"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""a0c0712a6a351f85d9f5757e9fff8946"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""626726ba8d34d15d02b6d043c55fe691"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]>\n        </document>\n        <KEY=""2cb473e0f102e2e4a40aa3006e412ae4"" web=""www.foo_bar_exmaple.com""><![CDATA[A large text with lots of strings and punctuations symbols [...] [...]\n]]>\n        </document>\n    </documents>\n</author>\n', 'key                                         type     language    feature            web                             data\ne95324a9a6c790ecb95e46cf15bE232ee517651      XXX        EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\ne95324a9a6c790ecb95e46cf15bE232ee517651     XXX         EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\n19e71144c50a8b9160b3cvdf2324f0955e906fce    XXX         EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\n21d4af9021a174f61b8erf284606c74d9e42        XXX         EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\n28a45eb2460823499763d70vdf9ca00ddbb665       XXX        EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\n', ""from lxml import objectify\nimport pandas as pd\n\npath = 'file_path'\nxml = objectify.parse(open(path))\nroot = xml.getroot()\nroot.getchildren()[0].getchildren()\ndf = pd.DataFrame(columns=('key','type', 'language', 'feature', 'web', 'data'))\n\nfor i in range(0,len(xml)):\n    obj = root.getchildren()[i].getchildren()\n    row = dict(zip(['key','type', 'language', 'feature', 'web', 'data'], [obj[0].text, obj[1].text]))\n    row_s = pd.Series(row)\n    row_s.name = i\n    df = df.append(row_s)\n""]";"[""from lxml import objectify\nimport pandas as pd\n\npath = 'file_path'\nxml = objectify.parse(open(path))\nroot = xml.getroot()\nroot.getchildren()[0].getchildren()\ndf = pd.DataFrame(columns=('key','type', 'language', 'feature', 'web', 'data'))\n\n""]";"[""from lxml import objectify\nimport pandas as pd\n\npath = 'file_path'\nxml = objectify.parse(open(path))\nroot = xml.getroot()\nroot.getchildren()[0].getchildren()\ndf = pd.DataFrame(columns=('key','type', 'language', 'feature', 'web', 'data'))\n\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nfrom lxml import objectify\nimport pandas as pd\n\npath = 'file_path'\nxml = objectify.parse(open(path))\nroot = xml.getroot()\nroot.getchildren()[0].getchildren()\ndf = pd.DataFrame(columns=('key','type', 'language', 'feature', 'web', 'data'))\n\n""]";True;0;1;"[""name 'xml_data' is not defined""]";['NameError'];0;1;"[""name 'xml_data' is not defined""]";['NameError'];0;1;"[""name 'xml_data' is not defined""]";['NameError']
962;962;962;962;6.0;1;28272137;;1;13;<python><pandas>;Pandas How to filter a Series;16334.0;['name\n383      3.000000\n663      1.000000\n726      1.000000\n737      9.000000\n833      8.166667\n'];['name\n383      3.000000\n663      1.000000\n726      1.000000\n737      9.000000\n833      8.166667\n'];['name\n383      3.000000\n663      1.000000\n726      1.000000\n737      9.000000\n833      8.166667\n'];['name\n'];['name\n'];False;['import pandas as pd\nname\n'];False;0;2;"[""name 'test' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'test' is not defined"", ""name 'test' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'test' is not defined"", ""name 'test' is not defined""]";['NameError', 'NameError']
963;963;963;963;1.0;0;28293028;;1;18;<python><pandas><plot>;Plotting grouped data in same plot using Pandas;14669.0;"[""bp = p_df.groupby('class').plot(kind='kde')\n""]";"[""bp = p_df.groupby('class').plot(kind='kde')\n""]";"[""bp = p_df.groupby('class').plot(kind='kde')\n"", 'p_df', 'dataframe']";"[""bp = p_df.groupby('class').plot(kind='kde')\n""]";"[""bp = p_df.groupby('class').plot(kind='kde')\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nbp = p_df.groupby('class').plot(kind='kde')\n""]";True;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
964;964;964;964;1.0;0;28311655;;1;24;<pandas>;Ignoring NaNs with str.contains;6219.0;"['DF[DF.col.str.contains(""foo"")]\nDF[DF.col.notnull()][DF.col.dropna().str.contains(""foo"")]\n']";"['DF[DF.col.str.contains(""foo"")]\n', 'DF[DF.col.notnull()][DF.col.dropna().str.contains(""foo"")]\n']";"['DF[DF.col.str.contains(""foo"")]\n', 'DF[DF.col.notnull()][DF.col.dropna().str.contains(""foo"")]\n']";"['DF[DF.col.str.contains(""foo"")]\nDF[DF.col.notnull()][DF.col.dropna().str.contains(""foo"")]\n']";"['DF[DF.col.str.contains(""foo"")]\nDF[DF.col.notnull()][DF.col.dropna().str.contains(""foo"")]\n']";False;"['import pandas as pd\nDF[DF.col.str.contains(""foo"")]\nDF[DF.col.notnull()][DF.col.dropna().str.contains(""foo"")]\n']";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
965;965;965;965;3.0;1;28371308;;1;17;<python><pandas><sorting>;Sort by column within multi index level in pandas;1573.0;"[""l = [[1,'A',99],[1,'B',102],[1,'C',105],[1,'D',97],[2,'A',19],[2,'B',14],[2,'C',10],[2,'D',17]]\ndf = pd.DataFrame(l,columns = ['idx1','idx2','col1'])\ndf.set_index(['idx1','idx2'],inplace=True)\n\n# assume data has been received like this...\nprint df\n\n           col1\nidx1 idx2      \n1    A       99\n     B      102\n     C      105\n     D       97\n2    A       19\n     B       14\n     C       10\n     D       17\n\n# I'd like to sort descending on col1, partitioning within index level = 'idx2'\n\n           col1\nidx1 idx2      \n1    C      105\n     B      102\n     A       99\n     D       97\n\n2    A       19\n     D       17\n     B       14\n     C       10\nl = [[1,'A',99],[1,'B',11],[1,'C',105],[1,'D',97],[2,'A',19],[2,'B',14],[2,'C',10],[2,'D',17]]\ndf = pd.DataFrame(l,columns = ['idx1','idx2','col1'])\ndf.set_index(['idx1','idx2'],inplace=True)\ndf = df.sort_index(by='col1', ascending=False)\nidx1 idx2      \n1    C      105\n     A       99\n     D       97\n2    A       19\n     D       17\n     B       14\n1    B       11\n2    C       10\nidx1 idx2      \n1    C      105\n     A       99\n     D       97\n     B       11\n\n2    A       19\n     D       17\n     B       14\n     C       10\n""]";"[""l = [[1,'A',99],[1,'B',102],[1,'C',105],[1,'D',97],[2,'A',19],[2,'B',14],[2,'C',10],[2,'D',17]]\ndf = pd.DataFrame(l,columns = ['idx1','idx2','col1'])\ndf.set_index(['idx1','idx2'],inplace=True)\n\n# assume data has been received like this...\nprint df\n\n           col1\nidx1 idx2      \n1    A       99\n     B      102\n     C      105\n     D       97\n2    A       19\n     B       14\n     C       10\n     D       17\n\n# I'd like to sort descending on col1, partitioning within index level = 'idx2'\n\n           col1\nidx1 idx2      \n1    C      105\n     B      102\n     A       99\n     D       97\n\n2    A       19\n     D       17\n     B       14\n     C       10\n"", ""l = [[1,'A',99],[1,'B',11],[1,'C',105],[1,'D',97],[2,'A',19],[2,'B',14],[2,'C',10],[2,'D',17]]\ndf = pd.DataFrame(l,columns = ['idx1','idx2','col1'])\ndf.set_index(['idx1','idx2'],inplace=True)\ndf = df.sort_index(by='col1', ascending=False)\n"", 'idx1 idx2      \n1    C      105\n     A       99\n     D       97\n2    A       19\n     D       17\n     B       14\n1    B       11\n2    C       10\n', 'idx1 idx2      \n1    C      105\n     A       99\n     D       97\n     B       11\n\n2    A       19\n     D       17\n     B       14\n     C       10\n']";"[""l = [[1,'A',99],[1,'B',102],[1,'C',105],[1,'D',97],[2,'A',19],[2,'B',14],[2,'C',10],[2,'D',17]]\ndf = pd.DataFrame(l,columns = ['idx1','idx2','col1'])\ndf.set_index(['idx1','idx2'],inplace=True)\n\n# assume data has been received like this...\nprint df\n\n           col1\nidx1 idx2      \n1    A       99\n     B      102\n     C      105\n     D       97\n2    A       19\n     B       14\n     C       10\n     D       17\n\n# I'd like to sort descending on col1, partitioning within index level = 'idx2'\n\n           col1\nidx1 idx2      \n1    C      105\n     B      102\n     A       99\n     D       97\n\n2    A       19\n     D       17\n     B       14\n     C       10\n"", ""l = [[1,'A',99],[1,'B',11],[1,'C',105],[1,'D',97],[2,'A',19],[2,'B',14],[2,'C',10],[2,'D',17]]\ndf = pd.DataFrame(l,columns = ['idx1','idx2','col1'])\ndf.set_index(['idx1','idx2'],inplace=True)\ndf = df.sort_index(by='col1', ascending=False)\n"", 'idx1 idx2      \n1    C      105\n     A       99\n     D       97\n2    A       19\n     D       17\n     B       14\n1    B       11\n2    C       10\n', 'idx1 idx2      \n1    C      105\n     A       99\n     D       97\n     B       11\n\n2    A       19\n     D       17\n     B       14\n     C       10\n']";"[""l = [[1,'A',99],[1,'B',102],[1,'C',105],[1,'D',97],[2,'A',19],[2,'B',14],[2,'C',10],[2,'D',17]]\ndf = pd.DataFrame(l,columns = ['idx1','idx2','col1'])\ndf.set_index(['idx1','idx2'],inplace=True)\n\n# assume data has been received like this...\n\n\n# I'd like to sort descending on col1, partitioning within index level = 'idx2'\n\n\nl = [[1,'A',99],[1,'B',11],[1,'C',105],[1,'D',97],[2,'A',19],[2,'B',14],[2,'C',10],[2,'D',17]]\ndf = pd.DataFrame(l,columns = ['idx1','idx2','col1'])\ndf.set_index(['idx1','idx2'],inplace=True)\ndf = df.sort_index(by='col1', ascending=False)\n\n""]";"[""import pandas as pd\nl = [[1,'A',99],[1,'B',102],[1,'C',105],[1,'D',97],[2,'A',19],[2,'B',14],[2,'C',10],[2,'D',17]]\ndf = pd.DataFrame(l,columns = ['idx1','idx2','col1'])\ndf.set_index(['idx1','idx2'],inplace=True)\n\n# assume data has been received like this...\n\n\n# I'd like to sort descending on col1, partitioning within index level = 'idx2'\n\n\nl = [[1,'A',99],[1,'B',11],[1,'C',105],[1,'D',97],[2,'A',19],[2,'B',14],[2,'C',10],[2,'D',17]]\ndf = pd.DataFrame(l,columns = ['idx1','idx2','col1'])\ndf.set_index(['idx1','idx2'],inplace=True)\ndf = df.sort_index(by='col1', ascending=False)\n\n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\nl = [[1,'A',99],[1,'B',102],[1,'C',105],[1,'D',97],[2,'A',19],[2,'B',14],[2,'C',10],[2,'D',17]]\ndf = pd.DataFrame(l,columns = ['idx1','idx2','col1'])\ndf.set_index(['idx1','idx2'],inplace=True)\n\n# assume data has been received like this...\n\n\n# I'd like to sort descending on col1, partitioning within index level = 'idx2'\n\n\nl = [[1,'A',99],[1,'B',11],[1,'C',105],[1,'D',97],[2,'A',19],[2,'B',14],[2,'C',10],[2,'D',17]]\ndf = pd.DataFrame(l,columns = ['idx1','idx2','col1'])\ndf.set_index(['idx1','idx2'],inplace=True)\ndf = df.sort_index(by='col1', ascending=False)\n\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
966;966;966;966;4.0;0;28384680;;1;14;<python><numpy><pandas><scikit-learn>;Scikit-Learn's Pipeline: A sparse matrix was passed, but dense data is required;7573.0;"[""df = pd.DataFrame\ndf = DataFrame.from_records(train)\n\ntest = [blah1, blah2, blah3]\n\npipeline = Pipeline([('vectorizer', CountVectorizer()), ('classifier', RandomForestClassifier())])\n\npipeline.fit(numpy.asarray(df[0]), numpy.asarray(df[1]))\npredicted = pipeline.predict(test)\nTypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n""]";"[""df = pd.DataFrame\ndf = DataFrame.from_records(train)\n\ntest = [blah1, blah2, blah3]\n\npipeline = Pipeline([('vectorizer', CountVectorizer()), ('classifier', RandomForestClassifier())])\n\npipeline.fit(numpy.asarray(df[0]), numpy.asarray(df[1]))\npredicted = pipeline.predict(test)\n"", 'TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n']";"[""df = pd.DataFrame\ndf = DataFrame.from_records(train)\n\ntest = [blah1, blah2, blah3]\n\npipeline = Pipeline([('vectorizer', CountVectorizer()), ('classifier', RandomForestClassifier())])\n\npipeline.fit(numpy.asarray(df[0]), numpy.asarray(df[1]))\npredicted = pipeline.predict(test)\n"", 'TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n', 'pipeline.fit(numpy.asarray(df[0]), numpy.asarray(df[1]))', 'toarray', 'todense']";"[""df = pd.DataFrame\ndf = DataFrame.from_records(train)\n\ntest = [blah1, blah2, blah3]\n\npipeline = Pipeline([('vectorizer', CountVectorizer()), ('classifier', RandomForestClassifier())])\n\npipeline.fit(numpy.asarray(df[0]), numpy.asarray(df[1]))\npredicted = pipeline.predict(test)\n""]";"[""from pandas import DataFrame\nimport pandas as pd\ndf = pd.DataFrame\ndf = DataFrame.from_records(train)\n\ntest = [blah1, blah2, blah3]\n\npipeline = Pipeline([('vectorizer', CountVectorizer()), ('classifier', RandomForestClassifier())])\n\npipeline.fit(numpy.asarray(df[0]), numpy.asarray(df[1]))\npredicted = pipeline.predict(test)\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame\ndf = DataFrame.from_records(train)\n\ntest = [blah1, blah2, blah3]\n\npipeline = Pipeline([('vectorizer', CountVectorizer()), ('classifier', RandomForestClassifier())])\n\npipeline.fit(numpy.asarray(df[0]), numpy.asarray(df[1]))\npredicted = pipeline.predict(test)\n""]";False;2;3;"[""No module named 'sklearn'"", 'Sucess', 'Sucess']";['ImportError', 'Sucess', 'Sucess'];2;3;"[""No module named 'sklearn'"", 'Sucess', 'Sucess']";['ImportError', 'Sucess', 'Sucess'];2;3;"[""No module named 'sklearn'"", 'Sucess', 'Sucess']";['ImportError', 'Sucess', 'Sucess']
967;967;967;967;3.0;1;28417293;;1;19;<pandas>;Sample Datasets in Pandas;7140.0;['data(iris)\ndata(mtcars)\n'];['data(iris)\n', 'data(mtcars)\n'];['data(iris)\n', 'data(mtcars)\n'];['data(iris)\ndata(mtcars)\n'];['data(iris)\ndata(mtcars)\n'];False;['import pandas as pd\ndata = pd.DataFrame()\ndata(iris)\ndata(mtcars)\n'];True;0;3;"[""No module named 'rpy2'"", 'Error tokenizing data. C error: Expected 1 fields in line 5, saw 2\n', ""No module named 'seaborn'""]";['ImportError', 'ParserError', 'ImportError'];0;3;"[""No module named 'rpy2'"", 'Error tokenizing data. C error: Expected 1 fields in line 5, saw 2\n', ""No module named 'seaborn'""]";['ImportError', 'ParserError', 'ImportError'];0;3;"[""No module named 'rpy2'"", 'Error tokenizing data. C error: Expected 1 fields in line 5, saw 2\n', ""No module named 'seaborn'""]";['ImportError', 'ParserError', 'ImportError']
968;968;968;968;1.0;0;28465633;;1;11;<python><pandas>;Easy way to apply transformation from `pandas.get_dummies` to new data?;1296.0;[''];[];['data', 'pandas.get_dummies(data)', 'pandas.get_dummies(new_data)'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
969;969;969;969;5.0;3;28538536;;1;19;<python><pandas>;Deleting multiple columns in Pandas;22682.0;"[""   'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27',\n   'Unnamed: 28', 'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31',\n   'Unnamed: 32', 'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 35',\n   'Unnamed: 36', 'Unnamed: 37', 'Unnamed: 38', 'Unnamed: 39',\n   'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42', 'Unnamed: 43',\n   'Unnamed: 44', 'Unnamed: 45', 'Unnamed: 46', 'Unnamed: 47',\n   'Unnamed: 48', 'Unnamed: 49', 'Unnamed: 50', 'Unnamed: 51',\n   'Unnamed: 52', 'Unnamed: 53', 'Unnamed: 54', 'Unnamed: 55',\n   'Unnamed: 56', 'Unnamed: 57', 'Unnamed: 58', 'Unnamed: 59',\n   'Unnamed: 60'\n    df.drop(df.columns[[22, 23, 24, 25, \n    26, 27, 28, 29, 30, 31, 32 ,55]], axis=1, inplace=True)\n""]";"[""   'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27',\n   'Unnamed: 28', 'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31',\n   'Unnamed: 32', 'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 35',\n   'Unnamed: 36', 'Unnamed: 37', 'Unnamed: 38', 'Unnamed: 39',\n   'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42', 'Unnamed: 43',\n   'Unnamed: 44', 'Unnamed: 45', 'Unnamed: 46', 'Unnamed: 47',\n   'Unnamed: 48', 'Unnamed: 49', 'Unnamed: 50', 'Unnamed: 51',\n   'Unnamed: 52', 'Unnamed: 53', 'Unnamed: 54', 'Unnamed: 55',\n   'Unnamed: 56', 'Unnamed: 57', 'Unnamed: 58', 'Unnamed: 59',\n   'Unnamed: 60'\n"", '    df.drop(df.columns[[22, 23, 24, 25, \n    26, 27, 28, 29, 30, 31, 32 ,55]], axis=1, inplace=True)\n']";"[""   'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27',\n   'Unnamed: 28', 'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31',\n   'Unnamed: 32', 'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 35',\n   'Unnamed: 36', 'Unnamed: 37', 'Unnamed: 38', 'Unnamed: 39',\n   'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42', 'Unnamed: 43',\n   'Unnamed: 44', 'Unnamed: 45', 'Unnamed: 46', 'Unnamed: 47',\n   'Unnamed: 48', 'Unnamed: 49', 'Unnamed: 50', 'Unnamed: 51',\n   'Unnamed: 52', 'Unnamed: 53', 'Unnamed: 54', 'Unnamed: 55',\n   'Unnamed: 56', 'Unnamed: 57', 'Unnamed: 58', 'Unnamed: 59',\n   'Unnamed: 60'\n"", '    df.drop(df.columns[[22, 23, 24, 25, \n    26, 27, 28, 29, 30, 31, 32 ,55]], axis=1, inplace=True)\n']";[''];[''];False;['import pandas as pd\n'];False;1;3;"[""name 'pd' is not defined"", 'Sucess', ""name 'yourdf' is not defined""]";['NameError', 'Sucess', 'NameError'];2;3;"['Sucess', 'Sucess', ""name 'yourdf' is not defined""]";['Sucess', 'Sucess', 'NameError'];2;3;"['Sucess', 'Sucess', ""name 'yourdf' is not defined""]";['Sucess', 'Sucess', 'NameError']
970;970;970;970;1.0;0;28590663;;1;16;<python><json><pandas>;Pandas dataframe to json without index;12308.0;"['DataFrame name: Stops\nid    location\n0     [50, 50]\n1     [60, 60]\n2     [70, 70]\n3     [80, 80]\n""stops"":\n[\n{\n    ""id"": 1,\n    ""location"": [50, 50]\n},\n{\n    ""id"": 2,\n    ""location"": [60, 60]\n},\n... (and so on)\n]\n""stops"":\n{\n""0"":\n    {\n        ""id"": 0,\n        ""location"": [50, 50]\n    },\n""1"":\n    {\n        ""id"": 1,\n        ""location"": [60, 60]\n    },\n... (and so on)\n}\n']";"['DataFrame name: Stops\nid    location\n0     [50, 50]\n1     [60, 60]\n2     [70, 70]\n3     [80, 80]\n', '""stops"":\n[\n{\n    ""id"": 1,\n    ""location"": [50, 50]\n},\n{\n    ""id"": 2,\n    ""location"": [60, 60]\n},\n... (and so on)\n]\n', '""stops"":\n{\n""0"":\n    {\n        ""id"": 0,\n        ""location"": [50, 50]\n    },\n""1"":\n    {\n        ""id"": 1,\n        ""location"": [60, 60]\n    },\n... (and so on)\n}\n']";"['DataFrame name: Stops\nid    location\n0     [50, 50]\n1     [60, 60]\n2     [70, 70]\n3     [80, 80]\n', '""stops"":\n[\n{\n    ""id"": 1,\n    ""location"": [50, 50]\n},\n{\n    ""id"": 2,\n    ""location"": [60, 60]\n},\n... (and so on)\n]\n', ""df.reset_index().to_json(orient='index)"", '""stops"":\n{\n""0"":\n    {\n        ""id"": 0,\n        ""location"": [50, 50]\n    },\n""1"":\n    {\n        ""id"": 1,\n        ""location"": [60, 60]\n    },\n... (and so on)\n}\n']";['0     [50, 50]\n1     [60, 60]\n2     [70, 70]\n3     [80, 80]\n'];['0     [50, 50]\n1     [60, 60]\n2     [70, 70]\n3     [80, 80]\n'];False;['import pandas as pd\n0     [50, 50]\n1     [60, 60]\n2     [70, 70]\n3     [80, 80]\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
971;971;971;971;1.0;0;28595701;;1;16;<python-3.x><pandas><concat><cbind>;pandas equivalent of R's cbind (concatenate/stack vectors vertically);12260.0;['import pandas\n....\n....\ntest1 = pandas.DataFrame([1,2,3,4,5])\n....\n....\ntest2 = pandas.DataFrame([4,2,1,3,7])\n....\n'];['import pandas\n....\n....\ntest1 = pandas.DataFrame([1,2,3,4,5])\n....\n....\ntest2 = pandas.DataFrame([4,2,1,3,7])\n....\n'];['import pandas\n....\n....\ntest1 = pandas.DataFrame([1,2,3,4,5])\n....\n....\ntest2 = pandas.DataFrame([4,2,1,3,7])\n....\n', 'test1.append(test2)', 'rbind', 'cbind'];['import pandas\ntest1 = pandas.DataFrame([1,2,3,4,5])\ntest2 = pandas.DataFrame([4,2,1,3,7])\n'];['import pandas\ntest1 = pandas.DataFrame([1,2,3,4,5])\ntest2 = pandas.DataFrame([4,2,1,3,7])\n'];False;['import pandas as pd\nimport pandas\ntest1 = pandas.DataFrame([1,2,3,4,5])\ntest2 = pandas.DataFrame([4,2,1,3,7])\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'test1' is not defined""]";['NameError'];0;1;"[""name 'test1' is not defined""]";['NameError']
972;972;972;972;3.0;4;28651079;;1;17;<python><pandas>;Pandas unstack problems: ValueError: Index contains duplicate entries, cannot reshape;11491.0;"[""ValueError: Index contains duplicate entries, cannot reshape\nIn [37]: e.set_index(['id', 'date', 'location'], inplace=True)\n\nIn [38]: e\nOut[38]: \n                                    value\nid           date       location       \nid1          2014-12-12 loc1        16.86\n             2014-12-11 loc1        17.18\n             2014-12-10 loc1        17.03\n             2014-12-09 loc1        17.28\nIn [39]: e.unstack('location')\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-39-bc1e237a0ed7> in <module>()\n----> 1 e.unstack('location')\n...\nC:\\Anaconda\\envs\\sandbox\\lib\\site-packages\\pandas\\core\\reshape.pyc in _make_selectors(self)\n    143 \n    144         if mask.sum() < len(self.index):\n--> 145             raise ValueError('Index contains duplicate entries, '\n    146                              'cannot reshape')\n    147 \n\nValueError: Index contains duplicate entries, cannot reshape\n""]";"['ValueError: Index contains duplicate entries, cannot reshape\n', ""In [37]: e.set_index(['id', 'date', 'location'], inplace=True)\n\nIn [38]: e\nOut[38]: \n                                    value\nid           date       location       \nid1          2014-12-12 loc1        16.86\n             2014-12-11 loc1        17.18\n             2014-12-10 loc1        17.03\n             2014-12-09 loc1        17.28\n"", ""In [39]: e.unstack('location')\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-39-bc1e237a0ed7> in <module>()\n----> 1 e.unstack('location')\n...\nC:\\Anaconda\\envs\\sandbox\\lib\\site-packages\\pandas\\core\\reshape.pyc in _make_selectors(self)\n    143 \n    144         if mask.sum() < len(self.index):\n--> 145             raise ValueError('Index contains duplicate entries, '\n    146                              'cannot reshape')\n    147 \n\nValueError: Index contains duplicate entries, cannot reshape\n""]";"['ValueError: Index contains duplicate entries, cannot reshape\n', ""In [37]: e.set_index(['id', 'date', 'location'], inplace=True)\n\nIn [38]: e\nOut[38]: \n                                    value\nid           date       location       \nid1          2014-12-12 loc1        16.86\n             2014-12-11 loc1        17.18\n             2014-12-10 loc1        17.03\n             2014-12-09 loc1        17.28\n"", ""In [39]: e.unstack('location')\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-39-bc1e237a0ed7> in <module>()\n----> 1 e.unstack('location')\n...\nC:\\Anaconda\\envs\\sandbox\\lib\\site-packages\\pandas\\core\\reshape.pyc in _make_selectors(self)\n    143 \n    144         if mask.sum() < len(self.index):\n--> 145             raise ValueError('Index contains duplicate entries, '\n    146                              'cannot reshape')\n    147 \n\nValueError: Index contains duplicate entries, cannot reshape\n""]";"[""e.set_index(['id', 'date', 'location'], inplace=True)\n\n""]";"[""e.set_index(['id', 'date', 'location'], inplace=True)\n\n""]";False;"[""import pandas as pd\ne.set_index(['id', 'date', 'location'], inplace=True)\n\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;['3'];['KeyError']
973;973;973;973;1.0;1;28661258;;1;11;<python><pandas><scipy><vectorization>;Python Pandas: Apply function to dataframe in place;10910.0;['      Name      Val1      Val2      Val3      Val4 \n0        A -1.540369 -0.077779  0.979606 -0.667112   \n1        B -0.787154  0.048412  0.775444 -0.510904   \n2        C -0.477234  0.414388  1.250544 -0.411658   \n3        D -1.430851  0.258759  1.247752 -0.883293   \n4        E -0.360181  0.485465  1.123589 -0.379157\nnorm.cdf(df._get_numeric_data)\n'];['      Name      Val1      Val2      Val3      Val4 \n0        A -1.540369 -0.077779  0.979606 -0.667112   \n1        B -0.787154  0.048412  0.775444 -0.510904   \n2        C -0.477234  0.414388  1.250544 -0.411658   \n3        D -1.430851  0.258759  1.247752 -0.883293   \n4        E -0.360181  0.485465  1.123589 -0.379157\n', 'norm.cdf(df._get_numeric_data)\n'];['norm.cdf', 'numpy.array', 'pandas.DataFrame', 'numpy.apply', 'numpy.apply_along_axs', 'norm.cdf', 'scipy', '      Name      Val1      Val2      Val3      Val4 \n0        A -1.540369 -0.077779  0.979606 -0.667112   \n1        B -0.787154  0.048412  0.775444 -0.510904   \n2        C -0.477234  0.414388  1.250544 -0.411658   \n3        D -1.430851  0.258759  1.247752 -0.883293   \n4        E -0.360181  0.485465  1.123589 -0.379157\n', 'Name', 'df._get_numeric_data()', 'set', 'norm.cdf(df._get_numeric_data)\n', 'df', 'norm.cdf'];['norm.cdf(df._get_numeric_data)\n'];['norm.cdf(df._get_numeric_data)\n'];False;['import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\nnorm.cdf(df._get_numeric_data)\n'];True;0;0;[];[];0;0;[];[];0;0;[];[]
974;974;974;974;4.0;1;28676916;;1;14;<python><pandas>;Calculate new value based on decreasing value;632.0;['import pandas as pd\n\nALLOWANCE = 100\nvalues = pd.Series([85, 10, 25, 30])\ndesired = pd.Series([0, 0, 20, 30])\n'];['import pandas as pd\n\nALLOWANCE = 100\nvalues = pd.Series([85, 10, 25, 30])\n', 'desired = pd.Series([0, 0, 20, 30])\n'];['Series', 'cumsum', 'diff', 'import pandas as pd\n\nALLOWANCE = 100\nvalues = pd.Series([85, 10, 25, 30])\n', 'desired = pd.Series([0, 0, 20, 30])\n', 'ALLOWANCE', 'Series', '85', '0', '15', 'ALLOWANCE', '10', '15', '0', '5', '25', '5', '20', '30', '30'];['import pandas as pd\n\nALLOWANCE = 100\nvalues = pd.Series([85, 10, 25, 30])\ndesired = pd.Series([0, 0, 20, 30])\n'];['import pandas as pd\n\nALLOWANCE = 100\nvalues = pd.Series([85, 10, 25, 30])\ndesired = pd.Series([0, 0, 20, 30])\n'];False;['import pandas as pd\nimport pandas as pd\n\nALLOWANCE = 100\nvalues = pd.Series([85, 10, 25, 30])\ndesired = pd.Series([0, 0, 20, 30])\n'];False;0;2;"[""name 'values' is not defined"", ""name 'values' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'values' is not defined"", ""name 'values' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'values' is not defined"", ""name 'values' is not defined""]";['NameError', 'NameError']
975;975;975;975;3.0;0;28679930;;1;23;<python><pandas>;How to drop rows from pandas data frame that contains a particular string in a particular column?;20370.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
976;976;976;976;3.0;5;28757389;;1;71;<python><pandas>;Loc vs. iloc vs. ix vs. at vs. iat?;31094.0;[''];[];['Pandas', '.loc', '.iloc', '.ix', '.loc', 'iloc', 'at', 'iat', '.ix', '.ix', '.ix'];[''];[''];False;['import pandas as pd\n'];False;1;3;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'NameError', 'NameError'];1;3;"['Sucess', ""'the label [100] is not in the [index]'"", 'index 2 is out of bounds for axis 0 with size 0']";['Sucess', 'KeyError', 'IndexError']
977;977;977;977;2.0;2;28773342;;1;13;<python><pandas><datetime><dataframe>;Truncate `TimeStamp` column to hour precision in pandas `DataFrame`;4070.0;"[""df['dt'].dtype, df['dt'][0]\n# (dtype('<M8[ns]'), Timestamp('2014-10-01 10:02:45'))\ndf['dt2'] = df['dt'].apply(lambda L: datetime(L.year, L.month, L.day, L.hour))\n""]";"[""df['dt'].dtype, df['dt'][0]\n# (dtype('<M8[ns]'), Timestamp('2014-10-01 10:02:45'))\n"", ""df['dt2'] = df['dt'].apply(lambda L: datetime(L.year, L.month, L.day, L.hour))\n""]";"['pandas.DataFrame', 'df', 'dt', ""df['dt'].dtype, df['dt'][0]\n# (dtype('<M8[ns]'), Timestamp('2014-10-01 10:02:45'))\n"", ""df['dt2'] = df['dt'].apply(lambda L: datetime(L.year, L.month, L.day, L.hour))\n"", 'pandas.tseries.offsets', 'DatetimeIndex', 'pandas']";"[""df['dt'].dtype, df['dt'][0]\n# (dtype('<M8[ns]'), Timestamp('2014-10-01 10:02:45'))\ndf['dt2'] = df['dt'].apply(lambda L: datetime(L.year, L.month, L.day, L.hour))\n""]";"[""df['dt'].dtype, df['dt'][0]\n# (dtype('<M8[ns]'), Timestamp('2014-10-01 10:02:45'))\ndf['dt2'] = df['dt'].apply(lambda L: datetime(L.year, L.month, L.day, L.hour))\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf['dt'].dtype, df['dt'][0]\n# (dtype('<M8[ns]'), Timestamp('2014-10-01 10:02:45'))\ndf['dt2'] = df['dt'].apply(lambda L: datetime(L.year, L.month, L.day, L.hour))\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'dt'""]";['KeyError']
978;978;978;978;8.0;0;28901683;;1;44;<python><pandas>;pandas get rows which are NOT in other dataframe;26226.0;"[""df1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]}) \ndf2 = pandas.DataFrame(data = {'col1' : [1, 2, 3], 'col2' : [10, 11, 12]})\n""]";"[""df1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]}) \ndf2 = pandas.DataFrame(data = {'col1' : [1, 2, 3], 'col2' : [10, 11, 12]})\n""]";"[""df1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]}) \ndf2 = pandas.DataFrame(data = {'col1' : [1, 2, 3], 'col2' : [10, 11, 12]})\n""]";"[""df1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]}) \ndf2 = pandas.DataFrame(data = {'col1' : [1, 2, 3], 'col2' : [10, 11, 12]})\n""]";"[""df1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]}) \ndf2 = pandas.DataFrame(data = {'col1' : [1, 2, 3], 'col2' : [10, 11, 12]})\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]}) \ndf2 = pandas.DataFrame(data = {'col1' : [1, 2, 3], 'col2' : [10, 11, 12]})\n""]";True;0;3;"[""name 'df1' is not defined"", ""name 'pandas' is not defined"", ""name 'df_1' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df1' is not defined"", ""name 'pandas' is not defined"", ""name 'df_1' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""'col1'"", ""name 'pandas' is not defined"", ""name 'df_1' is not defined""]";['KeyError', 'NameError', 'NameError']
979;979;979;979;1.0;0;28931224;;1;28;<python><python-2.7><pandas><matplotlib><data-visualization>;Adding value labels on a matplotlib bar chart;35505.0;"['import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option(\'display.mpl_style\', \'default\') \n%matplotlib inline\n\n\nfrequencies = [6, 16, 75, 160, 244, 260, 145, 73, 16, 4, 1]   # bring some raw data\n\nfreq_series = pd.Series.from_array(frequencies)   # in my original code I create a series and run on that, so for consistency I create a series from the list.\n\nx_labels = [108300.0, 110540.0, 112780.0, 115020.0, 117260.0, 119500.0, 121740.0, 123980.0, 126220.0, 128460.0, 130700.0]\n\n# now to plot the figure...\nplt.figure(figsize=(12, 8))\nfig = freq_series.plot(kind=\'bar\')\nfig.set_title(""Amount Frequency"")\nfig.set_xlabel(""Amount ($)"")\nfig.set_ylabel(""Frequency"")\nfig.set_xticklabels(x_labels)\n']";"['import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option(\'display.mpl_style\', \'default\') \n%matplotlib inline\n\n\nfrequencies = [6, 16, 75, 160, 244, 260, 145, 73, 16, 4, 1]   # bring some raw data\n\nfreq_series = pd.Series.from_array(frequencies)   # in my original code I create a series and run on that, so for consistency I create a series from the list.\n\nx_labels = [108300.0, 110540.0, 112780.0, 115020.0, 117260.0, 119500.0, 121740.0, 123980.0, 126220.0, 128460.0, 130700.0]\n\n# now to plot the figure...\nplt.figure(figsize=(12, 8))\nfig = freq_series.plot(kind=\'bar\')\nfig.set_title(""Amount Frequency"")\nfig.set_xlabel(""Amount ($)"")\nfig.set_ylabel(""Frequency"")\nfig.set_xticklabels(x_labels)\n']";"['import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option(\'display.mpl_style\', \'default\') \n%matplotlib inline\n\n\nfrequencies = [6, 16, 75, 160, 244, 260, 145, 73, 16, 4, 1]   # bring some raw data\n\nfreq_series = pd.Series.from_array(frequencies)   # in my original code I create a series and run on that, so for consistency I create a series from the list.\n\nx_labels = [108300.0, 110540.0, 112780.0, 115020.0, 117260.0, 119500.0, 121740.0, 123980.0, 126220.0, 128460.0, 130700.0]\n\n# now to plot the figure...\nplt.figure(figsize=(12, 8))\nfig = freq_series.plot(kind=\'bar\')\nfig.set_title(""Amount Frequency"")\nfig.set_xlabel(""Amount ($)"")\nfig.set_ylabel(""Frequency"")\nfig.set_xticklabels(x_labels)\n']";"['import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option(\'display.mpl_style\', \'default\') \n\n\nfrequencies = [6, 16, 75, 160, 244, 260, 145, 73, 16, 4, 1]   # bring some raw data\n\nfreq_series = pd.Series.from_array(frequencies)   # in my original code I create a series and run on that, so for consistency I create a series from the list.\n\nx_labels = [108300.0, 110540.0, 112780.0, 115020.0, 117260.0, 119500.0, 121740.0, 123980.0, 126220.0, 128460.0, 130700.0]\n\n# now to plot the figure...\nplt.figure(figsize=(12, 8))\nfig = freq_series.plot(kind=\'bar\')\nfig.set_title(""Amount Frequency"")\nfig.set_xlabel(""Amount ($)"")\nfig.set_ylabel(""Frequency"")\nfig.set_xticklabels(x_labels)\n']";"['import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option(\'display.mpl_style\', \'default\') \n\n\nfrequencies = [6, 16, 75, 160, 244, 260, 145, 73, 16, 4, 1]   # bring some raw data\n\nfreq_series = pd.Series.from_array(frequencies)   # in my original code I create a series and run on that, so for consistency I create a series from the list.\n\nx_labels = [108300.0, 110540.0, 112780.0, 115020.0, 117260.0, 119500.0, 121740.0, 123980.0, 126220.0, 128460.0, 130700.0]\n\n# now to plot the figure...\nplt.figure(figsize=(12, 8))\nfig = freq_series.plot(kind=\'bar\')\nfig.set_title(""Amount Frequency"")\nfig.set_xlabel(""Amount ($)"")\nfig.set_ylabel(""Frequency"")\nfig.set_xticklabels(x_labels)\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option(\'display.mpl_style\', \'default\') \n\n\nfrequencies = [6, 16, 75, 160, 244, 260, 145, 73, 16, 4, 1]   # bring some raw data\n\nfreq_series = pd.Series.from_array(frequencies)   # in my original code I create a series and run on that, so for consistency I create a series from the list.\n\nx_labels = [108300.0, 110540.0, 112780.0, 115020.0, 117260.0, 119500.0, 121740.0, 123980.0, 126220.0, 128460.0, 130700.0]\n\n# now to plot the figure...\nplt.figure(figsize=(12, 8))\nfig = freq_series.plot(kind=\'bar\')\nfig.set_title(""Amount Frequency"")\nfig.set_xlabel(""Amount ($)"")\nfig.set_ylabel(""Frequency"")\nfig.set_xticklabels(x_labels)\n']";True;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
980;980;980;980;1.0;0;28986489;;1;11;<python><replace><pandas><dataframe>;Python Pandas: How to replace a characters in a column of a dataframe?;15525.0;"[""range\n(2,30)\n(50,290)\n(400,1000)\n... \norg_info_exc['range'].replace(',','-',inplace=True)\n""]";"['range\n(2,30)\n(50,290)\n(400,1000)\n... \n', ""org_info_exc['range'].replace(',','-',inplace=True)\n""]";"['range\n(2,30)\n(50,290)\n(400,1000)\n... \n', ""org_info_exc['range'].replace(',','-',inplace=True)\n""]";"[""range\n(2,30)\n(50,290)\n(400,1000)\n... \norg_info_exc['range'].replace(',','-',inplace=True)\n""]";"[""range\n(2,30)\n(50,290)\n(400,1000)\n... \norg_info_exc['range'].replace(',','-',inplace=True)\n""]";False;"[""import pandas as pd\nrange\n(2,30)\n(50,290)\n(400,1000)\n... \norg_info_exc['range'].replace(',','-',inplace=True)\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError']
981;981;981;981;2.0;0;29177498;;1;28;<python-2.7><pandas><dataframe><nan>;Python Pandas replace NaN in one column with value from corresponding row of second column;14402.0;['File    heat    Farheit Temp_Rating\n   1    YesQ    75      N/A\n   1    NoR     115     N/A\n   1    YesA    63      N/A\n   1    NoT     83      41\n   1    NoY     100     80\n   1    YesZ    56      12\n   2    YesQ    111     N/A\n   2    NoR     60      N/A\n   2    YesA    19      N/A\n   2    NoT     106     77\n   2    NoY     45      21\n   2    YesZ    40      54\n   3    YesQ    84      N/A\n   3    NoR     67      N/A\n   3    YesA    94      N/A\n   3    NoT     68      39\n   3    NoY     63      46\n   3    YesZ    34      81\nFile        heat    Observation\n   1        YesQ    75\n   1        NoR     115\n   1        YesA    63\n   1        YesQ    41\n   1        NoR     80\n   1        YesA    12\n   2        YesQ    111\n   2        NoR     60\n   2        YesA    19\n   2        NoT     77\n   2        NoY     21\n   2        YesZ    54\n   3        YesQ    84\n   3        NoR     67\n   3        YesA    94\n   3        NoT     39\n   3        NoY     46\n   3        YesZ    81\n'];['File    heat    Farheit Temp_Rating\n   1    YesQ    75      N/A\n   1    NoR     115     N/A\n   1    YesA    63      N/A\n   1    NoT     83      41\n   1    NoY     100     80\n   1    YesZ    56      12\n   2    YesQ    111     N/A\n   2    NoR     60      N/A\n   2    YesA    19      N/A\n   2    NoT     106     77\n   2    NoY     45      21\n   2    YesZ    40      54\n   3    YesQ    84      N/A\n   3    NoR     67      N/A\n   3    YesA    94      N/A\n   3    NoT     68      39\n   3    NoY     63      46\n   3    YesZ    34      81\n', 'File        heat    Observation\n   1        YesQ    75\n   1        NoR     115\n   1        YesA    63\n   1        YesQ    41\n   1        NoR     80\n   1        YesA    12\n   2        YesQ    111\n   2        NoR     60\n   2        YesA    19\n   2        NoT     77\n   2        NoY     21\n   2        YesZ    54\n   3        YesQ    84\n   3        NoR     67\n   3        YesA    94\n   3        NoT     39\n   3        NoY     46\n   3        YesZ    81\n'];['File    heat    Farheit Temp_Rating\n   1    YesQ    75      N/A\n   1    NoR     115     N/A\n   1    YesA    63      N/A\n   1    NoT     83      41\n   1    NoY     100     80\n   1    YesZ    56      12\n   2    YesQ    111     N/A\n   2    NoR     60      N/A\n   2    YesA    19      N/A\n   2    NoT     106     77\n   2    NoY     45      21\n   2    YesZ    40      54\n   3    YesQ    84      N/A\n   3    NoR     67      N/A\n   3    YesA    94      N/A\n   3    NoT     68      39\n   3    NoY     63      46\n   3    YesZ    34      81\n', 'Temp_Rating', 'Farheit', 'File        heat    Observation\n   1        YesQ    75\n   1        NoR     115\n   1        YesA    63\n   1        YesQ    41\n   1        NoR     80\n   1        YesA    12\n   2        YesQ    111\n   2        NoR     60\n   2        YesA    19\n   2        NoT     77\n   2        NoY     21\n   2        YesZ    54\n   3        YesQ    84\n   3        NoR     67\n   3        YesA    94\n   3        NoT     39\n   3        NoY     46\n   3        YesZ    81\n', 'Temp_Rating', 'NaN', 'Farheit'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'DataFrame' object has no attribute 'Temp_Rating'""]";['AttributeError']
982;982;982;982;1.0;1;29206612;;1;19;<python><numpy><pandas><datetime64>;Difference between data type 'datetime64[ns]' and '<M8[ns]'?;10521.0;"[""In [346]: from datetime import datetime\n\nIn [347]: dates = [datetime(2011, 1, 2), datetime(2011, 1, 5), datetime(2011, 1, 7),\n\n .....: datetime(2011, 1, 8), datetime(2011, 1, 10), datetime(2011, 1, 12)]\n\nIn [348]: ts = Series(np.random.randn(6), index=dates)\n\nIn [349]: ts\n\nOut[349]: \n\n2011-01-02 0.690002\n\n2011-01-05 1.001543\n\n2011-01-07 -0.503087\n\n2011-01-08 -0.622274\n\n2011-01-10 -0.921169\n\n2011-01-12 -0.726213\nIn [353]: ts.index.dtype\n\nOut[353]: dtype('datetime64[ns]')\nts.index.dtype\ndtype('<M8[ns]')\n""]";"['In [346]: from datetime import datetime\n\nIn [347]: dates = [datetime(2011, 1, 2), datetime(2011, 1, 5), datetime(2011, 1, 7),\n\n .....: datetime(2011, 1, 8), datetime(2011, 1, 10), datetime(2011, 1, 12)]\n\nIn [348]: ts = Series(np.random.randn(6), index=dates)\n\nIn [349]: ts\n\nOut[349]: \n\n2011-01-02 0.690002\n\n2011-01-05 1.001543\n\n2011-01-07 -0.503087\n\n2011-01-08 -0.622274\n\n2011-01-10 -0.921169\n\n2011-01-12 -0.726213\n', ""In [353]: ts.index.dtype\n\nOut[353]: dtype('datetime64[ns]')\n"", ""ts.index.dtype\ndtype('<M8[ns]')\n""]";"['In [346]: from datetime import datetime\n\nIn [347]: dates = [datetime(2011, 1, 2), datetime(2011, 1, 5), datetime(2011, 1, 7),\n\n .....: datetime(2011, 1, 8), datetime(2011, 1, 10), datetime(2011, 1, 12)]\n\nIn [348]: ts = Series(np.random.randn(6), index=dates)\n\nIn [349]: ts\n\nOut[349]: \n\n2011-01-02 0.690002\n\n2011-01-05 1.001543\n\n2011-01-07 -0.503087\n\n2011-01-08 -0.622274\n\n2011-01-10 -0.921169\n\n2011-01-12 -0.726213\n', ""In [353]: ts.index.dtype\n\nOut[353]: dtype('datetime64[ns]')\n"", ""ts.index.dtype\ndtype('<M8[ns]')\n"", ""'datetime64[ns]'"", ""'<M8[ns]'""]";['from datetime import datetime\n\n\n\n\n\nts.index.dtype\n\n'];['from datetime import datetime\n\n\n\n\n\nts.index.dtype\n\n'];False;['import pandas as pd\nfrom datetime import datetime\n\n\n\n\n\nts.index.dtype\n\n'];False;0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
983;983;983;983;1.0;0;29226210;;1;21;<python><pandas><apache-spark><pyspark>;What is the Spark DataFrame method `toPandas` actually doing?;16117.0;"[""lines = sc.textFile('tail5.csv')\nparts = lines.map(lambda l : l.strip().split('\\t'))\nfnames = *some name list*\nschemaData = StructType([StructField(fname, StringType(), True) for fname in fnames])\nddf = sqlContext.createDataFrame(parts,schemaData)\n""]";"[""lines = sc.textFile('tail5.csv')\nparts = lines.map(lambda l : l.strip().split('\\t'))\nfnames = *some name list*\nschemaData = StructType([StructField(fname, StringType(), True) for fname in fnames])\nddf = sqlContext.createDataFrame(parts,schemaData)\n""]";"[""lines = sc.textFile('tail5.csv')\nparts = lines.map(lambda l : l.strip().split('\\t'))\nfnames = *some name list*\nschemaData = StructType([StructField(fname, StringType(), True) for fname in fnames])\nddf = sqlContext.createDataFrame(parts,schemaData)\n""]";"[""lines = sc.textFile('tail5.csv')\nparts = lines.map(lambda l : l.strip().split('\\t'))\nschemaData = StructType([StructField(fname, StringType(), True) for fname in fnames])\nddf = sqlContext.createDataFrame(parts,schemaData)\n""]";"[""lines = sc.textFile('tail5.csv')\nparts = lines.map(lambda l : l.strip().split('\\t'))\nschemaData = StructType([StructField(fname, StringType(), True) for fname in fnames])\nddf = sqlContext.createDataFrame(parts,schemaData)\n""]";False;"[""import pandas as pd\nlines = sc.textFile('tail5.csv')\nparts = lines.map(lambda l : l.strip().split('\\t'))\nschemaData = StructType([StructField(fname, StringType(), True) for fname in fnames])\nddf = sqlContext.createDataFrame(parts,schemaData)\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
984;984;984;984;2.0;0;29233283;;1;15;<python><pandas><plot>;Plotting multiple lines with pandas dataframe;11996.0;"[""   color  x   y\n0    red  0   0\n1    red  1   1\n2    red  2   2\n3    red  3   3\n4    red  4   4\n5    red  5   5\n6    red  6   6\n7    red  7   7\n8    red  8   8\n9    red  9   9\n10  blue  0   0\n11  blue  1   1\n12  blue  2   4\n13  blue  3   9\n14  blue  4  16\n15  blue  5  25\n16  blue  6  36\n17  blue  7  49\n18  blue  8  64\n19  blue  9  81\ndf.plot(x='x', y='y')\n""]";"['   color  x   y\n0    red  0   0\n1    red  1   1\n2    red  2   2\n3    red  3   3\n4    red  4   4\n5    red  5   5\n6    red  6   6\n7    red  7   7\n8    red  8   8\n9    red  9   9\n10  blue  0   0\n11  blue  1   1\n12  blue  2   4\n13  blue  3   9\n14  blue  4  16\n15  blue  5  25\n16  blue  6  36\n17  blue  7  49\n18  blue  8  64\n19  blue  9  81\n', ""df.plot(x='x', y='y')\n""]";"['   color  x   y\n0    red  0   0\n1    red  1   1\n2    red  2   2\n3    red  3   3\n4    red  4   4\n5    red  5   5\n6    red  6   6\n7    red  7   7\n8    red  8   8\n9    red  9   9\n10  blue  0   0\n11  blue  1   1\n12  blue  2   4\n13  blue  3   9\n14  blue  4  16\n15  blue  5  25\n16  blue  6  36\n17  blue  7  49\n18  blue  8  64\n19  blue  9  81\n', ""df.plot(x='x', y='y')\n""]";"[""df.plot(x='x', y='y')\n""]";"[""df.plot(x='x', y='y')\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf.plot(x='x', y='y')\n""]";True;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
985;985;985;985;1.0;2;29241836;;1;15;<python><pandas>;select multiple columns by labels pandas;20035.0;"[""df = DataFrame(randn(10, 10), index=range(0,10), columns=['A', 'B', 'C', 'D','E','F','G','H','I','J'])\nsubset = df.loc[:,'A':'C']\nsubset = df.loc[:,'C':]\nsubset = df.loc[:,('A':'C', 'E')]\nsubset = df.loc[:,('A':'C', 'E', 'G':'I')]\n""]";"[""df = DataFrame(randn(10, 10), index=range(0,10), columns=['A', 'B', 'C', 'D','E','F','G','H','I','J'])\n"", ""subset = df.loc[:,'A':'C']\n"", ""subset = df.loc[:,'C':]\n"", ""subset = df.loc[:,('A':'C', 'E')]\n"", ""subset = df.loc[:,('A':'C', 'E', 'G':'I')]\n""]";"[""df = DataFrame(randn(10, 10), index=range(0,10), columns=['A', 'B', 'C', 'D','E','F','G','H','I','J'])\n"", ""subset = df.loc[:,'A':'C']\n"", ""subset = df.loc[:,'C':]\n"", ""subset = df.loc[:,('A':'C', 'E')]\n"", ""subset = df.loc[:,('A':'C', 'E', 'G':'I')]\n""]";"[""df = DataFrame(randn(10, 10), index=range(0,10), columns=['A', 'B', 'C', 'D','E','F','G','H','I','J'])\nsubset = df.loc[:,'A':'C']\nsubset = df.loc[:,'C':]\n""]";"[""from pandas import DataFrame\ndf = DataFrame(randn(10, 10), index=range(0,10), columns=['A', 'B', 'C', 'D','E','F','G','H','I','J'])\nsubset = df.loc[:,'A':'C']\nsubset = df.loc[:,'C':]\n""]";True;"[""import pandas as pd\ndf = DataFrame(randn(10, 10), index=range(0,10), columns=['A', 'B', 'C', 'D','E','F','G','H','I','J'])\nsubset = df.loc[:,'A':'C']\nsubset = df.loc[:,'C':]\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
986;986;986;986;2.0;4;29287224;;1;31;<python><pandas>;Pandas read in table without headers;32704.0;[''];[];['usecols'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'file_path' is not defined""]";['NameError'];0;1;"[""name 'file_path' is not defined""]";['NameError']
987;987;987;987;3.0;0;29314033;;1;14;<python><pandas>;Python Pandas DataFrame remove Empty Cells;20419.0;"[""   In [67]: value_counts(Tenant,normalize=False)\n   Out[67]:\n                              32320\n   Thunderhead                8170\n   Big Data Others            5700\n   Cloud Cruiser              5700\n   Partnerpedia               5700\n   Comcast                    5700\n   SDP                        5700\n   Agora                      5700\n   dtype: int64\n   In [71]: df['Tenant'].isnull().sum()\n   Out[71]: 0\n""]";"['   In [67]: value_counts(Tenant,normalize=False)\n   Out[67]:\n                              32320\n   Thunderhead                8170\n   Big Data Others            5700\n   Cloud Cruiser              5700\n   Partnerpedia               5700\n   Comcast                    5700\n   SDP                        5700\n   Agora                      5700\n   dtype: int64\n', ""   In [71]: df['Tenant'].isnull().sum()\n   Out[71]: 0\n""]";"['   In [67]: value_counts(Tenant,normalize=False)\n   Out[67]:\n                              32320\n   Thunderhead                8170\n   Big Data Others            5700\n   Cloud Cruiser              5700\n   Partnerpedia               5700\n   Comcast                    5700\n   SDP                        5700\n   Agora                      5700\n   dtype: int64\n', ""   In [71]: df['Tenant'].isnull().sum()\n   Out[71]: 0\n""]";['value_counts(Tenant,normalize=False)\n   '];['value_counts(Tenant,normalize=False)\n   '];False;['import pandas as pd\nvalue_counts(Tenant,normalize=False)\n   '];False;1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""'Tenant'""]";['Sucess', 'KeyError']
988;988;988;988;2.0;0;29334463;;1;12;<python><pandas>;How can I partially read a huge CSV file?;10245.0;"[""with open('abc.csv') as f:\n    line = f.readline()\n    # pass until it reaches a particular line number....\ndatainput1 = pd.read_csv('matrix.txt',sep=',', header = None, nrows = 1 )\ndatainput2 = pd.read_csv('matrix.txt',sep=',', header = None, nrows = 1 )\n""]";"[""with open('abc.csv') as f:\n    line = f.readline()\n    # pass until it reaches a particular line number....\n"", ""datainput1 = pd.read_csv('matrix.txt',sep=',', header = None, nrows = 1 )\ndatainput2 = pd.read_csv('matrix.txt',sep=',', header = None, nrows = 1 )\n""]";"[""with open('abc.csv') as f:\n    line = f.readline()\n    # pass until it reaches a particular line number....\n"", ""datainput1 = pd.read_csv('matrix.txt',sep=',', header = None, nrows = 1 )\ndatainput2 = pd.read_csv('matrix.txt',sep=',', header = None, nrows = 1 )\n""]";"[""    # pass until it reaches a particular line number....\ndatainput1 = pd.read_csv('matrix.txt',sep=',', header = None, nrows = 1 )\ndatainput2 = pd.read_csv('matrix.txt',sep=',', header = None, nrows = 1 )\n""]";"[""import pandas as pd\n    # pass until it reaches a particular line number....\ndatainput1 = pd.read_csv('matrix.txt',sep=',', header = None, nrows = 1 )\ndatainput2 = pd.read_csv('matrix.txt',sep=',', header = None, nrows = 1 )\n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\n    # pass until it reaches a particular line number....\ndatainput1 = pd.read_csv('matrix.txt',sep=',', header = None, nrows = 1 )\ndatainput2 = pd.read_csv('matrix.txt',sep=',', header = None, nrows = 1 )\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""File b'matrix.txt' does not exist""]";['FileNotFoundError'];0;1;"[""File b'matrix.txt' does not exist""]";['FileNotFoundError']
989;989;989;989;3.0;0;29370057;;1;42;<python><pandas>;Select dataframe rows between two dates;55170.0;"[""stock = pd.read_csv('data_in/' + filename + '.csv', skipinitialspace=True)\n""]";"[""stock = pd.read_csv('data_in/' + filename + '.csv', skipinitialspace=True)\n""]";"[""stock = pd.read_csv('data_in/' + filename + '.csv', skipinitialspace=True)\n""]";"[""stock = pd.read_csv('data_in/' + filename + '.csv', skipinitialspace=True)\n""]";"[""import pandas as pd\nstock = pd.read_csv('data_in/' + filename + '.csv', skipinitialspace=True)\n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\nstock = pd.read_csv('data_in/' + filename + '.csv', skipinitialspace=True)\n""]";True;1;3;"[""name 'pd' is not defined"", ""name 'df' is not defined"", 'Sucess']";['NameError', 'NameError', 'Sucess'];1;3;"[""name 'pd' is not defined"", ""name 'df' is not defined"", 'Sucess']";['NameError', 'NameError', 'Sucess'];1;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess']";['NameError', 'NameError', 'Sucess']
990;990;990;990;3.0;4;29370211;;1;16;<python><pandas><split>;pandas split string into columns;20576.0;['Track ID    stats\n14.0    (-0.00924175824176, 0.41, -0.742016492568, 0.0036830094242, 0.00251748449963)\n28.0    (0.0411538461538, 0.318230769231, 0.758717081514, 0.00264000622468, 0.0106535783677)\n42.0    (-0.0144351648352, 0.168438461538, -0.80870348637, 0.000816872566404, 0.00316572586742)\n56.0    (0.0343461538462, 0.288730769231, 0.950844962874, 6.1608706775e-07, 0.00337262030771)\n70.0    (0.00905164835165, 0.151030769231, 0.670257006716, 0.0121790506745, 0.00302182567957)\n84.0    (-0.0047967032967, 0.171615384615, -0.552879463981, 0.0500316517755, 0.00217970256969)\n'];['Track ID    stats\n14.0    (-0.00924175824176, 0.41, -0.742016492568, 0.0036830094242, 0.00251748449963)\n28.0    (0.0411538461538, 0.318230769231, 0.758717081514, 0.00264000622468, 0.0106535783677)\n42.0    (-0.0144351648352, 0.168438461538, -0.80870348637, 0.000816872566404, 0.00316572586742)\n56.0    (0.0343461538462, 0.288730769231, 0.950844962874, 6.1608706775e-07, 0.00337262030771)\n70.0    (0.00905164835165, 0.151030769231, 0.670257006716, 0.0121790506745, 0.00302182567957)\n84.0    (-0.0047967032967, 0.171615384615, -0.552879463981, 0.0500316517755, 0.00217970256969)\n'];['DataFrame', 'Track ID', 'stats', 'Track ID    stats\n14.0    (-0.00924175824176, 0.41, -0.742016492568, 0.0036830094242, 0.00251748449963)\n28.0    (0.0411538461538, 0.318230769231, 0.758717081514, 0.00264000622468, 0.0106535783677)\n42.0    (-0.0144351648352, 0.168438461538, -0.80870348637, 0.000816872566404, 0.00316572586742)\n56.0    (0.0343461538462, 0.288730769231, 0.950844962874, 6.1608706775e-07, 0.00337262030771)\n70.0    (0.00905164835165, 0.151030769231, 0.670257006716, 0.0121790506745, 0.00302182567957)\n84.0    (-0.0047967032967, 0.171615384615, -0.552879463981, 0.0500316517755, 0.00217970256969)\n'];['14.0    (-0.00924175824176, 0.41, -0.742016492568, 0.0036830094242, 0.00251748449963)\n28.0    (0.0411538461538, 0.318230769231, 0.758717081514, 0.00264000622468, 0.0106535783677)\n42.0    (-0.0144351648352, 0.168438461538, -0.80870348637, 0.000816872566404, 0.00316572586742)\n56.0    (0.0343461538462, 0.288730769231, 0.950844962874, 6.1608706775e-07, 0.00337262030771)\n70.0    (0.00905164835165, 0.151030769231, 0.670257006716, 0.0121790506745, 0.00302182567957)\n84.0    (-0.0047967032967, 0.171615384615, -0.552879463981, 0.0500316517755, 0.00217970256969)\n'];['14.0    (-0.00924175824176, 0.41, -0.742016492568, 0.0036830094242, 0.00251748449963)\n28.0    (0.0411538461538, 0.318230769231, 0.758717081514, 0.00264000622468, 0.0106535783677)\n42.0    (-0.0144351648352, 0.168438461538, -0.80870348637, 0.000816872566404, 0.00316572586742)\n56.0    (0.0343461538462, 0.288730769231, 0.950844962874, 6.1608706775e-07, 0.00337262030771)\n70.0    (0.00905164835165, 0.151030769231, 0.670257006716, 0.0121790506745, 0.00302182567957)\n84.0    (-0.0047967032967, 0.171615384615, -0.552879463981, 0.0500316517755, 0.00217970256969)\n'];False;['import pandas as pd\n14.0    (-0.00924175824176, 0.41, -0.742016492568, 0.0036830094242, 0.00251748449963)\n28.0    (0.0411538461538, 0.318230769231, 0.758717081514, 0.00264000622468, 0.0106535783677)\n42.0    (-0.0144351648352, 0.168438461538, -0.80870348637, 0.000816872566404, 0.00316572586742)\n56.0    (0.0343461538462, 0.288730769231, 0.950844962874, 6.1608706775e-07, 0.00337262030771)\n70.0    (0.00905164835165, 0.151030769231, 0.670257006716, 0.0121790506745, 0.00302182567957)\n84.0    (-0.0047967032967, 0.171615384615, -0.552879463981, 0.0500316517755, 0.00217970256969)\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'stats'""]";['KeyError']
991;991;991;991;4.0;0;29432629;;1;33;<python><pandas><matplotlib><data-visualization><information-visualization>;Correlation matrix using pandas;39252.0;[''];[];['dataframe.corr()'];[''];[''];False;['import pandas as pd\n'];False;2;4;"['Sucess', 'Sucess', ""No module named 'seaborn'"", ""name 'pd' is not defined""]";['Sucess', 'Sucess', 'ImportError', 'NameError'];2;4;"['Sucess', 'Sucess', ""No module named 'seaborn'"", ""name 'dataframe' is not defined""]";['Sucess', 'Sucess', 'ImportError', 'NameError'];2;4;"['Sucess', 'Sucess', ""No module named 'seaborn'"", ""name 'dataframe' is not defined""]";['Sucess', 'Sucess', 'ImportError', 'NameError']
992;992;992;992;2.0;0;29459461;;1;11;<python><pandas>;Pandas Dataframe to excel sheet;18975.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'yourdf' is not defined""]";['NameError'];0;1;"[""name 'yourdf' is not defined""]";['NameError'];0;1;"[""name 'yourdf' is not defined""]";['NameError']
993;993;993;993;2.0;0;29464234;;1;13;<python><pandas><rows><matching>;Compare Python Pandas DataFrames for matching rows;25048.0;"['df1 = pd.DataFrame(np.random.rand(10,4),columns=list(\'ABCD\'))\nprint df1\n\n       A         B         C         D\n0.860379  0.726956  0.394529  0.833217\n0.014180  0.813828  0.559891  0.339647\n0.782838  0.698993  0.551252  0.361034\n0.833370  0.982056  0.741821  0.006864\n0.855955  0.546562  0.270425  0.136006\n0.491538  0.445024  0.971603  0.690001\n0.911696  0.065338  0.796946  0.853456\n0.744923  0.545661  0.492739  0.337628\n0.576235  0.219831  0.946772  0.752403\n0.164873  0.454862  0.745890  0.437729\ndf2 = df1.ix[4:8]\ndf2.reset_index(drop=True,inplace=True)\ndf2.loc[-1] = [2, 3, 4, 5]\ndf2.loc[-2] = [14, 15, 16, 17]\ndf2.reset_index(drop=True,inplace=True)\nprint df2\n\n           A         B         C         D\n    0.855955  0.546562  0.270425  0.136006\n    0.491538  0.445024  0.971603  0.690001\n    0.911696  0.065338  0.796946  0.853456\n    0.744923  0.545661  0.492739  0.337628\n    0.576235  0.219831  0.946772  0.752403\n    2.000000  3.000000  4.000000  5.000000\n   14.000000 15.000000 16.000000 17.000000\nlist1 = df2.ix[0].tolist()\ncols = df1.columns.tolist()\nprint df1.lookup(list1, cols)\n  File ""C:\\Users\\test.py"", line 19, in <module>\n    print df1.lookup(list1, cols)\n  File ""C:\\python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 2217, in lookup\n    raise KeyError(\'One or more row labels was not found\')\nKeyError: \'One or more row labels was not found\'\nprint (df2 == df1).all(1).any()\n  File ""C:\\Users\\test.py"", line 12, in <module>\n    print (df2 == df1).all(1).any()\n  File ""C:\\python27\\lib\\site-packages\\pandas\\core\\ops.py"", line 884, in f\n    return self._compare_frame(other, func, str_rep)\n  File ""C:\\python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 3010, in _compare_frame\n    raise ValueError(\'Can only compare identically-labeled \'\nValueError: Can only compare identically-labeled DataFrame objects\nprint df2.isin(df1)\n    A      B      C      D\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\n']";"[""df1 = pd.DataFrame(np.random.rand(10,4),columns=list('ABCD'))\nprint df1\n\n       A         B         C         D\n0.860379  0.726956  0.394529  0.833217\n0.014180  0.813828  0.559891  0.339647\n0.782838  0.698993  0.551252  0.361034\n0.833370  0.982056  0.741821  0.006864\n0.855955  0.546562  0.270425  0.136006\n0.491538  0.445024  0.971603  0.690001\n0.911696  0.065338  0.796946  0.853456\n0.744923  0.545661  0.492739  0.337628\n0.576235  0.219831  0.946772  0.752403\n0.164873  0.454862  0.745890  0.437729\n"", 'df2 = df1.ix[4:8]\ndf2.reset_index(drop=True,inplace=True)\ndf2.loc[-1] = [2, 3, 4, 5]\ndf2.loc[-2] = [14, 15, 16, 17]\ndf2.reset_index(drop=True,inplace=True)\nprint df2\n\n           A         B         C         D\n    0.855955  0.546562  0.270425  0.136006\n    0.491538  0.445024  0.971603  0.690001\n    0.911696  0.065338  0.796946  0.853456\n    0.744923  0.545661  0.492739  0.337628\n    0.576235  0.219831  0.946772  0.752403\n    2.000000  3.000000  4.000000  5.000000\n   14.000000 15.000000 16.000000 17.000000\n', 'list1 = df2.ix[0].tolist()\ncols = df1.columns.tolist()\nprint df1.lookup(list1, cols)\n', '  File ""C:\\Users\\test.py"", line 19, in <module>\n    print df1.lookup(list1, cols)\n  File ""C:\\python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 2217, in lookup\n    raise KeyError(\'One or more row labels was not found\')\nKeyError: \'One or more row labels was not found\'\n', 'print (df2 == df1).all(1).any()\n', '  File ""C:\\Users\\test.py"", line 12, in <module>\n    print (df2 == df1).all(1).any()\n  File ""C:\\python27\\lib\\site-packages\\pandas\\core\\ops.py"", line 884, in f\n    return self._compare_frame(other, func, str_rep)\n  File ""C:\\python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 3010, in _compare_frame\n    raise ValueError(\'Can only compare identically-labeled \'\nValueError: Can only compare identically-labeled DataFrame objects\n', 'print df2.isin(df1)\n', '    A      B      C      D\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\n']";"['df1', ""df1 = pd.DataFrame(np.random.rand(10,4),columns=list('ABCD'))\nprint df1\n\n       A         B         C         D\n0.860379  0.726956  0.394529  0.833217\n0.014180  0.813828  0.559891  0.339647\n0.782838  0.698993  0.551252  0.361034\n0.833370  0.982056  0.741821  0.006864\n0.855955  0.546562  0.270425  0.136006\n0.491538  0.445024  0.971603  0.690001\n0.911696  0.065338  0.796946  0.853456\n0.744923  0.545661  0.492739  0.337628\n0.576235  0.219831  0.946772  0.752403\n0.164873  0.454862  0.745890  0.437729\n"", 'df2', 'df1', 'df2', 'df2 = df1.ix[4:8]\ndf2.reset_index(drop=True,inplace=True)\ndf2.loc[-1] = [2, 3, 4, 5]\ndf2.loc[-2] = [14, 15, 16, 17]\ndf2.reset_index(drop=True,inplace=True)\nprint df2\n\n           A         B         C         D\n    0.855955  0.546562  0.270425  0.136006\n    0.491538  0.445024  0.971603  0.690001\n    0.911696  0.065338  0.796946  0.853456\n    0.744923  0.545661  0.492739  0.337628\n    0.576235  0.219831  0.946772  0.752403\n    2.000000  3.000000  4.000000  5.000000\n   14.000000 15.000000 16.000000 17.000000\n', 'df.lookup', 'list1 = df2.ix[0].tolist()\ncols = df1.columns.tolist()\nprint df1.lookup(list1, cols)\n', '  File ""C:\\Users\\test.py"", line 19, in <module>\n    print df1.lookup(list1, cols)\n  File ""C:\\python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 2217, in lookup\n    raise KeyError(\'One or more row labels was not found\')\nKeyError: \'One or more row labels was not found\'\n', '.all()', 'print (df2 == df1).all(1).any()\n', '  File ""C:\\Users\\test.py"", line 12, in <module>\n    print (df2 == df1).all(1).any()\n  File ""C:\\python27\\lib\\site-packages\\pandas\\core\\ops.py"", line 884, in f\n    return self._compare_frame(other, func, str_rep)\n  File ""C:\\python27\\lib\\site-packages\\pandas\\core\\frame.py"", line 3010, in _compare_frame\n    raise ValueError(\'Can only compare identically-labeled \'\nValueError: Can only compare identically-labeled DataFrame objects\n', 'isin()', 'print df2.isin(df1)\n', 'False', '    A      B      C      D\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\n', 'df2', 'df1']";"[""df1 = pd.DataFrame(np.random.rand(10,4),columns=list('ABCD'))\n\ndf2 = df1.ix[4:8]\ndf2.reset_index(drop=True,inplace=True)\ndf2.loc[-1] = [2, 3, 4, 5]\ndf2.loc[-2] = [14, 15, 16, 17]\ndf2.reset_index(drop=True,inplace=True)\n\nlist1 = df2.ix[0].tolist()\ncols = df1.columns.tolist()\nprint (df2 == df1).all(1).any()\n""]";"[""import pandas as pd\ndf1 = pd.DataFrame(np.random.rand(10,4),columns=list('ABCD'))\n\ndf2 = df1.ix[4:8]\ndf2.reset_index(drop=True,inplace=True)\ndf2.loc[-1] = [2, 3, 4, 5]\ndf2.loc[-2] = [14, 15, 16, 17]\ndf2.reset_index(drop=True,inplace=True)\n\nlist1 = df2.ix[0].tolist()\ncols = df1.columns.tolist()\nprint (df2 == df1).all(1).any()\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\ndf1 = pd.DataFrame(np.random.rand(10,4),columns=list('ABCD'))\n\ndf2 = df1.ix[4:8]\ndf2.reset_index(drop=True,inplace=True)\ndf2.loc[-1] = [2, 3, 4, 5]\ndf2.loc[-2] = [14, 15, 16, 17]\ndf2.reset_index(drop=True,inplace=True)\n\nlist1 = df2.ix[0].tolist()\ncols = df1.columns.tolist()\nprint (df2 == df1).all(1).any()\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'df1' is not defined""]";['NameError'];0;1;"[""name 'df1' is not defined""]";['NameError']
994;994;994;994;1.0;2;29498652;;1;11;<python><pandas><plot>;Plot bar graph from Pandas DataFrame;28656.0;"['Hour | V1 | V2 | A1 | A2\n 0   | 15 | 13 | 25 | 37  \n 1   | 26 | 52 | 21 | 45 \n 2   | 18 | 45 | 45 | 25 \n 3   | 65 | 38 | 98 | 14\nimport matplotlib.pyplot as plt\nax = df.plot(kind=\'bar\', title =""V comp"",figsize=(15,10),legend=True, fontsize=12)\nax.set_xlabel(""Hour"",fontsize=12)\nax.set_ylabel(""V"",fontsize=12)\n']";"['Hour | V1 | V2 | A1 | A2\n 0   | 15 | 13 | 25 | 37  \n 1   | 26 | 52 | 21 | 45 \n 2   | 18 | 45 | 45 | 25 \n 3   | 65 | 38 | 98 | 14\n', 'import matplotlib.pyplot as plt\nax = df.plot(kind=\'bar\', title =""V comp"",figsize=(15,10),legend=True, fontsize=12)\nax.set_xlabel(""Hour"",fontsize=12)\nax.set_ylabel(""V"",fontsize=12)\n']";"['DataFrame', 'Hour | V1 | V2 | A1 | A2\n 0   | 15 | 13 | 25 | 37  \n 1   | 26 | 52 | 21 | 45 \n 2   | 18 | 45 | 45 | 25 \n 3   | 65 | 38 | 98 | 14\n', 'V1', 'V2', 'Hour', 'import matplotlib.pyplot as plt\nax = df.plot(kind=\'bar\', title =""V comp"",figsize=(15,10),legend=True, fontsize=12)\nax.set_xlabel(""Hour"",fontsize=12)\nax.set_ylabel(""V"",fontsize=12)\n', 'V1', 'V2']";"['Hour | V1 | V2 | A1 | A2\nimport matplotlib.pyplot as plt\nax = df.plot(kind=\'bar\', title =""V comp"",figsize=(15,10),legend=True, fontsize=12)\nax.set_xlabel(""Hour"",fontsize=12)\nax.set_ylabel(""V"",fontsize=12)\n']";"['Hour | V1 | V2 | A1 | A2\nimport matplotlib.pyplot as plt\nax = df.plot(kind=\'bar\', title =""V comp"",figsize=(15,10),legend=True, fontsize=12)\nax.set_xlabel(""Hour"",fontsize=12)\nax.set_ylabel(""V"",fontsize=12)\n']";False;"['import pandas as pd\ndf = pd.DataFrame()\nHour | V1 | V2 | A1 | A2\nimport matplotlib.pyplot as plt\nax = df.plot(kind=\'bar\', title =""V comp"",figsize=(15,10),legend=True, fontsize=12)\nax.set_xlabel(""Hour"",fontsize=12)\nax.set_ylabel(""V"",fontsize=12)\n']";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"['""[\'V1\' \'V2\'] not in index""']";['KeyError']
995;995;995;995;5.0;0;29516084;;1;11;<python><amazon-web-services><pandas><amazon-ec2><elastic-beanstalk>;'gcc' failed during pandas build on AWS Elastic Beanstalk;2250.0;"[""  building 'pandas.msgpack' extension\n\n  gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D__LITTLE_ENDIAN__=1 -Ipandas/src/klib -Ipandas/src -I/opt/python/run/venv/local/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/msgpack.cpp -o build/temp.linux-x86_64-2.7/pandas/msgpack.o\n\n  gcc: error trying to exec 'cc1plus': execvp: No such file or directory\n\n  error: command 'gcc' failed with exit status 1\npackages:\n   yum:\n      gcc: []\n      gcc-c++: []\n""]";"[""  building 'pandas.msgpack' extension\n\n  gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D__LITTLE_ENDIAN__=1 -Ipandas/src/klib -Ipandas/src -I/opt/python/run/venv/local/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/msgpack.cpp -o build/temp.linux-x86_64-2.7/pandas/msgpack.o\n\n  gcc: error trying to exec 'cc1plus': execvp: No such file or directory\n\n  error: command 'gcc' failed with exit status 1\n"", 'packages:\n   yum:\n      gcc: []\n      gcc-c++: []\n']";"[""  building 'pandas.msgpack' extension\n\n  gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D__LITTLE_ENDIAN__=1 -Ipandas/src/klib -Ipandas/src -I/opt/python/run/venv/local/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/msgpack.cpp -o build/temp.linux-x86_64-2.7/pandas/msgpack.o\n\n  gcc: error trying to exec 'cc1plus': execvp: No such file or directory\n\n  error: command 'gcc' failed with exit status 1\n"", '64bit Amazon Linux 2015.03 v1.3.0 running Python 2.7', '.ebextensions/00_gcc.config', 'packages:\n   yum:\n      gcc: []\n      gcc-c++: []\n']";['\n\n\n'];['\n\n\n'];False;['import pandas as pd\n\n\n\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
996;996;996;996;3.0;0;29517072;;1;28;<python><pandas>;Add column to dataframe with default value;24041.0;['Date, Open, High, Low, Close\n01-01-2015, 565, 600, 400, 450\nName, Date, Open, High, Low, Close\nabc, 01-01-2015, 565, 600, 400, 450\n'];['Date, Open, High, Low, Close\n01-01-2015, 565, 600, 400, 450\n', 'Name, Date, Open, High, Low, Close\nabc, 01-01-2015, 565, 600, 400, 450\n'];['Date, Open, High, Low, Close\n01-01-2015, 565, 600, 400, 450\n', 'Name, Date, Open, High, Low, Close\nabc, 01-01-2015, 565, 600, 400, 450\n'];['Date, Open, High, Low, Close\nName, Date, Open, High, Low, Close\n'];['Date, Open, High, Low, Close\nName, Date, Open, High, Low, Close\n'];False;['import pandas as pd\nDate, Open, High, Low, Close\nName, Date, Open, High, Low, Close\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
997;997;997;997;4.0;0;29525808;;1;37;<python><pandas><sqlalchemy><flask-sqlalchemy>;SQLAlchemy ORM conversion to pandas DataFrame;9483.0;[''];[];['<Query object>', 'pandas.read_sql', '.db.session.query(Item).filter(Item.symbol.in_(add_symbols)', 'Item', 'add_symbols', 'SELECT ... from ... WHERE ... IN'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'pd' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'query' is not defined"", ""name 'session' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'query' is not defined"", ""name 'session' is not defined""]";['NameError', 'NameError']
998;998;998;998;8.0;1;29530232;;1;113;<python><pandas><nan>;Python pandas: check if any value is NaN in DataFrame;136260.0;[''];[];['pd.isnan'];[''];[''];False;['import pandas as pd\n'];False;2;4;"['Sucess', 'Sucess', ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'Sucess', 'NameError', 'NameError'];2;4;"['Sucess', 'Sucess', ""name 'df' is not defined"", ""name 'df' is not defined""]";['Sucess', 'Sucess', 'NameError', 'NameError'];4;4;['Sucess', 'Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess', 'Sucess']
999;999;999;999;2.0;0;29532894;;1;13;<python><pandas><matplotlib><plot><ipython-notebook>;iPython/Jupyter Notebook and Pandas, how to plot multiple graphs in a for loop?;10329.0;"[""from pandas import *\n%matplotlib inline\n\nys = [[0,1,2,3,4],[4,3,2,1,0]]\nx_ax = [0,1,2,3,4]\n\nfor y_ax in ys:\n    ts = Series(y_ax,index=x_ax)\n    ts.plot(kind='bar', figsize=(15,5))\n""]";"[""from pandas import *\n%matplotlib inline\n\nys = [[0,1,2,3,4],[4,3,2,1,0]]\nx_ax = [0,1,2,3,4]\n\nfor y_ax in ys:\n    ts = Series(y_ax,index=x_ax)\n    ts.plot(kind='bar', figsize=(15,5))\n""]";"[""from pandas import *\n%matplotlib inline\n\nys = [[0,1,2,3,4],[4,3,2,1,0]]\nx_ax = [0,1,2,3,4]\n\nfor y_ax in ys:\n    ts = Series(y_ax,index=x_ax)\n    ts.plot(kind='bar', figsize=(15,5))\n"", 'for']";['from pandas import *\n\nys = [[0,1,2,3,4],[4,3,2,1,0]]\nx_ax = [0,1,2,3,4]\n\n'];['from pandas import *\n\nys = [[0,1,2,3,4],[4,3,2,1,0]]\nx_ax = [0,1,2,3,4]\n\n'];False;['import pandas as pd\nfrom pandas import *\n\nys = [[0,1,2,3,4],[4,3,2,1,0]]\nx_ax = [0,1,2,3,4]\n\n'];False;0;2;"[""No module named 'matplotlib'"", ""No module named 'matplotlib'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'matplotlib'"", ""No module named 'matplotlib'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'matplotlib'"", ""No module named 'matplotlib'""]";['ImportError', 'ImportError']
1000;1000;1000;1000;4.0;10;29545704;;1;12;<python><numpy><pandas><gis><haversine>;Fast Haversine Approximation (Python/Pandas);3266.0;"['from math import radians, cos, sin, asin, sqrt\ndef haversine(lon1, lat1, lon2, lat2):\n    """"""\n    Calculate the great circle distance between two points \n    on the earth (specified in decimal degrees)\n    """"""\n    # convert decimal degrees to radians \n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    # haversine formula \n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n    c = 2 * asin(sqrt(a)) \n    km = 6367 * c\n    return km\n\n\nfor index, row in df.iterrows():\n    df.loc[index, \'distance\'] = haversine(row[\'a_longitude\'], row[\'a_latitude\'], row[\'b_longitude\'], row[\'b_latitude\'])\n']";"['from math import radians, cos, sin, asin, sqrt\ndef haversine(lon1, lat1, lon2, lat2):\n    """"""\n    Calculate the great circle distance between two points \n    on the earth (specified in decimal degrees)\n    """"""\n    # convert decimal degrees to radians \n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    # haversine formula \n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n    c = 2 * asin(sqrt(a)) \n    km = 6367 * c\n    return km\n\n\nfor index, row in df.iterrows():\n    df.loc[index, \'distance\'] = haversine(row[\'a_longitude\'], row[\'a_latitude\'], row[\'b_longitude\'], row[\'b_latitude\'])\n']";"['from math import radians, cos, sin, asin, sqrt\ndef haversine(lon1, lat1, lon2, lat2):\n    """"""\n    Calculate the great circle distance between two points \n    on the earth (specified in decimal degrees)\n    """"""\n    # convert decimal degrees to radians \n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    # haversine formula \n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n    c = 2 * asin(sqrt(a)) \n    km = 6367 * c\n    return km\n\n\nfor index, row in df.iterrows():\n    df.loc[index, \'distance\'] = haversine(row[\'a_longitude\'], row[\'a_latitude\'], row[\'b_longitude\'], row[\'b_latitude\'])\n']";['from math import radians, cos, sin, asin, sqrt\n    # convert decimal degrees to radians \n    # haversine formula \n\n\n'];['from math import radians, cos, sin, asin, sqrt\n    # convert decimal degrees to radians \n    # haversine formula \n\n\n'];False;['import pandas as pd\nfrom math import radians, cos, sin, asin, sqrt\n    # convert decimal degrees to radians \n    # haversine formula \n\n\n'];False;1;2;['Sucess', '/path/to/haversine.so: cannot open shared object file: No such file or directory'];['Sucess', 'OSError'];1;2;['Sucess', '/path/to/haversine.so: cannot open shared object file: No such file or directory'];['Sucess', 'OSError'];1;2;['Sucess', '/path/to/haversine.so: cannot open shared object file: No such file or directory'];['Sucess', 'OSError']
1001;1001;1001;1001;3.0;4;29550414;;1;18;<python><numpy><pandas><dataframe><tuples>;how to split column of tuples in pandas dataframe?;8089.0;"["">>> d1\n   y norm test  y norm train  len(y_train)  len(y_test)  \\\n0    64.904368    116.151232          1645          549   \n1    70.852681    112.639876          1645          549   \n\n                                    SVR RBF  \\\n0   (35.652207342877873, 22.95533537448393)   \n1  (39.563683797747622, 27.382483096332511)   \n\n                                        LCV  \\\n0  (19.365430594452338, 13.880062435173587)   \n1  (19.099614489458364, 14.018867136617146)   \n\n                                   RIDGE CV  \\\n0  (4.2907610988480362, 12.416745648065584)   \n1    (4.18864306788194, 12.980833914392477)   \n\n                                         RF  \\\n0   (9.9484841581029428, 16.46902345373697)   \n1  (10.139848213735391, 16.282141345406522)   \n\n                                           GB  \\\n0  (0.012816232716538605, 15.950164822266007)   \n1  (0.012814519804493328, 15.305745202851712)   \n\n                                             ET DATA  \n0  (0.00034337162272515505, 16.284800366214057)  j2m  \n1  (0.00024811554516431878, 15.556506191784194)  j2m  \n>>> \n>>> d1['LCV'].apply(pd.Series)\n                                          0\n0  (19.365430594452338, 13.880062435173587)\n1  (19.099614489458364, 14.018867136617146)\n>>> \n>>> d1['LCV'].apply(eval).apply(pd.Series)\n           0          1\n0  19.365431  13.880062\n1  19.099614  14.018867\n>>> \n""]";"['>>> d1\n   y norm test  y norm train  len(y_train)  len(y_test)  \\\n0    64.904368    116.151232          1645          549   \n1    70.852681    112.639876          1645          549   \n\n                                    SVR RBF  \\\n0   (35.652207342877873, 22.95533537448393)   \n1  (39.563683797747622, 27.382483096332511)   \n\n                                        LCV  \\\n0  (19.365430594452338, 13.880062435173587)   \n1  (19.099614489458364, 14.018867136617146)   \n\n                                   RIDGE CV  \\\n0  (4.2907610988480362, 12.416745648065584)   \n1    (4.18864306788194, 12.980833914392477)   \n\n                                         RF  \\\n0   (9.9484841581029428, 16.46902345373697)   \n1  (10.139848213735391, 16.282141345406522)   \n\n                                           GB  \\\n0  (0.012816232716538605, 15.950164822266007)   \n1  (0.012814519804493328, 15.305745202851712)   \n\n                                             ET DATA  \n0  (0.00034337162272515505, 16.284800366214057)  j2m  \n1  (0.00024811554516431878, 15.556506191784194)  j2m  \n>>> \n', "">>> d1['LCV'].apply(pd.Series)\n                                          0\n0  (19.365430594452338, 13.880062435173587)\n1  (19.099614489458364, 14.018867136617146)\n>>> \n"", "">>> d1['LCV'].apply(eval).apply(pd.Series)\n           0          1\n0  19.365431  13.880062\n1  19.099614  14.018867\n>>> \n""]";"['>>> d1\n   y norm test  y norm train  len(y_train)  len(y_test)  \\\n0    64.904368    116.151232          1645          549   \n1    70.852681    112.639876          1645          549   \n\n                                    SVR RBF  \\\n0   (35.652207342877873, 22.95533537448393)   \n1  (39.563683797747622, 27.382483096332511)   \n\n                                        LCV  \\\n0  (19.365430594452338, 13.880062435173587)   \n1  (19.099614489458364, 14.018867136617146)   \n\n                                   RIDGE CV  \\\n0  (4.2907610988480362, 12.416745648065584)   \n1    (4.18864306788194, 12.980833914392477)   \n\n                                         RF  \\\n0   (9.9484841581029428, 16.46902345373697)   \n1  (10.139848213735391, 16.282141345406522)   \n\n                                           GB  \\\n0  (0.012816232716538605, 15.950164822266007)   \n1  (0.012814519804493328, 15.305745202851712)   \n\n                                             ET DATA  \n0  (0.00034337162272515505, 16.284800366214057)  j2m  \n1  (0.00024811554516431878, 15.556506191784194)  j2m  \n>>> \n', 'LCV', 'LCV-a', 'LCV-b', "">>> d1['LCV'].apply(pd.Series)\n                                          0\n0  (19.365430594452338, 13.880062435173587)\n1  (19.099614489458364, 14.018867136617146)\n>>> \n"", "">>> d1['LCV'].apply(eval).apply(pd.Series)\n           0          1\n0  19.365431  13.880062\n1  19.099614  14.018867\n>>> \n""]";['\n0   (35.652207342877873, 22.95533537448393)   \n1  (39.563683797747622, 27.382483096332511)   \n\n0  (19.365430594452338, 13.880062435173587)   \n1  (19.099614489458364, 14.018867136617146)   \n\n0  (4.2907610988480362, 12.416745648065584)   \n1    (4.18864306788194, 12.980833914392477)   \n\n0   (9.9484841581029428, 16.46902345373697)   \n1  (10.139848213735391, 16.282141345406522)   \n\n0  (0.012816232716538605, 15.950164822266007)   \n1  (0.012814519804493328, 15.305745202851712)   \n\n0  (19.365430594452338, 13.880062435173587)\n1  (19.099614489458364, 14.018867136617146)\n'];['\n0   (35.652207342877873, 22.95533537448393)   \n1  (39.563683797747622, 27.382483096332511)   \n\n0  (19.365430594452338, 13.880062435173587)   \n1  (19.099614489458364, 14.018867136617146)   \n\n0  (4.2907610988480362, 12.416745648065584)   \n1    (4.18864306788194, 12.980833914392477)   \n\n0   (9.9484841581029428, 16.46902345373697)   \n1  (10.139848213735391, 16.282141345406522)   \n\n0  (0.012816232716538605, 15.950164822266007)   \n1  (0.012814519804493328, 15.305745202851712)   \n\n0  (19.365430594452338, 13.880062435173587)\n1  (19.099614489458364, 14.018867136617146)\n'];False;['import pandas as pd\n\n0   (35.652207342877873, 22.95533537448393)   \n1  (39.563683797747622, 27.382483096332511)   \n\n0  (19.365430594452338, 13.880062435173587)   \n1  (19.099614489458364, 14.018867136617146)   \n\n0  (4.2907610988480362, 12.416745648065584)   \n1    (4.18864306788194, 12.980833914392477)   \n\n0   (9.9484841581029428, 16.46902345373697)   \n1  (10.139848213735391, 16.282141345406522)   \n\n0  (0.012816232716538605, 15.950164822266007)   \n1  (0.012814519804493328, 15.305745202851712)   \n\n0  (19.365430594452338, 13.880062435173587)\n1  (19.099614489458364, 14.018867136617146)\n'];False;1;2;"[""name 'pd' is not defined"", 'Sucess']";['NameError', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
1002;1002;1002;1002;3.0;0;29576430;;1;60;<python><pandas><dataframe><permutation><shuffle>;Shuffle DataFrame rows;36846.0;['    Col1  Col2  Col3  Type\n0      1     2     3     1\n1      4     5     6     1\n...\n20     7     8     9     2\n21    10    11    12     2\n...\n45    13    14    15     3\n46    16    17    18     3\n...\n    Col1  Col2  Col3  Type\n0      7     8     9     2\n1     13    14    15     3\n...\n20     1     2     3     1\n21    10    11    12     2\n...\n45     4     5     6     1\n46    16    17    18     3\n...\n'];['    Col1  Col2  Col3  Type\n0      1     2     3     1\n1      4     5     6     1\n...\n20     7     8     9     2\n21    10    11    12     2\n...\n45    13    14    15     3\n46    16    17    18     3\n...\n', '    Col1  Col2  Col3  Type\n0      7     8     9     2\n1     13    14    15     3\n...\n20     1     2     3     1\n21    10    11    12     2\n...\n45     4     5     6     1\n46    16    17    18     3\n...\n'];['    Col1  Col2  Col3  Type\n0      1     2     3     1\n1      4     5     6     1\n...\n20     7     8     9     2\n21    10    11    12     2\n...\n45    13    14    15     3\n46    16    17    18     3\n...\n', 'Type', 'Type', 'Type', 'Type', '    Col1  Col2  Col3  Type\n0      7     8     9     2\n1     13    14    15     3\n...\n20     1     2     3     1\n21    10    11    12     2\n...\n45     4     5     6     1\n46    16    17    18     3\n...\n'];['...\n...\n...\n...\n...\n...\n'];['...\n...\n...\n...\n...\n...\n'];False;['import pandas as pd\n...\n...\n...\n...\n...\n...\n'];False;0;3;"[""name 'pd' is not defined"", ""name 'df' is not defined"", ""No module named 'sklearn'""]";['NameError', 'NameError', 'ImportError'];0;3;"[""name 'StringIO' is not defined"", ""name 'df' is not defined"", ""No module named 'sklearn'""]";['NameError', 'NameError', 'ImportError'];0;3;"[""name 'StringIO' is not defined"", ""name 'df' is not defined"", ""No module named 'sklearn'""]";['NameError', 'NameError', 'ImportError']
1003;1003;1003;1003;1.0;6;29607222;;1;15;<python><postgresql><pandas>;Update existing row in database from pandas df;3593.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
1004;1004;1004;1004;2.0;2;29655111;;1;13;<python><numpy><pandas><igraph>;igraph Graph from numpy or pandas adjacency matrix;5110.0;"[""node_names = ['A', 'B', 'C']\na = pd.DataFrame([[1,2,3],[3,1,1],[4,0,2]],\n    index=node_names, columns=node_names)\na_numpy = a.as_matrix()\n""]";"[""node_names = ['A', 'B', 'C']\na = pd.DataFrame([[1,2,3],[3,1,1],[4,0,2]],\n    index=node_names, columns=node_names)\na_numpy = a.as_matrix()\n""]";"['pandas.DataFrame', ""node_names = ['A', 'B', 'C']\na = pd.DataFrame([[1,2,3],[3,1,1],[4,0,2]],\n    index=node_names, columns=node_names)\na_numpy = a.as_matrix()\n"", 'igraph.Graph', 'pandas', 'numpy']";"[""node_names = ['A', 'B', 'C']\na_numpy = a.as_matrix()\n""]";"[""node_names = ['A', 'B', 'C']\na_numpy = a.as_matrix()\n""]";False;"[""import pandas as pd\nnode_names = ['A', 'B', 'C']\na_numpy = a.as_matrix()\n""]";False;0;2;"[""No module named 'igraph'"", ""No module named 'igraph'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'igraph'"", ""No module named 'igraph'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'igraph'"", ""No module named 'igraph'""]";['ImportError', 'ImportError']
1005;1005;1005;1005;1.0;5;29706278;;1;11;<python><sql><pandas><sqlalchemy><pyodbc>;python pandas to_sql with sqlalchemy : how to speed up exporting to MS SQL?;8173.0;"['import pandas as pd\nfrom sqlalchemy import create_engine, MetaData, Table, select\nServerName = ""myserver""\nDatabase = ""mydatabase""\nTableName = ""mytable""\n\nengine = create_engine(\'mssql+pyodbc://\' + ServerName + \'/\' + Database)\nconn = engine.connect()\n\nmetadata = MetaData(conn)\n\nmy_data_frame.to_sql(TableName,engine)\n']";"['import pandas as pd\nfrom sqlalchemy import create_engine, MetaData, Table, select\nServerName = ""myserver""\nDatabase = ""mydatabase""\nTableName = ""mytable""\n\nengine = create_engine(\'mssql+pyodbc://\' + ServerName + \'/\' + Database)\nconn = engine.connect()\n\nmetadata = MetaData(conn)\n\nmy_data_frame.to_sql(TableName,engine)\n']";"['import pandas as pd\nfrom sqlalchemy import create_engine, MetaData, Table, select\nServerName = ""myserver""\nDatabase = ""mydatabase""\nTableName = ""mytable""\n\nengine = create_engine(\'mssql+pyodbc://\' + ServerName + \'/\' + Database)\nconn = engine.connect()\n\nmetadata = MetaData(conn)\n\nmy_data_frame.to_sql(TableName,engine)\n']";"['import pandas as pd\nfrom sqlalchemy import create_engine, MetaData, Table, select\nServerName = ""myserver""\nDatabase = ""mydatabase""\nTableName = ""mytable""\n\nengine = create_engine(\'mssql+pyodbc://\' + ServerName + \'/\' + Database)\nconn = engine.connect()\n\nmetadata = MetaData(conn)\n\nmy_data_frame.to_sql(TableName,engine)\n']";"['import pandas as pd\nfrom sqlalchemy import create_engine, MetaData, Table, select\nServerName = ""myserver""\nDatabase = ""mydatabase""\nTableName = ""mytable""\n\nengine = create_engine(\'mssql+pyodbc://\' + ServerName + \'/\' + Database)\nconn = engine.connect()\n\nmetadata = MetaData(conn)\n\nmy_data_frame.to_sql(TableName,engine)\n']";False;"['import pandas as pd\nimport pandas as pd\nfrom sqlalchemy import create_engine, MetaData, Table, select\nServerName = ""myserver""\nDatabase = ""mydatabase""\nTableName = ""mytable""\n\nengine = create_engine(\'mssql+pyodbc://\' + ServerName + \'/\' + Database)\nconn = engine.connect()\n\nmetadata = MetaData(conn)\n\nmy_data_frame.to_sql(TableName,engine)\n']";False;0;0;[];[];0;0;[];[];0;0;[];[]
1006;1006;1006;1006;4.0;0;29763620;;1;29;<python><pandas>;How to select all columns, except one column in pandas using .ix;26374.0;"[""    import pandas\n    import numpy as np\n    df = DataFrame(np.random.rand(4,4), columns = list('abcd'))\n    df\n          a         b         c         d\n    0  0.418762  0.042369  0.869203  0.972314\n    1  0.991058  0.510228  0.594784  0.534366\n    2  0.407472  0.259811  0.396664  0.894202\n    3  0.726168  0.139531  0.324932  0.906575\n""]";"[""    import pandas\n    import numpy as np\n    df = DataFrame(np.random.rand(4,4), columns = list('abcd'))\n    df\n          a         b         c         d\n    0  0.418762  0.042369  0.869203  0.972314\n    1  0.991058  0.510228  0.594784  0.534366\n    2  0.407472  0.259811  0.396664  0.894202\n    3  0.726168  0.139531  0.324932  0.906575\n""]";"[""    import pandas\n    import numpy as np\n    df = DataFrame(np.random.rand(4,4), columns = list('abcd'))\n    df\n          a         b         c         d\n    0  0.418762  0.042369  0.869203  0.972314\n    1  0.991058  0.510228  0.594784  0.534366\n    2  0.407472  0.259811  0.396664  0.894202\n    3  0.726168  0.139531  0.324932  0.906575\n"", 'column b', 'df.ix']";[''];[''];False;['import pandas as pd\n'];False;1;3;"[""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError'];1;3;"[""name 'df' is not defined"", 'Sucess', ""name 'df' is not defined""]";['NameError', 'Sucess', 'NameError'];2;3;['\n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated', 'Sucess', 'Sucess'];['DeprecationWarning', 'Sucess', 'Sucess']
1007;1007;1007;1007;3.0;0;29765548;;1;17;<python><pandas>;Remove index name in pandas;14140.0;['In [10]: df\nOut[10]: \n         Column 1\nfoo              \nApples          1\nOranges         2\nPuppies         3\nDucks           4\nIn [10]: df\nOut[10]: \n         Column 1             \nApples          1\nOranges         2\nPuppies         3\nDucks           4\n'];['In [10]: df\nOut[10]: \n         Column 1\nfoo              \nApples          1\nOranges         2\nPuppies         3\nDucks           4\n', 'In [10]: df\nOut[10]: \n         Column 1             \nApples          1\nOranges         2\nPuppies         3\nDucks           4\n'];['In [10]: df\nOut[10]: \n         Column 1\nfoo              \nApples          1\nOranges         2\nPuppies         3\nDucks           4\n', 'index name', 'foo', 'In [10]: df\nOut[10]: \n         Column 1             \nApples          1\nOranges         2\nPuppies         3\nDucks           4\n'];['df\ndf\n'];['df\ndf\n'];False;['import pandas as pd\ndf = pd.DataFrame()\ndf\ndf\n'];True;0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'foo' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""name 'foo' is not defined""]";['NameError', 'NameError', 'NameError'];2;3;"['Sucess', 'Sucess', ""name 'foo' is not defined""]";['Sucess', 'Sucess', 'NameError']
1008;1008;1008;1008;3.0;0;29794959;;1;16;<python><pandas>;pandas - add new column to dataframe from dictionary;11620.0;"[""U,L\n111,en\n112,en\n112,es\n113,es\n113,ja\n113,zh\n114,es\nd = {112: 'en', 113: 'es', 114: 'es', 111: 'en'}\nU,L,D\n111,en,en\n112,en,en\n112,es,en\n113,es,es\n113,ja,es\n113,zh,es\n114,es,es\n""]";"['U,L\n111,en\n112,en\n112,es\n113,es\n113,ja\n113,zh\n114,es\n', ""d = {112: 'en', 113: 'es', 114: 'es', 111: 'en'}\n"", 'U,L,D\n111,en,en\n112,en,en\n112,es,en\n113,es,es\n113,ja,es\n113,zh,es\n114,es,es\n']";"['U,L\n111,en\n112,en\n112,es\n113,es\n113,ja\n113,zh\n114,es\n', ""d = {112: 'en', 113: 'es', 114: 'es', 111: 'en'}\n"", 'U,L,D\n111,en,en\n112,en,en\n112,es,en\n113,es,es\n113,ja,es\n113,zh,es\n114,es,es\n', 'pd.join()']";"[""U,L\n111,en\n112,en\n112,es\n113,es\n113,ja\n113,zh\n114,es\nd = {112: 'en', 113: 'es', 114: 'es', 111: 'en'}\nU,L,D\n111,en,en\n112,en,en\n112,es,en\n113,es,es\n113,ja,es\n113,zh,es\n114,es,es\n""]";"[""U,L\n111,en\n112,en\n112,es\n113,es\n113,ja\n113,zh\n114,es\nd = {112: 'en', 113: 'es', 114: 'es', 111: 'en'}\nU,L,D\n111,en,en\n112,en,en\n112,es,en\n113,es,es\n113,ja,es\n113,zh,es\n114,es,es\n""]";False;"[""import pandas as pd\nU,L\n111,en\n112,en\n112,es\n113,es\n113,ja\n113,zh\n114,es\nd = {112: 'en', 113: 'es', 114: 'es', 111: 'en'}\nU,L,D\n111,en,en\n112,en,en\n112,es,en\n113,es,es\n113,ja,es\n113,zh,es\n114,es,es\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'U'""]";['KeyError']
1009;1009;1009;1009;3.0;1;29815129;;1;38;<python><list><dictionary><pandas><dataframe>;Pandas DataFrame to List of Dictionaries;15409.0;"[""\ncustomer    item1      item2    item3\n1           apple      milk     tomato\n2           water      orange   potato\n3           juice      mango    chips\nrows = [{'customer': 1, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},\n    {'customer': 2, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},\n    {'customer': 3, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]\n""]";"['\ncustomer    item1      item2    item3\n1           apple      milk     tomato\n2           water      orange   potato\n3           juice      mango    chips\n', ""rows = [{'customer': 1, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},\n    {'customer': 2, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},\n    {'customer': 3, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]\n""]";"[""rows = [{'customer': 1, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},\n    {'customer': 2, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},\n    {'customer': 3, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]\n""]";['\n'];['\n'];False;['import pandas as pd\n\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
1010;1010;1010;1010;1.0;0;29836836;;1;11;<python><pandas><filtering><dataframe>;How do I filter a pandas DataFrame based on value counts?;5042.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1011;1011;1011;1011;3.0;0;29902714;;1;16;<python><string><pandas>;Print the complete string of a pandas dataframe;10013.0;"[""df = pd.DataFrame({'one' : ['one', 'two', 'This is very long string very long string very long string veryvery long string']})\n""]";"[""df = pd.DataFrame({'one' : ['one', 'two', 'This is very long string very long string very long string veryvery long string']})\n""]";"[""df = pd.DataFrame({'one' : ['one', 'two', 'This is very long string very long string very long string veryvery long string']})\n""]";"[""df = pd.DataFrame({'one' : ['one', 'two', 'This is very long string very long string very long string veryvery long string']})\n""]";"[""import pandas as pd\ndf = pd.DataFrame({'one' : ['one', 'two', 'This is very long string very long string very long string veryvery long string']})\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame({'one' : ['one', 'two', 'This is very long string very long string very long string veryvery long string']})\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;['single positional indexer is out-of-bounds'];['IndexError']
1012;1012;1012;1012;3.0;0;29919306;;1;30;<python><pandas><dataframe><max>;Find the column name which has the maximum value for each row;10758.0;['In [7]:\nframe.head()\nOut[7]:\nCommunications and Search   Business    General Lifestyle\n0   0.745763    0.050847    0.118644    0.084746\n0   0.333333    0.000000    0.583333    0.083333\n0   0.617021    0.042553    0.297872    0.042553\n0   0.435897    0.000000    0.410256    0.153846\n0   0.358974    0.076923    0.410256    0.153846\nIn [7]:\n    frame.head()\n    Out[7]:\n    Communications and Search   Business    General Lifestyle   Max\n    0   0.745763    0.050847    0.118644    0.084746           Communications \n    0   0.333333    0.000000    0.583333    0.083333           Business  \n    0   0.617021    0.042553    0.297872    0.042553           Communications \n    0   0.435897    0.000000    0.410256    0.153846           Communications \n    0   0.358974    0.076923    0.410256    0.153846           Business \n'];['In [7]:\nframe.head()\nOut[7]:\nCommunications and Search   Business    General Lifestyle\n0   0.745763    0.050847    0.118644    0.084746\n0   0.333333    0.000000    0.583333    0.083333\n0   0.617021    0.042553    0.297872    0.042553\n0   0.435897    0.000000    0.410256    0.153846\n0   0.358974    0.076923    0.410256    0.153846\n', 'In [7]:\n    frame.head()\n    Out[7]:\n    Communications and Search   Business    General Lifestyle   Max\n    0   0.745763    0.050847    0.118644    0.084746           Communications \n    0   0.333333    0.000000    0.583333    0.083333           Business  \n    0   0.617021    0.042553    0.297872    0.042553           Communications \n    0   0.435897    0.000000    0.410256    0.153846           Communications \n    0   0.358974    0.076923    0.410256    0.153846           Business \n'];['In [7]:\nframe.head()\nOut[7]:\nCommunications and Search   Business    General Lifestyle\n0   0.745763    0.050847    0.118644    0.084746\n0   0.333333    0.000000    0.583333    0.083333\n0   0.617021    0.042553    0.297872    0.042553\n0   0.435897    0.000000    0.410256    0.153846\n0   0.358974    0.076923    0.410256    0.153846\n', 'In [7]:\n    frame.head()\n    Out[7]:\n    Communications and Search   Business    General Lifestyle   Max\n    0   0.745763    0.050847    0.118644    0.084746           Communications \n    0   0.333333    0.000000    0.583333    0.083333           Business  \n    0   0.617021    0.042553    0.297872    0.042553           Communications \n    0   0.435897    0.000000    0.410256    0.153846           Communications \n    0   0.358974    0.076923    0.410256    0.153846           Business \n'];['frame.head()\nframe.head()\n    '];['frame.head()\nframe.head()\n    '];False;['import pandas as pd\nframe.head()\nframe.head()\n    '];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1013;1013;1013;1013;1.0;4;29954263;;1;16;<python><numpy><pandas>;"What does the term ""broadcasting"" mean in Pandas documentation?";2181.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
1014;1014;1014;1014;1.0;3;29971075;;1;20;<python><pandas><dataframe><count><nan>;Count number of non-NaN entries in every column of Dataframe;14021.0;"[""df1 = pd.DataFrame([(1,2,None),(None,4,None),(5,None,7),(5,None,None)], \n                    columns=['a','b','d'], index = ['A', 'B','C','D'])\n\n    a   b   d\nA   1   2 NaN\nB NaN   4 NaN\nC   5 NaN   7\nD   5 NaN NaN\na: 3\nb: 2\nd: 1\n""]";"[""df1 = pd.DataFrame([(1,2,None),(None,4,None),(5,None,7),(5,None,None)], \n                    columns=['a','b','d'], index = ['A', 'B','C','D'])\n\n    a   b   d\nA   1   2 NaN\nB NaN   4 NaN\nC   5 NaN   7\nD   5 NaN NaN\n"", 'a: 3\nb: 2\nd: 1\n']";"[""df1 = pd.DataFrame([(1,2,None),(None,4,None),(5,None,7),(5,None,None)], \n                    columns=['a','b','d'], index = ['A', 'B','C','D'])\n\n    a   b   d\nA   1   2 NaN\nB NaN   4 NaN\nC   5 NaN   7\nD   5 NaN NaN\n"", 'a: 3\nb: 2\nd: 1\n']";['\n'];['\n'];False;['import pandas as pd\n\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1015;1015;1015;1015;1.0;0;30009948;;1;12;<python><pandas>;How to reorder indexed rows based on a list in Pandas data frame;12941.0;"['company  Amazon  Apple  Yahoo\nname\nA             0    130      0\nC           173      0      0\nZ             0      0    150\nimport pandas as pd\ndf = pd.DataFrame({\'name\' : [\'A\', \'Z\',\'C\'],\n                   \'company\' : [\'Apple\', \'Yahoo\',\'Amazon\'],\n                   \'height\' : [130, 150,173]})\n\ndf = df.pivot(index=""name"", columns=""company"", values=""height"").fillna(0)\ncompany  Amazon  Apple  Yahoo\nname\nZ             0      0    150\nC           173      0      0\nA             0    130      0\n']";"['company  Amazon  Apple  Yahoo\nname\nA             0    130      0\nC           173      0      0\nZ             0      0    150\n', 'import pandas as pd\ndf = pd.DataFrame({\'name\' : [\'A\', \'Z\',\'C\'],\n                   \'company\' : [\'Apple\', \'Yahoo\',\'Amazon\'],\n                   \'height\' : [130, 150,173]})\n\ndf = df.pivot(index=""name"", columns=""company"", values=""height"").fillna(0)\n', 'company  Amazon  Apple  Yahoo\nname\nZ             0      0    150\nC           173      0      0\nA             0    130      0\n']";"['company  Amazon  Apple  Yahoo\nname\nA             0    130      0\nC           173      0      0\nZ             0      0    150\n', 'import pandas as pd\ndf = pd.DataFrame({\'name\' : [\'A\', \'Z\',\'C\'],\n                   \'company\' : [\'Apple\', \'Yahoo\',\'Amazon\'],\n                   \'height\' : [130, 150,173]})\n\ndf = df.pivot(index=""name"", columns=""company"", values=""height"").fillna(0)\n', 'name', '[""Z"", ""C"", ""A""]', 'company  Amazon  Apple  Yahoo\nname\nZ             0      0    150\nC           173      0      0\nA             0    130      0\n']";"['name\nimport pandas as pd\n\ndf = df.pivot(index=""name"", columns=""company"", values=""height"").fillna(0)\nname\n']";"['name\nimport pandas as pd\n\ndf = df.pivot(index=""name"", columns=""company"", values=""height"").fillna(0)\nname\n']";False;"['import pandas as pd\nname\nimport pandas as pd\n\ndf = df.pivot(index=""name"", columns=""company"", values=""height"").fillna(0)\nname\n']";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
1016;1016;1016;1016;1.0;0;30017491;;1;15;<python><pandas><machine-learning><nlp><scikit-learn>;Problems obtaining most informative features with scikit learn?;1804.0;"['def most_informative_feature_for_class(vectorizer, classifier, classlabel, n=10):\n    labelid = list(classifier.classes_).index(classlabel)\n    feature_names = vectorizer.get_feature_names()\n    topn = sorted(zip(classifier.coef_[labelid], feature_names))[-n:]\n\n    for coef, feat in topn:\n        print classlabel, feat, coef\nmost_informative_feature_for_class(tfidf_vect, clf, 5)\nX = tfidf_vect.fit_transform(df[\'content\'].values)\ny = df[\'label\'].values\n\n\nfrom sklearn import cross_validation\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X,\n                                                    y, test_size=0.33)\nclf = SVC(kernel=\'linear\', C=1)\nclf.fit(X, y)\nprediction = clf.predict(X_test)\n5 a_base_de_bien bastante   (0, 2451)   -0.210683496368\n  (0, 3533) -0.173621065386\n  (0, 8034) -0.135543062425\n  (0, 10346)    -0.173621065386\n  (0, 15231)    -0.154148294738\n  (0, 18261)    -0.158890483047\n  (0, 21083)    -0.297476572586\n  (0, 434)  -0.0596263855375\n  (0, 446)  -0.0753492277856\n  (0, 769)  -0.0753492277856\n  (0, 1118) -0.0753492277856\n  (0, 1439) -0.0753492277856\n  (0, 1605) -0.0753492277856\n  (0, 1755) -0.0637950312345\n  (0, 3504) -0.0753492277856\n  (0, 3511) -0.115802483001\n  (0, 4382) -0.0668983049212\n  (0, 5247) -0.315713152154\n  (0, 5396) -0.0753492277856\n  (0, 5753) -0.0716096348446\n  (0, 6507) -0.130661516772\n  (0, 7978) -0.0753492277856\n  (0, 8296) -0.144739048504\n  (0, 8740) -0.0753492277856\n  (0, 8906) -0.0753492277856\n  : :\n  (0, 23282)    0.418623443832\n  (0, 4100) 0.385906085143\n  (0, 15735)    0.207958503155\n  (0, 16620)    0.385906085143\n  (0, 19974)    0.0936828782325\n  (0, 20304)    0.385906085143\n  (0, 21721)    0.385906085143\n  (0, 22308)    0.301270427482\n  (0, 14903)    0.314164150621\n  (0, 16904)    0.0653764031957\n  (0, 20805)    0.0597723455204\n  (0, 21878)    0.403750815828\n  (0, 22582)    0.0226150073272\n  (0, 6532) 0.525138162099\n  (0, 6670) 0.525138162099\n  (0, 10341)    0.525138162099\n  (0, 13627)    0.278332617058\n  (0, 1600) 0.326774799211\n  (0, 2074) 0.310556919237\n  (0, 5262) 0.176400451433\n  (0, 6373) 0.290124806858\n  (0, 8593) 0.290124806858\n  (0, 12002)    0.282832270298\n  (0, 15008)    0.290124806858\n  (0, 19207)    0.326774799211\ndef print_top10(vectorizer, clf, class_labels):\n    """"""Prints features with the highest coefficient values, per class""""""\n    feature_names = vectorizer.get_feature_names()\n    for i, class_label in enumerate(class_labels):\n        top10 = np.argsort(clf.coef_[i])[-10:]\n        print(""%s: %s"" % (class_label,\n              "" "".join(feature_names[j] for j in top10)))\n\n\nprint_top10(tfidf_vect,clf,y)\n  File ""/Users/user/PycharmProjects/TESIS_FINAL/Classification/Supervised_learning/Final/experimentos/RBF/SVM_con_rbf.py"", line 237, in <module>\n    print_top10(tfidf_vect,clf,5)\n  File ""/Users/user/PycharmProjects/TESIS_FINAL/Classification/Supervised_learning/Final/experimentos/RBF/SVM_con_rbf.py"", line 231, in print_top10\n    for i, class_label in enumerate(class_labels):\nTypeError: \'int\' object is not iterable\n']";"['def most_informative_feature_for_class(vectorizer, classifier, classlabel, n=10):\n    labelid = list(classifier.classes_).index(classlabel)\n    feature_names = vectorizer.get_feature_names()\n    topn = sorted(zip(classifier.coef_[labelid], feature_names))[-n:]\n\n    for coef, feat in topn:\n        print classlabel, feat, coef\n', 'most_informative_feature_for_class(tfidf_vect, clf, 5)\n', ""X = tfidf_vect.fit_transform(df['content'].values)\ny = df['label'].values\n\n\nfrom sklearn import cross_validation\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X,\n                                                    y, test_size=0.33)\nclf = SVC(kernel='linear', C=1)\nclf.fit(X, y)\nprediction = clf.predict(X_test)\n"", '5 a_base_de_bien bastante   (0, 2451)   -0.210683496368\n  (0, 3533) -0.173621065386\n  (0, 8034) -0.135543062425\n  (0, 10346)    -0.173621065386\n  (0, 15231)    -0.154148294738\n  (0, 18261)    -0.158890483047\n  (0, 21083)    -0.297476572586\n  (0, 434)  -0.0596263855375\n  (0, 446)  -0.0753492277856\n  (0, 769)  -0.0753492277856\n  (0, 1118) -0.0753492277856\n  (0, 1439) -0.0753492277856\n  (0, 1605) -0.0753492277856\n  (0, 1755) -0.0637950312345\n  (0, 3504) -0.0753492277856\n  (0, 3511) -0.115802483001\n  (0, 4382) -0.0668983049212\n  (0, 5247) -0.315713152154\n  (0, 5396) -0.0753492277856\n  (0, 5753) -0.0716096348446\n  (0, 6507) -0.130661516772\n  (0, 7978) -0.0753492277856\n  (0, 8296) -0.144739048504\n  (0, 8740) -0.0753492277856\n  (0, 8906) -0.0753492277856\n  : :\n  (0, 23282)    0.418623443832\n  (0, 4100) 0.385906085143\n  (0, 15735)    0.207958503155\n  (0, 16620)    0.385906085143\n  (0, 19974)    0.0936828782325\n  (0, 20304)    0.385906085143\n  (0, 21721)    0.385906085143\n  (0, 22308)    0.301270427482\n  (0, 14903)    0.314164150621\n  (0, 16904)    0.0653764031957\n  (0, 20805)    0.0597723455204\n  (0, 21878)    0.403750815828\n  (0, 22582)    0.0226150073272\n  (0, 6532) 0.525138162099\n  (0, 6670) 0.525138162099\n  (0, 10341)    0.525138162099\n  (0, 13627)    0.278332617058\n  (0, 1600) 0.326774799211\n  (0, 2074) 0.310556919237\n  (0, 5262) 0.176400451433\n  (0, 6373) 0.290124806858\n  (0, 8593) 0.290124806858\n  (0, 12002)    0.282832270298\n  (0, 15008)    0.290124806858\n  (0, 19207)    0.326774799211\n', 'def print_top10(vectorizer, clf, class_labels):\n    """"""Prints features with the highest coefficient values, per class""""""\n    feature_names = vectorizer.get_feature_names()\n    for i, class_label in enumerate(class_labels):\n        top10 = np.argsort(clf.coef_[i])[-10:]\n        print(""%s: %s"" % (class_label,\n              "" "".join(feature_names[j] for j in top10)))\n\n\nprint_top10(tfidf_vect,clf,y)\n', '  File ""/Users/user/PycharmProjects/TESIS_FINAL/Classification/Supervised_learning/Final/experimentos/RBF/SVM_con_rbf.py"", line 237, in <module>\n    print_top10(tfidf_vect,clf,5)\n  File ""/Users/user/PycharmProjects/TESIS_FINAL/Classification/Supervised_learning/Final/experimentos/RBF/SVM_con_rbf.py"", line 231, in print_top10\n    for i, class_label in enumerate(class_labels):\nTypeError: \'int\' object is not iterable\n']";"['def most_informative_feature_for_class(vectorizer, classifier, classlabel, n=10):\n    labelid = list(classifier.classes_).index(classlabel)\n    feature_names = vectorizer.get_feature_names()\n    topn = sorted(zip(classifier.coef_[labelid], feature_names))[-n:]\n\n    for coef, feat in topn:\n        print classlabel, feat, coef\n', 'most_informative_feature_for_class(tfidf_vect, clf, 5)\n', ""X = tfidf_vect.fit_transform(df['content'].values)\ny = df['label'].values\n\n\nfrom sklearn import cross_validation\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X,\n                                                    y, test_size=0.33)\nclf = SVC(kernel='linear', C=1)\nclf.fit(X, y)\nprediction = clf.predict(X_test)\n"", 'most_informative_feature_for_class', '5 a_base_de_bien bastante   (0, 2451)   -0.210683496368\n  (0, 3533) -0.173621065386\n  (0, 8034) -0.135543062425\n  (0, 10346)    -0.173621065386\n  (0, 15231)    -0.154148294738\n  (0, 18261)    -0.158890483047\n  (0, 21083)    -0.297476572586\n  (0, 434)  -0.0596263855375\n  (0, 446)  -0.0753492277856\n  (0, 769)  -0.0753492277856\n  (0, 1118) -0.0753492277856\n  (0, 1439) -0.0753492277856\n  (0, 1605) -0.0753492277856\n  (0, 1755) -0.0637950312345\n  (0, 3504) -0.0753492277856\n  (0, 3511) -0.115802483001\n  (0, 4382) -0.0668983049212\n  (0, 5247) -0.315713152154\n  (0, 5396) -0.0753492277856\n  (0, 5753) -0.0716096348446\n  (0, 6507) -0.130661516772\n  (0, 7978) -0.0753492277856\n  (0, 8296) -0.144739048504\n  (0, 8740) -0.0753492277856\n  (0, 8906) -0.0753492277856\n  : :\n  (0, 23282)    0.418623443832\n  (0, 4100) 0.385906085143\n  (0, 15735)    0.207958503155\n  (0, 16620)    0.385906085143\n  (0, 19974)    0.0936828782325\n  (0, 20304)    0.385906085143\n  (0, 21721)    0.385906085143\n  (0, 22308)    0.301270427482\n  (0, 14903)    0.314164150621\n  (0, 16904)    0.0653764031957\n  (0, 20805)    0.0597723455204\n  (0, 21878)    0.403750815828\n  (0, 22582)    0.0226150073272\n  (0, 6532) 0.525138162099\n  (0, 6670) 0.525138162099\n  (0, 10341)    0.525138162099\n  (0, 13627)    0.278332617058\n  (0, 1600) 0.326774799211\n  (0, 2074) 0.310556919237\n  (0, 5262) 0.176400451433\n  (0, 6373) 0.290124806858\n  (0, 8593) 0.290124806858\n  (0, 12002)    0.282832270298\n  (0, 15008)    0.290124806858\n  (0, 19207)    0.326774799211\n', 'def print_top10(vectorizer, clf, class_labels):\n    """"""Prints features with the highest coefficient values, per class""""""\n    feature_names = vectorizer.get_feature_names()\n    for i, class_label in enumerate(class_labels):\n        top10 = np.argsort(clf.coef_[i])[-10:]\n        print(""%s: %s"" % (class_label,\n              "" "".join(feature_names[j] for j in top10)))\n\n\nprint_top10(tfidf_vect,clf,y)\n', '  File ""/Users/user/PycharmProjects/TESIS_FINAL/Classification/Supervised_learning/Final/experimentos/RBF/SVM_con_rbf.py"", line 237, in <module>\n    print_top10(tfidf_vect,clf,5)\n  File ""/Users/user/PycharmProjects/TESIS_FINAL/Classification/Supervised_learning/Final/experimentos/RBF/SVM_con_rbf.py"", line 231, in print_top10\n    for i, class_label in enumerate(class_labels):\nTypeError: \'int\' object is not iterable\n']";"[""\nmost_informative_feature_for_class(tfidf_vect, clf, 5)\nX = tfidf_vect.fit_transform(df['content'].values)\ny = df['label'].values\n\n\nfrom sklearn import cross_validation\nclf = SVC(kernel='linear', C=1)\nclf.fit(X, y)\nprediction = clf.predict(X_test)\n\n\nprint_top10(tfidf_vect,clf,y)\n""]";"[""\nmost_informative_feature_for_class(tfidf_vect, clf, 5)\nX = tfidf_vect.fit_transform(df['content'].values)\ny = df['label'].values\n\n\nfrom sklearn import cross_validation\nclf = SVC(kernel='linear', C=1)\nclf.fit(X, y)\nprediction = clf.predict(X_test)\n\n\nprint_top10(tfidf_vect,clf,y)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\n\nmost_informative_feature_for_class(tfidf_vect, clf, 5)\nX = tfidf_vect.fit_transform(df['content'].values)\ny = df['label'].values\n\n\nfrom sklearn import cross_validation\nclf = SVC(kernel='linear', C=1)\nclf.fit(X, y)\nprediction = clf.predict(X_test)\n\n\nprint_top10(tfidf_vect,clf,y)\n""]";True;0;1;"[""No module named 'sklearn'""]";['ImportError'];0;1;"[""No module named 'sklearn'""]";['ImportError'];0;1;"[""No module named 'sklearn'""]";['ImportError']
1017;1017;1017;1017;1.0;1;30023927;;1;17;<python><pandas><scikit-learn>;"sklearn.cross_validation.StratifiedShuffleSplit - error: ""indices are out-of-bounds""";6029.0;"['import pandas as pd\nimport numpy as np\n# UCI\'s wine dataset\nwine = pd.read_csv(""https://s3.amazonaws.com/demo-datasets/wine.csv"")\n\n# separate target variable from dataset\ntarget = wine[\'quality\']\ndata = wine.drop(\'quality\',axis = 1)\n\n# Stratified Split of train and test data\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nsss = StratifiedShuffleSplit(target, n_iter=3, test_size=0.2)\n\nfor train_index, test_index in sss:\n    xtrain, xtest = data[train_index], data[test_index]\n    ytrain, ytest = target[train_index], target[test_index]\n\n# Check target series for distribution of classes\nytrain.value_counts()\nytest.value_counts()\nIndexError: indices are out-of-bounds\n']";"['import pandas as pd\nimport numpy as np\n# UCI\'s wine dataset\nwine = pd.read_csv(""https://s3.amazonaws.com/demo-datasets/wine.csv"")\n\n# separate target variable from dataset\ntarget = wine[\'quality\']\ndata = wine.drop(\'quality\',axis = 1)\n\n# Stratified Split of train and test data\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nsss = StratifiedShuffleSplit(target, n_iter=3, test_size=0.2)\n\nfor train_index, test_index in sss:\n    xtrain, xtest = data[train_index], data[test_index]\n    ytrain, ytest = target[train_index], target[test_index]\n\n# Check target series for distribution of classes\nytrain.value_counts()\nytest.value_counts()\n', 'IndexError: indices are out-of-bounds\n']";"['import pandas as pd\nimport numpy as np\n# UCI\'s wine dataset\nwine = pd.read_csv(""https://s3.amazonaws.com/demo-datasets/wine.csv"")\n\n# separate target variable from dataset\ntarget = wine[\'quality\']\ndata = wine.drop(\'quality\',axis = 1)\n\n# Stratified Split of train and test data\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nsss = StratifiedShuffleSplit(target, n_iter=3, test_size=0.2)\n\nfor train_index, test_index in sss:\n    xtrain, xtest = data[train_index], data[test_index]\n    ytrain, ytest = target[train_index], target[test_index]\n\n# Check target series for distribution of classes\nytrain.value_counts()\nytest.value_counts()\n', 'IndexError: indices are out-of-bounds\n']";"['import pandas as pd\nimport numpy as np\n# UCI\'s wine dataset\nwine = pd.read_csv(""https://s3.amazonaws.com/demo-datasets/wine.csv"")\n\n# separate target variable from dataset\ntarget = wine[\'quality\']\ndata = wine.drop(\'quality\',axis = 1)\n\n# Stratified Split of train and test data\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nsss = StratifiedShuffleSplit(target, n_iter=3, test_size=0.2)\n\n\n# Check target series for distribution of classes\nytrain.value_counts()\nytest.value_counts()\n']";"['import pandas as pd\nimport numpy as np\n# UCI\'s wine dataset\nwine = pd.read_csv(""https://s3.amazonaws.com/demo-datasets/wine.csv"")\n\n# separate target variable from dataset\ntarget = wine[\'quality\']\ndata = wine.drop(\'quality\',axis = 1)\n\n# Stratified Split of train and test data\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nsss = StratifiedShuffleSplit(target, n_iter=3, test_size=0.2)\n\n\n# Check target series for distribution of classes\nytrain.value_counts()\nytest.value_counts()\n']";False;"['import pandas as pd\nimport pandas as pd\nimport numpy as np\n# UCI\'s wine dataset\nwine = pd.read_csv(""https://s3.amazonaws.com/demo-datasets/wine.csv"")\n\n# separate target variable from dataset\ntarget = wine[\'quality\']\ndata = wine.drop(\'quality\',axis = 1)\n\n# Stratified Split of train and test data\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nsss = StratifiedShuffleSplit(target, n_iter=3, test_size=0.2)\n\n\n# Check target series for distribution of classes\nytrain.value_counts()\nytest.value_counts()\n']";False;0;1;"[""name 'data' is not defined""]";['NameError'];0;1;"[""name 'data' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
1018;1018;1018;1018;3.0;1;30026815;;1;15;<python><pandas>;Add Multiple Columns to Pandas Dataframe from Function;15980.0;"['def getH(t): #gives the hour\n    return t.hour\ndef getW(d): #gives the week number\n    return d.isocalendar()[1] \ndef getD(d): #gives the weekday\n    return d.weekday() # 0 for Monday, 6 for Sunday\n\nmydf[""hour""] = mydf.apply(lambda row:getH(row[""mytime""]), axis=1)\nmydf[""weekday""] = mydf.apply(lambda row:getD(row[""mydate""]), axis=1)\nmydf[""weeknum""] = mydf.apply(lambda row:getW(row[""mydate""]), axis=1)\ndef getHWd(d,t):\n    return t.hour, d.isocalendar()[1], d.weekday()\n']";"['def getH(t): #gives the hour\n    return t.hour\ndef getW(d): #gives the week number\n    return d.isocalendar()[1] \ndef getD(d): #gives the weekday\n    return d.weekday() # 0 for Monday, 6 for Sunday\n\nmydf[""hour""] = mydf.apply(lambda row:getH(row[""mytime""]), axis=1)\nmydf[""weekday""] = mydf.apply(lambda row:getD(row[""mydate""]), axis=1)\nmydf[""weeknum""] = mydf.apply(lambda row:getW(row[""mydate""]), axis=1)\n', 'def getHWd(d,t):\n    return t.hour, d.isocalendar()[1], d.weekday()\n']";"['mydf', 'mydate', 'mytime', 'hour', 'weekday', 'weeknum', 'def getH(t): #gives the hour\n    return t.hour\ndef getW(d): #gives the week number\n    return d.isocalendar()[1] \ndef getD(d): #gives the weekday\n    return d.weekday() # 0 for Monday, 6 for Sunday\n\nmydf[""hour""] = mydf.apply(lambda row:getH(row[""mytime""]), axis=1)\nmydf[""weekday""] = mydf.apply(lambda row:getD(row[""mydate""]), axis=1)\nmydf[""weeknum""] = mydf.apply(lambda row:getW(row[""mydate""]), axis=1)\n', 'zip', 'merge', 'def getHWd(d,t):\n    return t.hour, d.isocalendar()[1], d.weekday()\n']";"['\nmydf[""hour""] = mydf.apply(lambda row:getH(row[""mytime""]), axis=1)\nmydf[""weekday""] = mydf.apply(lambda row:getD(row[""mydate""]), axis=1)\nmydf[""weeknum""] = mydf.apply(lambda row:getW(row[""mydate""]), axis=1)\n']";"['\nmydf[""hour""] = mydf.apply(lambda row:getH(row[""mytime""]), axis=1)\nmydf[""weekday""] = mydf.apply(lambda row:getD(row[""mydate""]), axis=1)\nmydf[""weeknum""] = mydf.apply(lambda row:getW(row[""mydate""]), axis=1)\n']";False;"['import pandas as pd\ndf = pd.DataFrame()\n\nmydf[""hour""] = mydf.apply(lambda row:getH(row[""mytime""]), axis=1)\nmydf[""weekday""] = mydf.apply(lambda row:getD(row[""mydate""]), axis=1)\nmydf[""weeknum""] = mydf.apply(lambda row:getW(row[""mydate""]), axis=1)\n']";True;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError']
1019;1019;1019;1019;2.0;1;30053329;;1;20;<python><pandas><numpy><dataframe><nan>;Elegant way to create empty pandas DataFrame with NaN of type float;43287.0;"[""import pandas as pd\n\ndf = pd.DataFrame(index=range(0,4),columns=['A'])\nimport pandas as pd\nimport numpy as np\n\ndummyarray = np.empty((4,1))\ndummyarray[:] = np.nan\n\ndf = pd.DataFrame(dummyarray)\n""]";"[""import pandas as pd\n\ndf = pd.DataFrame(index=range(0,4),columns=['A'])\n"", 'import pandas as pd\nimport numpy as np\n\ndummyarray = np.empty((4,1))\ndummyarray[:] = np.nan\n\ndf = pd.DataFrame(dummyarray)\n']";"[""import pandas as pd\n\ndf = pd.DataFrame(index=range(0,4),columns=['A'])\n"", 'interpolate()', 'import pandas as pd\nimport numpy as np\n\ndummyarray = np.empty((4,1))\ndummyarray[:] = np.nan\n\ndf = pd.DataFrame(dummyarray)\n', 'interpolate()']";"[""import pandas as pd\n\ndf = pd.DataFrame(index=range(0,4),columns=['A'])\nimport pandas as pd\nimport numpy as np\n\ndummyarray = np.empty((4,1))\ndummyarray[:] = np.nan\n\ndf = pd.DataFrame(dummyarray)\n""]";"[""import pandas as pd\n\ndf = pd.DataFrame(index=range(0,4),columns=['A'])\nimport pandas as pd\nimport numpy as np\n\ndummyarray = np.empty((4,1))\ndummyarray[:] = np.nan\n\ndf = pd.DataFrame(dummyarray)\n""]";False;"[""import pandas as pd\nimport pandas as pd\n\ndf = pd.DataFrame(index=range(0,4),columns=['A'])\nimport pandas as pd\nimport numpy as np\n\ndummyarray = np.empty((4,1))\ndummyarray[:] = np.nan\n\ndf = pd.DataFrame(dummyarray)\n""]";False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
1020;1020;1020;1020;1.0;1;30059260;;1;12;<pandas><count><row><dataframe><nan>;Python/Pandas: counting the number of missing/NaN in each row;9681.0;['In [91]: df\nOut[91]:\n 1    3      1      1      1\n 1    3      1      1      1\n 2    3      1      1      1\n 1    1    NaN    NaN    NaN\n 1    3      1      1      1\n 1    1      1      1      1\nIn [91]: list = <somecode with df>\nIn [92]: list\n    Out[91]:\n     [0,\n      0,\n      0,\n      3,\n      0,\n      0]\n'];['In [91]: df\nOut[91]:\n 1    3      1      1      1\n 1    3      1      1      1\n 2    3      1      1      1\n 1    1    NaN    NaN    NaN\n 1    3      1      1      1\n 1    1      1      1      1\n', 'In [91]: list = <somecode with df>\nIn [92]: list\n    Out[91]:\n     [0,\n      0,\n      0,\n      3,\n      0,\n      0]\n'];['In [91]: df\nOut[91]:\n 1    3      1      1      1\n 1    3      1      1      1\n 2    3      1      1      1\n 1    1    NaN    NaN    NaN\n 1    3      1      1      1\n 1    1      1      1      1\n', 'In [91]: list = <somecode with df>\nIn [92]: list\n    Out[91]:\n     [0,\n      0,\n      0,\n      3,\n      0,\n      0]\n'];['df\n    '];['df\n    '];False;['import pandas as pd\ndf = pd.DataFrame()\ndf\n    '];True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
1021;1021;1021;1021;2.0;9;30088006;;1;18;<python><json><python-2.7><pandas>;Loading a file with more than one line of JSON into Python's Pandas;10287.0;"['{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""P_Mk0ygOilLJo4_WEvabAA"", ""review_id"": ""OeT5kgUOe3vcN7H6ImVmZQ"", ""stars"": 3, ""date"": ""2005-08-26"", ""text"": ""This is a pretty typical cafe.  The sandwiches and wraps are good but a little overpriced and the food items are the same.  The chicken caesar salad wrap is my favorite here but everything else is pretty much par for the course."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/Users/d/anaconda/lib/python2.7/site-packages/pandas/io/json.py"", line 198, in read_json\n    date_unit).parse()\n  File ""/Users/d/anaconda/lib/python2.7/site-packages/pandas/io/json.py"", line 266, in parse\n    self._parse_no_numpy()\n  File ""/Users/d/anaconda/lib/python2.7/site-packages/pandas/io/json.py"", line 483, in _parse_no_numpy\n    loads(json, precise_float=self.precise_float), dtype=None)\nValueError: Trailing data\n{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""P_Mk0ygOilLJo4_WEvabAA"", ""review_id"": ""OeT5kgUOe3vcN7H6ImVmZQ"", ""stars"": 3, ""date"": ""2005-08-26"", ""text"": ""This is a pretty typical cafe.  The sandwiches and wraps are good but a little overpriced and the food items are the same.  The chicken caesar salad wrap is my favorite here but everything else is pretty much par for the course."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""TNJRTBrl0yjtpAACr1Bthg"", ""review_id"": ""qq3zF2dDUh3EjMDuKBqhEA"", ""stars"": 3, ""date"": ""2005-11-23"", ""text"": ""I agree with other reviewers - this is a pretty typical financial district cafe.  However, they have fantastic pies.  I ordered three pies for an office event (apple, pumpkin cheesecake, and pecan) - all were delicious, particularly the cheesecake.  The sucker weighed in about 4 pounds - no joke.\\n\\nNo surprises on the cafe side - great pies and cakes from the catering business."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""H_mngeK3DmjlOu595zZMsA"", ""review_id"": ""i3eQTINJXe3WUmyIpvhE9w"", ""stars"": 3, ""date"": ""2005-11-23"", ""text"": ""Decent enough food, but very overpriced. Just a large soup is almost $5. Their specials are $6.50, and with an overpriced soda or juice, it\'s approaching $10. A bit much for a cafe lunch!"", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\nParse error on line 14:\n...t7sRT4zwdbzQ8KQmw""}{    ""votes"": {   \n----------------------^\nExpecting \'EOF\', \'}\', \',\', \']\'\n']";"['{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""P_Mk0ygOilLJo4_WEvabAA"", ""review_id"": ""OeT5kgUOe3vcN7H6ImVmZQ"", ""stars"": 3, ""date"": ""2005-08-26"", ""text"": ""This is a pretty typical cafe.  The sandwiches and wraps are good but a little overpriced and the food items are the same.  The chicken caesar salad wrap is my favorite here but everything else is pretty much par for the course."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n', 'Traceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/Users/d/anaconda/lib/python2.7/site-packages/pandas/io/json.py"", line 198, in read_json\n    date_unit).parse()\n  File ""/Users/d/anaconda/lib/python2.7/site-packages/pandas/io/json.py"", line 266, in parse\n    self._parse_no_numpy()\n  File ""/Users/d/anaconda/lib/python2.7/site-packages/pandas/io/json.py"", line 483, in _parse_no_numpy\n    loads(json, precise_float=self.precise_float), dtype=None)\nValueError: Trailing data\n', '{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""P_Mk0ygOilLJo4_WEvabAA"", ""review_id"": ""OeT5kgUOe3vcN7H6ImVmZQ"", ""stars"": 3, ""date"": ""2005-08-26"", ""text"": ""This is a pretty typical cafe.  The sandwiches and wraps are good but a little overpriced and the food items are the same.  The chicken caesar salad wrap is my favorite here but everything else is pretty much par for the course."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""TNJRTBrl0yjtpAACr1Bthg"", ""review_id"": ""qq3zF2dDUh3EjMDuKBqhEA"", ""stars"": 3, ""date"": ""2005-11-23"", ""text"": ""I agree with other reviewers - this is a pretty typical financial district cafe.  However, they have fantastic pies.  I ordered three pies for an office event (apple, pumpkin cheesecake, and pecan) - all were delicious, particularly the cheesecake.  The sucker weighed in about 4 pounds - no joke.\\n\\nNo surprises on the cafe side - great pies and cakes from the catering business."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""H_mngeK3DmjlOu595zZMsA"", ""review_id"": ""i3eQTINJXe3WUmyIpvhE9w"", ""stars"": 3, ""date"": ""2005-11-23"", ""text"": ""Decent enough food, but very overpriced. Just a large soup is almost $5. Their specials are $6.50, and with an overpriced soda or juice, it\'s approaching $10. A bit much for a cafe lunch!"", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n', 'Parse error on line 14:\n...t7sRT4zwdbzQ8KQmw""}{    ""votes"": {   \n----------------------^\nExpecting \'EOF\', \'}\', \',\', \']\'\n']";"['{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""P_Mk0ygOilLJo4_WEvabAA"", ""review_id"": ""OeT5kgUOe3vcN7H6ImVmZQ"", ""stars"": 3, ""date"": ""2005-08-26"", ""text"": ""This is a pretty typical cafe.  The sandwiches and wraps are good but a little overpriced and the food items are the same.  The chicken caesar salad wrap is my favorite here but everything else is pretty much par for the course."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n', 'df = pd.read_json(path)', 'Traceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/Users/d/anaconda/lib/python2.7/site-packages/pandas/io/json.py"", line 198, in read_json\n    date_unit).parse()\n  File ""/Users/d/anaconda/lib/python2.7/site-packages/pandas/io/json.py"", line 266, in parse\n    self._parse_no_numpy()\n  File ""/Users/d/anaconda/lib/python2.7/site-packages/pandas/io/json.py"", line 483, in _parse_no_numpy\n    loads(json, precise_float=self.precise_float), dtype=None)\nValueError: Trailing data\n', 'Trailing data', '{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""P_Mk0ygOilLJo4_WEvabAA"", ""review_id"": ""OeT5kgUOe3vcN7H6ImVmZQ"", ""stars"": 3, ""date"": ""2005-08-26"", ""text"": ""This is a pretty typical cafe.  The sandwiches and wraps are good but a little overpriced and the food items are the same.  The chicken caesar salad wrap is my favorite here but everything else is pretty much par for the course."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""TNJRTBrl0yjtpAACr1Bthg"", ""review_id"": ""qq3zF2dDUh3EjMDuKBqhEA"", ""stars"": 3, ""date"": ""2005-11-23"", ""text"": ""I agree with other reviewers - this is a pretty typical financial district cafe.  However, they have fantastic pies.  I ordered three pies for an office event (apple, pumpkin cheesecake, and pecan) - all were delicious, particularly the cheesecake.  The sucker weighed in about 4 pounds - no joke.\\n\\nNo surprises on the cafe side - great pies and cakes from the catering business."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""H_mngeK3DmjlOu595zZMsA"", ""review_id"": ""i3eQTINJXe3WUmyIpvhE9w"", ""stars"": 3, ""date"": ""2005-11-23"", ""text"": ""Decent enough food, but very overpriced. Just a large soup is almost $5. Their specials are $6.50, and with an overpriced soda or juice, it\'s approaching $10. A bit much for a cafe lunch!"", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n', 'Parse error on line 14:\n...t7sRT4zwdbzQ8KQmw""}{    ""votes"": {   \n----------------------^\nExpecting \'EOF\', \'}\', \',\', \']\'\n']";"['{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""P_Mk0ygOilLJo4_WEvabAA"", ""review_id"": ""OeT5kgUOe3vcN7H6ImVmZQ"", ""stars"": 3, ""date"": ""2005-08-26"", ""text"": ""This is a pretty typical cafe.  The sandwiches and wraps are good but a little overpriced and the food items are the same.  The chicken caesar salad wrap is my favorite here but everything else is pretty much par for the course."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""P_Mk0ygOilLJo4_WEvabAA"", ""review_id"": ""OeT5kgUOe3vcN7H6ImVmZQ"", ""stars"": 3, ""date"": ""2005-08-26"", ""text"": ""This is a pretty typical cafe.  The sandwiches and wraps are good but a little overpriced and the food items are the same.  The chicken caesar salad wrap is my favorite here but everything else is pretty much par for the course."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""TNJRTBrl0yjtpAACr1Bthg"", ""review_id"": ""qq3zF2dDUh3EjMDuKBqhEA"", ""stars"": 3, ""date"": ""2005-11-23"", ""text"": ""I agree with other reviewers - this is a pretty typical financial district cafe.  However, they have fantastic pies.  I ordered three pies for an office event (apple, pumpkin cheesecake, and pecan) - all were delicious, particularly the cheesecake.  The sucker weighed in about 4 pounds - no joke.\\n\\nNo surprises on the cafe side - great pies and cakes from the catering business."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""H_mngeK3DmjlOu595zZMsA"", ""review_id"": ""i3eQTINJXe3WUmyIpvhE9w"", ""stars"": 3, ""date"": ""2005-11-23"", ""text"": ""Decent enough food, but very overpriced. Just a large soup is almost $5. Their specials are $6.50, and with an overpriced soda or juice, it\'s approaching $10. A bit much for a cafe lunch!"", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n']";"['{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""P_Mk0ygOilLJo4_WEvabAA"", ""review_id"": ""OeT5kgUOe3vcN7H6ImVmZQ"", ""stars"": 3, ""date"": ""2005-08-26"", ""text"": ""This is a pretty typical cafe.  The sandwiches and wraps are good but a little overpriced and the food items are the same.  The chicken caesar salad wrap is my favorite here but everything else is pretty much par for the course."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""P_Mk0ygOilLJo4_WEvabAA"", ""review_id"": ""OeT5kgUOe3vcN7H6ImVmZQ"", ""stars"": 3, ""date"": ""2005-08-26"", ""text"": ""This is a pretty typical cafe.  The sandwiches and wraps are good but a little overpriced and the food items are the same.  The chicken caesar salad wrap is my favorite here but everything else is pretty much par for the course."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""TNJRTBrl0yjtpAACr1Bthg"", ""review_id"": ""qq3zF2dDUh3EjMDuKBqhEA"", ""stars"": 3, ""date"": ""2005-11-23"", ""text"": ""I agree with other reviewers - this is a pretty typical financial district cafe.  However, they have fantastic pies.  I ordered three pies for an office event (apple, pumpkin cheesecake, and pecan) - all were delicious, particularly the cheesecake.  The sucker weighed in about 4 pounds - no joke.\\n\\nNo surprises on the cafe side - great pies and cakes from the catering business."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""H_mngeK3DmjlOu595zZMsA"", ""review_id"": ""i3eQTINJXe3WUmyIpvhE9w"", ""stars"": 3, ""date"": ""2005-11-23"", ""text"": ""Decent enough food, but very overpriced. Just a large soup is almost $5. Their specials are $6.50, and with an overpriced soda or juice, it\'s approaching $10. A bit much for a cafe lunch!"", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n']";False;"['import pandas as pd\n{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""P_Mk0ygOilLJo4_WEvabAA"", ""review_id"": ""OeT5kgUOe3vcN7H6ImVmZQ"", ""stars"": 3, ""date"": ""2005-08-26"", ""text"": ""This is a pretty typical cafe.  The sandwiches and wraps are good but a little overpriced and the food items are the same.  The chicken caesar salad wrap is my favorite here but everything else is pretty much par for the course."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""P_Mk0ygOilLJo4_WEvabAA"", ""review_id"": ""OeT5kgUOe3vcN7H6ImVmZQ"", ""stars"": 3, ""date"": ""2005-08-26"", ""text"": ""This is a pretty typical cafe.  The sandwiches and wraps are good but a little overpriced and the food items are the same.  The chicken caesar salad wrap is my favorite here but everything else is pretty much par for the course."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""TNJRTBrl0yjtpAACr1Bthg"", ""review_id"": ""qq3zF2dDUh3EjMDuKBqhEA"", ""stars"": 3, ""date"": ""2005-11-23"", ""text"": ""I agree with other reviewers - this is a pretty typical financial district cafe.  However, they have fantastic pies.  I ordered three pies for an office event (apple, pumpkin cheesecake, and pecan) - all were delicious, particularly the cheesecake.  The sucker weighed in about 4 pounds - no joke.\\n\\nNo surprises on the cafe side - great pies and cakes from the catering business."", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n{""votes"": {""funny"": 0, ""useful"": 0, ""cool"": 0}, ""user_id"": ""H_mngeK3DmjlOu595zZMsA"", ""review_id"": ""i3eQTINJXe3WUmyIpvhE9w"", ""stars"": 3, ""date"": ""2005-11-23"", ""text"": ""Decent enough food, but very overpriced. Just a large soup is almost $5. Their specials are $6.50, and with an overpriced soda or juice, it\'s approaching $10. A bit much for a cafe lunch!"", ""type"": ""review"", ""business_id"": ""Jp9svt7sRT4zwdbzQ8KQmw""}\n']";False;0;2;"[""name 'data' is not defined"", 'Expected object or value']";['NameError', 'ValueError'];0;2;"[""name 'data' is not defined"", 'Expected object or value']";['NameError', 'ValueError'];0;2;"[""name 'data' is not defined"", 'Expected object or value']";['NameError', 'ValueError']
1022;1022;1022;1022;4.0;1;30102232;;1;11;<python><pandas>;Pandas: can not write to excel file;4532.0;"[""writer = ExcelWriter('output.xlsx')\ndf1.to_excel(writer,'Sheet1')\ndf2.to_excel(writer,'Sheet2')\nwriter.save()\nTypeError: copy() got an unexpected keyword argument 'font'\n""]";"[""writer = ExcelWriter('output.xlsx')\ndf1.to_excel(writer,'Sheet1')\ndf2.to_excel(writer,'Sheet2')\nwriter.save()\n"", ""TypeError: copy() got an unexpected keyword argument 'font'\n""]";"[""writer = ExcelWriter('output.xlsx')\ndf1.to_excel(writer,'Sheet1')\ndf2.to_excel(writer,'Sheet2')\nwriter.save()\n"", ""TypeError: copy() got an unexpected keyword argument 'font'\n""]";"[""writer = ExcelWriter('output.xlsx')\ndf1.to_excel(writer,'Sheet1')\ndf2.to_excel(writer,'Sheet2')\nwriter.save()\n""]";"[""writer = ExcelWriter('output.xlsx')\ndf1.to_excel(writer,'Sheet1')\ndf2.to_excel(writer,'Sheet2')\nwriter.save()\n""]";False;"[""import pandas as pd\ndf2 = pd.DataFrame()\ndf1 = pd.DataFrame()\ndf = pd.DataFrame()\nwriter = ExcelWriter('output.xlsx')\ndf1.to_excel(writer,'Sheet1')\ndf2.to_excel(writer,'Sheet2')\nwriter.save()\n""]";True;1;3;"[""name 'xcell' is not defined"", 'Sucess', ""module 'pandas.core' has no attribute 'format'""]";['NameError', 'Sucess', 'AttributeError'];1;3;"[""name 'xcell' is not defined"", 'Sucess', ""module 'pandas.core' has no attribute 'format'""]";['NameError', 'Sucess', 'AttributeError'];1;3;"[""name 'xcell' is not defined"", 'Sucess', ""module 'pandas.core' has no attribute 'format'""]";['NameError', 'Sucess', 'AttributeError']
1023;1023;1023;1023;3.0;0;30132282;;1;13;<python><datetime><pandas>;datetime to string with series in python pandas;23754.0;"[""dates = p.to_datetime(p.Series(['20010101', '20010331']), format = '%Y%m%d')\ndates.str\n""]";"[""dates = p.to_datetime(p.Series(['20010101', '20010331']), format = '%Y%m%d')\ndates.str\n""]";"[""dates = p.to_datetime(p.Series(['20010101', '20010331']), format = '%Y%m%d')\ndates.str\n""]";"[""dates = p.to_datetime(p.Series(['20010101', '20010331']), format = '%Y%m%d')\ndates.str\n""]";"[""dates = p.to_datetime(p.Series(['20010101', '20010331']), format = '%Y%m%d')\ndates.str\n""]";False;"[""import pandas as pd\ndates = p.to_datetime(p.Series(['20010101', '20010331']), format = '%Y%m%d')\ndates.str\n""]";False;0;2;"[""name 'pd' is not defined"", ""name 'dates' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""name 'dates' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'dates' is not defined""]";['Sucess', 'NameError']
1024;1024;1024;1024;1.0;0;30133280;;1;20;<pandas><matplotlib><plot>;Pandas bar plot changes date format;5797.0;"['df_ts = df.resample(""W"", how=\'max\')\ndf_ts.plot(figsize=(12,8), stacked=True)\ndf_ts = df.resample(""W"", how=\'max\')\ndf_ts.plot(kind=\'bar\', figsize=(12,8), stacked=True)\nstart = pd.to_datetime(""1-1-2012"")\nidx = pd.date_range(start, periods= 365).tolist()\ndf=pd.DataFrame({\'A\':np.random.random(365), \'B\':np.random.random(365)})\ndf.index = idx\ndf_ts = df.resample(\'W\', how= \'max\')\ndf_ts.plot(kind=\'bar\', stacked=True)\n']";"['df_ts = df.resample(""W"", how=\'max\')\ndf_ts.plot(figsize=(12,8), stacked=True)\n', 'df_ts = df.resample(""W"", how=\'max\')\ndf_ts.plot(kind=\'bar\', figsize=(12,8), stacked=True)\n', 'start = pd.to_datetime(""1-1-2012"")\nidx = pd.date_range(start, periods= 365).tolist()\ndf=pd.DataFrame({\'A\':np.random.random(365), \'B\':np.random.random(365)})\ndf.index = idx\ndf_ts = df.resample(\'W\', how= \'max\')\ndf_ts.plot(kind=\'bar\', stacked=True)\n']";"['df_ts = df.resample(""W"", how=\'max\')\ndf_ts.plot(figsize=(12,8), stacked=True)\n', 'df_ts = df.resample(""W"", how=\'max\')\ndf_ts.plot(kind=\'bar\', figsize=(12,8), stacked=True)\n', 'start = pd.to_datetime(""1-1-2012"")\nidx = pd.date_range(start, periods= 365).tolist()\ndf=pd.DataFrame({\'A\':np.random.random(365), \'B\':np.random.random(365)})\ndf.index = idx\ndf_ts = df.resample(\'W\', how= \'max\')\ndf_ts.plot(kind=\'bar\', stacked=True)\n']";"['df_ts = df.resample(""W"", how=\'max\')\ndf_ts.plot(figsize=(12,8), stacked=True)\ndf_ts = df.resample(""W"", how=\'max\')\ndf_ts.plot(kind=\'bar\', figsize=(12,8), stacked=True)\nstart = pd.to_datetime(""1-1-2012"")\nidx = pd.date_range(start, periods= 365).tolist()\ndf=pd.DataFrame({\'A\':np.random.random(365), \'B\':np.random.random(365)})\ndf.index = idx\ndf_ts = df.resample(\'W\', how= \'max\')\ndf_ts.plot(kind=\'bar\', stacked=True)\n']";"['import pandas as pd\ndf_ts = df.resample(""W"", how=\'max\')\ndf_ts.plot(figsize=(12,8), stacked=True)\ndf_ts = df.resample(""W"", how=\'max\')\ndf_ts.plot(kind=\'bar\', figsize=(12,8), stacked=True)\nstart = pd.to_datetime(""1-1-2012"")\nidx = pd.date_range(start, periods= 365).tolist()\ndf=pd.DataFrame({\'A\':np.random.random(365), \'B\':np.random.random(365)})\ndf.index = idx\ndf_ts = df.resample(\'W\', how= \'max\')\ndf_ts.plot(kind=\'bar\', stacked=True)\n']";True;"['import pandas as pd\ndf = pd.DataFrame()\ndf_ts = df.resample(""W"", how=\'max\')\ndf_ts.plot(figsize=(12,8), stacked=True)\ndf_ts = df.resample(""W"", how=\'max\')\ndf_ts.plot(kind=\'bar\', figsize=(12,8), stacked=True)\nstart = pd.to_datetime(""1-1-2012"")\nidx = pd.date_range(start, periods= 365).tolist()\ndf=pd.DataFrame({\'A\':np.random.random(365), \'B\':np.random.random(365)})\ndf.index = idx\ndf_ts = df.resample(\'W\', how= \'max\')\ndf_ts.plot(kind=\'bar\', stacked=True)\n']";True;0;1;"[""name 'ax' is not defined""]";['NameError'];0;1;"[""name 'ax' is not defined""]";['NameError'];0;1;"[""name 'ax' is not defined""]";['NameError']
1025;1025;1025;1025;2.0;1;30211923;;1;20;<python><pandas>;What is the difference between pandas.qcut and pandas.cut?;7956.0;['factors = np.random.randn(30)\n\nIn [11]:\npd.cut(factors, 5)\nOut[11]:\n[(-0.411, 0.575], (-0.411, 0.575], (-0.411, 0.575], (-0.411, 0.575], (0.575, 1.561], ..., (-0.411, 0.575], (-1.397, -0.411], (0.575, 1.561], (-2.388, -1.397], (-0.411, 0.575]]\nLength: 30\nCategories (5, object): [(-2.388, -1.397] < (-1.397, -0.411] < (-0.411, 0.575] < (0.575, 1.561] < (1.561, 2.547]]\n\nIn [14]:\npd.qcut(factors, 5)\nOut[14]:\n[(-0.348, 0.0899], (-0.348, 0.0899], (0.0899, 1.19], (0.0899, 1.19], (0.0899, 1.19], ..., (0.0899, 1.19], (-1.137, -0.348], (1.19, 2.547], [-2.383, -1.137], (-0.348, 0.0899]]\nLength: 30\nCategories (5, object): [[-2.383, -1.137] < (-1.137, -0.348] < (-0.348, 0.0899] < (0.0899, 1.19] < (1.19, 2.547]]`\n'];['factors = np.random.randn(30)\n\nIn [11]:\npd.cut(factors, 5)\nOut[11]:\n[(-0.411, 0.575], (-0.411, 0.575], (-0.411, 0.575], (-0.411, 0.575], (0.575, 1.561], ..., (-0.411, 0.575], (-1.397, -0.411], (0.575, 1.561], (-2.388, -1.397], (-0.411, 0.575]]\nLength: 30\nCategories (5, object): [(-2.388, -1.397] < (-1.397, -0.411] < (-0.411, 0.575] < (0.575, 1.561] < (1.561, 2.547]]\n\nIn [14]:\npd.qcut(factors, 5)\nOut[14]:\n[(-0.348, 0.0899], (-0.348, 0.0899], (0.0899, 1.19], (0.0899, 1.19], (0.0899, 1.19], ..., (0.0899, 1.19], (-1.137, -0.348], (1.19, 2.547], [-2.383, -1.137], (-0.348, 0.0899]]\nLength: 30\nCategories (5, object): [[-2.383, -1.137] < (-1.137, -0.348] < (-0.348, 0.0899] < (0.0899, 1.19] < (1.19, 2.547]]`\n'];['factors = np.random.randn(30)\n\nIn [11]:\npd.cut(factors, 5)\nOut[11]:\n[(-0.411, 0.575], (-0.411, 0.575], (-0.411, 0.575], (-0.411, 0.575], (0.575, 1.561], ..., (-0.411, 0.575], (-1.397, -0.411], (0.575, 1.561], (-2.388, -1.397], (-0.411, 0.575]]\nLength: 30\nCategories (5, object): [(-2.388, -1.397] < (-1.397, -0.411] < (-0.411, 0.575] < (0.575, 1.561] < (1.561, 2.547]]\n\nIn [14]:\npd.qcut(factors, 5)\nOut[14]:\n[(-0.348, 0.0899], (-0.348, 0.0899], (0.0899, 1.19], (0.0899, 1.19], (0.0899, 1.19], ..., (0.0899, 1.19], (-1.137, -0.348], (1.19, 2.547], [-2.383, -1.137], (-0.348, 0.0899]]\nLength: 30\nCategories (5, object): [[-2.383, -1.137] < (-1.137, -0.348] < (-0.348, 0.0899] < (0.0899, 1.19] < (1.19, 2.547]]`\n'];['pd.cut(factors, 5)\npd.qcut(factors, 5)\n'];['import pandas as pd\npd.cut(factors, 5)\npd.qcut(factors, 5)\n'];True;['import pandas as pd\npd.cut(factors, 5)\npd.qcut(factors, 5)\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'factors' is not defined""]";['NameError'];0;1;"[""name 'factors' is not defined""]";['NameError']
1026;1026;1026;1026;2.0;3;30222533;;1;16;<python><datetime><pandas>;Create a day-of-week column in a Pandas dataframe using Python;14335.0;"[""import pandas as pd\n\nimport csv\n\ndf = pd.read_csv('data.csv', parse_dates=['date']))\n\ndf['day-of-week'] = df['date'].weekday()\n\n\nAttributeError: 'Series' object has no attribute 'weekday'\n""]";"[""import pandas as pd\n\nimport csv\n\ndf = pd.read_csv('data.csv', parse_dates=['date']))\n\ndf['day-of-week'] = df['date'].weekday()\n\n\nAttributeError: 'Series' object has no attribute 'weekday'\n""]";"[""import pandas as pd\n\nimport csv\n\ndf = pd.read_csv('data.csv', parse_dates=['date']))\n\ndf['day-of-week'] = df['date'].weekday()\n\n\nAttributeError: 'Series' object has no attribute 'weekday'\n""]";"[""import pandas as pd\n\nimport csv\n\n\ndf['day-of-week'] = df['date'].weekday()\n\n\n""]";"[""import pandas as pd\n\nimport csv\n\n\ndf['day-of-week'] = df['date'].weekday()\n\n\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\n\nimport csv\n\n\ndf['day-of-week'] = df['date'].weekday()\n\n\n""]";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1027;1027;1027;1027;2.0;4;30272300;;1;18;<file><csv><pandas><readfile><categorical-data>;Is it possible to read categorical columns with pandas' read_csv?;2330.0;[''];[];['dtype', 'read_csv', 'dtype={n: pandas.Categorical}'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'StringIO' is not defined""]";['NameError'];0;1;"[""name 'StringIO' is not defined""]";['NameError']
1028;1028;1028;1028;1.0;2;30317119;;1;13;<python><pandas><machine-learning><scikit-learn><nan>;classifiers in scikit-learn that handle nan/null;5960.0;['X_train = np.array([[1, np.nan, 3],[np.nan, 5, 6]])\ny_train = np.array([1, 2])\nclf = RandomForestRegressor(X_train, y_train)\nX_test = np.array([7, 8, np.nan])\ny_pred = clf.predict(X_test) # Fails!\n'];['X_train = np.array([[1, np.nan, 3],[np.nan, 5, 6]])\ny_train = np.array([1, 2])\nclf = RandomForestRegressor(X_train, y_train)\nX_test = np.array([7, 8, np.nan])\ny_pred = clf.predict(X_test) # Fails!\n'];['predict', 'X_train = np.array([[1, np.nan, 3],[np.nan, 5, 6]])\ny_train = np.array([1, 2])\nclf = RandomForestRegressor(X_train, y_train)\nX_test = np.array([7, 8, np.nan])\ny_pred = clf.predict(X_test) # Fails!\n'];['X_train = np.array([[1, np.nan, 3],[np.nan, 5, 6]])\ny_train = np.array([1, 2])\nclf = RandomForestRegressor(X_train, y_train)\nX_test = np.array([7, 8, np.nan])\ny_pred = clf.predict(X_test) # Fails!\n'];['X_train = np.array([[1, np.nan, 3],[np.nan, 5, 6]])\ny_train = np.array([1, 2])\nclf = RandomForestRegressor(X_train, y_train)\nX_test = np.array([7, 8, np.nan])\ny_pred = clf.predict(X_test) # Fails!\n'];False;['import pandas as pd\nX_train = np.array([[1, np.nan, 3],[np.nan, 5, 6]])\ny_train = np.array([1, 2])\nclf = RandomForestRegressor(X_train, y_train)\nX_test = np.array([7, 8, np.nan])\ny_pred = clf.predict(X_test) # Fails!\n'];False;0;1;"[""No module named 'sklearn'""]";['ImportError'];0;1;"[""No module named 'sklearn'""]";['ImportError'];0;1;['from __future__ imports must occur at the beginning of the file (<ast>, line 3)'];['SyntaxError']
1029;1029;1029;1029;2.0;1;30327417;;1;18;<python><pandas><random><integer><range>;Pandas: create new column in df with random integers from range;6072.0;"[""df1['randNumCol'] = random.sample(xrange(50000), len(df1))\nsample(1:5, 50000, replace = TRUE)\n""]";"[""df1['randNumCol'] = random.sample(xrange(50000), len(df1))\n"", 'sample(1:5, 50000, replace = TRUE)\n']";"[""df1['randNumCol'] = random.sample(xrange(50000), len(df1))\n"", 'sample(1:5, 50000, replace = TRUE)\n']";"[""df1['randNumCol'] = random.sample(xrange(50000), len(df1))\n""]";"[""df1['randNumCol'] = random.sample(xrange(50000), len(df1))\n""]";False;"[""import pandas as pd\ndf1 = pd.DataFrame()\ndf = pd.DataFrame()\ndf1['randNumCol'] = random.sample(xrange(50000), len(df1))\n""]";True;0;1;"[""name 'df1' is not defined""]";['NameError'];0;1;"[""name 'df1' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
1030;1030;1030;1030;2.0;2;30328646;;1;16;<python><pandas><group-by><mean>;Python Pandas : group by in group by and average?;27839.0;['cluster  org      time\n   1      a       8\n   1      a       6\n   2      h       34\n   1      c       23\n   2      d       74\n   3      w       6 \ncluster mean(time)\n1       15 ((8+6/2)+23)/2\n2       54   (74+34)/2\n3       6\n'];['cluster  org      time\n   1      a       8\n   1      a       6\n   2      h       34\n   1      c       23\n   2      d       74\n   3      w       6 \n', 'cluster mean(time)\n1       15 ((8+6/2)+23)/2\n2       54   (74+34)/2\n3       6\n'];['cluster  org      time\n   1      a       8\n   1      a       6\n   2      h       34\n   1      c       23\n   2      d       74\n   3      w       6 \n', 'cluster mean(time)\n1       15 ((8+6/2)+23)/2\n2       54   (74+34)/2\n3       6\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'cluster'""]";['KeyError']
1031;1031;1031;1031;3.0;0;30357276;;1;17;<python><pandas>;Pandas - FillNa with another column;11384.0;['Day  Cat1  Cat2\n1    cat   mouse\n2    dog   elephant\n3    cat   giraf\n4    NaN   ant\nDay  Cat1  Cat2\n1    cat   mouse\n2    dog   elephant\n3    cat   giraf\n4    ant   ant\n'];['Day  Cat1  Cat2\n1    cat   mouse\n2    dog   elephant\n3    cat   giraf\n4    NaN   ant\n', 'Day  Cat1  Cat2\n1    cat   mouse\n2    dog   elephant\n3    cat   giraf\n4    ant   ant\n'];['fillna', 'Day  Cat1  Cat2\n1    cat   mouse\n2    dog   elephant\n3    cat   giraf\n4    NaN   ant\n', 'Day  Cat1  Cat2\n1    cat   mouse\n2    dog   elephant\n3    cat   giraf\n4    ant   ant\n'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'Cat1'""]";['KeyError']
1032;1032;1032;1032;2.0;9;30462807;;1;14;<csv><pandas><utf-8>;Encoding Error in Panda read_csv;17680.0;"['import pandas as pd\n\nlocation = r""C:\\Users\\khtad\\Documents\\test.csv""\n\ndf = pd.read_csv(location, header=0, quotechar=\'""\')\n']";"['import pandas as pd\n\nlocation = r""C:\\Users\\khtad\\Documents\\test.csv""\n\ndf = pd.read_csv(location, header=0, quotechar=\'""\')\n']";"['import pandas as pd\n\nlocation = r""C:\\Users\\khtad\\Documents\\test.csv""\n\ndf = pd.read_csv(location, header=0, quotechar=\'""\')\n']";"['import pandas as pd\n\nlocation = r""C:\\Users\\khtad\\Documents\\test.csv""\n\ndf = pd.read_csv(location, header=0, quotechar=\'""\')\n']";"['import pandas as pd\n\nlocation = r""C:\\Users\\khtad\\Documents\\test.csv""\n\ndf = pd.read_csv(location, header=0, quotechar=\'""\')\n']";False;"['import pandas as pd\nimport pandas as pd\n\nlocation = r""C:\\Users\\khtad\\Documents\\test.csv""\n\ndf = pd.read_csv(location, header=0, quotechar=\'""\')\n']";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1033;1033;1033;1033;1.0;6;30519487;;1;19;<python><pandas><anaconda><python-3.4><kaggle>;Pandas error - invalid value encountered;6905.0;"[""import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('/Users/Ben/Documents/Kaggle/Titanic/train.csv')\ntrain\n//anaconda/lib/python3.4/site-packages/pandas/core/format.py:1969: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e8).any()\n//anaconda/lib/python3.4/site-packages/pandas/core/format.py:1970: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) &\n//anaconda/lib/python3.4/site-packages/pandas/core/format.py:1971: RuntimeWarning: invalid value encountered in greater\n  (abs_vals > 0)).any()\n""]";"[""import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('/Users/Ben/Documents/Kaggle/Titanic/train.csv')\ntrain\n"", '//anaconda/lib/python3.4/site-packages/pandas/core/format.py:1969: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e8).any()\n//anaconda/lib/python3.4/site-packages/pandas/core/format.py:1970: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) &\n//anaconda/lib/python3.4/site-packages/pandas/core/format.py:1971: RuntimeWarning: invalid value encountered in greater\n  (abs_vals > 0)).any()\n']";"[""import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('/Users/Ben/Documents/Kaggle/Titanic/train.csv')\ntrain\n"", '//anaconda/lib/python3.4/site-packages/pandas/core/format.py:1969: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e8).any()\n//anaconda/lib/python3.4/site-packages/pandas/core/format.py:1970: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) &\n//anaconda/lib/python3.4/site-packages/pandas/core/format.py:1971: RuntimeWarning: invalid value encountered in greater\n  (abs_vals > 0)).any()\n', 'IPython', 'Spyder', 'NaN', 'float64']";"[""import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('/Users/Ben/Documents/Kaggle/Titanic/train.csv')\ntrain\n""]";"[""import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('/Users/Ben/Documents/Kaggle/Titanic/train.csv')\ntrain\n""]";False;"[""import pandas as pd\nimport pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('/Users/Ben/Documents/Kaggle/Titanic/train.csv')\ntrain\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1034;1034;1034;1034;3.0;0;30522724;;1;30;<python><numpy><pandas>;Take multiple lists into dataframe;38497.0;"[""percentile_list = pd.DataFrame({'lst1Tite' : [lst1],\n 'lst2Tite' : [lst2],\n 'lst3Tite':[lst3]\n  }, columns=['lst1Tite','lst1Tite', 'lst1Tite'])\n""]";"[""percentile_list = pd.DataFrame({'lst1Tite' : [lst1],\n 'lst2Tite' : [lst2],\n 'lst3Tite':[lst3]\n  }, columns=['lst1Tite','lst1Tite', 'lst1Tite'])\n""]";"[""percentile_list = pd.DataFrame({'lst1Tite' : [lst1],\n 'lst2Tite' : [lst2],\n 'lst3Tite':[lst3]\n  }, columns=['lst1Tite','lst1Tite', 'lst1Tite'])\n""]";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'percentile_list' is not defined""]";['NameError'];0;1;"[""name 'percentile_list' is not defined""]";['NameError'];0;1;"[""name 'percentile_list' is not defined""]";['NameError']
1035;1035;1035;1035;6.0;3;30522982;;1;28;<python><pandas><dataset>;List with many dictionaries VS dictionary with few lists?;1050.0;"['users = [\n    {""id"": 0, ""name"": ""Ashley""},\n    {""id"": 1, ""name"": ""Ben""},\n    {""id"": 2, ""name"": ""Conrad""},\n    {""id"": 3, ""name"": ""Doug""},\n    {""id"": 4, ""name"": ""Evin""},\n    {""id"": 5, ""name"": ""Florian""},\n    {""id"": 6, ""name"": ""Gerald""}\n]\nusers2 = {\n    ""id"": [0, 1, 2, 3, 4, 5, 6],\n    ""name"": [""Ashley"", ""Ben"", ""Conrad"", ""Doug"",""Evin"", ""Florian"", ""Gerald""]\n}\nimport pandas as pd\npd_users = pd.DataFrame(users)\npd_users2 = pd.DataFrame(users2)\nprint pd_users == pd_users2\n']";"['users = [\n    {""id"": 0, ""name"": ""Ashley""},\n    {""id"": 1, ""name"": ""Ben""},\n    {""id"": 2, ""name"": ""Conrad""},\n    {""id"": 3, ""name"": ""Doug""},\n    {""id"": 4, ""name"": ""Evin""},\n    {""id"": 5, ""name"": ""Florian""},\n    {""id"": 6, ""name"": ""Gerald""}\n]\n', 'users2 = {\n    ""id"": [0, 1, 2, 3, 4, 5, 6],\n    ""name"": [""Ashley"", ""Ben"", ""Conrad"", ""Doug"",""Evin"", ""Florian"", ""Gerald""]\n}\n', 'import pandas as pd\npd_users = pd.DataFrame(users)\npd_users2 = pd.DataFrame(users2)\nprint pd_users == pd_users2\n']";"['users = [\n    {""id"": 0, ""name"": ""Ashley""},\n    {""id"": 1, ""name"": ""Ben""},\n    {""id"": 2, ""name"": ""Conrad""},\n    {""id"": 3, ""name"": ""Doug""},\n    {""id"": 4, ""name"": ""Evin""},\n    {""id"": 5, ""name"": ""Florian""},\n    {""id"": 6, ""name"": ""Gerald""}\n]\n', 'users2 = {\n    ""id"": [0, 1, 2, 3, 4, 5, 6],\n    ""name"": [""Ashley"", ""Ben"", ""Conrad"", ""Doug"",""Evin"", ""Florian"", ""Gerald""]\n}\n', 'import pandas as pd\npd_users = pd.DataFrame(users)\npd_users2 = pd.DataFrame(users2)\nprint pd_users == pd_users2\n']";['import pandas as pd\npd_users = pd.DataFrame(users)\npd_users2 = pd.DataFrame(users2)\n'];['import pandas as pd\npd_users = pd.DataFrame(users)\npd_users2 = pd.DataFrame(users2)\n'];False;['import pandas as pd\nimport pandas as pd\npd_users = pd.DataFrame(users)\npd_users2 = pd.DataFrame(users2)\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
1036;1036;1036;1036;2.0;2;30530663;;1;24;<python><pandas>;"How to ""select distinct"" across multiple data frame columns in pandas?";25025.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1037;1037;1037;1037;4.0;8;30627968;;1;18;<pandas><join><indexing><timespan><date-range>;Merge pandas dataframes where one value is between two others;3406.0;"[""df = pd.merge(A, B, how='inner', left_on='cusip', right_on='ncusip')\ndf = df[(df['fdate']>=df['namedt']) & (df['fdate']<=df['nameenddt'])]\n""]";"[""df = pd.merge(A, B, how='inner', left_on='cusip', right_on='ncusip')\ndf = df[(df['fdate']>=df['namedt']) & (df['fdate']<=df['nameenddt'])]\n""]";"['A.cusip==B.ncusip', 'A.fdate', 'B.namedt', 'B.nameenddt', ""df = pd.merge(A, B, how='inner', left_on='cusip', right_on='ncusip')\ndf = df[(df['fdate']>=df['namedt']) & (df['fdate']<=df['nameenddt'])]\n""]";"[""df = pd.merge(A, B, how='inner', left_on='cusip', right_on='ncusip')\ndf = df[(df['fdate']>=df['namedt']) & (df['fdate']<=df['nameenddt'])]\n""]";"[""import pandas as pd\ndf = pd.merge(A, B, how='inner', left_on='cusip', right_on='ncusip')\ndf = df[(df['fdate']>=df['namedt']) & (df['fdate']<=df['nameenddt'])]\n""]";True;"[""import pandas as pd\ndf = pd.merge(A, B, how='inner', left_on='cusip', right_on='ncusip')\ndf = df[(df['fdate']>=df['namedt']) & (df['fdate']<=df['nameenddt'])]\n""]";False;0;1;"[""name 'terms' is not defined""]";['NameError'];0;1;"[""name 'terms' is not defined""]";['NameError'];0;1;"[""name 'terms' is not defined""]";['NameError']
1038;1038;1038;1038;2.0;7;30631325;;1;27;<python><mysql><pandas><sqlalchemy><mysql-connector>;Writing to MySQL database with pandas using SQLAlchemy, to_sql;22941.0;"[""import pandas as pd\nimport mysql.connector\nfrom sqlalchemy import create_engine\n\nengine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)\ncnx = engine.raw_connection()\ndata = pd.read_sql('SELECT * FROM sample_table', cnx)\ndata.to_sql(name='sample_table2', con=cnx, if_exists = 'append', index=False)\ndata.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)\n>>AttributeError: 'Engine' object has no attribute 'cursor'\n""]";"[""import pandas as pd\nimport mysql.connector\nfrom sqlalchemy import create_engine\n\nengine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)\ncnx = engine.raw_connection()\ndata = pd.read_sql('SELECT * FROM sample_table', cnx)\ndata.to_sql(name='sample_table2', con=cnx, if_exists = 'append', index=False)\n"", ""data.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)\n>>AttributeError: 'Engine' object has no attribute 'cursor'\n""]";"[""import pandas as pd\nimport mysql.connector\nfrom sqlalchemy import create_engine\n\nengine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)\ncnx = engine.raw_connection()\ndata = pd.read_sql('SELECT * FROM sample_table', cnx)\ndata.to_sql(name='sample_table2', con=cnx, if_exists = 'append', index=False)\n"", ""data.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)\n>>AttributeError: 'Engine' object has no attribute 'cursor'\n""]";"[""import pandas as pd\nimport mysql.connector\nfrom sqlalchemy import create_engine\n\nengine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)\ncnx = engine.raw_connection()\ndata = pd.read_sql('SELECT * FROM sample_table', cnx)\ndata.to_sql(name='sample_table2', con=cnx, if_exists = 'append', index=False)\ndata.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)\n""]";"[""import pandas as pd\nimport mysql.connector\nfrom sqlalchemy import create_engine\n\nengine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)\ncnx = engine.raw_connection()\ndata = pd.read_sql('SELECT * FROM sample_table', cnx)\ndata.to_sql(name='sample_table2', con=cnx, if_exists = 'append', index=False)\ndata.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)\n""]";False;"[""import pandas as pd\nimport pandas as pd\nimport mysql.connector\nfrom sqlalchemy import create_engine\n\nengine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)\ncnx = engine.raw_connection()\ndata = pd.read_sql('SELECT * FROM sample_table', cnx)\ndata.to_sql(name='sample_table2', con=cnx, if_exists = 'append', index=False)\ndata.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)\n""]";False;0;1;"[""No module named 'mysql'""]";['ImportError'];0;1;"[""No module named 'mysql'""]";['ImportError'];0;1;"[""No module named 'mysql'""]";['ImportError']
1039;1039;1039;1039;1.0;1;30651724;;1;11;<python><pandas><msgpack>;Pandas msgpack vs pickle;3100.0;"[""df = pd.DataFrame(np.random.randn(10000, 100))\n\n>>> %timeit df.to_pickle('test.p')\n10 loops, best of 3: 22.4 ms per loop\n\n>>> %timeit df.to_msgpack('test.msg')\n10 loops, best of 3: 36.4 ms per loop\n\n>>> %timeit pd.read_pickle('test.p')\n100 loops, best of 3: 10.5 ms per loop\n\n>>> %timeit pd.read_msgpack('test.msg')\n10 loops, best of 3: 24.6 ms per loop\n""]";"[""df = pd.DataFrame(np.random.randn(10000, 100))\n\n>>> %timeit df.to_pickle('test.p')\n10 loops, best of 3: 22.4 ms per loop\n\n>>> %timeit df.to_msgpack('test.msg')\n10 loops, best of 3: 36.4 ms per loop\n\n>>> %timeit pd.read_pickle('test.p')\n100 loops, best of 3: 10.5 ms per loop\n\n>>> %timeit pd.read_msgpack('test.msg')\n10 loops, best of 3: 24.6 ms per loop\n""]";"['msgpack', 'pickle', ""df = pd.DataFrame(np.random.randn(10000, 100))\n\n>>> %timeit df.to_pickle('test.p')\n10 loops, best of 3: 22.4 ms per loop\n\n>>> %timeit df.to_msgpack('test.msg')\n10 loops, best of 3: 36.4 ms per loop\n\n>>> %timeit pd.read_pickle('test.p')\n100 loops, best of 3: 10.5 ms per loop\n\n>>> %timeit pd.read_msgpack('test.msg')\n10 loops, best of 3: 24.6 ms per loop\n""]";['df = pd.DataFrame(np.random.randn(10000, 100))\n\n\n\n\n'];['import pandas as pd\ndf = pd.DataFrame(np.random.randn(10000, 100))\n\n\n\n\n'];True;['import pandas as pd\ndf = pd.DataFrame(np.random.randn(10000, 100))\n\n\n\n\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1040;1040;1040;1040;1.0;2;30653642;;1;13;<python><pandas><machine-learning><nlp><scikit-learn>;Combining bag of words and other features in one model using sklearn and pandas;2293.0;"['import pandas as pd\n...\n\ndef features(p):\n    terms = vectorizer(p[0])\n    d = {\'feature_1\': p[1], \'feature_2\': p[2]}\n    for t in terms:\n        d[t] = d.get(t, 0) + 1\n    return d\n\nposts = pd.read_csv(\'path/to/csv\')\n\n# Create vectorizer for function to use\nvectorizer = CountVectorizer(binary=True, ngram_range=(1, 2)).build_tokenizer()\ny = posts[""score""].values.astype(np.float32) \nvect = DictVectorizer()\n\n# This is the part I want to fix\ntemp = zip(list(posts.message), list(posts.feature_1), list(posts.feature_2))\ntokenized = map(lambda x: features(x), temp)\nX = vect.fit_transform(tokenized)\nID,message,feature_1,feature_2\n1,\'This is the text\',4,7\n2,\'This is more text\',3,2\n...\n']";"['import pandas as pd\n...\n\ndef features(p):\n    terms = vectorizer(p[0])\n    d = {\'feature_1\': p[1], \'feature_2\': p[2]}\n    for t in terms:\n        d[t] = d.get(t, 0) + 1\n    return d\n\nposts = pd.read_csv(\'path/to/csv\')\n\n# Create vectorizer for function to use\nvectorizer = CountVectorizer(binary=True, ngram_range=(1, 2)).build_tokenizer()\ny = posts[""score""].values.astype(np.float32) \nvect = DictVectorizer()\n\n# This is the part I want to fix\ntemp = zip(list(posts.message), list(posts.feature_1), list(posts.feature_2))\ntokenized = map(lambda x: features(x), temp)\nX = vect.fit_transform(tokenized)\n', ""ID,message,feature_1,feature_2\n1,'This is the text',4,7\n2,'This is more text',3,2\n...\n""]";"['import pandas as pd\n...\n\ndef features(p):\n    terms = vectorizer(p[0])\n    d = {\'feature_1\': p[1], \'feature_2\': p[2]}\n    for t in terms:\n        d[t] = d.get(t, 0) + 1\n    return d\n\nposts = pd.read_csv(\'path/to/csv\')\n\n# Create vectorizer for function to use\nvectorizer = CountVectorizer(binary=True, ngram_range=(1, 2)).build_tokenizer()\ny = posts[""score""].values.astype(np.float32) \nvect = DictVectorizer()\n\n# This is the part I want to fix\ntemp = zip(list(posts.message), list(posts.feature_1), list(posts.feature_2))\ntokenized = map(lambda x: features(x), temp)\nX = vect.fit_transform(tokenized)\n', ""ID,message,feature_1,feature_2\n1,'This is the text',4,7\n2,'This is more text',3,2\n...\n""]";"['import pandas as pd\n...\n\n\nposts = pd.read_csv(\'path/to/csv\')\n\n# Create vectorizer for function to use\nvectorizer = CountVectorizer(binary=True, ngram_range=(1, 2)).build_tokenizer()\ny = posts[""score""].values.astype(np.float32) \nvect = DictVectorizer()\n\n# This is the part I want to fix\ntemp = zip(list(posts.message), list(posts.feature_1), list(posts.feature_2))\ntokenized = map(lambda x: features(x), temp)\nX = vect.fit_transform(tokenized)\nID,message,feature_1,feature_2\n1,\'This is the text\',4,7\n2,\'This is more text\',3,2\n...\n']";"['import pandas as pd\n...\n\n\nposts = pd.read_csv(\'path/to/csv\')\n\n# Create vectorizer for function to use\nvectorizer = CountVectorizer(binary=True, ngram_range=(1, 2)).build_tokenizer()\ny = posts[""score""].values.astype(np.float32) \nvect = DictVectorizer()\n\n# This is the part I want to fix\ntemp = zip(list(posts.message), list(posts.feature_1), list(posts.feature_2))\ntokenized = map(lambda x: features(x), temp)\nX = vect.fit_transform(tokenized)\nID,message,feature_1,feature_2\n1,\'This is the text\',4,7\n2,\'This is more text\',3,2\n...\n']";False;"['import pandas as pd\nimport pandas as pd\n...\n\n\nposts = pd.read_csv(\'path/to/csv\')\n\n# Create vectorizer for function to use\nvectorizer = CountVectorizer(binary=True, ngram_range=(1, 2)).build_tokenizer()\ny = posts[""score""].values.astype(np.float32) \nvect = DictVectorizer()\n\n# This is the part I want to fix\ntemp = zip(list(posts.message), list(posts.feature_1), list(posts.feature_2))\ntokenized = map(lambda x: features(x), temp)\nX = vect.fit_transform(tokenized)\nID,message,feature_1,feature_2\n1,\'This is the text\',4,7\n2,\'This is more text\',3,2\n...\n']";False;0;1;"[""name 'posts' is not defined""]";['NameError'];0;1;"[""name 'posts' is not defined""]";['NameError'];0;1;"[""name 'posts' is not defined""]";['NameError']
1041;1041;1041;1041;2.0;0;30781037;;1;12;<python><pandas>;"""Too many indexers"" with DataFrame.loc";5468.0;"['                           value\nfirst second third fourth       \nA0    B0     C1    D0          2\n                   D1          3\n             C3    D0          6\n                   D1          7\n      B1     C1    D0         10\n                   D1         11\n             C3    D0         14\n                   D1         15\nA1    B0     C1    D0         18\n                   D1         19\n             C3    D0         22\n                   D1         23\n      B1     C1    D0         26\n                   D1         27\n             C3    D0         30\n                   D1         31\nA2    B0     C1    D0         34\n                   D1         35\n             C3    D0         38\n                   D1         39\n      B1     C1    D0         42\n                   D1         43\n             C3    D0         46\n                   D1         47\nA3    B0     C1    D0         50\n                   D1         51\n             C3    D0         54\n                   D1         55\n      B1     C1    D0         58\n                   D1         59\n             C3    D0         62\n                   D1         63\nIn [26]: df.loc[\'A0\', :, \'C1\', :]\nOut[26]: \n                           value\nfirst second third fourth       \nA0    B0     C1    D0          2\n                   D1          3\n      B1     C1    D0         10\n                   D1         11\nIn [28]: df.loc[\'A0\', :, (\'C1\', \'C2\'), \'D1\']\nOut[28]: \n                           value\nfirst second third fourth       \nA0    B0     C1    D1          3\n             C2    D1          5\n      B1     C1    D1         11\n             C2    D1         13\nIn [30]: df.loc[:, :, \'C1\', :]\n---------------------------------------------------------------------------\nIndexingError                             Traceback (most recent call last)\n<ipython-input-30-57b56108d941> in <module>()\n----> 1 df.loc[:, :, \'C1\', :]\n\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in __getitem__(self, key)\n   1176     def __getitem__(self, key):\n   1177         if type(key) is tuple:\n-> 1178             return self._getitem_tuple(key)\n   1179         else:\n   1180             return self._getitem_axis(key, axis=0)\n\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in _getitem_tuple(self, tup)\n    694 \n    695         # no multi-index, so validate all of the indexers\n--> 696         self._has_valid_tuple(tup)\n    697 \n    698         # ugly hack for GH #836\n\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in _has_valid_tuple(self, key)\n    125         for i, k in enumerate(key):\n    126             if i >= self.obj.ndim:\n--> 127                 raise IndexingError(\'Too many indexers\')\n    128             if not self._has_valid_type(k, i):\n    129                 raise ValueError(""Location based indexing can only have [%s] ""\n\nIndexingError: Too many indexers\n']";"['                           value\nfirst second third fourth       \nA0    B0     C1    D0          2\n                   D1          3\n             C3    D0          6\n                   D1          7\n      B1     C1    D0         10\n                   D1         11\n             C3    D0         14\n                   D1         15\nA1    B0     C1    D0         18\n                   D1         19\n             C3    D0         22\n                   D1         23\n      B1     C1    D0         26\n                   D1         27\n             C3    D0         30\n                   D1         31\nA2    B0     C1    D0         34\n                   D1         35\n             C3    D0         38\n                   D1         39\n      B1     C1    D0         42\n                   D1         43\n             C3    D0         46\n                   D1         47\nA3    B0     C1    D0         50\n                   D1         51\n             C3    D0         54\n                   D1         55\n      B1     C1    D0         58\n                   D1         59\n             C3    D0         62\n                   D1         63\n', ""In [26]: df.loc['A0', :, 'C1', :]\nOut[26]: \n                           value\nfirst second third fourth       \nA0    B0     C1    D0          2\n                   D1          3\n      B1     C1    D0         10\n                   D1         11\n"", ""In [28]: df.loc['A0', :, ('C1', 'C2'), 'D1']\nOut[28]: \n                           value\nfirst second third fourth       \nA0    B0     C1    D1          3\n             C2    D1          5\n      B1     C1    D1         11\n             C2    D1         13\n"", 'In [30]: df.loc[:, :, \'C1\', :]\n---------------------------------------------------------------------------\nIndexingError                             Traceback (most recent call last)\n<ipython-input-30-57b56108d941> in <module>()\n----> 1 df.loc[:, :, \'C1\', :]\n\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in __getitem__(self, key)\n   1176     def __getitem__(self, key):\n   1177         if type(key) is tuple:\n-> 1178             return self._getitem_tuple(key)\n   1179         else:\n   1180             return self._getitem_axis(key, axis=0)\n\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in _getitem_tuple(self, tup)\n    694 \n    695         # no multi-index, so validate all of the indexers\n--> 696         self._has_valid_tuple(tup)\n    697 \n    698         # ugly hack for GH #836\n\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in _has_valid_tuple(self, key)\n    125         for i, k in enumerate(key):\n    126             if i >= self.obj.ndim:\n--> 127                 raise IndexingError(\'Too many indexers\')\n    128             if not self._has_valid_type(k, i):\n    129                 raise ValueError(""Location based indexing can only have [%s] ""\n\nIndexingError: Too many indexers\n']";"['loc', 'DataFrame', 'MultiIndex', 'DataFrame', '                           value\nfirst second third fourth       \nA0    B0     C1    D0          2\n                   D1          3\n             C3    D0          6\n                   D1          7\n      B1     C1    D0         10\n                   D1         11\n             C3    D0         14\n                   D1         15\nA1    B0     C1    D0         18\n                   D1         19\n             C3    D0         22\n                   D1         23\n      B1     C1    D0         26\n                   D1         27\n             C3    D0         30\n                   D1         31\nA2    B0     C1    D0         34\n                   D1         35\n             C3    D0         38\n                   D1         39\n      B1     C1    D0         42\n                   D1         43\n             C3    D0         46\n                   D1         47\nA3    B0     C1    D0         50\n                   D1         51\n             C3    D0         54\n                   D1         55\n      B1     C1    D0         58\n                   D1         59\n             C3    D0         62\n                   D1         63\n', 'A0', 'C1', ""In [26]: df.loc['A0', :, 'C1', :]\nOut[26]: \n                           value\nfirst second third fourth       \nA0    B0     C1    D0          2\n                   D1          3\n      B1     C1    D0         10\n                   D1         11\n"", ""In [28]: df.loc['A0', :, ('C1', 'C2'), 'D1']\nOut[28]: \n                           value\nfirst second third fourth       \nA0    B0     C1    D1          3\n             C2    D1          5\n      B1     C1    D1         11\n             C2    D1         13\n"", 'In [30]: df.loc[:, :, \'C1\', :]\n---------------------------------------------------------------------------\nIndexingError                             Traceback (most recent call last)\n<ipython-input-30-57b56108d941> in <module>()\n----> 1 df.loc[:, :, \'C1\', :]\n\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in __getitem__(self, key)\n   1176     def __getitem__(self, key):\n   1177         if type(key) is tuple:\n-> 1178             return self._getitem_tuple(key)\n   1179         else:\n   1180             return self._getitem_axis(key, axis=0)\n\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in _getitem_tuple(self, tup)\n    694 \n    695         # no multi-index, so validate all of the indexers\n--> 696         self._has_valid_tuple(tup)\n    697 \n    698         # ugly hack for GH #836\n\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in _has_valid_tuple(self, key)\n    125         for i, k in enumerate(key):\n    126             if i >= self.obj.ndim:\n--> 127                 raise IndexingError(\'Too many indexers\')\n    128             if not self._has_valid_type(k, i):\n    129                 raise ValueError(""Location based indexing can only have [%s] ""\n\nIndexingError: Too many indexers\n', ""df.xs('C1', level='third')"", '.loc']";"[""df.loc['A0', :, 'C1', :]\ndf.loc['A0', :, ('C1', 'C2'), 'D1']\n""]";"[""df.loc['A0', :, 'C1', :]\ndf.loc['A0', :, ('C1', 'C2'), 'D1']\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf.loc['A0', :, 'C1', :]\ndf.loc['A0', :, ('C1', 'C2'), 'D1']\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1042;1042;1042;1042;2.0;2;30787901;;1;15;<python><pandas><dataframe>;How to get a value from a Pandas DataFrame and not the index and object type;21224.0;"[""\nLetter    Number\nA          1\nB          2\nC          3\nD          4\nimport pandas as pd\n\nletters=pd.Series(('A', 'B', 'C', 'D'))\nnumbers=pd.Series((1, 2, 3, 4))\nkeys=('Letters', 'Numbers')\ndf=pd.concat((letters, numbers), axis=1, keys=keys)\ndf[df.Letters=='C'].Letters\n\n2    C\nName: Letters, dtype: object\n""]";"['\nLetter    Number\nA          1\nB          2\nC          3\nD          4\n', ""import pandas as pd\n\nletters=pd.Series(('A', 'B', 'C', 'D'))\nnumbers=pd.Series((1, 2, 3, 4))\nkeys=('Letters', 'Numbers')\ndf=pd.concat((letters, numbers), axis=1, keys=keys)\n"", ""df[df.Letters=='C'].Letters\n"", '\n2    C\nName: Letters, dtype: object\n']";"[""import pandas as pd\n\nletters=pd.Series(('A', 'B', 'C', 'D'))\nnumbers=pd.Series((1, 2, 3, 4))\nkeys=('Letters', 'Numbers')\ndf=pd.concat((letters, numbers), axis=1, keys=keys)\n"", ""df[df.Letters=='C'].Letters\n""]";"[""\nimport pandas as pd\n\nletters=pd.Series(('A', 'B', 'C', 'D'))\nnumbers=pd.Series((1, 2, 3, 4))\nkeys=('Letters', 'Numbers')\ndf=pd.concat((letters, numbers), axis=1, keys=keys)\ndf[df.Letters=='C'].Letters\n\n""]";"[""\nimport pandas as pd\n\nletters=pd.Series(('A', 'B', 'C', 'D'))\nnumbers=pd.Series((1, 2, 3, 4))\nkeys=('Letters', 'Numbers')\ndf=pd.concat((letters, numbers), axis=1, keys=keys)\ndf[df.Letters=='C'].Letters\n\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\n\nimport pandas as pd\n\nletters=pd.Series(('A', 'B', 'C', 'D'))\nnumbers=pd.Series((1, 2, 3, 4))\nkeys=('Letters', 'Numbers')\ndf=pd.concat((letters, numbers), axis=1, keys=keys)\ndf[df.Letters=='C'].Letters\n\n""]";True;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'DataFrame' object has no attribute 'Letters'"", ""'DataFrame' object has no attribute 'Letters'""]";['AttributeError', 'AttributeError']
1043;1043;1043;1043;3.0;0;30808430;;1;15;<python><python-2.7><pandas>;How to select columns from dataframe by regex;8035.0;['   a    b    c    d1   d2   d3 \n   10   14   12   44  45    78\n'];['   a    b    c    d1   d2   d3 \n   10   14   12   44  45    78\n'];['   a    b    c    d1   d2   d3 \n   10   14   12   44  45    78\n'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'np' is not defined"", ""'select' is deprecated and will be removed in a future release. You can use .loc[labels.map(crit)] as a replacement""]";['NameError', 'FutureWarning'];0;2;"[""name 'np' is not defined"", ""'select' is deprecated and will be removed in a future release. You can use .loc[labels.map(crit)] as a replacement""]";['NameError', 'FutureWarning'];0;2;"[""name 'np' is not defined"", ""'select' is deprecated and will be removed in a future release. You can use .loc[labels.map(crit)] as a replacement""]";['NameError', 'FutureWarning']
1044;1044;1044;1044;2.0;3;30922213;;1;12;<r><pandas><rpy2>;Minimal example of rpy2 regression using pandas data frame;3664.0;"[""x <- c(1,2,3,4,5)\ny <- c(2,1,3,5,4)\nM <- lm(y~x)\nsummary(M)$coefficients\n            Estimate Std. Error  t value  Pr(>|t|)\n(Intercept)      0.6  1.1489125 0.522233 0.6376181\nx                0.8  0.3464102 2.309401 0.1040880\nimport pandas\nimport pandas.rpy.common as common\nfrom rpy2 import robjects\nfrom rpy2.robjects.packages import importr\n\nbase = importr('base')\nstats = importr('stats')\n\ndataframe = pandas.DataFrame({'x': [1,2,3,4,5], \n                              'y': [2,1,3,5,4]})\n\nrobjects.globalenv['dataframe']\\\n   = common.convert_to_r_dataframe(dataframe) \n\nM = stats.lm('y~x', data=base.as_symbol('dataframe'))\n\nprint(base.summary(M).rx2('coefficients'))\n\n            Estimate Std. Error  t value  Pr(>|t|)\n(Intercept)      0.6  1.1489125 0.522233 0.6376181\nx                0.8  0.3464102 2.309401 0.1040880\nNotImplementedError: Conversion 'py2ri' not defined for objects of type '<class 'pandas.core.series.Series'>'\n""]";"['x <- c(1,2,3,4,5)\ny <- c(2,1,3,5,4)\nM <- lm(y~x)\nsummary(M)$coefficients\n            Estimate Std. Error  t value  Pr(>|t|)\n(Intercept)      0.6  1.1489125 0.522233 0.6376181\nx                0.8  0.3464102 2.309401 0.1040880\n', ""import pandas\nimport pandas.rpy.common as common\nfrom rpy2 import robjects\nfrom rpy2.robjects.packages import importr\n\nbase = importr('base')\nstats = importr('stats')\n\ndataframe = pandas.DataFrame({'x': [1,2,3,4,5], \n                              'y': [2,1,3,5,4]})\n\nrobjects.globalenv['dataframe']\\\n   = common.convert_to_r_dataframe(dataframe) \n\nM = stats.lm('y~x', data=base.as_symbol('dataframe'))\n\nprint(base.summary(M).rx2('coefficients'))\n\n            Estimate Std. Error  t value  Pr(>|t|)\n(Intercept)      0.6  1.1489125 0.522233 0.6376181\nx                0.8  0.3464102 2.309401 0.1040880\n"", ""NotImplementedError: Conversion 'py2ri' not defined for objects of type '<class 'pandas.core.series.Series'>'\n""]";"['x <- c(1,2,3,4,5)\ny <- c(2,1,3,5,4)\nM <- lm(y~x)\nsummary(M)$coefficients\n            Estimate Std. Error  t value  Pr(>|t|)\n(Intercept)      0.6  1.1489125 0.522233 0.6376181\nx                0.8  0.3464102 2.309401 0.1040880\n', ""import pandas\nimport pandas.rpy.common as common\nfrom rpy2 import robjects\nfrom rpy2.robjects.packages import importr\n\nbase = importr('base')\nstats = importr('stats')\n\ndataframe = pandas.DataFrame({'x': [1,2,3,4,5], \n                              'y': [2,1,3,5,4]})\n\nrobjects.globalenv['dataframe']\\\n   = common.convert_to_r_dataframe(dataframe) \n\nM = stats.lm('y~x', data=base.as_symbol('dataframe'))\n\nprint(base.summary(M).rx2('coefficients'))\n\n            Estimate Std. Error  t value  Pr(>|t|)\n(Intercept)      0.6  1.1489125 0.522233 0.6376181\nx                0.8  0.3464102 2.309401 0.1040880\n"", 'pandas.rpy.common', 'pandas2ri.py2ri(dataframe)', ""NotImplementedError: Conversion 'py2ri' not defined for objects of type '<class 'pandas.core.series.Series'>'\n""]";"[""x <- c(1,2,3,4,5)\ny <- c(2,1,3,5,4)\nimport pandas\nimport pandas.rpy.common as common\nfrom rpy2 import robjects\nfrom rpy2.robjects.packages import importr\n\nbase = importr('base')\nstats = importr('stats')\n\n\nrobjects.globalenv['dataframe']\\\n\nM = stats.lm('y~x', data=base.as_symbol('dataframe'))\n\nprint(base.summary(M).rx2('coefficients'))\n\n""]";"[""x <- c(1,2,3,4,5)\ny <- c(2,1,3,5,4)\nimport pandas\nimport pandas.rpy.common as common\nfrom rpy2 import robjects\nfrom rpy2.robjects.packages import importr\n\nbase = importr('base')\nstats = importr('stats')\n\n\nrobjects.globalenv['dataframe']\\\n\nM = stats.lm('y~x', data=base.as_symbol('dataframe'))\n\nprint(base.summary(M).rx2('coefficients'))\n\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nx <- c(1,2,3,4,5)\ny <- c(2,1,3,5,4)\nimport pandas\nimport pandas.rpy.common as common\nfrom rpy2 import robjects\nfrom rpy2.robjects.packages import importr\n\nbase = importr('base')\nstats = importr('stats')\n\n\nrobjects.globalenv['dataframe']\\\n\nM = stats.lm('y~x', data=base.as_symbol('dataframe'))\n\nprint(base.summary(M).rx2('coefficients'))\n\n""]";True;0;2;"[""No module named 'rpy2'"", ""name 'R' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'rpy2'"", ""name 'R' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'rpy2'"", ""name 'R' is not defined""]";['ImportError', 'NameError']
1045;1045;1045;1045;3.0;4;30925079;;1;18;<pandas>;Group by index + column in pandas;18605.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'pd' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'arrays' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'arrays' is not defined""]";['Sucess', 'NameError']
1046;1046;1046;1046;3.0;0;30926670;;1;26;<python><pandas>;Pandas: Add multiple empty columns to DataFrame;14668.0;"['df[""B""] = None\ndf[""C""] = None\ndf[""D""] = None\ndf[[""B"", ""C"", ""D""]] = None\n\nKeyError: ""[\'B\' \'C\' \'D\'] not in index""\n']";"['df[""B""] = None\ndf[""C""] = None\ndf[""D""] = None\n', 'df[[""B"", ""C"", ""D""]] = None\n\nKeyError: ""[\'B\' \'C\' \'D\'] not in index""\n']";"['df[""B""] = None\ndf[""C""] = None\ndf[""D""] = None\n', 'df[[""B"", ""C"", ""D""]] = None\n\nKeyError: ""[\'B\' \'C\' \'D\'] not in index""\n']";"['df[""B""] = None\ndf[""C""] = None\ndf[""D""] = None\ndf[[""B"", ""C"", ""D""]] = None\n\n']";"['df[""B""] = None\ndf[""C""] = None\ndf[""D""] = None\ndf[[""B"", ""C"", ""D""]] = None\n\n']";False;"['import pandas as pd\ndf = pd.DataFrame()\ndf[""B""] = None\ndf[""C""] = None\ndf[""D""] = None\ndf[[""B"", ""C"", ""D""]] = None\n\n']";True;0;2;"[""name 'pd' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""name 'np' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'np' is not defined""]";['Sucess', 'NameError']
1047;1047;1047;1047;3.0;0;31029560;;1;25;<python><pandas>;Plotting categorical data with pandas and matplotlib;20647.0;"[""     colour  direction\n1    red     up\n2    blue    up\n3    green   down\n4    red     left\n5    red     right\n6    yellow  down\n7    blue    down\ndf.plot(kind='hist')\n""]";"['     colour  direction\n1    red     up\n2    blue    up\n3    green   down\n4    red     left\n5    red     right\n6    yellow  down\n7    blue    down\n', ""df.plot(kind='hist')\n""]";"['     colour  direction\n1    red     up\n2    blue    up\n3    green   down\n4    red     left\n5    red     right\n6    yellow  down\n7    blue    down\n', ""df.plot(kind='hist')\n""]";"[""df.plot(kind='hist')\n""]";"[""df.plot(kind='hist')\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf.plot(kind='hist')\n""]";True;0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""No module named 'statsmodels'""]";['NameError', 'NameError', 'ImportError'];0;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", ""No module named 'statsmodels'""]";['NameError', 'NameError', 'ImportError'];0;3;"[""'colour'"", ""'colour'"", ""No module named 'statsmodels'""]";['KeyError', 'KeyError', 'ImportError']
1048;1048;1048;1048;1.0;2;31037298;;1;18;<python><pandas>;pandas get column average/mean;49735.0;"["">>> allDF \n         ID           birthyear  weight\n0        619040       1962       0.1231231\n1        600161       1963       0.981742\n2      25602033       1963       1.3123124     \n3        624870       1987       0.94212\nallDF[['weight']].mean(axis=1)\nallDF.groupby('weight').mean()\n""]";"['>>> allDF \n         ID           birthyear  weight\n0        619040       1962       0.1231231\n1        600161       1963       0.981742\n2      25602033       1963       1.3123124     \n3        624870       1987       0.94212\n', ""allDF[['weight']].mean(axis=1)\n"", ""allDF.groupby('weight').mean()\n""]";"['weight', '>>> allDF \n         ID           birthyear  weight\n0        619040       1962       0.1231231\n1        600161       1963       0.981742\n2      25602033       1963       1.3123124     \n3        624870       1987       0.94212\n', ""allDF[['weight']].mean(axis=1)\n"", ""allDF.groupby('weight').mean()\n""]";"[""allDF[['weight']].mean(axis=1)\nallDF.groupby('weight').mean()\n""]";"[""allDF[['weight']].mean(axis=1)\nallDF.groupby('weight').mean()\n""]";False;"[""import pandas as pd\nallDF[['weight']].mean(axis=1)\nallDF.groupby('weight').mean()\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'weight'""]";['KeyError']
1049;1049;1049;1049;3.0;0;31247198;;1;17;<python><file><pandas>;Python, Pandas : write content of DataFrame into text File;34026.0;"[""        X    Y  Z    Value \n0      18   55  1      70   \n1      18   55  2      67 \n2      18   57  2      75     \n3      18   58  1      35  \n4      19   54  2      70   \n18 55 1 70   \n18 55 2 67 \n18 57 2 75     \n18 58 1 35  \n19 54 2 70 \nf = open(writePath, 'a')\nf.writelines(['\\n', str(data['X']), ' ', str(data['Y']), ' ', str(data['Z']), ' ', str(data['Value'])])\nf.close()\n""]";"['        X    Y  Z    Value \n0      18   55  1      70   \n1      18   55  2      67 \n2      18   57  2      75     \n3      18   58  1      35  \n4      19   54  2      70   \n', '18 55 1 70   \n18 55 2 67 \n18 57 2 75     \n18 58 1 35  \n19 54 2 70 \n', ""f = open(writePath, 'a')\nf.writelines(['\\n', str(data['X']), ' ', str(data['Y']), ' ', str(data['Z']), ' ', str(data['Value'])])\nf.close()\n""]";"['        X    Y  Z    Value \n0      18   55  1      70   \n1      18   55  2      67 \n2      18   57  2      75     \n3      18   58  1      35  \n4      19   54  2      70   \n', '18 55 1 70   \n18 55 2 67 \n18 57 2 75     \n18 58 1 35  \n19 54 2 70 \n', ""f = open(writePath, 'a')\nf.writelines(['\\n', str(data['X']), ' ', str(data['Y']), ' ', str(data['Z']), ' ', str(data['Value'])])\nf.close()\n""]";"[""f = open(writePath, 'a')\nf.writelines(['\\n', str(data['X']), ' ', str(data['Y']), ' ', str(data['Z']), ' ', str(data['Value'])])\nf.close()\n""]";"[""f = open(writePath, 'a')\nf.writelines(['\\n', str(data['X']), ' ', str(data['Y']), ' ', str(data['Z']), ' ', str(data['Value'])])\nf.close()\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nf = open(writePath, 'a')\nf.writelines(['\\n', str(data['X']), ' ', str(data['Y']), ' ', str(data['Z']), ' ', str(data['Value'])])\nf.close()\n""]";True;1;2;"[""name 'np' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'np' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'np' is not defined"", 'Sucess']";['NameError', 'Sucess']
1050;1050;1050;1050;2.0;1;31329627;;1;14;<python><pandas><matplotlib><pip>;Why don't I have xlrd?;19895.0;"['import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.ExcelFile(""Obes-phys-acti-diet-eng-2014-tab.xls"")\nprint (data.sheet_names)\ndhcp-169-233-172-97:Obesity juliushamilton$ python3 ob.py\nTraceback (most recent call last):\n  File ""ob.py"", line 4, in <module>\n    data = pd.ExcelFile(""Obes-phys-acti-diet-eng-2014-tab.xls"")\n  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/io/excel.py"", line 169, in __init__\n    import xlrd  # throw an ImportError if we need to\nImportError: No module named \'xlrd\'\n']";"['import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.ExcelFile(""Obes-phys-acti-diet-eng-2014-tab.xls"")\nprint (data.sheet_names)\n', 'dhcp-169-233-172-97:Obesity juliushamilton$ python3 ob.py\nTraceback (most recent call last):\n  File ""ob.py"", line 4, in <module>\n    data = pd.ExcelFile(""Obes-phys-acti-diet-eng-2014-tab.xls"")\n  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/io/excel.py"", line 169, in __init__\n    import xlrd  # throw an ImportError if we need to\nImportError: No module named \'xlrd\'\n']";"['pandas', 'matplotlib', 'pip3 install', 'import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.ExcelFile(""Obes-phys-acti-diet-eng-2014-tab.xls"")\nprint (data.sheet_names)\n', 'dhcp-169-233-172-97:Obesity juliushamilton$ python3 ob.py\nTraceback (most recent call last):\n  File ""ob.py"", line 4, in <module>\n    data = pd.ExcelFile(""Obes-phys-acti-diet-eng-2014-tab.xls"")\n  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/io/excel.py"", line 169, in __init__\n    import xlrd  # throw an ImportError if we need to\nImportError: No module named \'xlrd\'\n', 'xlrd']";"['import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.ExcelFile(""Obes-phys-acti-diet-eng-2014-tab.xls"")\nprint (data.sheet_names)\n']";"['import pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.ExcelFile(""Obes-phys-acti-diet-eng-2014-tab.xls"")\nprint (data.sheet_names)\n']";False;"['import pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.ExcelFile(""Obes-phys-acti-diet-eng-2014-tab.xls"")\nprint (data.sheet_names)\n']";False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
1051;1051;1051;1051;2.0;3;31331358;;1;11;<python><pandas><export-to-csv><python-unicode>;Unicode Encode Error when writing pandas df to csv;9938.0;"['df.to_csv(""path"",header=True,index=False)\nUnicodeEncodeError: \'ascii\' codec can\'t encode character u\'\\xc7\' in position 20: ordinal not in range(128)\n']";"['df.to_csv(""path"",header=True,index=False)\n', ""UnicodeEncodeError: 'ascii' codec can't encode character u'\\xc7' in position 20: ordinal not in range(128)\n""]";"['df.to_csv(""path"",header=True,index=False)\n', ""UnicodeEncodeError: 'ascii' codec can't encode character u'\\xc7' in position 20: ordinal not in range(128)\n""]";"['df.to_csv(""path"",header=True,index=False)\n']";"['df.to_csv(""path"",header=True,index=False)\n']";False;"['import pandas as pd\ndf = pd.DataFrame()\ndf.to_csv(""path"",header=True,index=False)\n']";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
1052;1052;1052;1052;4.0;0;31357611;;1;21;<python><pandas><matplotlib><plot>;Format y axis as percent;18791.0;"[""df['myvar'].plot(kind='bar')\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.ticker as mtick\n\ndata = [8,12,15,17,18,18.5]\nperc = np.linspace(0,100,len(data))\n\nfig = plt.figure(1, (7,4))\nax = fig.add_subplot(1,1,1)\n\nax.plot(perc, data)\n\nfmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\nxticks = mtick.FormatStrFormatter(fmt)\nax.xaxis.set_major_formatter(xticks)\n\nplt.show()\n""]";"[""df['myvar'].plot(kind='bar')\n"", ""import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.ticker as mtick\n\ndata = [8,12,15,17,18,18.5]\nperc = np.linspace(0,100,len(data))\n\nfig = plt.figure(1, (7,4))\nax = fig.add_subplot(1,1,1)\n\nax.plot(perc, data)\n\nfmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\nxticks = mtick.FormatStrFormatter(fmt)\nax.xaxis.set_major_formatter(xticks)\n\nplt.show()\n""]";"[""df['myvar'].plot(kind='bar')\n"", ""import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.ticker as mtick\n\ndata = [8,12,15,17,18,18.5]\nperc = np.linspace(0,100,len(data))\n\nfig = plt.figure(1, (7,4))\nax = fig.add_subplot(1,1,1)\n\nax.plot(perc, data)\n\nfmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\nxticks = mtick.FormatStrFormatter(fmt)\nax.xaxis.set_major_formatter(xticks)\n\nplt.show()\n""]";"[""df['myvar'].plot(kind='bar')\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.ticker as mtick\n\ndata = [8,12,15,17,18,18.5]\nperc = np.linspace(0,100,len(data))\n\nfig = plt.figure(1, (7,4))\nax = fig.add_subplot(1,1,1)\n\nax.plot(perc, data)\n\nfmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\nxticks = mtick.FormatStrFormatter(fmt)\nax.xaxis.set_major_formatter(xticks)\n\nplt.show()\n""]";"[""df['myvar'].plot(kind='bar')\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.ticker as mtick\n\ndata = [8,12,15,17,18,18.5]\nperc = np.linspace(0,100,len(data))\n\nfig = plt.figure(1, (7,4))\nax = fig.add_subplot(1,1,1)\n\nax.plot(perc, data)\n\nfmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\nxticks = mtick.FormatStrFormatter(fmt)\nax.xaxis.set_major_formatter(xticks)\n\nplt.show()\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf['myvar'].plot(kind='bar')\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.ticker as mtick\n\ndata = [8,12,15,17,18,18.5]\nperc = np.linspace(0,100,len(data))\n\nfig = plt.figure(1, (7,4))\nax = fig.add_subplot(1,1,1)\n\nax.plot(perc, data)\n\nfmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\nxticks = mtick.FormatStrFormatter(fmt)\nax.xaxis.set_major_formatter(xticks)\n\nplt.show()\n""]";True;0;3;"[""name '_converter' is not defined"", ""No module named 'matplotlib'"", ""No module named 'matplotlib'""]";['NameError', 'ImportError', 'ImportError'];0;3;"[""name '_converter' is not defined"", ""No module named 'matplotlib'"", ""No module named 'matplotlib'""]";['NameError', 'ImportError', 'ImportError'];0;3;"[""name '_converter' is not defined"", ""No module named 'matplotlib'"", ""No module named 'matplotlib'""]";['NameError', 'ImportError', 'ImportError']
1053;1053;1053;1053;2.0;3;31361721;;1;24;<python><pandas><parallel-processing><dask>;python dask DataFrame, support for (trivially parallelizable) row apply?;4089.0;['ts.apply(func) # for pandas series\ndf.apply(func, axis = 1) # for pandas DF row apply\nddf.assign(A=lambda df: df.apply(func, axis=1)).compute() # dask DataFrame\ndf.apply(func, axis = 1) # for pandas DF row apply\nimport dask.dataframe as dd\ns = pd.Series([10000]*120)\nds = dd.from_pandas(s, npartitions = 3)\n\ndef slow_func(k):\n    A = np.random.normal(size = k) # k = 10000\n    s = 0\n    for a in A:\n        if a > 0:\n            s += 1\n        else:\n            s -= 1\n    return s\n\ns.apply(slow_func) # 0.43 sec\nds.map(slow_func).compute() # 2.04 sec\n'];['ts.apply(func) # for pandas series\ndf.apply(func, axis = 1) # for pandas DF row apply\n', 'ddf.assign(A=lambda df: df.apply(func, axis=1)).compute() # dask DataFrame\n', 'df.apply(func, axis = 1) # for pandas DF row apply\n', 'import dask.dataframe as dd\ns = pd.Series([10000]*120)\nds = dd.from_pandas(s, npartitions = 3)\n\ndef slow_func(k):\n    A = np.random.normal(size = k) # k = 10000\n    s = 0\n    for a in A:\n        if a > 0:\n            s += 1\n        else:\n            s -= 1\n    return s\n\ns.apply(slow_func) # 0.43 sec\nds.map(slow_func).compute() # 2.04 sec\n'];['ts.apply(func) # for pandas series\ndf.apply(func, axis = 1) # for pandas DF row apply\n', 'ddf.assign(A=lambda df: df.apply(func, axis=1)).compute() # dask DataFrame\n', 'df.apply(func, axis = 1) # for pandas DF row apply\n', 'import dask.dataframe as dd\ns = pd.Series([10000]*120)\nds = dd.from_pandas(s, npartitions = 3)\n\ndef slow_func(k):\n    A = np.random.normal(size = k) # k = 10000\n    s = 0\n    for a in A:\n        if a > 0:\n            s += 1\n        else:\n            s -= 1\n    return s\n\ns.apply(slow_func) # 0.43 sec\nds.map(slow_func).compute() # 2.04 sec\n'];['ts.apply(func) # for pandas series\ndf.apply(func, axis = 1) # for pandas DF row apply\nddf.assign(A=lambda df: df.apply(func, axis=1)).compute() # dask DataFrame\ndf.apply(func, axis = 1) # for pandas DF row apply\nimport dask.dataframe as dd\ns = pd.Series([10000]*120)\nds = dd.from_pandas(s, npartitions = 3)\n\n\ns.apply(slow_func) # 0.43 sec\nds.map(slow_func).compute() # 2.04 sec\n'];['from pandas import DataFrame\nimport pandas as pd\nts.apply(func) # for pandas series\ndf.apply(func, axis = 1) # for pandas DF row apply\nddf.assign(A=lambda df: df.apply(func, axis=1)).compute() # dask DataFrame\ndf.apply(func, axis = 1) # for pandas DF row apply\nimport dask.dataframe as dd\ns = pd.Series([10000]*120)\nds = dd.from_pandas(s, npartitions = 3)\n\n\ns.apply(slow_func) # 0.43 sec\nds.map(slow_func).compute() # 2.04 sec\n'];True;['import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\nts.apply(func) # for pandas series\ndf.apply(func, axis = 1) # for pandas DF row apply\nddf.assign(A=lambda df: df.apply(func, axis=1)).compute() # dask DataFrame\ndf.apply(func, axis = 1) # for pandas DF row apply\nimport dask.dataframe as dd\ns = pd.Series([10000]*120)\nds = dd.from_pandas(s, npartitions = 3)\n\n\ns.apply(slow_func) # 0.43 sec\nds.map(slow_func).compute() # 2.04 sec\n'];True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError']
1054;1054;1054;1054;3.0;2;31496628;;1;16;<python-3.x><pandas><tooltip><bokeh><timeserieschart>;In Bokeh, how do I add tooltips to a Timeseries chart (hover tool)?;8590.0;"['    import pandas as pd\n    import numpy as np\n    from bokeh.charts import TimeSeries\n    from bokeh.models import HoverTool\n    from bokeh.plotting import show\n\n    toy_df = pd.DataFrame(data=np.random.rand(5,3), columns = (\'a\', \'b\' ,\'c\'), index = pd.DatetimeIndex(start=\'01-01-2015\',periods=5, freq=\'d\'))   \n\n    p = TimeSeries(toy_df, tools=\'hover\')  \n\n    hover = p.select(dict(type=HoverTool))\n    hover.tooltips = [\n        (""Series"", ""@columns""),\n        (""Date"", ""$x""),\n        (""Value"", ""$y""),\n        ]\n\n    show(p)\n']";"['    import pandas as pd\n    import numpy as np\n    from bokeh.charts import TimeSeries\n    from bokeh.models import HoverTool\n    from bokeh.plotting import show\n\n    toy_df = pd.DataFrame(data=np.random.rand(5,3), columns = (\'a\', \'b\' ,\'c\'), index = pd.DatetimeIndex(start=\'01-01-2015\',periods=5, freq=\'d\'))   \n\n    p = TimeSeries(toy_df, tools=\'hover\')  \n\n    hover = p.select(dict(type=HoverTool))\n    hover.tooltips = [\n        (""Series"", ""@columns""),\n        (""Date"", ""$x""),\n        (""Value"", ""$y""),\n        ]\n\n    show(p)\n']";"['    import pandas as pd\n    import numpy as np\n    from bokeh.charts import TimeSeries\n    from bokeh.models import HoverTool\n    from bokeh.plotting import show\n\n    toy_df = pd.DataFrame(data=np.random.rand(5,3), columns = (\'a\', \'b\' ,\'c\'), index = pd.DatetimeIndex(start=\'01-01-2015\',periods=5, freq=\'d\'))   \n\n    p = TimeSeries(toy_df, tools=\'hover\')  \n\n    hover = p.select(dict(type=HoverTool))\n    hover.tooltips = [\n        (""Series"", ""@columns""),\n        (""Date"", ""$x""),\n        (""Value"", ""$y""),\n        ]\n\n    show(p)\n']";['\n\n\n\n'];['\n\n\n\n'];False;['import pandas as pd\n\n\n\n\n'];False;0;1;"[""No module named 'bokeh'""]";['ImportError'];0;1;"[""No module named 'bokeh'""]";['ImportError'];0;1;"[""No module named 'bokeh'""]";['ImportError']
1055;1055;1055;1055;2.0;5;31502958;;1;15;<numpy><pandas>;Drawing a bootstrap sample from a pandas.DataFrame;3432.0;"[""import pandas\nimport numpy as np\n# Generate some data\nn = 5000\nvalues = np.random.uniform(size=(n, 5))\n# Construct a pandas.DataFrame\ncolumns = ['a', 'b', 'c', 'd', 'e']\ndf = pandas.DataFrame(values, columns=columns)\n# Bootstrap\n%timeit df.iloc[np.random.randint(n, size=n)]\n# Out: 1000 loops, best of 3: 1.46 ms per loop\n%timeit values[np.random.randint(n, size=n)]\n# Out: 10000 loops, best of 3: 159 µs per loop\n%timeit pandas.DataFrame(df.values[np.random.randint(n, size=n)], columns=columns)\n# Out: 1000 loops, best of 3: 302 µs per loop\n%timeit df.sample(n, replace=True)\n# Out: 100 loops, best of 3: 5.14 ms per loop\n%timeit df.merge(pandas.DataFrame(index=np.random.randint(n, size=n)), left_index=True, right_index=True, how='right')\n# Out: 1000 loops, best of 3: 1.23 ms per loop\n""]";"[""import pandas\nimport numpy as np\n# Generate some data\nn = 5000\nvalues = np.random.uniform(size=(n, 5))\n# Construct a pandas.DataFrame\ncolumns = ['a', 'b', 'c', 'd', 'e']\ndf = pandas.DataFrame(values, columns=columns)\n# Bootstrap\n%timeit df.iloc[np.random.randint(n, size=n)]\n# Out: 1000 loops, best of 3: 1.46 ms per loop\n"", '%timeit values[np.random.randint(n, size=n)]\n# Out: 10000 loops, best of 3: 159 µs per loop\n', '%timeit pandas.DataFrame(df.values[np.random.randint(n, size=n)], columns=columns)\n# Out: 1000 loops, best of 3: 302 µs per loop\n', '%timeit df.sample(n, replace=True)\n# Out: 100 loops, best of 3: 5.14 ms per loop\n', ""%timeit df.merge(pandas.DataFrame(index=np.random.randint(n, size=n)), left_index=True, right_index=True, how='right')\n# Out: 1000 loops, best of 3: 1.23 ms per loop\n""]";"['pandas.DataFrame', 'iloc', ""import pandas\nimport numpy as np\n# Generate some data\nn = 5000\nvalues = np.random.uniform(size=(n, 5))\n# Construct a pandas.DataFrame\ncolumns = ['a', 'b', 'c', 'd', 'e']\ndf = pandas.DataFrame(values, columns=columns)\n# Bootstrap\n%timeit df.iloc[np.random.randint(n, size=n)]\n# Out: 1000 loops, best of 3: 1.46 ms per loop\n"", 'numpy', '%timeit values[np.random.randint(n, size=n)]\n# Out: 10000 loops, best of 3: 159 µs per loop\n', 'numpy', 'pandas.DataFrame', '%timeit pandas.DataFrame(df.values[np.random.randint(n, size=n)], columns=columns)\n# Out: 1000 loops, best of 3: 302 µs per loop\n', 'sample', '%timeit df.sample(n, replace=True)\n# Out: 100 loops, best of 3: 5.14 ms per loop\n', 'merge', ""%timeit df.merge(pandas.DataFrame(index=np.random.randint(n, size=n)), left_index=True, right_index=True, how='right')\n# Out: 1000 loops, best of 3: 1.23 ms per loop\n"", 'iloc', 'pandas.DataFrame']";"[""import pandas\nimport numpy as np\n# Generate some data\nn = 5000\nvalues = np.random.uniform(size=(n, 5))\n# Construct a pandas.DataFrame\ncolumns = ['a', 'b', 'c', 'd', 'e']\ndf = pandas.DataFrame(values, columns=columns)\n# Bootstrap\n# Out: 1000 loops, best of 3: 1.46 ms per loop\n# Out: 10000 loops, best of 3: 159 µs per loop\n# Out: 1000 loops, best of 3: 302 µs per loop\n# Out: 100 loops, best of 3: 5.14 ms per loop\n# Out: 1000 loops, best of 3: 1.23 ms per loop\n""]";"[""import pandas\nimport numpy as np\n# Generate some data\nn = 5000\nvalues = np.random.uniform(size=(n, 5))\n# Construct a pandas.DataFrame\ncolumns = ['a', 'b', 'c', 'd', 'e']\ndf = pandas.DataFrame(values, columns=columns)\n# Bootstrap\n# Out: 1000 loops, best of 3: 1.46 ms per loop\n# Out: 10000 loops, best of 3: 159 µs per loop\n# Out: 1000 loops, best of 3: 302 µs per loop\n# Out: 100 loops, best of 3: 5.14 ms per loop\n# Out: 1000 loops, best of 3: 1.23 ms per loop\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nimport pandas\nimport numpy as np\n# Generate some data\nn = 5000\nvalues = np.random.uniform(size=(n, 5))\n# Construct a pandas.DataFrame\ncolumns = ['a', 'b', 'c', 'd', 'e']\ndf = pandas.DataFrame(values, columns=columns)\n# Bootstrap\n# Out: 1000 loops, best of 3: 1.46 ms per loop\n# Out: 10000 loops, best of 3: 159 µs per loop\n# Out: 1000 loops, best of 3: 302 µs per loop\n# Out: 100 loops, best of 3: 5.14 ms per loop\n# Out: 1000 loops, best of 3: 1.23 ms per loop\n""]";True;0;0;[];[];0;0;[];[];0;0;[];[]
1056;1056;1056;1056;1.0;0;31511997;;1;19;<python><python-2.7><pandas><dataframe>;Pandas DataFrame: replace all values in a column, based on condition;21833.0;"[""df.loc[(df['First Season'] > 1990)] = 1\n""]";"[""df.loc[(df['First Season'] > 1990)] = 1\n""]";"[""df.loc[(df['First Season'] > 1990)] = 1\n""]";"[""df.loc[(df['First Season'] > 1990)] = 1\n""]";"[""df.loc[(df['First Season'] > 1990)] = 1\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf.loc[(df['First Season'] > 1990)] = 1\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'First Season'""]";['KeyError']
1057;1057;1057;1057;2.0;2;31569549;;1;16;<python><pandas>;How to GroupBy a Dataframe in Pandas and keep Columns;13417.0;['Name   Type   ID\nBook1  ebook  1\nBook2  paper  2\nBook3  paper  3\nBook1  ebook  1\nBook2  paper  2\nName   Type   ID    Count\nBook1  ebook  1     2\nBook2  paper  2     2\nBook3  paper  3     1\n'];['Name   Type   ID\nBook1  ebook  1\nBook2  paper  2\nBook3  paper  3\nBook1  ebook  1\nBook2  paper  2\n', 'Name   Type   ID    Count\nBook1  ebook  1     2\nBook2  paper  2     2\nBook3  paper  3     1\n'];['Name   Type   ID\nBook1  ebook  1\nBook2  paper  2\nBook3  paper  3\nBook1  ebook  1\nBook2  paper  2\n', 'Name   Type   ID    Count\nBook1  ebook  1     2\nBook2  paper  2     2\nBook3  paper  3     1\n'];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'Name'"", ""'Name'""]";['KeyError', 'KeyError']
1058;1058;1058;1058;1.0;1;31571217;;1;11;<python><pandas><machine-learning>;loc function in pandas;18712.0;"[""for i in range(0, 2):\nfor j in range(0, 3):\n    df.loc[ (df.Age.isnull()) & (df.Gender == i) & (df.Pclass == j+1),\\\n            'AgeFill'] = median_ages[i,j]\n""]";"[""for i in range(0, 2):\nfor j in range(0, 3):\n    df.loc[ (df.Age.isnull()) & (df.Gender == i) & (df.Pclass == j+1),\\\n            'AgeFill'] = median_ages[i,j]\n""]";"[""for i in range(0, 2):\nfor j in range(0, 3):\n    df.loc[ (df.Age.isnull()) & (df.Gender == i) & (df.Pclass == j+1),\\\n            'AgeFill'] = median_ages[i,j]\n""]";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'Age_fill'""]";['KeyError']
1059;1059;1059;1059;2.0;2;31593201;;1;218;<python><pandas><indexing><dataframe>;pandas iloc vs ix vs loc explanation?;118214.0;['df.loc[:5]\ndf.ix[:5]\ndf.iloc[:5]\n'];['df.loc[:5]\ndf.ix[:5]\ndf.iloc[:5]\n'];['DataFrame', 'df.loc[:5]\ndf.ix[:5]\ndf.iloc[:5]\n'];['df.loc[:5]\ndf.ix[:5]\ndf.iloc[:5]\n'];['df.loc[:5]\ndf.ix[:5]\ndf.iloc[:5]\n'];False;['import pandas as pd\ndf = pd.DataFrame()\ndf.loc[:5]\ndf.ix[:5]\ndf.iloc[:5]\n'];True;1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError']
1060;1060;1060;1060;1.0;2;31609600;;1;30;<python><pandas><ipython>;Jupyter (IPython) notebook not plotting;22330.0;"[' ipython notebook --pylab==inline\n""Support for specifying --pylab on the command line has been removed. Please use \'%pylab = inline\' or \'%matplotlib =inline\' in the notebook itself""\n""matplotlib.axes._subplots.AxesSubplot at 0xebf8b70"".\n']";"[' ipython notebook --pylab==inline\n', '""Support for specifying --pylab on the command line has been removed. Please use \'%pylab = inline\' or \'%matplotlib =inline\' in the notebook itself""\n', '""matplotlib.axes._subplots.AxesSubplot at 0xebf8b70"".\n']";"[' ipython notebook --pylab==inline\n', '""Support for specifying --pylab on the command line has been removed. Please use \'%pylab = inline\' or \'%matplotlib =inline\' in the notebook itself""\n', '""matplotlib.axes._subplots.AxesSubplot at 0xebf8b70"".\n']";"['""Support for specifying --pylab on the command line has been removed. Please use \'%pylab = inline\' or \'%matplotlib =inline\' in the notebook itself""\n']";"['""Support for specifying --pylab on the command line has been removed. Please use \'%pylab = inline\' or \'%matplotlib =inline\' in the notebook itself""\n']";False;"['import pandas as pd\n""Support for specifying --pylab on the command line has been removed. Please use \'%pylab = inline\' or \'%matplotlib =inline\' in the notebook itself""\n']";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1061;1061;1061;1061;1.0;0;31610889;;1;16;<python><pandas>;How to copy/paste DataFrame from StackOverflow into Python;1675.0;['In []: x\nOut[]: \n   bar  foo\n0    4    1\n1    5    2\n2    6    3\n'];['In []: x\nOut[]: \n   bar  foo\n0    4    1\n1    5    2\n2    6    3\n'];['DataFrame', 'In []: x\nOut[]: \n   bar  foo\n0    4    1\n1    5    2\n2    6    3\n', 'DataFrame'];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1062;1062;1062;1062;4.0;1;31617845;;1;12;<python><pandas>;How to select rows in a DataFrame between two values, in Python Pandas?;12492.0;"[""df = df[(99 <= df['closing_price'] <= 101)]\n""]";"[""df = df[(99 <= df['closing_price'] <= 101)]\n""]";"['df', 'closing_price', ""df = df[(99 <= df['closing_price'] <= 101)]\n""]";"[""df = df[(99 <= df['closing_price'] <= 101)]\n""]";"[""df = df[(99 <= df['closing_price'] <= 101)]\n""]";False;"[""import pandas as pd\ndf = df[(99 <= df['closing_price'] <= 101)]\n""]";False;0;3;"[""name 'df' is not defined"", ""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'np' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'np' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError']
1063;1063;1063;1063;2.0;5;31661604;;1;12;<python><pandas><scipy><scikit-learn><sparse-matrix>;Efficiently create sparse pivot tables in pandas?;1860.0;"[""import pandas as pd\nframe=pd.DataFrame()\nframe['person']=['me','you','him','you','him','me']\nframe['thing']=['a','a','b','c','d','d']\nframe['count']=[1,1,1,1,1,1]\n\nframe\n\n  person thing  count\n0     me     a      1\n1    you     a      1\n2    him     b      1\n3    you     c      1\n4    him     d      1\n5     me     d      1\n\nframe.pivot('person','thing')\n\n        count            \nthing       a   b   c   d\nperson                   \nhim       NaN   1 NaN   1\nme          1 NaN NaN   1\nyou         1 NaN   1 NaN\n""]";"[""import pandas as pd\nframe=pd.DataFrame()\nframe['person']=['me','you','him','you','him','me']\nframe['thing']=['a','a','b','c','d','d']\nframe['count']=[1,1,1,1,1,1]\n\nframe\n\n  person thing  count\n0     me     a      1\n1    you     a      1\n2    him     b      1\n3    you     c      1\n4    him     d      1\n5     me     d      1\n\nframe.pivot('person','thing')\n\n        count            \nthing       a   b   c   d\nperson                   \nhim       NaN   1 NaN   1\nme          1 NaN NaN   1\nyou         1 NaN   1 NaN\n""]";"[""import pandas as pd\nframe=pd.DataFrame()\nframe['person']=['me','you','him','you','him','me']\nframe['thing']=['a','a','b','c','d','d']\nframe['count']=[1,1,1,1,1,1]\n\nframe\n\n  person thing  count\n0     me     a      1\n1    you     a      1\n2    him     b      1\n3    you     c      1\n4    him     d      1\n5     me     d      1\n\nframe.pivot('person','thing')\n\n        count            \nthing       a   b   c   d\nperson                   \nhim       NaN   1 NaN   1\nme          1 NaN NaN   1\nyou         1 NaN   1 NaN\n""]";"[""import pandas as pd\nframe=pd.DataFrame()\nframe['person']=['me','you','him','you','him','me']\nframe['thing']=['a','a','b','c','d','d']\nframe['count']=[1,1,1,1,1,1]\n\nframe\n\n\nframe.pivot('person','thing')\n\nperson                   \n""]";"[""import pandas as pd\nframe=pd.DataFrame()\nframe['person']=['me','you','him','you','him','me']\nframe['thing']=['a','a','b','c','d','d']\nframe['count']=[1,1,1,1,1,1]\n\nframe\n\n\nframe.pivot('person','thing')\n\nperson                   \n""]";False;"[""import pandas as pd\nimport pandas as pd\nframe=pd.DataFrame()\nframe['person']=['me','you','him','you','him','me']\nframe['thing']=['a','a','b','c','d','d']\nframe['count']=[1,1,1,1,1,1]\n\nframe\n\n\nframe.pivot('person','thing')\n\nperson                   \n""]";False;0;1;"[""No module named 'scipy'""]";['ImportError'];0;1;"[""No module named 'scipy'""]";['ImportError'];0;1;"[""No module named 'scipy'""]";['ImportError']
1064;1064;1064;1064;4.0;11;31866802;;1;14;<python><arrays><numpy><pandas><dataframe>;Pandas DataFrame: How to natively get minimum across range of rows and columns;897.0;"[""2011-01-09 2481.22\n2011-01-10 2481.22\n1. Get the earliest row (only the values after the start time)\n2. Get the middle rows \n3. Get the last row (only the values before the end time)\n4. Concat (1), (2), and (3)\n5. Get the minimum of (4)\nimport numpy\nimport pandas\nimport datetime\n\nnumpy.random.seed(0)\n\nrandom_numbers = (numpy.random.rand(10, 8)*100 + 2000)\ncolumns        = [datetime.time(13,0) , datetime.time(13,30), datetime.time(14,0), datetime.time(14,30) , datetime.time(15,0), datetime.time(15,30) ,datetime.time(16,0), datetime.time(16,30)] \nindex          = pandas.date_range('2011/1/1', '2011/1/10')\ndf             = pandas.DataFrame(data = random_numbers, columns=columns, index = index).astype(int)\n\nprint df\n""]";"['2011-01-09 2481.22\n2011-01-10 2481.22\n', '1. Get the earliest row (only the values after the start time)\n2. Get the middle rows \n3. Get the last row (only the values before the end time)\n4. Concat (1), (2), and (3)\n5. Get the minimum of (4)\n', ""import numpy\nimport pandas\nimport datetime\n\nnumpy.random.seed(0)\n\nrandom_numbers = (numpy.random.rand(10, 8)*100 + 2000)\ncolumns        = [datetime.time(13,0) , datetime.time(13,30), datetime.time(14,0), datetime.time(14,30) , datetime.time(15,0), datetime.time(15,30) ,datetime.time(16,0), datetime.time(16,30)] \nindex          = pandas.date_range('2011/1/1', '2011/1/10')\ndf             = pandas.DataFrame(data = random_numbers, columns=columns, index = index).astype(int)\n\nprint df\n""]";"['2011-01-09 2481.22\n2011-01-10 2481.22\n', '1. Get the earliest row (only the values after the start time)\n2. Get the middle rows \n3. Get the last row (only the values before the end time)\n4. Concat (1), (2), and (3)\n5. Get the minimum of (4)\n', ""import numpy\nimport pandas\nimport datetime\n\nnumpy.random.seed(0)\n\nrandom_numbers = (numpy.random.rand(10, 8)*100 + 2000)\ncolumns        = [datetime.time(13,0) , datetime.time(13,30), datetime.time(14,0), datetime.time(14,30) , datetime.time(15,0), datetime.time(15,30) ,datetime.time(16,0), datetime.time(16,30)] \nindex          = pandas.date_range('2011/1/1', '2011/1/10')\ndf             = pandas.DataFrame(data = random_numbers, columns=columns, index = index).astype(int)\n\nprint df\n""]";"[""import numpy\nimport pandas\nimport datetime\n\nnumpy.random.seed(0)\n\nrandom_numbers = (numpy.random.rand(10, 8)*100 + 2000)\ncolumns        = [datetime.time(13,0) , datetime.time(13,30), datetime.time(14,0), datetime.time(14,30) , datetime.time(15,0), datetime.time(15,30) ,datetime.time(16,0), datetime.time(16,30)] \nindex          = pandas.date_range('2011/1/1', '2011/1/10')\ndf             = pandas.DataFrame(data = random_numbers, columns=columns, index = index).astype(int)\n\n""]";"[""import numpy\nimport pandas\nimport datetime\n\nnumpy.random.seed(0)\n\nrandom_numbers = (numpy.random.rand(10, 8)*100 + 2000)\ncolumns        = [datetime.time(13,0) , datetime.time(13,30), datetime.time(14,0), datetime.time(14,30) , datetime.time(15,0), datetime.time(15,30) ,datetime.time(16,0), datetime.time(16,30)] \nindex          = pandas.date_range('2011/1/1', '2011/1/10')\ndf             = pandas.DataFrame(data = random_numbers, columns=columns, index = index).astype(int)\n\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport numpy\nimport pandas\nimport datetime\n\nnumpy.random.seed(0)\n\nrandom_numbers = (numpy.random.rand(10, 8)*100 + 2000)\ncolumns        = [datetime.time(13,0) , datetime.time(13,30), datetime.time(14,0), datetime.time(14,30) , datetime.time(15,0), datetime.time(15,30) ,datetime.time(16,0), datetime.time(16,30)] \nindex          = pandas.date_range('2011/1/1', '2011/1/10')\ndf             = pandas.DataFrame(data = random_numbers, columns=columns, index = index).astype(int)\n\n""]";True;0;3;"[""name 'np' is not defined"", ""name 'datetime' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'np' is not defined"", ""name 'datetime' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];1;3;"[""name 'np' is not defined"", ""name 'datetime' is not defined"", 'Sucess']";['NameError', 'NameError', 'Sucess']
1065;1065;1065;1065;4.0;1;31988322;;1;13;<python><postgresql><pandas>;Pandas update sql;5751.0;"[""con = psycopg2.connect(database='mydb', user='abc', password='xyz')\ncur = con.cursor()\n\nfor index, row in df.iterrows():\n    sql = 'update table set column = %s where column = %s'\n    cur.execute(sql, (row['whatver'], row['something']))\ncon.commit()\nengine = create_engine('postgresql+psycopg2://user:pswd@mydb')\ndf.to_sql('table', engine, if_exists='append')\n""]";"[""con = psycopg2.connect(database='mydb', user='abc', password='xyz')\ncur = con.cursor()\n\nfor index, row in df.iterrows():\n    sql = 'update table set column = %s where column = %s'\n    cur.execute(sql, (row['whatver'], row['something']))\ncon.commit()\n"", ""engine = create_engine('postgresql+psycopg2://user:pswd@mydb')\ndf.to_sql('table', engine, if_exists='append')\n""]";"[""con = psycopg2.connect(database='mydb', user='abc', password='xyz')\ncur = con.cursor()\n\nfor index, row in df.iterrows():\n    sql = 'update table set column = %s where column = %s'\n    cur.execute(sql, (row['whatver'], row['something']))\ncon.commit()\n"", ""engine = create_engine('postgresql+psycopg2://user:pswd@mydb')\ndf.to_sql('table', engine, if_exists='append')\n""]";"[""con = psycopg2.connect(database='mydb', user='abc', password='xyz')\ncur = con.cursor()\n\ncon.commit()\nengine = create_engine('postgresql+psycopg2://user:pswd@mydb')\ndf.to_sql('table', engine, if_exists='append')\n""]";"[""con = psycopg2.connect(database='mydb', user='abc', password='xyz')\ncur = con.cursor()\n\ncon.commit()\nengine = create_engine('postgresql+psycopg2://user:pswd@mydb')\ndf.to_sql('table', engine, if_exists='append')\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\ncon = psycopg2.connect(database='mydb', user='abc', password='xyz')\ncur = con.cursor()\n\ncon.commit()\nengine = create_engine('postgresql+psycopg2://user:pswd@mydb')\ndf.to_sql('table', engine, if_exists='append')\n""]";True;0;1;"[""name 'create_engine' is not defined""]";['NameError'];0;1;"[""name 'create_engine' is not defined""]";['NameError'];0;1;"[""name 'create_engine' is not defined""]";['NameError']
1066;1066;1066;1066;4.0;0;32011359;;1;24;<python><pandas>;Convert categorical data in pandas dataframe;29142.0;"[""col1        int64\ncol2        int64\ncol3        category\ncol4        category\ncol5        category\nName: col3, dtype: category\nCategories (8, object): [B, C, E, G, H, N, S, W]\n[1, 2, 3, 4, 5, 6, 7, 8]\ndataframe['c'] = pandas.Categorical.from_array(dataframe.col3).codes\n""]";"['col1        int64\ncol2        int64\ncol3        category\ncol4        category\ncol5        category\n', 'Name: col3, dtype: category\nCategories (8, object): [B, C, E, G, H, N, S, W]\n', '[1, 2, 3, 4, 5, 6, 7, 8]\n', ""dataframe['c'] = pandas.Categorical.from_array(dataframe.col3).codes\n""]";"['col1        int64\ncol2        int64\ncol3        category\ncol4        category\ncol5        category\n', 'Name: col3, dtype: category\nCategories (8, object): [B, C, E, G, H, N, S, W]\n', '[1, 2, 3, 4, 5, 6, 7, 8]\n', ""dataframe['c'] = pandas.Categorical.from_array(dataframe.col3).codes\n""]";"[""[1, 2, 3, 4, 5, 6, 7, 8]\ndataframe['c'] = pandas.Categorical.from_array(dataframe.col3).codes\n""]";"[""[1, 2, 3, 4, 5, 6, 7, 8]\ndataframe['c'] = pandas.Categorical.from_array(dataframe.col3).codes\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\n[1, 2, 3, 4, 5, 6, 7, 8]\ndataframe['c'] = pandas.Categorical.from_array(dataframe.col3).codes\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1067;1067;1067;1067;3.0;0;32012012;;1;11;<python><pandas><group-by><time-series>;Pandas: resample timeseries with groupby;4612.0;"[""In [115]: times = pd.to_datetime(pd.Series(['2014-08-25 21:00:00','2014-08-25 21:04:00',\n                                            '2014-08-25 22:07:00','2014-08-25 22:09:00']))\n          locations = ['HK', 'LDN', 'LDN', 'LDN']\n          event = ['foo', 'bar', 'baz', 'qux']\n          df = pd.DataFrame({'Location': locations,\n                             'Event': event}, index=times)\n          df\nOut[115]:\n                               Event Location\n          2014-08-25 21:00:00  foo   HK\n          2014-08-25 21:04:00  bar   LDN\n          2014-08-25 22:07:00  baz   LDN\n          2014-08-25 22:09:00  qux   LDN\nOut[115]:\n                               HK    LDN\n          2014-08-25 21:00:00  1     1\n          2014-08-25 22:00:00  0     2\n""]";"[""In [115]: times = pd.to_datetime(pd.Series(['2014-08-25 21:00:00','2014-08-25 21:04:00',\n                                            '2014-08-25 22:07:00','2014-08-25 22:09:00']))\n          locations = ['HK', 'LDN', 'LDN', 'LDN']\n          event = ['foo', 'bar', 'baz', 'qux']\n          df = pd.DataFrame({'Location': locations,\n                             'Event': event}, index=times)\n          df\nOut[115]:\n                               Event Location\n          2014-08-25 21:00:00  foo   HK\n          2014-08-25 21:04:00  bar   LDN\n          2014-08-25 22:07:00  baz   LDN\n          2014-08-25 22:09:00  qux   LDN\n"", 'Out[115]:\n                               HK    LDN\n          2014-08-25 21:00:00  1     1\n          2014-08-25 22:00:00  0     2\n']";"[""In [115]: times = pd.to_datetime(pd.Series(['2014-08-25 21:00:00','2014-08-25 21:04:00',\n                                            '2014-08-25 22:07:00','2014-08-25 22:09:00']))\n          locations = ['HK', 'LDN', 'LDN', 'LDN']\n          event = ['foo', 'bar', 'baz', 'qux']\n          df = pd.DataFrame({'Location': locations,\n                             'Event': event}, index=times)\n          df\nOut[115]:\n                               Event Location\n          2014-08-25 21:00:00  foo   HK\n          2014-08-25 21:04:00  bar   LDN\n          2014-08-25 22:07:00  baz   LDN\n          2014-08-25 22:09:00  qux   LDN\n"", 'Out[115]:\n                               HK    LDN\n          2014-08-25 21:00:00  1     1\n          2014-08-25 22:00:00  0     2\n']";[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"['pd.TimeGrouper is deprecated and will be removed; Please use pd.Grouper(freq=...)', ""'Location'""]";['FutureWarning', 'KeyError']
1068;1068;1068;1068;2.0;0;32093829;;1;14;<python><pandas><duplicates>;Python(pandas): removing duplicates based on two columns keeping row with max value in another column;11397.0;['A B C\n1 2 1\n1 2 4\n2 7 1\n3 4 0\n3 4 8\nA B C\n1 2 4\n2 7 1\n3 4 8\n'];['A B C\n1 2 1\n1 2 4\n2 7 1\n3 4 0\n3 4 8\n', 'A B C\n1 2 4\n2 7 1\n3 4 8\n'];['A B C\n1 2 1\n1 2 4\n2 7 1\n3 4 0\n3 4 8\n', 'A B C\n1 2 4\n2 7 1\n3 4 8\n', 'drop_duplicates()'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError']
1069;1069;1069;1069;2.0;0;32244019;;1;22;<python><pandas><matplotlib>;How to rotate x-axis tick labels in Pandas barplot;14916.0;"['import matplotlib\nmatplotlib.style.use(\'ggplot\')\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.DataFrame({ \'celltype\':[""foo"",""bar"",""qux"",""woz""], \'s1\':[5,9,1,7], \'s2\':[12,90,13,87]})\ndf = df[[""celltype"",""s1"",""s2""]]\ndf.set_index([""celltype""],inplace=True)\ndf.plot(kind=\'bar\',alpha=0.75)\nplt.xlabel("""")\nplt.set_xticklabels(df.index,rotation=90)\n']";"['import matplotlib\nmatplotlib.style.use(\'ggplot\')\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.DataFrame({ \'celltype\':[""foo"",""bar"",""qux"",""woz""], \'s1\':[5,9,1,7], \'s2\':[12,90,13,87]})\ndf = df[[""celltype"",""s1"",""s2""]]\ndf.set_index([""celltype""],inplace=True)\ndf.plot(kind=\'bar\',alpha=0.75)\nplt.xlabel("""")\n', 'plt.set_xticklabels(df.index,rotation=90)\n']";"['import matplotlib\nmatplotlib.style.use(\'ggplot\')\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.DataFrame({ \'celltype\':[""foo"",""bar"",""qux"",""woz""], \'s1\':[5,9,1,7], \'s2\':[12,90,13,87]})\ndf = df[[""celltype"",""s1"",""s2""]]\ndf.set_index([""celltype""],inplace=True)\ndf.plot(kind=\'bar\',alpha=0.75)\nplt.xlabel("""")\n', 'plt.set_xticklabels(df.index,rotation=90)\n']";"['import matplotlib\nmatplotlib.style.use(\'ggplot\')\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.DataFrame({ \'celltype\':[""foo"",""bar"",""qux"",""woz""], \'s1\':[5,9,1,7], \'s2\':[12,90,13,87]})\ndf = df[[""celltype"",""s1"",""s2""]]\ndf.set_index([""celltype""],inplace=True)\ndf.plot(kind=\'bar\',alpha=0.75)\nplt.xlabel("""")\nplt.set_xticklabels(df.index,rotation=90)\n']";"['import matplotlib\nmatplotlib.style.use(\'ggplot\')\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.DataFrame({ \'celltype\':[""foo"",""bar"",""qux"",""woz""], \'s1\':[5,9,1,7], \'s2\':[12,90,13,87]})\ndf = df[[""celltype"",""s1"",""s2""]]\ndf.set_index([""celltype""],inplace=True)\ndf.plot(kind=\'bar\',alpha=0.75)\nplt.xlabel("""")\nplt.set_xticklabels(df.index,rotation=90)\n']";False;"['import pandas as pd\nimport matplotlib\nmatplotlib.style.use(\'ggplot\')\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.DataFrame({ \'celltype\':[""foo"",""bar"",""qux"",""woz""], \'s1\':[5,9,1,7], \'s2\':[12,90,13,87]})\ndf = df[[""celltype"",""s1"",""s2""]]\ndf.set_index([""celltype""],inplace=True)\ndf.plot(kind=\'bar\',alpha=0.75)\nplt.xlabel("""")\nplt.set_xticklabels(df.index,rotation=90)\n']";False;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
1070;1070;1070;1070;4.0;0;32244753;;1;35;<python><pandas><matplotlib><seaborn>;How to save a Seaborn plot into a file;27148.0;"['import matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nmatplotlib.style.use(\'ggplot\')\nimport seaborn as sns\nsns.set()\ndf = sns.load_dataset(\'iris\')\nsns_plot = sns.pairplot(df, hue=\'species\', size=2.5)\nfig = sns_plot.get_figure()\nfig.savefig(""output.png"")\n#sns.plt.show()\n  Traceback (most recent call last):\n  File ""test_searborn.py"", line 11, in <module>\n    fig = sns_plot.get_figure()\nAttributeError: \'PairGrid\' object has no attribute \'get_figure\'\n']";"['import matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nmatplotlib.style.use(\'ggplot\')\nimport seaborn as sns\nsns.set()\ndf = sns.load_dataset(\'iris\')\nsns_plot = sns.pairplot(df, hue=\'species\', size=2.5)\nfig = sns_plot.get_figure()\nfig.savefig(""output.png"")\n#sns.plt.show()\n', '  Traceback (most recent call last):\n  File ""test_searborn.py"", line 11, in <module>\n    fig = sns_plot.get_figure()\nAttributeError: \'PairGrid\' object has no attribute \'get_figure\'\n']";"['test_seaborn.py', 'import matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nmatplotlib.style.use(\'ggplot\')\nimport seaborn as sns\nsns.set()\ndf = sns.load_dataset(\'iris\')\nsns_plot = sns.pairplot(df, hue=\'species\', size=2.5)\nfig = sns_plot.get_figure()\nfig.savefig(""output.png"")\n#sns.plt.show()\n', '  Traceback (most recent call last):\n  File ""test_searborn.py"", line 11, in <module>\n    fig = sns_plot.get_figure()\nAttributeError: \'PairGrid\' object has no attribute \'get_figure\'\n', 'output.png']";"['import matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nmatplotlib.style.use(\'ggplot\')\nimport seaborn as sns\nsns.set()\ndf = sns.load_dataset(\'iris\')\nsns_plot = sns.pairplot(df, hue=\'species\', size=2.5)\nfig = sns_plot.get_figure()\nfig.savefig(""output.png"")\n#sns.plt.show()\n']";"['import matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nmatplotlib.style.use(\'ggplot\')\nimport seaborn as sns\nsns.set()\ndf = sns.load_dataset(\'iris\')\nsns_plot = sns.pairplot(df, hue=\'species\', size=2.5)\nfig = sns_plot.get_figure()\nfig.savefig(""output.png"")\n#sns.plt.show()\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nmatplotlib.style.use(\'ggplot\')\nimport seaborn as sns\nsns.set()\ndf = sns.load_dataset(\'iris\')\nsns_plot = sns.pairplot(df, hue=\'species\', size=2.5)\nfig = sns_plot.get_figure()\nfig.savefig(""output.png"")\n#sns.plt.show()\n']";True;0;4;"[""name 'sns_plot' is not defined"", ""name 'sns' is not defined"", ""name 'sns' is not defined"", ""name 'sns_plot' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""name 'sns_plot' is not defined"", ""name 'sns' is not defined"", ""name 'sns' is not defined"", ""name 'sns_plot' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""name 'sns_plot' is not defined"", ""name 'sns' is not defined"", ""name 'sns' is not defined"", ""name 'sns_plot' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError']
1071;1071;1071;1071;4.0;2;32400867;;1;22;<python><csv><pandas><request>;Pandas read_csv from url;17280.0;"['import pandas as pd\nimport requests\n\nurl=""https://github.com/cs109/2014_data/blob/master/countries.csv""\ns=requests.get(url).content\nc=pd.read_csv(s)\n']";"['import pandas as pd\nimport requests\n\nurl=""https://github.com/cs109/2014_data/blob/master/countries.csv""\ns=requests.get(url).content\nc=pd.read_csv(s)\n']";"['import pandas as pd\nimport requests\n\nurl=""https://github.com/cs109/2014_data/blob/master/countries.csv""\ns=requests.get(url).content\nc=pd.read_csv(s)\n']";"['import pandas as pd\nimport requests\n\nurl=""https://github.com/cs109/2014_data/blob/master/countries.csv""\ns=requests.get(url).content\nc=pd.read_csv(s)\n']";"['import pandas as pd\nimport requests\n\nurl=""https://github.com/cs109/2014_data/blob/master/countries.csv""\ns=requests.get(url).content\nc=pd.read_csv(s)\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\nimport pandas as pd\nimport requests\n\nurl=""https://github.com/cs109/2014_data/blob/master/countries.csv""\ns=requests.get(url).content\nc=pd.read_csv(s)\n']";True;2;3;"['Sucess', ""name 'pd' is not defined"", 'Sucess']";['Sucess', 'NameError', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess'];3;3;['Sucess', 'Sucess', 'Sucess'];['Sucess', 'Sucess', 'Sucess']
1072;1072;1072;1072;3.0;0;32444138;;1;14;<python><pandas>;Combine a list of pandas dataframes to one pandas dataframe;13235.0;"['import pandas as pd\ndfs = []\nsqlall = ""select * from mytable""\n\nfor chunk in pd.read_sql_query(sqlall , cnxn, chunksize=10000):\n    dfs.append(chunk)\ntype(dfs[0])\nOut[6]: pandas.core.frame.DataFrame\n\ntype(dfs)\nOut[7]: list\n\nlen(dfs)\nOut[8]: 408\n# sample dataframes\nd1 = pd.DataFrame({\'one\' : [1., 2., 3., 4.], \'two\' : [4., 3., 2., 1.]})\nd2 = pd.DataFrame({\'one\' : [5., 6., 7., 8.], \'two\' : [9., 10., 11., 12.]})\nd3 = pd.DataFrame({\'one\' : [15., 16., 17., 18.], \'two\' : [19., 10., 11., 12.]})\n\n# list of dataframes\nmydfs = [d1, d2, d3]\n']";"['import pandas as pd\ndfs = []\nsqlall = ""select * from mytable""\n\nfor chunk in pd.read_sql_query(sqlall , cnxn, chunksize=10000):\n    dfs.append(chunk)\n', 'type(dfs[0])\nOut[6]: pandas.core.frame.DataFrame\n\ntype(dfs)\nOut[7]: list\n\nlen(dfs)\nOut[8]: 408\n', ""# sample dataframes\nd1 = pd.DataFrame({'one' : [1., 2., 3., 4.], 'two' : [4., 3., 2., 1.]})\nd2 = pd.DataFrame({'one' : [5., 6., 7., 8.], 'two' : [9., 10., 11., 12.]})\nd3 = pd.DataFrame({'one' : [15., 16., 17., 18.], 'two' : [19., 10., 11., 12.]})\n\n# list of dataframes\nmydfs = [d1, d2, d3]\n""]";"['import pandas as pd\ndfs = []\nsqlall = ""select * from mytable""\n\nfor chunk in pd.read_sql_query(sqlall , cnxn, chunksize=10000):\n    dfs.append(chunk)\n', 'type(dfs[0])\nOut[6]: pandas.core.frame.DataFrame\n\ntype(dfs)\nOut[7]: list\n\nlen(dfs)\nOut[8]: 408\n', ""# sample dataframes\nd1 = pd.DataFrame({'one' : [1., 2., 3., 4.], 'two' : [4., 3., 2., 1.]})\nd2 = pd.DataFrame({'one' : [5., 6., 7., 8.], 'two' : [9., 10., 11., 12.]})\nd3 = pd.DataFrame({'one' : [15., 16., 17., 18.], 'two' : [19., 10., 11., 12.]})\n\n# list of dataframes\nmydfs = [d1, d2, d3]\n"", 'd1', 'd2', 'd3', 'chunksize']";"['import pandas as pd\ndfs = []\nsqlall = ""select * from mytable""\n\ntype(dfs[0])\n\ntype(dfs)\n\nlen(dfs)\n# sample dataframes\nd1 = pd.DataFrame({\'one\' : [1., 2., 3., 4.], \'two\' : [4., 3., 2., 1.]})\nd2 = pd.DataFrame({\'one\' : [5., 6., 7., 8.], \'two\' : [9., 10., 11., 12.]})\nd3 = pd.DataFrame({\'one\' : [15., 16., 17., 18.], \'two\' : [19., 10., 11., 12.]})\n\n# list of dataframes\nmydfs = [d1, d2, d3]\n']";"['import pandas as pd\ndfs = []\nsqlall = ""select * from mytable""\n\ntype(dfs[0])\n\ntype(dfs)\n\nlen(dfs)\n# sample dataframes\nd1 = pd.DataFrame({\'one\' : [1., 2., 3., 4.], \'two\' : [4., 3., 2., 1.]})\nd2 = pd.DataFrame({\'one\' : [5., 6., 7., 8.], \'two\' : [9., 10., 11., 12.]})\nd3 = pd.DataFrame({\'one\' : [15., 16., 17., 18.], \'two\' : [19., 10., 11., 12.]})\n\n# list of dataframes\nmydfs = [d1, d2, d3]\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\nimport pandas as pd\ndfs = []\nsqlall = ""select * from mytable""\n\ntype(dfs[0])\n\ntype(dfs)\n\nlen(dfs)\n# sample dataframes\nd1 = pd.DataFrame({\'one\' : [1., 2., 3., 4.], \'two\' : [4., 3., 2., 1.]})\nd2 = pd.DataFrame({\'one\' : [5., 6., 7., 8.], \'two\' : [9., 10., 11., 12.]})\nd3 = pd.DataFrame({\'one\' : [15., 16., 17., 18.], \'two\' : [19., 10., 11., 12.]})\n\n# list of dataframes\nmydfs = [d1, d2, d3]\n']";True;0;1;"[""name 'list_of_dataframes' is not defined""]";['NameError'];0;1;"[""name 'list_of_dataframes' is not defined""]";['NameError'];0;1;"[""name 'list_of_dataframes' is not defined""]";['NameError']
1073;1073;1073;1073;4.0;5;32468402;;1;22;<python><pandas><dataframe>;How to explode a list inside a Dataframe cell into separate rows;8761.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""'name'""]";['Sucess', 'KeyError']
1074;1074;1074;1074;7.0;8;32565302;;1;11;<python><pandas><anaconda>;python: after installing anaconda, how to import pandas;39480.0;"['import pandas as pd\nTraceback (most recent call last):\nFile ""<pyshell#0>"", line 1, in <module>\nimport pandasFile\nImportError: No module named pandasFile\n']";"['import pandas as pd\n', 'Traceback (most recent call last):\nFile ""<pyshell#0>"", line 1, in <module>\nimport pandasFile\nImportError: No module named pandasFile\n']";"['import pandas as pd\n', 'Traceback (most recent call last):\nFile ""<pyshell#0>"", line 1, in <module>\nimport pandasFile\nImportError: No module named pandasFile\n']";['import pandas as pd\nimport pandasFile\n'];['import pandas as pd\nimport pandasFile\n'];False;['import pandas as pd\nimport pandas as pd\nimport pandasFile\n'];False;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
1075;1075;1075;1075;4.0;0;32591466;;1;13;<python><pandas><dataframe>;Python pandas: how to specify data types when reading an Excel file?;15650.0;[''];[];['pandas.read_excel()', 'read_excel()'];[''];[''];False;['import pandas as pd\n'];False;0;2;"['The `sheetname` keyword is deprecated, use `sheet_name` instead', ""name 'pandas' is not defined""]";['FutureWarning', 'NameError'];0;2;"['The `sheetname` keyword is deprecated, use `sheet_name` instead', ""name 'pandas' is not defined""]";['FutureWarning', 'NameError'];0;2;"['The `sheetname` keyword is deprecated, use `sheet_name` instead', ""name 'pandas' is not defined""]";['FutureWarning', 'NameError']
1076;1076;1076;1076;3.0;0;32751229;;1;11;<python><pandas><group-by><aggregate>;Pandas sum by groupby, but exclude certain columns;24183.0;"[""Code    Country Item_Code   Item    Ele_Code    Unit    Y1961   Y1962   Y1963\n2   Afghanistan 15          Wheat   5312        Ha      10       20      30\n2   Afghanistan 25          Maize   5312        Ha      10       20      30\n4   Angola      15          Wheat   7312        Ha      30       40      50\n4   Angola      25          Maize   7312        Ha      30       40      50\nCode    Country Item_Code   Item    Ele_Code    Unit    Y1961   Y1962   Y1963\n    2   Afghanistan 15      C3      5312        Ha      20       40      60\n    4   Angola      25      C4      7312        Ha      60       80      100\ndf.groupby('Country').sum()\n""]";"['Code    Country Item_Code   Item    Ele_Code    Unit    Y1961   Y1962   Y1963\n2   Afghanistan 15          Wheat   5312        Ha      10       20      30\n2   Afghanistan 25          Maize   5312        Ha      10       20      30\n4   Angola      15          Wheat   7312        Ha      30       40      50\n4   Angola      25          Maize   7312        Ha      30       40      50\n', 'Code    Country Item_Code   Item    Ele_Code    Unit    Y1961   Y1962   Y1963\n    2   Afghanistan 15      C3      5312        Ha      20       40      60\n    4   Angola      25      C4      7312        Ha      60       80      100\n', ""df.groupby('Country').sum()\n""]";"['Code    Country Item_Code   Item    Ele_Code    Unit    Y1961   Y1962   Y1963\n2   Afghanistan 15          Wheat   5312        Ha      10       20      30\n2   Afghanistan 25          Maize   5312        Ha      10       20      30\n4   Angola      15          Wheat   7312        Ha      30       40      50\n4   Angola      25          Maize   7312        Ha      30       40      50\n', 'Code    Country Item_Code   Item    Ele_Code    Unit    Y1961   Y1962   Y1963\n    2   Afghanistan 15      C3      5312        Ha      20       40      60\n    4   Angola      25      C4      7312        Ha      60       80      100\n', ""df.groupby('Country').sum()\n""]";"[""df.groupby('Country').sum()\n""]";"[""df.groupby('Country').sum()\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf.groupby('Country').sum()\n""]";True;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'Country'"", ""'Country'""]";['KeyError', 'KeyError']
1077;1077;1077;1077;1.0;1;32752292;;1;17;<python><pandas><dataframe><size><shape>;Pandas: How to create a data frame of random integers?;16881.0;"[""df = pd.DataFrame(np.random.randn(100, 4), columns=list('ABCD')\n""]";"[""df = pd.DataFrame(np.random.randn(100, 4), columns=list('ABCD')\n""]";"['randn', ""df = pd.DataFrame(np.random.randn(100, 4), columns=list('ABCD')\n"", 'randint', 'randn']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
1078;1078;1078;1078;3.0;0;32760888;;1;12;<python><apache-spark><bigdata><pyspark><rdd>;PySpark DataFrames - way to enumerate without converting to Pandas?;7833.0;"['indexes=[2,3,6,7] \ndf[indexes]\nindexes=np.arange(df.count())\ndf_indexed=df.withColumn(\'index\', indexes)\n indexes=[2,3,6,7] \n df1.where(""index in indexes"").collect()\n']";"['indexes=[2,3,6,7] \ndf[indexes]\n', ""indexes=np.arange(df.count())\ndf_indexed=df.withColumn('index', indexes)\n"", ' indexes=[2,3,6,7] \n df1.where(""index in indexes"").collect()\n']";"['indexes=[2,3,6,7] \ndf[indexes]\n', ""indexes=np.arange(df.count())\ndf_indexed=df.withColumn('index', indexes)\n"", ' indexes=[2,3,6,7] \n df1.where(""index in indexes"").collect()\n']";"[""indexes=[2,3,6,7] \ndf[indexes]\nindexes=np.arange(df.count())\ndf_indexed=df.withColumn('index', indexes)\n""]";"[""indexes=[2,3,6,7] \ndf[indexes]\nindexes=np.arange(df.count())\ndf_indexed=df.withColumn('index', indexes)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nindexes=[2,3,6,7] \ndf[indexes]\nindexes=np.arange(df.count())\ndf_indexed=df.withColumn('index', indexes)\n""]";True;0;1;"[""No module named 'pyspark'""]";['ImportError'];0;1;"[""No module named 'pyspark'""]";['ImportError'];0;1;"[""No module named 'pyspark'""]";['ImportError']
1079;1079;1079;1079;3.0;5;32801806;;1;16;<python><pandas><append><concat>;pandas concat ignore_index doesn't work;6496.0;"[""df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n                    'B': ['B0', 'B1', 'B2', 'B3'],\n                    'D': ['D0', 'D1', 'D2', 'D3']},\n                    index=[0, 2, 3,4])\n\ndf2 = pd.DataFrame({'A1': ['A4', 'A5', 'A6', 'A7'],\n                    'C': ['C4', 'C5', 'C6', 'C7'],\n                    'D2': ['D4', 'D5', 'D6', 'D7']},\n                    index=[ 5, 6, 7,3])\ndf1\n#     A   B   D\n# 0  A0  B0  D0\n# 2  A1  B1  D1\n# 3  A2  B2  D2\n# 4  A3  B3  D3\n\ndf2\n#    A1   C  D2\n# 5  A4  C4  D4\n# 6  A5  C5  D5\n# 7  A6  C6  D6\n# 3  A7  C7  D7\n\ndfs = [df1,df2]\ndf = pd.concat( dfs,axis=1,ignore_index=True)     \nprint df   \n     0    1    2    3    4    5    \n0   A0   B0   D0  NaN  NaN  NaN  \n2   A1   B1   D1  NaN  NaN  NaN    \n3   A2   B2   D2   A7   C7   D7   \n4   A3   B3   D3  NaN  NaN  NaN  \n5  NaN  NaN  NaN   A4   C4   D4  \n6  NaN  NaN  NaN   A5   C5   D5  \n7  NaN  NaN  NaN   A6   C6   D6           \n df1.reset_index()    \n df2.reset_index() \npd.concat([df1,df2],axis=1) \n""]";"[""df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n                    'B': ['B0', 'B1', 'B2', 'B3'],\n                    'D': ['D0', 'D1', 'D2', 'D3']},\n                    index=[0, 2, 3,4])\n\ndf2 = pd.DataFrame({'A1': ['A4', 'A5', 'A6', 'A7'],\n                    'C': ['C4', 'C5', 'C6', 'C7'],\n                    'D2': ['D4', 'D5', 'D6', 'D7']},\n                    index=[ 5, 6, 7,3])\ndf1\n#     A   B   D\n# 0  A0  B0  D0\n# 2  A1  B1  D1\n# 3  A2  B2  D2\n# 4  A3  B3  D3\n\ndf2\n#    A1   C  D2\n# 5  A4  C4  D4\n# 6  A5  C5  D5\n# 7  A6  C6  D6\n# 3  A7  C7  D7\n\ndfs = [df1,df2]\ndf = pd.concat( dfs,axis=1,ignore_index=True)     \nprint df   \n"", '     0    1    2    3    4    5    \n0   A0   B0   D0  NaN  NaN  NaN  \n2   A1   B1   D1  NaN  NaN  NaN    \n3   A2   B2   D2   A7   C7   D7   \n4   A3   B3   D3  NaN  NaN  NaN  \n5  NaN  NaN  NaN   A4   C4   D4  \n6  NaN  NaN  NaN   A5   C5   D5  \n7  NaN  NaN  NaN   A6   C6   D6           \n', ' df1.reset_index()    \n df2.reset_index() \n', 'pd.concat([df1,df2],axis=1) \n']";"['concat', 'ignore_index=True', ""df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n                    'B': ['B0', 'B1', 'B2', 'B3'],\n                    'D': ['D0', 'D1', 'D2', 'D3']},\n                    index=[0, 2, 3,4])\n\ndf2 = pd.DataFrame({'A1': ['A4', 'A5', 'A6', 'A7'],\n                    'C': ['C4', 'C5', 'C6', 'C7'],\n                    'D2': ['D4', 'D5', 'D6', 'D7']},\n                    index=[ 5, 6, 7,3])\ndf1\n#     A   B   D\n# 0  A0  B0  D0\n# 2  A1  B1  D1\n# 3  A2  B2  D2\n# 4  A3  B3  D3\n\ndf2\n#    A1   C  D2\n# 5  A4  C4  D4\n# 6  A5  C5  D5\n# 7  A6  C6  D6\n# 3  A7  C7  D7\n\ndfs = [df1,df2]\ndf = pd.concat( dfs,axis=1,ignore_index=True)     \nprint df   \n"", '     0    1    2    3    4    5    \n0   A0   B0   D0  NaN  NaN  NaN  \n2   A1   B1   D1  NaN  NaN  NaN    \n3   A2   B2   D2   A7   C7   D7   \n4   A3   B3   D3  NaN  NaN  NaN  \n5  NaN  NaN  NaN   A4   C4   D4  \n6  NaN  NaN  NaN   A5   C5   D5  \n7  NaN  NaN  NaN   A6   C6   D6           \n', ' df1.reset_index()    \n df2.reset_index() \n', 'pd.concat([df1,df2],axis=1) \n']";['\ndf1\n#     A   B   D\n# 0  A0  B0  D0\n# 2  A1  B1  D1\n# 3  A2  B2  D2\n# 4  A3  B3  D3\n\ndf2\n#    A1   C  D2\n# 5  A4  C4  D4\n# 6  A5  C5  D5\n# 7  A6  C6  D6\n# 3  A7  C7  D7\n\ndfs = [df1,df2]\ndf = pd.concat( dfs,axis=1,ignore_index=True)     \npd.concat([df1,df2],axis=1) \n'];['import pandas as pd\n\ndf1\n#     A   B   D\n# 0  A0  B0  D0\n# 2  A1  B1  D1\n# 3  A2  B2  D2\n# 4  A3  B3  D3\n\ndf2\n#    A1   C  D2\n# 5  A4  C4  D4\n# 6  A5  C5  D5\n# 7  A6  C6  D6\n# 3  A7  C7  D7\n\ndfs = [df1,df2]\ndf = pd.concat( dfs,axis=1,ignore_index=True)     \npd.concat([df1,df2],axis=1) \n'];True;['import pandas as pd\ndf2 = pd.DataFrame()\ndf1 = pd.DataFrame()\n\ndf1\n#     A   B   D\n# 0  A0  B0  D0\n# 2  A1  B1  D1\n# 3  A2  B2  D2\n# 4  A3  B3  D3\n\ndf2\n#    A1   C  D2\n# 5  A4  C4  D4\n# 6  A5  C5  D5\n# 7  A6  C6  D6\n# 3  A7  C7  D7\n\ndfs = [df1,df2]\ndf = pd.concat( dfs,axis=1,ignore_index=True)     \npd.concat([df1,df2],axis=1) \n'];True;1;2;"['Sucess', ""name 'df1' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df1' is not defined""]";['Sucess', 'NameError'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
1080;1080;1080;1080;1.0;1;32888124;;1;11;<python><datetime><pandas><datetimeoffset>;pandas out of bounds nanosecond timestamp after offset rollforward plus adding a month offset;5520.0;['import pandas as pd\nBOMoffset = pd.tseries.offsets.MonthBegin()\n# here some code sets the all_treatments dataframe and the newrowix, micolix, mocolix counters\nall_treatments.iloc[newrowix,micolix] = BOMoffset.rollforward(all_treatments.iloc[i,micolix] + pd.tseries.offsets.DateOffset(months = x))\nall_treatments.iloc[newrowix,mocolix] = BOMoffset.rollforward(all_treatments.iloc[newrowix,micolix]+ pd.tseries.offsets.DateOffset(months = 1))\npandas.tslib.OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 2262-05-01 00:00:00\n'];['import pandas as pd\nBOMoffset = pd.tseries.offsets.MonthBegin()\n# here some code sets the all_treatments dataframe and the newrowix, micolix, mocolix counters\nall_treatments.iloc[newrowix,micolix] = BOMoffset.rollforward(all_treatments.iloc[i,micolix] + pd.tseries.offsets.DateOffset(months = x))\nall_treatments.iloc[newrowix,mocolix] = BOMoffset.rollforward(all_treatments.iloc[newrowix,micolix]+ pd.tseries.offsets.DateOffset(months = 1))\n', 'pandas.tslib.OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 2262-05-01 00:00:00\n'];"['import pandas as pd\nBOMoffset = pd.tseries.offsets.MonthBegin()\n# here some code sets the all_treatments dataframe and the newrowix, micolix, mocolix counters\nall_treatments.iloc[newrowix,micolix] = BOMoffset.rollforward(all_treatments.iloc[i,micolix] + pd.tseries.offsets.DateOffset(months = x))\nall_treatments.iloc[newrowix,mocolix] = BOMoffset.rollforward(all_treatments.iloc[newrowix,micolix]+ pd.tseries.offsets.DateOffset(months = 1))\n', 'all_treatments.iloc[i,micolix]', ""pd.to_datetime(all_treatments['INDATUMA'], errors='coerce',format='%Y%m%d')"", 'INDATUMA', '20070125', 'pandas.tslib.OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 2262-05-01 00:00:00\n']";['import pandas as pd\nBOMoffset = pd.tseries.offsets.MonthBegin()\n# here some code sets the all_treatments dataframe and the newrowix, micolix, mocolix counters\nall_treatments.iloc[newrowix,micolix] = BOMoffset.rollforward(all_treatments.iloc[i,micolix] + pd.tseries.offsets.DateOffset(months = x))\nall_treatments.iloc[newrowix,mocolix] = BOMoffset.rollforward(all_treatments.iloc[newrowix,micolix]+ pd.tseries.offsets.DateOffset(months = 1))\n'];['import pandas as pd\nBOMoffset = pd.tseries.offsets.MonthBegin()\n# here some code sets the all_treatments dataframe and the newrowix, micolix, mocolix counters\nall_treatments.iloc[newrowix,micolix] = BOMoffset.rollforward(all_treatments.iloc[i,micolix] + pd.tseries.offsets.DateOffset(months = x))\nall_treatments.iloc[newrowix,mocolix] = BOMoffset.rollforward(all_treatments.iloc[newrowix,micolix]+ pd.tseries.offsets.DateOffset(months = 1))\n'];False;['import pandas as pd\ndata = pd.DataFrame()\nimport pandas as pd\nBOMoffset = pd.tseries.offsets.MonthBegin()\n# here some code sets the all_treatments dataframe and the newrowix, micolix, mocolix counters\nall_treatments.iloc[newrowix,micolix] = BOMoffset.rollforward(all_treatments.iloc[i,micolix] + pd.tseries.offsets.DateOffset(months = x))\nall_treatments.iloc[newrowix,mocolix] = BOMoffset.rollforward(all_treatments.iloc[newrowix,micolix]+ pd.tseries.offsets.DateOffset(months = 1))\n'];True;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1081;1081;1081;1081;3.0;0;32957441;;1;15;<python><excel><pandas><dataframe><xlsxwriter>;Putting many python pandas dataframes to one excel worksheet;3792.0;"[""# Creating Excel Writer Object from Pandas  \nwriter = pd.ExcelWriter('test.xlsx',engine='xlsxwriter')   \nworkbook=writer.book\nworksheet=workbook.add_worksheet('Validation') \ndf.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)   \nanother_df.to_excel(writer,sheet_name='Validation',startrow=20, startcol=0) \n Sheetname 'Validation', with case ignored, is already in use.\nwriter = pd.ExcelWriter('test.xlsx',engine='xlsxwriter')   # Creating Excel Writer Object from Pandas  \nworkbook=writer.book\ndf.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)   \nanother_df.to_excel(writer,sheet_name='Validation',startrow=20, startcol=0) \n df.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)  \n""]";"[""# Creating Excel Writer Object from Pandas  \nwriter = pd.ExcelWriter('test.xlsx',engine='xlsxwriter')   \nworkbook=writer.book\nworksheet=workbook.add_worksheet('Validation') \ndf.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)   \nanother_df.to_excel(writer,sheet_name='Validation',startrow=20, startcol=0) \n"", "" Sheetname 'Validation', with case ignored, is already in use.\n"", ""writer = pd.ExcelWriter('test.xlsx',engine='xlsxwriter')   # Creating Excel Writer Object from Pandas  \nworkbook=writer.book\ndf.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)   \nanother_df.to_excel(writer,sheet_name='Validation',startrow=20, startcol=0) \n"", "" df.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)  \n""]";"[""# Creating Excel Writer Object from Pandas  \nwriter = pd.ExcelWriter('test.xlsx',engine='xlsxwriter')   \nworkbook=writer.book\nworksheet=workbook.add_worksheet('Validation') \ndf.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)   \nanother_df.to_excel(writer,sheet_name='Validation',startrow=20, startcol=0) \n"", "" Sheetname 'Validation', with case ignored, is already in use.\n"", ""writer = pd.ExcelWriter('test.xlsx',engine='xlsxwriter')   # Creating Excel Writer Object from Pandas  \nworkbook=writer.book\ndf.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)   \nanother_df.to_excel(writer,sheet_name='Validation',startrow=20, startcol=0) \n"", "" df.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)  \n""]";"[""# Creating Excel Writer Object from Pandas  \nwriter = pd.ExcelWriter('test.xlsx',engine='xlsxwriter')   \nworkbook=writer.book\nworksheet=workbook.add_worksheet('Validation') \ndf.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)   \nanother_df.to_excel(writer,sheet_name='Validation',startrow=20, startcol=0) \nwriter = pd.ExcelWriter('test.xlsx',engine='xlsxwriter')   # Creating Excel Writer Object from Pandas  \nworkbook=writer.book\ndf.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)   \nanother_df.to_excel(writer,sheet_name='Validation',startrow=20, startcol=0) \n""]";"[""import pandas as pd\n# Creating Excel Writer Object from Pandas  \nwriter = pd.ExcelWriter('test.xlsx',engine='xlsxwriter')   \nworkbook=writer.book\nworksheet=workbook.add_worksheet('Validation') \ndf.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)   \nanother_df.to_excel(writer,sheet_name='Validation',startrow=20, startcol=0) \nwriter = pd.ExcelWriter('test.xlsx',engine='xlsxwriter')   # Creating Excel Writer Object from Pandas  \nworkbook=writer.book\ndf.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)   \nanother_df.to_excel(writer,sheet_name='Validation',startrow=20, startcol=0) \n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\n# Creating Excel Writer Object from Pandas  \nwriter = pd.ExcelWriter('test.xlsx',engine='xlsxwriter')   \nworkbook=writer.book\nworksheet=workbook.add_worksheet('Validation') \ndf.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)   \nanother_df.to_excel(writer,sheet_name='Validation',startrow=20, startcol=0) \nwriter = pd.ExcelWriter('test.xlsx',engine='xlsxwriter')   # Creating Excel Writer Object from Pandas  \nworkbook=writer.book\ndf.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)   \nanother_df.to_excel(writer,sheet_name='Validation',startrow=20, startcol=0) \n""]";True;0;1;"[""name 'multiple_dfs' is not defined""]";['NameError'];0;1;"[""name 'multiple_dfs' is not defined""]";['NameError'];0;1;"[""name 'multiple_dfs' is not defined""]";['NameError']
1082;1082;1082;1082;3.0;0;33086881;;1;12;<python><pandas><merge><dataframe>;Merge two python pandas data frames of different length but keep all rows in output data frame;10678.0;['df1:                                 df2:\n\n      Column1  Column2  Column3           ColumnA  ColumnB ColumnC\n    0    a        x        x            0    c        y       y\n    1    c        x        x            1    e        z       z\n    2    e        x        x            2    a        s       s\n    3    d        x        x            3    d        f       f\n    4    h        x        x\n    5    k        x        x            \ndf1:\n    Column1  Column2  Column3  ColumnB  ColumnC\n  0    a        x        x        s        s\n  1    c        x        x        y        y\n  2    e        x        x        z        z\n  3    d        x        x        f        f\n  4    h        x        x        NaN      NaN\n  5    k        x        x        NaN      NaN\n'];['df1:                                 df2:\n\n      Column1  Column2  Column3           ColumnA  ColumnB ColumnC\n    0    a        x        x            0    c        y       y\n    1    c        x        x            1    e        z       z\n    2    e        x        x            2    a        s       s\n    3    d        x        x            3    d        f       f\n    4    h        x        x\n    5    k        x        x            \n', 'df1:\n    Column1  Column2  Column3  ColumnB  ColumnC\n  0    a        x        x        s        s\n  1    c        x        x        y        y\n  2    e        x        x        z        z\n  3    d        x        x        f        f\n  4    h        x        x        NaN      NaN\n  5    k        x        x        NaN      NaN\n'];"['df1:                                 df2:\n\n      Column1  Column2  Column3           ColumnA  ColumnB ColumnC\n    0    a        x        x            0    c        y       y\n    1    c        x        x            1    e        z       z\n    2    e        x        x            2    a        s       s\n    3    d        x        x            3    d        f       f\n    4    h        x        x\n    5    k        x        x            \n', 'df1:\n    Column1  Column2  Column3  ColumnB  ColumnC\n  0    a        x        x        s        s\n  1    c        x        x        y        y\n  2    e        x        x        z        z\n  3    d        x        x        f        f\n  4    h        x        x        NaN      NaN\n  5    k        x        x        NaN      NaN\n', ""df1.merge(df2,left_on='Column1', right_on='ColumnA')""]";['\n'];['\n'];False;['import pandas as pd\n\n'];False;0;1;"[""name 'df1' is not defined""]";['NameError'];0;1;"[""name 'df1' is not defined""]";['NameError'];0;1;"[""'ColumnA'""]";['KeyError']
1083;1083;1083;1083;1.0;2;33088010;;1;12;<python><pandas>;Pandas column bind (cbind) two data frames;8286.0;['    unique_id lacet_number \n15    5570613  TLA-0138365 \n24    5025490  EMP-0138757 \n36    4354431  DXN-0025343 \n     latitude  longitude \n0  -93.193560  31.217029  \n1  -93.948082  35.360874  \n2 -103.131508  37.787609  \n    unique_id lacet_number      latitude  longitude \n0     5570613  TLA-0138365    -93.193560  31.217029  \n1     5025490  EMP-0138757    -93.948082  35.360874  \n2     4354431  DXN-0025343   -103.131508  37.787609  \ndf_c = pd.concat([df_a, df_b], axis=1)\n    unique_id lacet_number    latitude  longitude\n0         NaN          NaN  -93.193560  31.217029\n1         NaN          NaN  -93.948082  35.360874\n2         NaN          NaN -103.131508  37.787609\n15    5570613  TLA-0138365         NaN        NaN\n24    5025490  EMP-0138757         NaN        NaN\n36    4354431  DXN-0025343         NaN        NaN\n'];['    unique_id lacet_number \n15    5570613  TLA-0138365 \n24    5025490  EMP-0138757 \n36    4354431  DXN-0025343 \n', '     latitude  longitude \n0  -93.193560  31.217029  \n1  -93.948082  35.360874  \n2 -103.131508  37.787609  \n', '    unique_id lacet_number      latitude  longitude \n0     5570613  TLA-0138365    -93.193560  31.217029  \n1     5025490  EMP-0138757    -93.948082  35.360874  \n2     4354431  DXN-0025343   -103.131508  37.787609  \n', 'df_c = pd.concat([df_a, df_b], axis=1)\n', '    unique_id lacet_number    latitude  longitude\n0         NaN          NaN  -93.193560  31.217029\n1         NaN          NaN  -93.948082  35.360874\n2         NaN          NaN -103.131508  37.787609\n15    5570613  TLA-0138365         NaN        NaN\n24    5025490  EMP-0138757         NaN        NaN\n36    4354431  DXN-0025343         NaN        NaN\n'];['df_a', '    unique_id lacet_number \n15    5570613  TLA-0138365 \n24    5025490  EMP-0138757 \n36    4354431  DXN-0025343 \n', 'df_b', 'df_a', '     latitude  longitude \n0  -93.193560  31.217029  \n1  -93.948082  35.360874  \n2 -103.131508  37.787609  \n', '    unique_id lacet_number      latitude  longitude \n0     5570613  TLA-0138365    -93.193560  31.217029  \n1     5025490  EMP-0138757    -93.948082  35.360874  \n2     4354431  DXN-0025343   -103.131508  37.787609  \n', 'df_c = pd.concat([df_a, df_b], axis=1)\n', '    unique_id lacet_number    latitude  longitude\n0         NaN          NaN  -93.193560  31.217029\n1         NaN          NaN  -93.948082  35.360874\n2         NaN          NaN -103.131508  37.787609\n15    5570613  TLA-0138365         NaN        NaN\n24    5025490  EMP-0138757         NaN        NaN\n36    4354431  DXN-0025343         NaN        NaN\n'];['df_c = pd.concat([df_a, df_b], axis=1)\n'];['import pandas as pd\ndf_c = pd.concat([df_a, df_b], axis=1)\n'];True;['import pandas as pd\ndf = pd.DataFrame()\ndf_c = pd.concat([df_a, df_b], axis=1)\n'];True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'df_a' is not defined""]";['NameError'];0;1;"[""name 'df_a' is not defined""]";['NameError']
1084;1084;1084;1084;1.0;0;33109107;;1;13;<python><numpy><pandas><scipy>;What is the difference between skew and kurtosis functions in pandas vs. scipy?;5608.0;"['import pandas as pd\nimport scipy.stats.stats as st\n\nheights = np.array([1.46, 1.79, 2.01, 1.75, 1.56, 1.69, 1.88, 1.76, 1.88, 1.78])\n\nprint ""skewness:"", st.skew(heights)\nprint ""kurtosis:"", st.kurtosis(heights)\nskewness: -0.393524456473\nkurtosis: -0.330672097724\nheights_df = pd.DataFrame(heights)\nprint ""skewness:"", heights_df.skew()\nprint ""kurtosis:"", heights_df.kurtosis() \nskewness: 0   -0.466663\nkurtosis: 0    0.379705\n']";"['import pandas as pd\nimport scipy.stats.stats as st\n\nheights = np.array([1.46, 1.79, 2.01, 1.75, 1.56, 1.69, 1.88, 1.76, 1.88, 1.78])\n\nprint ""skewness:"", st.skew(heights)\nprint ""kurtosis:"", st.kurtosis(heights)\n', 'skewness: -0.393524456473\nkurtosis: -0.330672097724\n', 'heights_df = pd.DataFrame(heights)\nprint ""skewness:"", heights_df.skew()\nprint ""kurtosis:"", heights_df.kurtosis() \n', 'skewness: 0   -0.466663\nkurtosis: 0    0.379705\n']";"['import pandas as pd\nimport scipy.stats.stats as st\n\nheights = np.array([1.46, 1.79, 2.01, 1.75, 1.56, 1.69, 1.88, 1.76, 1.88, 1.78])\n\nprint ""skewness:"", st.skew(heights)\nprint ""kurtosis:"", st.kurtosis(heights)\n', 'skewness: -0.393524456473\nkurtosis: -0.330672097724\n', 'heights_df = pd.DataFrame(heights)\nprint ""skewness:"", heights_df.skew()\nprint ""kurtosis:"", heights_df.kurtosis() \n', 'skewness: 0   -0.466663\nkurtosis: 0    0.379705\n']";['import pandas as pd\nimport scipy.stats.stats as st\n\nheights = np.array([1.46, 1.79, 2.01, 1.75, 1.56, 1.69, 1.88, 1.76, 1.88, 1.78])\n\nheights_df = pd.DataFrame(heights)\n'];['import pandas as pd\nimport scipy.stats.stats as st\n\nheights = np.array([1.46, 1.79, 2.01, 1.75, 1.56, 1.69, 1.88, 1.76, 1.88, 1.78])\n\nheights_df = pd.DataFrame(heights)\n'];False;['import pandas as pd\nimport pandas as pd\nimport scipy.stats.stats as st\n\nheights = np.array([1.46, 1.79, 2.01, 1.75, 1.56, 1.69, 1.88, 1.76, 1.88, 1.78])\n\nheights_df = pd.DataFrame(heights)\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1085;1085;1085;1085;4.0;6;33126477;;1;13;<python><pandas>;"Pandas "".convert_objects(convert_numeric=True)"" deprecated";6249.0;"['data[""S1Q2I""] = data[""S1Q2I""].convert_objects(convert_numeric=True)\nFutureWarning: convert_objects is deprecated.  \nUse the data-type specific converters pd.to_datetime, \npd.to_timedelta and pd.to_numeric. \ndata[""S3BD5Q2A""] = data[""S3BD5Q2A""].convert_objects(convert_numeric=True)\n']";"['data[""S1Q2I""] = data[""S1Q2I""].convert_objects(convert_numeric=True)\n', 'FutureWarning: convert_objects is deprecated.  \nUse the data-type specific converters pd.to_datetime, \npd.to_timedelta and pd.to_numeric. \ndata[""S3BD5Q2A""] = data[""S3BD5Q2A""].convert_objects(convert_numeric=True)\n']";"['data[""S1Q2I""] = data[""S1Q2I""].convert_objects(convert_numeric=True)\n', 'FutureWarning: convert_objects is deprecated.  \nUse the data-type specific converters pd.to_datetime, \npd.to_timedelta and pd.to_numeric. \ndata[""S3BD5Q2A""] = data[""S3BD5Q2A""].convert_objects(convert_numeric=True)\n']";"['data[""S1Q2I""] = data[""S1Q2I""].convert_objects(convert_numeric=True)\ndata[""S3BD5Q2A""] = data[""S3BD5Q2A""].convert_objects(convert_numeric=True)\n']";"['data[""S1Q2I""] = data[""S1Q2I""].convert_objects(convert_numeric=True)\ndata[""S3BD5Q2A""] = data[""S3BD5Q2A""].convert_objects(convert_numeric=True)\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\ndata[""S1Q2I""] = data[""S1Q2I""].convert_objects(convert_numeric=True)\ndata[""S3BD5Q2A""] = data[""S3BD5Q2A""].convert_objects(convert_numeric=True)\n']";True;0;0;[];[];0;0;[];[];0;0;[];[]
1086;1086;1086;1086;1.0;5;33159634;;1;11;<python><pandas>;pandas v0.17.0: AttributeError: 'unicode' object has no attribute 'version';4192.0;"['>>> import pandas\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/__init__.py"", line 44, in <module>\n    from pandas.core.api import *\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/core/api.py"", line 9, in <module>\n    from pandas.core.groupby import Grouper\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/core/groupby.py"", line 16, in <module>\n    from pandas.core.frame import DataFrame\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/core/frame.py"", line 41, in <module>\n    from pandas.core.series import Series\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/core/series.py"", line 2864, in <module>\n    import pandas.tools.plotting as _gfx\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/tools/plotting.py"", line 135, in <module>\n    if _mpl_ge_1_5_0():\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/tools/plotting.py"", line 130, in _mpl_ge_1_5_0\n    return (matplotlib.__version__  >= LooseVersion(\'1.5\')\n  File ""/usr/lib64/python2.7/distutils/version.py"", line 296, in __cmp__\n    return cmp(self.version, other.version)\nAttributeError: \'unicode\' object has no attribute \'version\'\n']";"['>>> import pandas\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/__init__.py"", line 44, in <module>\n    from pandas.core.api import *\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/core/api.py"", line 9, in <module>\n    from pandas.core.groupby import Grouper\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/core/groupby.py"", line 16, in <module>\n    from pandas.core.frame import DataFrame\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/core/frame.py"", line 41, in <module>\n    from pandas.core.series import Series\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/core/series.py"", line 2864, in <module>\n    import pandas.tools.plotting as _gfx\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/tools/plotting.py"", line 135, in <module>\n    if _mpl_ge_1_5_0():\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/tools/plotting.py"", line 130, in _mpl_ge_1_5_0\n    return (matplotlib.__version__  >= LooseVersion(\'1.5\')\n  File ""/usr/lib64/python2.7/distutils/version.py"", line 296, in __cmp__\n    return cmp(self.version, other.version)\nAttributeError: \'unicode\' object has no attribute \'version\'\n']";"['>>> import pandas\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/__init__.py"", line 44, in <module>\n    from pandas.core.api import *\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/core/api.py"", line 9, in <module>\n    from pandas.core.groupby import Grouper\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/core/groupby.py"", line 16, in <module>\n    from pandas.core.frame import DataFrame\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/core/frame.py"", line 41, in <module>\n    from pandas.core.series import Series\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/core/series.py"", line 2864, in <module>\n    import pandas.tools.plotting as _gfx\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/tools/plotting.py"", line 135, in <module>\n    if _mpl_ge_1_5_0():\n  File ""/usr/lib64/python2.7/site-packages/pandas-0.17.0-py2.7-linux-x86_64.egg/pandas/tools/plotting.py"", line 130, in _mpl_ge_1_5_0\n    return (matplotlib.__version__  >= LooseVersion(\'1.5\')\n  File ""/usr/lib64/python2.7/distutils/version.py"", line 296, in __cmp__\n    return cmp(self.version, other.version)\nAttributeError: \'unicode\' object has no attribute \'version\'\n']";[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1087;1087;1087;1087;2.0;0;33165734;;1;13;<python><pandas>;Update index after sorting data-frame;6572.0;"['x = np.tile(np.arange(3),3)\ny = np.repeat(np.arange(3),3)\ndf = pd.DataFrame({""x"": x, ""y"": y})\n   x  y\n0  0  0\n1  1  0\n2  2  0\n3  0  1\n4  1  1\n5  2  1\n6  0  2\n7  1  2\n8  2  2\ndf2 = df.sort([""x"", ""y""])   x  y\n0  0  0\n3  0  1\n6  0  2\n1  1  0\n4  1  1\n7  1  2\n2  2  0\n5  2  1\n8  2  2\n   x  y\n0  0  0\n1  0  1\n2  0  2\n3  1  0\n4  1  1\n5  1  2\n6  2  0\n7  2  1\n8  2  2\ndf2.reindex(np.arange(len(df2.index)))\n']";"['x = np.tile(np.arange(3),3)\ny = np.repeat(np.arange(3),3)\ndf = pd.DataFrame({""x"": x, ""y"": y})\n', '   x  y\n0  0  0\n1  1  0\n2  2  0\n3  0  1\n4  1  1\n5  2  1\n6  0  2\n7  1  2\n8  2  2\n', 'df2 = df.sort([""x"", ""y""])', '   x  y\n0  0  0\n3  0  1\n6  0  2\n1  1  0\n4  1  1\n7  1  2\n2  2  0\n5  2  1\n8  2  2\n', '   x  y\n0  0  0\n1  0  1\n2  0  2\n3  1  0\n4  1  1\n5  1  2\n6  2  0\n7  2  1\n8  2  2\n', 'df2.reindex(np.arange(len(df2.index)))\n']";"['x = np.tile(np.arange(3),3)\ny = np.repeat(np.arange(3),3)\ndf = pd.DataFrame({""x"": x, ""y"": y})\n', '   x  y\n0  0  0\n1  1  0\n2  2  0\n3  0  1\n4  1  1\n5  2  1\n6  0  2\n7  1  2\n8  2  2\n', 'x', 'y', 'df2 = df.sort([""x"", ""y""])', '   x  y\n0  0  0\n3  0  1\n6  0  2\n1  1  0\n4  1  1\n7  1  2\n2  2  0\n5  2  1\n8  2  2\n', '   x  y\n0  0  0\n1  0  1\n2  0  2\n3  1  0\n4  1  1\n5  1  2\n6  2  0\n7  2  1\n8  2  2\n', 'df2.reindex(np.arange(len(df2.index)))\n']";"['x = np.tile(np.arange(3),3)\ny = np.repeat(np.arange(3),3)\ndf = pd.DataFrame({""x"": x, ""y"": y})\ndf2.reindex(np.arange(len(df2.index)))\n']";"['import pandas as pd\nx = np.tile(np.arange(3),3)\ny = np.repeat(np.arange(3),3)\ndf = pd.DataFrame({""x"": x, ""y"": y})\ndf2.reindex(np.arange(len(df2.index)))\n']";True;"['import pandas as pd\ndf2 = pd.DataFrame()\nx = np.tile(np.arange(3),3)\ny = np.repeat(np.arange(3),3)\ndf = pd.DataFrame({""x"": x, ""y"": y})\ndf2.reindex(np.arange(len(df2.index)))\n']";True;0;1;"[""name 'df2' is not defined""]";['NameError'];0;1;"[""name 'df2' is not defined""]";['NameError'];0;1;"[""name 'df2' is not defined""]";['NameError']
1088;1088;1088;1088;3.0;2;33179122;;1;14;<python><pandas><matplotlib><data-visualization><seaborn>;Seaborn: countplot() with frequencies;9917.0;"['plt.figure(figsize=(12,8))\nax = sns.countplot(x=""AXLES"", data=dfWIM, order=[3,4,5,6,7,8,9,10,11,12])\nplt.title(\'Distribution of Truck Configurations\')\nplt.xlabel(\'Number of Axles\')\nplt.ylabel(\'Frequency [%]\')\n\nfor p in ax.patches:\n        ax.annotate(\'%{:.1f}\'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))\nplt.figure(figsize=(12,8))\nplt.title(\'Distribution of Truck Configurations\')\nplt.xlabel(\'Number of Axles\')\nplt.ylabel(\'Frequency [%]\')\n\nax = (dfWIM.AXLES.value_counts()/len(df)*100).sort_index().plot(kind=""bar"", rot=0)\nax.set_yticks(np.arange(0, 110, 10))\n\nax2 = ax.twinx()\nax2.set_yticks(np.arange(0, 110, 10)*len(df)/100)\n\nfor p in ax.patches:\n    ax.annotate(\'{:.2f}%\'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))\n']";"['plt.figure(figsize=(12,8))\nax = sns.countplot(x=""AXLES"", data=dfWIM, order=[3,4,5,6,7,8,9,10,11,12])\nplt.title(\'Distribution of Truck Configurations\')\nplt.xlabel(\'Number of Axles\')\nplt.ylabel(\'Frequency [%]\')\n\nfor p in ax.patches:\n        ax.annotate(\'%{:.1f}\'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))\n', 'plt.figure(figsize=(12,8))\nplt.title(\'Distribution of Truck Configurations\')\nplt.xlabel(\'Number of Axles\')\nplt.ylabel(\'Frequency [%]\')\n\nax = (dfWIM.AXLES.value_counts()/len(df)*100).sort_index().plot(kind=""bar"", rot=0)\nax.set_yticks(np.arange(0, 110, 10))\n\nax2 = ax.twinx()\nax2.set_yticks(np.arange(0, 110, 10)*len(df)/100)\n\nfor p in ax.patches:\n    ax.annotate(\'{:.2f}%\'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))\n']";"['df.AXLES.value_counts()/len(df.index)', 'countplot()', 'plt.figure(figsize=(12,8))\nax = sns.countplot(x=""AXLES"", data=dfWIM, order=[3,4,5,6,7,8,9,10,11,12])\nplt.title(\'Distribution of Truck Configurations\')\nplt.xlabel(\'Number of Axles\')\nplt.ylabel(\'Frequency [%]\')\n\nfor p in ax.patches:\n        ax.annotate(\'%{:.1f}\'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))\n', 'order', 'plt.figure(figsize=(12,8))\nplt.title(\'Distribution of Truck Configurations\')\nplt.xlabel(\'Number of Axles\')\nplt.ylabel(\'Frequency [%]\')\n\nax = (dfWIM.AXLES.value_counts()/len(df)*100).sort_index().plot(kind=""bar"", rot=0)\nax.set_yticks(np.arange(0, 110, 10))\n\nax2 = ax.twinx()\nax2.set_yticks(np.arange(0, 110, 10)*len(df)/100)\n\nfor p in ax.patches:\n    ax.annotate(\'{:.2f}%\'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))\n']";"['plt.figure(figsize=(12,8))\nax = sns.countplot(x=""AXLES"", data=dfWIM, order=[3,4,5,6,7,8,9,10,11,12])\nplt.title(\'Distribution of Truck Configurations\')\nplt.xlabel(\'Number of Axles\')\nplt.ylabel(\'Frequency [%]\')\n\nplt.figure(figsize=(12,8))\nplt.title(\'Distribution of Truck Configurations\')\nplt.xlabel(\'Number of Axles\')\nplt.ylabel(\'Frequency [%]\')\n\nax = (dfWIM.AXLES.value_counts()/len(df)*100).sort_index().plot(kind=""bar"", rot=0)\nax.set_yticks(np.arange(0, 110, 10))\n\nax2 = ax.twinx()\nax2.set_yticks(np.arange(0, 110, 10)*len(df)/100)\n\n']";"['plt.figure(figsize=(12,8))\nax = sns.countplot(x=""AXLES"", data=dfWIM, order=[3,4,5,6,7,8,9,10,11,12])\nplt.title(\'Distribution of Truck Configurations\')\nplt.xlabel(\'Number of Axles\')\nplt.ylabel(\'Frequency [%]\')\n\nplt.figure(figsize=(12,8))\nplt.title(\'Distribution of Truck Configurations\')\nplt.xlabel(\'Number of Axles\')\nplt.ylabel(\'Frequency [%]\')\n\nax = (dfWIM.AXLES.value_counts()/len(df)*100).sort_index().plot(kind=""bar"", rot=0)\nax.set_yticks(np.arange(0, 110, 10))\n\nax2 = ax.twinx()\nax2.set_yticks(np.arange(0, 110, 10)*len(df)/100)\n\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\nplt.figure(figsize=(12,8))\nax = sns.countplot(x=""AXLES"", data=dfWIM, order=[3,4,5,6,7,8,9,10,11,12])\nplt.title(\'Distribution of Truck Configurations\')\nplt.xlabel(\'Number of Axles\')\nplt.ylabel(\'Frequency [%]\')\n\nplt.figure(figsize=(12,8))\nplt.title(\'Distribution of Truck Configurations\')\nplt.xlabel(\'Number of Axles\')\nplt.ylabel(\'Frequency [%]\')\n\nax = (dfWIM.AXLES.value_counts()/len(df)*100).sort_index().plot(kind=""bar"", rot=0)\nax.set_yticks(np.arange(0, 110, 10))\n\nax2 = ax.twinx()\nax2.set_yticks(np.arange(0, 110, 10)*len(df)/100)\n\n']";True;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
1089;1089;1089;1089;3.0;0;33246771;;1;12;<python><pandas><dataframe><series>;Convert pandas data frame to series;30364.0;[''];[];['pd.Series(myResults)', 'ValueError: cannot copy sequence with size 23 to array axis with dimension 1'];[''];[''];False;['import pandas as pd\n'];False;1;2;"['Sucess', ""name 'pd' is not defined""]";['Sucess', 'NameError'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
1090;1090;1090;1090;2.0;0;33271098;;1;15;<python><pandas><group-by><dataframe>;Python: get a frequency count based on two columns (variables) in pandas dataframe;8713.0;['    Group           Size\n\n    Short          Small\n    Short          Small\n    Moderate       Medium\n    Moderate       Small\n    Tall           Large\n    Group           Size      Time\n\n    Short          Small        2\n    Moderate       Medium       1 \n    Moderate       Small        1\n    Tall           Large        1\n'];['    Group           Size\n\n    Short          Small\n    Short          Small\n    Moderate       Medium\n    Moderate       Small\n    Tall           Large\n', '    Group           Size      Time\n\n    Short          Small        2\n    Moderate       Medium       1 \n    Moderate       Small        1\n    Tall           Large        1\n'];['    Group           Size\n\n    Short          Small\n    Short          Small\n    Moderate       Medium\n    Moderate       Small\n    Tall           Large\n', '    Group           Size      Time\n\n    Short          Small        2\n    Moderate       Medium       1 \n    Moderate       Small        1\n    Tall           Large        1\n'];['\n\n'];['\n\n'];False;['import pandas as pd\n\n\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'Group'""]";['KeyError']
1091;1091;1091;1091;2.0;3;33311616;;1;12;<python><shapely><geopandas>;Find Coordinate of Closest Point on Polygon Shapely;3717.0;['>>> poly = Polygon([(0, 0), (2,8), (14, 10), (6,1)])\n>>> point=Point(12,4)\n>>> dist=point.distance(poly)\n>>> print dist\n2.49136439561\n>>> buff=point.buffer(dist) \n'];['>>> poly = Polygon([(0, 0), (2,8), (14, 10), (6,1)])\n>>> point=Point(12,4)\n', '>>> dist=point.distance(poly)\n>>> print dist\n2.49136439561\n', '>>> buff=point.buffer(dist) \n'];['>>> poly = Polygon([(0, 0), (2,8), (14, 10), (6,1)])\n>>> point=Point(12,4)\n', '>>> dist=point.distance(poly)\n>>> print dist\n2.49136439561\n', '>>> buff=point.buffer(dist) \n', 'list(poly.intersection(buff))'];['2.49136439561\n'];['2.49136439561\n'];False;['import pandas as pd\n2.49136439561\n'];False;0;1;"[""No module named 'shapely'""]";['ImportError'];0;1;"[""No module named 'shapely'""]";['ImportError'];0;1;"[""No module named 'shapely'""]";['ImportError']
1092;1092;1092;1092;3.0;1;33346591;;1;13;<python><pandas><numpy><difference>;What is the difference between size and count in pandas?;2168.0;[''];[];"['groupby(""x"").count', 'groupby(""x"").size']";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
1093;1093;1093;1093;3.0;2;33483670;;1;14;<python><pandas><group-by><series>;How to group a Series by values in pandas?;5479.0;['grouped = s.groupby(lambda x: x.date())\n'];['grouped = s.groupby(lambda x: x.date())\n'];['Series', 'Timestamp', 'grouped = s.groupby(lambda x: x.date())\n', 'groupby'];['grouped = s.groupby(lambda x: x.date())\n'];['grouped = s.groupby(lambda x: x.date())\n'];False;['import pandas as pd\ngrouped = s.groupby(lambda x: x.date())\n'];False;0;1;"[""name 's' is not defined""]";['NameError'];0;1;"[""name 's' is not defined""]";['NameError'];0;1;"[""name 's' is not defined""]";['NameError']
1094;1094;1094;1094;2.0;4;33505245;;1;11;<python><pandas><matplotlib>;How to change order of plots in pandas hist command;447.0;[''];[];"['""Feature_1"",""Feature_2"",....""Feature_25""', 'df.hist()', '""Feature_1"",""""Feature_10"",""Feature_11""...""Feature_2"",""Feature_20"",...']";[''];[''];False;['import pandas as pd\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
1095;1095;1095;1095;1.0;6;33530753;;1;45;<python><arrays><numpy><pandas>;numpy: Reliable (non-conservative) indicator if numpy array is view;839.0;[''];[];"['pandas', 'my_array.base is not None', 'numpy.may_share_memory()', ""flags['OWNDATA'])""]";[''];[''];False;['import pandas as pd\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
1096;1096;1096;1096;4.0;4;33768122;;1;13;<python><pandas>;Python: Pandas Dataframe how to multiply entire column with a scalar;19188.0;"[""df['quantity'] *= -1 # trying to multiply each row's quantity column with -1\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\nfor idx, row in df.iterrows():\n    df.loc[idx, 'quantity'] *= -1\n SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self.obj[item] = s\n""]";"[""df['quantity'] *= -1 # trying to multiply each row's quantity column with -1\n"", 'A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n', ""for idx, row in df.iterrows():\n    df.loc[idx, 'quantity'] *= -1\n"", ' SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self.obj[item] = s\n']";"[""df['quantity'] *= -1 # trying to multiply each row's quantity column with -1\n"", 'A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n', ""for idx, row in df.iterrows():\n    df.loc[idx, 'quantity'] *= -1\n"", '0.16.2', ' SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self.obj[item] = s\n']";"[""df['quantity'] *= -1 # trying to multiply each row's quantity column with -1\n\n""]";"[""df['quantity'] *= -1 # trying to multiply each row's quantity column with -1\n\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf['quantity'] *= -1 # trying to multiply each row's quantity column with -1\n\n""]";True;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""'quantity'"", ""'the label [quantity] is not in the [columns]'""]";['KeyError', 'KeyError']
1097;1097;1097;1097;1.0;5;33813815;;1;11;<python><pandas><parquet><blaze>;Simplest way to read Parquet file into Pandas DataFrame;11061.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1098;1098;1098;1098;3.0;0;33869292;;1;11;<python><pandas><ipython-notebook><bokeh>;How can I set the x-axis as datetimes on a bokeh plot?;6021.0;"[""import pandas as pd\nfrom datetime import datetime as dt\nfrom bokeh.io import output_notebook\nfrom bokeh.charts import Bar, Line, show\n\ndf = pd.DataFrame(data=[1,2,3],\n                  index=[dt(2015, 1, 1), dt(2015, 1, 2), dt(2015, 1, 3)],\n                  columns=['foo'])\n\noutput_notebook()\nshow(Line(df))\n""]";"[""import pandas as pd\nfrom datetime import datetime as dt\nfrom bokeh.io import output_notebook\nfrom bokeh.charts import Bar, Line, show\n\ndf = pd.DataFrame(data=[1,2,3],\n                  index=[dt(2015, 1, 1), dt(2015, 1, 2), dt(2015, 1, 3)],\n                  columns=['foo'])\n\noutput_notebook()\nshow(Line(df))\n""]";"[""import pandas as pd\nfrom datetime import datetime as dt\nfrom bokeh.io import output_notebook\nfrom bokeh.charts import Bar, Line, show\n\ndf = pd.DataFrame(data=[1,2,3],\n                  index=[dt(2015, 1, 1), dt(2015, 1, 2), dt(2015, 1, 3)],\n                  columns=['foo'])\n\noutput_notebook()\nshow(Line(df))\n""]";['import pandas as pd\nfrom datetime import datetime as dt\nfrom bokeh.io import output_notebook\nfrom bokeh.charts import Bar, Line, show\n\n\noutput_notebook()\nshow(Line(df))\n'];['import pandas as pd\nfrom datetime import datetime as dt\nfrom bokeh.io import output_notebook\nfrom bokeh.charts import Bar, Line, show\n\n\noutput_notebook()\nshow(Line(df))\n'];False;['import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\nfrom datetime import datetime as dt\nfrom bokeh.io import output_notebook\nfrom bokeh.charts import Bar, Line, show\n\n\noutput_notebook()\nshow(Line(df))\n'];True;0;1;"[""No module named 'bokeh'""]";['ImportError'];0;1;"[""No module named 'bokeh'""]";['ImportError'];0;1;"[""No module named 'bokeh'""]";['ImportError']
1099;1099;1099;1099;2.0;4;33952142;;1;12;<python><pandas>;Prevent pandas from interpreting 'NA' as NaN in a string;2275.0;"[""import pandas as pd\n\ndf = pd.read_csv(\n    'sample.tsv',\n    sep='\\t',\n    encoding='utf-8',\n)\n\nfor df_tuples in df.itertuples(index=True):\n    print(df_tuples)\n""]";"[""import pandas as pd\n\ndf = pd.read_csv(\n    'sample.tsv',\n    sep='\\t',\n    encoding='utf-8',\n)\n\nfor df_tuples in df.itertuples(index=True):\n    print(df_tuples)\n""]";"[""import pandas as pd\n\ndf = pd.read_csv(\n    'sample.tsv',\n    sep='\\t',\n    encoding='utf-8',\n)\n\nfor df_tuples in df.itertuples(index=True):\n    print(df_tuples)\n"", ""quotechar='\\''"", 'dtype=dict(valid_cols)']";['import pandas as pd\n\n\n'];['import pandas as pd\n\n\n'];False;['import pandas as pd\nimport pandas as pd\n\n\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'DataFrame' object has no attribute 'CHAIN'""]";['AttributeError']
1100;1100;1100;1100;4.0;2;33957720;;1;20;<python><object><pandas>;How to convert column with dtype as object to string in Pandas Dataframe;22670.0;"[""a=lambda x: str(x).split(',')\ndf['column'].apply(a)\ndf['column'].astype(str)\n""]";"[""a=lambda x: str(x).split(',')\ndf['column'].apply(a)\n"", ""df['column'].astype(str)\n""]";"[""a=lambda x: str(x).split(',')\ndf['column'].apply(a)\n"", ""df['column'].astype(str)\n""]";"[""a=lambda x: str(x).split(',')\ndf['column'].apply(a)\ndf['column'].astype(str)\n""]";"[""a=lambda x: str(x).split(',')\ndf['column'].apply(a)\ndf['column'].astype(str)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\na=lambda x: str(x).split(',')\ndf['column'].apply(a)\ndf['column'].astype(str)\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'column'""]";['KeyError']
1101;1101;1101;1101;1.0;5;33961756;;1;16;<pandas><pylint>;Disabling Pylint no member- E1101 error for specific libraries;3680.0;"['import pandas.io.data\nimport pandas as pd\nspy = pandas.io.data.DataReader(""SPY"", ""yahoo"")\nspy.to_csv(""test.csv"")\nspy = pd.read_csv(""test.csv"")\nclose_px = spy.ix[""2012"":]\nE:  6,11: Instance of \'tuple\' has no \'ix\' member (no-member)\nE:  6,11: Instance of \'TextFileReader\' has no \'ix\' member (no-member)\n']";"['import pandas.io.data\nimport pandas as pd\nspy = pandas.io.data.DataReader(""SPY"", ""yahoo"")\nspy.to_csv(""test.csv"")\nspy = pd.read_csv(""test.csv"")\nclose_px = spy.ix[""2012"":]\n', ""E:  6,11: Instance of 'tuple' has no 'ix' member (no-member)\nE:  6,11: Instance of 'TextFileReader' has no 'ix' member (no-member)\n""]";"['E1101', '#pylint: disable=E1101', 'import pandas.io.data\nimport pandas as pd\nspy = pandas.io.data.DataReader(""SPY"", ""yahoo"")\nspy.to_csv(""test.csv"")\nspy = pd.read_csv(""test.csv"")\nclose_px = spy.ix[""2012"":]\n', ""E:  6,11: Instance of 'tuple' has no 'ix' member (no-member)\nE:  6,11: Instance of 'TextFileReader' has no 'ix' member (no-member)\n""]";"['import pandas.io.data\nimport pandas as pd\nspy = pandas.io.data.DataReader(""SPY"", ""yahoo"")\nspy.to_csv(""test.csv"")\nspy = pd.read_csv(""test.csv"")\nclose_px = spy.ix[""2012"":]\n']";"['import pandas.io.data\nimport pandas as pd\nspy = pandas.io.data.DataReader(""SPY"", ""yahoo"")\nspy.to_csv(""test.csv"")\nspy = pd.read_csv(""test.csv"")\nclose_px = spy.ix[""2012"":]\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\nimport pandas.io.data\nimport pandas as pd\nspy = pandas.io.data.DataReader(""SPY"", ""yahoo"")\nspy.to_csv(""test.csv"")\nspy = pd.read_csv(""test.csv"")\nclose_px = spy.ix[""2012"":]\n']";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1102;1102;1102;1102;4.0;0;34001922;;1;21;<python><pandas><classification><tensorflow>;FailedPreconditionError: Attempting to use uninitialized in Tensorflow;20199.0;"['# Stuff from tensorflow tutorial \nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\nx = tf.placeholder(""float"", shape=[None, 784])\ny_ = tf.placeholder(""float"", shape=[None, 10])\n\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n\ny = tf.nn.softmax(tf.matmul(x,W) + b)\n\ncross_entropy = -tf.reduce_sum(y_*tf.log(y))\n\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n# Read dataframe from training data\ncsvfile=\'train.csv\'\nfrom pandas import DataFrame, read_csv\ndf = read_csv(csvfile)\n\n# Strip off the target data and make it a separate dataframe.\nTarget=df.label\ndel df[""label""]\n\n# Split data into training and testing sets\nmsk = np.random.rand(len(df)) < 0.8\ndfTest = df[~msk]\nTargetTest = Target[~msk]\ndf = df[msk]\nTarget = Target[msk]\n\n# One hot encode the target\nOHTarget=pd.get_dummies(Target)\nOHTargetTest=pd.get_dummies(TargetTest)\nfor i in range(100):\n    batch = np.array(df[i*50:i*50+50].values)\n    batch = np.multiply(batch, 1.0 / 255.0)\n    Target_batch = np.array(OHTarget[i*50:i*50+50].values)\n    Target_batch = np.multiply(Target_batch, 1.0 / 255.0)\n    train_step.run(feed_dict={x: batch, y_: Target_batch})\n---------------------------------------------------------------------------\nFailedPreconditionError                   Traceback (most recent call last)\n<ipython-input-82-967faab7d494> in <module>()\n      4     Target_batch = np.array(OHTarget[i*50:i*50+50].values)\n      5     Target_batch = np.multiply(Target_batch, 1.0 / 255.0)\n----> 6     train_step.run(feed_dict={x: batch, y_: Target_batch})\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in run(self, feed_dict, session)\n   1265         none, the default session will be used.\n   1266     """"""\n-> 1267     _run_using_default_session(self, feed_dict, self.graph, session)\n   1268\n   1269\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in _run_using_default_session(operation, feed_dict, graph, session)\n   2761                        ""the operation\'s graph is different from the session\'s ""\n   2762                        ""graph."")\n-> 2763   session.run(operation, feed_dict)\n   2764\n   2765\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict)\n    343\n    344     # Run request and get response.\n--> 345     results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n    346\n    347     # User may have fetched the same tensor multiple times, but we\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, target_list, fetch_list, feed_dict)\n    417         # pylint: disable=protected-access\n    418         raise errors._make_specific_exception(node_def, op, e.error_message,\n--> 419                                               e.code)\n    420         # pylint: enable=protected-access\n    421       raise e_type, e_value, e_traceback\n\nFailedPreconditionError: Attempting to use uninitialized value Variable_1\n     [[Node: gradients/add_grad/Shape_1 = Shape[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_1)]]\nCaused by op u\'gradients/add_grad/Shape_1\', defined at:\n  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main\n    ...........\n\n...which was originally created as op u\'add\', defined at:\n  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main\n    ""__main__"", fname, loader, pkg_name)\n[elided 17 identical lines from previous traceback]\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File ""<ipython-input-45-59183d86e462>"", line 1, in <module>\n    y = tf.nn.softmax(tf.matmul(x,W) + b)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 403, in binary_op_wrapper\n    return func(x, y, name=name)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 44, in add\n    return _op_def_lib.apply_op(""Add"", x=x, y=y, name=name)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 633, in apply_op\n    op_def=op_def)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1710, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 988, in __init__\n    self._traceback = _extract_stack()\n']";"['# Stuff from tensorflow tutorial \nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\nx = tf.placeholder(""float"", shape=[None, 784])\ny_ = tf.placeholder(""float"", shape=[None, 10])\n\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n\ny = tf.nn.softmax(tf.matmul(x,W) + b)\n\ncross_entropy = -tf.reduce_sum(y_*tf.log(y))\n\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n', '# Read dataframe from training data\ncsvfile=\'train.csv\'\nfrom pandas import DataFrame, read_csv\ndf = read_csv(csvfile)\n\n# Strip off the target data and make it a separate dataframe.\nTarget=df.label\ndel df[""label""]\n\n# Split data into training and testing sets\nmsk = np.random.rand(len(df)) < 0.8\ndfTest = df[~msk]\nTargetTest = Target[~msk]\ndf = df[msk]\nTarget = Target[msk]\n\n# One hot encode the target\nOHTarget=pd.get_dummies(Target)\nOHTargetTest=pd.get_dummies(TargetTest)\n', 'for i in range(100):\n    batch = np.array(df[i*50:i*50+50].values)\n    batch = np.multiply(batch, 1.0 / 255.0)\n    Target_batch = np.array(OHTarget[i*50:i*50+50].values)\n    Target_batch = np.multiply(Target_batch, 1.0 / 255.0)\n    train_step.run(feed_dict={x: batch, y_: Target_batch})\n', '---------------------------------------------------------------------------\nFailedPreconditionError                   Traceback (most recent call last)\n<ipython-input-82-967faab7d494> in <module>()\n      4     Target_batch = np.array(OHTarget[i*50:i*50+50].values)\n      5     Target_batch = np.multiply(Target_batch, 1.0 / 255.0)\n----> 6     train_step.run(feed_dict={x: batch, y_: Target_batch})\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in run(self, feed_dict, session)\n   1265         none, the default session will be used.\n   1266     """"""\n-> 1267     _run_using_default_session(self, feed_dict, self.graph, session)\n   1268\n   1269\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in _run_using_default_session(operation, feed_dict, graph, session)\n   2761                        ""the operation\'s graph is different from the session\'s ""\n   2762                        ""graph."")\n-> 2763   session.run(operation, feed_dict)\n   2764\n   2765\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict)\n    343\n    344     # Run request and get response.\n--> 345     results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n    346\n    347     # User may have fetched the same tensor multiple times, but we\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, target_list, fetch_list, feed_dict)\n    417         # pylint: disable=protected-access\n    418         raise errors._make_specific_exception(node_def, op, e.error_message,\n--> 419                                               e.code)\n    420         # pylint: enable=protected-access\n    421       raise e_type, e_value, e_traceback\n\nFailedPreconditionError: Attempting to use uninitialized value Variable_1\n     [[Node: gradients/add_grad/Shape_1 = Shape[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_1)]]\nCaused by op u\'gradients/add_grad/Shape_1\', defined at:\n  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main\n    ...........\n\n...which was originally created as op u\'add\', defined at:\n  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main\n    ""__main__"", fname, loader, pkg_name)\n[elided 17 identical lines from previous traceback]\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File ""<ipython-input-45-59183d86e462>"", line 1, in <module>\n    y = tf.nn.softmax(tf.matmul(x,W) + b)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 403, in binary_op_wrapper\n    return func(x, y, name=name)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 44, in add\n    return _op_def_lib.apply_op(""Add"", x=x, y=y, name=name)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 633, in apply_op\n    op_def=op_def)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1710, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 988, in __init__\n    self._traceback = _extract_stack()\n']";"['# Stuff from tensorflow tutorial \nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\nx = tf.placeholder(""float"", shape=[None, 784])\ny_ = tf.placeholder(""float"", shape=[None, 10])\n\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n\ny = tf.nn.softmax(tf.matmul(x,W) + b)\n\ncross_entropy = -tf.reduce_sum(y_*tf.log(y))\n\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n', '# Read dataframe from training data\ncsvfile=\'train.csv\'\nfrom pandas import DataFrame, read_csv\ndf = read_csv(csvfile)\n\n# Strip off the target data and make it a separate dataframe.\nTarget=df.label\ndel df[""label""]\n\n# Split data into training and testing sets\nmsk = np.random.rand(len(df)) < 0.8\ndfTest = df[~msk]\nTargetTest = Target[~msk]\ndf = df[msk]\nTarget = Target[msk]\n\n# One hot encode the target\nOHTarget=pd.get_dummies(Target)\nOHTargetTest=pd.get_dummies(TargetTest)\n', 'for i in range(100):\n    batch = np.array(df[i*50:i*50+50].values)\n    batch = np.multiply(batch, 1.0 / 255.0)\n    Target_batch = np.array(OHTarget[i*50:i*50+50].values)\n    Target_batch = np.multiply(Target_batch, 1.0 / 255.0)\n    train_step.run(feed_dict={x: batch, y_: Target_batch})\n', '---------------------------------------------------------------------------\nFailedPreconditionError                   Traceback (most recent call last)\n<ipython-input-82-967faab7d494> in <module>()\n      4     Target_batch = np.array(OHTarget[i*50:i*50+50].values)\n      5     Target_batch = np.multiply(Target_batch, 1.0 / 255.0)\n----> 6     train_step.run(feed_dict={x: batch, y_: Target_batch})\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in run(self, feed_dict, session)\n   1265         none, the default session will be used.\n   1266     """"""\n-> 1267     _run_using_default_session(self, feed_dict, self.graph, session)\n   1268\n   1269\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in _run_using_default_session(operation, feed_dict, graph, session)\n   2761                        ""the operation\'s graph is different from the session\'s ""\n   2762                        ""graph."")\n-> 2763   session.run(operation, feed_dict)\n   2764\n   2765\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict)\n    343\n    344     # Run request and get response.\n--> 345     results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n    346\n    347     # User may have fetched the same tensor multiple times, but we\n\n/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, target_list, fetch_list, feed_dict)\n    417         # pylint: disable=protected-access\n    418         raise errors._make_specific_exception(node_def, op, e.error_message,\n--> 419                                               e.code)\n    420         # pylint: enable=protected-access\n    421       raise e_type, e_value, e_traceback\n\nFailedPreconditionError: Attempting to use uninitialized value Variable_1\n     [[Node: gradients/add_grad/Shape_1 = Shape[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_1)]]\nCaused by op u\'gradients/add_grad/Shape_1\', defined at:\n  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main\n    ...........\n\n...which was originally created as op u\'add\', defined at:\n  File ""/Users/user32/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main\n    ""__main__"", fname, loader, pkg_name)\n[elided 17 identical lines from previous traceback]\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File ""<ipython-input-45-59183d86e462>"", line 1, in <module>\n    y = tf.nn.softmax(tf.matmul(x,W) + b)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 403, in binary_op_wrapper\n    return func(x, y, name=name)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 44, in add\n    return _op_def_lib.apply_op(""Add"", x=x, y=y, name=name)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 633, in apply_op\n    op_def=op_def)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1710, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File ""/Users/user32/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 988, in __init__\n    self._traceback = _extract_stack()\n']";"['# Stuff from tensorflow tutorial \nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\nx = tf.placeholder(""float"", shape=[None, 784])\ny_ = tf.placeholder(""float"", shape=[None, 10])\n\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n\ny = tf.nn.softmax(tf.matmul(x,W) + b)\n\ncross_entropy = -tf.reduce_sum(y_*tf.log(y))\n\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n# Read dataframe from training data\ncsvfile=\'train.csv\'\nfrom pandas import DataFrame, read_csv\ndf = read_csv(csvfile)\n\n# Strip off the target data and make it a separate dataframe.\nTarget=df.label\ndel df[""label""]\n\n# Split data into training and testing sets\nmsk = np.random.rand(len(df)) < 0.8\ndfTest = df[~msk]\nTargetTest = Target[~msk]\ndf = df[msk]\nTarget = Target[msk]\n\n# One hot encode the target\nOHTarget=pd.get_dummies(Target)\nOHTargetTest=pd.get_dummies(TargetTest)\n\n\n\n\n\n\n']";"['import pandas as pd\n# Stuff from tensorflow tutorial \nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\nx = tf.placeholder(""float"", shape=[None, 784])\ny_ = tf.placeholder(""float"", shape=[None, 10])\n\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n\ny = tf.nn.softmax(tf.matmul(x,W) + b)\n\ncross_entropy = -tf.reduce_sum(y_*tf.log(y))\n\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n# Read dataframe from training data\ncsvfile=\'train.csv\'\nfrom pandas import DataFrame, read_csv\ndf = read_csv(csvfile)\n\n# Strip off the target data and make it a separate dataframe.\nTarget=df.label\ndel df[""label""]\n\n# Split data into training and testing sets\nmsk = np.random.rand(len(df)) < 0.8\ndfTest = df[~msk]\nTargetTest = Target[~msk]\ndf = df[msk]\nTarget = Target[msk]\n\n# One hot encode the target\nOHTarget=pd.get_dummies(Target)\nOHTargetTest=pd.get_dummies(TargetTest)\n\n\n\n\n\n\n']";True;"['import pandas as pd\ndata = pd.DataFrame()\n# Stuff from tensorflow tutorial \nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\nx = tf.placeholder(""float"", shape=[None, 784])\ny_ = tf.placeholder(""float"", shape=[None, 10])\n\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n\ny = tf.nn.softmax(tf.matmul(x,W) + b)\n\ncross_entropy = -tf.reduce_sum(y_*tf.log(y))\n\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n# Read dataframe from training data\ncsvfile=\'train.csv\'\nfrom pandas import DataFrame, read_csv\ndf = read_csv(csvfile)\n\n# Strip off the target data and make it a separate dataframe.\nTarget=df.label\ndel df[""label""]\n\n# Split data into training and testing sets\nmsk = np.random.rand(len(df)) < 0.8\ndfTest = df[~msk]\nTargetTest = Target[~msk]\ndf = df[msk]\nTarget = Target[msk]\n\n# One hot encode the target\nOHTarget=pd.get_dummies(Target)\nOHTargetTest=pd.get_dummies(TargetTest)\n\n\n\n\n\n\n']";True;0;2;"[""name 'tf' is not defined"", ""name 'tf' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'tf' is not defined"", ""name 'tf' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'tf' is not defined"", ""name 'tf' is not defined""]";['NameError', 'NameError']
1103;1103;1103;1103;2.0;0;34091877;;1;30;<python><csv><pandas><header>;How to add header row to a pandas DataFrame;55423.0;"['Cov = pd.read_csv(""path/to/file.txt"", sep=\'\\t\')\nFrame=pd.DataFrame([Cov], columns = [""Sequence"", ""Start"", ""End"", ""Coverage""])\nFrame.to_csv(""path/to/file.txt"", sep=\'\\t\')\nValueError: Shape of passed values is (1, 1), indices imply (4, 1)\n']";"['Cov = pd.read_csv(""path/to/file.txt"", sep=\'\\t\')\nFrame=pd.DataFrame([Cov], columns = [""Sequence"", ""Start"", ""End"", ""Coverage""])\nFrame.to_csv(""path/to/file.txt"", sep=\'\\t\')\n', 'ValueError: Shape of passed values is (1, 1), indices imply (4, 1)\n']";"['pandas', 'Cov = pd.read_csv(""path/to/file.txt"", sep=\'\\t\')\nFrame=pd.DataFrame([Cov], columns = [""Sequence"", ""Start"", ""End"", ""Coverage""])\nFrame.to_csv(""path/to/file.txt"", sep=\'\\t\')\n', 'ValueError: Shape of passed values is (1, 1), indices imply (4, 1)\n']";"['Cov = pd.read_csv(""path/to/file.txt"", sep=\'\\t\')\nFrame=pd.DataFrame([Cov], columns = [""Sequence"", ""Start"", ""End"", ""Coverage""])\nFrame.to_csv(""path/to/file.txt"", sep=\'\\t\')\n']";"['import pandas as pd\nCov = pd.read_csv(""path/to/file.txt"", sep=\'\\t\')\nFrame=pd.DataFrame([Cov], columns = [""Sequence"", ""Start"", ""End"", ""Coverage""])\nFrame.to_csv(""path/to/file.txt"", sep=\'\\t\')\n']";True;"['import pandas as pd\nCov = pd.read_csv(""path/to/file.txt"", sep=\'\\t\')\nFrame=pd.DataFrame([Cov], columns = [""Sequence"", ""Start"", ""End"", ""Coverage""])\nFrame.to_csv(""path/to/file.txt"", sep=\'\\t\')\n']";False;0;2;"[""name 'pd' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'Cov' is not defined"", ""File b'path/to/file.txt' does not exist""]";['NameError', 'FileNotFoundError'];0;2;"[""name 'Cov' is not defined"", ""File b'path/to/file.txt' does not exist""]";['NameError', 'FileNotFoundError']
1104;1104;1104;1104;4.0;0;34398054;;1;13;<pandas><ipython><ipython-notebook>;IPython Notebook cell multiple outputs;3664.0;['# salaries and teams are Pandas dataframe\nsalaries.head()\nteams.head()\n'];['# salaries and teams are Pandas dataframe\nsalaries.head()\nteams.head()\n'];['# salaries and teams are Pandas dataframe\nsalaries.head()\nteams.head()\n', 'teams', 'salaries', 'teams', 'salaries.head()', 'salaries', 'teams.head()'];['# salaries and teams are Pandas dataframe\nsalaries.head()\nteams.head()\n'];['# salaries and teams are Pandas dataframe\nsalaries.head()\nteams.head()\n'];False;['import pandas as pd\ndata = pd.DataFrame()\n# salaries and teams are Pandas dataframe\nsalaries.head()\nteams.head()\n'];True;1;2;"[""name 'salaries' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'salaries' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'salaries' is not defined"", 'Sucess']";['NameError', 'Sucess']
1105;1105;1105;1105;1.0;0;34531416;;1;11;<python><pandas>;comparing dtyped [float64] array with a scalar of type [bool] in Pandas DataFrame;5334.0;"[""DF1 = DF[DF['a'] == 0]\nDF2 = DF[DF['b'] == 0]\nDF3 = DF[DF['a'] == 0 |  DF['b'] == 0]\nTypeError: cannot compare a dtyped [float64] array with a scalar of type [bool]\n""]";"[""DF1 = DF[DF['a'] == 0]\n"", ""DF2 = DF[DF['b'] == 0]\n"", ""DF3 = DF[DF['a'] == 0 |  DF['b'] == 0]\n"", 'TypeError: cannot compare a dtyped [float64] array with a scalar of type [bool]\n']";"[""DF1 = DF[DF['a'] == 0]\n"", ""DF2 = DF[DF['b'] == 0]\n"", ""DF3 = DF[DF['a'] == 0 |  DF['b'] == 0]\n"", 'TypeError: cannot compare a dtyped [float64] array with a scalar of type [bool]\n']";"[""DF1 = DF[DF['a'] == 0]\nDF2 = DF[DF['b'] == 0]\nDF3 = DF[DF['a'] == 0 |  DF['b'] == 0]\n""]";"[""DF1 = DF[DF['a'] == 0]\nDF2 = DF[DF['b'] == 0]\nDF3 = DF[DF['a'] == 0 |  DF['b'] == 0]\n""]";False;"[""import pandas as pd\nDF1 = DF[DF['a'] == 0]\nDF2 = DF[DF['b'] == 0]\nDF3 = DF[DF['a'] == 0 |  DF['b'] == 0]\n""]";False;0;1;"[""name 'DF' is not defined""]";['NameError'];0;1;"[""name 'DF' is not defined""]";['NameError'];0;1;"[""name 'DF' is not defined""]";['NameError']
1106;1106;1106;1106;2.0;0;34555135;;1;12;<python><pandas>;Pandas: read_html;12389.0;"[""import pandas as pd\nimport html5lib\nf_states = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states') \n""]";"[""import pandas as pd\nimport html5lib\nf_states = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states') \n""]";"[""import pandas as pd\nimport html5lib\nf_states = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states') \n""]";"[""import pandas as pd\nimport html5lib\nf_states = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states') \n""]";"[""import pandas as pd\nimport html5lib\nf_states = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states') \n""]";False;"[""import pandas as pd\nimport pandas as pd\nimport html5lib\nf_states = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states') \n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1107;1107;1107;1107;2.0;3;34615854;;1;11;<python><pandas><seaborn>;Seaborn countplot with normalized y axis per group;1816.0;"['# Plot percentage of occupation per income class\ngrouped = df.groupby([\'income\'], sort=False)\noccupation_counts = grouped[\'occupation\'].value_counts(normalize=True, sort=False)\n\noccupation_data = [\n    {\'occupation\': occupation, \'income\': income, \'percentage\': percentage*100} for \n    (income, occupation), percentage in dict(occupation_counts).items()\n]\n\ndf_occupation = pd.DataFrame(occupation_data)\n\np = sns.barplot(x=""occupation"", y=""percentage"", hue=""income"", data=df_occupation)\n_ = plt.setp(p.get_xticklabels(), rotation=90)  # Rotate labels\n# Read the adult dataset\ndf = pd.read_csv(\n    ""data/adult.data"",\n    engine=\'c\',\n    lineterminator=\'\\n\',\n\n    names=[\'age\', \'workclass\', \'fnlwgt\', \'education\', \'education_num\',\n           \'marital_status\', \'occupation\', \'relationship\', \'race\', \'sex\',\n           \'capital_gain\', \'capital_loss\', \'hours_per_week\',\n           \'native_country\', \'income\'],\n    header=None,\n    skipinitialspace=True,\n    na_values=""?""\n)\n']";"['# Plot percentage of occupation per income class\ngrouped = df.groupby([\'income\'], sort=False)\noccupation_counts = grouped[\'occupation\'].value_counts(normalize=True, sort=False)\n\noccupation_data = [\n    {\'occupation\': occupation, \'income\': income, \'percentage\': percentage*100} for \n    (income, occupation), percentage in dict(occupation_counts).items()\n]\n\ndf_occupation = pd.DataFrame(occupation_data)\n\np = sns.barplot(x=""occupation"", y=""percentage"", hue=""income"", data=df_occupation)\n_ = plt.setp(p.get_xticklabels(), rotation=90)  # Rotate labels\n', '# Read the adult dataset\ndf = pd.read_csv(\n    ""data/adult.data"",\n    engine=\'c\',\n    lineterminator=\'\\n\',\n\n    names=[\'age\', \'workclass\', \'fnlwgt\', \'education\', \'education_num\',\n           \'marital_status\', \'occupation\', \'relationship\', \'race\', \'sex\',\n           \'capital_gain\', \'capital_loss\', \'hours_per_week\',\n           \'native_country\', \'income\'],\n    header=None,\n    skipinitialspace=True,\n    na_values=""?""\n)\n']";"['hue', '# Plot percentage of occupation per income class\ngrouped = df.groupby([\'income\'], sort=False)\noccupation_counts = grouped[\'occupation\'].value_counts(normalize=True, sort=False)\n\noccupation_data = [\n    {\'occupation\': occupation, \'income\': income, \'percentage\': percentage*100} for \n    (income, occupation), percentage in dict(occupation_counts).items()\n]\n\ndf_occupation = pd.DataFrame(occupation_data)\n\np = sns.barplot(x=""occupation"", y=""percentage"", hue=""income"", data=df_occupation)\n_ = plt.setp(p.get_xticklabels(), rotation=90)  # Rotate labels\n', '# Read the adult dataset\ndf = pd.read_csv(\n    ""data/adult.data"",\n    engine=\'c\',\n    lineterminator=\'\\n\',\n\n    names=[\'age\', \'workclass\', \'fnlwgt\', \'education\', \'education_num\',\n           \'marital_status\', \'occupation\', \'relationship\', \'race\', \'sex\',\n           \'capital_gain\', \'capital_loss\', \'hours_per_week\',\n           \'native_country\', \'income\'],\n    header=None,\n    skipinitialspace=True,\n    na_values=""?""\n)\n', 'hue']";"['# Plot percentage of occupation per income class\ngrouped = df.groupby([\'income\'], sort=False)\noccupation_counts = grouped[\'occupation\'].value_counts(normalize=True, sort=False)\n\n\ndf_occupation = pd.DataFrame(occupation_data)\n\np = sns.barplot(x=""occupation"", y=""percentage"", hue=""income"", data=df_occupation)\n_ = plt.setp(p.get_xticklabels(), rotation=90)  # Rotate labels\n# Read the adult dataset\n\n']";"['import pandas as pd\n# Plot percentage of occupation per income class\ngrouped = df.groupby([\'income\'], sort=False)\noccupation_counts = grouped[\'occupation\'].value_counts(normalize=True, sort=False)\n\n\ndf_occupation = pd.DataFrame(occupation_data)\n\np = sns.barplot(x=""occupation"", y=""percentage"", hue=""income"", data=df_occupation)\n_ = plt.setp(p.get_xticklabels(), rotation=90)  # Rotate labels\n# Read the adult dataset\n\n']";True;"['import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame()\n# Plot percentage of occupation per income class\ngrouped = df.groupby([\'income\'], sort=False)\noccupation_counts = grouped[\'occupation\'].value_counts(normalize=True, sort=False)\n\n\ndf_occupation = pd.DataFrame(occupation_data)\n\np = sns.barplot(x=""occupation"", y=""percentage"", hue=""income"", data=df_occupation)\n_ = plt.setp(p.get_xticklabels(), rotation=90)  # Rotate labels\n# Read the adult dataset\n\n']";True;0;0;[];[];0;0;[];[];0;0;[];[]
1108;1108;1108;1108;1.0;0;34682828;;1;20;<python><pandas>;pandas: Extracting specific selected columns from a DataFrame to new DataFrame;27288.0;"[""import pandas as pd\nold = pd.DataFrame({'A' : [4,5], 'B' : [10,20], 'C' : [100,50], 'D' : [-30,-50]})\nnew = pd.DataFrame(zip(old.A, old.C, old.D)) # raises TypeError: data argument can't be an iterator \n""]";"[""import pandas as pd\nold = pd.DataFrame({'A' : [4,5], 'B' : [10,20], 'C' : [100,50], 'D' : [-30,-50]})\nnew = pd.DataFrame(zip(old.A, old.C, old.D)) # raises TypeError: data argument can't be an iterator \n""]";"[""import pandas as pd\nold = pd.DataFrame({'A' : [4,5], 'B' : [10,20], 'C' : [100,50], 'D' : [-30,-50]})\nnew = pd.DataFrame(zip(old.A, old.C, old.D)) # raises TypeError: data argument can't be an iterator \n""]";"[""import pandas as pd\nold = pd.DataFrame({'A' : [4,5], 'B' : [10,20], 'C' : [100,50], 'D' : [-30,-50]})\nnew = pd.DataFrame(zip(old.A, old.C, old.D)) # raises TypeError: data argument can't be an iterator \n""]";"[""import pandas as pd\nold = pd.DataFrame({'A' : [4,5], 'B' : [10,20], 'C' : [100,50], 'D' : [-30,-50]})\nnew = pd.DataFrame(zip(old.A, old.C, old.D)) # raises TypeError: data argument can't be an iterator \n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nimport pandas as pd\nold = pd.DataFrame({'A' : [4,5], 'B' : [10,20], 'C' : [100,50], 'D' : [-30,-50]})\nnew = pd.DataFrame(zip(old.A, old.C, old.D)) # raises TypeError: data argument can't be an iterator \n""]";True;0;1;"[""name 'old' is not defined""]";['NameError'];0;1;"[""name 'old' is not defined""]";['NameError'];0;1;"[""name 'old' is not defined""]";['NameError']
1109;1109;1109;1109;3.0;0;34794067;;1;18;<python><pandas><nan>;How to set a cell to NaN in a pandas dataframe;18964.0;"[""mydata = {'x' : [10, 50, 18, 32, 47, 20], 'y' : ['12', '11', 'N/A', '13', '15', 'N/A']}\ndf = pd.DataFrame(mydata)\n\ndf[df.y == 'N/A']['y'] = np.nan\n""]";"[""mydata = {'x' : [10, 50, 18, 32, 47, 20], 'y' : ['12', '11', 'N/A', '13', '15', 'N/A']}\ndf = pd.DataFrame(mydata)\n\ndf[df.y == 'N/A']['y'] = np.nan\n""]";"[""mydata = {'x' : [10, 50, 18, 32, 47, 20], 'y' : ['12', '11', 'N/A', '13', '15', 'N/A']}\ndf = pd.DataFrame(mydata)\n\ndf[df.y == 'N/A']['y'] = np.nan\n""]";"[""mydata = {'x' : [10, 50, 18, 32, 47, 20], 'y' : ['12', '11', 'N/A', '13', '15', 'N/A']}\ndf = pd.DataFrame(mydata)\n\ndf[df.y == 'N/A']['y'] = np.nan\n""]";"[""import pandas as pd\nmydata = {'x' : [10, 50, 18, 32, 47, 20], 'y' : ['12', '11', 'N/A', '13', '15', 'N/A']}\ndf = pd.DataFrame(mydata)\n\ndf[df.y == 'N/A']['y'] = np.nan\n""]";True;"[""import pandas as pd\nmydata = {'x' : [10, 50, 18, 32, 47, 20], 'y' : ['12', '11', 'N/A', '13', '15', 'N/A']}\ndf = pd.DataFrame(mydata)\n\ndf[df.y == 'N/A']['y'] = np.nan\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
1110;1110;1110;1110;1.0;6;34862336;;1;11;<python-3.x><pandas>;Performance of str.strip for Pandas;360.0;"[""%%timeit\nfcr['id'] = fcr['id'].astype(str).map(str.strip)\n10 loops, best of 3: 47.8 ms per loop\n\n%%timeit\nfcr['id'] = fcr['id'].map(str.strip)\n10 loops, best of 3: 25.2 ms per loop\n\n%%timeit\nfcr['id'] = fcr['id'].str.strip(' ')\n10 loops, best of 3: 55.5 ms per loop\n""]";"[""%%timeit\nfcr['id'] = fcr['id'].astype(str).map(str.strip)\n10 loops, best of 3: 47.8 ms per loop\n\n%%timeit\nfcr['id'] = fcr['id'].map(str.strip)\n10 loops, best of 3: 25.2 ms per loop\n\n%%timeit\nfcr['id'] = fcr['id'].str.strip(' ')\n10 loops, best of 3: 55.5 ms per loop\n""]";"[""%%timeit\nfcr['id'] = fcr['id'].astype(str).map(str.strip)\n10 loops, best of 3: 47.8 ms per loop\n\n%%timeit\nfcr['id'] = fcr['id'].map(str.strip)\n10 loops, best of 3: 25.2 ms per loop\n\n%%timeit\nfcr['id'] = fcr['id'].str.strip(' ')\n10 loops, best of 3: 55.5 ms per loop\n""]";"[""fcr['id'] = fcr['id'].astype(str).map(str.strip)\n\nfcr['id'] = fcr['id'].map(str.strip)\n\nfcr['id'] = fcr['id'].str.strip(' ')\n""]";"[""fcr['id'] = fcr['id'].astype(str).map(str.strip)\n\nfcr['id'] = fcr['id'].map(str.strip)\n\nfcr['id'] = fcr['id'].str.strip(' ')\n""]";False;"[""import pandas as pd\nfcr['id'] = fcr['id'].astype(str).map(str.strip)\n\nfcr['id'] = fcr['id'].map(str.strip)\n\nfcr['id'] = fcr['id'].str.strip(' ')\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'id'""]";['KeyError']
1111;1111;1111;1111;3.0;0;34883101;;1;12;<datetime><pandas>;Pandas converting row with unix timestamp (in milliseconds) to datetime;4306.0;"[""import sys\nif sys.version_info[0] < 3:\n    from StringIO import StringIO\nelse:\n    from io import StringIO\nimport pandas as pd\n\ndata = 'RUN,UNIXTIME,VALUE\\n1,1447160702320,10\\n2,1447160702364,20\\n3,1447160722364,42'\n\ndf = pd.read_csv(StringIO(data))\n\nconvert = lambda x: datetime.datetime.fromtimestamp(x / 1e3)\nconverted_df = df['UNIXTIME'].apply(convert)\n0    1447160702320\n1    1447160702364\n2    1447160722364\nName: UNIXTIME, dtype: int64\n0   2015-11-10 14:05:02.320\n1   2015-11-10 14:05:02.364\n2   2015-11-10 14:05:22.364\nName: UNIXTIME, dtype: datetime64[ns]\n""]";"[""import sys\nif sys.version_info[0] < 3:\n    from StringIO import StringIO\nelse:\n    from io import StringIO\nimport pandas as pd\n\ndata = 'RUN,UNIXTIME,VALUE\\n1,1447160702320,10\\n2,1447160702364,20\\n3,1447160722364,42'\n\ndf = pd.read_csv(StringIO(data))\n\nconvert = lambda x: datetime.datetime.fromtimestamp(x / 1e3)\nconverted_df = df['UNIXTIME'].apply(convert)\n"", '0    1447160702320\n1    1447160702364\n2    1447160722364\nName: UNIXTIME, dtype: int64\n', '0   2015-11-10 14:05:02.320\n1   2015-11-10 14:05:02.364\n2   2015-11-10 14:05:22.364\nName: UNIXTIME, dtype: datetime64[ns]\n']";"['DataFrame', ""import sys\nif sys.version_info[0] < 3:\n    from StringIO import StringIO\nelse:\n    from io import StringIO\nimport pandas as pd\n\ndata = 'RUN,UNIXTIME,VALUE\\n1,1447160702320,10\\n2,1447160702364,20\\n3,1447160722364,42'\n\ndf = pd.read_csv(StringIO(data))\n\nconvert = lambda x: datetime.datetime.fromtimestamp(x / 1e3)\nconverted_df = df['UNIXTIME'].apply(convert)\n"", '0    1447160702320\n1    1447160702364\n2    1447160722364\nName: UNIXTIME, dtype: int64\n', '0   2015-11-10 14:05:02.320\n1   2015-11-10 14:05:02.364\n2   2015-11-10 14:05:22.364\nName: UNIXTIME, dtype: datetime64[ns]\n', 'pd.apply()']";"[""import sys\nimport pandas as pd\n\ndata = 'RUN,UNIXTIME,VALUE\\n1,1447160702320,10\\n2,1447160702364,20\\n3,1447160722364,42'\n\ndf = pd.read_csv(StringIO(data))\n\nconvert = lambda x: datetime.datetime.fromtimestamp(x / 1e3)\nconverted_df = df['UNIXTIME'].apply(convert)\n""]";"[""import sys\nimport pandas as pd\n\ndata = 'RUN,UNIXTIME,VALUE\\n1,1447160702320,10\\n2,1447160702364,20\\n3,1447160722364,42'\n\ndf = pd.read_csv(StringIO(data))\n\nconvert = lambda x: datetime.datetime.fromtimestamp(x / 1e3)\nconverted_df = df['UNIXTIME'].apply(convert)\n""]";False;"[""import pandas as pd\nimport sys\nimport pandas as pd\n\ndata = 'RUN,UNIXTIME,VALUE\\n1,1447160702320,10\\n2,1447160702364,20\\n3,1447160722364,42'\n\ndf = pd.read_csv(StringIO(data))\n\nconvert = lambda x: datetime.datetime.fromtimestamp(x / 1e3)\nconverted_df = df['UNIXTIME'].apply(convert)\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'UNIXTIME'""]";['KeyError']
1112;1112;1112;1112;3.0;3;34896455;;1;11;<python><pandas>;How to do Pearson correlation of selected columns of a Pandas data frame;15117.0;"['gene,stem1,stem2,stem3,b1,b2,b3,special_col\nfoo,20,10,11,23,22,79,3\nbar,17,13,505,12,13,88,1\nqui,17,13,5,12,13,88,3\nIn [17]: import pandas as pd\nIn [20]: df = pd.read_table(""http://dpaste.com/3PQV3FA.txt"",sep="","")\nIn [21]: df\nOut[21]:\n  gene  stem1  stem2  stem3  b1  b2  b3  special_col\n0  foo     20     10     11  23  22  79            3\n1  bar     17     13    505  12  13  88            1\n2  qui     17     13      5  12  13  88            3\nColn   PearCorr\nstem1  0.5\nstem2 -0.5\nstem3 -0.9999453506011533\nb1    0.5\nb2    0.5\nb3    -0.5\nIn [27]: import scipy.stats\nIn [39]: scipy.stats.pearsonr([3, 1, 3], [11,505,5])\nOut[39]: (-0.9999453506011533, 0.0066556395400007278)\n']";"['gene,stem1,stem2,stem3,b1,b2,b3,special_col\nfoo,20,10,11,23,22,79,3\nbar,17,13,505,12,13,88,1\nqui,17,13,5,12,13,88,3\n', 'In [17]: import pandas as pd\nIn [20]: df = pd.read_table(""http://dpaste.com/3PQV3FA.txt"",sep="","")\nIn [21]: df\nOut[21]:\n  gene  stem1  stem2  stem3  b1  b2  b3  special_col\n0  foo     20     10     11  23  22  79            3\n1  bar     17     13    505  12  13  88            1\n2  qui     17     13      5  12  13  88            3\n', 'Coln   PearCorr\nstem1  0.5\nstem2 -0.5\nstem3 -0.9999453506011533\nb1    0.5\nb2    0.5\nb3    -0.5\n', 'In [27]: import scipy.stats\nIn [39]: scipy.stats.pearsonr([3, 1, 3], [11,505,5])\nOut[39]: (-0.9999453506011533, 0.0066556395400007278)\n']";"['gene,stem1,stem2,stem3,b1,b2,b3,special_col\nfoo,20,10,11,23,22,79,3\nbar,17,13,505,12,13,88,1\nqui,17,13,5,12,13,88,3\n', 'In [17]: import pandas as pd\nIn [20]: df = pd.read_table(""http://dpaste.com/3PQV3FA.txt"",sep="","")\nIn [21]: df\nOut[21]:\n  gene  stem1  stem2  stem3  b1  b2  b3  special_col\n0  foo     20     10     11  23  22  79            3\n1  bar     17     13    505  12  13  88            1\n2  qui     17     13      5  12  13  88            3\n', 'special_col', 'gene', 'special column', 'colnames[1:number_of_column-1]', 'Coln   PearCorr\nstem1  0.5\nstem2 -0.5\nstem3 -0.9999453506011533\nb1    0.5\nb2    0.5\nb3    -0.5\n', 'In [27]: import scipy.stats\nIn [39]: scipy.stats.pearsonr([3, 1, 3], [11,505,5])\nOut[39]: (-0.9999453506011533, 0.0066556395400007278)\n']";['import pandas as pd\nimport scipy.stats\n'];['import pandas as pd\nimport scipy.stats\n'];False;['import pandas as pd\nimport pandas as pd\nimport scipy.stats\n'];False;0;3;"[""name 'df' is not defined"", ""name 'data' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'df' is not defined"", ""name 'data' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError', 'NameError'];1;3;"['Sucess', ""'special_col'"", 'single positional indexer is out-of-bounds']";['Sucess', 'KeyError', 'IndexError']
1113;1113;1113;1113;3.0;2;34913590;;1;11;<python><pandas><dataframe>;Fillna in multiple columns in place in Python Pandas;4398.0;"[""df = pd.DataFrame({'Name':['Jack','Sue',pd.np.nan,'Bob','Alice','John'],\n    'A': [1, 2.1, pd.np.nan, 4.7, 5.6, 6.8],\n    'B': [.25, pd.np.nan, pd.np.nan, 4, 12.2, 14.4],\n    'City':['Seattle','SF','LA','OC',pd.np.nan,pd.np.nan]})\ndf['Name'].fillna('.',inplace=True)\ndf['City'].fillna('.',inplace=True)\ndf.fillna(0,inplace=True)\n""]";"[""df = pd.DataFrame({'Name':['Jack','Sue',pd.np.nan,'Bob','Alice','John'],\n    'A': [1, 2.1, pd.np.nan, 4.7, 5.6, 6.8],\n    'B': [.25, pd.np.nan, pd.np.nan, 4, 12.2, 14.4],\n    'City':['Seattle','SF','LA','OC',pd.np.nan,pd.np.nan]})\n"", ""df['Name'].fillna('.',inplace=True)\ndf['City'].fillna('.',inplace=True)\ndf.fillna(0,inplace=True)\n""]";"[""df = pd.DataFrame({'Name':['Jack','Sue',pd.np.nan,'Bob','Alice','John'],\n    'A': [1, 2.1, pd.np.nan, 4.7, 5.6, 6.8],\n    'B': [.25, pd.np.nan, pd.np.nan, 4, 12.2, 14.4],\n    'City':['Seattle','SF','LA','OC',pd.np.nan,pd.np.nan]})\n"", ""df['Name'].fillna('.',inplace=True)\ndf['City'].fillna('.',inplace=True)\ndf.fillna(0,inplace=True)\n""]";"[""df['Name'].fillna('.',inplace=True)\ndf['City'].fillna('.',inplace=True)\ndf.fillna(0,inplace=True)\n""]";"[""df['Name'].fillna('.',inplace=True)\ndf['City'].fillna('.',inplace=True)\ndf.fillna(0,inplace=True)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\ndf['Name'].fillna('.',inplace=True)\ndf['City'].fillna('.',inplace=True)\ndf.fillna(0,inplace=True)\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
1114;1114;1114;1114;3.0;1;34962104;;1;32;<python><pandas><dataframe><python-3.5>;Pandas: How can I use the apply() function for a single column?;36873.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;1;2;"[""name 'a' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'a' is not defined"", 'Sucess']";['NameError', 'Sucess'];1;2;"[""name 'a' is not defined"", 'Sucess']";['NameError', 'Sucess']
1115;1115;1115;1115;1.0;2;35093729;;1;11;<python><csv><numpy><pandas>;Adding data to Pandas Dataframe from a CSV file causing Value Errors;260.0;"["">>> df.ix['index 5','Total Dollars'] += 10\n>>> print type(df.ix['index 5','Total Dollars'] \nint64 <class 'pandas.core.series.Series'>\n>>> print type(df.ix['index 5','Total Dollars']\nint64\n""]";"["">>> df.ix['index 5','Total Dollars'] += 10\n"", "">>> print type(df.ix['index 5','Total Dollars'] \nint64 <class 'pandas.core.series.Series'>\n"", "">>> print type(df.ix['index 5','Total Dollars']\nint64\n""]";"['int', 'Pandas', 'DataFrame', "">>> df.ix['index 5','Total Dollars'] += 10\n"", 'ValueError: Must have equal len keys and value when setting with an iterable', 'datatype', "">>> print type(df.ix['index 5','Total Dollars'] \nint64 <class 'pandas.core.series.Series'>\n"", "">>> print type(df.ix['index 5','Total Dollars']\nint64\n""]";['int64\n'];['int64\n'];False;['import pandas as pd\nint64\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
1116;1116;1116;1116;1.0;0;35318269;;1;12;<pandas>;TypeError: pivot_table() got an unexpected keyword argument 'rows';3901.0;"[""mean_ratings = data.pivot_table('rating', rows='title', cols='gender', aggfunc='mean')\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-55-cb4d494f2f39> in <module>()\n----> 1 mean_ratings = data.pivot_table('rating', rows='title', cols='gender', aggfunc='mean')\n\nTypeError: pivot_table() got an unexpected keyword argument 'rows'\n""]";"[""mean_ratings = data.pivot_table('rating', rows='title', cols='gender', aggfunc='mean')\n"", ""---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-55-cb4d494f2f39> in <module>()\n----> 1 mean_ratings = data.pivot_table('rating', rows='title', cols='gender', aggfunc='mean')\n\nTypeError: pivot_table() got an unexpected keyword argument 'rows'\n""]";"[""mean_ratings = data.pivot_table('rating', rows='title', cols='gender', aggfunc='mean')\n"", ""---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-55-cb4d494f2f39> in <module>()\n----> 1 mean_ratings = data.pivot_table('rating', rows='title', cols='gender', aggfunc='mean')\n\nTypeError: pivot_table() got an unexpected keyword argument 'rows'\n""]";"[""mean_ratings = data.pivot_table('rating', rows='title', cols='gender', aggfunc='mean')\n\n""]";"[""mean_ratings = data.pivot_table('rating', rows='title', cols='gender', aggfunc='mean')\n\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nmean_ratings = data.pivot_table('rating', rows='title', cols='gender', aggfunc='mean')\n\n""]";True;0;1;"[""name 'data' is not defined""]";['NameError'];0;1;"[""name 'data' is not defined""]";['NameError'];0;1;"[""pivot_table() got an unexpected keyword argument 'rows'""]";['TypeError']
1117;1117;1117;1117;1.0;0;35368645;;1;17;<python><pandas><indexing><dataframe><rows>;pandas - change df.index from float64 to unicode or string;8345.0;"[""#check type\ntype(df.index)\n'pandas.core.index.Float64Index'\n\n#change type to unicode\nif not isinstance(df.index, unicode):\n    df.index = df.index.astype(unicode)\nTypeError: Setting <class 'pandas.core.index.Float64Index'> dtype to anything other than float64 or object is not supported\n""]";"[""#check type\ntype(df.index)\n'pandas.core.index.Float64Index'\n\n#change type to unicode\nif not isinstance(df.index, unicode):\n    df.index = df.index.astype(unicode)\n"", ""TypeError: Setting <class 'pandas.core.index.Float64Index'> dtype to anything other than float64 or object is not supported\n""]";"[""#check type\ntype(df.index)\n'pandas.core.index.Float64Index'\n\n#change type to unicode\nif not isinstance(df.index, unicode):\n    df.index = df.index.astype(unicode)\n"", ""TypeError: Setting <class 'pandas.core.index.Float64Index'> dtype to anything other than float64 or object is not supported\n""]";"[""#check type\ntype(df.index)\n'pandas.core.index.Float64Index'\n\n#change type to unicode\n""]";"[""#check type\ntype(df.index)\n'pandas.core.index.Float64Index'\n\n#change type to unicode\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\n#check type\ntype(df.index)\n'pandas.core.index.Float64Index'\n\n#change type to unicode\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'unicode' is not defined""]";['NameError']
1118;1118;1118;1118;2.0;0;35414625;;1;12;<python><pandas><pivot><multi-index>;pandas: how to run a pivot with a multi-index?;4754.0;"[""import pandas as pd\nimport numpy as np\n\ndf= pd.DataFrame()\nmonth = np.arange(1,13)\nvalues1 = np.random.randint(0,100,12)\nvalues2 = np.random.randint(200,300,12)\n\n\ndf['month'] =  np.hstack(( month, month ))\ndf['year']=2004\ndf['value'] = np.hstack(( values1, values2 ))\ndf['item']= np.hstack(( np.repeat('item 1',12), np.repeat('item 2',12) ))\n\n# This doesn't work: ValueError: Wrong number of items passed 24, placement implies 2\n# mypiv = df.pivot( ['year', 'month'], 'item' ,'value' )\n\n#This doesn't work, either:\n#df.set_index(['year', 'month'], inplace=True)\n# ValueError: cannot label index with a null key\n#mypiv = df.pivot(columns='item', values='value')\n\n#This below works but is not ideal: I have to first concatenate then separate the fields I need\ndf['new field']= df['year'] * 100 + df['month']\n\nmypiv = df.pivot('new field', 'item', 'value').reset_index()\nmypiv['year'] = mypiv['new field'].apply( lambda x: int(x) / 100)  \nmypiv['month'] = mypiv['new field'] % 100\n""]";"[""import pandas as pd\nimport numpy as np\n\ndf= pd.DataFrame()\nmonth = np.arange(1,13)\nvalues1 = np.random.randint(0,100,12)\nvalues2 = np.random.randint(200,300,12)\n\n\ndf['month'] =  np.hstack(( month, month ))\ndf['year']=2004\ndf['value'] = np.hstack(( values1, values2 ))\ndf['item']= np.hstack(( np.repeat('item 1',12), np.repeat('item 2',12) ))\n\n# This doesn't work: ValueError: Wrong number of items passed 24, placement implies 2\n# mypiv = df.pivot( ['year', 'month'], 'item' ,'value' )\n\n#This doesn't work, either:\n#df.set_index(['year', 'month'], inplace=True)\n# ValueError: cannot label index with a null key\n#mypiv = df.pivot(columns='item', values='value')\n\n#This below works but is not ideal: I have to first concatenate then separate the fields I need\ndf['new field']= df['year'] * 100 + df['month']\n\nmypiv = df.pivot('new field', 'item', 'value').reset_index()\nmypiv['year'] = mypiv['new field'].apply( lambda x: int(x) / 100)  \nmypiv['month'] = mypiv['new field'] % 100\n""]";"[""import pandas as pd\nimport numpy as np\n\ndf= pd.DataFrame()\nmonth = np.arange(1,13)\nvalues1 = np.random.randint(0,100,12)\nvalues2 = np.random.randint(200,300,12)\n\n\ndf['month'] =  np.hstack(( month, month ))\ndf['year']=2004\ndf['value'] = np.hstack(( values1, values2 ))\ndf['item']= np.hstack(( np.repeat('item 1',12), np.repeat('item 2',12) ))\n\n# This doesn't work: ValueError: Wrong number of items passed 24, placement implies 2\n# mypiv = df.pivot( ['year', 'month'], 'item' ,'value' )\n\n#This doesn't work, either:\n#df.set_index(['year', 'month'], inplace=True)\n# ValueError: cannot label index with a null key\n#mypiv = df.pivot(columns='item', values='value')\n\n#This below works but is not ideal: I have to first concatenate then separate the fields I need\ndf['new field']= df['year'] * 100 + df['month']\n\nmypiv = df.pivot('new field', 'item', 'value').reset_index()\nmypiv['year'] = mypiv['new field'].apply( lambda x: int(x) / 100)  \nmypiv['month'] = mypiv['new field'] % 100\n""]";"[""import pandas as pd\nimport numpy as np\n\ndf= pd.DataFrame()\nmonth = np.arange(1,13)\nvalues1 = np.random.randint(0,100,12)\nvalues2 = np.random.randint(200,300,12)\n\n\ndf['month'] =  np.hstack(( month, month ))\ndf['year']=2004\ndf['value'] = np.hstack(( values1, values2 ))\ndf['item']= np.hstack(( np.repeat('item 1',12), np.repeat('item 2',12) ))\n\n# This doesn't work: ValueError: Wrong number of items passed 24, placement implies 2\n# mypiv = df.pivot( ['year', 'month'], 'item' ,'value' )\n\n#This doesn't work, either:\n#df.set_index(['year', 'month'], inplace=True)\n# ValueError: cannot label index with a null key\n#mypiv = df.pivot(columns='item', values='value')\n\n#This below works but is not ideal: I have to first concatenate then separate the fields I need\ndf['new field']= df['year'] * 100 + df['month']\n\nmypiv = df.pivot('new field', 'item', 'value').reset_index()\nmypiv['year'] = mypiv['new field'].apply( lambda x: int(x) / 100)  \nmypiv['month'] = mypiv['new field'] % 100\n""]";"[""import pandas as pd\nimport numpy as np\n\ndf= pd.DataFrame()\nmonth = np.arange(1,13)\nvalues1 = np.random.randint(0,100,12)\nvalues2 = np.random.randint(200,300,12)\n\n\ndf['month'] =  np.hstack(( month, month ))\ndf['year']=2004\ndf['value'] = np.hstack(( values1, values2 ))\ndf['item']= np.hstack(( np.repeat('item 1',12), np.repeat('item 2',12) ))\n\n# This doesn't work: ValueError: Wrong number of items passed 24, placement implies 2\n# mypiv = df.pivot( ['year', 'month'], 'item' ,'value' )\n\n#This doesn't work, either:\n#df.set_index(['year', 'month'], inplace=True)\n# ValueError: cannot label index with a null key\n#mypiv = df.pivot(columns='item', values='value')\n\n#This below works but is not ideal: I have to first concatenate then separate the fields I need\ndf['new field']= df['year'] * 100 + df['month']\n\nmypiv = df.pivot('new field', 'item', 'value').reset_index()\nmypiv['year'] = mypiv['new field'].apply( lambda x: int(x) / 100)  \nmypiv['month'] = mypiv['new field'] % 100\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nimport pandas as pd\nimport numpy as np\n\ndf= pd.DataFrame()\nmonth = np.arange(1,13)\nvalues1 = np.random.randint(0,100,12)\nvalues2 = np.random.randint(200,300,12)\n\n\ndf['month'] =  np.hstack(( month, month ))\ndf['year']=2004\ndf['value'] = np.hstack(( values1, values2 ))\ndf['item']= np.hstack(( np.repeat('item 1',12), np.repeat('item 2',12) ))\n\n# This doesn't work: ValueError: Wrong number of items passed 24, placement implies 2\n# mypiv = df.pivot( ['year', 'month'], 'item' ,'value' )\n\n#This doesn't work, either:\n#df.set_index(['year', 'month'], inplace=True)\n# ValueError: cannot label index with a null key\n#mypiv = df.pivot(columns='item', values='value')\n\n#This below works but is not ideal: I have to first concatenate then separate the fields I need\ndf['new field']= df['year'] * 100 + df['month']\n\nmypiv = df.pivot('new field', 'item', 'value').reset_index()\nmypiv['year'] = mypiv['new field'].apply( lambda x: int(x) / 100)  \nmypiv['month'] = mypiv['new field'] % 100\n""]";True;1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'df' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""'year'""]";['Sucess', 'KeyError']
1119;1119;1119;1119;6.0;2;35634238;;1;16;<python><pandas>;How to save a pandas DataFrame table as a png;11155.0;[''];[];"[""index=['name1','name2',...]""]";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError'];0;1;"[""No module named 'matplotlib'""]";['ImportError']
1120;1120;1120;1120;2.0;3;35782929;;1;19;<python><python-3.x><pandas><memory-management>;Pandas GroupBy memory deallocation;862.0;['import resource\nimport gc\n\nimport pandas as pd\nimport numpy as np\n\ni = np.random.choice(list(range(100)), 4000)\ncols = list(range(int(2e4)))\n\ndf = pd.DataFrame(1, index=i, columns=cols)\n\ngb = df.groupby(level=0)\n# gb = list(gb)\nfor i in range(3):\n    print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1e6)\n    for idx, x in enumerate(gb):\n        if idx == 0:\n            print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1e6)\n    # del idx, x\n    # gc.collect()\n0.671732\n1.297424\n1.297952\n1.923288\n1.923288\n2.548624\n0.671768\n1.297412\n1.297992\n1.297992\n1.297992\n1.297992\n1.32874\n1.32874\n1.32874\n1.32874\n1.32874\n1.32874\n'];['import resource\nimport gc\n\nimport pandas as pd\nimport numpy as np\n\ni = np.random.choice(list(range(100)), 4000)\ncols = list(range(int(2e4)))\n\ndf = pd.DataFrame(1, index=i, columns=cols)\n\ngb = df.groupby(level=0)\n# gb = list(gb)\nfor i in range(3):\n    print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1e6)\n    for idx, x in enumerate(gb):\n        if idx == 0:\n            print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1e6)\n    # del idx, x\n    # gc.collect()\n', '0.671732\n1.297424\n1.297952\n1.923288\n1.923288\n2.548624\n', '0.671768\n1.297412\n1.297992\n1.297992\n1.297992\n1.297992\n', '1.32874\n1.32874\n1.32874\n1.32874\n1.32874\n1.32874\n'];['resource.getrusage(resource.RUSAGE_SELF).ru_maxrss', 'import resource\nimport gc\n\nimport pandas as pd\nimport numpy as np\n\ni = np.random.choice(list(range(100)), 4000)\ncols = list(range(int(2e4)))\n\ndf = pd.DataFrame(1, index=i, columns=cols)\n\ngb = df.groupby(level=0)\n# gb = list(gb)\nfor i in range(3):\n    print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1e6)\n    for idx, x in enumerate(gb):\n        if idx == 0:\n            print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1e6)\n    # del idx, x\n    # gc.collect()\n', '0.671732\n1.297424\n1.297952\n1.923288\n1.923288\n2.548624\n', 'del idx, x', 'gc.collect()', 'del', '0.671768\n1.297412\n1.297992\n1.297992\n1.297992\n1.297992\n', 'gb = list(gb)', '1.32874\n1.32874\n1.32874\n1.32874\n1.32874\n1.32874\n'];['import resource\nimport gc\n\nimport pandas as pd\nimport numpy as np\n\ni = np.random.choice(list(range(100)), 4000)\ncols = list(range(int(2e4)))\n\ndf = pd.DataFrame(1, index=i, columns=cols)\n\ngb = df.groupby(level=0)\n# gb = list(gb)\n    # del idx, x\n    # gc.collect()\n0.671732\n1.297424\n1.297952\n1.923288\n1.923288\n2.548624\n0.671768\n1.297412\n1.297992\n1.297992\n1.297992\n1.297992\n1.32874\n1.32874\n1.32874\n1.32874\n1.32874\n1.32874\n'];['import resource\nimport gc\n\nimport pandas as pd\nimport numpy as np\n\ni = np.random.choice(list(range(100)), 4000)\ncols = list(range(int(2e4)))\n\ndf = pd.DataFrame(1, index=i, columns=cols)\n\ngb = df.groupby(level=0)\n# gb = list(gb)\n    # del idx, x\n    # gc.collect()\n0.671732\n1.297424\n1.297952\n1.923288\n1.923288\n2.548624\n0.671768\n1.297412\n1.297992\n1.297992\n1.297992\n1.297992\n1.32874\n1.32874\n1.32874\n1.32874\n1.32874\n1.32874\n'];False;['import pandas as pd\nimport resource\nimport gc\n\nimport pandas as pd\nimport numpy as np\n\ni = np.random.choice(list(range(100)), 4000)\ncols = list(range(int(2e4)))\n\ndf = pd.DataFrame(1, index=i, columns=cols)\n\ngb = df.groupby(level=0)\n# gb = list(gb)\n    # del idx, x\n    # gc.collect()\n0.671732\n1.297424\n1.297952\n1.923288\n1.923288\n2.548624\n0.671768\n1.297412\n1.297992\n1.297992\n1.297992\n1.297992\n1.32874\n1.32874\n1.32874\n1.32874\n1.32874\n1.32874\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
1121;1121;1121;1121;5.0;6;35826912;;1;11;<python><pandas><scikit-learn>;What is a good heuristic to detect if a column in a pandas.DataFrame is categorical?;1612.0;[''];[];['object', '1)', '2)', '2)', '2)'];[''];[''];False;['import pandas as pd\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
1122;1122;1122;1122;3.0;0;36000993;;1;12;<python><arrays><numpy><pandas>;Numpy isnan() fails on an array of floats (from pandas dataframe apply);9699.0;"['set([type(x) for x in tester])\nOut[59]: {float}\n\ntester\nOut[60]: \narray([-0.7000000000000001, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n   nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n   nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n   nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n   nan, nan], dtype=object)\n\nset([type(x) for x in tester])\nOut[61]: {float}\n\nnp.isnan(tester)\nTraceback (most recent call last):\n\nFile ""<ipython-input-62-e3638605b43c>"", line 1, in <module>\nnp.isnan(tester)\n\nTypeError: ufunc \'isnan\' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule \'\'safe\'\'\n\nset([np.isnan(x) for x in tester])\nOut[65]: {False, True}\n\ntype(tester)\nOut[66]: numpy.ndarray\n']";"['set([type(x) for x in tester])\nOut[59]: {float}\n\ntester\nOut[60]: \narray([-0.7000000000000001, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n   nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n   nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n   nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n   nan, nan], dtype=object)\n\nset([type(x) for x in tester])\nOut[61]: {float}\n\nnp.isnan(tester)\nTraceback (most recent call last):\n\nFile ""<ipython-input-62-e3638605b43c>"", line 1, in <module>\nnp.isnan(tester)\n\nTypeError: ufunc \'isnan\' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule \'\'safe\'\'\n\nset([np.isnan(x) for x in tester])\nOut[65]: {False, True}\n\ntype(tester)\nOut[66]: numpy.ndarray\n']";"['set([type(x) for x in tester])\nOut[59]: {float}\n\ntester\nOut[60]: \narray([-0.7000000000000001, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n   nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n   nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n   nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n   nan, nan], dtype=object)\n\nset([type(x) for x in tester])\nOut[61]: {float}\n\nnp.isnan(tester)\nTraceback (most recent call last):\n\nFile ""<ipython-input-62-e3638605b43c>"", line 1, in <module>\nnp.isnan(tester)\n\nTypeError: ufunc \'isnan\' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule \'\'safe\'\'\n\nset([np.isnan(x) for x in tester])\nOut[65]: {False, True}\n\ntype(tester)\nOut[66]: numpy.ndarray\n']";['set([type(x) for x in tester])\n\ntester\n\nset([type(x) for x in tester])\n\nnp.isnan(tester)\n\nnp.isnan(tester)\n\n\nset([np.isnan(x) for x in tester])\n\ntype(tester)\n'];['set([type(x) for x in tester])\n\ntester\n\nset([type(x) for x in tester])\n\nnp.isnan(tester)\n\nnp.isnan(tester)\n\n\nset([np.isnan(x) for x in tester])\n\ntype(tester)\n'];False;['import pandas as pd\nset([type(x) for x in tester])\n\ntester\n\nset([type(x) for x in tester])\n\nnp.isnan(tester)\n\nnp.isnan(tester)\n\n\nset([np.isnan(x) for x in tester])\n\ntype(tester)\n'];False;0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
1123;1123;1123;1123;2.0;7;36038927;;1;11;<python><pandas><statsmodels>;What's the difference between pandas ACF and statsmodel ACF?;3835.0;"[""import pandas as pd\nfrom pandas_datareader import data\nimport matplotlib.pyplot as plt\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom statsmodels.tsa.stattools import acf, pacf\n\nticker = 'AAPL'\ntime_ago = datetime.datetime.today().date() - relativedelta(months = 6)\n\nticker_data = data.get_data_yahoo(ticker, time_ago)['Adj Close'].pct_change().dropna()\nticker_data_len = len(ticker_data)\n\nticker_data_acf_1 =  acf(ticker_data)[1:32]\nticker_data_acf_2 = [ticker_data.autocorr(i) for i in range(1,32)]\n\ntest_df = pd.DataFrame([ticker_data_acf_1, ticker_data_acf_2]).T\ntest_df.columns = ['Pandas Autocorr', 'Statsmodels Autocorr']\ntest_df.index += 1\ntest_df.plot(kind='bar')\n""]";"[""import pandas as pd\nfrom pandas_datareader import data\nimport matplotlib.pyplot as plt\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom statsmodels.tsa.stattools import acf, pacf\n\nticker = 'AAPL'\ntime_ago = datetime.datetime.today().date() - relativedelta(months = 6)\n\nticker_data = data.get_data_yahoo(ticker, time_ago)['Adj Close'].pct_change().dropna()\nticker_data_len = len(ticker_data)\n\nticker_data_acf_1 =  acf(ticker_data)[1:32]\nticker_data_acf_2 = [ticker_data.autocorr(i) for i in range(1,32)]\n\ntest_df = pd.DataFrame([ticker_data_acf_1, ticker_data_acf_2]).T\ntest_df.columns = ['Pandas Autocorr', 'Statsmodels Autocorr']\ntest_df.index += 1\ntest_df.plot(kind='bar')\n""]";"['autocorr', 'acf', 'statsmodels.tsa', ""import pandas as pd\nfrom pandas_datareader import data\nimport matplotlib.pyplot as plt\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom statsmodels.tsa.stattools import acf, pacf\n\nticker = 'AAPL'\ntime_ago = datetime.datetime.today().date() - relativedelta(months = 6)\n\nticker_data = data.get_data_yahoo(ticker, time_ago)['Adj Close'].pct_change().dropna()\nticker_data_len = len(ticker_data)\n\nticker_data_acf_1 =  acf(ticker_data)[1:32]\nticker_data_acf_2 = [ticker_data.autocorr(i) for i in range(1,32)]\n\ntest_df = pd.DataFrame([ticker_data_acf_1, ticker_data_acf_2]).T\ntest_df.columns = ['Pandas Autocorr', 'Statsmodels Autocorr']\ntest_df.index += 1\ntest_df.plot(kind='bar')\n""]";"[""import pandas as pd\nfrom pandas_datareader import data\nimport matplotlib.pyplot as plt\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom statsmodels.tsa.stattools import acf, pacf\n\nticker = 'AAPL'\ntime_ago = datetime.datetime.today().date() - relativedelta(months = 6)\n\nticker_data = data.get_data_yahoo(ticker, time_ago)['Adj Close'].pct_change().dropna()\nticker_data_len = len(ticker_data)\n\nticker_data_acf_1 =  acf(ticker_data)[1:32]\nticker_data_acf_2 = [ticker_data.autocorr(i) for i in range(1,32)]\n\ntest_df = pd.DataFrame([ticker_data_acf_1, ticker_data_acf_2]).T\ntest_df.columns = ['Pandas Autocorr', 'Statsmodels Autocorr']\ntest_df.index += 1\ntest_df.plot(kind='bar')\n""]";"[""import pandas as pd\nfrom pandas_datareader import data\nimport matplotlib.pyplot as plt\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom statsmodels.tsa.stattools import acf, pacf\n\nticker = 'AAPL'\ntime_ago = datetime.datetime.today().date() - relativedelta(months = 6)\n\nticker_data = data.get_data_yahoo(ticker, time_ago)['Adj Close'].pct_change().dropna()\nticker_data_len = len(ticker_data)\n\nticker_data_acf_1 =  acf(ticker_data)[1:32]\nticker_data_acf_2 = [ticker_data.autocorr(i) for i in range(1,32)]\n\ntest_df = pd.DataFrame([ticker_data_acf_1, ticker_data_acf_2]).T\ntest_df.columns = ['Pandas Autocorr', 'Statsmodels Autocorr']\ntest_df.index += 1\ntest_df.plot(kind='bar')\n""]";False;"[""import pandas as pd\nimport pandas as pd\nfrom pandas_datareader import data\nimport matplotlib.pyplot as plt\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom statsmodels.tsa.stattools import acf, pacf\n\nticker = 'AAPL'\ntime_ago = datetime.datetime.today().date() - relativedelta(months = 6)\n\nticker_data = data.get_data_yahoo(ticker, time_ago)['Adj Close'].pct_change().dropna()\nticker_data_len = len(ticker_data)\n\nticker_data_acf_1 =  acf(ticker_data)[1:32]\nticker_data_acf_2 = [ticker_data.autocorr(i) for i in range(1,32)]\n\ntest_df = pd.DataFrame([ticker_data_acf_1, ticker_data_acf_2]).T\ntest_df.columns = ['Pandas Autocorr', 'Statsmodels Autocorr']\ntest_df.index += 1\ntest_df.plot(kind='bar')\n""]";False;0;0;[];[];0;0;[];[];0;0;[];[]
1124;1124;1124;1124;1.0;0;36226083;;1;20;<python><pandas><dataframe><nan>;How to find which columns contain any NaN value in Pandas dataframe (python);12115.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
1125;1125;1125;1125;1.0;0;36519086;;1;29;<python><pandas><ipython>;Pandas: how to get rid of `Unnamed:` column in a dataframe;12496.0;"[""merge.to_csv('xy.df', mode = 'w', inplace=False)\n""]";"[""merge.to_csv('xy.df', mode = 'w', inplace=False)\n""]";"['csv', 'df', 'unnamed:0', ""merge.to_csv('xy.df', mode = 'w', inplace=False)\n"", 'unnamed:0']";"[""merge.to_csv('xy.df', mode = 'w', inplace=False)\n""]";"[""merge.to_csv('xy.df', mode = 'w', inplace=False)\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\nmerge.to_csv('xy.df', mode = 'w', inplace=False)\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
1126;1126;1126;1126;6.0;0;36539396;;1;11;<python><pandas>;How to create a DataFrame while preserving order of the columns?;2740.0;"[""foo = np.array( [ 1, 2, 3 ] )\nbar = np.array( [ 4, 5, 6 ] )\npd.DataFrame( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) } )\n\n    bar foo\n0   4   1\n1   5   2\n2   6   3\npd.DataFrame( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) }, columns = [ 'foo', 'bar' ] )\n""]";"['foo = np.array( [ 1, 2, 3 ] )\nbar = np.array( [ 4, 5, 6 ] )\n', ""pd.DataFrame( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) } )\n\n    bar foo\n0   4   1\n1   5   2\n2   6   3\n"", ""pd.DataFrame( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) }, columns = [ 'foo', 'bar' ] )\n""]";"['numpy', 'Pandas', 'Pandas', 'numpy', 'Pandas', 'foo = np.array( [ 1, 2, 3 ] )\nbar = np.array( [ 4, 5, 6 ] )\n', 'bar', 'dict', ""pd.DataFrame( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) } )\n\n    bar foo\n0   4   1\n1   5   2\n2   6   3\n"", ""pd.DataFrame( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) }, columns = [ 'foo', 'bar' ] )\n""]";"[""foo = np.array( [ 1, 2, 3 ] )\nbar = np.array( [ 4, 5, 6 ] )\npd.DataFrame( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) } )\n\npd.DataFrame( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) }, columns = [ 'foo', 'bar' ] )\n""]";"[""import pandas as pd\nfoo = np.array( [ 1, 2, 3 ] )\nbar = np.array( [ 4, 5, 6 ] )\npd.DataFrame( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) } )\n\npd.DataFrame( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) }, columns = [ 'foo', 'bar' ] )\n""]";True;"[""import pandas as pd\nfoo = np.array( [ 1, 2, 3 ] )\nbar = np.array( [ 4, 5, 6 ] )\npd.DataFrame( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) } )\n\npd.DataFrame( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) }, columns = [ 'foo', 'bar' ] )\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1127;1127;1127;1127;1.0;0;36631163;;1;15;<python><pandas><machine-learning><scikit-learn><dummy-variable>;Panda's get_dummies vs. Sklearn's OneHotEncoder() :: What is more efficient?;2615.0;"['import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nsns.set()\n\n%matplotlib inline\n\n#Iris Plot\niris = load_iris()\nn_samples, m_features = iris.data.shape\n\n#Load Data\nX, y = iris.data, iris.target\nD_target_dummy = dict(zip(np.arange(iris.target_names.shape[0]), iris.target_names))\n\nDF_data = pd.DataFrame(X,columns=iris.feature_names)\nDF_data[""target""] = pd.Series(y).map(D_target_dummy)\n#sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n#0                  5.1               3.5                1.4               0.2   \n#1                  4.9               3.0                1.4               0.2   \n#2                  4.7               3.2                1.3               0.2   \n#3                  4.6               3.1                1.5               0.2   \n#4                  5.0               3.6                1.4               0.2   \n#5                  5.4               3.9                1.7               0.4   \n\nDF_dummies = pd.get_dummies(DF_data[""target""])\n#setosa  versicolor  virginica\n#0         1           0          0\n#1         1           0          0\n#2         1           0          0\n#3         1           0          0\n#4         1           0          0\n#5         1           0          0\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\ndef f1(DF_data):\n    Enc_ohe, Enc_label = OneHotEncoder(), LabelEncoder()\n    DF_data[""Dummies""] = Enc_label.fit_transform(DF_data[""target""])\n    DF_dummies2 = pd.DataFrame(Enc_ohe.fit_transform(DF_data[[""Dummies""]]).todense(), columns = Enc_label.classes_)\n    return(DF_dummies2)\n\n%timeit pd.get_dummies(DF_data[""target""])\n#1000 loops, best of 3: 777 µs per loop\n\n%timeit f1(DF_data)\n#100 loops, best of 3: 2.91 ms per loop\n']";"['import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nsns.set()\n\n%matplotlib inline\n\n#Iris Plot\niris = load_iris()\nn_samples, m_features = iris.data.shape\n\n#Load Data\nX, y = iris.data, iris.target\nD_target_dummy = dict(zip(np.arange(iris.target_names.shape[0]), iris.target_names))\n\nDF_data = pd.DataFrame(X,columns=iris.feature_names)\nDF_data[""target""] = pd.Series(y).map(D_target_dummy)\n#sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n#0                  5.1               3.5                1.4               0.2   \n#1                  4.9               3.0                1.4               0.2   \n#2                  4.7               3.2                1.3               0.2   \n#3                  4.6               3.1                1.5               0.2   \n#4                  5.0               3.6                1.4               0.2   \n#5                  5.4               3.9                1.7               0.4   \n\nDF_dummies = pd.get_dummies(DF_data[""target""])\n#setosa  versicolor  virginica\n#0         1           0          0\n#1         1           0          0\n#2         1           0          0\n#3         1           0          0\n#4         1           0          0\n#5         1           0          0\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\ndef f1(DF_data):\n    Enc_ohe, Enc_label = OneHotEncoder(), LabelEncoder()\n    DF_data[""Dummies""] = Enc_label.fit_transform(DF_data[""target""])\n    DF_dummies2 = pd.DataFrame(Enc_ohe.fit_transform(DF_data[[""Dummies""]]).todense(), columns = Enc_label.classes_)\n    return(DF_dummies2)\n\n%timeit pd.get_dummies(DF_data[""target""])\n#1000 loops, best of 3: 777 µs per loop\n\n%timeit f1(DF_data)\n#100 loops, best of 3: 2.91 ms per loop\n']";"['pd.get_dummies', 'sklearn.preprocessing.OneHotEncoder()', 'OneHotEnocder()', 'sklearn', 'pd.dummies', 'sklearn.preprocessing.OneHotEncoder()', 'OneHotEncoder()', 'pandas', 'import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nsns.set()\n\n%matplotlib inline\n\n#Iris Plot\niris = load_iris()\nn_samples, m_features = iris.data.shape\n\n#Load Data\nX, y = iris.data, iris.target\nD_target_dummy = dict(zip(np.arange(iris.target_names.shape[0]), iris.target_names))\n\nDF_data = pd.DataFrame(X,columns=iris.feature_names)\nDF_data[""target""] = pd.Series(y).map(D_target_dummy)\n#sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n#0                  5.1               3.5                1.4               0.2   \n#1                  4.9               3.0                1.4               0.2   \n#2                  4.7               3.2                1.3               0.2   \n#3                  4.6               3.1                1.5               0.2   \n#4                  5.0               3.6                1.4               0.2   \n#5                  5.4               3.9                1.7               0.4   \n\nDF_dummies = pd.get_dummies(DF_data[""target""])\n#setosa  versicolor  virginica\n#0         1           0          0\n#1         1           0          0\n#2         1           0          0\n#3         1           0          0\n#4         1           0          0\n#5         1           0          0\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\ndef f1(DF_data):\n    Enc_ohe, Enc_label = OneHotEncoder(), LabelEncoder()\n    DF_data[""Dummies""] = Enc_label.fit_transform(DF_data[""target""])\n    DF_dummies2 = pd.DataFrame(Enc_ohe.fit_transform(DF_data[[""Dummies""]]).todense(), columns = Enc_label.classes_)\n    return(DF_dummies2)\n\n%timeit pd.get_dummies(DF_data[""target""])\n#1000 loops, best of 3: 777 µs per loop\n\n%timeit f1(DF_data)\n#100 loops, best of 3: 2.91 ms per loop\n']";"['import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nsns.set()\n\n\n#Iris Plot\niris = load_iris()\nn_samples, m_features = iris.data.shape\n\n#Load Data\nX, y = iris.data, iris.target\nD_target_dummy = dict(zip(np.arange(iris.target_names.shape[0]), iris.target_names))\n\nDF_data = pd.DataFrame(X,columns=iris.feature_names)\nDF_data[""target""] = pd.Series(y).map(D_target_dummy)\n#sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n#0                  5.1               3.5                1.4               0.2   \n#1                  4.9               3.0                1.4               0.2   \n#2                  4.7               3.2                1.3               0.2   \n#3                  4.6               3.1                1.5               0.2   \n#4                  5.0               3.6                1.4               0.2   \n#5                  5.4               3.9                1.7               0.4   \n\nDF_dummies = pd.get_dummies(DF_data[""target""])\n#setosa  versicolor  virginica\n#0         1           0          0\n#1         1           0          0\n#2         1           0          0\n#3         1           0          0\n#4         1           0          0\n#5         1           0          0\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\n#1000 loops, best of 3: 777 µs per loop\n\n#100 loops, best of 3: 2.91 ms per loop\n']";"['import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nsns.set()\n\n\n#Iris Plot\niris = load_iris()\nn_samples, m_features = iris.data.shape\n\n#Load Data\nX, y = iris.data, iris.target\nD_target_dummy = dict(zip(np.arange(iris.target_names.shape[0]), iris.target_names))\n\nDF_data = pd.DataFrame(X,columns=iris.feature_names)\nDF_data[""target""] = pd.Series(y).map(D_target_dummy)\n#sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n#0                  5.1               3.5                1.4               0.2   \n#1                  4.9               3.0                1.4               0.2   \n#2                  4.7               3.2                1.3               0.2   \n#3                  4.6               3.1                1.5               0.2   \n#4                  5.0               3.6                1.4               0.2   \n#5                  5.4               3.9                1.7               0.4   \n\nDF_dummies = pd.get_dummies(DF_data[""target""])\n#setosa  versicolor  virginica\n#0         1           0          0\n#1         1           0          0\n#2         1           0          0\n#3         1           0          0\n#4         1           0          0\n#5         1           0          0\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\n#1000 loops, best of 3: 777 µs per loop\n\n#100 loops, best of 3: 2.91 ms per loop\n']";False;"['import pandas as pd\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nsns.set()\n\n\n#Iris Plot\niris = load_iris()\nn_samples, m_features = iris.data.shape\n\n#Load Data\nX, y = iris.data, iris.target\nD_target_dummy = dict(zip(np.arange(iris.target_names.shape[0]), iris.target_names))\n\nDF_data = pd.DataFrame(X,columns=iris.feature_names)\nDF_data[""target""] = pd.Series(y).map(D_target_dummy)\n#sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n#0                  5.1               3.5                1.4               0.2   \n#1                  4.9               3.0                1.4               0.2   \n#2                  4.7               3.2                1.3               0.2   \n#3                  4.6               3.1                1.5               0.2   \n#4                  5.0               3.6                1.4               0.2   \n#5                  5.4               3.9                1.7               0.4   \n\nDF_dummies = pd.get_dummies(DF_data[""target""])\n#setosa  versicolor  virginica\n#0         1           0          0\n#1         1           0          0\n#2         1           0          0\n#3         1           0          0\n#4         1           0          0\n#5         1           0          0\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\n#1000 loops, best of 3: 777 µs per loop\n\n#100 loops, best of 3: 2.91 ms per loop\n']";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1128;1128;1128;1128;3.0;1;36684013;;1;14;<python><pandas><dataframe>;extract column value based on another column pandas dataframe;12926.0;[''];[];['A  B\n p1 1\n p1 2\n p3 3\n p2 4', 'A', 'B=3'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'B'""]";['KeyError']
1129;1129;1129;1129;3.0;0;36694313;;1;12;<python><pandas><xlsxwriter>;pandas xlsxwriter, format header;2410.0;"[""import pandas as pd\ndata = pd.DataFrame({'test_data': [1,2,3,4,5]})\nwriter = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')\n\ndata.to_excel(writer, sheet_name='test', index=False)\n\nworkbook  = writer.book\nworksheet = writer.sheets['test']\n\nfont_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10})\nheader_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10, 'bold': True})\n\nworksheet.set_column('A:A', None, font_fmt)\nworksheet.set_row(0, None, header_fmt)\n\nwriter.save()\n""]";"[""import pandas as pd\ndata = pd.DataFrame({'test_data': [1,2,3,4,5]})\nwriter = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')\n\ndata.to_excel(writer, sheet_name='test', index=False)\n\nworkbook  = writer.book\nworksheet = writer.sheets['test']\n\nfont_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10})\nheader_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10, 'bold': True})\n\nworksheet.set_column('A:A', None, font_fmt)\nworksheet.set_row(0, None, header_fmt)\n\nwriter.save()\n""]";"[""import pandas as pd\ndata = pd.DataFrame({'test_data': [1,2,3,4,5]})\nwriter = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')\n\ndata.to_excel(writer, sheet_name='test', index=False)\n\nworkbook  = writer.book\nworksheet = writer.sheets['test']\n\nfont_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10})\nheader_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10, 'bold': True})\n\nworksheet.set_column('A:A', None, font_fmt)\nworksheet.set_row(0, None, header_fmt)\n\nwriter.save()\n""]";"[""import pandas as pd\ndata = pd.DataFrame({'test_data': [1,2,3,4,5]})\nwriter = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')\n\ndata.to_excel(writer, sheet_name='test', index=False)\n\nworkbook  = writer.book\nworksheet = writer.sheets['test']\n\nfont_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10})\nheader_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10, 'bold': True})\n\nworksheet.set_column('A:A', None, font_fmt)\nworksheet.set_row(0, None, header_fmt)\n\nwriter.save()\n""]";"[""import pandas as pd\ndata = pd.DataFrame({'test_data': [1,2,3,4,5]})\nwriter = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')\n\ndata.to_excel(writer, sheet_name='test', index=False)\n\nworkbook  = writer.book\nworksheet = writer.sheets['test']\n\nfont_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10})\nheader_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10, 'bold': True})\n\nworksheet.set_column('A:A', None, font_fmt)\nworksheet.set_row(0, None, header_fmt)\n\nwriter.save()\n""]";False;"[""import pandas as pd\nimport pandas as pd\ndata = pd.DataFrame({'test_data': [1,2,3,4,5]})\nwriter = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')\n\ndata.to_excel(writer, sheet_name='test', index=False)\n\nworkbook  = writer.book\nworksheet = writer.sheets['test']\n\nfont_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10})\nheader_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10, 'bold': True})\n\nworksheet.set_column('A:A', None, font_fmt)\nworksheet.set_row(0, None, header_fmt)\n\nwriter.save()\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""module 'pandas.core' has no attribute 'format'""]";['AttributeError']
1130;1130;1130;1130;2.0;0;36891977;;1;12;<python><pandas><dataframe><diff>;Pandas: Diff of two Dataframes;6038.0;"[""df1 = DataFrame({\n'Buyer': ['Carl', 'Carl', 'Carl'],\n'Quantity': [18, 3, 5, ]})\n\ndf2 = DataFrame({\n'Buyer': ['Carl', 'Mark', 'Carl', 'Carl'],\n'Quantity': [2, 1, 18, 5]})\nBuyer     Quantity \nCarl         2\nMark         1\nBuyer     Quantity \nCarl         3\n""]";"[""df1 = DataFrame({\n'Buyer': ['Carl', 'Carl', 'Carl'],\n'Quantity': [18, 3, 5, ]})\n\ndf2 = DataFrame({\n'Buyer': ['Carl', 'Mark', 'Carl', 'Carl'],\n'Quantity': [2, 1, 18, 5]})\n"", 'Buyer     Quantity \nCarl         2\nMark         1\n', 'Buyer     Quantity \nCarl         3\n']";"[""df1 = DataFrame({\n'Buyer': ['Carl', 'Carl', 'Carl'],\n'Quantity': [18, 3, 5, ]})\n\ndf2 = DataFrame({\n'Buyer': ['Carl', 'Mark', 'Carl', 'Carl'],\n'Quantity': [2, 1, 18, 5]})\n"", 'Buyer     Quantity \nCarl         2\nMark         1\n', 'Buyer     Quantity \nCarl         3\n']";['\n'];['\n'];False;['import pandas as pd\n\n'];False;0;1;"[""name 'df1' is not defined""]";['NameError'];0;1;"[""name 'df1' is not defined""]";['NameError'];0;1;['No common columns to perform merge on'];['MergeError']
1131;1131;1131;1131;3.0;2;36921951;;1;26;<python><pandas><dataframe><boolean><filtering>;Truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all();44469.0;"["" result = result[(result['var']>0.25) or (result['var']<-0.25)]\n""]";"["" result = result[(result['var']>0.25) or (result['var']<-0.25)]\n""]";"["" result = result[(result['var']>0.25) or (result['var']<-0.25)]\n""]";[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'result' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'result' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'result' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError']
1132;1132;1132;1132;1.0;9;37010212;;1;15;<python><csv><pandas><dataframe>;What is the fastest way to upload a big csv file in notebook to work with python pandas?;1296.0;"[""location = r'C:\\Users\\Name\\Folder_1\\Folder_2\\file.csv'\ndf = pd.read_csv(location)\n""]";"[""location = r'C:\\Users\\Name\\Folder_1\\Folder_2\\file.csv'\ndf = pd.read_csv(location)\n""]";"[""location = r'C:\\Users\\Name\\Folder_1\\Folder_2\\file.csv'\ndf = pd.read_csv(location)\n""]";"[""location = r'C:\\Users\\Name\\Folder_1\\Folder_2\\file.csv'\ndf = pd.read_csv(location)\n""]";"[""import pandas as pd\nlocation = r'C:\\Users\\Name\\Folder_1\\Folder_2\\file.csv'\ndf = pd.read_csv(location)\n""]";True;"[""import pandas as pd\nlocation = r'C:\\Users\\Name\\Folder_1\\Folder_2\\file.csv'\ndf = pd.read_csv(location)\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
1133;1133;1133;1133;0.0;11;37078880;;1;29;<python><r><pandas><parallel-processing><mclapply>;Status of parallelization of pandas.apply();4502.0;[''];[];['parallelization', 'pandas.apply()', 'pandas.apply()', 'parallelization', 'R', 'mclapply'];[''];[''];False;['import pandas as pd\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
1134;1134;1134;1134;8.0;0;37292872;;1;12;<python><pandas><machine-learning><anaconda><one-hot-encoding>;How can I one hot encode in Python?;26080.0;"['num_rows_to_read = 10000\ntrain_small = pd.read_csv(""../../dataset/train.csv"",   nrows=num_rows_to_read)\nnon_categorial_features = [\'orig_destination_distance\',\n                          \'srch_adults_cnt\',\n                          \'srch_children_cnt\',\n                          \'srch_rm_cnt\',\n                          \'cnt\']\n\nfor categorical_feature in list(train_small.columns):\n    if categorical_feature not in non_categorial_features:\n        train_small[categorical_feature] = train_small[categorical_feature].astype(\'category\')\ntrain_small_with_dummies = pd.get_dummies(train_small, sparse=True)\n']";"['num_rows_to_read = 10000\ntrain_small = pd.read_csv(""../../dataset/train.csv"",   nrows=num_rows_to_read)\n', ""non_categorial_features = ['orig_destination_distance',\n                          'srch_adults_cnt',\n                          'srch_children_cnt',\n                          'srch_rm_cnt',\n                          'cnt']\n\nfor categorical_feature in list(train_small.columns):\n    if categorical_feature not in non_categorial_features:\n        train_small[categorical_feature] = train_small[categorical_feature].astype('category')\n"", 'train_small_with_dummies = pd.get_dummies(train_small, sparse=True)\n']";"['num_rows_to_read = 10000\ntrain_small = pd.read_csv(""../../dataset/train.csv"",   nrows=num_rows_to_read)\n', ""non_categorial_features = ['orig_destination_distance',\n                          'srch_adults_cnt',\n                          'srch_children_cnt',\n                          'srch_rm_cnt',\n                          'cnt']\n\nfor categorical_feature in list(train_small.columns):\n    if categorical_feature not in non_categorial_features:\n        train_small[categorical_feature] = train_small[categorical_feature].astype('category')\n"", 'train_small_with_dummies = pd.get_dummies(train_small, sparse=True)\n']";"['num_rows_to_read = 10000\ntrain_small = pd.read_csv(""../../dataset/train.csv"",   nrows=num_rows_to_read)\n\ntrain_small_with_dummies = pd.get_dummies(train_small, sparse=True)\n']";"['import pandas as pd\nnum_rows_to_read = 10000\ntrain_small = pd.read_csv(""../../dataset/train.csv"",   nrows=num_rows_to_read)\n\ntrain_small_with_dummies = pd.get_dummies(train_small, sparse=True)\n']";True;"['import pandas as pd\ndata = pd.DataFrame()\nnum_rows_to_read = 10000\ntrain_small = pd.read_csv(""../../dataset/train.csv"",   nrows=num_rows_to_read)\n\ntrain_small_with_dummies = pd.get_dummies(train_small, sparse=True)\n']";True;3;4;"[""No module named 'sklearn'"", 'Sucess', 'Sucess', 'Sucess']";['ImportError', 'Sucess', 'Sucess', 'Sucess'];3;4;"[""No module named 'sklearn'"", 'Sucess', 'Sucess', 'Sucess']";['ImportError', 'Sucess', 'Sucess', 'Sucess'];3;4;"[""No module named 'sklearn'"", 'Sucess', 'Sucess', 'Sucess']";['ImportError', 'Sucess', 'Sucess', 'Sucess']
1135;1135;1135;1135;5.0;4;37425961;;1;11;<python><pandas><machine-learning><dummy-variable>;Dummy variables when not all categories are present;1745.0;"[""categories = ['a', 'b', 'c']\n\n   cat\n1   a\n2   b\n3   a\n  cat_a  cat_b  cat_c\n1   1      0      0\n2   0      1      0\n3   1      0      0\n""]";"[""categories = ['a', 'b', 'c']\n\n   cat\n1   a\n2   b\n3   a\n"", '  cat_a  cat_b  cat_c\n1   1      0      0\n2   0      1      0\n3   1      0      0\n']";"['get_dummies', 'get_dummies', 'get_dummies', ""categories = ['a', 'b', 'c']\n\n   cat\n1   a\n2   b\n3   a\n"", '  cat_a  cat_b  cat_c\n1   1      0      0\n2   0      1      0\n3   1      0      0\n']";"[""categories = ['a', 'b', 'c']\n\n""]";"[""categories = ['a', 'b', 'c']\n\n""]";False;"[""import pandas as pd\ncategories = ['a', 'b', 'c']\n\n""]";False;1;2;"['Sucess', ""name 'cat' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'cat' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'cat' is not defined""]";['Sucess', 'NameError']
1136;1136;1136;1136;3.0;1;37561991;;1;15;<python><pandas><numpy><dataframe><types>;What does a dtype of 'O' mean?;7438.0;"[""dtype('O')\n""]";"[""dtype('O')\n""]";"[""'Test'"", ""myFrame['Test'].dtype"", ""dtype('O')\n""]";"[""dtype('O')\n""]";"[""dtype('O')\n""]";False;"[""import pandas as pd\ndtype('O')\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1137;1137;1137;1137;5.0;0;38105539;;1;17;<dataset><scikit-learn><pandas>;How to convert a Scikit-learn dataset to a Pandas dataset?;7406.0;['from sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\n'];['from sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\n'];['from sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\n'];['from sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\n'];['from sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\n'];False;['import pandas as pd\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\n'];False;0;2;"[""No module named 'sklearn'"", ""No module named 'sklearn'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'sklearn'"", ""No module named 'sklearn'""]";['ImportError', 'ImportError'];0;2;"[""No module named 'sklearn'"", ""No module named 'sklearn'""]";['ImportError', 'ImportError']
1138;1138;1138;1138;1.0;0;38134012;;1;13;<python><python-2.7><pandas>;Pandas dataframe fillna() only some columns in place;8092.0;"[""import pandas as pd\ndf = pd.DataFrame(data={'a':[1,2,3,None],'b':[4,5,None,6],'c':[None,None,7,8]})\nprint df\ndf.fillna(value=0, inplace=True)\nprint df\n     a    b    c\n0  1.0  4.0  NaN\n1  2.0  5.0  NaN\n2  3.0  NaN  7.0\n3  NaN  6.0  8.0\n     a    b    c\n0  1.0  4.0  0.0\n1  2.0  5.0  0.0\n2  3.0  0.0  7.0\n3  0.0  6.0  8.0\n""]";"[""import pandas as pd\ndf = pd.DataFrame(data={'a':[1,2,3,None],'b':[4,5,None,6],'c':[None,None,7,8]})\nprint df\ndf.fillna(value=0, inplace=True)\nprint df\n"", '     a    b    c\n0  1.0  4.0  NaN\n1  2.0  5.0  NaN\n2  3.0  NaN  7.0\n3  NaN  6.0  8.0\n     a    b    c\n0  1.0  4.0  0.0\n1  2.0  5.0  0.0\n2  3.0  0.0  7.0\n3  0.0  6.0  8.0\n']";"[""import pandas as pd\ndf = pd.DataFrame(data={'a':[1,2,3,None],'b':[4,5,None,6],'c':[None,None,7,8]})\nprint df\ndf.fillna(value=0, inplace=True)\nprint df\n"", '     a    b    c\n0  1.0  4.0  NaN\n1  2.0  5.0  NaN\n2  3.0  NaN  7.0\n3  NaN  6.0  8.0\n     a    b    c\n0  1.0  4.0  0.0\n1  2.0  5.0  0.0\n2  3.0  0.0  7.0\n3  0.0  6.0  8.0\n', 'None', '0', 'None', 'a', 'b', 'c']";"[""import pandas as pd\ndf = pd.DataFrame(data={'a':[1,2,3,None],'b':[4,5,None,6],'c':[None,None,7,8]})\ndf.fillna(value=0, inplace=True)\n""]";"[""import pandas as pd\ndf = pd.DataFrame(data={'a':[1,2,3,None],'b':[4,5,None,6],'c':[None,None,7,8]})\ndf.fillna(value=0, inplace=True)\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nimport pandas as pd\ndf = pd.DataFrame(data={'a':[1,2,3,None],'b':[4,5,None,6],'c':[None,None,7,8]})\ndf.fillna(value=0, inplace=True)\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"['""[\'a\' \'b\'] not in index""']";['KeyError']
1139;1139;1139;1139;1.0;3;38154997;;1;11;<python><performance><pandas><dataframe>;Setting values on Pandas DataFrame subset (copy) is slow;596.0;"[""import timeit\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.rand(10, 10))\n\ndft = df[[True, False] * 5]\n# df = dft\ndft2 = dft.copy()\n\nnew_data = np.random.rand(5, 10)\n\nprint(timeit.timeit('dft.loc[:, :] = new_data', setup='from __main__ import dft, new_data', number=100))\nprint(timeit.timeit('dft2.loc[:, :] = new_data', setup='from __main__ import dft2, new_data', number=100))\n""]";"[""import timeit\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.rand(10, 10))\n\ndft = df[[True, False] * 5]\n# df = dft\ndft2 = dft.copy()\n\nnew_data = np.random.rand(5, 10)\n\nprint(timeit.timeit('dft.loc[:, :] = new_data', setup='from __main__ import dft, new_data', number=100))\nprint(timeit.timeit('dft2.loc[:, :] = new_data', setup='from __main__ import dft2, new_data', number=100))\n""]";"[""import timeit\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.rand(10, 10))\n\ndft = df[[True, False] * 5]\n# df = dft\ndft2 = dft.copy()\n\nnew_data = np.random.rand(5, 10)\n\nprint(timeit.timeit('dft.loc[:, :] = new_data', setup='from __main__ import dft, new_data', number=100))\nprint(timeit.timeit('dft2.loc[:, :] = new_data', setup='from __main__ import dft2, new_data', number=100))\n"", 'dft', 'dft2', 'dft', 'dft', 'dft2', 'df', 'df = dft']";"[""import timeit\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.rand(10, 10))\n\ndft = df[[True, False] * 5]\n# df = dft\ndft2 = dft.copy()\n\nnew_data = np.random.rand(5, 10)\n\nprint(timeit.timeit('dft.loc[:, :] = new_data', setup='from __main__ import dft, new_data', number=100))\nprint(timeit.timeit('dft2.loc[:, :] = new_data', setup='from __main__ import dft2, new_data', number=100))\n""]";"[""import timeit\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.rand(10, 10))\n\ndft = df[[True, False] * 5]\n# df = dft\ndft2 = dft.copy()\n\nnew_data = np.random.rand(5, 10)\n\nprint(timeit.timeit('dft.loc[:, :] = new_data', setup='from __main__ import dft, new_data', number=100))\nprint(timeit.timeit('dft2.loc[:, :] = new_data', setup='from __main__ import dft2, new_data', number=100))\n""]";False;"[""import pandas as pd\nimport timeit\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.rand(10, 10))\n\ndft = df[[True, False] * 5]\n# df = dft\ndft2 = dft.copy()\n\nnew_data = np.random.rand(5, 10)\n\nprint(timeit.timeit('dft.loc[:, :] = new_data', setup='from __main__ import dft, new_data', number=100))\nprint(timeit.timeit('dft2.loc[:, :] = new_data', setup='from __main__ import dft2, new_data', number=100))\n""]";False;0;0;[];[];0;0;[];[];0;0;[];[]
1140;1140;1140;1140;3.0;2;38231591;;1;14;<python><pandas><dictionary><dataframe>;Splitting dictionary/list inside a Pandas Column into Separate Columns;6897.0;"['[1] df\nStation ID     Pollutants\n8809           {""a"": ""46"", ""b"": ""3"", ""c"": ""12""}\n8810           {""a"": ""36"", ""b"": ""5"", ""c"": ""8""}\n8811           {""b"": ""2"", ""c"": ""7""}\n8812           {""c"": ""11""}\n8813           {""a"": ""82"", ""c"": ""15""}\n[2] df2\nStation ID     a      b       c\n8809           46     3       12\n8810           36     5       8\n8811           NaN    2       7\n8812           NaN    NaN     11\n8813           82     NaN     15\n[3] df \n[4] objs = [df, pandas.DataFrame(df[\'Pollutant Levels\'].tolist()).iloc[:, :3]]\n[5] df2 = pandas.concat(objs, axis=1).drop(\'Pollutant Levels\', axis=1)\n[6] print(df2)\nIndexError: out-of-bounds on slice (end) \n#My data format \nu{\'a\': \'1\', \'b\': \'2\', \'c\': \'3\'}\n\n#and not\n{u\'a\': \'1\', u\'b\': \'2\', u\'c\': \'3\'}\n']";"['[1] df\nStation ID     Pollutants\n8809           {""a"": ""46"", ""b"": ""3"", ""c"": ""12""}\n8810           {""a"": ""36"", ""b"": ""5"", ""c"": ""8""}\n8811           {""b"": ""2"", ""c"": ""7""}\n8812           {""c"": ""11""}\n8813           {""a"": ""82"", ""c"": ""15""}\n', '[2] df2\nStation ID     a      b       c\n8809           46     3       12\n8810           36     5       8\n8811           NaN    2       7\n8812           NaN    NaN     11\n8813           82     NaN     15\n', ""[3] df \n[4] objs = [df, pandas.DataFrame(df['Pollutant Levels'].tolist()).iloc[:, :3]]\n[5] df2 = pandas.concat(objs, axis=1).drop('Pollutant Levels', axis=1)\n[6] print(df2)\n"", 'IndexError: out-of-bounds on slice (end) \n', ""#My data format \nu{'a': '1', 'b': '2', 'c': '3'}\n\n#and not\n{u'a': '1', u'b': '2', u'c': '3'}\n""]";"['[1] df\nStation ID     Pollutants\n8809           {""a"": ""46"", ""b"": ""3"", ""c"": ""12""}\n8810           {""a"": ""36"", ""b"": ""5"", ""c"": ""8""}\n8811           {""b"": ""2"", ""c"": ""7""}\n8812           {""c"": ""11""}\n8813           {""a"": ""82"", ""c"": ""15""}\n', '[2] df2\nStation ID     a      b       c\n8809           46     3       12\n8810           36     5       8\n8811           NaN    2       7\n8812           NaN    NaN     11\n8813           82     NaN     15\n', ""[3] df \n[4] objs = [df, pandas.DataFrame(df['Pollutant Levels'].tolist()).iloc[:, :3]]\n[5] df2 = pandas.concat(objs, axis=1).drop('Pollutant Levels', axis=1)\n[6] print(df2)\n"", 'IndexError: out-of-bounds on slice (end) \n', ""#My data format \nu{'a': '1', 'b': '2', 'c': '3'}\n\n#and not\n{u'a': '1', u'b': '2', u'c': '3'}\n""]";"[""#My data format \n\n#and not\n{u'a': '1', u'b': '2', u'c': '3'}\n""]";"[""#My data format \n\n#and not\n{u'a': '1', u'b': '2', u'c': '3'}\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\n#My data format \n\n#and not\n{u'a': '1', u'b': '2', u'c': '3'}\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1141;1141;1141;1141;3.0;7;38250710;;1;34;<pandas><numpy><dataframe><machine-learning><scikit-learn>;How to split data into 3 sets (train, validation and test)?;12872.0;[''];[];['sklearn.cross_validation', 'train_test_split'];[''];[''];False;['import pandas as pd\n'];False;0;3;"[""name 'train_validate_test_split' is not defined"", ""name 'np' is not defined"", ""name 'train_test_split' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'train_validate_test_split' is not defined"", ""name 'np' is not defined"", ""name 'train_test_split' is not defined""]";['NameError', 'NameError', 'NameError'];0;3;"[""name 'train_validate_test_split' is not defined"", ""name 'np' is not defined"", ""name 'train_test_split' is not defined""]";['NameError', 'NameError', 'NameError']
1142;1142;1142;1142;2.0;12;38254067;;1;15;<python><performance><pandas>;Comparison of Pandas lookup times;326.0;"[""import pandas as pd\nimport numpy as np\nimport itertools\n\nletters = [chr(x) for x in range(ord('a'), ord('z'))]\nletter_combinations = [''.join(x) for x in itertools.combinations(letters, 3)]\n\ndf1 = pd.DataFrame({\n        'value': np.random.normal(size=(1000000)), \n        'letter': np.random.choice(letter_combinations, 1000000)\n    })\ndf2 = df1.sort_values('letter')\ndf3 = df1.set_index('letter')\ndf4 = df3.sort_index()\nprint(df1.head(5))\n\n\n>>>\n  letter     value\n0    bdh  0.253778\n1    cem -1.915726\n2    mru -0.434007\n3    lnw -1.286693\n4    fjv  0.245523\nprint('~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / UNSORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n%timeit df1[df1.letter == 'ben']\n%timeit df1[df1.letter == 'amy']\n%timeit df1[df1.letter == 'abe']\n\nprint('~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / SORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n%timeit df2[df2.letter == 'ben']\n%timeit df2[df2.letter == 'amy']\n%timeit df2[df2.letter == 'abe']\n\nprint('~~~~~~~~~~~~~~~~~~~~~INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n%timeit df3.loc['ben']\n%timeit df3.loc['amy']\n%timeit df3.loc['abe']\n\nprint('~~~~~~~~~~~~~~~~~~~~~SORTED INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n%timeit df4.loc['ben']\n%timeit df4.loc['amy']\n%timeit df4.loc['abe']\n~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / UNSORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n10 loops, best of 3: 59.7 ms per loop\n10 loops, best of 3: 59.7 ms per loop\n10 loops, best of 3: 59.7 ms per loop\n~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / SORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n10 loops, best of 3: 192 ms per loop\n10 loops, best of 3: 192 ms per loop\n10 loops, best of 3: 193 ms per loop\n~~~~~~~~~~~~~~~~~~~~~INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThe slowest run took 4.66 times longer than the fastest. This could mean that an intermediate result is being cached \n10 loops, best of 3: 40.9 ms per loop\n10 loops, best of 3: 41 ms per loop\n10 loops, best of 3: 40.9 ms per loop\n~~~~~~~~~~~~~~~~~~~~~SORTED INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThe slowest run took 1621.00 times longer than the fastest. This could mean that an intermediate result is being cached \n1 loops, best of 3: 259 µs per loop\n1000 loops, best of 3: 242 µs per loop\n1000 loops, best of 3: 243 µs per loop\n""]";"[""import pandas as pd\nimport numpy as np\nimport itertools\n\nletters = [chr(x) for x in range(ord('a'), ord('z'))]\nletter_combinations = [''.join(x) for x in itertools.combinations(letters, 3)]\n\ndf1 = pd.DataFrame({\n        'value': np.random.normal(size=(1000000)), \n        'letter': np.random.choice(letter_combinations, 1000000)\n    })\ndf2 = df1.sort_values('letter')\ndf3 = df1.set_index('letter')\ndf4 = df3.sort_index()\n"", 'print(df1.head(5))\n\n\n>>>\n  letter     value\n0    bdh  0.253778\n1    cem -1.915726\n2    mru -0.434007\n3    lnw -1.286693\n4    fjv  0.245523\n', ""print('~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / UNSORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n%timeit df1[df1.letter == 'ben']\n%timeit df1[df1.letter == 'amy']\n%timeit df1[df1.letter == 'abe']\n\nprint('~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / SORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n%timeit df2[df2.letter == 'ben']\n%timeit df2[df2.letter == 'amy']\n%timeit df2[df2.letter == 'abe']\n\nprint('~~~~~~~~~~~~~~~~~~~~~INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n%timeit df3.loc['ben']\n%timeit df3.loc['amy']\n%timeit df3.loc['abe']\n\nprint('~~~~~~~~~~~~~~~~~~~~~SORTED INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n%timeit df4.loc['ben']\n%timeit df4.loc['amy']\n%timeit df4.loc['abe']\n"", '~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / UNSORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n10 loops, best of 3: 59.7 ms per loop\n10 loops, best of 3: 59.7 ms per loop\n10 loops, best of 3: 59.7 ms per loop\n~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / SORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n10 loops, best of 3: 192 ms per loop\n10 loops, best of 3: 192 ms per loop\n10 loops, best of 3: 193 ms per loop\n~~~~~~~~~~~~~~~~~~~~~INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThe slowest run took 4.66 times longer than the fastest. This could mean that an intermediate result is being cached \n10 loops, best of 3: 40.9 ms per loop\n10 loops, best of 3: 41 ms per loop\n10 loops, best of 3: 40.9 ms per loop\n~~~~~~~~~~~~~~~~~~~~~SORTED INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThe slowest run took 1621.00 times longer than the fastest. This could mean that an intermediate result is being cached \n1 loops, best of 3: 259 µs per loop\n1000 loops, best of 3: 242 µs per loop\n1000 loops, best of 3: 243 µs per loop\n']";"[""import pandas as pd\nimport numpy as np\nimport itertools\n\nletters = [chr(x) for x in range(ord('a'), ord('z'))]\nletter_combinations = [''.join(x) for x in itertools.combinations(letters, 3)]\n\ndf1 = pd.DataFrame({\n        'value': np.random.normal(size=(1000000)), \n        'letter': np.random.choice(letter_combinations, 1000000)\n    })\ndf2 = df1.sort_values('letter')\ndf3 = df1.set_index('letter')\ndf4 = df3.sort_index()\n"", 'print(df1.head(5))\n\n\n>>>\n  letter     value\n0    bdh  0.253778\n1    cem -1.915726\n2    mru -0.434007\n3    lnw -1.286693\n4    fjv  0.245523\n', ""print('~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / UNSORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n%timeit df1[df1.letter == 'ben']\n%timeit df1[df1.letter == 'amy']\n%timeit df1[df1.letter == 'abe']\n\nprint('~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / SORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n%timeit df2[df2.letter == 'ben']\n%timeit df2[df2.letter == 'amy']\n%timeit df2[df2.letter == 'abe']\n\nprint('~~~~~~~~~~~~~~~~~~~~~INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n%timeit df3.loc['ben']\n%timeit df3.loc['amy']\n%timeit df3.loc['abe']\n\nprint('~~~~~~~~~~~~~~~~~~~~~SORTED INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n%timeit df4.loc['ben']\n%timeit df4.loc['amy']\n%timeit df4.loc['abe']\n"", '~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / UNSORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n10 loops, best of 3: 59.7 ms per loop\n10 loops, best of 3: 59.7 ms per loop\n10 loops, best of 3: 59.7 ms per loop\n~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / SORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n10 loops, best of 3: 192 ms per loop\n10 loops, best of 3: 192 ms per loop\n10 loops, best of 3: 193 ms per loop\n~~~~~~~~~~~~~~~~~~~~~INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThe slowest run took 4.66 times longer than the fastest. This could mean that an intermediate result is being cached \n10 loops, best of 3: 40.9 ms per loop\n10 loops, best of 3: 41 ms per loop\n10 loops, best of 3: 40.9 ms per loop\n~~~~~~~~~~~~~~~~~~~~~SORTED INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThe slowest run took 1621.00 times longer than the fastest. This could mean that an intermediate result is being cached \n1 loops, best of 3: 259 µs per loop\n1000 loops, best of 3: 242 µs per loop\n1000 loops, best of 3: 243 µs per loop\n', 'df2', 'df1', 'The slowest run took x times longer than the fastest. This could mean that an intermediate result is being cached', '.loc[]']";"[""import pandas as pd\nimport numpy as np\nimport itertools\n\nletters = [chr(x) for x in range(ord('a'), ord('z'))]\nletter_combinations = [''.join(x) for x in itertools.combinations(letters, 3)]\n\ndf2 = df1.sort_values('letter')\ndf3 = df1.set_index('letter')\ndf4 = df3.sort_index()\nprint(df1.head(5))\n\n\nprint('~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / UNSORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n\nprint('~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / SORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n\nprint('~~~~~~~~~~~~~~~~~~~~~INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n\nprint('~~~~~~~~~~~~~~~~~~~~~SORTED INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n""]";"[""import pandas as pd\nimport numpy as np\nimport itertools\n\nletters = [chr(x) for x in range(ord('a'), ord('z'))]\nletter_combinations = [''.join(x) for x in itertools.combinations(letters, 3)]\n\ndf2 = df1.sort_values('letter')\ndf3 = df1.set_index('letter')\ndf4 = df3.sort_index()\nprint(df1.head(5))\n\n\nprint('~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / UNSORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n\nprint('~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / SORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n\nprint('~~~~~~~~~~~~~~~~~~~~~INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n\nprint('~~~~~~~~~~~~~~~~~~~~~SORTED INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n""]";False;"[""import pandas as pd\ndf1 = pd.DataFrame()\ndf = pd.DataFrame()\nimport pandas as pd\nimport numpy as np\nimport itertools\n\nletters = [chr(x) for x in range(ord('a'), ord('z'))]\nletter_combinations = [''.join(x) for x in itertools.combinations(letters, 3)]\n\ndf2 = df1.sort_values('letter')\ndf3 = df1.set_index('letter')\ndf4 = df3.sort_index()\nprint(df1.head(5))\n\n\nprint('~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / UNSORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n\nprint('~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / SORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n\nprint('~~~~~~~~~~~~~~~~~~~~~INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n\nprint('~~~~~~~~~~~~~~~~~~~~~SORTED INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n""]";True;0;1;"[""name 'diffs' is not defined""]";['NameError'];0;1;"[""name 'diffs' is not defined""]";['NameError'];0;1;"[""name 'diffs' is not defined""]";['NameError']
1143;1143;1143;1143;2.0;6;38352742;;1;14;<python><pandas>;Pandas Design Considerations for MultiIndexed Dataframes;439.0;"[""import pandas as pd\n\ndf = pd.DataFrame({'index_date' : ['12/07/2016','12/07/2016','12/07/2016','12/07/2016','12/07/2016'], \n               'portfolio' : ['A','B','C','D','E'], \n               'reporting_ccy' : ['GBP','GBP','GBP','GBP','GBP'],\n               'portfolio_ccy' : ['JPY','USD','USD','EUR','EUR'],\n               'amount' : [100,200,300,400,500],\n               'injection' : [1,2,3,4,5],\n               'to_usd' : [1.3167,1.3167,1.3167,1.3167,1.3167],\n               'to_ccy' : [0.009564,1,1,1.1093,1.1093],\n               'm5' : [2,4,6,8,10],\n               'm6' : [1,3,5,7,9]}); \ndf_pivot = df.pivot_table(index='index_date',columns=['portfolio','portfolio_ccy','reporting_ccy']).swaplevel(0, 1, axis=1).sortlevel(axis=1)\ndf_pivot.columns.names = ['portfolio','measures', 'portfolio_ccy', 'reporting_ccy']\ndf1 = pd.DataFrame({'index_date' : ['12/07/2016','12/07/2016','12/07/2016','12/07/2016','12/07/2016'], \n           'portfolio' : ['A','B','C','D','E'], \n           'reporting_ccy' : ['JPY','USD','USD','EUR','EUR'],\n           'portfolio_ccy' : ['JPY','USD','USD','EUR','EUR'],\n           'amount' : [13767.2522, 263.34, 395.01, 474.785901, 593.4823763],\n           'injection' : [1,2,3,4,5],\n           'to_usd' : [0.009564, 1, 1, 1.1093, 1.1093],\n           'to_ccy' : [1.3167, 1.3167, 1.3167, 1.3167, 1.3167],\n           'm5' : [2,4,6,8,10],\n           'm6' : [1,3,5,7,9]}); \ndf_concat = pd.concat([df,df1])\ndf_pivot1 = df_concat.pivot_table(index='index_date',columns=['portfolio','portfolio_ccy','reporting_ccy']).swaplevel(0, 1, axis=1).sortlevel(axis=1)\ndf_pivot1.columns.names = ['portfolio','measures', 'portfolio_ccy', 'reporting_ccy']\ndf_pivot1.xs(('amount', 'A'), level=('measures','portfolio'), drop_level=False, axis=1)\n""]";"[""import pandas as pd\n\ndf = pd.DataFrame({'index_date' : ['12/07/2016','12/07/2016','12/07/2016','12/07/2016','12/07/2016'], \n               'portfolio' : ['A','B','C','D','E'], \n               'reporting_ccy' : ['GBP','GBP','GBP','GBP','GBP'],\n               'portfolio_ccy' : ['JPY','USD','USD','EUR','EUR'],\n               'amount' : [100,200,300,400,500],\n               'injection' : [1,2,3,4,5],\n               'to_usd' : [1.3167,1.3167,1.3167,1.3167,1.3167],\n               'to_ccy' : [0.009564,1,1,1.1093,1.1093],\n               'm5' : [2,4,6,8,10],\n               'm6' : [1,3,5,7,9]}); \n"", ""df_pivot = df.pivot_table(index='index_date',columns=['portfolio','portfolio_ccy','reporting_ccy']).swaplevel(0, 1, axis=1).sortlevel(axis=1)\n"", ""df_pivot.columns.names = ['portfolio','measures', 'portfolio_ccy', 'reporting_ccy']\n"", ""df1 = pd.DataFrame({'index_date' : ['12/07/2016','12/07/2016','12/07/2016','12/07/2016','12/07/2016'], \n           'portfolio' : ['A','B','C','D','E'], \n           'reporting_ccy' : ['JPY','USD','USD','EUR','EUR'],\n           'portfolio_ccy' : ['JPY','USD','USD','EUR','EUR'],\n           'amount' : [13767.2522, 263.34, 395.01, 474.785901, 593.4823763],\n           'injection' : [1,2,3,4,5],\n           'to_usd' : [0.009564, 1, 1, 1.1093, 1.1093],\n           'to_ccy' : [1.3167, 1.3167, 1.3167, 1.3167, 1.3167],\n           'm5' : [2,4,6,8,10],\n           'm6' : [1,3,5,7,9]}); \n"", ""df_concat = pd.concat([df,df1])\ndf_pivot1 = df_concat.pivot_table(index='index_date',columns=['portfolio','portfolio_ccy','reporting_ccy']).swaplevel(0, 1, axis=1).sortlevel(axis=1)\ndf_pivot1.columns.names = ['portfolio','measures', 'portfolio_ccy', 'reporting_ccy']\n"", ""df_pivot1.xs(('amount', 'A'), level=('measures','portfolio'), drop_level=False, axis=1)\n""]";"[""import pandas as pd\n\ndf = pd.DataFrame({'index_date' : ['12/07/2016','12/07/2016','12/07/2016','12/07/2016','12/07/2016'], \n               'portfolio' : ['A','B','C','D','E'], \n               'reporting_ccy' : ['GBP','GBP','GBP','GBP','GBP'],\n               'portfolio_ccy' : ['JPY','USD','USD','EUR','EUR'],\n               'amount' : [100,200,300,400,500],\n               'injection' : [1,2,3,4,5],\n               'to_usd' : [1.3167,1.3167,1.3167,1.3167,1.3167],\n               'to_ccy' : [0.009564,1,1,1.1093,1.1093],\n               'm5' : [2,4,6,8,10],\n               'm6' : [1,3,5,7,9]}); \n"", ""df_pivot = df.pivot_table(index='index_date',columns=['portfolio','portfolio_ccy','reporting_ccy']).swaplevel(0, 1, axis=1).sortlevel(axis=1)\n"", ""df_pivot.columns.names = ['portfolio','measures', 'portfolio_ccy', 'reporting_ccy']\n"", ""df1 = pd.DataFrame({'index_date' : ['12/07/2016','12/07/2016','12/07/2016','12/07/2016','12/07/2016'], \n           'portfolio' : ['A','B','C','D','E'], \n           'reporting_ccy' : ['JPY','USD','USD','EUR','EUR'],\n           'portfolio_ccy' : ['JPY','USD','USD','EUR','EUR'],\n           'amount' : [13767.2522, 263.34, 395.01, 474.785901, 593.4823763],\n           'injection' : [1,2,3,4,5],\n           'to_usd' : [0.009564, 1, 1, 1.1093, 1.1093],\n           'to_ccy' : [1.3167, 1.3167, 1.3167, 1.3167, 1.3167],\n           'm5' : [2,4,6,8,10],\n           'm6' : [1,3,5,7,9]}); \n"", ""df_concat = pd.concat([df,df1])\ndf_pivot1 = df_concat.pivot_table(index='index_date',columns=['portfolio','portfolio_ccy','reporting_ccy']).swaplevel(0, 1, axis=1).sortlevel(axis=1)\ndf_pivot1.columns.names = ['portfolio','measures', 'portfolio_ccy', 'reporting_ccy']\n"", ""df_pivot1.xs(('amount', 'A'), level=('measures','portfolio'), drop_level=False, axis=1)\n"", 'df_pivot1.columns.get_level_values(3).unique()', 'df.assign()']";"[""import pandas as pd\n\ndf_pivot = df.pivot_table(index='index_date',columns=['portfolio','portfolio_ccy','reporting_ccy']).swaplevel(0, 1, axis=1).sortlevel(axis=1)\ndf_pivot.columns.names = ['portfolio','measures', 'portfolio_ccy', 'reporting_ccy']\ndf_concat = pd.concat([df,df1])\ndf_pivot1 = df_concat.pivot_table(index='index_date',columns=['portfolio','portfolio_ccy','reporting_ccy']).swaplevel(0, 1, axis=1).sortlevel(axis=1)\ndf_pivot1.columns.names = ['portfolio','measures', 'portfolio_ccy', 'reporting_ccy']\ndf_pivot1.xs(('amount', 'A'), level=('measures','portfolio'), drop_level=False, axis=1)\n""]";"[""import pandas as pd\n\ndf_pivot = df.pivot_table(index='index_date',columns=['portfolio','portfolio_ccy','reporting_ccy']).swaplevel(0, 1, axis=1).sortlevel(axis=1)\ndf_pivot.columns.names = ['portfolio','measures', 'portfolio_ccy', 'reporting_ccy']\ndf_concat = pd.concat([df,df1])\ndf_pivot1 = df_concat.pivot_table(index='index_date',columns=['portfolio','portfolio_ccy','reporting_ccy']).swaplevel(0, 1, axis=1).sortlevel(axis=1)\ndf_pivot1.columns.names = ['portfolio','measures', 'portfolio_ccy', 'reporting_ccy']\ndf_pivot1.xs(('amount', 'A'), level=('measures','portfolio'), drop_level=False, axis=1)\n""]";False;"[""import pandas as pd\ndf1 = pd.DataFrame()\ndf = pd.DataFrame()\nimport pandas as pd\n\ndf_pivot = df.pivot_table(index='index_date',columns=['portfolio','portfolio_ccy','reporting_ccy']).swaplevel(0, 1, axis=1).sortlevel(axis=1)\ndf_pivot.columns.names = ['portfolio','measures', 'portfolio_ccy', 'reporting_ccy']\ndf_concat = pd.concat([df,df1])\ndf_pivot1 = df_concat.pivot_table(index='index_date',columns=['portfolio','portfolio_ccy','reporting_ccy']).swaplevel(0, 1, axis=1).sortlevel(axis=1)\ndf_pivot1.columns.names = ['portfolio','measures', 'portfolio_ccy', 'reporting_ccy']\ndf_pivot1.xs(('amount', 'A'), level=('measures','portfolio'), drop_level=False, axis=1)\n""]";True;0;0;[];[];0;0;[];[];0;0;[];[]
1144;1144;1144;1144;1.0;1;38470550;;1;14;<python><pandas>;why is a sum of strings converted to floats;430.0;"[""df = pd.DataFrame([['3', '11'], ['0', '2']], columns=list('AB'))\ndf\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2 entries, 0 to 1\nData columns (total 2 columns):\nA    2 non-null object\nB    2 non-null object\ndtypes: object(2)\nmemory usage: 104.0+ bytes\ndf.sum()\n\nA     30.0\nB    112.0\ndtype: float64\n""]";"[""df = pd.DataFrame([['3', '11'], ['0', '2']], columns=list('AB'))\ndf\n"", ""df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2 entries, 0 to 1\nData columns (total 2 columns):\nA    2 non-null object\nB    2 non-null object\ndtypes: object(2)\nmemory usage: 104.0+ bytes\n"", 'df.sum()\n\nA     30.0\nB    112.0\ndtype: float64\n']";"[""df = pd.DataFrame([['3', '11'], ['0', '2']], columns=list('AB'))\ndf\n"", ""df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2 entries, 0 to 1\nData columns (total 2 columns):\nA    2 non-null object\nB    2 non-null object\ndtypes: object(2)\nmemory usage: 104.0+ bytes\n"", 'df.sum()\n\nA     30.0\nB    112.0\ndtype: float64\n']";"[""df = pd.DataFrame([['3', '11'], ['0', '2']], columns=list('AB'))\ndf\ndf.info()\n\ndf.sum()\n\n""]";"[""import pandas as pd\ndf = pd.DataFrame([['3', '11'], ['0', '2']], columns=list('AB'))\ndf\ndf.info()\n\ndf.sum()\n\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame([['3', '11'], ['0', '2']], columns=list('AB'))\ndf\ndf.info()\n\ndf.sum()\n\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1145;1145;1145;1145;1.0;5;38579532;;1;12;<python><numpy><pandas>;pandas equivalent of np.where;4669.0;"[""# df is pd.DataFrame\n# how to write this using df.where?\ndf['C'] = np.where((df['A']<0) | (df['B']>0), df['A']+df['B'], df['A']/df['B'])\n""]";"[""# df is pd.DataFrame\n# how to write this using df.where?\ndf['C'] = np.where((df['A']<0) | (df['B']>0), df['A']+df['B'], df['A']/df['B'])\n""]";"['np.where', 'when', 'otherwise', 'np.where', 'Series', 'pandas', 'numpy', 'pd.Series', 'pd.DataFrame', 'pandas.DataFrame.where', 'np.where', 'where', ""# df is pd.DataFrame\n# how to write this using df.where?\ndf['C'] = np.where((df['A']<0) | (df['B']>0), df['A']+df['B'], df['A']/df['B'])\n"", 'where', 'np.where']";"[""# df is pd.DataFrame\n# how to write this using df.where?\ndf['C'] = np.where((df['A']<0) | (df['B']>0), df['A']+df['B'], df['A']/df['B'])\n""]";"[""import pandas as pd\n# df is pd.DataFrame\n# how to write this using df.where?\ndf['C'] = np.where((df['A']<0) | (df['B']>0), df['A']+df['B'], df['A']/df['B'])\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame()\n# df is pd.DataFrame\n# how to write this using df.where?\ndf['C'] = np.where((df['A']<0) | (df['B']>0), df['A']+df['B'], df['A']/df['B'])\n""]";True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'A'""]";['KeyError']
1146;1146;1146;1146;3.0;0;38713200;;1;19;<python><list><pandas><dataframe><series>;How do I turn a dataframe into a series of lists?;936.0;"[""df = pd.DataFrame([[1, 2, 3, 4], [5, 6, 7, 8]], ['a', 'b'], ['A', 'B', 'C', 'D'])\n\nprint df\n\n   A  B  C  D\na  1  2  3  4\nb  5  6  7  8\npd.Series([[1, 2, 3, 4], [5, 6, 7, 8]], ['a', 'b'])\n\na    [1, 2, 3, 4]\nb    [5, 6, 7, 8]\ndtype: object\ndf.apply(list, axis=1)\n""]";"[""df = pd.DataFrame([[1, 2, 3, 4], [5, 6, 7, 8]], ['a', 'b'], ['A', 'B', 'C', 'D'])\n\nprint df\n\n   A  B  C  D\na  1  2  3  4\nb  5  6  7  8\n"", ""pd.Series([[1, 2, 3, 4], [5, 6, 7, 8]], ['a', 'b'])\n\na    [1, 2, 3, 4]\nb    [5, 6, 7, 8]\ndtype: object\n"", 'df.apply(list, axis=1)\n']";"[""df = pd.DataFrame([[1, 2, 3, 4], [5, 6, 7, 8]], ['a', 'b'], ['A', 'B', 'C', 'D'])\n\nprint df\n\n   A  B  C  D\na  1  2  3  4\nb  5  6  7  8\n"", 'df', ""pd.Series([[1, 2, 3, 4], [5, 6, 7, 8]], ['a', 'b'])\n\na    [1, 2, 3, 4]\nb    [5, 6, 7, 8]\ndtype: object\n"", 'df.apply(list, axis=1)\n', 'df']";"[""df = pd.DataFrame([[1, 2, 3, 4], [5, 6, 7, 8]], ['a', 'b'], ['A', 'B', 'C', 'D'])\n\n\npd.Series([[1, 2, 3, 4], [5, 6, 7, 8]], ['a', 'b'])\n\na    [1, 2, 3, 4]\nb    [5, 6, 7, 8]\ndf.apply(list, axis=1)\n""]";"[""import pandas as pd\ndf = pd.DataFrame([[1, 2, 3, 4], [5, 6, 7, 8]], ['a', 'b'], ['A', 'B', 'C', 'D'])\n\n\npd.Series([[1, 2, 3, 4], [5, 6, 7, 8]], ['a', 'b'])\n\na    [1, 2, 3, 4]\nb    [5, 6, 7, 8]\ndf.apply(list, axis=1)\n""]";True;"[""import pandas as pd\ndf = pd.DataFrame([[1, 2, 3, 4], [5, 6, 7, 8]], ['a', 'b'], ['A', 'B', 'C', 'D'])\n\n\npd.Series([[1, 2, 3, 4], [5, 6, 7, 8]], ['a', 'b'])\n\na    [1, 2, 3, 4]\nb    [5, 6, 7, 8]\ndf.apply(list, axis=1)\n""]";False;0;2;"[""name 'df' is not defined"", ""name 'pd' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'a' is not defined"", ""name 'a' is not defined""]";['NameError', 'NameError']
1147;1147;1147;1147;1.0;4;38717282;;1;11;<python><pandas><ggplot2>;Python ggplot- ggsave function not defined;1042.0;"['from ggplot import *\ndf=pd.DataFrame({""Animal"":[""dog"",""dolphin"",""chicken"",""ant"",""spider""],""Legs"":[4,0,2,6,8]})\np=ggplot(df, aes(x=""Animal"", weight=""Legs"")) + geom_bar(fill=\'blue\')\np\nggsave(""test.png"",p)\n']";"['from ggplot import *\ndf=pd.DataFrame({""Animal"":[""dog"",""dolphin"",""chicken"",""ant"",""spider""],""Legs"":[4,0,2,6,8]})\np=ggplot(df, aes(x=""Animal"", weight=""Legs"")) + geom_bar(fill=\'blue\')\np\nggsave(""test.png"",p)\n']";"['3.5', 'ggplot', 'from ggplot import *\ndf=pd.DataFrame({""Animal"":[""dog"",""dolphin"",""chicken"",""ant"",""spider""],""Legs"":[4,0,2,6,8]})\np=ggplot(df, aes(x=""Animal"", weight=""Legs"")) + geom_bar(fill=\'blue\')\np\nggsave(""test.png"",p)\n']";"['from ggplot import *\ndf=pd.DataFrame({""Animal"":[""dog"",""dolphin"",""chicken"",""ant"",""spider""],""Legs"":[4,0,2,6,8]})\np=ggplot(df, aes(x=""Animal"", weight=""Legs"")) + geom_bar(fill=\'blue\')\np\nggsave(""test.png"",p)\n']";"['import pandas as pd\nfrom ggplot import *\ndf=pd.DataFrame({""Animal"":[""dog"",""dolphin"",""chicken"",""ant"",""spider""],""Legs"":[4,0,2,6,8]})\np=ggplot(df, aes(x=""Animal"", weight=""Legs"")) + geom_bar(fill=\'blue\')\np\nggsave(""test.png"",p)\n']";True;"['import pandas as pd\ndf = pd.DataFrame()\nfrom ggplot import *\ndf=pd.DataFrame({""Animal"":[""dog"",""dolphin"",""chicken"",""ant"",""spider""],""Legs"":[4,0,2,6,8]})\np=ggplot(df, aes(x=""Animal"", weight=""Legs"")) + geom_bar(fill=\'blue\')\np\nggsave(""test.png"",p)\n']";True;0;0;[];[];0;0;[];[];0;0;[];[]
1148;1148;1148;1148;6.0;2;38733719;;1;12;<python><csv><pandas><dataframe><sframe>;Efficient way to get the unique values from 2 or more columns in a Dataframe;651.0;"["">>> from sframe import SFrame\n>>> sf =SFrame({'x':[1,1,2,5,7], 'y':[2,4,6,8,2], 'z':[2,5,8,6,2]})\n>>> sf\nColumns:\n    x   int\n    y   int\n    z   int\n\nRows: 5\n\nData:\n+---+---+---+\n| x | y | z |\n+---+---+---+\n| 1 | 2 | 2 |\n| 1 | 4 | 5 |\n| 2 | 6 | 8 |\n| 5 | 8 | 6 |\n| 7 | 2 | 2 |\n+---+---+---+\n[5 rows x 3 columns]\n>>> sf['x'].unique().append(sf['y'].unique()).unique()\ndtype: int\nRows: 7\n[2, 8, 5, 4, 1, 7, 6]\n>>> sf['x'].append(sf['y']).unique()\ndtype: int\nRows: 7\n[2, 8, 5, 4, 1, 7, 6]\n""]";"["">>> from sframe import SFrame\n>>> sf =SFrame({'x':[1,1,2,5,7], 'y':[2,4,6,8,2], 'z':[2,5,8,6,2]})\n>>> sf\nColumns:\n    x   int\n    y   int\n    z   int\n\nRows: 5\n\nData:\n+---+---+---+\n| x | y | z |\n+---+---+---+\n| 1 | 2 | 2 |\n| 1 | 4 | 5 |\n| 2 | 6 | 8 |\n| 5 | 8 | 6 |\n| 7 | 2 | 2 |\n+---+---+---+\n[5 rows x 3 columns]\n"", "">>> sf['x'].unique().append(sf['y'].unique()).unique()\ndtype: int\nRows: 7\n[2, 8, 5, 4, 1, 7, 6]\n"", "">>> sf['x'].append(sf['y']).unique()\ndtype: int\nRows: 7\n[2, 8, 5, 4, 1, 7, 6]\n""]";"['SFrame', "">>> from sframe import SFrame\n>>> sf =SFrame({'x':[1,1,2,5,7], 'y':[2,4,6,8,2], 'z':[2,5,8,6,2]})\n>>> sf\nColumns:\n    x   int\n    y   int\n    z   int\n\nRows: 5\n\nData:\n+---+---+---+\n| x | y | z |\n+---+---+---+\n| 1 | 2 | 2 |\n| 1 | 4 | 5 |\n| 2 | 6 | 8 |\n| 5 | 8 | 6 |\n| 7 | 2 | 2 |\n+---+---+---+\n[5 rows x 3 columns]\n"", 'x', 'y', "">>> sf['x'].unique().append(sf['y'].unique()).unique()\ndtype: int\nRows: 7\n[2, 8, 5, 4, 1, 7, 6]\n"", "">>> sf['x'].append(sf['y']).unique()\ndtype: int\nRows: 7\n[2, 8, 5, 4, 1, 7, 6]\n"", 'pandas']";['\n\n[2, 8, 5, 4, 1, 7, 6]\n[2, 8, 5, 4, 1, 7, 6]\n'];['\n\n[2, 8, 5, 4, 1, 7, 6]\n[2, 8, 5, 4, 1, 7, 6]\n'];False;['import pandas as pd\n\n\n[2, 8, 5, 4, 1, 7, 6]\n[2, 8, 5, 4, 1, 7, 6]\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
1149;1149;1149;1149;3.0;0;38783027;;1;16;<pandas><ipython-notebook><jupyter-notebook>;Jupyter notebook display two pandas tables side by side;3149.0;['display(df1)\ndisplay(df2)\n'];['display(df1)\ndisplay(df2)\n'];['display(df1)\ndisplay(df2)\n'];['display(df1)\ndisplay(df2)\n'];['display(df1)\ndisplay(df2)\n'];False;['import pandas as pd\ndf2 = pd.DataFrame()\ndf1 = pd.DataFrame()\ndf = pd.DataFrame()\ndisplay(df1)\ndisplay(df2)\n'];True;0;1;"[""name 'CSS' is not defined""]";['NameError'];0;1;"[""name 'CSS' is not defined""]";['NameError'];0;1;"[""name 'CSS' is not defined""]";['NameError']
1150;1150;1150;1150;1.0;8;38886080;;1;15;<python><pandas><series><loc>;Python: Pandas Series - Why use loc?;1881.0;"[""%timeit df_user1 = df.loc[df.user_id=='5561']\n\n100 loops, best of 3: 11.9 ms per loop\n%timeit df_user1_noloc = df[df.user_id=='5561']\n\n100 loops, best of 3: 12 ms per loop\ndf['time']    # equivalent to df.loc[:, 'time']\n""]";"[""%timeit df_user1 = df.loc[df.user_id=='5561']\n\n100 loops, best of 3: 11.9 ms per loop\n"", ""%timeit df_user1_noloc = df[df.user_id=='5561']\n\n100 loops, best of 3: 12 ms per loop\n"", ""df['time']    # equivalent to df.loc[:, 'time']\n""]";"[""%timeit df_user1 = df.loc[df.user_id=='5561']\n\n100 loops, best of 3: 11.9 ms per loop\n"", ""%timeit df_user1_noloc = df[df.user_id=='5561']\n\n100 loops, best of 3: 12 ms per loop\n"", ""df['time']    # equivalent to df.loc[:, 'time']\n""]";"[""\n\ndf['time']    # equivalent to df.loc[:, 'time']\n""]";"[""\n\ndf['time']    # equivalent to df.loc[:, 'time']\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\n\n\ndf['time']    # equivalent to df.loc[:, 'time']\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;['Item wrong length 1 instead of 3.'];['ValueError'];0;1;['Item wrong length 1 instead of 3.'];['ValueError']
1151;1151;1151;1151;6.0;13;38902239;;1;13;<python><pandas><numpy><dataframe>;Performance issues with pandas and filtering on datetime column;319.0;['    time    volume  complete    closeBid    closeAsk    openBid openAsk highBid highAsk lowBid  lowAsk  closeMid\n0   2016-08-07 21:00:00+00:00   9   True    0.84734 0.84842 0.84706 0.84814 0.84734 0.84842 0.84706 0.84814 0.84788\n1   2016-08-07 21:05:00+00:00   10  True    0.84735 0.84841 0.84752 0.84832 0.84752 0.84846 0.84712 0.8482  0.84788\n2   2016-08-07 21:10:00+00:00   10  True    0.84742 0.84817 0.84739 0.84828 0.84757 0.84831 0.84735 0.84817 0.847795\n3   2016-08-07 21:15:00+00:00   18  True    0.84732 0.84811 0.84737 0.84813 0.84737 0.84813 0.84721 0.8479  0.847715\n4   2016-08-07 21:20:00+00:00   4   True    0.84755 0.84822 0.84739 0.84812 0.84755 0.84822 0.84739 0.84812 0.847885\n5   2016-08-07 21:25:00+00:00   4   True    0.84769 0.84843 0.84758 0.84827 0.84769 0.84843 0.84758 0.84827 0.84806\n6   2016-08-07 21:30:00+00:00   5   True    0.84764 0.84851 0.84768 0.84852 0.8478  0.84857 0.84764 0.84851 0.848075\n7   2016-08-07 21:35:00+00:00   4   True    0.84755 0.84825 0.84762 0.84844 0.84765 0.84844 0.84755 0.84824 0.8479\n8   2016-08-07 21:40:00+00:00   1   True    0.84759 0.84812 0.84759 0.84812 0.84759 0.84812 0.84759 0.84812 0.847855\n9   2016-08-07 21:45:00+00:00   3   True    0.84727 0.84817 0.84743 0.8482  0.84743 0.84822 0.84727 0.84817 0.84772\nclass Runner():\n    def execute_tick(self, clock_tick, previous_tick):\n        candles = self.broker.get_new_candles(clock_tick, previous_tick)\n        if candles:\n            run_calculations(candles)\n\nclass Broker():\n    def get_new_candles(clock_tick, previous_tick)\n        start = previous_tick - timedelta(minutes=1)\n        end = clock_tick - timedelta(minutes=3)\n        return df[(df.time > start) & (df.time <= end)]\n'];['    time    volume  complete    closeBid    closeAsk    openBid openAsk highBid highAsk lowBid  lowAsk  closeMid\n0   2016-08-07 21:00:00+00:00   9   True    0.84734 0.84842 0.84706 0.84814 0.84734 0.84842 0.84706 0.84814 0.84788\n1   2016-08-07 21:05:00+00:00   10  True    0.84735 0.84841 0.84752 0.84832 0.84752 0.84846 0.84712 0.8482  0.84788\n2   2016-08-07 21:10:00+00:00   10  True    0.84742 0.84817 0.84739 0.84828 0.84757 0.84831 0.84735 0.84817 0.847795\n3   2016-08-07 21:15:00+00:00   18  True    0.84732 0.84811 0.84737 0.84813 0.84737 0.84813 0.84721 0.8479  0.847715\n4   2016-08-07 21:20:00+00:00   4   True    0.84755 0.84822 0.84739 0.84812 0.84755 0.84822 0.84739 0.84812 0.847885\n5   2016-08-07 21:25:00+00:00   4   True    0.84769 0.84843 0.84758 0.84827 0.84769 0.84843 0.84758 0.84827 0.84806\n6   2016-08-07 21:30:00+00:00   5   True    0.84764 0.84851 0.84768 0.84852 0.8478  0.84857 0.84764 0.84851 0.848075\n7   2016-08-07 21:35:00+00:00   4   True    0.84755 0.84825 0.84762 0.84844 0.84765 0.84844 0.84755 0.84824 0.8479\n8   2016-08-07 21:40:00+00:00   1   True    0.84759 0.84812 0.84759 0.84812 0.84759 0.84812 0.84759 0.84812 0.847855\n9   2016-08-07 21:45:00+00:00   3   True    0.84727 0.84817 0.84743 0.8482  0.84743 0.84822 0.84727 0.84817 0.84772\n', 'class Runner():\n    def execute_tick(self, clock_tick, previous_tick):\n        candles = self.broker.get_new_candles(clock_tick, previous_tick)\n        if candles:\n            run_calculations(candles)\n\nclass Broker():\n    def get_new_candles(clock_tick, previous_tick)\n        start = previous_tick - timedelta(minutes=1)\n        end = clock_tick - timedelta(minutes=3)\n        return df[(df.time > start) & (df.time <= end)]\n'];['    time    volume  complete    closeBid    closeAsk    openBid openAsk highBid highAsk lowBid  lowAsk  closeMid\n0   2016-08-07 21:00:00+00:00   9   True    0.84734 0.84842 0.84706 0.84814 0.84734 0.84842 0.84706 0.84814 0.84788\n1   2016-08-07 21:05:00+00:00   10  True    0.84735 0.84841 0.84752 0.84832 0.84752 0.84846 0.84712 0.8482  0.84788\n2   2016-08-07 21:10:00+00:00   10  True    0.84742 0.84817 0.84739 0.84828 0.84757 0.84831 0.84735 0.84817 0.847795\n3   2016-08-07 21:15:00+00:00   18  True    0.84732 0.84811 0.84737 0.84813 0.84737 0.84813 0.84721 0.8479  0.847715\n4   2016-08-07 21:20:00+00:00   4   True    0.84755 0.84822 0.84739 0.84812 0.84755 0.84822 0.84739 0.84812 0.847885\n5   2016-08-07 21:25:00+00:00   4   True    0.84769 0.84843 0.84758 0.84827 0.84769 0.84843 0.84758 0.84827 0.84806\n6   2016-08-07 21:30:00+00:00   5   True    0.84764 0.84851 0.84768 0.84852 0.8478  0.84857 0.84764 0.84851 0.848075\n7   2016-08-07 21:35:00+00:00   4   True    0.84755 0.84825 0.84762 0.84844 0.84765 0.84844 0.84755 0.84824 0.8479\n8   2016-08-07 21:40:00+00:00   1   True    0.84759 0.84812 0.84759 0.84812 0.84759 0.84812 0.84759 0.84812 0.847855\n9   2016-08-07 21:45:00+00:00   3   True    0.84727 0.84817 0.84743 0.8482  0.84743 0.84822 0.84727 0.84817 0.84772\n', 'class Runner():\n    def execute_tick(self, clock_tick, previous_tick):\n        candles = self.broker.get_new_candles(clock_tick, previous_tick)\n        if candles:\n            run_calculations(candles)\n\nclass Broker():\n    def get_new_candles(clock_tick, previous_tick)\n        start = previous_tick - timedelta(minutes=1)\n        end = clock_tick - timedelta(minutes=3)\n        return df[(df.time > start) & (df.time <= end)]\n', 'df[(df.time > start) & (df.time <= end)]', 'dict', 'dict'];['\n'];['\n'];False;['import pandas as pd\n\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
1152;1152;1152;1152;2.0;1;38935541;;1;14;<python><r><pandas>;dplyr summarize equivalent in pandas;2356.0;"[""import pandas as pd\ndata = pd.DataFrame(\n    {'col1':[1,1,1,1,1,2,2,2,2,2],\n    'col2':[1,2,3,4,5,6,7,8,9,0],\n     'col3':[-1,-2,-3,-4,-5,-6,-7,-8,-9,0]\n    }\n)\nresult = []\nfor k,v in data.groupby('col1'):\n    result.append([k, max(v['col2']), min(v['col3'])])\nprint pd.DataFrame(result, columns=['col1', 'col2_agg', 'col3_agg'])\ndata %>% groupby(col1) %>% summarize(col2_agg=max(col2), col3_agg=min(col3))\n""]";"[""import pandas as pd\ndata = pd.DataFrame(\n    {'col1':[1,1,1,1,1,2,2,2,2,2],\n    'col2':[1,2,3,4,5,6,7,8,9,0],\n     'col3':[-1,-2,-3,-4,-5,-6,-7,-8,-9,0]\n    }\n)\nresult = []\nfor k,v in data.groupby('col1'):\n    result.append([k, max(v['col2']), min(v['col3'])])\nprint pd.DataFrame(result, columns=['col1', 'col2_agg', 'col3_agg'])\n"", 'data %>% groupby(col1) %>% summarize(col2_agg=max(col2), col3_agg=min(col3))\n']";"[""import pandas as pd\ndata = pd.DataFrame(\n    {'col1':[1,1,1,1,1,2,2,2,2,2],\n    'col2':[1,2,3,4,5,6,7,8,9,0],\n     'col3':[-1,-2,-3,-4,-5,-6,-7,-8,-9,0]\n    }\n)\nresult = []\nfor k,v in data.groupby('col1'):\n    result.append([k, max(v['col2']), min(v['col3'])])\nprint pd.DataFrame(result, columns=['col1', 'col2_agg', 'col3_agg'])\n"", 'for loop groupby', 'groupby.agg', 'data %>% groupby(col1) %>% summarize(col2_agg=max(col2), col3_agg=min(col3))\n', 'groupby().summarize(newcolumn=max(col2 * col3))']";['import pandas as pd\nresult = []\n'];['import pandas as pd\nresult = []\n'];False;['import pandas as pd\nimport pandas as pd\nresult = []\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'col1'""]";['KeyError']
1153;1153;1153;1153;1.0;2;38956660;;1;14;<python><pandas><pycharm>;Dataframe not showing in Pycharm;2214.0;[''];[];[];[''];[''];False;['import pandas as pd\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
1154;1154;1154;1154;3.0;4;39100971;;1;18;<python><pandas><memory>;How do I release memory used by a pandas dataframe?;11401.0;"[""import pandas\ndf = pandas.read_csv('large_txt_file.txt')\ndel df\n""]";"[""import pandas\ndf = pandas.read_csv('large_txt_file.txt')\n"", 'del df\n']";"[""import pandas\ndf = pandas.read_csv('large_txt_file.txt')\n"", 'del df\n']";"[""import pandas\ndf = pandas.read_csv('large_txt_file.txt')\ndel df\n""]";"[""import pandas\ndf = pandas.read_csv('large_txt_file.txt')\ndel df\n""]";False;"[""import pandas as pd\nimport pandas\ndf = pandas.read_csv('large_txt_file.txt')\ndel df\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'Out' is not defined""]";['NameError']
1155;1155;1155;1155;4.0;0;39132742;;1;11;<python><pandas><dataframe><group-by><crosstab>;Groupby value counts on the dataframe pandas;3883.0;"[""df = pd.DataFrame([\n    (1, 1, 'term1'),\n    (1, 2, 'term2'),\n    (1, 1, 'term1'),\n    (1, 1, 'term2'),\n    (2, 2, 'term3'),\n    (2, 3, 'term1'),\n    (2, 2, 'term1')\n], columns=['id', 'group', 'term'])\n""]";"[""df = pd.DataFrame([\n    (1, 1, 'term1'),\n    (1, 2, 'term2'),\n    (1, 1, 'term1'),\n    (1, 1, 'term2'),\n    (2, 2, 'term3'),\n    (2, 3, 'term1'),\n    (2, 2, 'term1')\n], columns=['id', 'group', 'term'])\n""]";"[""df = pd.DataFrame([\n    (1, 1, 'term1'),\n    (1, 2, 'term2'),\n    (1, 1, 'term1'),\n    (1, 1, 'term2'),\n    (2, 2, 'term3'),\n    (2, 3, 'term1'),\n    (2, 2, 'term1')\n], columns=['id', 'group', 'term'])\n"", 'id', 'group', 'df.iterrows()', ""df.groupby(['id', 'group']).value_counts()""]";[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""'id'""]";['KeyError']
1156;1156;1156;1156;3.0;11;39156545;;1;12;<python><pandas><data-visualization><seaborn><bokeh>;interactive conditional histogram bucket slicing data visualization;881.0;"[""df.head()\nOut[1]:\n        A   B   C\ncity0   40  12  73\ncity1   65  56  10\ncity2   77  58  71\ncity3   89  53  49\ncity4   33  98  90\ndf = pd.DataFrame(np.random.randint(100,size=(1000000,3)), columns=list('ABC'))\n\nindx = ['city'+str(x) for x in range(0,1000000)]\ndf.index = indx\n    df.head()\n    Out[1]:\n            A   B   C  Abkt Bbkt\n    city0   40  12  73  2  1\n    city1   65  56  10  4  3\n    city2   77  58  71  4  3\n    city3   89  53  49  5  3\n    city4   33  98  90  2  5\n1-20 = 1\n21-40 = 2\n41-60 = 3\n61-80 = 4\n81-100 = 5\nValueError: The first argument of bincount must be non-negative\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x000000A2350615C0>]], dtype=object)\n""]";"['df.head()\nOut[1]:\n        A   B   C\ncity0   40  12  73\ncity1   65  56  10\ncity2   77  58  71\ncity3   89  53  49\ncity4   33  98  90\n', ""df = pd.DataFrame(np.random.randint(100,size=(1000000,3)), columns=list('ABC'))\n\nindx = ['city'+str(x) for x in range(0,1000000)]\ndf.index = indx\n"", '    df.head()\n    Out[1]:\n            A   B   C  Abkt Bbkt\n    city0   40  12  73  2  1\n    city1   65  56  10  4  3\n    city2   77  58  71  4  3\n    city3   89  53  49  5  3\n    city4   33  98  90  2  5\n', '1-20 = 1\n21-40 = 2\n41-60 = 3\n61-80 = 4\n81-100 = 5\n', 'ValueError: The first argument of bincount must be non-negative\n', 'array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000000A2350615C0>]], dtype=object)\n']";"['df.head()\nOut[1]:\n        A   B   C\ncity0   40  12  73\ncity1   65  56  10\ncity2   77  58  71\ncity3   89  53  49\ncity4   33  98  90\n', ""df = pd.DataFrame(np.random.randint(100,size=(1000000,3)), columns=list('ABC'))\n\nindx = ['city'+str(x) for x in range(0,1000000)]\ndf.index = indx\n"", '    df.head()\n    Out[1]:\n            A   B   C  Abkt Bbkt\n    city0   40  12  73  2  1\n    city1   65  56  10  4  3\n    city2   77  58  71  4  3\n    city3   89  53  49  5  3\n    city4   33  98  90  2  5\n', '1-20 = 1\n21-40 = 2\n41-60 = 3\n61-80 = 4\n81-100 = 5\n', 'df.hist()', 'ValueError: The first argument of bincount must be non-negative\n', ""df[['A']].hist(bins=10,range=(0,10))"", 'array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000000A2350615C0>]], dtype=object)\n', '#2', 'jupyter notebook', 'Jupyter Notebook']";"[""df.head()\ndf = pd.DataFrame(np.random.randint(100,size=(1000000,3)), columns=list('ABC'))\n\nindx = ['city'+str(x) for x in range(0,1000000)]\ndf.index = indx\n""]";"[""import pandas as pd\ndf.head()\ndf = pd.DataFrame(np.random.randint(100,size=(1000000,3)), columns=list('ABC'))\n\nindx = ['city'+str(x) for x in range(0,1000000)]\ndf.index = indx\n""]";True;"[""import pandas as pd\ndf.head()\ndf = pd.DataFrame(np.random.randint(100,size=(1000000,3)), columns=list('ABC'))\n\nindx = ['city'+str(x) for x in range(0,1000000)]\ndf.index = indx\n""]";False;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError']
1157;1157;1157;1157;1.0;8;39321914;;1;15;<python><pandas>;Unpredictable pandas slice assignment behavior with no SettingWithCopyWarning;320.0;"[""# pandas 0.18.1, python 3.5.1\nimport pandas as pd\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data[['a', 'b']]\ndata = data['a']\nnew_data.loc[0, 'a'] = 100 # no warning, doesn't propagate to data\n\ndata[0] == 1\nTrue\n\n\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data['a']\ndata = data['a']\nnew_data.loc[0] = 100 # no warning, propagates to data\n\ndata[0] == 100\nTrue\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data['a']\ndata = data[['a']]\nnew_data.loc[0] = 100 # warning, so we're safe\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\ndata = data.groupby('a')\nnew_data = data.filter(lambda g: len(g)==1)\nnew_data.loc[0, 'a'] = 100 # no warning, does not propagate to data\nassert data.filter(lambda g: True).loc[0, 'a'] == 1\ndata = pd.DataFrame({'a': [1, 2, 2], 'b': ['a', 'b', 'c']})\ndata = data.groupby('a')\nnew_data = data.filter(lambda g: len(g)==1)\nnew_data.loc[0, 'a'] = 100 # warning, does not propagate to data\nassert data.filter(lambda g: True).loc[0, 'a'] == 1\n# pandas 0.18.1, python 3.5.1\nimport pandas as pd\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c'], c: range(3)})\nnew_data = data[['a', 'b']]\ndata = data['a']\nnew_data.loc[0, 'a'] = 100 # no warning, doesn't propagate to data\n\ndata[0] == 1\nTrue\n""]";"[""# pandas 0.18.1, python 3.5.1\nimport pandas as pd\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data[['a', 'b']]\ndata = data['a']\nnew_data.loc[0, 'a'] = 100 # no warning, doesn't propagate to data\n\ndata[0] == 1\nTrue\n\n\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data['a']\ndata = data['a']\nnew_data.loc[0] = 100 # no warning, propagates to data\n\ndata[0] == 100\nTrue\n"", ""data = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data['a']\ndata = data[['a']]\nnew_data.loc[0] = 100 # warning, so we're safe\n"", ""data = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\ndata = data.groupby('a')\nnew_data = data.filter(lambda g: len(g)==1)\nnew_data.loc[0, 'a'] = 100 # no warning, does not propagate to data\nassert data.filter(lambda g: True).loc[0, 'a'] == 1\n"", ""data = pd.DataFrame({'a': [1, 2, 2], 'b': ['a', 'b', 'c']})\ndata = data.groupby('a')\nnew_data = data.filter(lambda g: len(g)==1)\nnew_data.loc[0, 'a'] = 100 # warning, does not propagate to data\nassert data.filter(lambda g: True).loc[0, 'a'] == 1\n"", ""# pandas 0.18.1, python 3.5.1\nimport pandas as pd\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c'], c: range(3)})\nnew_data = data[['a', 'b']]\ndata = data['a']\nnew_data.loc[0, 'a'] = 100 # no warning, doesn't propagate to data\n\ndata[0] == 1\nTrue\n""]";"['SettingWithCopy', ""# pandas 0.18.1, python 3.5.1\nimport pandas as pd\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data[['a', 'b']]\ndata = data['a']\nnew_data.loc[0, 'a'] = 100 # no warning, doesn't propagate to data\n\ndata[0] == 1\nTrue\n\n\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data['a']\ndata = data['a']\nnew_data.loc[0] = 100 # no warning, propagates to data\n\ndata[0] == 100\nTrue\n"", ""data = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data['a']\ndata = data[['a']]\nnew_data.loc[0] = 100 # warning, so we're safe\n"", ""data = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\ndata = data.groupby('a')\nnew_data = data.filter(lambda g: len(g)==1)\nnew_data.loc[0, 'a'] = 100 # no warning, does not propagate to data\nassert data.filter(lambda g: True).loc[0, 'a'] == 1\n"", ""data = pd.DataFrame({'a': [1, 2, 2], 'b': ['a', 'b', 'c']})\ndata = data.groupby('a')\nnew_data = data.filter(lambda g: len(g)==1)\nnew_data.loc[0, 'a'] = 100 # warning, does not propagate to data\nassert data.filter(lambda g: True).loc[0, 'a'] == 1\n"", ""# pandas 0.18.1, python 3.5.1\nimport pandas as pd\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c'], c: range(3)})\nnew_data = data[['a', 'b']]\ndata = data['a']\nnew_data.loc[0, 'a'] = 100 # no warning, doesn't propagate to data\n\ndata[0] == 1\nTrue\n""]";"[""# pandas 0.18.1, python 3.5.1\nimport pandas as pd\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data[['a', 'b']]\ndata = data['a']\nnew_data.loc[0, 'a'] = 100 # no warning, doesn't propagate to data\n\ndata[0] == 1\nTrue\n\n\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data['a']\ndata = data['a']\nnew_data.loc[0] = 100 # no warning, propagates to data\n\ndata[0] == 100\nTrue\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data['a']\ndata = data[['a']]\nnew_data.loc[0] = 100 # warning, so we're safe\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\ndata = data.groupby('a')\nnew_data = data.filter(lambda g: len(g)==1)\nnew_data.loc[0, 'a'] = 100 # no warning, does not propagate to data\nassert data.filter(lambda g: True).loc[0, 'a'] == 1\ndata = pd.DataFrame({'a': [1, 2, 2], 'b': ['a', 'b', 'c']})\ndata = data.groupby('a')\nnew_data = data.filter(lambda g: len(g)==1)\nnew_data.loc[0, 'a'] = 100 # warning, does not propagate to data\nassert data.filter(lambda g: True).loc[0, 'a'] == 1\n# pandas 0.18.1, python 3.5.1\nimport pandas as pd\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c'], c: range(3)})\nnew_data = data[['a', 'b']]\ndata = data['a']\nnew_data.loc[0, 'a'] = 100 # no warning, doesn't propagate to data\n\ndata[0] == 1\nTrue\n""]";"[""# pandas 0.18.1, python 3.5.1\nimport pandas as pd\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data[['a', 'b']]\ndata = data['a']\nnew_data.loc[0, 'a'] = 100 # no warning, doesn't propagate to data\n\ndata[0] == 1\nTrue\n\n\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data['a']\ndata = data['a']\nnew_data.loc[0] = 100 # no warning, propagates to data\n\ndata[0] == 100\nTrue\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data['a']\ndata = data[['a']]\nnew_data.loc[0] = 100 # warning, so we're safe\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\ndata = data.groupby('a')\nnew_data = data.filter(lambda g: len(g)==1)\nnew_data.loc[0, 'a'] = 100 # no warning, does not propagate to data\nassert data.filter(lambda g: True).loc[0, 'a'] == 1\ndata = pd.DataFrame({'a': [1, 2, 2], 'b': ['a', 'b', 'c']})\ndata = data.groupby('a')\nnew_data = data.filter(lambda g: len(g)==1)\nnew_data.loc[0, 'a'] = 100 # warning, does not propagate to data\nassert data.filter(lambda g: True).loc[0, 'a'] == 1\n# pandas 0.18.1, python 3.5.1\nimport pandas as pd\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c'], c: range(3)})\nnew_data = data[['a', 'b']]\ndata = data['a']\nnew_data.loc[0, 'a'] = 100 # no warning, doesn't propagate to data\n\ndata[0] == 1\nTrue\n""]";False;"[""import pandas as pd\n# pandas 0.18.1, python 3.5.1\nimport pandas as pd\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data[['a', 'b']]\ndata = data['a']\nnew_data.loc[0, 'a'] = 100 # no warning, doesn't propagate to data\n\ndata[0] == 1\nTrue\n\n\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data['a']\ndata = data['a']\nnew_data.loc[0] = 100 # no warning, propagates to data\n\ndata[0] == 100\nTrue\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\nnew_data = data['a']\ndata = data[['a']]\nnew_data.loc[0] = 100 # warning, so we're safe\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\ndata = data.groupby('a')\nnew_data = data.filter(lambda g: len(g)==1)\nnew_data.loc[0, 'a'] = 100 # no warning, does not propagate to data\nassert data.filter(lambda g: True).loc[0, 'a'] == 1\ndata = pd.DataFrame({'a': [1, 2, 2], 'b': ['a', 'b', 'c']})\ndata = data.groupby('a')\nnew_data = data.filter(lambda g: len(g)==1)\nnew_data.loc[0, 'a'] = 100 # warning, does not propagate to data\nassert data.filter(lambda g: True).loc[0, 'a'] == 1\n# pandas 0.18.1, python 3.5.1\nimport pandas as pd\ndata = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c'], c: range(3)})\nnew_data = data[['a', 'b']]\ndata = data['a']\nnew_data.loc[0, 'a'] = 100 # no warning, doesn't propagate to data\n\ndata[0] == 1\nTrue\n""]";False;0;0;[];[];0;0;[];[];0;0;[];[]
1158;1158;1158;1158;2.0;4;39674713;;1;17;<python><pandas><keras><lstm>;Neural Network LSTM input shape from dataframe;6582.0;[''];[];['(nb_samples, timesteps, input_dim)', 'T', '(nb_samples=1, timesteps=T, input_dim=N)', 'T/M', 'T', 'M', '[[[a_11, a_12, ..., a_1M], [a_21, a_22, ..., a_2M], ..., [a_N1, a_N2, ..., a_NM]], [[b_11, b_12, ..., b_1M], [b_21, b_22, ..., b_2M], ..., [b_N1, b_N2, ..., b_NM]], ..., [[x_11, x_12, ..., a_1M], [x_21, x_22, ..., x_2M], ..., [x_N1, x_N2, ..., x_NM]]]', 'T', 'N'];[''];[''];False;['import pandas as pd\n'];False;0;1;"[""name 'input_cols' is not defined""]";['NameError'];0;1;"[""name 'input_cols' is not defined""]";['NameError'];0;1;"[""name 'input_cols' is not defined""]";['NameError']
1159;1159;1159;1159;5.0;5;40143902;;1;11;<python><performance><pandas><hashtable><lookup>;Efficient lookup by common words;296.0;"["">>> df = pd.DataFrame([['foo', 'bar', 'joe'], \n                       ['foo'], \n                       ['bar', 'joe'], \n                       ['zoo']], \n                      index=['id1', 'id2', 'id3', 'id4'])\n>>> df.index.rename('id', inplace=True)  # btw, is there a way to include this into prev line?\n>>> print df\n\n       0     1     2\nid                  \nid1  foo   bar   joe\nid2  foo  None  None\nid3  bar   joe  None\nid4  zoo  None  None\n\ndef filter_by_tokens(df, tokens):\n    # search within each column and then concatenate and dedup results    \n    results = [df.loc[lambda df: df[i].isin(tokens)] for i in range(df.shape[1])]\n    return pd.concat(results).reset_index().drop_duplicates().set_index(df.index.name)\n\n>>> print filter_by_tokens(df, ['foo', 'zoo'])\n\n       0     1     2\nid                  \nid1  foo   bar   joe\nid2  foo  None  None\nid4  zoo  None  None    \n>>> df = pd.Series([{'foo', 'bar', 'joe'}, \n                    {'foo'}, \n                    {'bar', 'joe'}, \n                    {'zoo'}], \n                    index=['id1', 'id2', 'id3', 'id4'])\n>>> df.index.rename('id', inplace=True)\n>>> print df\n\nid\nid1    {foo, bar, joe}\nid2              {foo}\nid3         {bar, joe}\nid4              {zoo}\ndtype: object\n\ndef filter_by_tokens(df, tokens):\n    return df[df.map(lambda x: bool(x & set(tokens)))]\n\n>>> print filter_by_tokens(df, ['foo', 'zoo'])\n\nid\nid1    {foo, bar, joe}\nid2              {foo}\nid4              {zoo}\ndtype: object\n+--------------------+----------------+\n|       method       | best of 3, sec |\n+--------------------+----------------+\n| original           | 5.75           |\n| original, squeezed | 5.29           |\n| zip                | 2.54           |\n| merge              | 8.87           |\n| mul+any            | MemoryError    |\n| isin               | IndexingError  |\n| query              | 3.7            |\n+--------------------+----------------+\n""]";"["">>> df = pd.DataFrame([['foo', 'bar', 'joe'], \n                       ['foo'], \n                       ['bar', 'joe'], \n                       ['zoo']], \n                      index=['id1', 'id2', 'id3', 'id4'])\n>>> df.index.rename('id', inplace=True)  # btw, is there a way to include this into prev line?\n>>> print df\n\n       0     1     2\nid                  \nid1  foo   bar   joe\nid2  foo  None  None\nid3  bar   joe  None\nid4  zoo  None  None\n\ndef filter_by_tokens(df, tokens):\n    # search within each column and then concatenate and dedup results    \n    results = [df.loc[lambda df: df[i].isin(tokens)] for i in range(df.shape[1])]\n    return pd.concat(results).reset_index().drop_duplicates().set_index(df.index.name)\n\n>>> print filter_by_tokens(df, ['foo', 'zoo'])\n\n       0     1     2\nid                  \nid1  foo   bar   joe\nid2  foo  None  None\nid4  zoo  None  None    \n"", "">>> df = pd.Series([{'foo', 'bar', 'joe'}, \n                    {'foo'}, \n                    {'bar', 'joe'}, \n                    {'zoo'}], \n                    index=['id1', 'id2', 'id3', 'id4'])\n>>> df.index.rename('id', inplace=True)\n>>> print df\n\nid\nid1    {foo, bar, joe}\nid2              {foo}\nid3         {bar, joe}\nid4              {zoo}\ndtype: object\n\ndef filter_by_tokens(df, tokens):\n    return df[df.map(lambda x: bool(x & set(tokens)))]\n\n>>> print filter_by_tokens(df, ['foo', 'zoo'])\n\nid\nid1    {foo, bar, joe}\nid2              {foo}\nid4              {zoo}\ndtype: object\n"", '+--------------------+----------------+\n|       method       | best of 3, sec |\n+--------------------+----------------+\n| original           | 5.75           |\n| original, squeezed | 5.29           |\n| zip                | 2.54           |\n| merge              | 8.87           |\n| mul+any            | MemoryError    |\n| isin               | IndexingError  |\n| query              | 3.7            |\n+--------------------+----------------+\n']";"['original', "">>> df = pd.DataFrame([['foo', 'bar', 'joe'], \n                       ['foo'], \n                       ['bar', 'joe'], \n                       ['zoo']], \n                      index=['id1', 'id2', 'id3', 'id4'])\n>>> df.index.rename('id', inplace=True)  # btw, is there a way to include this into prev line?\n>>> print df\n\n       0     1     2\nid                  \nid1  foo   bar   joe\nid2  foo  None  None\nid3  bar   joe  None\nid4  zoo  None  None\n\ndef filter_by_tokens(df, tokens):\n    # search within each column and then concatenate and dedup results    \n    results = [df.loc[lambda df: df[i].isin(tokens)] for i in range(df.shape[1])]\n    return pd.concat(results).reset_index().drop_duplicates().set_index(df.index.name)\n\n>>> print filter_by_tokens(df, ['foo', 'zoo'])\n\n       0     1     2\nid                  \nid1  foo   bar   joe\nid2  foo  None  None\nid4  zoo  None  None    \n"", 'original, squeezed', "">>> df = pd.Series([{'foo', 'bar', 'joe'}, \n                    {'foo'}, \n                    {'bar', 'joe'}, \n                    {'zoo'}], \n                    index=['id1', 'id2', 'id3', 'id4'])\n>>> df.index.rename('id', inplace=True)\n>>> print df\n\nid\nid1    {foo, bar, joe}\nid2              {foo}\nid3         {bar, joe}\nid4              {zoo}\ndtype: object\n\ndef filter_by_tokens(df, tokens):\n    return df[df.map(lambda x: bool(x & set(tokens)))]\n\n>>> print filter_by_tokens(df, ['foo', 'zoo'])\n\nid\nid1    {foo, bar, joe}\nid2              {foo}\nid4              {zoo}\ndtype: object\n"", 'filter_by_tokens', '+--------------------+----------------+\n|       method       | best of 3, sec |\n+--------------------+----------------+\n| original           | 5.75           |\n| original, squeezed | 5.29           |\n| zip                | 2.54           |\n| merge              | 8.87           |\n| mul+any            | MemoryError    |\n| isin               | IndexingError  |\n| query              | 3.7            |\n+--------------------+----------------+\n', 'mul+any', 'd1 = pd.get_dummies(df.stack()).groupby(level=0).sum()', 'isin', 'IndexingError: Unalignable boolean Series key provided', ""s[d1.isin({'zoo', 'foo'}).unstack().any(1)]"", 'df.stack().isin(set(tokens)).unstack()', 'zip', 'original squeezed']";['\nid                  \n\n    # search within each column and then concatenate and dedup results    \n\n\nid                  \n\nid\n\n\n\nid\n'];['\nid                  \n\n    # search within each column and then concatenate and dedup results    \n\n\nid                  \n\nid\n\n\n\nid\n'];False;['import pandas as pd\n\nid                  \n\n    # search within each column and then concatenate and dedup results    \n\n\nid                  \n\nid\n\n\n\nid\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
1160;1160;1160;1160;5.0;0;40144769;;1;11;<python><pandas>;How to select the last column of dataframe;6781.0;['df[df.columns[len(df.columns)-1]]\n'];['df[df.columns[len(df.columns)-1]]\n'];['df[df.columns[len(df.columns)-1]]\n'];['df[df.columns[len(df.columns)-1]]\n'];['df[df.columns[len(df.columns)-1]]\n'];False;['import pandas as pd\ndf = pd.DataFrame()\ndf[df.columns[len(df.columns)-1]]\n'];True;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;['single positional indexer is out-of-bounds'];['IndexError']
1161;1161;1161;1161;6.0;11;40209520;;1;11;<python><pandas><numpy>;daily data, resample every 3 days, calculate over trailing 5 days efficiently;667.0;"[""tidx = pd.date_range('2012-12-31', periods=11, freq='D')\ndf = pd.DataFrame(dict(A=np.arange(len(tidx))), tidx)\ndf\n""]";"[""tidx = pd.date_range('2012-12-31', periods=11, freq='D')\ndf = pd.DataFrame(dict(A=np.arange(len(tidx))), tidx)\ndf\n""]";"['df', ""tidx = pd.date_range('2012-12-31', periods=11, freq='D')\ndf = pd.DataFrame(dict(A=np.arange(len(tidx))), tidx)\ndf\n"", '5D']";"[""tidx = pd.date_range('2012-12-31', periods=11, freq='D')\ndf = pd.DataFrame(dict(A=np.arange(len(tidx))), tidx)\ndf\n""]";"[""import pandas as pd\ntidx = pd.date_range('2012-12-31', periods=11, freq='D')\ndf = pd.DataFrame(dict(A=np.arange(len(tidx))), tidx)\ndf\n""]";True;"[""import pandas as pd\ntidx = pd.date_range('2012-12-31', periods=11, freq='D')\ndf = pd.DataFrame(dict(A=np.arange(len(tidx))), tidx)\ndf\n""]";False;0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];0;1;"[""Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'Index'""]";['TypeError']
1162;1162;1162;1162;2.0;0;40225683;;1;12;<python><pandas><dataframe><multi-level>;How to simply add a column level to a pandas dataframe;1947.0;"[""df = pd.DataFrame(index=list('abcde'), data={'A': range(5), 'B': range(5)})\n df\nOut[92]: \n   A  B\na  0  0\nb  1  1\nc  2  2\nd  3  3\ne  4  4\n df\nOut[92]: \n   A  B\n   C  C\na  0  0\nb  1  1\nc  2  2\nd  3  3\ne  4  4\n""]";"[""df = pd.DataFrame(index=list('abcde'), data={'A': range(5), 'B': range(5)})\n df\nOut[92]: \n   A  B\na  0  0\nb  1  1\nc  2  2\nd  3  3\ne  4  4\n"", ' df\nOut[92]: \n   A  B\n   C  C\na  0  0\nb  1  1\nc  2  2\nd  3  3\ne  4  4\n']";"[""df = pd.DataFrame(index=list('abcde'), data={'A': range(5), 'B': range(5)})\n df\nOut[92]: \n   A  B\na  0  0\nb  1  1\nc  2  2\nd  3  3\ne  4  4\n"", ' df\nOut[92]: \n   A  B\n   C  C\na  0  0\nb  1  1\nc  2  2\nd  3  3\ne  4  4\n']";"[""df = pd.DataFrame(index=list('abcde'), data={'A': range(5), 'B': range(5)})\n""]";"[""import pandas as pd\ndf = pd.DataFrame(index=list('abcde'), data={'A': range(5), 'B': range(5)})\n""]";True;"[""import pandas as pd\ndata = pd.DataFrame()\ndf = pd.DataFrame(index=list('abcde'), data={'A': range(5), 'B': range(5)})\n""]";True;0;1;"[""name 'pd' is not defined""]";['NameError'];0;1;"[""name 'df' is not defined""]";['NameError'];1;1;['Sucess'];['Sucess']
1163;1163;1163;1163;2.0;9;41190852;;1;12;<python><arrays><performance><pandas><numpy>;Most efficient way to forward-fill NaN values in numpy array;1088.0;"[""import numpy as np\narr = np.array([[5, np.nan, np.nan, 7, 2],\n                [3, np.nan, 1, 8, np.nan],\n                [4, 9, 6, np.nan, np.nan]])\narray([[  5.,  nan,  nan,   7.,   2.],\n       [  3.,  nan,   1.,   8.,  nan],\n       [  4.,   9.,   6.,  nan,  nan]])\narray([[  5.,   5.,   5.,  7.,  2.],\n       [  3.,   3.,   1.,  8.,  8.],\n       [  4.,   9.,   6.,  6.,  6.]])\nfor row_idx in range(arr.shape[0]):\n    for col_idx in range(arr.shape[1]):\n        if np.isnan(arr[row_idx][col_idx]):\n            arr[row_idx][col_idx] = arr[row_idx][col_idx - 1]\nimport pandas as pd\ndf = pd.DataFrame(arr)\ndf.fillna(method='ffill', axis=1, inplace=True)\narr = df.as_matrix()\nimport numba as nb\nimport numpy as np\nimport pandas as pd\n\ndef random_array():\n    choices = [1, 2, 3, 4, 5, 6, 7, 8, 9, np.nan]\n    out = np.random.choice(choices, size=(1000, 10))\n    return out\n\ndef loops_fill(arr):\n    out = arr.copy()\n    for row_idx in range(out.shape[0]):\n        for col_idx in range(1, out.shape[1]):\n            if np.isnan(out[row_idx, col_idx]):\n                out[row_idx, col_idx] = out[row_idx, col_idx - 1]\n    return out\n\n@nb.jit\ndef numba_loops_fill(arr):\n    '''Numba decorator solution provided by shx2.'''\n    out = arr.copy()\n    for row_idx in range(out.shape[0]):\n        for col_idx in range(1, out.shape[1]):\n            if np.isnan(out[row_idx, col_idx]):\n                out[row_idx, col_idx] = out[row_idx, col_idx - 1]\n    return out\n\ndef pandas_fill(arr):\n    df = pd.DataFrame(arr)\n    df.fillna(method='ffill', axis=1, inplace=True)\n    out = df.as_matrix()\n    return out\n\ndef numpy_fill(arr):\n    '''Solution provided by Divakar.'''\n    mask = np.isnan(arr)\n    idx = np.where(~mask,np.arange(mask.shape[1]),0)\n    np.maximum.accumulate(idx,axis=1, out=idx)\n    out = arr[np.arange(idx.shape[0])[:,None], idx]\n    return out\n%timeit -n 1000 loops_fill(random_array())\n%timeit -n 1000 numba_loops_fill(random_array())\n%timeit -n 1000 pandas_fill(random_array())\n%timeit -n 1000 numpy_fill(random_array())\n1000 loops, best of 3: 9.64 ms per loop\n1000 loops, best of 3: 377 µs per loop\n1000 loops, best of 3: 455 µs per loop\n1000 loops, best of 3: 351 µs per loop\n""]";"['import numpy as np\narr = np.array([[5, np.nan, np.nan, 7, 2],\n                [3, np.nan, 1, 8, np.nan],\n                [4, 9, 6, np.nan, np.nan]])\n', 'array([[  5.,  nan,  nan,   7.,   2.],\n       [  3.,  nan,   1.,   8.,  nan],\n       [  4.,   9.,   6.,  nan,  nan]])\n', 'array([[  5.,   5.,   5.,  7.,  2.],\n       [  3.,   3.,   1.,  8.,  8.],\n       [  4.,   9.,   6.,  6.,  6.]])\n', 'for row_idx in range(arr.shape[0]):\n    for col_idx in range(arr.shape[1]):\n        if np.isnan(arr[row_idx][col_idx]):\n            arr[row_idx][col_idx] = arr[row_idx][col_idx - 1]\n', ""import pandas as pd\ndf = pd.DataFrame(arr)\ndf.fillna(method='ffill', axis=1, inplace=True)\narr = df.as_matrix()\n"", ""import numba as nb\nimport numpy as np\nimport pandas as pd\n\ndef random_array():\n    choices = [1, 2, 3, 4, 5, 6, 7, 8, 9, np.nan]\n    out = np.random.choice(choices, size=(1000, 10))\n    return out\n\ndef loops_fill(arr):\n    out = arr.copy()\n    for row_idx in range(out.shape[0]):\n        for col_idx in range(1, out.shape[1]):\n            if np.isnan(out[row_idx, col_idx]):\n                out[row_idx, col_idx] = out[row_idx, col_idx - 1]\n    return out\n\n@nb.jit\ndef numba_loops_fill(arr):\n    '''Numba decorator solution provided by shx2.'''\n    out = arr.copy()\n    for row_idx in range(out.shape[0]):\n        for col_idx in range(1, out.shape[1]):\n            if np.isnan(out[row_idx, col_idx]):\n                out[row_idx, col_idx] = out[row_idx, col_idx - 1]\n    return out\n\ndef pandas_fill(arr):\n    df = pd.DataFrame(arr)\n    df.fillna(method='ffill', axis=1, inplace=True)\n    out = df.as_matrix()\n    return out\n\ndef numpy_fill(arr):\n    '''Solution provided by Divakar.'''\n    mask = np.isnan(arr)\n    idx = np.where(~mask,np.arange(mask.shape[1]),0)\n    np.maximum.accumulate(idx,axis=1, out=idx)\n    out = arr[np.arange(idx.shape[0])[:,None], idx]\n    return out\n"", '%timeit -n 1000 loops_fill(random_array())\n%timeit -n 1000 numba_loops_fill(random_array())\n%timeit -n 1000 pandas_fill(random_array())\n%timeit -n 1000 numpy_fill(random_array())\n', '1000 loops, best of 3: 9.64 ms per loop\n1000 loops, best of 3: 377 µs per loop\n1000 loops, best of 3: 455 µs per loop\n1000 loops, best of 3: 351 µs per loop\n']";"['arr', 'import numpy as np\narr = np.array([[5, np.nan, np.nan, 7, 2],\n                [3, np.nan, 1, 8, np.nan],\n                [4, 9, 6, np.nan, np.nan]])\n', 'arr', 'array([[  5.,  nan,  nan,   7.,   2.],\n       [  3.,  nan,   1.,   8.,  nan],\n       [  4.,   9.,   6.,  nan,  nan]])\n', 'nan', 'arr', 'nan', 'array([[  5.,   5.,   5.,  7.,  2.],\n       [  3.,   3.,   1.,  8.,  8.],\n       [  4.,   9.,   6.,  6.,  6.]])\n', 'for row_idx in range(arr.shape[0]):\n    for col_idx in range(arr.shape[1]):\n        if np.isnan(arr[row_idx][col_idx]):\n            arr[row_idx][col_idx] = arr[row_idx][col_idx - 1]\n', ""import pandas as pd\ndf = pd.DataFrame(arr)\ndf.fillna(method='ffill', axis=1, inplace=True)\narr = df.as_matrix()\n"", 'nan', ""import numba as nb\nimport numpy as np\nimport pandas as pd\n\ndef random_array():\n    choices = [1, 2, 3, 4, 5, 6, 7, 8, 9, np.nan]\n    out = np.random.choice(choices, size=(1000, 10))\n    return out\n\ndef loops_fill(arr):\n    out = arr.copy()\n    for row_idx in range(out.shape[0]):\n        for col_idx in range(1, out.shape[1]):\n            if np.isnan(out[row_idx, col_idx]):\n                out[row_idx, col_idx] = out[row_idx, col_idx - 1]\n    return out\n\n@nb.jit\ndef numba_loops_fill(arr):\n    '''Numba decorator solution provided by shx2.'''\n    out = arr.copy()\n    for row_idx in range(out.shape[0]):\n        for col_idx in range(1, out.shape[1]):\n            if np.isnan(out[row_idx, col_idx]):\n                out[row_idx, col_idx] = out[row_idx, col_idx - 1]\n    return out\n\ndef pandas_fill(arr):\n    df = pd.DataFrame(arr)\n    df.fillna(method='ffill', axis=1, inplace=True)\n    out = df.as_matrix()\n    return out\n\ndef numpy_fill(arr):\n    '''Solution provided by Divakar.'''\n    mask = np.isnan(arr)\n    idx = np.where(~mask,np.arange(mask.shape[1]),0)\n    np.maximum.accumulate(idx,axis=1, out=idx)\n    out = arr[np.arange(idx.shape[0])[:,None], idx]\n    return out\n"", '%timeit -n 1000 loops_fill(random_array())\n%timeit -n 1000 numba_loops_fill(random_array())\n%timeit -n 1000 pandas_fill(random_array())\n%timeit -n 1000 numpy_fill(random_array())\n', '1000 loops, best of 3: 9.64 ms per loop\n1000 loops, best of 3: 377 µs per loop\n1000 loops, best of 3: 455 µs per loop\n1000 loops, best of 3: 351 µs per loop\n']";"[""import numpy as np\nimport pandas as pd\ndf = pd.DataFrame(arr)\ndf.fillna(method='ffill', axis=1, inplace=True)\narr = df.as_matrix()\nimport numba as nb\nimport numpy as np\nimport pandas as pd\n\n\n\n\n\n""]";"[""import numpy as np\nimport pandas as pd\ndf = pd.DataFrame(arr)\ndf.fillna(method='ffill', axis=1, inplace=True)\narr = df.as_matrix()\nimport numba as nb\nimport numpy as np\nimport pandas as pd\n\n\n\n\n\n""]";False;"[""import pandas as pd\nimport numpy as np\nimport pandas as pd\ndf = pd.DataFrame(arr)\ndf.fillna(method='ffill', axis=1, inplace=True)\narr = df.as_matrix()\nimport numba as nb\nimport numpy as np\nimport pandas as pd\n\n\n\n\n\n""]";False;0;1;"[""name 'arr' is not defined""]";['NameError'];0;1;"[""name 'arr' is not defined""]";['NameError'];0;1;"[""name 'arr' is not defined""]";['NameError']
1164;1164;1164;1164;3.0;3;41205001;;1;11;<python><python-3.x><pandas><numpy><decimal>;How do I check for correlation using Decimal numbers/data with python 3;146.0;"[""from decimal import Decimal\nimport numpy as np\nimport pandas as pd\n\na = [Decimal(2.3), Decimal(1.5), Decimal(5.7), Decimal(4.6), Decimal(5.5), Decimal(1.5)]\nb = [Decimal(2.1), Decimal(1.2), Decimal(5.3), Decimal(4.4), Decimal(5.3), Decimal(1.7)]\n\nh = [2.3,1.5,5.7,4.6,5.5,1.5]\nj = [2.1,1.2,5.3,4.4,5.3,1.7]\n\ncorr_data1 = pd.DataFrame({'A': a, 'B': b}) \n\ncorr_data2 = corr_data1.corr()\nprint(corr_data2)\n\ncorr_data3 = pd.DataFrame({'H': h, 'J': j})\n\ncorr_data4 = corr_data3.corr()\nprint(corr_data4)\nEmpty DataFrame\nColumns: []\nIndex: []\n          H         J\nH  1.000000  0.995657\nJ  0.995657  1.000000\n""]";"[""from decimal import Decimal\nimport numpy as np\nimport pandas as pd\n\na = [Decimal(2.3), Decimal(1.5), Decimal(5.7), Decimal(4.6), Decimal(5.5), Decimal(1.5)]\nb = [Decimal(2.1), Decimal(1.2), Decimal(5.3), Decimal(4.4), Decimal(5.3), Decimal(1.7)]\n\nh = [2.3,1.5,5.7,4.6,5.5,1.5]\nj = [2.1,1.2,5.3,4.4,5.3,1.7]\n\ncorr_data1 = pd.DataFrame({'A': a, 'B': b}) \n\ncorr_data2 = corr_data1.corr()\nprint(corr_data2)\n\ncorr_data3 = pd.DataFrame({'H': h, 'J': j})\n\ncorr_data4 = corr_data3.corr()\nprint(corr_data4)\n"", 'Empty DataFrame\nColumns: []\nIndex: []\n', '          H         J\nH  1.000000  0.995657\nJ  0.995657  1.000000\n']";"[""from decimal import Decimal\nimport numpy as np\nimport pandas as pd\n\na = [Decimal(2.3), Decimal(1.5), Decimal(5.7), Decimal(4.6), Decimal(5.5), Decimal(1.5)]\nb = [Decimal(2.1), Decimal(1.2), Decimal(5.3), Decimal(4.4), Decimal(5.3), Decimal(1.7)]\n\nh = [2.3,1.5,5.7,4.6,5.5,1.5]\nj = [2.1,1.2,5.3,4.4,5.3,1.7]\n\ncorr_data1 = pd.DataFrame({'A': a, 'B': b}) \n\ncorr_data2 = corr_data1.corr()\nprint(corr_data2)\n\ncorr_data3 = pd.DataFrame({'H': h, 'J': j})\n\ncorr_data4 = corr_data3.corr()\nprint(corr_data4)\n"", 'Empty DataFrame\nColumns: []\nIndex: []\n', '          H         J\nH  1.000000  0.995657\nJ  0.995657  1.000000\n']";"[""from decimal import Decimal\nimport numpy as np\nimport pandas as pd\n\na = [Decimal(2.3), Decimal(1.5), Decimal(5.7), Decimal(4.6), Decimal(5.5), Decimal(1.5)]\nb = [Decimal(2.1), Decimal(1.2), Decimal(5.3), Decimal(4.4), Decimal(5.3), Decimal(1.7)]\n\nh = [2.3,1.5,5.7,4.6,5.5,1.5]\nj = [2.1,1.2,5.3,4.4,5.3,1.7]\n\ncorr_data1 = pd.DataFrame({'A': a, 'B': b}) \n\ncorr_data2 = corr_data1.corr()\nprint(corr_data2)\n\ncorr_data3 = pd.DataFrame({'H': h, 'J': j})\n\ncorr_data4 = corr_data3.corr()\nprint(corr_data4)\n""]";"[""from decimal import Decimal\nimport numpy as np\nimport pandas as pd\n\na = [Decimal(2.3), Decimal(1.5), Decimal(5.7), Decimal(4.6), Decimal(5.5), Decimal(1.5)]\nb = [Decimal(2.1), Decimal(1.2), Decimal(5.3), Decimal(4.4), Decimal(5.3), Decimal(1.7)]\n\nh = [2.3,1.5,5.7,4.6,5.5,1.5]\nj = [2.1,1.2,5.3,4.4,5.3,1.7]\n\ncorr_data1 = pd.DataFrame({'A': a, 'B': b}) \n\ncorr_data2 = corr_data1.corr()\nprint(corr_data2)\n\ncorr_data3 = pd.DataFrame({'H': h, 'J': j})\n\ncorr_data4 = corr_data3.corr()\nprint(corr_data4)\n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nfrom decimal import Decimal\nimport numpy as np\nimport pandas as pd\n\na = [Decimal(2.3), Decimal(1.5), Decimal(5.7), Decimal(4.6), Decimal(5.5), Decimal(1.5)]\nb = [Decimal(2.1), Decimal(1.2), Decimal(5.3), Decimal(4.4), Decimal(5.3), Decimal(1.7)]\n\nh = [2.3,1.5,5.7,4.6,5.5,1.5]\nj = [2.1,1.2,5.3,4.4,5.3,1.7]\n\ncorr_data1 = pd.DataFrame({'A': a, 'B': b}) \n\ncorr_data2 = corr_data1.corr()\nprint(corr_data2)\n\ncorr_data3 = pd.DataFrame({'H': h, 'J': j})\n\ncorr_data4 = corr_data3.corr()\nprint(corr_data4)\n""]";True;0;0;[];[];0;0;[];[];0;0;[];[]
1165;1165;1165;1165;1.0;0;41464177;;1;13;<python><pandas><numpy>;Identify clusters linked by delta to the left and different delta to the right;263.0;['a = np.array([0, 2, 3, 4, 5, 10, 11, 11, 14, 19, 20, 20])\ndelta_left, delta_right = 1, 1\n#   a = [ 0  .  2  3  4  5  .  .  .  . 10 11  .  . 14  .  .  .  . 19 20\n#                                         11                         20\n#\n#                                     [10--|-12]                 [19--|-21]\n#           [1--|--3]                 [10--|-12]                 [19--|-21]\n#    [-1--|--1]   [3--|--5]         [9--|-11]                 [18--|-20]\n#   +--+--|--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--|\n#              [2--|--4]                       [13--|-15]\n#\n#         ?    ????????                 ????        ?              ????\n#         ?   cluster 2                Cluster 3    ?           Cluster 5\n#     Cluster 1                                 Cluster 4\nprint(clusters)\n[1 2 2 2 2 3 3 3 4 5 5 5]\ndelta_left, delta_right = 2, 1\n#   a = [ 0  .  2  3  4  5  .  .  .  . 10 11  .  . 14  .  .  .  . 19 20\n#                                         11                         20\n#\n#                                   [9-----|-12]              [18-----|-21]\n#        [0-----|--3]               [9-----|-12]              [18-----|-21]\n# [-2-----|--1][2-----|--5]      [8-----|-11]              [17-----|-20]\n#   +--+--|--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--|\n#           [1 ----|--4]                    [12-----|-15]\n#\n#         ?????????????                 ????        ?              ????\n#           cluster 1                Cluster 2      ?           Cluster 4\n#                                               Cluster 3\nprint(clusters)\n[1 1 1 1 1 2 2 2 3 4 4 4]\n'];['a = np.array([0, 2, 3, 4, 5, 10, 11, 11, 14, 19, 20, 20])\n', 'delta_left, delta_right = 1, 1\n', '#   a = [ 0  .  2  3  4  5  .  .  .  . 10 11  .  . 14  .  .  .  . 19 20\n#                                         11                         20\n#\n#                                     [10--|-12]                 [19--|-21]\n#           [1--|--3]                 [10--|-12]                 [19--|-21]\n#    [-1--|--1]   [3--|--5]         [9--|-11]                 [18--|-20]\n#   +--+--|--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--|\n#              [2--|--4]                       [13--|-15]\n#\n#         ?    ????????                 ????        ?              ????\n#         ?   cluster 2                Cluster 3    ?           Cluster 5\n#     Cluster 1                                 Cluster 4\n', 'print(clusters)\n[1 2 2 2 2 3 3 3 4 5 5 5]\n', 'delta_left, delta_right = 2, 1\n', '#   a = [ 0  .  2  3  4  5  .  .  .  . 10 11  .  . 14  .  .  .  . 19 20\n#                                         11                         20\n#\n#                                   [9-----|-12]              [18-----|-21]\n#        [0-----|--3]               [9-----|-12]              [18-----|-21]\n# [-2-----|--1][2-----|--5]      [8-----|-11]              [17-----|-20]\n#   +--+--|--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--|\n#           [1 ----|--4]                    [12-----|-15]\n#\n#         ?????????????                 ????        ?              ????\n#           cluster 1                Cluster 2      ?           Cluster 4\n#                                               Cluster 3\n', 'print(clusters)\n[1 1 1 1 1 2 2 2 3 4 4 4]\n'];['a', 'a = np.array([0, 2, 3, 4, 5, 10, 11, 11, 14, 19, 20, 20])\n', 'delta_left, delta_right = 1, 1\n', '#   a = [ 0  .  2  3  4  5  .  .  .  . 10 11  .  . 14  .  .  .  . 19 20\n#                                         11                         20\n#\n#                                     [10--|-12]                 [19--|-21]\n#           [1--|--3]                 [10--|-12]                 [19--|-21]\n#    [-1--|--1]   [3--|--5]         [9--|-11]                 [18--|-20]\n#   +--+--|--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--|\n#              [2--|--4]                       [13--|-15]\n#\n#         ?    ????????                 ????        ?              ????\n#         ?   cluster 2                Cluster 3    ?           Cluster 5\n#     Cluster 1                                 Cluster 4\n', '[-1, 1]', '[1, 3]', 'clusters', 'print(clusters)\n[1 2 2 2 2 3 3 3 4 5 5 5]\n', 'delta_left, delta_right = 2, 1\n', 'x', '[x - 2, x + 1]', '#   a = [ 0  .  2  3  4  5  .  .  .  . 10 11  .  . 14  .  .  .  . 19 20\n#                                         11                         20\n#\n#                                   [9-----|-12]              [18-----|-21]\n#        [0-----|--3]               [9-----|-12]              [18-----|-21]\n# [-2-----|--1][2-----|--5]      [8-----|-11]              [17-----|-20]\n#   +--+--|--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--|\n#           [1 ----|--4]                    [12-----|-15]\n#\n#         ?????????????                 ????        ?              ????\n#           cluster 1                Cluster 2      ?           Cluster 4\n#                                               Cluster 3\n', '[9, 12]', '[12, 15]', 'clusters', 'print(clusters)\n[1 1 1 1 1 2 2 2 3 4 4 4]\n'];['a = np.array([0, 2, 3, 4, 5, 10, 11, 11, 14, 19, 20, 20])\ndelta_left, delta_right = 1, 1\n#   a = [ 0  .  2  3  4  5  .  .  .  . 10 11  .  . 14  .  .  .  . 19 20\n#                                         11                         20\n#\n#                                     [10--|-12]                 [19--|-21]\n#           [1--|--3]                 [10--|-12]                 [19--|-21]\n#    [-1--|--1]   [3--|--5]         [9--|-11]                 [18--|-20]\n#   +--+--|--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--|\n#              [2--|--4]                       [13--|-15]\n#\n#         ?    ????????                 ????        ?              ????\n#         ?   cluster 2                Cluster 3    ?           Cluster 5\n#     Cluster 1                                 Cluster 4\nprint(clusters)\ndelta_left, delta_right = 2, 1\n#   a = [ 0  .  2  3  4  5  .  .  .  . 10 11  .  . 14  .  .  .  . 19 20\n#                                         11                         20\n#\n#                                   [9-----|-12]              [18-----|-21]\n#        [0-----|--3]               [9-----|-12]              [18-----|-21]\n# [-2-----|--1][2-----|--5]      [8-----|-11]              [17-----|-20]\n#   +--+--|--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--|\n#           [1 ----|--4]                    [12-----|-15]\n#\n#         ?????????????                 ????        ?              ????\n#           cluster 1                Cluster 2      ?           Cluster 4\n#                                               Cluster 3\nprint(clusters)\n'];['a = np.array([0, 2, 3, 4, 5, 10, 11, 11, 14, 19, 20, 20])\ndelta_left, delta_right = 1, 1\n#   a = [ 0  .  2  3  4  5  .  .  .  . 10 11  .  . 14  .  .  .  . 19 20\n#                                         11                         20\n#\n#                                     [10--|-12]                 [19--|-21]\n#           [1--|--3]                 [10--|-12]                 [19--|-21]\n#    [-1--|--1]   [3--|--5]         [9--|-11]                 [18--|-20]\n#   +--+--|--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--|\n#              [2--|--4]                       [13--|-15]\n#\n#         ?    ????????                 ????        ?              ????\n#         ?   cluster 2                Cluster 3    ?           Cluster 5\n#     Cluster 1                                 Cluster 4\nprint(clusters)\ndelta_left, delta_right = 2, 1\n#   a = [ 0  .  2  3  4  5  .  .  .  . 10 11  .  . 14  .  .  .  . 19 20\n#                                         11                         20\n#\n#                                   [9-----|-12]              [18-----|-21]\n#        [0-----|--3]               [9-----|-12]              [18-----|-21]\n# [-2-----|--1][2-----|--5]      [8-----|-11]              [17-----|-20]\n#   +--+--|--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--|\n#           [1 ----|--4]                    [12-----|-15]\n#\n#         ?????????????                 ????        ?              ????\n#           cluster 1                Cluster 2      ?           Cluster 4\n#                                               Cluster 3\nprint(clusters)\n'];False;['import pandas as pd\na = np.array([0, 2, 3, 4, 5, 10, 11, 11, 14, 19, 20, 20])\ndelta_left, delta_right = 1, 1\n#   a = [ 0  .  2  3  4  5  .  .  .  . 10 11  .  . 14  .  .  .  . 19 20\n#                                         11                         20\n#\n#                                     [10--|-12]                 [19--|-21]\n#           [1--|--3]                 [10--|-12]                 [19--|-21]\n#    [-1--|--1]   [3--|--5]         [9--|-11]                 [18--|-20]\n#   +--+--|--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--|\n#              [2--|--4]                       [13--|-15]\n#\n#         ?    ????????                 ????        ?              ????\n#         ?   cluster 2                Cluster 3    ?           Cluster 5\n#     Cluster 1                                 Cluster 4\nprint(clusters)\ndelta_left, delta_right = 2, 1\n#   a = [ 0  .  2  3  4  5  .  .  .  . 10 11  .  . 14  .  .  .  . 19 20\n#                                         11                         20\n#\n#                                   [9-----|-12]              [18-----|-21]\n#        [0-----|--3]               [9-----|-12]              [18-----|-21]\n# [-2-----|--1][2-----|--5]      [8-----|-11]              [17-----|-20]\n#   +--+--|--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--|\n#           [1 ----|--4]                    [12-----|-15]\n#\n#         ?????????????                 ????        ?              ????\n#           cluster 1                Cluster 2      ?           Cluster 4\n#                                               Cluster 3\nprint(clusters)\n'];False;0;1;"[""name 'delta_cluster' is not defined""]";['NameError'];0;1;"[""name 'delta_cluster' is not defined""]";['NameError'];0;1;"[""name 'delta_cluster' is not defined""]";['NameError']
1166;1166;1166;1166;1.0;0;41890870;;1;12;<python><pandas><numpy>;how to calculate coskew and cokurtosis;312.0;"[""import pandas as pd\nimport numpy as np\n\nnp.random.seed([3,1415])\ndf = pd.DataFrame(np.random.rand(10, 2), columns=list('ab'))\n\ndf\n\n          a         b\n0  0.444939  0.407554\n1  0.460148  0.465239\n2  0.462691  0.016545\n3  0.850445  0.817744\n4  0.777962  0.757983\n5  0.934829  0.831104\n6  0.879891  0.926879\n7  0.721535  0.117642\n8  0.145906  0.199844\n9  0.437564  0.100702\n""]";"[""import pandas as pd\nimport numpy as np\n\nnp.random.seed([3,1415])\ndf = pd.DataFrame(np.random.rand(10, 2), columns=list('ab'))\n\ndf\n\n          a         b\n0  0.444939  0.407554\n1  0.460148  0.465239\n2  0.462691  0.016545\n3  0.850445  0.817744\n4  0.777962  0.757983\n5  0.934829  0.831104\n6  0.879891  0.926879\n7  0.721535  0.117642\n8  0.145906  0.199844\n9  0.437564  0.100702\n""]";"['pd.Series.skew', 'pd.Series.kurt', 'pd.DataFrame.skew', 'pd.DataFrame.kurt', 'pd.DataFrame', 'df', ""import pandas as pd\nimport numpy as np\n\nnp.random.seed([3,1415])\ndf = pd.DataFrame(np.random.rand(10, 2), columns=list('ab'))\n\ndf\n\n          a         b\n0  0.444939  0.407554\n1  0.460148  0.465239\n2  0.462691  0.016545\n3  0.850445  0.817744\n4  0.777962  0.757983\n5  0.934829  0.831104\n6  0.879891  0.926879\n7  0.721535  0.117642\n8  0.145906  0.199844\n9  0.437564  0.100702\n"", 'a', 'b']";"[""import pandas as pd\nimport numpy as np\n\nnp.random.seed([3,1415])\ndf = pd.DataFrame(np.random.rand(10, 2), columns=list('ab'))\n\ndf\n\n""]";"[""import pandas as pd\nimport numpy as np\n\nnp.random.seed([3,1415])\ndf = pd.DataFrame(np.random.rand(10, 2), columns=list('ab'))\n\ndf\n\n""]";False;"[""import pandas as pd\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed([3,1415])\ndf = pd.DataFrame(np.random.rand(10, 2), columns=list('ab'))\n\ndf\n\n""]";False;0;1;"[""name 'coskew' is not defined""]";['NameError'];0;1;"[""name 'coskew' is not defined""]";['NameError'];0;1;"[""name 'coskew' is not defined""]";['NameError']
1167;1167;1167;1167;3.0;1;41929772;;1;13;<python><pandas><group-by><difference><data-science>;Time difference within group by objects in Python Pandas;820.0;['from    to         datetime              other\n-------------------------------------------------\n11      1     2016-11-06 22:00:00          -\n11      1     2016-11-06 20:00:00          -\n11      1     2016-11-06 15:45:00          -\n11      12    2016-11-06 15:00:00          -\n11      1     2016-11-06 12:00:00          -\n11      18    2016-11-05 10:00:00          -\n11      12    2016-11-05 10:00:00          -\n12      1     2016-10-05 10:00:59          -\n12      3     2016-09-06 10:00:34          -\nfrom    to     timediff in minutes                                          others\n11      1            120\n11      1            255\n11      1            225\n11      1            0 (preferrably subtract this date from the epoch)\n11      12           300\n11      12           0\n11      18           0\n12      1            25\n12      3            0\n'];['from    to         datetime              other\n-------------------------------------------------\n11      1     2016-11-06 22:00:00          -\n11      1     2016-11-06 20:00:00          -\n11      1     2016-11-06 15:45:00          -\n11      12    2016-11-06 15:00:00          -\n11      1     2016-11-06 12:00:00          -\n11      18    2016-11-05 10:00:00          -\n11      12    2016-11-05 10:00:00          -\n12      1     2016-10-05 10:00:59          -\n12      3     2016-09-06 10:00:34          -\n', 'from    to     timediff in minutes                                          others\n11      1            120\n11      1            255\n11      1            225\n11      1            0 (preferrably subtract this date from the epoch)\n11      12           300\n11      12           0\n11      18           0\n12      1            25\n12      3            0\n'];['from    to         datetime              other\n-------------------------------------------------\n11      1     2016-11-06 22:00:00          -\n11      1     2016-11-06 20:00:00          -\n11      1     2016-11-06 15:45:00          -\n11      12    2016-11-06 15:00:00          -\n11      1     2016-11-06 12:00:00          -\n11      18    2016-11-05 10:00:00          -\n11      12    2016-11-05 10:00:00          -\n12      1     2016-10-05 10:00:59          -\n12      3     2016-09-06 10:00:34          -\n', 'from    to     timediff in minutes                                          others\n11      1            120\n11      1            255\n11      1            225\n11      1            0 (preferrably subtract this date from the epoch)\n11      12           300\n11      12           0\n11      18           0\n12      1            25\n12      3            0\n'];[''];[''];False;['import pandas as pd\n'];False;1;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess']";['NameError', 'NameError', 'Sucess'];1;3;"[""name 'df' is not defined"", ""name 'df' is not defined"", 'Sucess']";['NameError', 'NameError', 'Sucess'];1;3;"[""name 'df' is not defined"", ""'from'"", 'Sucess']";['NameError', 'KeyError', 'Sucess']
1168;1168;1168;1168;5.0;8;42076126;;1;12;<python><pandas><scipy><percentile>;vectorize percentile value of column B of column A (for groups);374.0;"[""dt  src dest    a   b\n0   2016-01-01  YYZ SFO 548.12  279.28\n1   2016-01-01  DFW PDX 111.35  -65.50\n2   2016-02-01  YYZ SFO 64.84   342.35\n3   2016-02-01  DFW PDX 63.81   61.64\n4   2016-03-01  YYZ SFO 614.29  262.83\n\n{'a': {0: 548.12,\n  1: 111.34999999999999,\n  2: 64.840000000000003,\n  3: 63.810000000000002,\n  4: 614.28999999999996,\n  5: -207.49000000000001,\n  6: 151.31999999999999,\n  7: -56.43,\n  8: 611.37,\n  9: -296.62,\n  10: 6417.5699999999997,\n  11: -376.25999999999999,\n  12: 465.12,\n  13: -821.73000000000002,\n  14: 1270.6700000000001,\n  15: -1410.0899999999999,\n  16: 1312.6600000000001,\n  17: -326.25999999999999,\n  18: 1683.3699999999999,\n  19: -24.440000000000001,\n  20: 583.60000000000002,\n  21: -5.2400000000000002,\n  22: 1122.74,\n  23: 195.21000000000001,\n  24: 97.040000000000006,\n  25: 133.94},\n 'b': {0: 279.27999999999997,\n  1: -65.5,\n  2: 342.35000000000002,\n  3: 61.640000000000001,\n  4: 262.82999999999998,\n  5: 115.89,\n  6: 268.63999999999999,\n  7: 2.3500000000000001,\n  8: 91.849999999999994,\n  9: 62.119999999999997,\n  10: 778.33000000000004,\n  11: -142.78,\n  12: 1675.53,\n  13: -214.36000000000001,\n  14: 983.80999999999995,\n  15: -207.62,\n  16: 632.13999999999999,\n  17: -132.53,\n  18: 422.36000000000001,\n  19: 13.470000000000001,\n  20: 642.73000000000002,\n  21: -144.59999999999999,\n  22: 213.15000000000001,\n  23: -50.200000000000003,\n  24: 338.27999999999997,\n  25: -129.69},\n 'dest': {0: 'SFO',\n  1: 'PDX',\n  2: 'SFO',\n  3: 'PDX',\n  4: 'SFO',\n  5: 'PDX',\n  6: 'SFO',\n  7: 'PDX',\n  8: 'SFO',\n  9: 'PDX',\n  10: 'SFO',\n  11: 'PDX',\n  12: 'SFO',\n  13: 'PDX',\n  14: 'SFO',\n  15: 'PDX',\n  16: 'SFO',\n  17: 'PDX',\n  18: 'SFO',\n  19: 'PDX',\n  20: 'SFO',\n  21: 'PDX',\n  22: 'SFO',\n  23: 'PDX',\n  24: 'SFO',\n  25: 'PDX'},\n 'dt': {0: Timestamp('2016-01-01 00:00:00'),\n  1: Timestamp('2016-01-01 00:00:00'),\n  2: Timestamp('2016-02-01 00:00:00'),\n  3: Timestamp('2016-02-01 00:00:00'),\n  4: Timestamp('2016-03-01 00:00:00'),\n  5: Timestamp('2016-03-01 00:00:00'),\n  6: Timestamp('2016-04-01 00:00:00'),\n  7: Timestamp('2016-04-01 00:00:00'),\n  8: Timestamp('2016-05-01 00:00:00'),\n  9: Timestamp('2016-05-01 00:00:00'),\n  10: Timestamp('2016-06-01 00:00:00'),\n  11: Timestamp('2016-06-01 00:00:00'),\n  12: Timestamp('2016-07-01 00:00:00'),\n  13: Timestamp('2016-07-01 00:00:00'),\n  14: Timestamp('2016-08-01 00:00:00'),\n  15: Timestamp('2016-08-01 00:00:00'),\n  16: Timestamp('2016-09-01 00:00:00'),\n  17: Timestamp('2016-09-01 00:00:00'),\n  18: Timestamp('2016-10-01 00:00:00'),\n  19: Timestamp('2016-10-01 00:00:00'),\n  20: Timestamp('2016-11-01 00:00:00'),\n  21: Timestamp('2016-11-01 00:00:00'),\n  22: Timestamp('2016-12-01 00:00:00'),\n  23: Timestamp('2016-12-01 00:00:00'),\n  24: Timestamp('2017-01-01 00:00:00'),\n  25: Timestamp('2017-01-01 00:00:00')},\n 'src': {0: 'YYZ',\n  1: 'DFW',\n  2: 'YYZ',\n  3: 'DFW',\n  4: 'YYZ',\n  5: 'DFW',\n  6: 'YYZ',\n  7: 'DFW',\n  8: 'YYZ',\n  9: 'DFW',\n  10: 'YYZ',\n  11: 'DFW',\n  12: 'YYZ',\n  13: 'DFW',\n  14: 'YYZ',\n  15: 'DFW',\n  16: 'YYZ',\n  17: 'DFW',\n  18: 'YYZ',\n  19: 'DFW',\n  20: 'YYZ',\n  21: 'DFW',\n  22: 'YYZ',\n  23: 'DFW',\n  24: 'YYZ',\n  25: 'DFW'}}\nfrom scipy import stats\nimport datetime as dt\nimport pandas as pd\n\np0 = dt.datetime(2017,1,1)\n\n# lets slice df for src=YYZ and dest = SFO\nx = df[(df.src =='YYZ') &\n(df.dest =='SFO') &\n(df.dt ==p0)].b.values[0]\n\n# given B, what percentile does it fall in for the entire column A for YYZ, SFO\nstats.percentileofscore(df['a'],x)\n61.53846153846154\n    src dest  percentile\n0   YYZ SFO   61.54\n1   DFW PDX   23.07\n2   XXX YYY   blahblah1\n3   AAA BBB   blahblah2\n...\ndef b_percentile_a(df,x,y,b):\n    z = df[(df['src'] == x ) & (df['dest'] == y)].a\n    r = stats.percentileofscore(z,b)\n    return r\n\nb_vector_df = df[df.dt == p0]\n\nb_vector_df['p0_a_percentile_b'] = \\\n    b_vector_df.apply(lambda x: b_percentile_a(df,x.src,x.dest,x.b), axis=1)\n""]";"[""dt  src dest    a   b\n0   2016-01-01  YYZ SFO 548.12  279.28\n1   2016-01-01  DFW PDX 111.35  -65.50\n2   2016-02-01  YYZ SFO 64.84   342.35\n3   2016-02-01  DFW PDX 63.81   61.64\n4   2016-03-01  YYZ SFO 614.29  262.83\n\n{'a': {0: 548.12,\n  1: 111.34999999999999,\n  2: 64.840000000000003,\n  3: 63.810000000000002,\n  4: 614.28999999999996,\n  5: -207.49000000000001,\n  6: 151.31999999999999,\n  7: -56.43,\n  8: 611.37,\n  9: -296.62,\n  10: 6417.5699999999997,\n  11: -376.25999999999999,\n  12: 465.12,\n  13: -821.73000000000002,\n  14: 1270.6700000000001,\n  15: -1410.0899999999999,\n  16: 1312.6600000000001,\n  17: -326.25999999999999,\n  18: 1683.3699999999999,\n  19: -24.440000000000001,\n  20: 583.60000000000002,\n  21: -5.2400000000000002,\n  22: 1122.74,\n  23: 195.21000000000001,\n  24: 97.040000000000006,\n  25: 133.94},\n 'b': {0: 279.27999999999997,\n  1: -65.5,\n  2: 342.35000000000002,\n  3: 61.640000000000001,\n  4: 262.82999999999998,\n  5: 115.89,\n  6: 268.63999999999999,\n  7: 2.3500000000000001,\n  8: 91.849999999999994,\n  9: 62.119999999999997,\n  10: 778.33000000000004,\n  11: -142.78,\n  12: 1675.53,\n  13: -214.36000000000001,\n  14: 983.80999999999995,\n  15: -207.62,\n  16: 632.13999999999999,\n  17: -132.53,\n  18: 422.36000000000001,\n  19: 13.470000000000001,\n  20: 642.73000000000002,\n  21: -144.59999999999999,\n  22: 213.15000000000001,\n  23: -50.200000000000003,\n  24: 338.27999999999997,\n  25: -129.69},\n 'dest': {0: 'SFO',\n  1: 'PDX',\n  2: 'SFO',\n  3: 'PDX',\n  4: 'SFO',\n  5: 'PDX',\n  6: 'SFO',\n  7: 'PDX',\n  8: 'SFO',\n  9: 'PDX',\n  10: 'SFO',\n  11: 'PDX',\n  12: 'SFO',\n  13: 'PDX',\n  14: 'SFO',\n  15: 'PDX',\n  16: 'SFO',\n  17: 'PDX',\n  18: 'SFO',\n  19: 'PDX',\n  20: 'SFO',\n  21: 'PDX',\n  22: 'SFO',\n  23: 'PDX',\n  24: 'SFO',\n  25: 'PDX'},\n 'dt': {0: Timestamp('2016-01-01 00:00:00'),\n  1: Timestamp('2016-01-01 00:00:00'),\n  2: Timestamp('2016-02-01 00:00:00'),\n  3: Timestamp('2016-02-01 00:00:00'),\n  4: Timestamp('2016-03-01 00:00:00'),\n  5: Timestamp('2016-03-01 00:00:00'),\n  6: Timestamp('2016-04-01 00:00:00'),\n  7: Timestamp('2016-04-01 00:00:00'),\n  8: Timestamp('2016-05-01 00:00:00'),\n  9: Timestamp('2016-05-01 00:00:00'),\n  10: Timestamp('2016-06-01 00:00:00'),\n  11: Timestamp('2016-06-01 00:00:00'),\n  12: Timestamp('2016-07-01 00:00:00'),\n  13: Timestamp('2016-07-01 00:00:00'),\n  14: Timestamp('2016-08-01 00:00:00'),\n  15: Timestamp('2016-08-01 00:00:00'),\n  16: Timestamp('2016-09-01 00:00:00'),\n  17: Timestamp('2016-09-01 00:00:00'),\n  18: Timestamp('2016-10-01 00:00:00'),\n  19: Timestamp('2016-10-01 00:00:00'),\n  20: Timestamp('2016-11-01 00:00:00'),\n  21: Timestamp('2016-11-01 00:00:00'),\n  22: Timestamp('2016-12-01 00:00:00'),\n  23: Timestamp('2016-12-01 00:00:00'),\n  24: Timestamp('2017-01-01 00:00:00'),\n  25: Timestamp('2017-01-01 00:00:00')},\n 'src': {0: 'YYZ',\n  1: 'DFW',\n  2: 'YYZ',\n  3: 'DFW',\n  4: 'YYZ',\n  5: 'DFW',\n  6: 'YYZ',\n  7: 'DFW',\n  8: 'YYZ',\n  9: 'DFW',\n  10: 'YYZ',\n  11: 'DFW',\n  12: 'YYZ',\n  13: 'DFW',\n  14: 'YYZ',\n  15: 'DFW',\n  16: 'YYZ',\n  17: 'DFW',\n  18: 'YYZ',\n  19: 'DFW',\n  20: 'YYZ',\n  21: 'DFW',\n  22: 'YYZ',\n  23: 'DFW',\n  24: 'YYZ',\n  25: 'DFW'}}\n"", ""from scipy import stats\nimport datetime as dt\nimport pandas as pd\n\np0 = dt.datetime(2017,1,1)\n\n# lets slice df for src=YYZ and dest = SFO\nx = df[(df.src =='YYZ') &\n(df.dest =='SFO') &\n(df.dt ==p0)].b.values[0]\n\n# given B, what percentile does it fall in for the entire column A for YYZ, SFO\nstats.percentileofscore(df['a'],x)\n61.53846153846154\n"", '    src dest  percentile\n0   YYZ SFO   61.54\n1   DFW PDX   23.07\n2   XXX YYY   blahblah1\n3   AAA BBB   blahblah2\n...\n', ""def b_percentile_a(df,x,y,b):\n    z = df[(df['src'] == x ) & (df['dest'] == y)].a\n    r = stats.percentileofscore(z,b)\n    return r\n\nb_vector_df = df[df.dt == p0]\n\nb_vector_df['p0_a_percentile_b'] = \\\n    b_vector_df.apply(lambda x: b_percentile_a(df,x.src,x.dest,x.b), axis=1)\n""]";"['src', 'dest', 'a', 'b', ""dt  src dest    a   b\n0   2016-01-01  YYZ SFO 548.12  279.28\n1   2016-01-01  DFW PDX 111.35  -65.50\n2   2016-02-01  YYZ SFO 64.84   342.35\n3   2016-02-01  DFW PDX 63.81   61.64\n4   2016-03-01  YYZ SFO 614.29  262.83\n\n{'a': {0: 548.12,\n  1: 111.34999999999999,\n  2: 64.840000000000003,\n  3: 63.810000000000002,\n  4: 614.28999999999996,\n  5: -207.49000000000001,\n  6: 151.31999999999999,\n  7: -56.43,\n  8: 611.37,\n  9: -296.62,\n  10: 6417.5699999999997,\n  11: -376.25999999999999,\n  12: 465.12,\n  13: -821.73000000000002,\n  14: 1270.6700000000001,\n  15: -1410.0899999999999,\n  16: 1312.6600000000001,\n  17: -326.25999999999999,\n  18: 1683.3699999999999,\n  19: -24.440000000000001,\n  20: 583.60000000000002,\n  21: -5.2400000000000002,\n  22: 1122.74,\n  23: 195.21000000000001,\n  24: 97.040000000000006,\n  25: 133.94},\n 'b': {0: 279.27999999999997,\n  1: -65.5,\n  2: 342.35000000000002,\n  3: 61.640000000000001,\n  4: 262.82999999999998,\n  5: 115.89,\n  6: 268.63999999999999,\n  7: 2.3500000000000001,\n  8: 91.849999999999994,\n  9: 62.119999999999997,\n  10: 778.33000000000004,\n  11: -142.78,\n  12: 1675.53,\n  13: -214.36000000000001,\n  14: 983.80999999999995,\n  15: -207.62,\n  16: 632.13999999999999,\n  17: -132.53,\n  18: 422.36000000000001,\n  19: 13.470000000000001,\n  20: 642.73000000000002,\n  21: -144.59999999999999,\n  22: 213.15000000000001,\n  23: -50.200000000000003,\n  24: 338.27999999999997,\n  25: -129.69},\n 'dest': {0: 'SFO',\n  1: 'PDX',\n  2: 'SFO',\n  3: 'PDX',\n  4: 'SFO',\n  5: 'PDX',\n  6: 'SFO',\n  7: 'PDX',\n  8: 'SFO',\n  9: 'PDX',\n  10: 'SFO',\n  11: 'PDX',\n  12: 'SFO',\n  13: 'PDX',\n  14: 'SFO',\n  15: 'PDX',\n  16: 'SFO',\n  17: 'PDX',\n  18: 'SFO',\n  19: 'PDX',\n  20: 'SFO',\n  21: 'PDX',\n  22: 'SFO',\n  23: 'PDX',\n  24: 'SFO',\n  25: 'PDX'},\n 'dt': {0: Timestamp('2016-01-01 00:00:00'),\n  1: Timestamp('2016-01-01 00:00:00'),\n  2: Timestamp('2016-02-01 00:00:00'),\n  3: Timestamp('2016-02-01 00:00:00'),\n  4: Timestamp('2016-03-01 00:00:00'),\n  5: Timestamp('2016-03-01 00:00:00'),\n  6: Timestamp('2016-04-01 00:00:00'),\n  7: Timestamp('2016-04-01 00:00:00'),\n  8: Timestamp('2016-05-01 00:00:00'),\n  9: Timestamp('2016-05-01 00:00:00'),\n  10: Timestamp('2016-06-01 00:00:00'),\n  11: Timestamp('2016-06-01 00:00:00'),\n  12: Timestamp('2016-07-01 00:00:00'),\n  13: Timestamp('2016-07-01 00:00:00'),\n  14: Timestamp('2016-08-01 00:00:00'),\n  15: Timestamp('2016-08-01 00:00:00'),\n  16: Timestamp('2016-09-01 00:00:00'),\n  17: Timestamp('2016-09-01 00:00:00'),\n  18: Timestamp('2016-10-01 00:00:00'),\n  19: Timestamp('2016-10-01 00:00:00'),\n  20: Timestamp('2016-11-01 00:00:00'),\n  21: Timestamp('2016-11-01 00:00:00'),\n  22: Timestamp('2016-12-01 00:00:00'),\n  23: Timestamp('2016-12-01 00:00:00'),\n  24: Timestamp('2017-01-01 00:00:00'),\n  25: Timestamp('2017-01-01 00:00:00')},\n 'src': {0: 'YYZ',\n  1: 'DFW',\n  2: 'YYZ',\n  3: 'DFW',\n  4: 'YYZ',\n  5: 'DFW',\n  6: 'YYZ',\n  7: 'DFW',\n  8: 'YYZ',\n  9: 'DFW',\n  10: 'YYZ',\n  11: 'DFW',\n  12: 'YYZ',\n  13: 'DFW',\n  14: 'YYZ',\n  15: 'DFW',\n  16: 'YYZ',\n  17: 'DFW',\n  18: 'YYZ',\n  19: 'DFW',\n  20: 'YYZ',\n  21: 'DFW',\n  22: 'YYZ',\n  23: 'DFW',\n  24: 'YYZ',\n  25: 'DFW'}}\n"", 'src', 'dest', 'b', 'date = 2017-01-01', 'src', 'dest', 'a', 'i.e. src=YYZ and dest=SFT', ""from scipy import stats\nimport datetime as dt\nimport pandas as pd\n\np0 = dt.datetime(2017,1,1)\n\n# lets slice df for src=YYZ and dest = SFO\nx = df[(df.src =='YYZ') &\n(df.dest =='SFO') &\n(df.dt ==p0)].b.values[0]\n\n# given B, what percentile does it fall in for the entire column A for YYZ, SFO\nstats.percentileofscore(df['a'],x)\n61.53846153846154\n"", 'vectorize', 'pandas features', 'groupby', 'apply', '    src dest  percentile\n0   YYZ SFO   61.54\n1   DFW PDX   23.07\n2   XXX YYY   blahblah1\n3   AAA BBB   blahblah2\n...\n', ""def b_percentile_a(df,x,y,b):\n    z = df[(df['src'] == x ) & (df['dest'] == y)].a\n    r = stats.percentileofscore(z,b)\n    return r\n\nb_vector_df = df[df.dt == p0]\n\nb_vector_df['p0_a_percentile_b'] = \\\n    b_vector_df.apply(lambda x: b_percentile_a(df,x.src,x.dest,x.b), axis=1)\n"", '5.16', '100', '55,000', '~50', '36', 'several days']";"[""\nfrom scipy import stats\nimport datetime as dt\nimport pandas as pd\n\np0 = dt.datetime(2017,1,1)\n\n# lets slice df for src=YYZ and dest = SFO\n\n# given B, what percentile does it fall in for the entire column A for YYZ, SFO\nstats.percentileofscore(df['a'],x)\n61.53846153846154\n...\n\nb_vector_df = df[df.dt == p0]\n\n""]";"[""\nfrom scipy import stats\nimport datetime as dt\nimport pandas as pd\n\np0 = dt.datetime(2017,1,1)\n\n# lets slice df for src=YYZ and dest = SFO\n\n# given B, what percentile does it fall in for the entire column A for YYZ, SFO\nstats.percentileofscore(df['a'],x)\n61.53846153846154\n...\n\nb_vector_df = df[df.dt == p0]\n\n""]";False;"[""import pandas as pd\n\nfrom scipy import stats\nimport datetime as dt\nimport pandas as pd\n\np0 = dt.datetime(2017,1,1)\n\n# lets slice df for src=YYZ and dest = SFO\n\n# given B, what percentile does it fall in for the entire column A for YYZ, SFO\nstats.percentileofscore(df['a'],x)\n61.53846153846154\n...\n\nb_vector_df = df[df.dt == p0]\n\n""]";False;0;1;"[""No module named 'PercentileData'""]";['ImportError'];0;1;"[""No module named 'PercentileData'""]";['ImportError'];0;1;"[""No module named 'PercentileData'""]";['ImportError']
1169;1169;1169;1169;9.0;3;42157944;;1;34;<python><regex><performance><parsing><pandas>;How can I speed up reading multiple files and putting the data into a dataframe?;1476.0;"['import re\nimport pandas as pd\n\ndf = pd.DataFrame()\npaths = [""../gitignore/test1.txt"", ""../gitignore/test2.txt""]\nreg_ex = re.compile(\'^(.+) (.+)\\n\')\n# read all files to determine what indices are available\nfor path in paths:\n    file_obj = open(path, \'r\')\n    print file_obj.readlines()\n\n[\'a 1\\n\', \'b 2\\n\', \'end\']\n[\'c 3\\n\', \'d 4\\n\', \'end\']\n\nindices = []\nfor path in paths:\n    index = []\n    with open(path, \'r\') as file_obj:\n        line = True\n        while line:\n            try:\n                line = file_obj.readline()\n                match = reg_ex.match(line)\n                index += match.group(1)\n            except AttributeError:\n                pass\n    indices.append(index)\n# read files again and put data into a master dataframe\nfor path, index in zip(paths, indices):\n    subset_df = pd.DataFrame(index=index, columns=[""Number""])\n    with open(path, \'r\') as file_obj:\n        line = True\n        while line:\n            try:\n                line = file_obj.readline()\n                match = reg_ex.match(line)\n                subset_df.loc[[match.group(1)]] = match.group(2)\n            except AttributeError:\n                pass\n    df = pd.concat([df, subset_df]).sort_index()\nprint df\n\n  Number\na      1\nb      2\nc      3\nd      4\na 1\nb 2\nend\nc 3\nd 4\nend\n']";"['import re\nimport pandas as pd\n\ndf = pd.DataFrame()\npaths = [""../gitignore/test1.txt"", ""../gitignore/test2.txt""]\nreg_ex = re.compile(\'^(.+) (.+)\\n\')\n# read all files to determine what indices are available\nfor path in paths:\n    file_obj = open(path, \'r\')\n    print file_obj.readlines()\n\n[\'a 1\\n\', \'b 2\\n\', \'end\']\n[\'c 3\\n\', \'d 4\\n\', \'end\']\n\nindices = []\nfor path in paths:\n    index = []\n    with open(path, \'r\') as file_obj:\n        line = True\n        while line:\n            try:\n                line = file_obj.readline()\n                match = reg_ex.match(line)\n                index += match.group(1)\n            except AttributeError:\n                pass\n    indices.append(index)\n# read files again and put data into a master dataframe\nfor path, index in zip(paths, indices):\n    subset_df = pd.DataFrame(index=index, columns=[""Number""])\n    with open(path, \'r\') as file_obj:\n        line = True\n        while line:\n            try:\n                line = file_obj.readline()\n                match = reg_ex.match(line)\n                subset_df.loc[[match.group(1)]] = match.group(2)\n            except AttributeError:\n                pass\n    df = pd.concat([df, subset_df]).sort_index()\nprint df\n\n  Number\na      1\nb      2\nc      3\nd      4\n', 'a 1\nb 2\nend\n', 'c 3\nd 4\nend\n']";"['import re\nimport pandas as pd\n\ndf = pd.DataFrame()\npaths = [""../gitignore/test1.txt"", ""../gitignore/test2.txt""]\nreg_ex = re.compile(\'^(.+) (.+)\\n\')\n# read all files to determine what indices are available\nfor path in paths:\n    file_obj = open(path, \'r\')\n    print file_obj.readlines()\n\n[\'a 1\\n\', \'b 2\\n\', \'end\']\n[\'c 3\\n\', \'d 4\\n\', \'end\']\n\nindices = []\nfor path in paths:\n    index = []\n    with open(path, \'r\') as file_obj:\n        line = True\n        while line:\n            try:\n                line = file_obj.readline()\n                match = reg_ex.match(line)\n                index += match.group(1)\n            except AttributeError:\n                pass\n    indices.append(index)\n# read files again and put data into a master dataframe\nfor path, index in zip(paths, indices):\n    subset_df = pd.DataFrame(index=index, columns=[""Number""])\n    with open(path, \'r\') as file_obj:\n        line = True\n        while line:\n            try:\n                line = file_obj.readline()\n                match = reg_ex.match(line)\n                subset_df.loc[[match.group(1)]] = match.group(2)\n            except AttributeError:\n                pass\n    df = pd.concat([df, subset_df]).sort_index()\nprint df\n\n  Number\na      1\nb      2\nc      3\nd      4\n', 'a 1\nb 2\nend\n', 'c 3\nd 4\nend\n']";"['import re\nimport pandas as pd\n\ndf = pd.DataFrame()\npaths = [""../gitignore/test1.txt"", ""../gitignore/test2.txt""]\nreg_ex = re.compile(\'^(.+) (.+)\\n\')\n# read all files to determine what indices are available\n\n[\'a 1\\n\', \'b 2\\n\', \'end\']\n[\'c 3\\n\', \'d 4\\n\', \'end\']\n\nindices = []\n# read files again and put data into a master dataframe\n\nend\nend\n']";"['import re\nimport pandas as pd\n\ndf = pd.DataFrame()\npaths = [""../gitignore/test1.txt"", ""../gitignore/test2.txt""]\nreg_ex = re.compile(\'^(.+) (.+)\\n\')\n# read all files to determine what indices are available\n\n[\'a 1\\n\', \'b 2\\n\', \'end\']\n[\'c 3\\n\', \'d 4\\n\', \'end\']\n\nindices = []\n# read files again and put data into a master dataframe\n\nend\nend\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\nimport re\nimport pandas as pd\n\ndf = pd.DataFrame()\npaths = [""../gitignore/test1.txt"", ""../gitignore/test2.txt""]\nreg_ex = re.compile(\'^(.+) (.+)\\n\')\n# read all files to determine what indices are available\n\n[\'a 1\\n\', \'b 2\\n\', \'end\']\n[\'c 3\\n\', \'d 4\\n\', \'end\']\n\nindices = []\n# read files again and put data into a master dataframe\n\nend\nend\n']";True;2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess'];2;2;['Sucess', 'Sucess'];['Sucess', 'Sucess']
1170;1170;1170;1170;1.0;6;42347868;;1;24;<python>;Convert to date using formatters parameter in pandas to_string;839.0;"['import pandas as pd\n\nurl = ""https://raw.github.com/pandas-dev/pandas/master/pandas/tests/data/tips.csv""\ndf = pd.read_csv(url)\ndf[""date""] = list(range(42005, 42005+len(df)))\nprint(\n  df\n  .head(10)\n  .to_string(\n    formatters={""total_bill"": ""${:,.2f}"".format, \n                ""tip"": ""${:,.2f}"".format\n    }\n  )\n)\n']";"['import pandas as pd\n\nurl = ""https://raw.github.com/pandas-dev/pandas/master/pandas/tests/data/tips.csv""\ndf = pd.read_csv(url)\ndf[""date""] = list(range(42005, 42005+len(df)))\n', 'print(\n  df\n  .head(10)\n  .to_string(\n    formatters={""total_bill"": ""${:,.2f}"".format, \n                ""tip"": ""${:,.2f}"".format\n    }\n  )\n)\n']";"['import pandas as pd\n\nurl = ""https://raw.github.com/pandas-dev/pandas/master/pandas/tests/data/tips.csv""\ndf = pd.read_csv(url)\ndf[""date""] = list(range(42005, 42005+len(df)))\n', 'print(\n  df\n  .head(10)\n  .to_string(\n    formatters={""total_bill"": ""${:,.2f}"".format, \n                ""tip"": ""${:,.2f}"".format\n    }\n  )\n)\n']";"['import pandas as pd\n\nurl = ""https://raw.github.com/pandas-dev/pandas/master/pandas/tests/data/tips.csv""\ndf = pd.read_csv(url)\ndf[""date""] = list(range(42005, 42005+len(df)))\n']";"['import pandas as pd\n\nurl = ""https://raw.github.com/pandas-dev/pandas/master/pandas/tests/data/tips.csv""\ndf = pd.read_csv(url)\ndf[""date""] = list(range(42005, 42005+len(df)))\n']";False;"['import pandas as pd\ndata = pd.DataFrame()\nimport pandas as pd\n\nurl = ""https://raw.github.com/pandas-dev/pandas/master/pandas/tests/data/tips.csv""\ndf = pd.read_csv(url)\ndf[""date""] = list(range(42005, 42005+len(df)))\n']";True;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1171;1171;1171;1171;4.0;0;42516070;;1;14;<python><pandas><slice><mask><argmax>;Set value of first item in slice in python pandas;672.0;['df = pandas.DataFrame(numpy.random.rand(3,1))\ndf[df[0]>0][0] = 0\ndf.iloc[df[0]>0,:][0] = 0\ndf[df[0]>0,:].iloc[0] = 0\n'];['df = pandas.DataFrame(numpy.random.rand(3,1))\ndf[df[0]>0][0] = 0\n', 'df.iloc[df[0]>0,:][0] = 0\ndf[df[0]>0,:].iloc[0] = 0\n'];['df = pandas.DataFrame(numpy.random.rand(3,1))\ndf[df[0]>0][0] = 0\n', 'df.iloc[df[0]>0,:][0] = 0\ndf[df[0]>0,:].iloc[0] = 0\n'];['df = pandas.DataFrame(numpy.random.rand(3,1))\ndf[df[0]>0][0] = 0\ndf.iloc[df[0]>0,:][0] = 0\ndf[df[0]>0,:].iloc[0] = 0\n'];['df = pandas.DataFrame(numpy.random.rand(3,1))\ndf[df[0]>0][0] = 0\ndf.iloc[df[0]>0,:][0] = 0\ndf[df[0]>0,:].iloc[0] = 0\n'];False;['import pandas as pd\ndf = pandas.DataFrame(numpy.random.rand(3,1))\ndf[df[0]>0][0] = 0\ndf.iloc[df[0]>0,:][0] = 0\ndf[df[0]>0,:].iloc[0] = 0\n'];False;0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError'];0;1;"[""name 'np' is not defined""]";['NameError']
1172;1172;1172;1172;4.0;9;43423347;;1;23;<python><pandas>;why is blindly using df.copy() a bad idea to fix the SettingWithCopyWarning;697.0;"[""df = pd.DataFrame([[1]])\n\nd1 = df[:]\n\nd1.is_copy\n\n<weakref at 0x1115a4188; to 'DataFrame' at 0x1119bb0f0>\nd1 = d1.copy()\n""]";"[""df = pd.DataFrame([[1]])\n\nd1 = df[:]\n\nd1.is_copy\n\n<weakref at 0x1115a4188; to 'DataFrame' at 0x1119bb0f0>\n"", 'd1 = d1.copy()\n']";"['SettingWithCopyWarning', 'df', 'is_copy', ""df = pd.DataFrame([[1]])\n\nd1 = df[:]\n\nd1.is_copy\n\n<weakref at 0x1115a4188; to 'DataFrame' at 0x1119bb0f0>\n"", 'None', 'd1 = d1.copy()\n', 'SettingWithCopyWarning', 'copy', 'df = df.copy()', 'SettingWithCopyWarning']";['df = pd.DataFrame([[1]])\n\nd1 = df[:]\n\nd1.is_copy\n\nd1 = d1.copy()\n'];['import pandas as pd\ndf = pd.DataFrame([[1]])\n\nd1 = df[:]\n\nd1.is_copy\n\nd1 = d1.copy()\n'];True;['import pandas as pd\ndf = pd.DataFrame([[1]])\n\nd1 = df[:]\n\nd1.is_copy\n\nd1 = d1.copy()\n'];False;0;2;"[""name 'pd' is not defined"", ""name 'Line' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'q' is not defined"", ""name 'Line' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'q' is not defined"", ""name 'Line' is not defined""]";['NameError', 'NameError']
1173;1173;1173;1173;2.0;1;43545879;;1;13;<python><pandas><matplotlib><plot>;Bar Chart with multiple labels;506.0;"[""df = pd.DataFrame(np.random.rand(6, 4),\n                 index=['one', 'two', 'three', 'four', 'five', 'six'],\n                 columns=pd.Index(['A', 'B', 'C', 'D'], \n                 name='Genus')).round(2)\n\n\ndf.plot(kind='bar',figsize=(10,4))\n""]";"[""df = pd.DataFrame(np.random.rand(6, 4),\n                 index=['one', 'two', 'three', 'four', 'five', 'six'],\n                 columns=pd.Index(['A', 'B', 'C', 'D'], \n                 name='Genus')).round(2)\n\n\ndf.plot(kind='bar',figsize=(10,4))\n""]";"[""df = pd.DataFrame(np.random.rand(6, 4),\n                 index=['one', 'two', 'three', 'four', 'five', 'six'],\n                 columns=pd.Index(['A', 'B', 'C', 'D'], \n                 name='Genus')).round(2)\n\n\ndf.plot(kind='bar',figsize=(10,4))\n""]";"[""\n\ndf.plot(kind='bar',figsize=(10,4))\n""]";"[""\n\ndf.plot(kind='bar',figsize=(10,4))\n""]";False;"[""import pandas as pd\ndf = pd.DataFrame()\n\n\ndf.plot(kind='bar',figsize=(10,4))\n""]";True;0;2;"[""name 'df' is not defined"", ""No module named 'matplotlib'""]";['NameError', 'ImportError'];0;2;"[""name 'df' is not defined"", ""No module named 'matplotlib'""]";['NameError', 'ImportError'];0;2;"[""name '_converter' is not defined"", ""No module named 'matplotlib'""]";['NameError', 'ImportError']
1174;1174;1174;1174;4.0;1;43577744;;1;16;<python><python-2.7><pandas><numpy><scipy>;Fastest way to create strictly increasing lists in Python;1592.0;['a = [2,1,2,3,4,5,4,6,5,7,8,9,8,10,11]\nb = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\na_new = [2,3,4,5,6,7,8,9,10,11]\nb_new = [1,4,5,6,8,10,11,12,14,15]\n'];['a = [2,1,2,3,4,5,4,6,5,7,8,9,8,10,11]\nb = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n', 'a_new = [2,3,4,5,6,7,8,9,10,11]\nb_new = [1,4,5,6,8,10,11,12,14,15]\n'];['a', 'b', 'a = [2,1,2,3,4,5,4,6,5,7,8,9,8,10,11]\nb = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n', 'a_new', 'a', 'a', 'b', 'a_new = [2,3,4,5,6,7,8,9,10,11]\nb_new = [1,4,5,6,8,10,11,12,14,15]\n', 'for'];['a = [2,1,2,3,4,5,4,6,5,7,8,9,8,10,11]\nb = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\na_new = [2,3,4,5,6,7,8,9,10,11]\nb_new = [1,4,5,6,8,10,11,12,14,15]\n'];['a = [2,1,2,3,4,5,4,6,5,7,8,9,8,10,11]\nb = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\na_new = [2,3,4,5,6,7,8,9,10,11]\nb_new = [1,4,5,6,8,10,11,12,14,15]\n'];False;['import pandas as pd\na = [2,1,2,3,4,5,4,6,5,7,8,9,8,10,11]\nb = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\na_new = [2,3,4,5,6,7,8,9,10,11]\nb_new = [1,4,5,6,8,10,11,12,14,15]\n'];False;1;4;"[""name 'np' is not defined"", 'Sucess', ""name 'arr' is not defined"", ""No module named 'numba'""]";['NameError', 'Sucess', 'NameError', 'ImportError'];1;4;"[""name 'np' is not defined"", 'Sucess', ""name 'arr' is not defined"", ""No module named 'numba'""]";['NameError', 'Sucess', 'NameError', 'ImportError'];1;4;"[""name 'np' is not defined"", 'Sucess', ""name 'arr' is not defined"", ""No module named 'numba'""]";['NameError', 'Sucess', 'NameError', 'ImportError']
1175;1175;1175;1175;5.0;2;43791970;;1;11;<python><pandas><dataframe><finance><portfolio>;Pandas: assigning columns with multiple conditions and date thresholds;821.0;['Date    Stock   Weight  Percentile  Final weight\n1/1/2000    Apple   0.010   0.75    0.010\n1/1/2000    IBM    0.011    0.4     0\n1/1/2000    Google  0.012   0.45    0\n1/1/2000    Nokia   0.022   0.81    0.022\n2/1/2000    Apple   0.014   0.56    0\n2/1/2000    Google  0.015   0.45    0\n2/1/2000    Nokia   0.016   0.55    0\n3/1/2000    Apple   0.020   0.52    0\n3/1/2000    Google  0.030   0.51    0\n3/1/2000    Nokia   0.040   0.47    0\nDate    Stock   Weight  Percentile  Final weight\n1/1/2000    Apple   0.010   0.75    0.010\n1/1/2000    IBM     0.011   0.4     0\n1/1/2000    Google  0.012   0.45    0\n1/1/2000    Nokia   0.022   0.81    0.022\n2/1/2000    Apple   0.014   0.56    0.014\n2/1/2000    Google  0.015   0.45    0\n2/1/2000    Nokia   0.016   0.55    0.016\n3/1/2000    Apple   0.020   0.52    0.020\n3/1/2000    Google  0.030   0.51    0\n3/1/2000    Nokia   0.040   0.47    0\n'];['Date    Stock   Weight  Percentile  Final weight\n1/1/2000    Apple   0.010   0.75    0.010\n1/1/2000    IBM    0.011    0.4     0\n1/1/2000    Google  0.012   0.45    0\n1/1/2000    Nokia   0.022   0.81    0.022\n2/1/2000    Apple   0.014   0.56    0\n2/1/2000    Google  0.015   0.45    0\n2/1/2000    Nokia   0.016   0.55    0\n3/1/2000    Apple   0.020   0.52    0\n3/1/2000    Google  0.030   0.51    0\n3/1/2000    Nokia   0.040   0.47    0\n', 'Date    Stock   Weight  Percentile  Final weight\n1/1/2000    Apple   0.010   0.75    0.010\n1/1/2000    IBM     0.011   0.4     0\n1/1/2000    Google  0.012   0.45    0\n1/1/2000    Nokia   0.022   0.81    0.022\n2/1/2000    Apple   0.014   0.56    0.014\n2/1/2000    Google  0.015   0.45    0\n2/1/2000    Nokia   0.016   0.55    0.016\n3/1/2000    Apple   0.020   0.52    0.020\n3/1/2000    Google  0.030   0.51    0\n3/1/2000    Nokia   0.040   0.47    0\n'];['Date    Stock   Weight  Percentile  Final weight\n1/1/2000    Apple   0.010   0.75    0.010\n1/1/2000    IBM    0.011    0.4     0\n1/1/2000    Google  0.012   0.45    0\n1/1/2000    Nokia   0.022   0.81    0.022\n2/1/2000    Apple   0.014   0.56    0\n2/1/2000    Google  0.015   0.45    0\n2/1/2000    Nokia   0.016   0.55    0\n3/1/2000    Apple   0.020   0.52    0\n3/1/2000    Google  0.030   0.51    0\n3/1/2000    Nokia   0.040   0.47    0\n', 'Final_weight', 'Weight', 'Percentile', '0.7', 'Weight', 'Final_weight', 'Percentile is > 0.7', 'Percentile', '>0.7', 'Percentile', '0.5', '0.5', 'Final_weight would become 0', 'Date    Stock   Weight  Percentile  Final weight\n1/1/2000    Apple   0.010   0.75    0.010\n1/1/2000    IBM     0.011   0.4     0\n1/1/2000    Google  0.012   0.45    0\n1/1/2000    Nokia   0.022   0.81    0.022\n2/1/2000    Apple   0.014   0.56    0.014\n2/1/2000    Google  0.015   0.45    0\n2/1/2000    Nokia   0.016   0.55    0.016\n3/1/2000    Apple   0.020   0.52    0.020\n3/1/2000    Google  0.030   0.51    0\n3/1/2000    Nokia   0.040   0.47    0\n'];[''];[''];False;['import pandas as pd\n'];False;0;0;[];[];0;0;[];[];0;0;[];[]
1176;1176;1176;1176;2.0;5;44030936;;1;13;<python><pandas><lambda><vectorization><ranking>;Performance enhancement of ranking function by replacement of lambda x with vectorization;463.0;"[""ranked = df[['period_id', 'sector_name'] + to_rank].groupby(['period_id', 'sector_name']).transform(lambda x: (x.rank(ascending = True) - 1)*100/len(x))        \nimport pandas as pd\nimport numpy as np\nimport random\n\nto_rank = ['var_1', 'var_2', 'var_3']\ndf = pd.DataFrame({'var_1' : np.random.randn(1000), 'var_2' : np.random.randn(1000), 'var_3' : np.random.randn(1000)})\ndf['date_id'] = np.random.choice(range(2001, 2012), df.shape[0])\ndf['category'] = ','.join(chr(random.randrange(97, 97 + 4 + 1)).upper() for x in range(1,df.shape[0]+1)).split(',')\ndef rank_fun(df, to_rank): # calls ranking function f(x) to rank each category at each date\n    #extra data tidying logic here beyond scope of question - can remove\n    ranked = df[to_rank].apply(lambda x: f(x))\n    return ranked\n\n\ndef f(x):\n    nans = x[np.isnan(x)] # Remove nans as these will be ranked with 50\n    sub_df = x.dropna() # \n    nans_ranked = nans.replace(np.nan, 50) # give nans rank of 50\n\n    if len(sub_df.index) == 0: #check not all nan.  If no non-nan data, then return with rank 50\n        return nans_ranked\n\n    if len(sub_df.unique()) == 1: # if all data has same value, return rank 50\n        sub_df[:] = 50\n        return sub_df\n\n    #Check that we don't have too many clustered values, such that we can't bin due to overlap of ties, and reduce bin size provided we can at least quintile rank.\n    max_cluster = sub_df.value_counts().iloc[0] #value_counts sorts by counts, so first element will contain the max\n    max_bins = len(sub_df) / max_cluster \n\n    if max_bins > 100: #if largest cluster <1% of available data, then we can percentile_rank\n        max_bins = 100\n\n    if max_bins < 5: #if we don't have the resolution to quintile rank then assume no data.\n        sub_df[:] = 50\n        return sub_df\n\n    bins = int(max_bins) # bin using highest resolution that the data supports, subject to constraints above (max 100 bins, min 5 bins)\n\n    sub_df_ranked = pd.qcut(sub_df, bins, labels=False) #currently using pd.qcut.  pd.rank( seems to have extra functionality, but overheads similar in practice\n    sub_df_ranked *= (100 / bins) #Since we bin using the resolution specified in bins, to convert back to decile rank, we have to multiply by 100/bins.  E.g. with quintiles, we'll have scores 1 - 5, so have to multiply by 100 / 5 = 20 to convert to percentile ranking\n    ranked_df = pd.concat([sub_df_ranked, nans_ranked])\n    return ranked_df\n# ensure don't get duplicate columns if ranking already executed\nranked_cols = [col + '_ranked' for col in to_rank]\n\nranked = df[['date_id', 'category'] + to_rank].groupby(['date_id', 'category'], as_index = False).apply(lambda x: rank_fun(x, to_rank)) \nranked.columns = ranked_cols        \nranked.reset_index(inplace = True)\nranked.set_index('level_1', inplace = True)    \ndf = df.join(ranked[ranked_cols])\n 2                                           def tst_fun(df, field):\n 3         1          685    685.0      0.2      x = df[field]\n 4         1        20726  20726.0      5.8      nans = x[np.isnan(x)]\n 5         1        28448  28448.0      8.0      sub_df = x.dropna()\n 6         1          387    387.0      0.1      nans_ranked = nans.replace(np.nan, 50)\n 7         1            5      5.0      0.0      if len(sub_df.index) == 0: \n 8                                                   pass #check not empty.  May be empty due to nans for first 5 years e.g. no revenue/operating margin data pre 1990\n 9                                                   return nans_ranked\n10                                           \n11         1        65559  65559.0     18.4      if len(sub_df.unique()) == 1: \n12                                                   sub_df[:] = 50 #e.g. for subranks where all factors had nan so ranked as 50 e.g. in 1990\n13                                                   return sub_df\n14                                           \n15                                               #Finally, check that we don't have too many clustered values, such that we can't bin, and reduce bin size provided we can at least quintile rank.\n16         1        74610  74610.0     20.9      max_cluster = sub_df.value_counts().iloc[0] #value_counts sorts by counts, so first element will contain the max\n17                                               # print(counts)\n18         1            9      9.0      0.0      max_bins = len(sub_df) / max_cluster #\n19                                           \n20         1            3      3.0      0.0      if max_bins > 100: \n21         1            0      0.0      0.0          max_bins = 100 #if largest cluster <1% of available data, then we can percentile_rank\n22                                           \n23                                           \n24         1            0      0.0      0.0      if max_bins < 5: \n25                                                   sub_df[:] = 50 #if we don't have the resolution to quintile rank then assume no data.\n26                                           \n27                                               #     return sub_df\n28                                           \n29         1            1      1.0      0.0      bins = int(max_bins) # bin using highest resolution that the data supports, subject to constraints above (max 100 bins, min 5 bins)\n30                                           \n31                                               #should track bin resolution for all data.  To add.\n32                                           \n33                                               #if get here, then neither nans_ranked, nor sub_df are empty\n34                                               # sub_df_ranked = pd.qcut(sub_df, bins, labels=False)\n35         1       160530 160530.0     45.0      sub_df_ranked = (sub_df.rank(ascending = True) - 1)*100/len(x)\n36                                           \n37         1         5777   5777.0      1.6      ranked_df = pd.concat([sub_df_ranked, nans_ranked])\n38                                               \n39         1            1      1.0      0.0      return ranked_df\n""]";"[""ranked = df[['period_id', 'sector_name'] + to_rank].groupby(['period_id', 'sector_name']).transform(lambda x: (x.rank(ascending = True) - 1)*100/len(x))        \n"", ""import pandas as pd\nimport numpy as np\nimport random\n\nto_rank = ['var_1', 'var_2', 'var_3']\ndf = pd.DataFrame({'var_1' : np.random.randn(1000), 'var_2' : np.random.randn(1000), 'var_3' : np.random.randn(1000)})\ndf['date_id'] = np.random.choice(range(2001, 2012), df.shape[0])\ndf['category'] = ','.join(chr(random.randrange(97, 97 + 4 + 1)).upper() for x in range(1,df.shape[0]+1)).split(',')\n"", ""def rank_fun(df, to_rank): # calls ranking function f(x) to rank each category at each date\n    #extra data tidying logic here beyond scope of question - can remove\n    ranked = df[to_rank].apply(lambda x: f(x))\n    return ranked\n\n\ndef f(x):\n    nans = x[np.isnan(x)] # Remove nans as these will be ranked with 50\n    sub_df = x.dropna() # \n    nans_ranked = nans.replace(np.nan, 50) # give nans rank of 50\n\n    if len(sub_df.index) == 0: #check not all nan.  If no non-nan data, then return with rank 50\n        return nans_ranked\n\n    if len(sub_df.unique()) == 1: # if all data has same value, return rank 50\n        sub_df[:] = 50\n        return sub_df\n\n    #Check that we don't have too many clustered values, such that we can't bin due to overlap of ties, and reduce bin size provided we can at least quintile rank.\n    max_cluster = sub_df.value_counts().iloc[0] #value_counts sorts by counts, so first element will contain the max\n    max_bins = len(sub_df) / max_cluster \n\n    if max_bins > 100: #if largest cluster <1% of available data, then we can percentile_rank\n        max_bins = 100\n\n    if max_bins < 5: #if we don't have the resolution to quintile rank then assume no data.\n        sub_df[:] = 50\n        return sub_df\n\n    bins = int(max_bins) # bin using highest resolution that the data supports, subject to constraints above (max 100 bins, min 5 bins)\n\n    sub_df_ranked = pd.qcut(sub_df, bins, labels=False) #currently using pd.qcut.  pd.rank( seems to have extra functionality, but overheads similar in practice\n    sub_df_ranked *= (100 / bins) #Since we bin using the resolution specified in bins, to convert back to decile rank, we have to multiply by 100/bins.  E.g. with quintiles, we'll have scores 1 - 5, so have to multiply by 100 / 5 = 20 to convert to percentile ranking\n    ranked_df = pd.concat([sub_df_ranked, nans_ranked])\n    return ranked_df\n"", ""# ensure don't get duplicate columns if ranking already executed\nranked_cols = [col + '_ranked' for col in to_rank]\n\nranked = df[['date_id', 'category'] + to_rank].groupby(['date_id', 'category'], as_index = False).apply(lambda x: rank_fun(x, to_rank)) \nranked.columns = ranked_cols        \nranked.reset_index(inplace = True)\nranked.set_index('level_1', inplace = True)    \ndf = df.join(ranked[ranked_cols])\n"", "" 2                                           def tst_fun(df, field):\n 3         1          685    685.0      0.2      x = df[field]\n 4         1        20726  20726.0      5.8      nans = x[np.isnan(x)]\n 5         1        28448  28448.0      8.0      sub_df = x.dropna()\n 6         1          387    387.0      0.1      nans_ranked = nans.replace(np.nan, 50)\n 7         1            5      5.0      0.0      if len(sub_df.index) == 0: \n 8                                                   pass #check not empty.  May be empty due to nans for first 5 years e.g. no revenue/operating margin data pre 1990\n 9                                                   return nans_ranked\n10                                           \n11         1        65559  65559.0     18.4      if len(sub_df.unique()) == 1: \n12                                                   sub_df[:] = 50 #e.g. for subranks where all factors had nan so ranked as 50 e.g. in 1990\n13                                                   return sub_df\n14                                           \n15                                               #Finally, check that we don't have too many clustered values, such that we can't bin, and reduce bin size provided we can at least quintile rank.\n16         1        74610  74610.0     20.9      max_cluster = sub_df.value_counts().iloc[0] #value_counts sorts by counts, so first element will contain the max\n17                                               # print(counts)\n18         1            9      9.0      0.0      max_bins = len(sub_df) / max_cluster #\n19                                           \n20         1            3      3.0      0.0      if max_bins > 100: \n21         1            0      0.0      0.0          max_bins = 100 #if largest cluster <1% of available data, then we can percentile_rank\n22                                           \n23                                           \n24         1            0      0.0      0.0      if max_bins < 5: \n25                                                   sub_df[:] = 50 #if we don't have the resolution to quintile rank then assume no data.\n26                                           \n27                                               #     return sub_df\n28                                           \n29         1            1      1.0      0.0      bins = int(max_bins) # bin using highest resolution that the data supports, subject to constraints above (max 100 bins, min 5 bins)\n30                                           \n31                                               #should track bin resolution for all data.  To add.\n32                                           \n33                                               #if get here, then neither nans_ranked, nor sub_df are empty\n34                                               # sub_df_ranked = pd.qcut(sub_df, bins, labels=False)\n35         1       160530 160530.0     45.0      sub_df_ranked = (sub_df.rank(ascending = True) - 1)*100/len(x)\n36                                           \n37         1         5777   5777.0      1.6      ranked_df = pd.concat([sub_df_ranked, nans_ranked])\n38                                               \n39         1            1      1.0      0.0      return ranked_df\n""]";"['.rank(', ""ranked = df[['period_id', 'sector_name'] + to_rank].groupby(['period_id', 'sector_name']).transform(lambda x: (x.rank(ascending = True) - 1)*100/len(x))        \n"", '.apply(lambda x', '.value_counts()', 'pd.qcut()', 'df.rank()', 'pd.qcut()', ""import pandas as pd\nimport numpy as np\nimport random\n\nto_rank = ['var_1', 'var_2', 'var_3']\ndf = pd.DataFrame({'var_1' : np.random.randn(1000), 'var_2' : np.random.randn(1000), 'var_3' : np.random.randn(1000)})\ndf['date_id'] = np.random.choice(range(2001, 2012), df.shape[0])\ndf['category'] = ','.join(chr(random.randrange(97, 97 + 4 + 1)).upper() for x in range(1,df.shape[0]+1)).split(',')\n"", ""def rank_fun(df, to_rank): # calls ranking function f(x) to rank each category at each date\n    #extra data tidying logic here beyond scope of question - can remove\n    ranked = df[to_rank].apply(lambda x: f(x))\n    return ranked\n\n\ndef f(x):\n    nans = x[np.isnan(x)] # Remove nans as these will be ranked with 50\n    sub_df = x.dropna() # \n    nans_ranked = nans.replace(np.nan, 50) # give nans rank of 50\n\n    if len(sub_df.index) == 0: #check not all nan.  If no non-nan data, then return with rank 50\n        return nans_ranked\n\n    if len(sub_df.unique()) == 1: # if all data has same value, return rank 50\n        sub_df[:] = 50\n        return sub_df\n\n    #Check that we don't have too many clustered values, such that we can't bin due to overlap of ties, and reduce bin size provided we can at least quintile rank.\n    max_cluster = sub_df.value_counts().iloc[0] #value_counts sorts by counts, so first element will contain the max\n    max_bins = len(sub_df) / max_cluster \n\n    if max_bins > 100: #if largest cluster <1% of available data, then we can percentile_rank\n        max_bins = 100\n\n    if max_bins < 5: #if we don't have the resolution to quintile rank then assume no data.\n        sub_df[:] = 50\n        return sub_df\n\n    bins = int(max_bins) # bin using highest resolution that the data supports, subject to constraints above (max 100 bins, min 5 bins)\n\n    sub_df_ranked = pd.qcut(sub_df, bins, labels=False) #currently using pd.qcut.  pd.rank( seems to have extra functionality, but overheads similar in practice\n    sub_df_ranked *= (100 / bins) #Since we bin using the resolution specified in bins, to convert back to decile rank, we have to multiply by 100/bins.  E.g. with quintiles, we'll have scores 1 - 5, so have to multiply by 100 / 5 = 20 to convert to percentile ranking\n    ranked_df = pd.concat([sub_df_ranked, nans_ranked])\n    return ranked_df\n"", ""# ensure don't get duplicate columns if ranking already executed\nranked_cols = [col + '_ranked' for col in to_rank]\n\nranked = df[['date_id', 'category'] + to_rank].groupby(['date_id', 'category'], as_index = False).apply(lambda x: rank_fun(x, to_rank)) \nranked.columns = ranked_cols        \nranked.reset_index(inplace = True)\nranked.set_index('level_1', inplace = True)    \ndf = df.join(ranked[ranked_cols])\n"", 'pd.qcut(', 'df.rank(', '%lprun', 'f(x)', '.apply(lambda x', "" 2                                           def tst_fun(df, field):\n 3         1          685    685.0      0.2      x = df[field]\n 4         1        20726  20726.0      5.8      nans = x[np.isnan(x)]\n 5         1        28448  28448.0      8.0      sub_df = x.dropna()\n 6         1          387    387.0      0.1      nans_ranked = nans.replace(np.nan, 50)\n 7         1            5      5.0      0.0      if len(sub_df.index) == 0: \n 8                                                   pass #check not empty.  May be empty due to nans for first 5 years e.g. no revenue/operating margin data pre 1990\n 9                                                   return nans_ranked\n10                                           \n11         1        65559  65559.0     18.4      if len(sub_df.unique()) == 1: \n12                                                   sub_df[:] = 50 #e.g. for subranks where all factors had nan so ranked as 50 e.g. in 1990\n13                                                   return sub_df\n14                                           \n15                                               #Finally, check that we don't have too many clustered values, such that we can't bin, and reduce bin size provided we can at least quintile rank.\n16         1        74610  74610.0     20.9      max_cluster = sub_df.value_counts().iloc[0] #value_counts sorts by counts, so first element will contain the max\n17                                               # print(counts)\n18         1            9      9.0      0.0      max_bins = len(sub_df) / max_cluster #\n19                                           \n20         1            3      3.0      0.0      if max_bins > 100: \n21         1            0      0.0      0.0          max_bins = 100 #if largest cluster <1% of available data, then we can percentile_rank\n22                                           \n23                                           \n24         1            0      0.0      0.0      if max_bins < 5: \n25                                                   sub_df[:] = 50 #if we don't have the resolution to quintile rank then assume no data.\n26                                           \n27                                               #     return sub_df\n28                                           \n29         1            1      1.0      0.0      bins = int(max_bins) # bin using highest resolution that the data supports, subject to constraints above (max 100 bins, min 5 bins)\n30                                           \n31                                               #should track bin resolution for all data.  To add.\n32                                           \n33                                               #if get here, then neither nans_ranked, nor sub_df are empty\n34                                               # sub_df_ranked = pd.qcut(sub_df, bins, labels=False)\n35         1       160530 160530.0     45.0      sub_df_ranked = (sub_df.rank(ascending = True) - 1)*100/len(x)\n36                                           \n37         1         5777   5777.0      1.6      ranked_df = pd.concat([sub_df_ranked, nans_ranked])\n38                                               \n39         1            1      1.0      0.0      return ranked_df\n""]";"[""ranked = df[['period_id', 'sector_name'] + to_rank].groupby(['period_id', 'sector_name']).transform(lambda x: (x.rank(ascending = True) - 1)*100/len(x))        \nimport pandas as pd\nimport numpy as np\nimport random\n\nto_rank = ['var_1', 'var_2', 'var_3']\ndf = pd.DataFrame({'var_1' : np.random.randn(1000), 'var_2' : np.random.randn(1000), 'var_3' : np.random.randn(1000)})\ndf['date_id'] = np.random.choice(range(2001, 2012), df.shape[0])\ndf['category'] = ','.join(chr(random.randrange(97, 97 + 4 + 1)).upper() for x in range(1,df.shape[0]+1)).split(',')\n    #extra data tidying logic here beyond scope of question - can remove\n\n\n\n\n\n    #Check that we don't have too many clustered values, such that we can't bin due to overlap of ties, and reduce bin size provided we can at least quintile rank.\n\n\n\n\n# ensure don't get duplicate columns if ranking already executed\nranked_cols = [col + '_ranked' for col in to_rank]\n\nranked = df[['date_id', 'category'] + to_rank].groupby(['date_id', 'category'], as_index = False).apply(lambda x: rank_fun(x, to_rank)) \nranked.columns = ranked_cols        \nranked.reset_index(inplace = True)\nranked.set_index('level_1', inplace = True)    \ndf = df.join(ranked[ranked_cols])\n10                                           \n14                                           \n15                                               #Finally, check that we don't have too many clustered values, such that we can't bin, and reduce bin size provided we can at least quintile rank.\n17                                               # print(counts)\n19                                           \n22                                           \n23                                           \n26                                           \n27                                               #     return sub_df\n28                                           \n30                                           \n31                                               #should track bin resolution for all data.  To add.\n32                                           \n33                                               #if get here, then neither nans_ranked, nor sub_df are empty\n34                                               # sub_df_ranked = pd.qcut(sub_df, bins, labels=False)\n36                                           \n38                                               \n""]";"[""ranked = df[['period_id', 'sector_name'] + to_rank].groupby(['period_id', 'sector_name']).transform(lambda x: (x.rank(ascending = True) - 1)*100/len(x))        \nimport pandas as pd\nimport numpy as np\nimport random\n\nto_rank = ['var_1', 'var_2', 'var_3']\ndf = pd.DataFrame({'var_1' : np.random.randn(1000), 'var_2' : np.random.randn(1000), 'var_3' : np.random.randn(1000)})\ndf['date_id'] = np.random.choice(range(2001, 2012), df.shape[0])\ndf['category'] = ','.join(chr(random.randrange(97, 97 + 4 + 1)).upper() for x in range(1,df.shape[0]+1)).split(',')\n    #extra data tidying logic here beyond scope of question - can remove\n\n\n\n\n\n    #Check that we don't have too many clustered values, such that we can't bin due to overlap of ties, and reduce bin size provided we can at least quintile rank.\n\n\n\n\n# ensure don't get duplicate columns if ranking already executed\nranked_cols = [col + '_ranked' for col in to_rank]\n\nranked = df[['date_id', 'category'] + to_rank].groupby(['date_id', 'category'], as_index = False).apply(lambda x: rank_fun(x, to_rank)) \nranked.columns = ranked_cols        \nranked.reset_index(inplace = True)\nranked.set_index('level_1', inplace = True)    \ndf = df.join(ranked[ranked_cols])\n10                                           \n14                                           \n15                                               #Finally, check that we don't have too many clustered values, such that we can't bin, and reduce bin size provided we can at least quintile rank.\n17                                               # print(counts)\n19                                           \n22                                           \n23                                           \n26                                           \n27                                               #     return sub_df\n28                                           \n30                                           \n31                                               #should track bin resolution for all data.  To add.\n32                                           \n33                                               #if get here, then neither nans_ranked, nor sub_df are empty\n34                                               # sub_df_ranked = pd.qcut(sub_df, bins, labels=False)\n36                                           \n38                                               \n""]";False;"[""import pandas as pd\ndata = pd.DataFrame()\nranked = df[['period_id', 'sector_name'] + to_rank].groupby(['period_id', 'sector_name']).transform(lambda x: (x.rank(ascending = True) - 1)*100/len(x))        \nimport pandas as pd\nimport numpy as np\nimport random\n\nto_rank = ['var_1', 'var_2', 'var_3']\ndf = pd.DataFrame({'var_1' : np.random.randn(1000), 'var_2' : np.random.randn(1000), 'var_3' : np.random.randn(1000)})\ndf['date_id'] = np.random.choice(range(2001, 2012), df.shape[0])\ndf['category'] = ','.join(chr(random.randrange(97, 97 + 4 + 1)).upper() for x in range(1,df.shape[0]+1)).split(',')\n    #extra data tidying logic here beyond scope of question - can remove\n\n\n\n\n\n    #Check that we don't have too many clustered values, such that we can't bin due to overlap of ties, and reduce bin size provided we can at least quintile rank.\n\n\n\n\n# ensure don't get duplicate columns if ranking already executed\nranked_cols = [col + '_ranked' for col in to_rank]\n\nranked = df[['date_id', 'category'] + to_rank].groupby(['date_id', 'category'], as_index = False).apply(lambda x: rank_fun(x, to_rank)) \nranked.columns = ranked_cols        \nranked.reset_index(inplace = True)\nranked.set_index('level_1', inplace = True)    \ndf = df.join(ranked[ranked_cols])\n10                                           \n14                                           \n15                                               #Finally, check that we don't have too many clustered values, such that we can't bin, and reduce bin size provided we can at least quintile rank.\n17                                               # print(counts)\n19                                           \n22                                           \n23                                           \n26                                           \n27                                               #     return sub_df\n28                                           \n30                                           \n31                                               #should track bin resolution for all data.  To add.\n32                                           \n33                                               #if get here, then neither nans_ranked, nor sub_df are empty\n34                                               # sub_df_ranked = pd.qcut(sub_df, bins, labels=False)\n36                                           \n38                                               \n""]";True;0;0;[];[];0;0;[];[];0;0;[];[]
1177;1177;1177;1177;2.0;2;44123874;;1;12;<python><pandas><numpy><dataframe>;'DataFrame' object has no attribute 'sort';8204.0;"[""final.loc[-1] =['', 'P','Actual']\nfinal.index = final.index + 1  # shifting index\nfinal = final.sort()\nfinal.columns=[final.columns,final.iloc[0]]\nfinal = final.iloc[1:].reset_index(drop=True)\nfinal.columns.names = (None, None)\n""]";"[""final.loc[-1] =['', 'P','Actual']\nfinal.index = final.index + 1  # shifting index\nfinal = final.sort()\nfinal.columns=[final.columns,final.iloc[0]]\nfinal = final.iloc[1:].reset_index(drop=True)\nfinal.columns.names = (None, None)\n""]";"[""final.loc[-1] =['', 'P','Actual']\nfinal.index = final.index + 1  # shifting index\nfinal = final.sort()\nfinal.columns=[final.columns,final.iloc[0]]\nfinal = final.iloc[1:].reset_index(drop=True)\nfinal.columns.names = (None, None)\n""]";"[""final.loc[-1] =['', 'P','Actual']\nfinal.index = final.index + 1  # shifting index\nfinal = final.sort()\nfinal.columns=[final.columns,final.iloc[0]]\nfinal = final.iloc[1:].reset_index(drop=True)\nfinal.columns.names = (None, None)\n""]";"[""final.loc[-1] =['', 'P','Actual']\nfinal.index = final.index + 1  # shifting index\nfinal = final.sort()\nfinal.columns=[final.columns,final.iloc[0]]\nfinal = final.iloc[1:].reset_index(drop=True)\nfinal.columns.names = (None, None)\n""]";False;"[""import pandas as pd\nfinal.loc[-1] =['', 'P','Actual']\nfinal.index = final.index + 1  # shifting index\nfinal = final.sort()\nfinal.columns=[final.columns,final.iloc[0]]\nfinal = final.iloc[1:].reset_index(drop=True)\nfinal.columns.names = (None, None)\n""]";False;1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess'];1;1;['Sucess'];['Sucess']
1178;1178;1178;1178;6.0;6;44207926;;1;12;<python><pandas><numpy>;How to I factorize a list of tuples?;401.0;"[""tups = [(1, 2), ('a', 'b'), (3, 4), ('c', 5), (6, 'd'), ('a', 'b'), (3, 4)]\n[0, 1, 2, 3, 4, 1, 2]\npd.factorize(tups)[0]\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-84-c84947ac948c> in <module>()\n----> 1 pd.factorize(tups)[0]\n\n//anaconda/envs/3.6/lib/python3.6/site-packages/pandas/core/algorithms.py in factorize(values, sort, order, na_sentinel, size_hint)\n    553     uniques = vec_klass()\n    554     check_nulls = not is_integer_dtype(original)\n--> 555     labels = table.get_labels(values, uniques, 0, na_sentinel, check_nulls)\n    556 \n    557     labels = _ensure_platform_int(labels)\n\npandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_labels (pandas/_libs/hashtable.c:21804)()\n\nValueError: Buffer has wrong number of dimensions (expected 1, got 2)\nnp.unique(tups, return_inverse=1)[1]\n\narray([0, 1, 6, 7, 2, 3, 8, 4, 5, 9, 6, 7, 2, 3])\npd.factorize([hash(t) for t in tups])[0]\n\narray([0, 1, 2, 3, 4, 1, 2])\nlst = [10, 7, 4, 33, 1005, 7, 4]\n\n%timeit pd.factorize(lst * 1000)[0]\n1000 loops, best of 3: 506 µs per loop\n\n%timeit pd.factorize([hash(i) for i in lst * 1000])[0]\n1000 loops, best of 3: 937 µs per loop\nfrom itertools import count\n\ndef champ(tups):\n    d = {}\n    c = count()\n    return np.array(\n        [d[tup] if tup in d else d.setdefault(tup, next(c)) for tup in tups]\n    )\n\ndef root(tups):\n    return pd.Series(tups).factorize()[0]\n\ndef iobe(tups):\n    return np.unique(tups, return_inverse=True, axis=0)[1]\n\ndef get_row_view(a):\n    void_dt = np.dtype((np.void, a.dtype.itemsize * np.prod(a.shape[1:])))\n    a = np.ascontiguousarray(a)\n    return a.reshape(a.shape[0], -1).view(void_dt).ravel()\n\ndef diva(tups):\n    return np.unique(get_row_view(np.array(tups)), return_inverse=1)[1]\n\ndef gdib(tups):\n    return pd.factorize([str(t) for t in tups])[0]\n\nfrom string import ascii_letters\n\ndef tups_creator_1(size, len_of_str=3, num_ints_to_choose_from=1000, seed=None):\n    c = len_of_str\n    n = num_ints_to_choose_from\n    np.random.seed(seed)\n    d = pd.DataFrame(np.random.choice(list(ascii_letters), (size, c))).sum(1).tolist()\n    i = np.random.randint(n, size=size)\n    return list(zip(d, i))\n\nresults = pd.DataFrame(\n    index=pd.Index([100, 1000, 5000, 10000, 20000, 30000, 40000, 50000], name='Size'),\n    columns=pd.Index('champ root iobe diva gdib'.split(), name='Method')\n)\n\nfor i in results.index:\n    tups = tups_creator_1(i, max(1, int(np.log10(i))), max(10, i // 10))\n    for j in results.columns:\n        stmt = '{}(tups)'.format(j)\n        setup = 'from __main__ import {}, tups'.format(j)\n        results.set_value(i, j, timeit(stmt, setup, number=100) / 100)\n\nresults.plot(title='Avg Seconds', logx=True, logy=True)\n""]";"[""tups = [(1, 2), ('a', 'b'), (3, 4), ('c', 5), (6, 'd'), ('a', 'b'), (3, 4)]\n"", '[0, 1, 2, 3, 4, 1, 2]\n', 'pd.factorize(tups)[0]\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-84-c84947ac948c> in <module>()\n----> 1 pd.factorize(tups)[0]\n\n//anaconda/envs/3.6/lib/python3.6/site-packages/pandas/core/algorithms.py in factorize(values, sort, order, na_sentinel, size_hint)\n    553     uniques = vec_klass()\n    554     check_nulls = not is_integer_dtype(original)\n--> 555     labels = table.get_labels(values, uniques, 0, na_sentinel, check_nulls)\n    556 \n    557     labels = _ensure_platform_int(labels)\n\npandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_labels (pandas/_libs/hashtable.c:21804)()\n\nValueError: Buffer has wrong number of dimensions (expected 1, got 2)\n', 'np.unique(tups, return_inverse=1)[1]\n\narray([0, 1, 6, 7, 2, 3, 8, 4, 5, 9, 6, 7, 2, 3])\n', 'pd.factorize([hash(t) for t in tups])[0]\n\narray([0, 1, 2, 3, 4, 1, 2])\n', 'lst = [10, 7, 4, 33, 1005, 7, 4]\n\n%timeit pd.factorize(lst * 1000)[0]\n1000 loops, best of 3: 506 µs per loop\n\n%timeit pd.factorize([hash(i) for i in lst * 1000])[0]\n1000 loops, best of 3: 937 µs per loop\n', ""from itertools import count\n\ndef champ(tups):\n    d = {}\n    c = count()\n    return np.array(\n        [d[tup] if tup in d else d.setdefault(tup, next(c)) for tup in tups]\n    )\n\ndef root(tups):\n    return pd.Series(tups).factorize()[0]\n\ndef iobe(tups):\n    return np.unique(tups, return_inverse=True, axis=0)[1]\n\ndef get_row_view(a):\n    void_dt = np.dtype((np.void, a.dtype.itemsize * np.prod(a.shape[1:])))\n    a = np.ascontiguousarray(a)\n    return a.reshape(a.shape[0], -1).view(void_dt).ravel()\n\ndef diva(tups):\n    return np.unique(get_row_view(np.array(tups)), return_inverse=1)[1]\n\ndef gdib(tups):\n    return pd.factorize([str(t) for t in tups])[0]\n\nfrom string import ascii_letters\n\ndef tups_creator_1(size, len_of_str=3, num_ints_to_choose_from=1000, seed=None):\n    c = len_of_str\n    n = num_ints_to_choose_from\n    np.random.seed(seed)\n    d = pd.DataFrame(np.random.choice(list(ascii_letters), (size, c))).sum(1).tolist()\n    i = np.random.randint(n, size=size)\n    return list(zip(d, i))\n\nresults = pd.DataFrame(\n    index=pd.Index([100, 1000, 5000, 10000, 20000, 30000, 40000, 50000], name='Size'),\n    columns=pd.Index('champ root iobe diva gdib'.split(), name='Method')\n)\n\nfor i in results.index:\n    tups = tups_creator_1(i, max(1, int(np.log10(i))), max(10, i // 10))\n    for j in results.columns:\n        stmt = '{}(tups)'.format(j)\n        setup = 'from __main__ import {}, tups'.format(j)\n        results.set_value(i, j, timeit(stmt, setup, number=100) / 100)\n\nresults.plot(title='Avg Seconds', logx=True, logy=True)\n""]";"['tups', ""tups = [(1, 2), ('a', 'b'), (3, 4), ('c', 5), (6, 'd'), ('a', 'b'), (3, 4)]\n"", '[0, 1, 2, 3, 4, 1, 2]\n', 'pandas.factorize', 'pd.factorize(tups)[0]\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-84-c84947ac948c> in <module>()\n----> 1 pd.factorize(tups)[0]\n\n//anaconda/envs/3.6/lib/python3.6/site-packages/pandas/core/algorithms.py in factorize(values, sort, order, na_sentinel, size_hint)\n    553     uniques = vec_klass()\n    554     check_nulls = not is_integer_dtype(original)\n--> 555     labels = table.get_labels(values, uniques, 0, na_sentinel, check_nulls)\n    556 \n    557     labels = _ensure_platform_int(labels)\n\npandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_labels (pandas/_libs/hashtable.c:21804)()\n\nValueError: Buffer has wrong number of dimensions (expected 1, got 2)\n', 'numpy.unique', 'np.unique(tups, return_inverse=1)[1]\n\narray([0, 1, 6, 7, 2, 3, 8, 4, 5, 9, 6, 7, 2, 3])\n', 'pd.factorize([hash(t) for t in tups])[0]\n\narray([0, 1, 2, 3, 4, 1, 2])\n', 'lst = [10, 7, 4, 33, 1005, 7, 4]\n\n%timeit pd.factorize(lst * 1000)[0]\n1000 loops, best of 3: 506 µs per loop\n\n%timeit pd.factorize([hash(i) for i in lst * 1000])[0]\n1000 loops, best of 3: 937 µs per loop\n', 'code', ""from itertools import count\n\ndef champ(tups):\n    d = {}\n    c = count()\n    return np.array(\n        [d[tup] if tup in d else d.setdefault(tup, next(c)) for tup in tups]\n    )\n\ndef root(tups):\n    return pd.Series(tups).factorize()[0]\n\ndef iobe(tups):\n    return np.unique(tups, return_inverse=True, axis=0)[1]\n\ndef get_row_view(a):\n    void_dt = np.dtype((np.void, a.dtype.itemsize * np.prod(a.shape[1:])))\n    a = np.ascontiguousarray(a)\n    return a.reshape(a.shape[0], -1).view(void_dt).ravel()\n\ndef diva(tups):\n    return np.unique(get_row_view(np.array(tups)), return_inverse=1)[1]\n\ndef gdib(tups):\n    return pd.factorize([str(t) for t in tups])[0]\n\nfrom string import ascii_letters\n\ndef tups_creator_1(size, len_of_str=3, num_ints_to_choose_from=1000, seed=None):\n    c = len_of_str\n    n = num_ints_to_choose_from\n    np.random.seed(seed)\n    d = pd.DataFrame(np.random.choice(list(ascii_letters), (size, c))).sum(1).tolist()\n    i = np.random.randint(n, size=size)\n    return list(zip(d, i))\n\nresults = pd.DataFrame(\n    index=pd.Index([100, 1000, 5000, 10000, 20000, 30000, 40000, 50000], name='Size'),\n    columns=pd.Index('champ root iobe diva gdib'.split(), name='Method')\n)\n\nfor i in results.index:\n    tups = tups_creator_1(i, max(1, int(np.log10(i))), max(10, i // 10))\n    for j in results.columns:\n        stmt = '{}(tups)'.format(j)\n        setup = 'from __main__ import {}, tups'.format(j)\n        results.set_value(i, j, timeit(stmt, setup, number=100) / 100)\n\nresults.plot(title='Avg Seconds', logx=True, logy=True)\n""]";"[""tups = [(1, 2), ('a', 'b'), (3, 4), ('c', 5), (6, 'd'), ('a', 'b'), (3, 4)]\n[0, 1, 2, 3, 4, 1, 2]\npd.factorize(tups)[0]\n\n\n\n\nnp.unique(tups, return_inverse=1)[1]\n\narray([0, 1, 6, 7, 2, 3, 8, 4, 5, 9, 6, 7, 2, 3])\npd.factorize([hash(t) for t in tups])[0]\n\narray([0, 1, 2, 3, 4, 1, 2])\nlst = [10, 7, 4, 33, 1005, 7, 4]\n\n\nfrom itertools import count\n\n\n\n\n\n\n\nfrom string import ascii_letters\n\n\n\n\nresults.plot(title='Avg Seconds', logx=True, logy=True)\n""]";"[""import pandas as pd\ntups = [(1, 2), ('a', 'b'), (3, 4), ('c', 5), (6, 'd'), ('a', 'b'), (3, 4)]\n[0, 1, 2, 3, 4, 1, 2]\npd.factorize(tups)[0]\n\n\n\n\nnp.unique(tups, return_inverse=1)[1]\n\narray([0, 1, 6, 7, 2, 3, 8, 4, 5, 9, 6, 7, 2, 3])\npd.factorize([hash(t) for t in tups])[0]\n\narray([0, 1, 2, 3, 4, 1, 2])\nlst = [10, 7, 4, 33, 1005, 7, 4]\n\n\nfrom itertools import count\n\n\n\n\n\n\n\nfrom string import ascii_letters\n\n\n\n\nresults.plot(title='Avg Seconds', logx=True, logy=True)\n""]";True;"[""import pandas as pd\ntups = [(1, 2), ('a', 'b'), (3, 4), ('c', 5), (6, 'd'), ('a', 'b'), (3, 4)]\n[0, 1, 2, 3, 4, 1, 2]\npd.factorize(tups)[0]\n\n\n\n\nnp.unique(tups, return_inverse=1)[1]\n\narray([0, 1, 6, 7, 2, 3, 8, 4, 5, 9, 6, 7, 2, 3])\npd.factorize([hash(t) for t in tups])[0]\n\narray([0, 1, 2, 3, 4, 1, 2])\nlst = [10, 7, 4, 33, 1005, 7, 4]\n\n\nfrom itertools import count\n\n\n\n\n\n\n\nfrom string import ascii_letters\n\n\n\n\nresults.plot(title='Avg Seconds', logx=True, logy=True)\n""]";False;1;2;"['Sucess', ""name 'pd' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'tups' is not defined""]";['Sucess', 'NameError'];1;2;"['Sucess', ""name 'tups' is not defined""]";['Sucess', 'NameError']
1179;1179;1179;1179;1.0;4;44380068;;1;21;<python><pandas><numpy><linear-regression><statsmodels>;Pandas rolling regression: alternatives to looping;700.0;"['from datetime import date\nfrom pandas_datareader.data import DataReader\nimport statsmodels.formula.api as smf\n\nsyms = {\'TWEXBMTH\' : \'usd\', \n        \'T10Y2YM\' : \'term_spread\', \n        \'PCOPPUSDM\' : \'copper\'\n       }\n\nstart = date(2000, 1, 1)\ndata = (DataReader(syms.keys(), \'fred\', start)\n        .pct_change()\n        .dropna())\ndata = data.rename(columns = syms)\ndata = data.assign(intercept = 1.) # required by statsmodels OLS\n\ndef sliding_windows(x, window):\n    """"""Create rolling/sliding windows of length ~window~.\n\n    Given an array of shape (y, z), it will return ""blocks"" of shape\n    (x - window + 1, window, z).""""""\n\n    return np.array([x[i:i + window] for i \n                    in range(0, x.shape[0] - window + 1)])\n\ndata.head(3)\nOut[33]: \n                 usd  term_spread    copper  intercept\nDATE                                                  \n2000-02-01  0.012573    -1.409091 -0.019972        1.0\n2000-03-01 -0.000079     2.000000 -0.037202        1.0\n2000-04-01  0.005642     0.518519 -0.033275        1.0\n\nwindow = 36\nwins = sliding_windows(data.values, window=window)\ny, x = wins[:, :, 0], wins[:, :, 1:]\n\ncoefs = []\n\nfor endog, exog in zip(y, x):\n    model = smf.OLS(endog, exog).fit()\n        # The full set of model attributes gets lost with each loop\n    coefs.append(model.params)\n\ndf = pd.DataFrame(coefs, columns=data.iloc[:, 1:].columns,\n                  index=data.index[window - 1:])\n\ndf.head(3) # rolling 36m coefficients\nOut[70]: \n            term_spread    copper  intercept\nDATE                                        \n2003-01-01    -0.000122 -0.018426   0.001937\n2003-02-01     0.000391 -0.015740   0.001597\n2003-03-01     0.000655 -0.016811   0.001546\n']";"['from datetime import date\nfrom pandas_datareader.data import DataReader\nimport statsmodels.formula.api as smf\n\nsyms = {\'TWEXBMTH\' : \'usd\', \n        \'T10Y2YM\' : \'term_spread\', \n        \'PCOPPUSDM\' : \'copper\'\n       }\n\nstart = date(2000, 1, 1)\ndata = (DataReader(syms.keys(), \'fred\', start)\n        .pct_change()\n        .dropna())\ndata = data.rename(columns = syms)\ndata = data.assign(intercept = 1.) # required by statsmodels OLS\n\ndef sliding_windows(x, window):\n    """"""Create rolling/sliding windows of length ~window~.\n\n    Given an array of shape (y, z), it will return ""blocks"" of shape\n    (x - window + 1, window, z).""""""\n\n    return np.array([x[i:i + window] for i \n                    in range(0, x.shape[0] - window + 1)])\n\ndata.head(3)\nOut[33]: \n                 usd  term_spread    copper  intercept\nDATE                                                  \n2000-02-01  0.012573    -1.409091 -0.019972        1.0\n2000-03-01 -0.000079     2.000000 -0.037202        1.0\n2000-04-01  0.005642     0.518519 -0.033275        1.0\n\nwindow = 36\nwins = sliding_windows(data.values, window=window)\ny, x = wins[:, :, 0], wins[:, :, 1:]\n\ncoefs = []\n\nfor endog, exog in zip(y, x):\n    model = smf.OLS(endog, exog).fit()\n        # The full set of model attributes gets lost with each loop\n    coefs.append(model.params)\n\ndf = pd.DataFrame(coefs, columns=data.iloc[:, 1:].columns,\n                  index=data.index[window - 1:])\n\ndf.head(3) # rolling 36m coefficients\nOut[70]: \n            term_spread    copper  intercept\nDATE                                        \n2003-01-01    -0.000122 -0.018426   0.001937\n2003-02-01     0.000391 -0.015740   0.001597\n2003-03-01     0.000655 -0.016811   0.001546\n']";"['MovingOLS', 'stats/ols', 'MovingOLS', 'model = pd.MovingOLS(y, x)', '.t_stat', '.rmse', '.std_err', 'rolling.apply', '.rolling', 'func', '.apply', 'from datetime import date\nfrom pandas_datareader.data import DataReader\nimport statsmodels.formula.api as smf\n\nsyms = {\'TWEXBMTH\' : \'usd\', \n        \'T10Y2YM\' : \'term_spread\', \n        \'PCOPPUSDM\' : \'copper\'\n       }\n\nstart = date(2000, 1, 1)\ndata = (DataReader(syms.keys(), \'fred\', start)\n        .pct_change()\n        .dropna())\ndata = data.rename(columns = syms)\ndata = data.assign(intercept = 1.) # required by statsmodels OLS\n\ndef sliding_windows(x, window):\n    """"""Create rolling/sliding windows of length ~window~.\n\n    Given an array of shape (y, z), it will return ""blocks"" of shape\n    (x - window + 1, window, z).""""""\n\n    return np.array([x[i:i + window] for i \n                    in range(0, x.shape[0] - window + 1)])\n\ndata.head(3)\nOut[33]: \n                 usd  term_spread    copper  intercept\nDATE                                                  \n2000-02-01  0.012573    -1.409091 -0.019972        1.0\n2000-03-01 -0.000079     2.000000 -0.037202        1.0\n2000-04-01  0.005642     0.518519 -0.033275        1.0\n\nwindow = 36\nwins = sliding_windows(data.values, window=window)\ny, x = wins[:, :, 0], wins[:, :, 1:]\n\ncoefs = []\n\nfor endog, exog in zip(y, x):\n    model = smf.OLS(endog, exog).fit()\n        # The full set of model attributes gets lost with each loop\n    coefs.append(model.params)\n\ndf = pd.DataFrame(coefs, columns=data.iloc[:, 1:].columns,\n                  index=data.index[window - 1:])\n\ndf.head(3) # rolling 36m coefficients\nOut[70]: \n            term_spread    copper  intercept\nDATE                                        \n2003-01-01    -0.000122 -0.018426   0.001937\n2003-02-01     0.000391 -0.015740   0.001597\n2003-03-01     0.000655 -0.016811   0.001546\n']";['from datetime import date\nfrom pandas_datareader.data import DataReader\nimport statsmodels.formula.api as smf\n\n\nstart = date(2000, 1, 1)\ndata = data.rename(columns = syms)\ndata = data.assign(intercept = 1.) # required by statsmodels OLS\n\n\n\n\ndata.head(3)\nDATE                                                  \n\nwindow = 36\nwins = sliding_windows(data.values, window=window)\ny, x = wins[:, :, 0], wins[:, :, 1:]\n\ncoefs = []\n\n        # The full set of model attributes gets lost with each loop\n\n\ndf.head(3) # rolling 36m coefficients\nDATE                                        \n'];['from datetime import date\nfrom pandas_datareader.data import DataReader\nimport statsmodels.formula.api as smf\n\n\nstart = date(2000, 1, 1)\ndata = data.rename(columns = syms)\ndata = data.assign(intercept = 1.) # required by statsmodels OLS\n\n\n\n\ndata.head(3)\nDATE                                                  \n\nwindow = 36\nwins = sliding_windows(data.values, window=window)\ny, x = wins[:, :, 0], wins[:, :, 1:]\n\ncoefs = []\n\n        # The full set of model attributes gets lost with each loop\n\n\ndf.head(3) # rolling 36m coefficients\nDATE                                        \n'];False;['import pandas as pd\ndf = pd.DataFrame()\nfrom datetime import date\nfrom pandas_datareader.data import DataReader\nimport statsmodels.formula.api as smf\n\n\nstart = date(2000, 1, 1)\ndata = data.rename(columns = syms)\ndata = data.assign(intercept = 1.) # required by statsmodels OLS\n\n\n\n\ndata.head(3)\nDATE                                                  \n\nwindow = 36\nwins = sliding_windows(data.values, window=window)\ny, x = wins[:, :, 0], wins[:, :, 1:]\n\ncoefs = []\n\n        # The full set of model attributes gets lost with each loop\n\n\ndf.head(3) # rolling 36m coefficients\nDATE                                        \n'];True;0;0;[];[];0;0;[];[];0;0;[];[]
1180;1180;1180;1180;8.0;8;44715393;;1;13;<python><pandas><memory><memory-management>;How to concatenate multiple pandas.DataFrames without running into MemoryError;433.0;['concat_df = pd.concat([df1, df2, df3])\n'];['concat_df = pd.concat([df1, df2, df3])\n'];['concat_df = pd.concat([df1, df2, df3])\n'];['concat_df = pd.concat([df1, df2, df3])\n'];['import pandas as pd\nconcat_df = pd.concat([df1, df2, df3])\n'];True;['import pandas as pd\ndf2 = pd.DataFrame()\ndf1 = pd.DataFrame()\nconcat_df = pd.concat([df1, df2, df3])\n'];True;0;2;"[""name 'df1' is not defined"", ""name 'df1' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df1' is not defined"", ""name 'df1' is not defined""]";['NameError', 'NameError'];0;2;"[""'bool' object is not iterable"", ""name 'filename' is not defined""]";['TypeError', 'NameError']
1181;1181;1181;1181;5.0;4;44946141;;1;12;<python><loops><csv><pandas><parallel-processing>;How to input large data into python pandas using looping or parallel computing?;332.0;"['file = ""./data.csv""\ndf = pd.read_csv(file, sep=""/"", header=0, dtype=str)\nid    venue           time             code    value ......\nAAA   Paris      28/05/2016 09:10      PAR      45   ......\n111   Budapest   14/08/2016 19:00      BUD      62   ......\nAAA   Tokyo      05/11/2016 23:20      TYO      56   ......\n111   LA         12/12/2016 05:55      LAX      05   ......\n111   New York   08/01/2016 04:25      NYC      14   ......\nAAA   Sydney     04/05/2016 21:40      SYD      2    ......\nABX   HongKong   28/03/2016 17:10      HKG      5    ......\nABX   London     25/07/2016 13:02      LON      22   ......\nAAA   Dubai      01/04/2016 18:45      DXB      19   ......\n.\n.\n.\n.\n']";"['file = ""./data.csv""\ndf = pd.read_csv(file, sep=""/"", header=0, dtype=str)\n', 'id    venue           time             code    value ......\nAAA   Paris      28/05/2016 09:10      PAR      45   ......\n111   Budapest   14/08/2016 19:00      BUD      62   ......\nAAA   Tokyo      05/11/2016 23:20      TYO      56   ......\n111   LA         12/12/2016 05:55      LAX      05   ......\n111   New York   08/01/2016 04:25      NYC      14   ......\nAAA   Sydney     04/05/2016 21:40      SYD      2    ......\nABX   HongKong   28/03/2016 17:10      HKG      5    ......\nABX   London     25/07/2016 13:02      LON      22   ......\nAAA   Dubai      01/04/2016 18:45      DXB      19   ......\n.\n.\n.\n.\n']";"['file = ""./data.csv""\ndf = pd.read_csv(file, sep=""/"", header=0, dtype=str)\n', 'id    venue           time             code    value ......\nAAA   Paris      28/05/2016 09:10      PAR      45   ......\n111   Budapest   14/08/2016 19:00      BUD      62   ......\nAAA   Tokyo      05/11/2016 23:20      TYO      56   ......\n111   LA         12/12/2016 05:55      LAX      05   ......\n111   New York   08/01/2016 04:25      NYC      14   ......\nAAA   Sydney     04/05/2016 21:40      SYD      2    ......\nABX   HongKong   28/03/2016 17:10      HKG      5    ......\nABX   London     25/07/2016 13:02      LON      22   ......\nAAA   Dubai      01/04/2016 18:45      DXB      19   ......\n.\n.\n.\n.\n']";"['file = ""./data.csv""\ndf = pd.read_csv(file, sep=""/"", header=0, dtype=str)\n']";"['import pandas as pd\nfile = ""./data.csv""\ndf = pd.read_csv(file, sep=""/"", header=0, dtype=str)\n']";True;"['import pandas as pd\ndata = pd.DataFrame()\nfile = ""./data.csv""\ndf = pd.read_csv(file, sep=""/"", header=0, dtype=str)\n']";True;0;2;"[""name 'pd' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""File b'./data.csv' does not exist"", ""name 'df' is not defined""]";['FileNotFoundError', 'NameError'];1;2;"[""File b'./data.csv' does not exist"", 'Sucess']";['FileNotFoundError', 'Sucess']
1182;1182;1182;1182;8.0;4;44998223;;1;16;<python><pandas><numpy><dataframe><duplicates>;Group duplicate column IDs in pandas dataframe;579.0;"[""df = pd.DataFrame({'A': [1, 2, 3, 4, 5],'B': [2, 4, 2, 1, 9],\n                   'C': [1, 2, 3, 4, 5],'D': [2, 4, 2, 1, 9],\n                   'E': [3, 4, 2, 1, 2],'F': [1, 1, 1, 1, 1]},\n                   index = ['a1', 'a2', 'a3', 'a4', 'a5'])\n[('A', 'C'), ('B', 'D')]\n""]";"[""df = pd.DataFrame({'A': [1, 2, 3, 4, 5],'B': [2, 4, 2, 1, 9],\n                   'C': [1, 2, 3, 4, 5],'D': [2, 4, 2, 1, 9],\n                   'E': [3, 4, 2, 1, 2],'F': [1, 1, 1, 1, 1]},\n                   index = ['a1', 'a2', 'a3', 'a4', 'a5'])\n"", ""[('A', 'C'), ('B', 'D')]\n""]";"[""df = pd.DataFrame({'A': [1, 2, 3, 4, 5],'B': [2, 4, 2, 1, 9],\n                   'C': [1, 2, 3, 4, 5],'D': [2, 4, 2, 1, 9],\n                   'E': [3, 4, 2, 1, 2],'F': [1, 1, 1, 1, 1]},\n                   index = ['a1', 'a2', 'a3', 'a4', 'a5'])\n"", ""[('A', 'C'), ('B', 'D')]\n""]";"[""[('A', 'C'), ('B', 'D')]\n""]";"[""[('A', 'C'), ('B', 'D')]\n""]";False;"[""import pandas as pd\n[('A', 'C'), ('B', 'D')]\n""]";False;0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'df' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'cross' is not defined"", ""name 'group_duplicate_cols' is not defined""]";['NameError', 'NameError']
1183;1183;1183;1183;3.0;0;45193131;;1;11;<python><pandas>;return n smallest indexes by column using pandas;369.0;"[""df = pd.DataFrame({'X': [1, 2, 3, 4, 5,6,7,8,9,10],\n'Y': [10,20,30,40,50,-10,-20,-30,-40,-50],\n'Z': [20,18,16,14,12,10,8,6,4,2]},index=list('ABCDEFGHIJ'))\n    X   Y   Z\nA   1  10  20\nB   2  20  18\nC   3  30  16\nD   4  40  14\nE   5  50  12\nF   6 -10  10\nG   7 -20   8\nH   8 -30   6\nI   9 -40   4\nJ  10 -50   2\n   X  Y  Z\n0  A  J  J\n1  B  I  I\n2  C  H  H\n""]";"[""df = pd.DataFrame({'X': [1, 2, 3, 4, 5,6,7,8,9,10],\n'Y': [10,20,30,40,50,-10,-20,-30,-40,-50],\n'Z': [20,18,16,14,12,10,8,6,4,2]},index=list('ABCDEFGHIJ'))\n"", '    X   Y   Z\nA   1  10  20\nB   2  20  18\nC   3  30  16\nD   4  40  14\nE   5  50  12\nF   6 -10  10\nG   7 -20   8\nH   8 -30   6\nI   9 -40   4\nJ  10 -50   2\n', '   X  Y  Z\n0  A  J  J\n1  B  I  I\n2  C  H  H\n']";"[""df = pd.DataFrame({'X': [1, 2, 3, 4, 5,6,7,8,9,10],\n'Y': [10,20,30,40,50,-10,-20,-30,-40,-50],\n'Z': [20,18,16,14,12,10,8,6,4,2]},index=list('ABCDEFGHIJ'))\n"", '    X   Y   Z\nA   1  10  20\nB   2  20  18\nC   3  30  16\nD   4  40  14\nE   5  50  12\nF   6 -10  10\nG   7 -20   8\nH   8 -30   6\nI   9 -40   4\nJ  10 -50   2\n', '   X  Y  Z\n0  A  J  J\n1  B  I  I\n2  C  H  H\n']";[''];[''];False;['import pandas as pd\n'];False;0;2;"[""name 'df' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError'];0;2;"[""name 'df' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError'];1;2;"['Sucess', ""name 'np' is not defined""]";['Sucess', 'NameError']
1184;1184;1184;1184;1.0;1;45254174;;1;13;<python><pandas><dataframe>;How do pandas Rolling objects work?;402.0;"[""print(type(df.rolling))\n<class 'pandas.core.window.Rolling'>\ndef rwindows(a, window):\n    if a.ndim == 1:\n        a = a.reshape(-1, 1)\n    shape = a.shape[0] - window + 1, window, a.shape[-1]\n    strides = (a.strides[0],) + a.strides\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.squeeze(windows)\n# as_strided version\n\na = np.arange(5)\nprint(rwindows(a, 3))           # 1d input\n[[0 1 2]\n [1 2 3]\n [2 3 4]]\n\nb = np.arange(10).reshape(5,2)\nprint(rwindows(b, 4))           # 2d input\n[[[0 1]\n  [2 3]\n  [4 5]\n  [6 7]]\n\n [[2 3]\n  [4 5]\n  [6 7]\n  [8 9]]]\ndef prod(a, b):\n    return a * b\ndf.rolling(3).apply(prod, args=((df + 2).rolling(3),))\n-----------------------------------------------------------------------\n...\nTypeError: unsupported operand type(s) for *: 'float' and 'Rolling'\n""]";"[""print(type(df.rolling))\n<class 'pandas.core.window.Rolling'>\n"", 'def rwindows(a, window):\n    if a.ndim == 1:\n        a = a.reshape(-1, 1)\n    shape = a.shape[0] - window + 1, window, a.shape[-1]\n    strides = (a.strides[0],) + a.strides\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.squeeze(windows)\n', '# as_strided version\n\na = np.arange(5)\nprint(rwindows(a, 3))           # 1d input\n[[0 1 2]\n [1 2 3]\n [2 3 4]]\n\nb = np.arange(10).reshape(5,2)\nprint(rwindows(b, 4))           # 2d input\n[[[0 1]\n  [2 3]\n  [4 5]\n  [6 7]]\n\n [[2 3]\n  [4 5]\n  [6 7]\n  [8 9]]]\n', ""def prod(a, b):\n    return a * b\ndf.rolling(3).apply(prod, args=((df + 2).rolling(3),))\n-----------------------------------------------------------------------\n...\nTypeError: unsupported operand type(s) for *: 'float' and 'Rolling'\n""]";"['DataFrame.rolling', 'Series.rolling', ""print(type(df.rolling))\n<class 'pandas.core.window.Rolling'>\n"", 'np.as_strided', 'def rwindows(a, window):\n    if a.ndim == 1:\n        a = a.reshape(-1, 1)\n    shape = a.shape[0] - window + 1, window, a.shape[-1]\n    strides = (a.strides[0],) + a.strides\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.squeeze(windows)\n', 'rwindows', 'ndarray', '.rolling', 'ndarray', '__dict__', '_get_index()', '_create_blocks', 'strided', '# as_strided version\n\na = np.arange(5)\nprint(rwindows(a, 3))           # 1d input\n[[0 1 2]\n [1 2 3]\n [2 3 4]]\n\nb = np.arange(10).reshape(5,2)\nprint(rwindows(b, 4))           # 2d input\n[[[0 1]\n  [2 3]\n  [4 5]\n  [6 7]]\n\n [[2 3]\n  [4 5]\n  [6 7]\n  [8 9]]]\n', 'func', ""def prod(a, b):\n    return a * b\ndf.rolling(3).apply(prod, args=((df + 2).rolling(3),))\n-----------------------------------------------------------------------\n...\nTypeError: unsupported operand type(s) for *: 'float' and 'Rolling'\n"", '.rolling']";['print(type(df.rolling))\n# as_strided version\n\na = np.arange(5)\nprint(rwindows(a, 3))           # 1d input\n\nb = np.arange(10).reshape(5,2)\nprint(rwindows(b, 4))           # 2d input\n\ndf.rolling(3).apply(prod, args=((df + 2).rolling(3),))\n...\n'];['print(type(df.rolling))\n# as_strided version\n\na = np.arange(5)\nprint(rwindows(a, 3))           # 1d input\n\nb = np.arange(10).reshape(5,2)\nprint(rwindows(b, 4))           # 2d input\n\ndf.rolling(3).apply(prod, args=((df + 2).rolling(3),))\n...\n'];False;['import pandas as pd\ndf = pd.DataFrame()\nprint(type(df.rolling))\n# as_strided version\n\na = np.arange(5)\nprint(rwindows(a, 3))           # 1d input\n\nb = np.arange(10).reshape(5,2)\nprint(rwindows(b, 4))           # 2d input\n\ndf.rolling(3).apply(prod, args=((df + 2).rolling(3),))\n...\n'];True;0;0;[];[];0;0;[];[];0;0;[];[]
1185;1185;1185;1185;7.0;0;45332960;;1;14;<python><pandas><numpy>;Interweave two dataframes;596.0;"[""d1 = pd.DataFrame(np.ones((3, 3), dtype=int), list('abc'), [0, 1, 2])\nd2 = pd.DataFrame(np.zeros((3, 2), dtype=int), list('abc'), [3, 4])\nd1\n\n   0  1  2\na  1  1  1\nb  1  1  1\nc  1  1  1\nd2\n\n   3  4\na  0  0\nb  0  0\nc  0  0\npd.concat([d1[0], d2[3], d1[1], d2[4], d1[2]], axis=1)\n\n   0  3  1  4  2\na  1  0  1  0  1\nb  1  0  1  0  1\nc  1  0  1  0  1\n""]";"[""d1 = pd.DataFrame(np.ones((3, 3), dtype=int), list('abc'), [0, 1, 2])\nd2 = pd.DataFrame(np.zeros((3, 2), dtype=int), list('abc'), [3, 4])\n"", 'd1\n\n   0  1  2\na  1  1  1\nb  1  1  1\nc  1  1  1\n', 'd2\n\n   3  4\na  0  0\nb  0  0\nc  0  0\n', 'pd.concat([d1[0], d2[3], d1[1], d2[4], d1[2]], axis=1)\n\n   0  3  1  4  2\na  1  0  1  0  1\nb  1  0  1  0  1\nc  1  0  1  0  1\n']";"['d1', 'd2', ""d1 = pd.DataFrame(np.ones((3, 3), dtype=int), list('abc'), [0, 1, 2])\nd2 = pd.DataFrame(np.zeros((3, 2), dtype=int), list('abc'), [3, 4])\n"", 'd1\n\n   0  1  2\na  1  1  1\nb  1  1  1\nc  1  1  1\n', 'd2\n\n   3  4\na  0  0\nb  0  0\nc  0  0\n', 'd2', 'd1', 'pd.concat([d1[0], d2[3], d1[1], d2[4], d1[2]], axis=1)\n\n   0  3  1  4  2\na  1  0  1  0  1\nb  1  0  1  0  1\nc  1  0  1  0  1\n']";"[""d1 = pd.DataFrame(np.ones((3, 3), dtype=int), list('abc'), [0, 1, 2])\nd2 = pd.DataFrame(np.zeros((3, 2), dtype=int), list('abc'), [3, 4])\nd1\n\nd2\n\npd.concat([d1[0], d2[3], d1[1], d2[4], d1[2]], axis=1)\n\n""]";"[""import pandas as pd\nd1 = pd.DataFrame(np.ones((3, 3), dtype=int), list('abc'), [0, 1, 2])\nd2 = pd.DataFrame(np.zeros((3, 2), dtype=int), list('abc'), [3, 4])\nd1\n\nd2\n\npd.concat([d1[0], d2[3], d1[1], d2[4], d1[2]], axis=1)\n\n""]";True;"[""import pandas as pd\nd1 = pd.DataFrame(np.ones((3, 3), dtype=int), list('abc'), [0, 1, 2])\nd2 = pd.DataFrame(np.zeros((3, 2), dtype=int), list('abc'), [3, 4])\nd1\n\nd2\n\npd.concat([d1[0], d2[3], d1[1], d2[4], d1[2]], axis=1)\n\n""]";False;0;2;"[""No module named 'toolz'"", ""name 'd1' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'toolz'"", ""name 'd1' is not defined""]";['ImportError', 'NameError'];0;2;"[""No module named 'toolz'"", ""name 'd1' is not defined""]";['ImportError', 'NameError']
1186;1186;1186;1186;6.0;1;45494649;;1;13;<python><pandas><dataframe>;Return subset based on a list of boolean values;635.0;[''];[];['[0,1,0,0,1,1,0,0,0,1]'];[''];[''];False;['import pandas as pd\n'];False;0;4;"[""name 'pd' is not defined"", ""name 'df' is not defined"", ""name 'pd' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""name 'np' is not defined"", ""name 'df' is not defined"", ""name 'np' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError'];0;4;"[""name 'np' is not defined"", ""name 'np' is not defined"", ""name 'np' is not defined"", ""name 'np' is not defined""]";['NameError', 'NameError', 'NameError', 'NameError']
1187;1187;1187;1187;1.0;11;45740537;;1;12;<python><pandas><dataframe>;Copying MultiIndex dataframes with pd.read_clipboard?;199.0;['          C\nA   B      \n1.1 111  20\n    222  31\n3.3 222  24\n    333  65\n5.5 333  22\n6.6 777  74 \ndf = pd.read_clipboard(index_col=[0, 1])\nParserError: Error tokenizing data. C error: Expected 2 fields in line 3, saw 3\n'];['          C\nA   B      \n1.1 111  20\n    222  31\n3.3 222  24\n    333  65\n5.5 333  22\n6.6 777  74 \n', 'df = pd.read_clipboard(index_col=[0, 1])\n', 'ParserError: Error tokenizing data. C error: Expected 2 fields in line 3, saw 3\n'];['          C\nA   B      \n1.1 111  20\n    222  31\n3.3 222  24\n    333  65\n5.5 333  22\n6.6 777  74 \n', 'pd.read_clipboard', 'df = pd.read_clipboard(index_col=[0, 1])\n', 'ParserError: Error tokenizing data. C error: Expected 2 fields in line 3, saw 3\n', 'pd.read_clipboard'];['df = pd.read_clipboard(index_col=[0, 1])\n'];['import pandas as pd\ndf = pd.read_clipboard(index_col=[0, 1])\n'];True;['import pandas as pd\ndf = pd.read_clipboard(index_col=[0, 1])\n'];False;0;1;"[""name 'read_clipboard_mi' is not defined""]";['NameError'];0;1;"[""name 'read_clipboard_mi' is not defined""]";['NameError'];0;1;"[""name 'read_clipboard_mi' is not defined""]";['NameError']
1188;1188;1188;1188;2.0;3;45820242;;1;12;<python><python-3.x><pandas><recursion><finance>;Recursion: account value with distributions;264.0;"[""dist = 5000.\nv0 = float(1e6)\nr = pd.Series(np.random.rand(12) * .01,\n              index=pd.date_range('2017', freq='M', periods=12))\nvalue = pd.Series(np.empty_like(r), index=r.index)\nfrom pandas.tseries import offsets\nvalue = (value.append(Series(v0, index=[value.index[0] - offsets.MonthEnd(1)]))\n              .sort_index())\nfor date in value.index[1:]:\n    if date.is_quarter_end:\n        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] \\\n                        * (1 + r.loc[date]) - dist\n    else:\n        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] \\\n                        * (1 + r.loc[date]) \nimport pandas as pd\nfrom pandas.tseries import offsets\nfrom pandas import Series\nimport numpy as np\n\ndist = 5000.\nv0 = float(1e6)\nr = pd.Series(np.random.rand(12) * .01, index=pd.date_range('2017', freq='M', periods=12))\nvalue = pd.Series(np.empty_like(r), index=r.index)\nvalue = (value.append(Series(v0, index=[value.index[0] - offsets.MonthEnd(1)])).sort_index())\nfor date in value.index[1:]:\n    if date.is_quarter_end:\n        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] * (1 + r.loc[date]) - dist\n    else:\n        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] * (1 + r.loc[date]) \nfor each date in index of value:\n    if the date is not a quarter end:\n        multiply previous value by (1 + r) for that month\n    if the date is a quarter end:\n        multiply previous value by (1 + r) for that month and subtract dist\n""]";"['dist = 5000.\nv0 = float(1e6)\n', ""r = pd.Series(np.random.rand(12) * .01,\n              index=pd.date_range('2017', freq='M', periods=12))\n"", 'value = pd.Series(np.empty_like(r), index=r.index)\n', 'from pandas.tseries import offsets\nvalue = (value.append(Series(v0, index=[value.index[0] - offsets.MonthEnd(1)]))\n              .sort_index())\n', 'for date in value.index[1:]:\n    if date.is_quarter_end:\n        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] \\\n                        * (1 + r.loc[date]) - dist\n    else:\n        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] \\\n                        * (1 + r.loc[date]) \n', ""import pandas as pd\nfrom pandas.tseries import offsets\nfrom pandas import Series\nimport numpy as np\n\ndist = 5000.\nv0 = float(1e6)\nr = pd.Series(np.random.rand(12) * .01, index=pd.date_range('2017', freq='M', periods=12))\nvalue = pd.Series(np.empty_like(r), index=r.index)\nvalue = (value.append(Series(v0, index=[value.index[0] - offsets.MonthEnd(1)])).sort_index())\nfor date in value.index[1:]:\n    if date.is_quarter_end:\n        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] * (1 + r.loc[date]) - dist\n    else:\n        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] * (1 + r.loc[date]) \n"", 'for each date in index of value:\n    if the date is not a quarter end:\n        multiply previous value by (1 + r) for that month\n    if the date is a quarter end:\n        multiply previous value by (1 + r) for that month and subtract dist\n']";"['np.where', 'scipy.signal', 'dist = 5000.\nv0 = float(1e6)\n', ""r = pd.Series(np.random.rand(12) * .01,\n              index=pd.date_range('2017', freq='M', periods=12))\n"", 'value = pd.Series(np.empty_like(r), index=r.index)\n', 'value', 'v0', 'from pandas.tseries import offsets\nvalue = (value.append(Series(v0, index=[value.index[0] - offsets.MonthEnd(1)]))\n              .sort_index())\n', 'for date in value.index[1:]:\n    if date.is_quarter_end:\n        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] \\\n                        * (1 + r.loc[date]) - dist\n    else:\n        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] \\\n                        * (1 + r.loc[date]) \n', ""import pandas as pd\nfrom pandas.tseries import offsets\nfrom pandas import Series\nimport numpy as np\n\ndist = 5000.\nv0 = float(1e6)\nr = pd.Series(np.random.rand(12) * .01, index=pd.date_range('2017', freq='M', periods=12))\nvalue = pd.Series(np.empty_like(r), index=r.index)\nvalue = (value.append(Series(v0, index=[value.index[0] - offsets.MonthEnd(1)])).sort_index())\nfor date in value.index[1:]:\n    if date.is_quarter_end:\n        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] * (1 + r.loc[date]) - dist\n    else:\n        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] * (1 + r.loc[date]) \n"", 'for each date in index of value:\n    if the date is not a quarter end:\n        multiply previous value by (1 + r) for that month\n    if the date is a quarter end:\n        multiply previous value by (1 + r) for that month and subtract dist\n']";"[""dist = 5000.\nv0 = float(1e6)\nvalue = pd.Series(np.empty_like(r), index=r.index)\nfrom pandas.tseries import offsets\nimport pandas as pd\nfrom pandas.tseries import offsets\nfrom pandas import Series\nimport numpy as np\n\ndist = 5000.\nv0 = float(1e6)\nr = pd.Series(np.random.rand(12) * .01, index=pd.date_range('2017', freq='M', periods=12))\nvalue = pd.Series(np.empty_like(r), index=r.index)\nvalue = (value.append(Series(v0, index=[value.index[0] - offsets.MonthEnd(1)])).sort_index())\n""]";"[""dist = 5000.\nv0 = float(1e6)\nvalue = pd.Series(np.empty_like(r), index=r.index)\nfrom pandas.tseries import offsets\nimport pandas as pd\nfrom pandas.tseries import offsets\nfrom pandas import Series\nimport numpy as np\n\ndist = 5000.\nv0 = float(1e6)\nr = pd.Series(np.random.rand(12) * .01, index=pd.date_range('2017', freq='M', periods=12))\nvalue = pd.Series(np.empty_like(r), index=r.index)\nvalue = (value.append(Series(v0, index=[value.index[0] - offsets.MonthEnd(1)])).sort_index())\n""]";False;"[""import pandas as pd\ndist = 5000.\nv0 = float(1e6)\nvalue = pd.Series(np.empty_like(r), index=r.index)\nfrom pandas.tseries import offsets\nimport pandas as pd\nfrom pandas.tseries import offsets\nfrom pandas import Series\nimport numpy as np\n\ndist = 5000.\nv0 = float(1e6)\nr = pd.Series(np.random.rand(12) * .01, index=pd.date_range('2017', freq='M', periods=12))\nvalue = pd.Series(np.empty_like(r), index=r.index)\nvalue = (value.append(Series(v0, index=[value.index[0] - offsets.MonthEnd(1)])).sort_index())\n""]";False;0;1;"[""name 'r' is not defined""]";['NameError'];0;1;"[""name 'r' is not defined""]";['NameError'];0;1;"[""name 'r' is not defined""]";['NameError']
